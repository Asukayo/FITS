Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13336064.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4865407
	speed: 0.1526s/iter; left time: 3929.2654s
	iters: 200, epoch: 1 | loss: 0.4170183
	speed: 0.1410s/iter; left time: 3617.1788s
	iters: 300, epoch: 1 | loss: 0.4009430
	speed: 0.1426s/iter; left time: 3642.3593s
	iters: 400, epoch: 1 | loss: 0.4023440
	speed: 0.1384s/iter; left time: 3522.9714s
	iters: 500, epoch: 1 | loss: 0.3493643
	speed: 0.1381s/iter; left time: 3499.9815s
Epoch: 1 cost time: 73.74345541000366
Epoch: 1, Steps: 517 | Train Loss: 0.4631563 Vali Loss: 0.9700422 Test Loss: 0.4183573
Validation loss decreased (inf --> 0.970042).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3917540
	speed: 0.9251s/iter; left time: 23342.8561s
	iters: 200, epoch: 2 | loss: 0.3908713
	speed: 0.1392s/iter; left time: 3497.8676s
	iters: 300, epoch: 2 | loss: 0.3705388
	speed: 0.1372s/iter; left time: 3434.6906s
	iters: 400, epoch: 2 | loss: 0.3487034
	speed: 0.1315s/iter; left time: 3279.9200s
	iters: 500, epoch: 2 | loss: 0.3665170
	speed: 0.1302s/iter; left time: 3232.6546s
Epoch: 2 cost time: 70.9115617275238
Epoch: 2, Steps: 517 | Train Loss: 0.4020340 Vali Loss: 0.9442633 Test Loss: 0.4143307
Validation loss decreased (0.970042 --> 0.944263).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3524590
	speed: 0.9080s/iter; left time: 22443.8683s
	iters: 200, epoch: 3 | loss: 0.4158526
	speed: 0.1362s/iter; left time: 3352.1555s
	iters: 300, epoch: 3 | loss: 0.3944664
	speed: 0.1335s/iter; left time: 3274.2039s
	iters: 400, epoch: 3 | loss: 0.4259334
	speed: 0.1368s/iter; left time: 3339.0596s
	iters: 500, epoch: 3 | loss: 0.4280007
	speed: 0.1342s/iter; left time: 3262.6323s
Epoch: 3 cost time: 70.84153604507446
Epoch: 3, Steps: 517 | Train Loss: 0.3987357 Vali Loss: 0.9391162 Test Loss: 0.4149600
Validation loss decreased (0.944263 --> 0.939116).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3883250
	speed: 0.9557s/iter; left time: 23127.2223s
	iters: 200, epoch: 4 | loss: 0.3950288
	speed: 0.1500s/iter; left time: 3613.8243s
	iters: 300, epoch: 4 | loss: 0.4166485
	speed: 0.1531s/iter; left time: 3674.5454s
	iters: 400, epoch: 4 | loss: 0.3851050
	speed: 0.1480s/iter; left time: 3537.5995s
	iters: 500, epoch: 4 | loss: 0.3469140
	speed: 0.1503s/iter; left time: 3577.1394s
Epoch: 4 cost time: 79.10462856292725
Epoch: 4, Steps: 517 | Train Loss: 0.3979800 Vali Loss: 0.9377401 Test Loss: 0.4156190
Validation loss decreased (0.939116 --> 0.937740).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3984901
	speed: 0.9556s/iter; left time: 22631.1171s
	iters: 200, epoch: 5 | loss: 0.3995118
	speed: 0.1536s/iter; left time: 3621.3546s
	iters: 300, epoch: 5 | loss: 0.3716374
	speed: 0.1494s/iter; left time: 3508.7123s
	iters: 400, epoch: 5 | loss: 0.3732896
	speed: 0.1403s/iter; left time: 3280.5614s
	iters: 500, epoch: 5 | loss: 0.4071443
	speed: 0.1386s/iter; left time: 3226.4797s
Epoch: 5 cost time: 75.65310406684875
Epoch: 5, Steps: 517 | Train Loss: 0.3976023 Vali Loss: 0.9347472 Test Loss: 0.4157221
Validation loss decreased (0.937740 --> 0.934747).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4469457
	speed: 0.9506s/iter; left time: 22020.4433s
	iters: 200, epoch: 6 | loss: 0.4207673
	speed: 0.1373s/iter; left time: 3167.6676s
	iters: 300, epoch: 6 | loss: 0.4094218
	speed: 0.1385s/iter; left time: 3180.1282s
	iters: 400, epoch: 6 | loss: 0.4406368
	speed: 0.1352s/iter; left time: 3091.6712s
	iters: 500, epoch: 6 | loss: 0.3999763
	speed: 0.1324s/iter; left time: 3013.7706s
Epoch: 6 cost time: 71.81473469734192
Epoch: 6, Steps: 517 | Train Loss: 0.3973260 Vali Loss: 0.9340853 Test Loss: 0.4155606
Validation loss decreased (0.934747 --> 0.934085).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4223998
	speed: 0.8828s/iter; left time: 19993.8212s
	iters: 200, epoch: 7 | loss: 0.4031811
	speed: 0.1366s/iter; left time: 3079.2989s
	iters: 300, epoch: 7 | loss: 0.3774787
	speed: 0.1368s/iter; left time: 3070.6326s
	iters: 400, epoch: 7 | loss: 0.3718120
	speed: 0.1322s/iter; left time: 2954.8489s
	iters: 500, epoch: 7 | loss: 0.4133227
	speed: 0.1311s/iter; left time: 2917.2483s
Epoch: 7 cost time: 69.90349960327148
Epoch: 7, Steps: 517 | Train Loss: 0.3972107 Vali Loss: 0.9325200 Test Loss: 0.4158239
Validation loss decreased (0.934085 --> 0.932520).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3908783
	speed: 0.8524s/iter; left time: 18865.2638s
	iters: 200, epoch: 8 | loss: 0.4277461
	speed: 0.1381s/iter; left time: 3043.2321s
	iters: 300, epoch: 8 | loss: 0.3915658
	speed: 0.1360s/iter; left time: 2981.7803s
	iters: 400, epoch: 8 | loss: 0.4247186
	speed: 0.1368s/iter; left time: 2987.0654s
	iters: 500, epoch: 8 | loss: 0.3912002
	speed: 0.1373s/iter; left time: 2983.9080s
Epoch: 8 cost time: 71.73720693588257
Epoch: 8, Steps: 517 | Train Loss: 0.3971099 Vali Loss: 0.9336134 Test Loss: 0.4156572
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3680861
	speed: 0.9554s/iter; left time: 20651.2593s
	iters: 200, epoch: 9 | loss: 0.4003055
	speed: 0.1402s/iter; left time: 3017.4173s
	iters: 300, epoch: 9 | loss: 0.3880333
	speed: 0.1076s/iter; left time: 2304.2755s
	iters: 400, epoch: 9 | loss: 0.4752170
	speed: 0.1255s/iter; left time: 2675.1437s
	iters: 500, epoch: 9 | loss: 0.3864962
	speed: 0.1313s/iter; left time: 2785.7073s
Epoch: 9 cost time: 67.87673211097717
Epoch: 9, Steps: 517 | Train Loss: 0.3970022 Vali Loss: 0.9327003 Test Loss: 0.4155261
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4220575
	speed: 0.8808s/iter; left time: 18582.5978s
	iters: 200, epoch: 10 | loss: 0.4198205
	speed: 0.1362s/iter; left time: 2859.7685s
	iters: 300, epoch: 10 | loss: 0.4188527
	speed: 0.1299s/iter; left time: 2715.6686s
	iters: 400, epoch: 10 | loss: 0.4140797
	speed: 0.1335s/iter; left time: 2775.8778s
	iters: 500, epoch: 10 | loss: 0.4296773
	speed: 0.1316s/iter; left time: 2724.5032s
Epoch: 10 cost time: 69.5111894607544
Epoch: 10, Steps: 517 | Train Loss: 0.3969077 Vali Loss: 0.9312860 Test Loss: 0.4163905
Validation loss decreased (0.932520 --> 0.931286).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3887250
	speed: 0.8136s/iter; left time: 16743.8325s
	iters: 200, epoch: 11 | loss: 0.4266175
	speed: 0.1185s/iter; left time: 2426.1142s
	iters: 300, epoch: 11 | loss: 0.4178330
	speed: 0.1187s/iter; left time: 2418.8470s
	iters: 400, epoch: 11 | loss: 0.4097154
	speed: 0.1172s/iter; left time: 2376.5500s
	iters: 500, epoch: 11 | loss: 0.4101777
	speed: 0.1208s/iter; left time: 2437.9626s
Epoch: 11 cost time: 62.54292702674866
Epoch: 11, Steps: 517 | Train Loss: 0.3968252 Vali Loss: 0.9307176 Test Loss: 0.4154906
Validation loss decreased (0.931286 --> 0.930718).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4265834
	speed: 0.8385s/iter; left time: 16823.0262s
	iters: 200, epoch: 12 | loss: 0.3523453
	speed: 0.1275s/iter; left time: 2545.5217s
	iters: 300, epoch: 12 | loss: 0.4054928
	speed: 0.1308s/iter; left time: 2599.1453s
	iters: 400, epoch: 12 | loss: 0.3511832
	speed: 0.1301s/iter; left time: 2571.0279s
	iters: 500, epoch: 12 | loss: 0.3890487
	speed: 0.1369s/iter; left time: 2691.3566s
Epoch: 12 cost time: 69.11842465400696
Epoch: 12, Steps: 517 | Train Loss: 0.3968092 Vali Loss: 0.9312078 Test Loss: 0.4160838
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3727604
	speed: 1.0430s/iter; left time: 20387.5243s
	iters: 200, epoch: 13 | loss: 0.4132529
	speed: 0.1726s/iter; left time: 3356.1502s
	iters: 300, epoch: 13 | loss: 0.3656322
	speed: 0.1570s/iter; left time: 3038.1700s
	iters: 400, epoch: 13 | loss: 0.4158247
	speed: 0.1558s/iter; left time: 2997.9877s
	iters: 500, epoch: 13 | loss: 0.4574454
	speed: 0.1473s/iter; left time: 2819.4945s
Epoch: 13 cost time: 82.82810378074646
Epoch: 13, Steps: 517 | Train Loss: 0.3968315 Vali Loss: 0.9294851 Test Loss: 0.4156370
Validation loss decreased (0.930718 --> 0.929485).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3553826
	speed: 0.9017s/iter; left time: 17159.3592s
	iters: 200, epoch: 14 | loss: 0.4355993
	speed: 0.1293s/iter; left time: 2447.7718s
	iters: 300, epoch: 14 | loss: 0.3803926
	speed: 0.1282s/iter; left time: 2414.0915s
	iters: 400, epoch: 14 | loss: 0.3616651
	speed: 0.1343s/iter; left time: 2516.3368s
	iters: 500, epoch: 14 | loss: 0.4298305
	speed: 0.1261s/iter; left time: 2348.5451s
Epoch: 14 cost time: 67.94806122779846
Epoch: 14, Steps: 517 | Train Loss: 0.3967144 Vali Loss: 0.9308257 Test Loss: 0.4160578
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3853708
	speed: 0.8621s/iter; left time: 15960.8443s
	iters: 200, epoch: 15 | loss: 0.3757898
	speed: 0.1361s/iter; left time: 2506.1359s
	iters: 300, epoch: 15 | loss: 0.4288780
	speed: 0.1261s/iter; left time: 2309.5885s
	iters: 400, epoch: 15 | loss: 0.3944523
	speed: 0.0995s/iter; left time: 1811.2955s
	iters: 500, epoch: 15 | loss: 0.4118228
	speed: 0.1179s/iter; left time: 2135.6764s
Epoch: 15 cost time: 63.91965317726135
Epoch: 15, Steps: 517 | Train Loss: 0.3966684 Vali Loss: 0.9295278 Test Loss: 0.4156682
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3778092
	speed: 0.8135s/iter; left time: 14639.5405s
	iters: 200, epoch: 16 | loss: 0.4445509
	speed: 0.1203s/iter; left time: 2152.3539s
	iters: 300, epoch: 16 | loss: 0.3716916
	speed: 0.1178s/iter; left time: 2097.2108s
	iters: 400, epoch: 16 | loss: 0.3587003
	speed: 0.1324s/iter; left time: 2343.6642s
	iters: 500, epoch: 16 | loss: 0.4475129
	speed: 0.1349s/iter; left time: 2372.9148s
Epoch: 16 cost time: 66.33146667480469
Epoch: 16, Steps: 517 | Train Loss: 0.3966758 Vali Loss: 0.9305354 Test Loss: 0.4155733
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4190786
	speed: 0.9743s/iter; left time: 17029.8709s
	iters: 200, epoch: 17 | loss: 0.3978420
	speed: 0.1606s/iter; left time: 2791.6155s
	iters: 300, epoch: 17 | loss: 0.3406345
	speed: 0.1495s/iter; left time: 2583.9912s
	iters: 400, epoch: 17 | loss: 0.4237067
	speed: 0.1426s/iter; left time: 2449.8780s
	iters: 500, epoch: 17 | loss: 0.4106780
	speed: 0.1456s/iter; left time: 2487.4081s
Epoch: 17 cost time: 79.24717664718628
Epoch: 17, Steps: 517 | Train Loss: 0.3966382 Vali Loss: 0.9306316 Test Loss: 0.4151656
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3844220
	speed: 0.9434s/iter; left time: 16002.3082s
	iters: 200, epoch: 18 | loss: 0.4145262
	speed: 0.1352s/iter; left time: 2279.2285s
	iters: 300, epoch: 18 | loss: 0.3907147
	speed: 0.1334s/iter; left time: 2236.3931s
	iters: 400, epoch: 18 | loss: 0.3941192
	speed: 0.1348s/iter; left time: 2246.6989s
	iters: 500, epoch: 18 | loss: 0.3950319
	speed: 0.1283s/iter; left time: 2125.1963s
Epoch: 18 cost time: 69.68386435508728
Epoch: 18, Steps: 517 | Train Loss: 0.3966341 Vali Loss: 0.9303768 Test Loss: 0.4158909
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4097304
	speed: 0.8260s/iter; left time: 13584.2604s
	iters: 200, epoch: 19 | loss: 0.3603241
	speed: 0.1307s/iter; left time: 2136.6453s
	iters: 300, epoch: 19 | loss: 0.3747359
	speed: 0.1364s/iter; left time: 2216.3417s
	iters: 400, epoch: 19 | loss: 0.4591236
	speed: 0.1352s/iter; left time: 2182.6899s
	iters: 500, epoch: 19 | loss: 0.3776678
	speed: 0.1387s/iter; left time: 2225.1312s
Epoch: 19 cost time: 69.98854947090149
Epoch: 19, Steps: 517 | Train Loss: 0.3965704 Vali Loss: 0.9310517 Test Loss: 0.4157698
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4340891
	speed: 0.9411s/iter; left time: 14989.2939s
	iters: 200, epoch: 20 | loss: 0.3600932
	speed: 0.1406s/iter; left time: 2225.8078s
	iters: 300, epoch: 20 | loss: 0.3646042
	speed: 0.1432s/iter; left time: 2252.9225s
	iters: 400, epoch: 20 | loss: 0.3659372
	speed: 0.1425s/iter; left time: 2226.7673s
	iters: 500, epoch: 20 | loss: 0.4382904
	speed: 0.1350s/iter; left time: 2096.1980s
Epoch: 20 cost time: 73.94986462593079
Epoch: 20, Steps: 517 | Train Loss: 0.3965726 Vali Loss: 0.9305035 Test Loss: 0.4157759
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3851571
	speed: 0.9128s/iter; left time: 14067.7276s
	iters: 200, epoch: 21 | loss: 0.4338522
	speed: 0.1461s/iter; left time: 2236.3661s
	iters: 300, epoch: 21 | loss: 0.3505000
	speed: 0.1526s/iter; left time: 2320.5621s
	iters: 400, epoch: 21 | loss: 0.4275887
	speed: 0.1552s/iter; left time: 2345.0916s
	iters: 500, epoch: 21 | loss: 0.3341765
	speed: 0.1499s/iter; left time: 2250.6758s
Epoch: 21 cost time: 78.25841164588928
Epoch: 21, Steps: 517 | Train Loss: 0.3964691 Vali Loss: 0.9306708 Test Loss: 0.4154077
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3981365
	speed: 1.0171s/iter; left time: 15148.2040s
	iters: 200, epoch: 22 | loss: 0.3958334
	speed: 0.1414s/iter; left time: 2091.5306s
	iters: 300, epoch: 22 | loss: 0.3605455
	speed: 0.1519s/iter; left time: 2231.7738s
	iters: 400, epoch: 22 | loss: 0.4073317
	speed: 0.1489s/iter; left time: 2173.3338s
	iters: 500, epoch: 22 | loss: 0.3517092
	speed: 0.1496s/iter; left time: 2168.7035s
Epoch: 22 cost time: 77.4173629283905
Epoch: 22, Steps: 517 | Train Loss: 0.3964468 Vali Loss: 0.9313438 Test Loss: 0.4153130
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4509952
	speed: 0.9037s/iter; left time: 12992.1618s
	iters: 200, epoch: 23 | loss: 0.3886468
	speed: 0.1329s/iter; left time: 1896.7580s
	iters: 300, epoch: 23 | loss: 0.4056609
	speed: 0.1302s/iter; left time: 1845.6703s
	iters: 400, epoch: 23 | loss: 0.3749058
	speed: 0.1315s/iter; left time: 1851.1263s
	iters: 500, epoch: 23 | loss: 0.3870540
	speed: 0.1284s/iter; left time: 1795.1847s
Epoch: 23 cost time: 69.05630683898926
Epoch: 23, Steps: 517 | Train Loss: 0.3965162 Vali Loss: 0.9303854 Test Loss: 0.4161134
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3657363
	speed: 0.8795s/iter; left time: 12189.8094s
	iters: 200, epoch: 24 | loss: 0.4042519
	speed: 0.1288s/iter; left time: 1772.8868s
	iters: 300, epoch: 24 | loss: 0.4140237
	speed: 0.1287s/iter; left time: 1757.8754s
	iters: 400, epoch: 24 | loss: 0.4150006
	speed: 0.1332s/iter; left time: 1806.7705s
	iters: 500, epoch: 24 | loss: 0.3973235
	speed: 0.1409s/iter; left time: 1895.9847s
Epoch: 24 cost time: 69.78301072120667
Epoch: 24, Steps: 517 | Train Loss: 0.3964022 Vali Loss: 0.9305760 Test Loss: 0.4155414
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3935840
	speed: 0.9242s/iter; left time: 12330.9952s
	iters: 200, epoch: 25 | loss: 0.3754739
	speed: 0.1610s/iter; left time: 2132.0691s
	iters: 300, epoch: 25 | loss: 0.4083961
	speed: 0.1513s/iter; left time: 1988.7545s
	iters: 400, epoch: 25 | loss: 0.3733834
	speed: 0.1366s/iter; left time: 1781.0642s
	iters: 500, epoch: 25 | loss: 0.3766675
	speed: 0.1451s/iter; left time: 1878.5383s
Epoch: 25 cost time: 78.20383524894714
Epoch: 25, Steps: 517 | Train Loss: 0.3964286 Vali Loss: 0.9309794 Test Loss: 0.4155253
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4075766
	speed: 0.9129s/iter; left time: 11708.4776s
	iters: 200, epoch: 26 | loss: 0.3950591
	speed: 0.1351s/iter; left time: 1719.6187s
	iters: 300, epoch: 26 | loss: 0.4012030
	speed: 0.1215s/iter; left time: 1533.5889s
	iters: 400, epoch: 26 | loss: 0.4373360
	speed: 0.1246s/iter; left time: 1561.1102s
	iters: 500, epoch: 26 | loss: 0.3937790
	speed: 0.1621s/iter; left time: 2014.3142s
Epoch: 26 cost time: 71.93740177154541
Epoch: 26, Steps: 517 | Train Loss: 0.3963869 Vali Loss: 0.9307589 Test Loss: 0.4159074
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3919526
	speed: 1.0240s/iter; left time: 12604.4264s
	iters: 200, epoch: 27 | loss: 0.4034074
	speed: 0.1352s/iter; left time: 1650.8042s
	iters: 300, epoch: 27 | loss: 0.3558266
	speed: 0.1279s/iter; left time: 1549.0073s
	iters: 400, epoch: 27 | loss: 0.4184051
	speed: 0.1285s/iter; left time: 1542.5759s
	iters: 500, epoch: 27 | loss: 0.4196151
	speed: 0.1299s/iter; left time: 1546.4471s
Epoch: 27 cost time: 69.23453950881958
Epoch: 27, Steps: 517 | Train Loss: 0.3963409 Vali Loss: 0.9306686 Test Loss: 0.4157081
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4011735
	speed: 0.8184s/iter; left time: 9650.0811s
	iters: 200, epoch: 28 | loss: 0.4010836
	speed: 0.1176s/iter; left time: 1375.2990s
	iters: 300, epoch: 28 | loss: 0.4012402
	speed: 0.1270s/iter; left time: 1471.6931s
	iters: 400, epoch: 28 | loss: 0.4097835
	speed: 0.1190s/iter; left time: 1368.0766s
	iters: 500, epoch: 28 | loss: 0.4389369
	speed: 0.1259s/iter; left time: 1434.4440s
Epoch: 28 cost time: 63.51748323440552
Epoch: 28, Steps: 517 | Train Loss: 0.3963566 Vali Loss: 0.9302694 Test Loss: 0.4157895
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3750225
	speed: 0.9136s/iter; left time: 10301.1334s
	iters: 200, epoch: 29 | loss: 0.3681414
	speed: 0.1474s/iter; left time: 1647.4565s
	iters: 300, epoch: 29 | loss: 0.4166268
	speed: 0.1449s/iter; left time: 1605.3024s
	iters: 400, epoch: 29 | loss: 0.3679209
	speed: 0.1333s/iter; left time: 1462.5784s
	iters: 500, epoch: 29 | loss: 0.3983234
	speed: 0.1466s/iter; left time: 1593.9392s
Epoch: 29 cost time: 74.44001603126526
Epoch: 29, Steps: 517 | Train Loss: 0.3963381 Vali Loss: 0.9306365 Test Loss: 0.4156975
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4210993
	speed: 0.9185s/iter; left time: 9881.2945s
	iters: 200, epoch: 30 | loss: 0.3953990
	speed: 0.1268s/iter; left time: 1351.3585s
	iters: 300, epoch: 30 | loss: 0.4040356
	speed: 0.1271s/iter; left time: 1342.1777s
	iters: 400, epoch: 30 | loss: 0.3597284
	speed: 0.1300s/iter; left time: 1359.8654s
	iters: 500, epoch: 30 | loss: 0.3977582
	speed: 0.1315s/iter; left time: 1362.5773s
Epoch: 30 cost time: 68.05502152442932
Epoch: 30, Steps: 517 | Train Loss: 0.3964031 Vali Loss: 0.9304613 Test Loss: 0.4156177
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3656713
	speed: 0.8790s/iter; left time: 9002.3388s
	iters: 200, epoch: 31 | loss: 0.4088473
	speed: 0.1473s/iter; left time: 1493.9640s
	iters: 300, epoch: 31 | loss: 0.4254733
	speed: 0.1544s/iter; left time: 1550.1492s
	iters: 400, epoch: 31 | loss: 0.3702127
	speed: 0.1513s/iter; left time: 1504.5006s
	iters: 500, epoch: 31 | loss: 0.3708112
	speed: 0.1548s/iter; left time: 1523.8168s
Epoch: 31 cost time: 78.17181324958801
Epoch: 31, Steps: 517 | Train Loss: 0.3963138 Vali Loss: 0.9308617 Test Loss: 0.4156789
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3995818
	speed: 0.9693s/iter; left time: 9425.1888s
	iters: 200, epoch: 32 | loss: 0.4294197
	speed: 0.1446s/iter; left time: 1391.7727s
	iters: 300, epoch: 32 | loss: 0.3365508
	speed: 0.1432s/iter; left time: 1363.5283s
	iters: 400, epoch: 32 | loss: 0.4015177
	speed: 0.1482s/iter; left time: 1396.8397s
	iters: 500, epoch: 32 | loss: 0.3890454
	speed: 0.1488s/iter; left time: 1387.6565s
Epoch: 32 cost time: 76.81863498687744
Epoch: 32, Steps: 517 | Train Loss: 0.3963196 Vali Loss: 0.9297333 Test Loss: 0.4155326
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3549136
	speed: 0.9892s/iter; left time: 9107.6555s
	iters: 200, epoch: 33 | loss: 0.3883370
	speed: 0.1590s/iter; left time: 1447.8220s
	iters: 300, epoch: 33 | loss: 0.4009959
	speed: 0.1536s/iter; left time: 1383.4929s
	iters: 400, epoch: 33 | loss: 0.4113871
	speed: 0.1591s/iter; left time: 1417.5305s
	iters: 500, epoch: 33 | loss: 0.3659853
	speed: 0.1593s/iter; left time: 1403.2394s
Epoch: 33 cost time: 82.23032689094543
Epoch: 33, Steps: 517 | Train Loss: 0.3962289 Vali Loss: 0.9309961 Test Loss: 0.4155240
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4154558777809143, mae:0.41178086400032043, rse:0.613243818283081, corr:[0.52602595 0.5324783  0.53419125 0.53414685 0.5348066  0.53670466
 0.5387261  0.53977907 0.5400332  0.5400938  0.5404147  0.540911
 0.5413696  0.5415127  0.5412022  0.540368   0.53913385 0.5378388
 0.5366476  0.535552   0.5343866  0.5330039  0.5315163  0.53033817
 0.5293569  0.52857685 0.52775186 0.5267048  0.52586854 0.52562356
 0.5261331  0.527265   0.52836746 0.5290214  0.52894104 0.5286913
 0.52843654 0.52827996 0.5280383  0.52750117 0.5269114  0.5264663
 0.5263566  0.52653295 0.5265768  0.5263169  0.52602017 0.52589333
 0.5259261  0.52594644 0.52595997 0.5259402  0.5260407  0.5261883
 0.52637833 0.5264628  0.526343   0.5260113  0.52571785 0.5254671
 0.5252827  0.52511424 0.5249533  0.5247812  0.52486867 0.52529347
 0.52582914 0.52622473 0.5264508  0.5265523  0.52671945 0.5269506
 0.52711016 0.527023   0.52667576 0.52607274 0.5253528  0.52476376
 0.5244548  0.52445    0.52461904 0.5247606  0.52475864 0.524555
 0.52430856 0.5240715  0.5239387  0.5238928  0.5239524  0.5241994
 0.5245865  0.52503234 0.52545196 0.5257396  0.5258544  0.5257704
 0.525633   0.5256145  0.5255771  0.525521   0.5253604  0.5250766
 0.5247379  0.52440584 0.52423435 0.5242864  0.5243469  0.52434367
 0.52416235 0.5238872  0.5236074  0.5233297  0.5230369  0.5227141
 0.5223405  0.52201265 0.5218268  0.5217736  0.5217993  0.52185994
 0.52182716 0.52162385 0.52122104 0.5207843  0.5203928  0.52000207
 0.5196402  0.5194288  0.51935357 0.519375   0.51931614 0.5191183
 0.5189185  0.5188315  0.518837   0.5188849  0.5188919  0.51864403
 0.51829183 0.5181321  0.5182819  0.51869804 0.51909536 0.5191714
 0.5188406  0.51823664 0.5177217  0.5175094  0.51762146 0.5178311
 0.5180011  0.51820487 0.51844686 0.5186481  0.51867527 0.51856965
 0.518368   0.5181562  0.5181026  0.5181683  0.51838297 0.5186707
 0.5189881  0.5192664  0.519436   0.51939917 0.51916754 0.51890844
 0.51882327 0.5190304  0.5195335  0.5200751  0.5202955  0.52011555
 0.5197274  0.5194401  0.5194635  0.5197464  0.52001256 0.52006775
 0.51986784 0.5196094  0.5194962  0.51969075 0.52009857 0.52048546
 0.5206555  0.52065194 0.52062327 0.52067876 0.5208246  0.5209624
 0.5210203  0.52099216 0.5207829  0.5203725  0.5198051  0.51917386
 0.51848996 0.51775724 0.51695824 0.5161884  0.5155109  0.51499146
 0.5146138  0.5142581  0.513781   0.5131494  0.51239586 0.5116332
 0.510939   0.5104129  0.5099967  0.50958306 0.50903904 0.5084591
 0.50793755 0.50746244 0.5070835  0.50676197 0.5065302  0.5064811
 0.50668293 0.50691944 0.50707763 0.5070543  0.50680393 0.5064954
 0.5063684  0.50657284 0.5070176  0.5073806  0.5074664  0.50730294
 0.5070709  0.5070345  0.5072044  0.50762635 0.50802684 0.50825745
 0.50821644 0.5079115  0.507555   0.50730765 0.50721323 0.50714564
 0.50703144 0.5069269  0.5068893  0.50697994 0.5070952  0.5072216
 0.50721407 0.50715977 0.5070799  0.50707304 0.5072172  0.50747854
 0.5077149  0.50778526 0.50761634 0.5073303  0.5071783  0.5074305
 0.5079554  0.5085035  0.5088028  0.508775   0.50848836 0.5081284
 0.5079568  0.50803524 0.50829303 0.5085363  0.5085704  0.5083758
 0.50803804 0.5078231  0.5077698  0.50788784 0.5079739  0.50788033
 0.50768834 0.50749135 0.5074189  0.5075249  0.5076584  0.5075964
 0.5072299  0.5066476  0.50598717 0.5053796  0.50492847 0.5045982
 0.504268   0.5037727  0.5031505  0.50245905 0.50182676 0.5013632
 0.50102514 0.5007619  0.5005086  0.50016785 0.49972415 0.4992291
 0.49889883 0.49872494 0.49869457 0.49873623 0.49880877 0.49878642
 0.49863523 0.498445   0.49824464 0.49813545 0.49817064 0.49834812
 0.4985325  0.49857318 0.49846062 0.49828205 0.49814478 0.49808612
 0.49807507 0.49809176 0.49805003 0.4980067  0.4980161  0.49809968
 0.49827647 0.4983994  0.4983333  0.49812075 0.49787074 0.4978054
 0.4978894  0.49790305 0.49786648 0.49777728 0.49771717 0.49771398
 0.49770537 0.49769527 0.4976021  0.49745247 0.49732783 0.49733478
 0.49742094 0.497564   0.49764395 0.4975304  0.4972693  0.49692377
 0.4965783  0.49636692 0.49632365 0.49638864 0.49649757 0.49656954
 0.4964878  0.49633306 0.4961619  0.4960419  0.49599388 0.4959635
 0.4958478  0.4957296  0.49566528 0.49566582 0.495796   0.49602136
 0.496252   0.49641925 0.4964962  0.4965752  0.49672812 0.49704352
 0.49749392 0.49796236 0.49832088 0.49846655 0.4983819  0.49814913
 0.49784186 0.49751863 0.49720117 0.4968222  0.49634165 0.49591255
 0.49555445 0.49526763 0.49499637 0.49472645 0.4943798  0.49401954
 0.493704   0.49345195 0.49319607 0.49282777 0.4922642  0.49167663
 0.49117103 0.4909074  0.49078646 0.49073038 0.4906606  0.4906408
 0.4906712  0.4907597  0.490842   0.4908851  0.4909115  0.4909226
 0.49106687 0.4911396  0.4911155  0.49089745 0.49063602 0.4905369
 0.4905886  0.4908027  0.4909956  0.49108854 0.4910166  0.4909096
 0.49080053 0.49076137 0.4907502  0.49087393 0.4910318  0.49123657
 0.49128446 0.4910549  0.49067438 0.49036047 0.49024162 0.49031788
 0.49046427 0.4905878  0.49067417 0.49071184 0.49069458 0.4906397
 0.4905602  0.49053323 0.49053797 0.49051097 0.4903831  0.49010217
 0.48980105 0.48961285 0.48966417 0.48992708 0.49024847 0.4904806
 0.4903941  0.4900764  0.48971975 0.4895592  0.48957923 0.48975667
 0.4899525  0.49016985 0.49034584 0.49049982 0.49057198 0.49047384
 0.4902408  0.4899351  0.4897584  0.48981994 0.49008265 0.49038133
 0.4905096  0.4904486  0.4902491  0.48997292 0.48969996 0.48947966
 0.48926094 0.4889392  0.48841888 0.48767325 0.4868223  0.4860861
 0.48561487 0.48524913 0.48485646 0.4843366  0.48360246 0.4827591
 0.48198095 0.48143312 0.4810864  0.48080555 0.4805033  0.48018137
 0.4798249  0.47953787 0.47932497 0.47904056 0.47881693 0.47859865
 0.478404   0.47819385 0.4779685  0.47789222 0.47804195 0.4784479
 0.47896183 0.4793064  0.47937387 0.47932252 0.47925994 0.47931513
 0.4794785  0.47973993 0.4799856  0.48005724 0.4799874  0.47993383
 0.48000163 0.48021778 0.48051867 0.48076037 0.4809409  0.48107317
 0.4811268  0.48112792 0.4811402  0.4812836  0.4814975  0.4815954
 0.48148626 0.4812205  0.48097447 0.48087576 0.48090842 0.48094347
 0.48086986 0.48065615 0.48040193 0.48025963 0.4803023  0.48046702
 0.48050424 0.48036498 0.48020804 0.4801568  0.48026255 0.48050448
 0.48059976 0.48040983 0.48001713 0.4796761  0.47954643 0.4796756
 0.4798887  0.48001522 0.4799484  0.4797304  0.47950277 0.4794158
 0.4795449  0.47995874 0.48040104 0.4807264  0.4808452  0.48081645
 0.4807717  0.48082113 0.48103127 0.48124945 0.48127177 0.4809512
 0.48031345 0.4795163  0.47868326 0.47789454 0.47720695 0.47672182
 0.47637066 0.47608063 0.47574154 0.47530177 0.47479284 0.47422394
 0.4736895  0.4731732  0.47264713 0.47209445 0.47160015 0.4711936
 0.47089785 0.47068357 0.47053182 0.47038218 0.47021478 0.4700339
 0.469873   0.4696454  0.46937045 0.46912628 0.46909627 0.4692919
 0.46968278 0.46998867 0.47008124 0.47001898 0.46999392 0.4702506
 0.4707273  0.47108325 0.4711677  0.47096124 0.47069204 0.4706033
 0.4708371  0.4712894  0.47163653 0.47181773 0.4720425  0.4723691
 0.4726322  0.47274432 0.47263923 0.47239947 0.47232524 0.47253466
 0.47280762 0.47301182 0.47303316 0.47284663 0.47271496 0.47274426
 0.47285947 0.47291073 0.47270265 0.472036   0.47120807 0.4705786
 0.47041202 0.47063604 0.47097942 0.4711566  0.4710183  0.47072908
 0.47046256 0.47030953 0.4703963  0.47064158 0.47085604 0.47092536
 0.470903   0.47091025 0.47100347 0.4710929  0.4710338  0.4707696
 0.47040534 0.47021082 0.47029114 0.47055167 0.47092348 0.47120327
 0.4713183  0.47132853 0.47137505 0.47152764 0.47165495 0.47164664
 0.47148594 0.47124836 0.4708807  0.47033343 0.46970588 0.46915892
 0.4688003  0.46853694 0.4683141  0.46796256 0.467351   0.46680143
 0.4663152  0.46608266 0.46595377 0.46585247 0.4656344  0.46529198
 0.46495888 0.46465853 0.46440968 0.46404204 0.4637091  0.46360955
 0.46373996 0.46404174 0.46417323 0.46394613 0.4635614  0.4631991
 0.46323404 0.46361467 0.46410957 0.4645366  0.46458307 0.46440703
 0.4642557  0.4643313  0.46465537 0.46510807 0.46569774 0.4662787
 0.4669276  0.46751118 0.46775287 0.46790585 0.4675274  0.46402675]
