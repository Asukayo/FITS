Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4477143
	speed: 0.1105s/iter; left time: 1414.7679s
	iters: 200, epoch: 1 | loss: 0.4287235
	speed: 0.1074s/iter; left time: 1363.6077s
Epoch: 1 cost time: 27.226524829864502
Epoch: 1, Steps: 258 | Train Loss: 0.5067373 Vali Loss: 1.0141231 Test Loss: 0.4416979
Validation loss decreased (inf --> 1.014123).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3928874
	speed: 0.4639s/iter; left time: 5819.2235s
	iters: 200, epoch: 2 | loss: 0.3963578
	speed: 0.1135s/iter; left time: 1411.8778s
Epoch: 2 cost time: 29.285506010055542
Epoch: 2, Steps: 258 | Train Loss: 0.4131293 Vali Loss: 0.9652311 Test Loss: 0.4177590
Validation loss decreased (1.014123 --> 0.965231).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3841108
	speed: 0.4710s/iter; left time: 5786.5074s
	iters: 200, epoch: 3 | loss: 0.3888155
	speed: 0.1021s/iter; left time: 1243.8601s
Epoch: 3 cost time: 27.979230165481567
Epoch: 3, Steps: 258 | Train Loss: 0.4026298 Vali Loss: 0.9498407 Test Loss: 0.4145276
Validation loss decreased (0.965231 --> 0.949841).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4062220
	speed: 0.4533s/iter; left time: 5452.3909s
	iters: 200, epoch: 4 | loss: 0.4580542
	speed: 0.1013s/iter; left time: 1208.0061s
Epoch: 4 cost time: 27.367557764053345
Epoch: 4, Steps: 258 | Train Loss: 0.3994790 Vali Loss: 0.9434583 Test Loss: 0.4150304
Validation loss decreased (0.949841 --> 0.943458).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4420487
	speed: 0.4125s/iter; left time: 4855.2167s
	iters: 200, epoch: 5 | loss: 0.4116120
	speed: 0.0923s/iter; left time: 1077.0279s
Epoch: 5 cost time: 24.937462329864502
Epoch: 5, Steps: 258 | Train Loss: 0.3983752 Vali Loss: 0.9391553 Test Loss: 0.4151621
Validation loss decreased (0.943458 --> 0.939155).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3986238
	speed: 0.4339s/iter; left time: 4995.1482s
	iters: 200, epoch: 6 | loss: 0.4199855
	speed: 0.0987s/iter; left time: 1126.0391s
Epoch: 6 cost time: 26.70417618751526
Epoch: 6, Steps: 258 | Train Loss: 0.3978785 Vali Loss: 0.9385544 Test Loss: 0.4153945
Validation loss decreased (0.939155 --> 0.938554).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4066201
	speed: 0.4580s/iter; left time: 5153.8878s
	iters: 200, epoch: 7 | loss: 0.4103454
	speed: 0.0975s/iter; left time: 1087.4440s
Epoch: 7 cost time: 26.213812828063965
Epoch: 7, Steps: 258 | Train Loss: 0.3974246 Vali Loss: 0.9359883 Test Loss: 0.4161712
Validation loss decreased (0.938554 --> 0.935988).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4382278
	speed: 0.4413s/iter; left time: 4852.2813s
	iters: 200, epoch: 8 | loss: 0.4317838
	speed: 0.0940s/iter; left time: 1024.4642s
Epoch: 8 cost time: 26.098341941833496
Epoch: 8, Steps: 258 | Train Loss: 0.3973270 Vali Loss: 0.9353138 Test Loss: 0.4158286
Validation loss decreased (0.935988 --> 0.935314).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3957881
	speed: 0.4601s/iter; left time: 4939.9115s
	iters: 200, epoch: 9 | loss: 0.4034412
	speed: 0.1059s/iter; left time: 1126.7233s
Epoch: 9 cost time: 28.030545949935913
Epoch: 9, Steps: 258 | Train Loss: 0.3971203 Vali Loss: 0.9334255 Test Loss: 0.4160736
Validation loss decreased (0.935314 --> 0.933426).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4185200
	speed: 0.4277s/iter; left time: 4482.3901s
	iters: 200, epoch: 10 | loss: 0.3754901
	speed: 0.0986s/iter; left time: 1023.4654s
Epoch: 10 cost time: 25.23591446876526
Epoch: 10, Steps: 258 | Train Loss: 0.3968780 Vali Loss: 0.9343039 Test Loss: 0.4162051
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3780868
	speed: 0.3667s/iter; left time: 3748.2300s
	iters: 200, epoch: 11 | loss: 0.3939108
	speed: 0.0880s/iter; left time: 890.1740s
Epoch: 11 cost time: 24.100112676620483
Epoch: 11, Steps: 258 | Train Loss: 0.3967993 Vali Loss: 0.9329076 Test Loss: 0.4160784
Validation loss decreased (0.933426 --> 0.932908).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4012182
	speed: 0.4184s/iter; left time: 4168.1226s
	iters: 200, epoch: 12 | loss: 0.4129649
	speed: 0.0937s/iter; left time: 924.2117s
Epoch: 12 cost time: 25.067310333251953
Epoch: 12, Steps: 258 | Train Loss: 0.3968941 Vali Loss: 0.9324608 Test Loss: 0.4163824
Validation loss decreased (0.932908 --> 0.932461).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3642791
	speed: 0.4038s/iter; left time: 3918.6697s
	iters: 200, epoch: 13 | loss: 0.4125378
	speed: 0.0913s/iter; left time: 876.5235s
Epoch: 13 cost time: 24.44627070426941
Epoch: 13, Steps: 258 | Train Loss: 0.3968457 Vali Loss: 0.9324865 Test Loss: 0.4162943
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3912449
	speed: 0.4059s/iter; left time: 3834.2245s
	iters: 200, epoch: 14 | loss: 0.3794019
	speed: 0.0874s/iter; left time: 817.3525s
Epoch: 14 cost time: 24.580681562423706
Epoch: 14, Steps: 258 | Train Loss: 0.3967264 Vali Loss: 0.9322935 Test Loss: 0.4161349
Validation loss decreased (0.932461 --> 0.932293).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3797838
	speed: 0.4242s/iter; left time: 3897.8176s
	iters: 200, epoch: 15 | loss: 0.4117922
	speed: 0.0928s/iter; left time: 843.6632s
Epoch: 15 cost time: 24.980634689331055
Epoch: 15, Steps: 258 | Train Loss: 0.3967743 Vali Loss: 0.9322794 Test Loss: 0.4164195
Validation loss decreased (0.932293 --> 0.932279).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3737689
	speed: 0.4101s/iter; left time: 3662.7767s
	iters: 200, epoch: 16 | loss: 0.3741005
	speed: 0.1058s/iter; left time: 934.7031s
Epoch: 16 cost time: 26.759979009628296
Epoch: 16, Steps: 258 | Train Loss: 0.3966416 Vali Loss: 0.9312431 Test Loss: 0.4157592
Validation loss decreased (0.932279 --> 0.931243).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3928449
	speed: 0.4606s/iter; left time: 3994.4950s
	iters: 200, epoch: 17 | loss: 0.3642943
	speed: 0.1064s/iter; left time: 912.4047s
Epoch: 17 cost time: 27.46009635925293
Epoch: 17, Steps: 258 | Train Loss: 0.3966053 Vali Loss: 0.9324467 Test Loss: 0.4159833
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4133730
	speed: 0.4160s/iter; left time: 3500.6604s
	iters: 200, epoch: 18 | loss: 0.3939097
	speed: 0.0841s/iter; left time: 699.4152s
Epoch: 18 cost time: 22.305710554122925
Epoch: 18, Steps: 258 | Train Loss: 0.3965566 Vali Loss: 0.9307193 Test Loss: 0.4160227
Validation loss decreased (0.931243 --> 0.930719).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4241287
	speed: 0.3361s/iter; left time: 2741.6849s
	iters: 200, epoch: 19 | loss: 0.3936009
	speed: 0.0832s/iter; left time: 670.1802s
Epoch: 19 cost time: 22.260740756988525
Epoch: 19, Steps: 258 | Train Loss: 0.3965602 Vali Loss: 0.9309354 Test Loss: 0.4161092
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3972679
	speed: 0.4301s/iter; left time: 3396.9706s
	iters: 200, epoch: 20 | loss: 0.3904997
	speed: 0.1025s/iter; left time: 799.7496s
Epoch: 20 cost time: 27.659079790115356
Epoch: 20, Steps: 258 | Train Loss: 0.3964000 Vali Loss: 0.9321612 Test Loss: 0.4159058
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3829969
	speed: 0.4223s/iter; left time: 3226.9165s
	iters: 200, epoch: 21 | loss: 0.3849583
	speed: 0.0957s/iter; left time: 721.7402s
Epoch: 21 cost time: 24.795358180999756
Epoch: 21, Steps: 258 | Train Loss: 0.3965087 Vali Loss: 0.9312366 Test Loss: 0.4163118
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3900791
	speed: 0.3890s/iter; left time: 2871.6820s
	iters: 200, epoch: 22 | loss: 0.4388872
	speed: 0.0925s/iter; left time: 673.9266s
Epoch: 22 cost time: 24.30189061164856
Epoch: 22, Steps: 258 | Train Loss: 0.3963683 Vali Loss: 0.9305050 Test Loss: 0.4161939
Validation loss decreased (0.930719 --> 0.930505).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4090898
	speed: 0.3902s/iter; left time: 2779.9966s
	iters: 200, epoch: 23 | loss: 0.3845296
	speed: 0.0835s/iter; left time: 586.8257s
Epoch: 23 cost time: 23.82136583328247
Epoch: 23, Steps: 258 | Train Loss: 0.3964171 Vali Loss: 0.9309833 Test Loss: 0.4163081
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4037973
	speed: 0.3975s/iter; left time: 2729.4344s
	iters: 200, epoch: 24 | loss: 0.4079799
	speed: 0.0608s/iter; left time: 411.5571s
Epoch: 24 cost time: 19.877333164215088
Epoch: 24, Steps: 258 | Train Loss: 0.3964108 Vali Loss: 0.9316263 Test Loss: 0.4164067
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3954456
	speed: 0.3888s/iter; left time: 2569.6268s
	iters: 200, epoch: 25 | loss: 0.4070653
	speed: 0.0885s/iter; left time: 576.0226s
Epoch: 25 cost time: 23.77392053604126
Epoch: 25, Steps: 258 | Train Loss: 0.3964370 Vali Loss: 0.9304574 Test Loss: 0.4164515
Validation loss decreased (0.930505 --> 0.930457).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3916013
	speed: 0.3950s/iter; left time: 2508.3409s
	iters: 200, epoch: 26 | loss: 0.4253387
	speed: 0.0833s/iter; left time: 520.9350s
Epoch: 26 cost time: 23.322283029556274
Epoch: 26, Steps: 258 | Train Loss: 0.3962236 Vali Loss: 0.9314858 Test Loss: 0.4160408
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3758536
	speed: 0.3934s/iter; left time: 2397.2697s
	iters: 200, epoch: 27 | loss: 0.4052127
	speed: 0.1023s/iter; left time: 612.8995s
Epoch: 27 cost time: 26.623273611068726
Epoch: 27, Steps: 258 | Train Loss: 0.3963693 Vali Loss: 0.9309318 Test Loss: 0.4161363
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4010943
	speed: 0.4326s/iter; left time: 2524.0287s
	iters: 200, epoch: 28 | loss: 0.3935373
	speed: 0.1012s/iter; left time: 580.3415s
Epoch: 28 cost time: 26.734111785888672
Epoch: 28, Steps: 258 | Train Loss: 0.3962329 Vali Loss: 0.9312547 Test Loss: 0.4160787
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4224916
	speed: 0.4087s/iter; left time: 2279.0612s
	iters: 200, epoch: 29 | loss: 0.3956257
	speed: 0.0941s/iter; left time: 515.1335s
Epoch: 29 cost time: 23.981473445892334
Epoch: 29, Steps: 258 | Train Loss: 0.3962714 Vali Loss: 0.9314520 Test Loss: 0.4163448
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4014650
	speed: 0.3732s/iter; left time: 1984.9731s
	iters: 200, epoch: 30 | loss: 0.3889440
	speed: 0.0937s/iter; left time: 489.0223s
Epoch: 30 cost time: 24.25865936279297
Epoch: 30, Steps: 258 | Train Loss: 0.3963841 Vali Loss: 0.9311702 Test Loss: 0.4162447
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4016446
	speed: 0.4298s/iter; left time: 2175.0243s
	iters: 200, epoch: 31 | loss: 0.3967610
	speed: 0.0940s/iter; left time: 466.3070s
Epoch: 31 cost time: 25.83220863342285
Epoch: 31, Steps: 258 | Train Loss: 0.3963604 Vali Loss: 0.9308642 Test Loss: 0.4163989
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3904861
	speed: 0.4189s/iter; left time: 2011.9555s
	iters: 200, epoch: 32 | loss: 0.3954504
	speed: 0.0883s/iter; left time: 415.4213s
Epoch: 32 cost time: 23.865578174591064
Epoch: 32, Steps: 258 | Train Loss: 0.3962648 Vali Loss: 0.9295639 Test Loss: 0.4163006
Validation loss decreased (0.930457 --> 0.929564).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3704237
	speed: 0.3794s/iter; left time: 1724.5991s
	iters: 200, epoch: 33 | loss: 0.3957152
	speed: 0.0848s/iter; left time: 376.8099s
Epoch: 33 cost time: 22.559799909591675
Epoch: 33, Steps: 258 | Train Loss: 0.3962415 Vali Loss: 0.9310322 Test Loss: 0.4162151
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4196925
	speed: 0.3633s/iter; left time: 1557.4463s
	iters: 200, epoch: 34 | loss: 0.3757139
	speed: 0.0885s/iter; left time: 370.7529s
Epoch: 34 cost time: 22.922046422958374
Epoch: 34, Steps: 258 | Train Loss: 0.3963743 Vali Loss: 0.9311543 Test Loss: 0.4163463
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4082580
	speed: 0.3696s/iter; left time: 1489.2475s
	iters: 200, epoch: 35 | loss: 0.4041365
	speed: 0.0761s/iter; left time: 299.1182s
Epoch: 35 cost time: 21.66604208946228
Epoch: 35, Steps: 258 | Train Loss: 0.3962697 Vali Loss: 0.9303564 Test Loss: 0.4162902
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3941039
	speed: 0.3441s/iter; left time: 1297.7658s
	iters: 200, epoch: 36 | loss: 0.4044402
	speed: 0.0528s/iter; left time: 193.8257s
Epoch: 36 cost time: 17.800164222717285
Epoch: 36, Steps: 258 | Train Loss: 0.3961375 Vali Loss: 0.9314566 Test Loss: 0.4163692
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3853141
	speed: 0.3931s/iter; left time: 1380.9379s
	iters: 200, epoch: 37 | loss: 0.3650906
	speed: 0.0902s/iter; left time: 307.9155s
Epoch: 37 cost time: 23.962353467941284
Epoch: 37, Steps: 258 | Train Loss: 0.3963369 Vali Loss: 0.9305736 Test Loss: 0.4163113
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3602857
	speed: 0.3738s/iter; left time: 1216.7282s
	iters: 200, epoch: 38 | loss: 0.3957328
	speed: 0.1011s/iter; left time: 319.0943s
Epoch: 38 cost time: 24.34855365753174
Epoch: 38, Steps: 258 | Train Loss: 0.3962822 Vali Loss: 0.9305460 Test Loss: 0.4162286
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.3972861
	speed: 0.4206s/iter; left time: 1260.4663s
	iters: 200, epoch: 39 | loss: 0.3944027
	speed: 0.0934s/iter; left time: 270.6895s
Epoch: 39 cost time: 25.928617238998413
Epoch: 39, Steps: 258 | Train Loss: 0.3962696 Vali Loss: 0.9310983 Test Loss: 0.4161305
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4304809
	speed: 0.4281s/iter; left time: 1172.5926s
	iters: 200, epoch: 40 | loss: 0.3703970
	speed: 0.0897s/iter; left time: 236.6422s
Epoch: 40 cost time: 24.586833238601685
Epoch: 40, Steps: 258 | Train Loss: 0.3961222 Vali Loss: 0.9309270 Test Loss: 0.4162343
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4410628
	speed: 0.3892s/iter; left time: 965.7044s
	iters: 200, epoch: 41 | loss: 0.3950317
	speed: 0.0781s/iter; left time: 185.8724s
Epoch: 41 cost time: 22.480149507522583
Epoch: 41, Steps: 258 | Train Loss: 0.3962156 Vali Loss: 0.9302255 Test Loss: 0.4163276
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4308936
	speed: 0.4211s/iter; left time: 936.1640s
	iters: 200, epoch: 42 | loss: 0.3778821
	speed: 0.1025s/iter; left time: 217.5334s
Epoch: 42 cost time: 26.522281408309937
Epoch: 42, Steps: 258 | Train Loss: 0.3960397 Vali Loss: 0.9307780 Test Loss: 0.4162551
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3624757
	speed: 0.4206s/iter; left time: 826.4350s
	iters: 200, epoch: 43 | loss: 0.4151791
	speed: 0.0906s/iter; left time: 169.0119s
Epoch: 43 cost time: 25.073904991149902
Epoch: 43, Steps: 258 | Train Loss: 0.3961092 Vali Loss: 0.9307898 Test Loss: 0.4161677
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4103840
	speed: 0.3893s/iter; left time: 664.5629s
	iters: 200, epoch: 44 | loss: 0.3773710
	speed: 0.0895s/iter; left time: 143.8865s
Epoch: 44 cost time: 23.708654165267944
Epoch: 44, Steps: 258 | Train Loss: 0.3961729 Vali Loss: 0.9311799 Test Loss: 0.4164141
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.3883588
	speed: 0.3786s/iter; left time: 548.5671s
	iters: 200, epoch: 45 | loss: 0.4268656
	speed: 0.0958s/iter; left time: 129.2637s
Epoch: 45 cost time: 23.637736558914185
Epoch: 45, Steps: 258 | Train Loss: 0.3962207 Vali Loss: 0.9312026 Test Loss: 0.4163670
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4238570
	speed: 0.3845s/iter; left time: 457.9192s
	iters: 200, epoch: 46 | loss: 0.3594434
	speed: 0.0900s/iter; left time: 98.2002s
Epoch: 46 cost time: 21.74613308906555
Epoch: 46, Steps: 258 | Train Loss: 0.3961601 Vali Loss: 0.9305281 Test Loss: 0.4162663
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.3995604
	speed: 0.2301s/iter; left time: 214.7028s
	iters: 200, epoch: 47 | loss: 0.4273967
	speed: 0.0840s/iter; left time: 70.0071s
Epoch: 47 cost time: 22.108307600021362
Epoch: 47, Steps: 258 | Train Loss: 0.3961841 Vali Loss: 0.9307982 Test Loss: 0.4162151
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.3784197
	speed: 0.3726s/iter; left time: 251.5190s
	iters: 200, epoch: 48 | loss: 0.3915083
	speed: 0.0860s/iter; left time: 49.4693s
Epoch: 48 cost time: 23.2389235496521
Epoch: 48, Steps: 258 | Train Loss: 0.3962302 Vali Loss: 0.9305947 Test Loss: 0.4162823
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.3641892
	speed: 0.3816s/iter; left time: 159.1365s
	iters: 200, epoch: 49 | loss: 0.4170124
	speed: 0.0831s/iter; left time: 26.3532s
Epoch: 49 cost time: 22.72358226776123
Epoch: 49, Steps: 258 | Train Loss: 0.3962268 Vali Loss: 0.9302789 Test Loss: 0.4163508
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.3897492
	speed: 0.4038s/iter; left time: 64.2091s
	iters: 200, epoch: 50 | loss: 0.4367202
	speed: 0.1012s/iter; left time: 5.9687s
Epoch: 50 cost time: 26.277577877044678
Epoch: 50, Steps: 258 | Train Loss: 0.3960843 Vali Loss: 0.9303783 Test Loss: 0.4163012
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.0497355408796396e-05
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41528311371803284, mae:0.411517858505249, rse:0.6131163239479065, corr:[0.5262465  0.53163564 0.5335726  0.5342934  0.53543246 0.53717744
 0.53860146 0.53918344 0.5394005  0.53974116 0.5403809  0.54100597
 0.5414095  0.5414853  0.5412169  0.540555   0.5395744  0.53851074
 0.53745717 0.53638715 0.53512543 0.53351885 0.53171784 0.53024566
 0.5291137  0.5283747  0.5277169  0.5268412  0.5260756  0.5257944
 0.52622783 0.5273063  0.528409   0.5291358  0.52921045 0.52919656
 0.5291878  0.5291865  0.5289599  0.5283429  0.5277251  0.52743906
 0.5276978  0.528308   0.5286703  0.52855176 0.52827084 0.52816343
 0.5282592  0.528295   0.5281305  0.5276729  0.5271594  0.52674574
 0.5266582  0.5268205  0.5270099  0.52699983 0.5268461  0.5264959
 0.52608883 0.5257206  0.52544284 0.52519345 0.5251748  0.52543795
 0.5258127  0.52611756 0.5263338  0.5264616  0.52662015 0.52674377
 0.526727   0.5265203  0.5262485  0.52596945 0.5257538  0.52566284
 0.5256336  0.5255564  0.52535397 0.5250273  0.5247049  0.524461
 0.5244317  0.52452147 0.5246331  0.5246308  0.5245454  0.524573
 0.5247985  0.5251976  0.5256582  0.52599627 0.52610195 0.52591443
 0.52558064 0.52531624 0.52504873 0.52483195 0.52459806 0.52433497
 0.52410704 0.5239223  0.5238488  0.5238921  0.52381635 0.5236018
 0.5232523  0.52298397 0.52288723 0.52284306 0.522687   0.52233666
 0.5218171  0.5213402  0.5210981  0.5210811  0.5211321  0.5210991
 0.5208414  0.5204163  0.51998407 0.51982164 0.51992065 0.51997566
 0.5197684  0.51937497 0.5189773  0.5188167  0.5188406  0.5188883
 0.5188745  0.51877904 0.5186346  0.51858747 0.5187103  0.5187661
 0.5187409  0.51875645 0.5188853  0.5192255  0.5196623  0.51995796
 0.5199619  0.5196903  0.5194556  0.51946694 0.5197348  0.5199627
 0.5199212  0.51966584 0.51934487 0.51914114 0.51912016 0.51930183
 0.51953477 0.51967996 0.5197525  0.5196935  0.5196249  0.5195811
 0.51960015 0.51969975 0.51989853 0.5201436  0.5203668  0.5205448
 0.5206549  0.5207151  0.520841   0.5210151  0.5210758  0.5209908
 0.52083516 0.52070487 0.52066976 0.52073234 0.5208233  0.5209217
 0.5210047  0.52110165 0.5211772  0.5212814  0.5213971  0.521496
 0.5215865  0.52178174 0.5221094  0.5224716  0.5227229  0.5227204
 0.5224231  0.52193725 0.52130306 0.5206177  0.51997894 0.51944405
 0.51894426 0.51840615 0.5177409  0.5169808  0.5161702  0.5154116
 0.514761   0.51419514 0.51364285 0.5130991  0.51255125 0.5120196
 0.51149505 0.5110172  0.51052487 0.5099546  0.50924593 0.50856245
 0.50804746 0.5076559  0.50735825 0.5070554  0.5067878  0.5066926
 0.5068758  0.50713664 0.50736713 0.5074921  0.5074432  0.50731754
 0.5072474  0.5072998  0.5074314  0.50751156 0.50753075 0.5075494
 0.5076006  0.5077276  0.50779593 0.5079014  0.50799596 0.5081421
 0.5082867  0.5083134  0.5082713  0.5082325  0.5082494  0.508263
 0.50824785 0.50820786 0.50813025 0.50804377 0.5079011  0.5077882
 0.50766784 0.5076468  0.50766045 0.5076582  0.50763685 0.50762165
 0.50764596 0.5077387  0.5078541  0.5079436  0.5079934  0.5081047
 0.5082107  0.50833505 0.50848335 0.5086648  0.50878835 0.50874835
 0.5085964  0.5084089  0.50831914 0.50837845 0.5084943  0.5085456
 0.5084185  0.5082334  0.5080454  0.50803447 0.5081766  0.5083578
 0.50853056 0.5086041  0.50857717 0.50849843 0.50833404 0.50802267
 0.50756353 0.5070463  0.506528   0.5060104  0.50552875 0.5050522
 0.50452703 0.503872   0.50321215 0.50262624 0.50218403 0.5018838
 0.50158876 0.5012268  0.5008044  0.5003738  0.5000196  0.49975404
 0.49962696 0.49945223 0.49913493 0.4987111  0.49837878 0.49822637
 0.49827567 0.49847475 0.49860907 0.49859312 0.4984649  0.49836174
 0.4983244  0.4982801  0.49817064 0.4979919  0.49779978 0.49766436
 0.49761713 0.4976449  0.49758714 0.49740294 0.49711493 0.49685553
 0.496831   0.49701092 0.497221   0.4973624  0.49738488 0.4974353
 0.49751762 0.49749666 0.4974462  0.4973455  0.49722752 0.49709192
 0.49692938 0.49683344 0.4967976  0.49681386 0.4968488  0.49688727
 0.49683437 0.49675602 0.49670604 0.49669322 0.49675772 0.49684274
 0.49686816 0.49685687 0.49682802 0.4968152  0.4968584  0.49691397
 0.49686298 0.49675903 0.49663016 0.49653068 0.49649706 0.49649814
 0.49640688 0.49626958 0.49612272 0.49598193 0.49595672 0.49606723
 0.49625576 0.49645138 0.49658176 0.49669382 0.49682102 0.4970449
 0.49736717 0.49772844 0.49804923 0.49826533 0.4983657  0.4983927
 0.49834687 0.49819183 0.49789637 0.49741247 0.4967698  0.49620187
 0.49576762 0.49545112 0.49516174 0.49485996 0.4944757  0.4940912
 0.49378285 0.49357477 0.49337414 0.49304813 0.49251705 0.49197435
 0.4915296  0.49130678 0.49115154 0.49096456 0.49065197 0.4903068
 0.49003196 0.48993415 0.48998836 0.49011588 0.4902335  0.49023592
 0.49028683 0.49030107 0.49037537 0.49040744 0.49043018 0.49049518
 0.490526   0.49059147 0.49063376 0.49066508 0.49064422 0.49064282
 0.4906272  0.49063957 0.49064085 0.49073362 0.49082693 0.49097544
 0.49102762 0.4909023  0.49069935 0.4905405  0.4904507  0.49041075
 0.4903692  0.49031004 0.49025849 0.49021754 0.4901952  0.49018645
 0.49015054 0.49013346 0.49014404 0.4901835  0.4902393  0.49024695
 0.4902657  0.4902965  0.4903588  0.4904114  0.49042654 0.4904565
 0.49040702 0.49035606 0.4903495  0.49044704 0.49052605 0.49057758
 0.4905388  0.49048248 0.49041292 0.49041536 0.49047047 0.49048868
 0.49043813 0.49027094 0.49009463 0.490003   0.49005035 0.4902042
 0.49035957 0.49050605 0.49061    0.49059424 0.4904337  0.49014872
 0.4897412  0.48922706 0.48863256 0.48797497 0.487322   0.48677585
 0.4863733  0.48592773 0.4853903  0.484776   0.48406658 0.48332682
 0.4826297  0.48205334 0.48154867 0.48103282 0.4805053  0.48004273
 0.47963867 0.479336   0.4790896  0.47874328 0.47847545 0.4782858
 0.47822043 0.47820228 0.47813234 0.4780651  0.4780317  0.47812065
 0.47833526 0.4785319  0.47864205 0.47876006 0.4788862  0.47904906
 0.47921848 0.47944346 0.47969317 0.47985107 0.47994348 0.48008502
 0.48031262 0.48058623 0.48079222 0.48081848 0.48076922 0.48076877
 0.48082456 0.48087683 0.48085162 0.48081398 0.48078176 0.48073146
 0.48069152 0.48066187 0.48063362 0.48056963 0.48044688 0.48029724
 0.48019207 0.48016733 0.48022303 0.48027912 0.4802557  0.48020697
 0.48018613 0.48025423 0.48037013 0.48037994 0.4802423  0.480086
 0.4799225  0.47987    0.4799823  0.4802206  0.48042133 0.4805012
 0.48041806 0.48026988 0.48015785 0.4801496  0.48021445 0.48025215
 0.48019385 0.48018947 0.48019984 0.4802909  0.48042792 0.48056453
 0.48066142 0.480726   0.4808321  0.48091745 0.4808797  0.48064137
 0.4802316  0.47974873 0.47922325 0.47865063 0.47804683 0.4775162
 0.47702423 0.47656962 0.47613034 0.47570354 0.47529492 0.4748263
 0.47432292 0.47375545 0.4731502  0.47252738 0.47197855 0.47151572
 0.47113636 0.47081193 0.4705697  0.47038665 0.47022083 0.4700244
 0.46984652 0.46966594 0.46955267 0.46954066 0.46964568 0.46975687
 0.46991083 0.470024   0.47013134 0.47029907 0.47053656 0.47084963
 0.471122   0.47120428 0.47118297 0.47113463 0.47119418 0.47136238
 0.47158816 0.4717652  0.47172526 0.47156915 0.47158533 0.47183445
 0.4721284  0.47237006 0.472474   0.47245777 0.47246155 0.4725131
 0.47247157 0.4723966  0.47232494 0.4722226  0.47216436 0.47202948
 0.4717171  0.47132263 0.47101295 0.4707965  0.47079632 0.47091243
 0.47102687 0.47103113 0.4709376  0.47083586 0.4707737  0.47079837
 0.47080845 0.4706778  0.47051612 0.47042418 0.47042948 0.47048727
 0.47054827 0.47057384 0.47055873 0.47050524 0.4704175  0.4702954
 0.470181   0.47019354 0.47031093 0.47042975 0.4706075  0.47080925
 0.47102347 0.47122225 0.47137657 0.47145563 0.4713637  0.47110197
 0.47077364 0.47053927 0.47034267 0.47000846 0.46948338 0.46886498
 0.46833837 0.46795532 0.46777618 0.4676349  0.4672845  0.4669123
 0.46645963 0.46616068 0.46594116 0.46578088 0.46554893 0.46520358
 0.46487552 0.46461245 0.464451   0.46420932 0.46398932 0.46390942
 0.46395874 0.4641833  0.4643885  0.46443394 0.4644051  0.46426174
 0.4642323  0.4643351  0.4645693  0.46495688 0.46519884 0.46530363
 0.46536463 0.46555346 0.46597332 0.4665391  0.46718496 0.4676671
 0.46803534 0.46829736 0.46842527 0.46881193 0.46909884 0.4663609 ]
