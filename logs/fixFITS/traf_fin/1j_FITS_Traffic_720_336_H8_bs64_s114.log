Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H8', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j336_H8_FITS_custom_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=258, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10760408064.0
params:  97902.0
Trainable parameters:  97902
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 72.84874844551086
Epoch: 1, Steps: 87 | Train Loss: 0.8229635 Vali Loss: 0.7153024 Test Loss: 0.8226689
Validation loss decreased (inf --> 0.715302).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 71.11259293556213
Epoch: 2, Steps: 87 | Train Loss: 0.4656222 Vali Loss: 0.5093637 Test Loss: 0.5900922
Validation loss decreased (0.715302 --> 0.509364).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 84.42237758636475
Epoch: 3, Steps: 87 | Train Loss: 0.3444911 Vali Loss: 0.4173498 Test Loss: 0.4886685
Validation loss decreased (0.509364 --> 0.417350).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 80.73994398117065
Epoch: 4, Steps: 87 | Train Loss: 0.2900331 Vali Loss: 0.3757313 Test Loss: 0.4442694
Validation loss decreased (0.417350 --> 0.375731).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 74.9606864452362
Epoch: 5, Steps: 87 | Train Loss: 0.2662577 Vali Loss: 0.3573276 Test Loss: 0.4263408
Validation loss decreased (0.375731 --> 0.357328).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 83.03638482093811
Epoch: 6, Steps: 87 | Train Loss: 0.2562924 Vali Loss: 0.3493762 Test Loss: 0.4192927
Validation loss decreased (0.357328 --> 0.349376).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 70.09388852119446
Epoch: 7, Steps: 87 | Train Loss: 0.2522675 Vali Loss: 0.3458919 Test Loss: 0.4166195
Validation loss decreased (0.349376 --> 0.345892).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 73.98802256584167
Epoch: 8, Steps: 87 | Train Loss: 0.2504661 Vali Loss: 0.3443155 Test Loss: 0.4156486
Validation loss decreased (0.345892 --> 0.344315).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 70.31726908683777
Epoch: 9, Steps: 87 | Train Loss: 0.2497194 Vali Loss: 0.3436660 Test Loss: 0.4153111
Validation loss decreased (0.344315 --> 0.343666).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 65.59591317176819
Epoch: 10, Steps: 87 | Train Loss: 0.2492595 Vali Loss: 0.3424880 Test Loss: 0.4149088
Validation loss decreased (0.343666 --> 0.342488).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 64.76933193206787
Epoch: 11, Steps: 87 | Train Loss: 0.2490567 Vali Loss: 0.3424913 Test Loss: 0.4150185
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 74.1831316947937
Epoch: 12, Steps: 87 | Train Loss: 0.2488888 Vali Loss: 0.3423738 Test Loss: 0.4147097
Validation loss decreased (0.342488 --> 0.342374).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 71.02600526809692
Epoch: 13, Steps: 87 | Train Loss: 0.2488818 Vali Loss: 0.3420866 Test Loss: 0.4143962
Validation loss decreased (0.342374 --> 0.342087).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 67.98716497421265
Epoch: 14, Steps: 87 | Train Loss: 0.2487515 Vali Loss: 0.3417859 Test Loss: 0.4144495
Validation loss decreased (0.342087 --> 0.341786).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 70.96678853034973
Epoch: 15, Steps: 87 | Train Loss: 0.2486575 Vali Loss: 0.3414437 Test Loss: 0.4145673
Validation loss decreased (0.341786 --> 0.341444).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 74.4243814945221
Epoch: 16, Steps: 87 | Train Loss: 0.2486441 Vali Loss: 0.3412438 Test Loss: 0.4142956
Validation loss decreased (0.341444 --> 0.341244).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 78.26909303665161
Epoch: 17, Steps: 87 | Train Loss: 0.2485752 Vali Loss: 0.3412756 Test Loss: 0.4143160
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 74.164715051651
Epoch: 18, Steps: 87 | Train Loss: 0.2486008 Vali Loss: 0.3414105 Test Loss: 0.4142938
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 79.91282343864441
Epoch: 19, Steps: 87 | Train Loss: 0.2485481 Vali Loss: 0.3406750 Test Loss: 0.4142239
Validation loss decreased (0.341244 --> 0.340675).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 77.36456489562988
Epoch: 20, Steps: 87 | Train Loss: 0.2483756 Vali Loss: 0.3405883 Test Loss: 0.4140792
Validation loss decreased (0.340675 --> 0.340588).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 82.0456395149231
Epoch: 21, Steps: 87 | Train Loss: 0.2484562 Vali Loss: 0.3409930 Test Loss: 0.4141616
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 81.71544742584229
Epoch: 22, Steps: 87 | Train Loss: 0.2483613 Vali Loss: 0.3407229 Test Loss: 0.4139238
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 75.48105263710022
Epoch: 23, Steps: 87 | Train Loss: 0.2484338 Vali Loss: 0.3407356 Test Loss: 0.4141520
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j336_H8_FITS_custom_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4119414687156677, mae:0.28094229102134705, rse:0.527493417263031, corr:[0.275914   0.2806647  0.28412974 0.28266883 0.28454807 0.2860785
 0.28480735 0.28544    0.28556257 0.28421804 0.28492078 0.2853167
 0.2840497  0.2840298  0.28416067 0.283242   0.2833694  0.283927
 0.28359988 0.28374806 0.28413364 0.28389502 0.28414813 0.28489268
 0.2857282  0.28595802 0.28632647 0.2858884  0.28577802 0.28621742
 0.2855781  0.28490126 0.285126   0.28471822 0.28422526 0.28480154
 0.28476876 0.28415245 0.28450692 0.2848113  0.28454092 0.28481433
 0.28527695 0.28525868 0.28541857 0.28532767 0.284643   0.28470743
 0.28531379 0.28494376 0.2847822  0.2850369  0.2847199  0.28459072
 0.28496024 0.28468964 0.2843831  0.28469834 0.28427127 0.2835771
 0.28409952 0.2846575  0.28467363 0.2850563  0.28509846 0.28448078
 0.28453025 0.28454798 0.28403682 0.2840958  0.28436574 0.28416413
 0.2841177  0.28427497 0.28394568 0.2838578  0.28417343 0.28392094
 0.28381285 0.28440487 0.2845484  0.28433463 0.28457347 0.28448808
 0.28405312 0.28421688 0.284402   0.284216   0.28455812 0.28491968
 0.28469637 0.28467    0.2848195  0.28455523 0.2846287  0.28507477
 0.2846507  0.28448436 0.2848722  0.28453743 0.28400066 0.28415734
 0.28416064 0.2836996  0.28361836 0.28368852 0.2836512  0.2839651
 0.2843024  0.28430897 0.28441057 0.28431353 0.28389084 0.28404915
 0.2843703  0.2838644  0.28359964 0.28402135 0.28412902 0.2840591
 0.284245   0.2842911  0.28417683 0.2844924  0.28481168 0.28482592
 0.28495154 0.28478396 0.28433448 0.2843662  0.28446755 0.28416735
 0.2842617  0.28473178 0.28475258 0.28470835 0.28486496 0.28457543
 0.2842092  0.2843097  0.2843362  0.28426042 0.28439403 0.28440046
 0.28448486 0.28465024 0.2845995  0.28461006 0.28505474 0.28514084
 0.28476837 0.28492704 0.28520837 0.2848777  0.28469107 0.2848083
 0.2846337  0.2847034  0.28509784 0.28492814 0.28463653 0.28477785
 0.2846228  0.28424105 0.2842586  0.28422368 0.28408748 0.2846017
 0.28561538 0.28539738 0.28559023 0.2858692  0.28564298 0.28570554
 0.28581595 0.28553563 0.2857272  0.28618613 0.28597656 0.28583658
 0.28615132 0.28602576 0.2857278  0.28574917 0.28559586 0.2854041
 0.28538743 0.2849982  0.28466666 0.28517932 0.28538555 0.28493124
 0.28535563 0.28549588 0.28518194 0.28522682 0.2855093  0.28540638
 0.28538117 0.28544474 0.28521362 0.28534964 0.2856164  0.28517386
 0.28496626 0.28537932 0.285098   0.2844171  0.28441787 0.28431174
 0.2838475  0.28398293 0.28428042 0.28430903 0.28467178 0.2849897
 0.28469494 0.28443697 0.28450412 0.28410065 0.28396136 0.28442356
 0.28445533 0.28440866 0.28475004 0.2845521  0.2840038  0.2838759
 0.28370327 0.28365463 0.28418717 0.2842448  0.28390837 0.28421536
 0.28428447 0.28367153 0.28376228 0.28420064 0.28410578 0.28423515
 0.28449616 0.28410465 0.28384236 0.28400126 0.28375432 0.28361866
 0.28411564 0.2842795  0.28425026 0.28457764 0.2842818  0.28349286
 0.2834155  0.28334573 0.28290126 0.28308198 0.28343114 0.28340086
 0.28369525 0.28388166 0.28354856 0.28362653 0.28374174 0.28324386
 0.283132   0.28363273 0.28347814 0.28328547 0.2837535  0.28384498
 0.28363472 0.2836558  0.28323603 0.2828854  0.28336167 0.28331622
 0.28275597 0.28302452 0.28315946 0.2826745  0.2830347  0.2834224
 0.28271988 0.28261802 0.28321978 0.28305167 0.28299576 0.28341255
 0.2830755  0.2829649  0.2833081  0.28295165 0.28280985 0.28347325
 0.28331354 0.2826591  0.2829668  0.28312308 0.28288957 0.28339803
 0.2835433  0.28310823 0.283662   0.28407958 0.2836136  0.28401324
 0.2843981  0.28354105 0.28350642 0.28407234 0.28344062 0.2832313
 0.28391096 0.28347194 0.28328392 0.283811   0.2833633  0.2832364
 0.2837515  0.2829026  0.28253338 0.28353947 0.28308502 0.2829598
 0.2847574  0.2846195  0.28379327 0.2850971  0.2846     0.28333935
 0.2847421  0.2830372  0.28030878 0.28232008 0.2786316  0.28724214]
