Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H8', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j336_H8_FITS_custom_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=258, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10760408064.0
params:  97902.0
Trainable parameters:  97902
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 68.06656885147095
Epoch: 1, Steps: 87 | Train Loss: 1.0937679 Vali Loss: 1.1489041 Test Loss: 1.3359381
Validation loss decreased (inf --> 1.148904).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 71.07417154312134
Epoch: 2, Steps: 87 | Train Loss: 0.8118962 Vali Loss: 1.0261689 Test Loss: 1.1910701
Validation loss decreased (1.148904 --> 1.026169).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 70.36667275428772
Epoch: 3, Steps: 87 | Train Loss: 0.7132353 Vali Loss: 0.9597570 Test Loss: 1.1136006
Validation loss decreased (1.026169 --> 0.959757).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 73.04753351211548
Epoch: 4, Steps: 87 | Train Loss: 0.6449172 Vali Loss: 0.9036157 Test Loss: 1.0481362
Validation loss decreased (0.959757 --> 0.903616).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 81.42300152778625
Epoch: 5, Steps: 87 | Train Loss: 0.5893841 Vali Loss: 0.8577189 Test Loss: 0.9955137
Validation loss decreased (0.903616 --> 0.857719).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 78.54676032066345
Epoch: 6, Steps: 87 | Train Loss: 0.5424081 Vali Loss: 0.8158394 Test Loss: 0.9468255
Validation loss decreased (0.857719 --> 0.815839).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 81.36928653717041
Epoch: 7, Steps: 87 | Train Loss: 0.5020812 Vali Loss: 0.7808998 Test Loss: 0.9065427
Validation loss decreased (0.815839 --> 0.780900).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 79.56515765190125
Epoch: 8, Steps: 87 | Train Loss: 0.4669319 Vali Loss: 0.7457020 Test Loss: 0.8660477
Validation loss decreased (0.780900 --> 0.745702).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 78.23866438865662
Epoch: 9, Steps: 87 | Train Loss: 0.4362254 Vali Loss: 0.7180399 Test Loss: 0.8341280
Validation loss decreased (0.745702 --> 0.718040).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 76.44558691978455
Epoch: 10, Steps: 87 | Train Loss: 0.4091101 Vali Loss: 0.6909316 Test Loss: 0.8037017
Validation loss decreased (0.718040 --> 0.690932).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 78.57068634033203
Epoch: 11, Steps: 87 | Train Loss: 0.3850519 Vali Loss: 0.6683168 Test Loss: 0.7771481
Validation loss decreased (0.690932 --> 0.668317).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 78.06563425064087
Epoch: 12, Steps: 87 | Train Loss: 0.3636878 Vali Loss: 0.6469139 Test Loss: 0.7521820
Validation loss decreased (0.668317 --> 0.646914).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 81.37334132194519
Epoch: 13, Steps: 87 | Train Loss: 0.3445275 Vali Loss: 0.6268303 Test Loss: 0.7292893
Validation loss decreased (0.646914 --> 0.626830).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 77.38102698326111
Epoch: 14, Steps: 87 | Train Loss: 0.3273402 Vali Loss: 0.6086557 Test Loss: 0.7086593
Validation loss decreased (0.626830 --> 0.608656).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 78.31557703018188
Epoch: 15, Steps: 87 | Train Loss: 0.3118168 Vali Loss: 0.5913188 Test Loss: 0.6890016
Validation loss decreased (0.608656 --> 0.591319).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 79.49759531021118
Epoch: 16, Steps: 87 | Train Loss: 0.2978310 Vali Loss: 0.5768915 Test Loss: 0.6723170
Validation loss decreased (0.591319 --> 0.576892).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 77.06113147735596
Epoch: 17, Steps: 87 | Train Loss: 0.2851397 Vali Loss: 0.5637906 Test Loss: 0.6571448
Validation loss decreased (0.576892 --> 0.563791).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 82.6959879398346
Epoch: 18, Steps: 87 | Train Loss: 0.2736057 Vali Loss: 0.5527073 Test Loss: 0.6444489
Validation loss decreased (0.563791 --> 0.552707).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 76.77963447570801
Epoch: 19, Steps: 87 | Train Loss: 0.2630507 Vali Loss: 0.5401339 Test Loss: 0.6303478
Validation loss decreased (0.552707 --> 0.540134).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 76.21211290359497
Epoch: 20, Steps: 87 | Train Loss: 0.2534570 Vali Loss: 0.5297998 Test Loss: 0.6184283
Validation loss decreased (0.540134 --> 0.529800).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 77.22003030776978
Epoch: 21, Steps: 87 | Train Loss: 0.2446670 Vali Loss: 0.5203394 Test Loss: 0.6070487
Validation loss decreased (0.529800 --> 0.520339).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 68.01220273971558
Epoch: 22, Steps: 87 | Train Loss: 0.2366076 Vali Loss: 0.5113087 Test Loss: 0.5971844
Validation loss decreased (0.520339 --> 0.511309).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 77.90281915664673
Epoch: 23, Steps: 87 | Train Loss: 0.2292009 Vali Loss: 0.5035145 Test Loss: 0.5885404
Validation loss decreased (0.511309 --> 0.503515).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 73.93927454948425
Epoch: 24, Steps: 87 | Train Loss: 0.2223983 Vali Loss: 0.4966120 Test Loss: 0.5805670
Validation loss decreased (0.503515 --> 0.496612).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 79.81993269920349
Epoch: 25, Steps: 87 | Train Loss: 0.2161194 Vali Loss: 0.4895359 Test Loss: 0.5727841
Validation loss decreased (0.496612 --> 0.489536).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 76.55614304542542
Epoch: 26, Steps: 87 | Train Loss: 0.2103253 Vali Loss: 0.4836064 Test Loss: 0.5654351
Validation loss decreased (0.489536 --> 0.483606).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 78.57806634902954
Epoch: 27, Steps: 87 | Train Loss: 0.2049371 Vali Loss: 0.4770461 Test Loss: 0.5585445
Validation loss decreased (0.483606 --> 0.477046).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 83.35774898529053
Epoch: 28, Steps: 87 | Train Loss: 0.1999980 Vali Loss: 0.4712204 Test Loss: 0.5518083
Validation loss decreased (0.477046 --> 0.471220).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 78.04529047012329
Epoch: 29, Steps: 87 | Train Loss: 0.1953570 Vali Loss: 0.4667906 Test Loss: 0.5464892
Validation loss decreased (0.471220 --> 0.466791).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 75.28742003440857
Epoch: 30, Steps: 87 | Train Loss: 0.1910675 Vali Loss: 0.4618094 Test Loss: 0.5408961
Validation loss decreased (0.466791 --> 0.461809).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 78.14201498031616
Epoch: 31, Steps: 87 | Train Loss: 0.1870945 Vali Loss: 0.4574332 Test Loss: 0.5358751
Validation loss decreased (0.461809 --> 0.457433).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 80.50790166854858
Epoch: 32, Steps: 87 | Train Loss: 0.1833756 Vali Loss: 0.4532015 Test Loss: 0.5317498
Validation loss decreased (0.457433 --> 0.453202).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 87.24217200279236
Epoch: 33, Steps: 87 | Train Loss: 0.1799500 Vali Loss: 0.4487478 Test Loss: 0.5266265
Validation loss decreased (0.453202 --> 0.448748).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 67.3987455368042
Epoch: 34, Steps: 87 | Train Loss: 0.1767039 Vali Loss: 0.4457003 Test Loss: 0.5228972
Validation loss decreased (0.448748 --> 0.445700).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 66.6535816192627
Epoch: 35, Steps: 87 | Train Loss: 0.1736767 Vali Loss: 0.4419861 Test Loss: 0.5190364
Validation loss decreased (0.445700 --> 0.441986).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 68.80241465568542
Epoch: 36, Steps: 87 | Train Loss: 0.1708883 Vali Loss: 0.4398383 Test Loss: 0.5158666
Validation loss decreased (0.441986 --> 0.439838).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 69.23951125144958
Epoch: 37, Steps: 87 | Train Loss: 0.1682269 Vali Loss: 0.4361502 Test Loss: 0.5121467
Validation loss decreased (0.439838 --> 0.436150).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 70.6483702659607
Epoch: 38, Steps: 87 | Train Loss: 0.1657436 Vali Loss: 0.4334746 Test Loss: 0.5087720
Validation loss decreased (0.436150 --> 0.433475).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 73.3551037311554
Epoch: 39, Steps: 87 | Train Loss: 0.1634604 Vali Loss: 0.4312117 Test Loss: 0.5062615
Validation loss decreased (0.433475 --> 0.431212).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 72.84896183013916
Epoch: 40, Steps: 87 | Train Loss: 0.1612542 Vali Loss: 0.4281482 Test Loss: 0.5033399
Validation loss decreased (0.431212 --> 0.428148).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 73.01777672767639
Epoch: 41, Steps: 87 | Train Loss: 0.1592037 Vali Loss: 0.4257253 Test Loss: 0.5005503
Validation loss decreased (0.428148 --> 0.425725).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 70.07275366783142
Epoch: 42, Steps: 87 | Train Loss: 0.1573112 Vali Loss: 0.4240029 Test Loss: 0.4984332
Validation loss decreased (0.425725 --> 0.424003).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 74.22335720062256
Epoch: 43, Steps: 87 | Train Loss: 0.1554971 Vali Loss: 0.4219110 Test Loss: 0.4958667
Validation loss decreased (0.424003 --> 0.421911).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 80.59601783752441
Epoch: 44, Steps: 87 | Train Loss: 0.1537863 Vali Loss: 0.4191830 Test Loss: 0.4937185
Validation loss decreased (0.421911 --> 0.419183).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 67.43037962913513
Epoch: 45, Steps: 87 | Train Loss: 0.1522182 Vali Loss: 0.4178361 Test Loss: 0.4919841
Validation loss decreased (0.419183 --> 0.417836).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 70.87541484832764
Epoch: 46, Steps: 87 | Train Loss: 0.1506856 Vali Loss: 0.4161011 Test Loss: 0.4896538
Validation loss decreased (0.417836 --> 0.416101).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 68.4334807395935
Epoch: 47, Steps: 87 | Train Loss: 0.1492387 Vali Loss: 0.4145167 Test Loss: 0.4880508
Validation loss decreased (0.416101 --> 0.414517).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 71.24097084999084
Epoch: 48, Steps: 87 | Train Loss: 0.1479342 Vali Loss: 0.4126447 Test Loss: 0.4862498
Validation loss decreased (0.414517 --> 0.412645).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 73.22249698638916
Epoch: 49, Steps: 87 | Train Loss: 0.1466405 Vali Loss: 0.4117364 Test Loss: 0.4847869
Validation loss decreased (0.412645 --> 0.411736).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 77.4815902709961
Epoch: 50, Steps: 87 | Train Loss: 0.1454186 Vali Loss: 0.4097778 Test Loss: 0.4831862
Validation loss decreased (0.411736 --> 0.409778).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=258, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10760408064.0
params:  97902.0
Trainable parameters:  97902
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 73.28194236755371
Epoch: 1, Steps: 87 | Train Loss: 0.2689240 Vali Loss: 0.3469127 Test Loss: 0.4178644
Validation loss decreased (inf --> 0.346913).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 74.83932995796204
Epoch: 2, Steps: 87 | Train Loss: 0.2498899 Vali Loss: 0.3424155 Test Loss: 0.4153566
Validation loss decreased (0.346913 --> 0.342416).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 75.46612024307251
Epoch: 3, Steps: 87 | Train Loss: 0.2489694 Vali Loss: 0.3409761 Test Loss: 0.4146551
Validation loss decreased (0.342416 --> 0.340976).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 77.75049352645874
Epoch: 4, Steps: 87 | Train Loss: 0.2488502 Vali Loss: 0.3415487 Test Loss: 0.4149506
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 75.15678811073303
Epoch: 5, Steps: 87 | Train Loss: 0.2488615 Vali Loss: 0.3410179 Test Loss: 0.4145984
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 77.79316568374634
Epoch: 6, Steps: 87 | Train Loss: 0.2486763 Vali Loss: 0.3408430 Test Loss: 0.4146884
Validation loss decreased (0.340976 --> 0.340843).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 78.8060736656189
Epoch: 7, Steps: 87 | Train Loss: 0.2486898 Vali Loss: 0.3405582 Test Loss: 0.4145113
Validation loss decreased (0.340843 --> 0.340558).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 76.41652703285217
Epoch: 8, Steps: 87 | Train Loss: 0.2485853 Vali Loss: 0.3402842 Test Loss: 0.4141229
Validation loss decreased (0.340558 --> 0.340284).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 81.05130243301392
Epoch: 9, Steps: 87 | Train Loss: 0.2486393 Vali Loss: 0.3403685 Test Loss: 0.4142624
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 79.17945337295532
Epoch: 10, Steps: 87 | Train Loss: 0.2485625 Vali Loss: 0.3405703 Test Loss: 0.4140472
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 73.75055480003357
Epoch: 11, Steps: 87 | Train Loss: 0.2485423 Vali Loss: 0.3409488 Test Loss: 0.4143322
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j336_H8_FITS_custom_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.41195815801620483, mae:0.2809264361858368, rse:0.5275040864944458, corr:[0.27437752 0.28421032 0.28513622 0.28460625 0.28473642 0.28486308
 0.2848591  0.28547642 0.28549293 0.28491217 0.28489643 0.28498033
 0.28489247 0.2845584  0.28384233 0.2836483  0.2839767  0.28397012
 0.2841131  0.28467816 0.28465515 0.28412423 0.28402922 0.28454208
 0.28566673 0.28548825 0.2849786  0.2844692  0.28433764 0.28474295
 0.28509125 0.28503102 0.28481928 0.2847767  0.28496024 0.2851612
 0.28495276 0.2844043  0.28408545 0.28420678 0.2843072  0.28402823
 0.2840502  0.28452635 0.2847815  0.28490338 0.28540084 0.2857574
 0.28558537 0.2852541  0.285063   0.28445002 0.28423345 0.2846702
 0.28459147 0.28437245 0.28478315 0.2847958  0.28422225 0.28429595
 0.28467292 0.28461978 0.28506777 0.2858802  0.28567877 0.28478613
 0.2845455  0.2845243  0.28438732 0.28454018 0.28475896 0.2847564
 0.28463778 0.28447908 0.2840108  0.2835269  0.28329566 0.28301552
 0.28294766 0.28338388 0.28378266 0.28368527 0.283353   0.28319407
 0.28336078 0.28358603 0.28383192 0.28432423 0.28480384 0.28483313
 0.28471413 0.2845045  0.2841188  0.28386453 0.2838412  0.2839078
 0.28390273 0.2841213  0.2841595  0.28394273 0.2839418  0.28402954
 0.28389156 0.2837288  0.28386694 0.284104   0.2841452  0.28410417
 0.28408018 0.28392527 0.28391385 0.28431585 0.2846219  0.28451747
 0.28449047 0.28457928 0.28423798 0.28343746 0.2832312  0.28390542
 0.28443363 0.28462923 0.28492174 0.28516567 0.28500798 0.28469715
 0.28463924 0.28453478 0.28433326 0.28440297 0.28451067 0.2843097
 0.28409332 0.28415698 0.28436255 0.2846715  0.2850338  0.28503728
 0.28471377 0.28445497 0.28407407 0.28346932 0.28331944 0.2840833
 0.28523606 0.28562045 0.28540036 0.28518245 0.28499714 0.2847214
 0.2845707  0.28446934 0.28409335 0.2838162  0.28416267 0.28469363
 0.2848587  0.28491548 0.2851688  0.28555712 0.2858358  0.28572664
 0.2854461  0.28539202 0.28537974 0.28510764 0.2846982  0.28479934
 0.28612256 0.28659555 0.28674367 0.2865356  0.2859317  0.28547797
 0.28548175 0.285622   0.28560787 0.2854759  0.28524682 0.28505412
 0.28496227 0.2848265  0.2848347  0.28510427 0.28525844 0.28515464
 0.28508464 0.28494957 0.2845452  0.28443673 0.28463364 0.28479517
 0.28538036 0.2856943  0.28600207 0.28592926 0.28568596 0.28572857
 0.28581557 0.2856149  0.2853418  0.28516278 0.28489473 0.28463733
 0.28449196 0.28429642 0.28438732 0.28493685 0.28506333 0.2843955
 0.28393236 0.2840726  0.28404292 0.28376812 0.28375104 0.28408852
 0.28454965 0.2847432  0.28487757 0.28505075 0.2852778  0.28522927
 0.28482944 0.2846676  0.28466386 0.2841258  0.28356338 0.28362742
 0.28375772 0.2837843  0.28426033 0.284699   0.28443372 0.28399965
 0.28387287 0.2838183  0.28391019 0.2840468  0.28411067 0.2842874
 0.2844001  0.28420946 0.28424814 0.28438985 0.28400415 0.2835744
 0.28375956 0.28408805 0.28415388 0.28425255 0.28443685 0.2843263
 0.28387162 0.28348896 0.28353852 0.28378847 0.2837449  0.28339684
 0.2832119  0.28333274 0.28345004 0.2836446  0.28403288 0.284036
 0.28360564 0.28363785 0.28388166 0.28367677 0.2832884  0.2831926
 0.28318217 0.28301284 0.2829512  0.28319162 0.2831621  0.28246042
 0.28197238 0.28217095 0.2823872  0.28238556 0.28252083 0.28279015
 0.2830351  0.28319687 0.28318283 0.2831262  0.28308988 0.28279805
 0.2825134  0.28309956 0.28379348 0.2835887  0.28299817 0.28270483
 0.28244114 0.28235304 0.28269804 0.2827918  0.28249943 0.28222027
 0.281754   0.2813509  0.28178227 0.28243068 0.2825594  0.2828753
 0.28365508 0.28414372 0.28425494 0.28413063 0.28353092 0.28297308
 0.2832696  0.28371516 0.28402126 0.28427082 0.2840134  0.28345606
 0.28347862 0.28341287 0.28288513 0.2829952  0.28320816 0.28275195
 0.2825529  0.28250927 0.2824658  0.28325433 0.28376338 0.2836003
 0.28408575 0.28369838 0.2824343  0.28267092 0.28201637 0.2843954 ]
