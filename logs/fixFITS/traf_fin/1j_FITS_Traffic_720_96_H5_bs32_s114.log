Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j96_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j96_H5_FITS_custom_ftM_sl720_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1702208640.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2801662
	speed: 0.4231s/iter; left time: 3744.5313s
Epoch: 1 cost time: 74.93368434906006
Epoch: 1, Steps: 179 | Train Loss: 0.4174609 Vali Loss: 0.3441644 Test Loss: 0.4094111
Validation loss decreased (inf --> 0.344164).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2574889
	speed: 1.0946s/iter; left time: 9492.2785s
Epoch: 2 cost time: 66.0529272556305
Epoch: 2, Steps: 179 | Train Loss: 0.2418904 Vali Loss: 0.3366936 Test Loss: 0.4015551
Validation loss decreased (0.344164 --> 0.336694).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2317985
	speed: 0.9775s/iter; left time: 8301.7288s
Epoch: 3 cost time: 58.04170823097229
Epoch: 3, Steps: 179 | Train Loss: 0.2397675 Vali Loss: 0.3355746 Test Loss: 0.3998432
Validation loss decreased (0.336694 --> 0.335575).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2630422
	speed: 0.9428s/iter; left time: 7838.7170s
Epoch: 4 cost time: 62.29091215133667
Epoch: 4, Steps: 179 | Train Loss: 0.2392427 Vali Loss: 0.3339531 Test Loss: 0.3994856
Validation loss decreased (0.335575 --> 0.333953).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2475280
	speed: 1.0083s/iter; left time: 8202.2856s
Epoch: 5 cost time: 71.8921902179718
Epoch: 5, Steps: 179 | Train Loss: 0.2390437 Vali Loss: 0.3336802 Test Loss: 0.3994344
Validation loss decreased (0.333953 --> 0.333680).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2363306
	speed: 1.3042s/iter; left time: 10376.0420s
Epoch: 6 cost time: 82.97235703468323
Epoch: 6, Steps: 179 | Train Loss: 0.2387640 Vali Loss: 0.3349319 Test Loss: 0.3987499
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2252029
	speed: 1.2681s/iter; left time: 9862.0629s
Epoch: 7 cost time: 83.65939545631409
Epoch: 7, Steps: 179 | Train Loss: 0.2386354 Vali Loss: 0.3331634 Test Loss: 0.3986886
Validation loss decreased (0.333680 --> 0.333163).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2325577
	speed: 1.3431s/iter; left time: 10205.2528s
Epoch: 8 cost time: 86.89459180831909
Epoch: 8, Steps: 179 | Train Loss: 0.2385175 Vali Loss: 0.3343757 Test Loss: 0.3982627
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2401489
	speed: 1.3374s/iter; left time: 9922.4026s
Epoch: 9 cost time: 79.36210441589355
Epoch: 9, Steps: 179 | Train Loss: 0.2384485 Vali Loss: 0.3335699 Test Loss: 0.3979660
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2171632
	speed: 1.2567s/iter; left time: 9098.6215s
Epoch: 10 cost time: 77.46894669532776
Epoch: 10, Steps: 179 | Train Loss: 0.2383573 Vali Loss: 0.3328964 Test Loss: 0.3988983
Validation loss decreased (0.333163 --> 0.332896).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2443932
	speed: 1.2705s/iter; left time: 8970.8845s
Epoch: 11 cost time: 80.56830787658691
Epoch: 11, Steps: 179 | Train Loss: 0.2383381 Vali Loss: 0.3333887 Test Loss: 0.3981493
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2209363
	speed: 1.2531s/iter; left time: 8623.8876s
Epoch: 12 cost time: 74.87179780006409
Epoch: 12, Steps: 179 | Train Loss: 0.2382234 Vali Loss: 0.3331944 Test Loss: 0.3987709
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2217038
	speed: 1.0898s/iter; left time: 7304.7215s
Epoch: 13 cost time: 77.68370771408081
Epoch: 13, Steps: 179 | Train Loss: 0.2381621 Vali Loss: 0.3337389 Test Loss: 0.3980235
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j96_H5_FITS_custom_ftM_sl720_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.39867058396339417, mae:0.2855154275894165, rse:0.522830069065094, corr:[0.29063773 0.29594904 0.29565078 0.29645273 0.2969173  0.296282
 0.2954609  0.29536912 0.29597571 0.29663938 0.29696697 0.2970608
 0.2971212  0.29702976 0.29661205 0.29593515 0.29537895 0.2952945
 0.29557186 0.29583728 0.29583344 0.2955976  0.2954707  0.29610825
 0.2972688  0.2973705  0.29722574 0.29729503 0.29717132 0.29653957
 0.2956039  0.29489946 0.29487127 0.29543728 0.29604277 0.29628706
 0.2963137  0.2963784  0.29657084 0.29670647 0.29659453 0.29627725
 0.29600117 0.29596117 0.29603165 0.2959351  0.29565018 0.29556218
 0.29573756 0.29572043 0.29567018 0.2956511  0.29560158 0.2954543
 0.29519838 0.29492655 0.29480153 0.29488742 0.2950756  0.29525223
 0.2954866  0.29576725 0.2960558  0.29626724 0.2963486  0.29629686
 0.2961403  0.29600787 0.2960261  0.2960986  0.29588842 0.29529196
 0.29447097 0.2940139  0.29410395 0.29416984 0.29378277 0.29316387
 0.29295784 0.29356465 0.29463843 0.29539275 0.2954577  0.29523343
 0.29534966 0.29586443 0.29632547 0.2963322  0.2959795  0.29569805
 0.29566923 0.29547867 0.29458016 0.29329762 0.29324076 0.2956007 ]
