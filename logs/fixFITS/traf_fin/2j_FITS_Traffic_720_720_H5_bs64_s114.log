Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j720_H5_FITS_custom_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6007795200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 103.94122457504272
Epoch: 1, Steps: 84 | Train Loss: 1.2746090 Vali Loss: 1.2859462 Test Loss: 1.5187593
Validation loss decreased (inf --> 1.285946).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 96.73722386360168
Epoch: 2, Steps: 84 | Train Loss: 0.9219673 Vali Loss: 1.1038191 Test Loss: 1.2964489
Validation loss decreased (1.285946 --> 1.103819).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 85.00882697105408
Epoch: 3, Steps: 84 | Train Loss: 0.8041567 Vali Loss: 1.0281272 Test Loss: 1.2046467
Validation loss decreased (1.103819 --> 1.028127).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 84.54410243034363
Epoch: 4, Steps: 84 | Train Loss: 0.7355616 Vali Loss: 0.9700653 Test Loss: 1.1357127
Validation loss decreased (1.028127 --> 0.970065).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 95.18016624450684
Epoch: 5, Steps: 84 | Train Loss: 0.6823426 Vali Loss: 0.9240780 Test Loss: 1.0810505
Validation loss decreased (0.970065 --> 0.924078).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 83.83797812461853
Epoch: 6, Steps: 84 | Train Loss: 0.6376727 Vali Loss: 0.8845679 Test Loss: 1.0349827
Validation loss decreased (0.924078 --> 0.884568).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 92.09534025192261
Epoch: 7, Steps: 84 | Train Loss: 0.5992448 Vali Loss: 0.8469106 Test Loss: 0.9913256
Validation loss decreased (0.884568 --> 0.846911).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 87.75323629379272
Epoch: 8, Steps: 84 | Train Loss: 0.5656480 Vali Loss: 0.8176750 Test Loss: 0.9563208
Validation loss decreased (0.846911 --> 0.817675).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 77.43483591079712
Epoch: 9, Steps: 84 | Train Loss: 0.5359323 Vali Loss: 0.7888219 Test Loss: 0.9219910
Validation loss decreased (0.817675 --> 0.788822).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 80.60166263580322
Epoch: 10, Steps: 84 | Train Loss: 0.5096339 Vali Loss: 0.7627358 Test Loss: 0.8917353
Validation loss decreased (0.788822 --> 0.762736).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 88.13233089447021
Epoch: 11, Steps: 84 | Train Loss: 0.4861744 Vali Loss: 0.7385830 Test Loss: 0.8634793
Validation loss decreased (0.762736 --> 0.738583).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 95.06599283218384
Epoch: 12, Steps: 84 | Train Loss: 0.4650838 Vali Loss: 0.7172461 Test Loss: 0.8381754
Validation loss decreased (0.738583 --> 0.717246).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 101.7924690246582
Epoch: 13, Steps: 84 | Train Loss: 0.4460695 Vali Loss: 0.6988068 Test Loss: 0.8167355
Validation loss decreased (0.717246 --> 0.698807).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 106.84465265274048
Epoch: 14, Steps: 84 | Train Loss: 0.4288850 Vali Loss: 0.6807750 Test Loss: 0.7961821
Validation loss decreased (0.698807 --> 0.680775).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 102.77989435195923
Epoch: 15, Steps: 84 | Train Loss: 0.4133254 Vali Loss: 0.6650171 Test Loss: 0.7768031
Validation loss decreased (0.680775 --> 0.665017).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 105.29167914390564
Epoch: 16, Steps: 84 | Train Loss: 0.3991312 Vali Loss: 0.6502615 Test Loss: 0.7597717
Validation loss decreased (0.665017 --> 0.650262).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 97.2810754776001
Epoch: 17, Steps: 84 | Train Loss: 0.3861984 Vali Loss: 0.6362828 Test Loss: 0.7432514
Validation loss decreased (0.650262 --> 0.636283).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 98.6902801990509
Epoch: 18, Steps: 84 | Train Loss: 0.3743536 Vali Loss: 0.6243873 Test Loss: 0.7290478
Validation loss decreased (0.636283 --> 0.624387).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 108.36438059806824
Epoch: 19, Steps: 84 | Train Loss: 0.3635072 Vali Loss: 0.6120003 Test Loss: 0.7145913
Validation loss decreased (0.624387 --> 0.612000).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 103.88008856773376
Epoch: 20, Steps: 84 | Train Loss: 0.3536075 Vali Loss: 0.6023055 Test Loss: 0.7031062
Validation loss decreased (0.612000 --> 0.602305).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 96.24754810333252
Epoch: 21, Steps: 84 | Train Loss: 0.3443781 Vali Loss: 0.5916513 Test Loss: 0.6911692
Validation loss decreased (0.602305 --> 0.591651).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 106.89527010917664
Epoch: 22, Steps: 84 | Train Loss: 0.3360292 Vali Loss: 0.5829268 Test Loss: 0.6808698
Validation loss decreased (0.591651 --> 0.582927).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 87.26564121246338
Epoch: 23, Steps: 84 | Train Loss: 0.3281435 Vali Loss: 0.5742196 Test Loss: 0.6699666
Validation loss decreased (0.582927 --> 0.574220).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 81.90829658508301
Epoch: 24, Steps: 84 | Train Loss: 0.3210143 Vali Loss: 0.5666243 Test Loss: 0.6616222
Validation loss decreased (0.574220 --> 0.566624).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 81.1925721168518
Epoch: 25, Steps: 84 | Train Loss: 0.3142931 Vali Loss: 0.5590119 Test Loss: 0.6529164
Validation loss decreased (0.566624 --> 0.559012).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 92.307537317276
Epoch: 26, Steps: 84 | Train Loss: 0.3081423 Vali Loss: 0.5531362 Test Loss: 0.6452807
Validation loss decreased (0.559012 --> 0.553136).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 98.9626727104187
Epoch: 27, Steps: 84 | Train Loss: 0.3024376 Vali Loss: 0.5462213 Test Loss: 0.6378927
Validation loss decreased (0.553136 --> 0.546221).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 105.72625303268433
Epoch: 28, Steps: 84 | Train Loss: 0.2970715 Vali Loss: 0.5410520 Test Loss: 0.6313549
Validation loss decreased (0.546221 --> 0.541052).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 105.67191886901855
Epoch: 29, Steps: 84 | Train Loss: 0.2920780 Vali Loss: 0.5355654 Test Loss: 0.6247799
Validation loss decreased (0.541052 --> 0.535565).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 118.80816197395325
Epoch: 30, Steps: 84 | Train Loss: 0.2874672 Vali Loss: 0.5299083 Test Loss: 0.6186886
Validation loss decreased (0.535565 --> 0.529908).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 92.63594245910645
Epoch: 31, Steps: 84 | Train Loss: 0.2831339 Vali Loss: 0.5259929 Test Loss: 0.6135469
Validation loss decreased (0.529908 --> 0.525993).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 88.72332048416138
Epoch: 32, Steps: 84 | Train Loss: 0.2790515 Vali Loss: 0.5214338 Test Loss: 0.6082295
Validation loss decreased (0.525993 --> 0.521434).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 87.18924498558044
Epoch: 33, Steps: 84 | Train Loss: 0.2753277 Vali Loss: 0.5171602 Test Loss: 0.6032556
Validation loss decreased (0.521434 --> 0.517160).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 92.13773608207703
Epoch: 34, Steps: 84 | Train Loss: 0.2718079 Vali Loss: 0.5129781 Test Loss: 0.5988418
Validation loss decreased (0.517160 --> 0.512978).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 93.97538828849792
Epoch: 35, Steps: 84 | Train Loss: 0.2685112 Vali Loss: 0.5101772 Test Loss: 0.5946317
Validation loss decreased (0.512978 --> 0.510177).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 80.78941226005554
Epoch: 36, Steps: 84 | Train Loss: 0.2654158 Vali Loss: 0.5071388 Test Loss: 0.5907154
Validation loss decreased (0.510177 --> 0.507139).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 80.8412446975708
Epoch: 37, Steps: 84 | Train Loss: 0.2624412 Vali Loss: 0.5036631 Test Loss: 0.5871978
Validation loss decreased (0.507139 --> 0.503663).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 84.71668219566345
Epoch: 38, Steps: 84 | Train Loss: 0.2597550 Vali Loss: 0.5008965 Test Loss: 0.5834873
Validation loss decreased (0.503663 --> 0.500896).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 85.52411103248596
Epoch: 39, Steps: 84 | Train Loss: 0.2571591 Vali Loss: 0.4975352 Test Loss: 0.5802053
Validation loss decreased (0.500896 --> 0.497535).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 85.8701536655426
Epoch: 40, Steps: 84 | Train Loss: 0.2547208 Vali Loss: 0.4948326 Test Loss: 0.5770597
Validation loss decreased (0.497535 --> 0.494833).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 86.56111073493958
Epoch: 41, Steps: 84 | Train Loss: 0.2524576 Vali Loss: 0.4920458 Test Loss: 0.5739796
Validation loss decreased (0.494833 --> 0.492046).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 85.31632351875305
Epoch: 42, Steps: 84 | Train Loss: 0.2503168 Vali Loss: 0.4900777 Test Loss: 0.5713052
Validation loss decreased (0.492046 --> 0.490078).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 85.57595944404602
Epoch: 43, Steps: 84 | Train Loss: 0.2482449 Vali Loss: 0.4878999 Test Loss: 0.5689299
Validation loss decreased (0.490078 --> 0.487900).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 91.771080493927
Epoch: 44, Steps: 84 | Train Loss: 0.2464362 Vali Loss: 0.4856971 Test Loss: 0.5663470
Validation loss decreased (0.487900 --> 0.485697).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 99.136385679245
Epoch: 45, Steps: 84 | Train Loss: 0.2446376 Vali Loss: 0.4839188 Test Loss: 0.5640562
Validation loss decreased (0.485697 --> 0.483919).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 101.41901326179504
Epoch: 46, Steps: 84 | Train Loss: 0.2429472 Vali Loss: 0.4821156 Test Loss: 0.5617319
Validation loss decreased (0.483919 --> 0.482116).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 116.02982902526855
Epoch: 47, Steps: 84 | Train Loss: 0.2413177 Vali Loss: 0.4798170 Test Loss: 0.5597301
Validation loss decreased (0.482116 --> 0.479817).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 136.59666752815247
Epoch: 48, Steps: 84 | Train Loss: 0.2398321 Vali Loss: 0.4793212 Test Loss: 0.5579601
Validation loss decreased (0.479817 --> 0.479321).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 104.65655755996704
Epoch: 49, Steps: 84 | Train Loss: 0.2384134 Vali Loss: 0.4770520 Test Loss: 0.5560245
Validation loss decreased (0.479321 --> 0.477052).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 104.30170226097107
Epoch: 50, Steps: 84 | Train Loss: 0.2369967 Vali Loss: 0.4755812 Test Loss: 0.5543256
Validation loss decreased (0.477052 --> 0.475581).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6007795200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 106.812992811203
Epoch: 1, Steps: 84 | Train Loss: 0.3219864 Vali Loss: 0.4180062 Test Loss: 0.4866766
Validation loss decreased (inf --> 0.418006).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 107.35863280296326
Epoch: 2, Steps: 84 | Train Loss: 0.2890619 Vali Loss: 0.3982632 Test Loss: 0.4643906
Validation loss decreased (0.418006 --> 0.398263).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 88.93245720863342
Epoch: 3, Steps: 84 | Train Loss: 0.2784335 Vali Loss: 0.3932619 Test Loss: 0.4583926
Validation loss decreased (0.398263 --> 0.393262).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 94.67197608947754
Epoch: 4, Steps: 84 | Train Loss: 0.2757603 Vali Loss: 0.3927243 Test Loss: 0.4572764
Validation loss decreased (0.393262 --> 0.392724).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 119.47046303749084
Epoch: 5, Steps: 84 | Train Loss: 0.2751022 Vali Loss: 0.3923246 Test Loss: 0.4572574
Validation loss decreased (0.392724 --> 0.392325).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 109.62396478652954
Epoch: 6, Steps: 84 | Train Loss: 0.2749661 Vali Loss: 0.3921356 Test Loss: 0.4570523
Validation loss decreased (0.392325 --> 0.392136).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 120.4447910785675
Epoch: 7, Steps: 84 | Train Loss: 0.2748944 Vali Loss: 0.3920769 Test Loss: 0.4565751
Validation loss decreased (0.392136 --> 0.392077).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 115.61087989807129
Epoch: 8, Steps: 84 | Train Loss: 0.2747969 Vali Loss: 0.3921382 Test Loss: 0.4565415
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 113.36032152175903
Epoch: 9, Steps: 84 | Train Loss: 0.2748093 Vali Loss: 0.3921625 Test Loss: 0.4569044
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 111.3365466594696
Epoch: 10, Steps: 84 | Train Loss: 0.2746803 Vali Loss: 0.3921798 Test Loss: 0.4566630
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j720_H5_FITS_custom_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.45603248476982117, mae:0.3098515272140503, rse:0.5522028803825378, corr:[0.268787   0.26850837 0.26808727 0.27065945 0.27189195 0.27115485
 0.2704178  0.27078342 0.2714617  0.2713364  0.27049875 0.26993346
 0.2701555  0.27057293 0.2705279  0.2700978  0.2698627  0.2700366
 0.27020717 0.2700036  0.26968175 0.26972392 0.2703354  0.2715122
 0.27262032 0.27243382 0.27195984 0.2719757  0.27223343 0.2722252
 0.2717958  0.27121696 0.2708779  0.27087033 0.2709436  0.27089056
 0.27083287 0.2708802  0.27101535 0.27111295 0.27114666 0.27118826
 0.27126038 0.27130345 0.27122876 0.27098376 0.27071398 0.27084458
 0.27124864 0.27126834 0.27107045 0.2708643  0.2707862  0.27087104
 0.2709721  0.27091315 0.27071434 0.27053425 0.27044028 0.27035075
 0.27023375 0.27006823 0.27008522 0.27043402 0.2709644  0.2713146
 0.27122143 0.2708208  0.27047858 0.2704362  0.2705883  0.27076808
 0.27068827 0.2704668  0.27034536 0.2702684  0.2701546  0.27001265
 0.26989698 0.26979885 0.26964906 0.26948154 0.26931933 0.2693811
 0.26976612 0.2702293  0.27047455 0.27046427 0.27041495 0.2704248
 0.27058968 0.27054274 0.2703112  0.2701966  0.27033484 0.27062312
 0.2705905  0.27025536 0.26990256 0.26983985 0.27018902 0.2706363
 0.27069846 0.27020806 0.2695326  0.26928282 0.26949292 0.26987737
 0.2701315  0.27019894 0.2702565  0.27041665 0.27049676 0.27027223
 0.27013507 0.2701043  0.27024212 0.27033383 0.27028486 0.27005547
 0.26979765 0.26987097 0.2703018  0.27064797 0.27071232 0.27054673
 0.27036953 0.27026415 0.2702585  0.2703326  0.27036905 0.2704853
 0.27073497 0.27103648 0.2712384  0.2712518  0.2710555  0.2705195
 0.27046064 0.27052942 0.27065015 0.27084586 0.2710736  0.27131948
 0.27143994 0.27122936 0.27088076 0.27057746 0.27044144 0.27043703
 0.27045345 0.27041605 0.27032706 0.27036622 0.2704769  0.27058697
 0.27059147 0.27047464 0.27039057 0.2705065  0.2708069  0.27112442
 0.27136007 0.27144387 0.27147403 0.27151242 0.27155715 0.27195227
 0.27265832 0.2725691  0.27226803 0.27201912 0.27177107 0.27170682
 0.27194497 0.27231684 0.27245674 0.27218845 0.27170345 0.27142298
 0.27156368 0.27187026 0.2720225  0.2719135  0.27170187 0.27159458
 0.2715742  0.27165866 0.27183855 0.27204344 0.27228892 0.272758
 0.27318135 0.27288166 0.27244452 0.2723052  0.2724214  0.2726069
 0.27267352 0.27254874 0.2723097  0.27206278 0.27182782 0.27165693
 0.27163732 0.27173802 0.27192706 0.27207828 0.27204004 0.2717626
 0.2713645  0.27111503 0.27119637 0.27149892 0.27175128 0.2718928
 0.27186453 0.27162817 0.27148837 0.27151147 0.27155745 0.27154613
 0.27149495 0.27143836 0.2713566  0.27117884 0.27091303 0.27072304
 0.27080455 0.27105564 0.27130845 0.27149934 0.27168095 0.2718518
 0.27187416 0.27167422 0.2714809  0.2714308  0.2716634  0.27197966
 0.27195135 0.2714998  0.27102143 0.2708688  0.27105984 0.2712902
 0.2712894  0.27110064 0.27097803 0.27112353 0.2713402  0.27141023
 0.2713396  0.2712952  0.27142447 0.27170402 0.27197447 0.2720604
 0.27196795 0.27173963 0.27156803 0.27156767 0.27170378 0.2718526
 0.2717958  0.271614   0.27138144 0.2711119  0.27088645 0.27083132
 0.27085996 0.27088684 0.27087384 0.27094546 0.2709763  0.2709532
 0.27096772 0.2710778  0.27127683 0.27150372 0.27166435 0.27169865
 0.27185315 0.27188984 0.27175283 0.27152938 0.27133757 0.2711941
 0.2710147  0.2709866  0.27113    0.2712552  0.27130052 0.27131474
 0.27139887 0.27161747 0.27194455 0.272226   0.27226073 0.27209055
 0.2718491  0.27167037 0.27163288 0.2717222  0.2718632  0.2719044
 0.27212587 0.2722693  0.27228585 0.27212903 0.27183184 0.2715833
 0.2715245  0.27155977 0.27168113 0.27174637 0.2716967  0.2716286
 0.27169725 0.27193776 0.27221596 0.27240178 0.27242023 0.2723405
 0.27221918 0.27213806 0.272202   0.2724288  0.27269724 0.2727825
 0.2727185  0.27251294 0.27232838 0.27230957 0.27241766 0.27277932
 0.27322164 0.27300805 0.27274138 0.27276516 0.27289656 0.27297673
 0.2729808  0.27298695 0.2730073  0.27295333 0.27271482 0.27235594
 0.2720684  0.27198505 0.27210686 0.2722544  0.2723048  0.27231112
 0.27241284 0.27268225 0.2729934  0.27309126 0.27282065 0.27261025
 0.2726857  0.2725954  0.27257904 0.27267045 0.272652   0.27249268
 0.272341   0.2723015  0.2723377  0.27231485 0.27215132 0.27201447
 0.27209622 0.272421   0.2728484  0.2731667  0.2732348  0.2730693
 0.27277166 0.27255085 0.27259573 0.27284563 0.27307805 0.27320823
 0.2730813  0.27275452 0.2726696  0.27290908 0.2731694  0.27311495
 0.27270022 0.27218926 0.27187973 0.27181315 0.2718104  0.27177265
 0.27177602 0.27183136 0.27195552 0.27207944 0.27214992 0.27214688
 0.27202824 0.27179414 0.27152872 0.27140346 0.27150553 0.27184594
 0.2721288  0.27215275 0.27195397 0.27170163 0.27159643 0.27173117
 0.2719998  0.27215979 0.27211973 0.27197242 0.2717916  0.27171656
 0.27181932 0.27193972 0.27199444 0.27204797 0.27218187 0.27233332
 0.27239934 0.27218884 0.2718866  0.27178654 0.27184862 0.2720007
 0.27204233 0.27210656 0.27217263 0.2722033  0.2720792  0.271758
 0.27139023 0.27126318 0.2714874  0.2718796  0.27211148 0.27208367
 0.2720652  0.27227238 0.27255902 0.272556   0.27207598 0.27149224
 0.27147055 0.27179635 0.27198374 0.27164388 0.27101347 0.27062544
 0.27065885 0.27104503 0.27142903 0.2715069  0.2714412  0.2714531
 0.27152836 0.27151698 0.27137172 0.27121672 0.2712217  0.2714333
 0.27176455 0.27211916 0.27242047 0.27256188 0.27245116 0.2721657
 0.27193707 0.27186027 0.27198002 0.27213553 0.27213278 0.2720417
 0.27200684 0.27203655 0.27221066 0.27237883 0.27238435 0.27222776
 0.27205256 0.2719896  0.27203536 0.27208614 0.2721008  0.27213126
 0.27221918 0.2722957  0.27223334 0.27204898 0.27189225 0.27194974
 0.27222762 0.2725086  0.2725689  0.27234787 0.27207497 0.2722507
 0.27278754 0.27272862 0.2723719  0.2720422  0.2718663  0.27193066
 0.27209365 0.2721243  0.2719646  0.27181098 0.2718611  0.27212292
 0.2723154  0.27221897 0.27200305 0.27191782 0.27204812 0.27217275
 0.27202675 0.2717251  0.27157956 0.27172986 0.2720849  0.27258888
 0.27292666 0.27262753 0.2722441  0.2721076  0.2720778  0.27200833
 0.27191582 0.27189896 0.27196503 0.27200451 0.2718939  0.27168044
 0.27146924 0.27130702 0.27115184 0.270927   0.27067053 0.2705219
 0.27057493 0.27082017 0.27116    0.27144447 0.27153516 0.27158764
 0.2716386  0.27151665 0.27137828 0.27129292 0.2712156  0.2711498
 0.2711153  0.27110025 0.27108386 0.2710485  0.27094018 0.27075788
 0.27053636 0.2703031  0.2701998  0.27034235 0.27065554 0.27090302
 0.27085072 0.27057496 0.2704244  0.27064914 0.27108988 0.2713726
 0.27112883 0.2705078  0.27001354 0.2698844  0.27006775 0.27035752
 0.27057904 0.2707107  0.27081215 0.27085468 0.2706961  0.27034855
 0.27000454 0.26974437 0.2695556  0.26946244 0.2695916  0.2700152
 0.27048892 0.270636   0.27033728 0.2698755  0.26961404 0.26966235
 0.2696508  0.26946107 0.2692315  0.2691154  0.2692993  0.2696129
 0.26960343 0.26913196 0.26859194 0.26849672 0.2688923  0.26933023
 0.2693749  0.26902175 0.26863316 0.2685334  0.26875046 0.26909643
 0.26936197 0.2694514  0.26950812 0.26961482 0.26978537 0.26986927
 0.26975396 0.2696747  0.2696381  0.2694928  0.2693268  0.2692272
 0.26915017 0.26904628 0.2689428  0.26891658 0.26906422 0.2693826
 0.26969805 0.26981685 0.26974055 0.26963893 0.2696497  0.2698308
 0.2699882  0.26993486 0.26972958 0.2695856  0.2696858  0.27005705
 0.27041978 0.27043208 0.2703068  0.2703895  0.27081007 0.27123296
 0.27117658 0.27056542 0.2698207  0.26945314 0.26959246 0.26990128
 0.26996824 0.26974055 0.2695097  0.26951146 0.26970255 0.26986593
 0.26995263 0.27015588 0.27050442 0.27077916 0.27077135 0.27084514
 0.27114746 0.27112934 0.27128178 0.27167156 0.2719221  0.2718802
 0.27157953 0.2710148  0.2701811  0.26932907 0.26895452 0.26936558
 0.27017555 0.2705883  0.27033976 0.26989362 0.26993957 0.27052993
 0.27109003 0.27125186 0.27124482 0.2713993  0.27167562 0.27192584
 0.27184382 0.27119043 0.27094895 0.2714862  0.27203834 0.27182612
 0.27094838 0.27020627 0.27013683 0.2703888  0.27024958 0.2697846
 0.26987994 0.2708822  0.27202073 0.2721228  0.27131808 0.27101326
 0.27191705 0.27246284 0.27037594 0.26670653 0.2670197  0.27260104]
