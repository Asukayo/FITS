Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11298406400.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8640679
	speed: 0.6615s/iter; left time: 5524.2610s
Epoch: 1 cost time: 112.28458499908447
Epoch: 1, Steps: 169 | Train Loss: 0.9917500 Vali Loss: 1.0244862 Test Loss: 1.2033013
Validation loss decreased (inf --> 1.024486).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6687857
	speed: 1.7963s/iter; left time: 14697.5905s
Epoch: 2 cost time: 108.96500635147095
Epoch: 2, Steps: 169 | Train Loss: 0.6899602 Vali Loss: 0.8952539 Test Loss: 1.0482150
Validation loss decreased (1.024486 --> 0.895254).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5657064
	speed: 1.8709s/iter; left time: 14991.1360s
Epoch: 3 cost time: 116.6580593585968
Epoch: 3, Steps: 169 | Train Loss: 0.5757174 Vali Loss: 0.8041573 Test Loss: 0.9412349
Validation loss decreased (0.895254 --> 0.804157).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4847315
	speed: 1.8218s/iter; left time: 14289.8567s
Epoch: 4 cost time: 113.78344678878784
Epoch: 4, Steps: 169 | Train Loss: 0.4922328 Vali Loss: 0.7320426 Test Loss: 0.8555416
Validation loss decreased (0.804157 --> 0.732043).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4262914
	speed: 1.7981s/iter; left time: 13800.3613s
Epoch: 5 cost time: 120.00133538246155
Epoch: 5, Steps: 169 | Train Loss: 0.4275087 Vali Loss: 0.6714419 Test Loss: 0.7852163
Validation loss decreased (0.732043 --> 0.671442).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3693901
	speed: 1.9729s/iter; left time: 14808.3389s
Epoch: 6 cost time: 117.72677659988403
Epoch: 6, Steps: 169 | Train Loss: 0.3760365 Vali Loss: 0.6219634 Test Loss: 0.7273430
Validation loss decreased (0.671442 --> 0.621963).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3284102
	speed: 1.8244s/iter; left time: 13385.7513s
Epoch: 7 cost time: 113.7180528640747
Epoch: 7, Steps: 169 | Train Loss: 0.3344335 Vali Loss: 0.5824116 Test Loss: 0.6812554
Validation loss decreased (0.621963 --> 0.582412).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2959648
	speed: 1.8849s/iter; left time: 13511.0272s
Epoch: 8 cost time: 124.97399258613586
Epoch: 8, Steps: 169 | Train Loss: 0.3005236 Vali Loss: 0.5475376 Test Loss: 0.6399079
Validation loss decreased (0.582412 --> 0.547538).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2715134
	speed: 2.0559s/iter; left time: 14389.5664s
Epoch: 9 cost time: 120.48371291160583
Epoch: 9, Steps: 169 | Train Loss: 0.2726850 Vali Loss: 0.5202272 Test Loss: 0.6066435
Validation loss decreased (0.547538 --> 0.520227).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2467030
	speed: 1.8817s/iter; left time: 12851.7112s
Epoch: 10 cost time: 120.57171106338501
Epoch: 10, Steps: 169 | Train Loss: 0.2496965 Vali Loss: 0.4974373 Test Loss: 0.5804945
Validation loss decreased (0.520227 --> 0.497437).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2255563
	speed: 1.8841s/iter; left time: 12549.9685s
Epoch: 11 cost time: 123.3496880531311
Epoch: 11, Steps: 169 | Train Loss: 0.2306892 Vali Loss: 0.4777984 Test Loss: 0.5574303
Validation loss decreased (0.497437 --> 0.477798).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2140172
	speed: 2.0154s/iter; left time: 13084.1679s
Epoch: 12 cost time: 118.0392816066742
Epoch: 12, Steps: 169 | Train Loss: 0.2148763 Vali Loss: 0.4615790 Test Loss: 0.5376835
Validation loss decreased (0.477798 --> 0.461579).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2008518
	speed: 1.8426s/iter; left time: 11650.9917s
Epoch: 13 cost time: 112.74116349220276
Epoch: 13, Steps: 169 | Train Loss: 0.2017399 Vali Loss: 0.4476308 Test Loss: 0.5224875
Validation loss decreased (0.461579 --> 0.447631).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1920718
	speed: 1.8041s/iter; left time: 11102.4831s
Epoch: 14 cost time: 112.38635969161987
Epoch: 14, Steps: 169 | Train Loss: 0.1908274 Vali Loss: 0.4361964 Test Loss: 0.5088715
Validation loss decreased (0.447631 --> 0.436196).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1835805
	speed: 1.9016s/iter; left time: 11380.8212s
Epoch: 15 cost time: 118.94401001930237
Epoch: 15, Steps: 169 | Train Loss: 0.1817579 Vali Loss: 0.4273986 Test Loss: 0.4979149
Validation loss decreased (0.436196 --> 0.427399).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1757410
	speed: 1.9333s/iter; left time: 11243.8994s
Epoch: 16 cost time: 114.15564155578613
Epoch: 16, Steps: 169 | Train Loss: 0.1741523 Vali Loss: 0.4197985 Test Loss: 0.4900048
Validation loss decreased (0.427399 --> 0.419799).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1664947
	speed: 1.8925s/iter; left time: 10686.6812s
Epoch: 17 cost time: 117.38911867141724
Epoch: 17, Steps: 169 | Train Loss: 0.1678552 Vali Loss: 0.4136485 Test Loss: 0.4815297
Validation loss decreased (0.419799 --> 0.413649).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1638820
	speed: 1.8528s/iter; left time: 10149.8208s
Epoch: 18 cost time: 119.2680504322052
Epoch: 18, Steps: 169 | Train Loss: 0.1626000 Vali Loss: 0.4081287 Test Loss: 0.4761291
Validation loss decreased (0.413649 --> 0.408129).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1585695
	speed: 1.9129s/iter; left time: 10155.8058s
Epoch: 19 cost time: 119.07434248924255
Epoch: 19, Steps: 169 | Train Loss: 0.1582723 Vali Loss: 0.4040175 Test Loss: 0.4705827
Validation loss decreased (0.408129 --> 0.404018).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1545467
	speed: 1.9549s/iter; left time: 10048.4292s
Epoch: 20 cost time: 118.00147747993469
Epoch: 20, Steps: 169 | Train Loss: 0.1546526 Vali Loss: 0.4006123 Test Loss: 0.4669205
Validation loss decreased (0.404018 --> 0.400612).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1506705
	speed: 1.8906s/iter; left time: 9398.0315s
Epoch: 21 cost time: 116.94776606559753
Epoch: 21, Steps: 169 | Train Loss: 0.1516649 Vali Loss: 0.3977249 Test Loss: 0.4637106
Validation loss decreased (0.400612 --> 0.397725).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1470854
	speed: 1.8862s/iter; left time: 9057.7151s
Epoch: 22 cost time: 117.07879638671875
Epoch: 22, Steps: 169 | Train Loss: 0.1491653 Vali Loss: 0.3957871 Test Loss: 0.4610346
Validation loss decreased (0.397725 --> 0.395787).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1482029
	speed: 1.9193s/iter; left time: 8891.9947s
Epoch: 23 cost time: 117.93294429779053
Epoch: 23, Steps: 169 | Train Loss: 0.1471191 Vali Loss: 0.3933846 Test Loss: 0.4587266
Validation loss decreased (0.395787 --> 0.393385).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1485153
	speed: 1.8737s/iter; left time: 8364.0989s
Epoch: 24 cost time: 114.12688946723938
Epoch: 24, Steps: 169 | Train Loss: 0.1454590 Vali Loss: 0.3908355 Test Loss: 0.4564461
Validation loss decreased (0.393385 --> 0.390835).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1454246
	speed: 1.7913s/iter; left time: 7693.5856s
Epoch: 25 cost time: 117.05083417892456
Epoch: 25, Steps: 169 | Train Loss: 0.1440590 Vali Loss: 0.3903098 Test Loss: 0.4556258
Validation loss decreased (0.390835 --> 0.390310).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1435775
	speed: 1.9202s/iter; left time: 7922.9288s
Epoch: 26 cost time: 112.15817546844482
Epoch: 26, Steps: 169 | Train Loss: 0.1429357 Vali Loss: 0.3893516 Test Loss: 0.4542125
Validation loss decreased (0.390310 --> 0.389352).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1487992
	speed: 1.7771s/iter; left time: 7031.7965s
Epoch: 27 cost time: 107.54338765144348
Epoch: 27, Steps: 169 | Train Loss: 0.1420057 Vali Loss: 0.3883697 Test Loss: 0.4530959
Validation loss decreased (0.389352 --> 0.388370).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1397530
	speed: 1.8908s/iter; left time: 7162.4281s
Epoch: 28 cost time: 122.31613683700562
Epoch: 28, Steps: 169 | Train Loss: 0.1412657 Vali Loss: 0.3877050 Test Loss: 0.4525058
Validation loss decreased (0.388370 --> 0.387705).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1422001
	speed: 1.9323s/iter; left time: 6993.1293s
Epoch: 29 cost time: 113.0713300704956
Epoch: 29, Steps: 169 | Train Loss: 0.1406478 Vali Loss: 0.3873275 Test Loss: 0.4515139
Validation loss decreased (0.387705 --> 0.387328).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1401802
	speed: 1.8217s/iter; left time: 6285.0087s
Epoch: 30 cost time: 115.21230864524841
Epoch: 30, Steps: 169 | Train Loss: 0.1401574 Vali Loss: 0.3867984 Test Loss: 0.4511611
Validation loss decreased (0.387328 --> 0.386798).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1396673
	speed: 1.8648s/iter; left time: 6118.4688s
Epoch: 31 cost time: 116.60561656951904
Epoch: 31, Steps: 169 | Train Loss: 0.1397684 Vali Loss: 0.3869978 Test Loss: 0.4510286
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1373551
	speed: 1.9052s/iter; left time: 5928.8436s
Epoch: 32 cost time: 119.4938097000122
Epoch: 32, Steps: 169 | Train Loss: 0.1394670 Vali Loss: 0.3863321 Test Loss: 0.4506153
Validation loss decreased (0.386798 --> 0.386332).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1381062
	speed: 1.8533s/iter; left time: 5454.1361s
Epoch: 33 cost time: 113.14425659179688
Epoch: 33, Steps: 169 | Train Loss: 0.1392306 Vali Loss: 0.3864527 Test Loss: 0.4503709
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1386083
	speed: 1.7936s/iter; left time: 4975.3345s
Epoch: 34 cost time: 114.80014085769653
Epoch: 34, Steps: 169 | Train Loss: 0.1390310 Vali Loss: 0.3867099 Test Loss: 0.4505962
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1394936
	speed: 1.7945s/iter; left time: 4674.5669s
Epoch: 35 cost time: 111.61274862289429
Epoch: 35, Steps: 169 | Train Loss: 0.1388310 Vali Loss: 0.3857123 Test Loss: 0.4504583
Validation loss decreased (0.386332 --> 0.385712).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1375713
	speed: 1.8557s/iter; left time: 4520.5559s
Epoch: 36 cost time: 106.17773914337158
Epoch: 36, Steps: 169 | Train Loss: 0.1387335 Vali Loss: 0.3856151 Test Loss: 0.4500237
Validation loss decreased (0.385712 --> 0.385615).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1480217
	speed: 1.8901s/iter; left time: 4284.8638s
Epoch: 37 cost time: 123.92409563064575
Epoch: 37, Steps: 169 | Train Loss: 0.1386439 Vali Loss: 0.3860990 Test Loss: 0.4503564
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1361410
	speed: 1.9136s/iter; left time: 4014.6358s
Epoch: 38 cost time: 113.62156057357788
Epoch: 38, Steps: 169 | Train Loss: 0.1385749 Vali Loss: 0.3853492 Test Loss: 0.4500299
Validation loss decreased (0.385615 --> 0.385349).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1381770
	speed: 1.8594s/iter; left time: 3586.7516s
Epoch: 39 cost time: 112.63368630409241
Epoch: 39, Steps: 169 | Train Loss: 0.1384898 Vali Loss: 0.3858404 Test Loss: 0.4502483
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1383870
	speed: 1.8791s/iter; left time: 3307.2853s
Epoch: 40 cost time: 113.27167296409607
Epoch: 40, Steps: 169 | Train Loss: 0.1384591 Vali Loss: 0.3852001 Test Loss: 0.4502672
Validation loss decreased (0.385349 --> 0.385200).  Saving model ...
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1430159
	speed: 1.9158s/iter; left time: 3048.0858s
Epoch: 41 cost time: 117.96099138259888
Epoch: 41, Steps: 169 | Train Loss: 0.1384458 Vali Loss: 0.3860677 Test Loss: 0.4500096
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1450221
	speed: 1.9242s/iter; left time: 2736.2325s
Epoch: 42 cost time: 121.73111915588379
Epoch: 42, Steps: 169 | Train Loss: 0.1384028 Vali Loss: 0.3852493 Test Loss: 0.4497844
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1408245
	speed: 1.8200s/iter; left time: 2280.4737s
Epoch: 43 cost time: 107.91006112098694
Epoch: 43, Steps: 169 | Train Loss: 0.1383788 Vali Loss: 0.3860310 Test Loss: 0.4498438
EarlyStopping counter: 3 out of 3
Early stopping
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11298406400.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2676593
	speed: 0.6585s/iter; left time: 5499.1221s
Epoch: 1 cost time: 109.26317715644836
Epoch: 1, Steps: 169 | Train Loss: 0.2710751 Vali Loss: 0.3851498 Test Loss: 0.4499058
Validation loss decreased (inf --> 0.385150).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2606755
	speed: 1.6902s/iter; left time: 13829.1302s
Epoch: 2 cost time: 103.45668697357178
Epoch: 2, Steps: 169 | Train Loss: 0.2708552 Vali Loss: 0.3858998 Test Loss: 0.4512110
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2729749
	speed: 1.7592s/iter; left time: 14096.1692s
Epoch: 3 cost time: 108.71110963821411
Epoch: 3, Steps: 169 | Train Loss: 0.2707147 Vali Loss: 0.3863502 Test Loss: 0.4509552
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2705323
	speed: 1.8712s/iter; left time: 14678.0046s
Epoch: 4 cost time: 123.82856941223145
Epoch: 4, Steps: 169 | Train Loss: 0.2706351 Vali Loss: 0.3864798 Test Loss: 0.4515169
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4493626356124878, mae:0.300613135099411, rse:0.548149824142456, corr:[0.25636187 0.2682465  0.26943123 0.26861295 0.26821023 0.26834574
 0.2676794  0.26752087 0.26725048 0.26746804 0.26726186 0.26697314
 0.26738942 0.26709867 0.26651824 0.26653615 0.2669208  0.26729834
 0.26730934 0.26736918 0.26746148 0.26759136 0.26773772 0.26795927
 0.26931658 0.2690658  0.26785377 0.26715848 0.26693454 0.26785013
 0.26872402 0.26858726 0.26844972 0.2683873  0.26853433 0.26800916
 0.26711082 0.2667134  0.26634312 0.2664364  0.26670885 0.26690874
 0.26657945 0.26640603 0.26741856 0.26752153 0.26710072 0.26735497
 0.268005   0.2684003  0.2681968  0.2680862  0.2681958  0.2676006
 0.26740617 0.2675645  0.26728562 0.2669563  0.26704383 0.26767552
 0.26740664 0.26686293 0.2678387  0.26835743 0.2680349  0.26779032
 0.2672718  0.26672655 0.26677325 0.2672841  0.26728484 0.26726055
 0.26763892 0.26764387 0.26728103 0.26707634 0.2670709  0.267348
 0.26741624 0.26715407 0.2674161  0.26765248 0.26717034 0.26697844
 0.26741362 0.26776972 0.2676424  0.26711214 0.26704404 0.2672333
 0.26744726 0.2672732  0.26654756 0.26675296 0.2669469  0.26671478
 0.267001   0.267039   0.2672743  0.26790518 0.26753423 0.26716828
 0.26715603 0.26678675 0.26654828 0.266739   0.26695302 0.26729646
 0.26747453 0.26712    0.26711285 0.26727855 0.26720998 0.267492
 0.267367   0.2667703  0.2670501  0.2672248  0.26729962 0.2671884
 0.26708037 0.2677027  0.26753828 0.26694348 0.2666169  0.2665913
 0.26678333 0.26675522 0.26716226 0.2670367  0.26668185 0.26738283
 0.26789987 0.26832855 0.26841804 0.26767123 0.2671076  0.26680365
 0.26739493 0.2675912  0.26718038 0.26696727 0.26710352 0.26722017
 0.2676283  0.26844752 0.26872948 0.26782647 0.26735857 0.2676496
 0.2676184  0.26790145 0.26802096 0.26773924 0.2680616  0.26846692
 0.26868218 0.26868135 0.2681416  0.26749733 0.26728055 0.26785275
 0.26810068 0.26782528 0.26826483 0.2685311  0.26837513 0.26823303
 0.26887956 0.26917768 0.26943818 0.2691337  0.26951978 0.2699015
 0.26916417 0.2692533  0.26939937 0.2688369  0.26889843 0.26877725
 0.26880428 0.26897228 0.26833364 0.26856905 0.2696395  0.26968858
 0.26935044 0.2695386  0.26936004 0.26917696 0.26985112 0.26986653
 0.27004874 0.27030632 0.26965472 0.26912388 0.2691448  0.26897645
 0.2687625  0.26832026 0.26820388 0.26839897 0.26853466 0.26866502
 0.26833043 0.26805896 0.2682468  0.26863205 0.26931491 0.26970977
 0.2693591  0.268886   0.26879317 0.26880985 0.26912796 0.26969907
 0.26988629 0.26964045 0.26910406 0.26865256 0.26824018 0.26759937
 0.26797205 0.26887488 0.2689916  0.2687959  0.26856863 0.26839635
 0.26816258 0.26818973 0.26884672 0.26929253 0.26971012 0.27007556
 0.26961428 0.26863015 0.26821026 0.2689072  0.2693663  0.26932523
 0.26939443 0.2686266  0.26788378 0.26799384 0.267824   0.26801512
 0.26830423 0.26756737 0.26729852 0.26790425 0.26825935 0.26803532
 0.26776978 0.26822338 0.2687707  0.26886117 0.26899526 0.2690319
 0.2689973  0.2689284  0.26848635 0.2687612  0.26947448 0.2690185
 0.26875687 0.26955077 0.26982665 0.2694469  0.26890483 0.26901606
 0.2694647  0.26957485 0.26924092 0.2687172  0.26823628 0.26804364
 0.26792523 0.26805234 0.26840773 0.26833275 0.26844645 0.26877233
 0.26908654 0.2690486  0.2685229  0.26848462 0.26894808 0.26896787
 0.26861966 0.2687088  0.26923162 0.2694161  0.26876825 0.26874533
 0.26945576 0.26887864 0.26836497 0.26913813 0.26946065 0.2694296
 0.2694617  0.26950663 0.27030262 0.27097625 0.2704437  0.27017257
 0.27069592 0.27005732 0.26906195 0.26880428 0.2688463  0.26900202
 0.2692071  0.26932365 0.26986504 0.2700127  0.27028638 0.270964
 0.27015662 0.26959702 0.26996964 0.2695651  0.26970854 0.2700828
 0.2701374  0.27074453 0.27063414 0.27021742 0.2704306  0.2704632
 0.27059752 0.27053618 0.26982182 0.26919088 0.26877406 0.268317
 0.26926047 0.27037483 0.27115512 0.27101558 0.27005282 0.2700849
 0.27002302 0.26937693 0.2698806  0.27027923 0.26988763 0.26976353
 0.27034542 0.27028045 0.26950228 0.27000412 0.2707493  0.27109268
 0.27145737 0.2710628  0.27041835 0.26994362 0.2700563  0.27075785
 0.27156997 0.27174997 0.27141297 0.27114588 0.27133366 0.2708835
 0.27061543 0.2699789  0.26913846 0.26994953 0.2706297  0.27031896
 0.27016866 0.2698253  0.26936716 0.2694248  0.26977146 0.26969042
 0.26978981 0.27096844 0.27129808 0.2703255  0.27023822 0.27069822
 0.27080792 0.27098122 0.27081767 0.27028158 0.2699733  0.2696454
 0.26972014 0.2701823  0.27038273 0.27019793 0.2702539  0.2705302
 0.27002233 0.26914635 0.26925793 0.26989174 0.27010015 0.27007347
 0.2699828  0.26993433 0.26966727 0.26929674 0.26953065 0.2699174
 0.26986885 0.26972604 0.26927227 0.26897642 0.26882097 0.26829216
 0.26877144 0.26920733 0.26854452 0.2689131  0.26968813 0.26953855
 0.2688012  0.268087   0.26798138 0.26817325 0.26860568 0.26903152
 0.26943377 0.2697221  0.27003017 0.2703247  0.26964322 0.26914388
 0.26982266 0.2702053  0.2698693  0.2696893  0.2693356  0.2682961
 0.26814774 0.26908067 0.26908073 0.26915216 0.27001554 0.2699717
 0.2694221  0.269062   0.26864585 0.2682023  0.268575   0.2695284
 0.26976794 0.2695786  0.26947653 0.26916108 0.26905343 0.26876354
 0.268424   0.26857892 0.2686985  0.26867092 0.2683322  0.26823097
 0.26854563 0.26846686 0.2681174  0.2686158  0.26927498 0.26985687
 0.270333   0.2699884  0.2700783  0.27014092 0.26943138 0.26975474
 0.2704778  0.27037266 0.270024   0.2697265  0.26992768 0.27015102
 0.26980487 0.26970866 0.26974773 0.26949194 0.2698405  0.27013612
 0.26922825 0.268663   0.26876795 0.26838034 0.26849958 0.26911232
 0.26968047 0.26996198 0.26934868 0.2694922  0.27020603 0.27033168
 0.2705577  0.27041993 0.2702545  0.27017143 0.26937872 0.26900256
 0.26999423 0.2707238  0.27057537 0.26968083 0.26987478 0.27065894
 0.27010798 0.2693087  0.26923937 0.269714   0.2695262  0.26891258
 0.26974586 0.27044854 0.27023396 0.27020493 0.26994488 0.26966414
 0.26956707 0.26989737 0.26992455 0.2691504  0.26923868 0.26988673
 0.27029663 0.270354   0.2699773  0.26955366 0.26916075 0.26898882
 0.26957864 0.2696426  0.26890332 0.2690612  0.2697373  0.26998544
 0.27022955 0.26986012 0.26952606 0.270378   0.27015555 0.26883078
 0.26861972 0.2690468  0.26905748 0.26879016 0.26934788 0.26994422
 0.2694209  0.2697372  0.27037022 0.26983443 0.26933178 0.26883537
 0.2688074  0.2689203  0.2682829  0.26846474 0.26867715 0.26813114
 0.26817656 0.26832578 0.26920143 0.27042574 0.27041277 0.27019417
 0.26932463 0.2681783  0.26858374 0.26905403 0.26893145 0.26905692
 0.26896393 0.26877847 0.2685124  0.2681683  0.26805243 0.26729912
 0.26705018 0.26771602 0.26766312 0.26759788 0.26772135 0.2673393
 0.2675083  0.26828393 0.2679309  0.26739827 0.2679451  0.26785958
 0.26724884 0.266682   0.26657388 0.26748073 0.2675508  0.26731387
 0.26752788 0.26763794 0.26778305 0.2683019  0.26833287 0.26765868
 0.26750803 0.26760125 0.26694292 0.26684162 0.26737157 0.26728407
 0.26736853 0.2678085  0.26734063 0.2667191  0.26744017 0.26872054
 0.2689683  0.26816696 0.26738602 0.2675303  0.26777047 0.26751748
 0.2676825  0.26763445 0.2675624  0.26808247 0.2684627  0.26877174
 0.2687244  0.26816493 0.2674153  0.2669927  0.26709643 0.26754957
 0.26772287 0.26723784 0.26685688 0.26675612 0.2672203  0.26793355
 0.26788682 0.26816    0.2689079  0.26845717 0.26733425 0.26723137
 0.26752034 0.26757476 0.26775897 0.2682425  0.26908296 0.26920396
 0.26877904 0.2679751  0.2668866  0.26654154 0.26699075 0.26787865
 0.26884392 0.26845118 0.267246   0.2671194  0.2672771  0.26738402
 0.26766554 0.26753247 0.26721743 0.26744065 0.2676308  0.26765373
 0.26891497 0.26901713 0.26835442 0.2682873  0.26916364 0.26993048
 0.2690994  0.26875252 0.2684862  0.2673727  0.26768178 0.2682211
 0.26838484 0.26868927 0.26808363 0.26871443 0.27010402 0.27017364
 0.2698668  0.2690038  0.26875997 0.26917833 0.26893663 0.26893327
 0.2695821  0.26948696 0.26908684 0.26923758 0.26927966 0.2692606
 0.2692499  0.26864746 0.26810434 0.26761803 0.26795742 0.26894978
 0.26939946 0.26871827 0.2688778  0.27030486 0.27078354 0.27036104
 0.26953697 0.26857373 0.26776972 0.2684048  0.26898196 0.26827115]
