Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j720_H5_FITS_custom_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6007795200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 92.93596196174622
Epoch: 1, Steps: 84 | Train Loss: 1.0968284 Vali Loss: 1.0028503 Test Loss: 1.1773057
Validation loss decreased (inf --> 1.002850).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 100.07778334617615
Epoch: 2, Steps: 84 | Train Loss: 0.7157260 Vali Loss: 0.7921734 Test Loss: 0.9238139
Validation loss decreased (1.002850 --> 0.792173).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 94.32482290267944
Epoch: 3, Steps: 84 | Train Loss: 0.5710338 Vali Loss: 0.6653908 Test Loss: 0.7734364
Validation loss decreased (0.792173 --> 0.665391).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 102.38153648376465
Epoch: 4, Steps: 84 | Train Loss: 0.4773099 Vali Loss: 0.5791316 Test Loss: 0.6723266
Validation loss decreased (0.665391 --> 0.579132).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 102.33142232894897
Epoch: 5, Steps: 84 | Train Loss: 0.4135288 Vali Loss: 0.5201877 Test Loss: 0.6033170
Validation loss decreased (0.579132 --> 0.520188).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 104.54515051841736
Epoch: 6, Steps: 84 | Train Loss: 0.3697530 Vali Loss: 0.4792625 Test Loss: 0.5558955
Validation loss decreased (0.520188 --> 0.479262).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 100.07618427276611
Epoch: 7, Steps: 84 | Train Loss: 0.3396431 Vali Loss: 0.4513948 Test Loss: 0.5239265
Validation loss decreased (0.479262 --> 0.451395).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 106.16612005233765
Epoch: 8, Steps: 84 | Train Loss: 0.3190126 Vali Loss: 0.4325824 Test Loss: 0.5013326
Validation loss decreased (0.451395 --> 0.432582).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 108.15352988243103
Epoch: 9, Steps: 84 | Train Loss: 0.3049915 Vali Loss: 0.4201723 Test Loss: 0.4865090
Validation loss decreased (0.432582 --> 0.420172).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 120.73239803314209
Epoch: 10, Steps: 84 | Train Loss: 0.2954815 Vali Loss: 0.4117723 Test Loss: 0.4774310
Validation loss decreased (0.420172 --> 0.411772).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 146.42201018333435
Epoch: 11, Steps: 84 | Train Loss: 0.2891209 Vali Loss: 0.4058941 Test Loss: 0.4707763
Validation loss decreased (0.411772 --> 0.405894).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 114.02117371559143
Epoch: 12, Steps: 84 | Train Loss: 0.2847517 Vali Loss: 0.4019967 Test Loss: 0.4662594
Validation loss decreased (0.405894 --> 0.401997).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 117.51622271537781
Epoch: 13, Steps: 84 | Train Loss: 0.2818370 Vali Loss: 0.3990085 Test Loss: 0.4632058
Validation loss decreased (0.401997 --> 0.399009).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 91.85278558731079
Epoch: 14, Steps: 84 | Train Loss: 0.2799064 Vali Loss: 0.3974691 Test Loss: 0.4620546
Validation loss decreased (0.399009 --> 0.397469).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 103.15173745155334
Epoch: 15, Steps: 84 | Train Loss: 0.2785472 Vali Loss: 0.3963264 Test Loss: 0.4601982
Validation loss decreased (0.397469 --> 0.396326).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 114.26915955543518
Epoch: 16, Steps: 84 | Train Loss: 0.2776262 Vali Loss: 0.3955747 Test Loss: 0.4595741
Validation loss decreased (0.396326 --> 0.395575).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 121.87921476364136
Epoch: 17, Steps: 84 | Train Loss: 0.2770350 Vali Loss: 0.3950520 Test Loss: 0.4589937
Validation loss decreased (0.395575 --> 0.395052).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 113.14758801460266
Epoch: 18, Steps: 84 | Train Loss: 0.2765120 Vali Loss: 0.3950104 Test Loss: 0.4585905
Validation loss decreased (0.395052 --> 0.395010).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 144.08502411842346
Epoch: 19, Steps: 84 | Train Loss: 0.2761983 Vali Loss: 0.3946204 Test Loss: 0.4583875
Validation loss decreased (0.395010 --> 0.394620).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 169.82378697395325
Epoch: 20, Steps: 84 | Train Loss: 0.2759825 Vali Loss: 0.3944184 Test Loss: 0.4582904
Validation loss decreased (0.394620 --> 0.394418).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 127.61772537231445
Epoch: 21, Steps: 84 | Train Loss: 0.2757039 Vali Loss: 0.3936160 Test Loss: 0.4579792
Validation loss decreased (0.394418 --> 0.393616).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 126.90581965446472
Epoch: 22, Steps: 84 | Train Loss: 0.2757250 Vali Loss: 0.3935120 Test Loss: 0.4577747
Validation loss decreased (0.393616 --> 0.393512).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 115.88592314720154
Epoch: 23, Steps: 84 | Train Loss: 0.2754166 Vali Loss: 0.3938124 Test Loss: 0.4576145
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 101.65203928947449
Epoch: 24, Steps: 84 | Train Loss: 0.2754924 Vali Loss: 0.3932443 Test Loss: 0.4576263
Validation loss decreased (0.393512 --> 0.393244).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 108.70238518714905
Epoch: 25, Steps: 84 | Train Loss: 0.2753036 Vali Loss: 0.3929619 Test Loss: 0.4574663
Validation loss decreased (0.393244 --> 0.392962).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 117.28377556800842
Epoch: 26, Steps: 84 | Train Loss: 0.2753024 Vali Loss: 0.3934320 Test Loss: 0.4575698
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 124.19003081321716
Epoch: 27, Steps: 84 | Train Loss: 0.2753041 Vali Loss: 0.3929705 Test Loss: 0.4576306
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 118.68273067474365
Epoch: 28, Steps: 84 | Train Loss: 0.2751929 Vali Loss: 0.3931662 Test Loss: 0.4576488
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j720_H5_FITS_custom_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.45692089200019836, mae:0.3112574517726898, rse:0.5527405142784119, corr:[0.26892436 0.26322025 0.26073024 0.26845768 0.27393928 0.2741678
 0.27270946 0.27266553 0.27388266 0.27421877 0.2724745  0.26989642
 0.2686698  0.2691427  0.2696565  0.2689435  0.26777947 0.26775435
 0.26901954 0.27020237 0.27035853 0.26997042 0.27023256 0.2718497
 0.27381262 0.27401164 0.2732499  0.27284834 0.27311167 0.27336168
 0.2728164  0.27158573 0.27064055 0.2706213  0.2709902  0.27077794
 0.26989934 0.2691597  0.2693119  0.27014562 0.27087134 0.27107567
 0.2710503  0.27126774 0.271726   0.27197435 0.27179328 0.27168715
 0.27198458 0.2722803  0.27238718 0.27196348 0.2710516  0.27025136
 0.27005994 0.27028987 0.27033675 0.26992178 0.26940966 0.26934376
 0.26982164 0.27025706 0.27026844 0.27005342 0.27009335 0.2705198
 0.27096286 0.2710906  0.27097675 0.27096668 0.27119797 0.27154225
 0.2715011  0.2710218  0.2704959  0.27016446 0.27001375 0.26981094
 0.26942542 0.2690548  0.26902333 0.26937786 0.26964578 0.26955906
 0.26933596 0.26938084 0.26984164 0.27043116 0.27076864 0.2707296
 0.27073947 0.2708302  0.27095172 0.2709679  0.27080515 0.2706721
 0.27055323 0.27051505 0.27036446 0.26989567 0.26931658 0.26898292
 0.26899797 0.26910573 0.26906094 0.26901153 0.26907903 0.2694044
 0.26987258 0.27019027 0.2702486  0.27022806 0.27029523 0.27039355
 0.27064782 0.27069306 0.27057672 0.27046815 0.2705891  0.27073312
 0.27059853 0.2703494  0.27020323 0.270114   0.27007014 0.26995432
 0.26977152 0.26963213 0.2697549  0.27015227 0.27048695 0.27062672
 0.2706627  0.2708353  0.27124593 0.271712   0.27188066 0.27140686
 0.2711749  0.27102736 0.27093744 0.27084085 0.2706835  0.27063015
 0.27074602 0.27075276 0.27057406 0.27025458 0.27000847 0.270017
 0.27024075 0.27046168 0.2705095  0.27055073 0.27068532 0.27099133
 0.27135187 0.27155766 0.27156174 0.27152333 0.27162552 0.27188224
 0.27213955 0.27212903 0.2719007  0.27171174 0.27171728 0.27210513
 0.27257314 0.2720629  0.27147648 0.27136418 0.2714956  0.2715246
 0.27133882 0.27117115 0.27127638 0.27162427 0.27192745 0.27202988
 0.2720685  0.2722291  0.2725986  0.27300546 0.27319998 0.27309197
 0.27272087 0.27233803 0.27206323 0.27181512 0.2716209  0.2718107
 0.27229556 0.2722442  0.27186543 0.27143678 0.27113533 0.27115846
 0.27147287 0.27179784 0.27190956 0.27185345 0.2718288  0.27203092
 0.2724259  0.27271244 0.27274445 0.2726061  0.27250624 0.2725445
 0.2725973  0.27250803 0.27225867 0.27195328 0.27174085 0.2718195
 0.27199396 0.27185196 0.27158058 0.27143085 0.2714346  0.27147
 0.27144033 0.2714236  0.27158284 0.2719022  0.27215266 0.27219096
 0.2721524  0.2721333  0.27215308 0.27213213 0.27200598 0.27178907
 0.271543   0.27133328 0.27124232 0.2711035  0.27098286 0.27099574
 0.27107975 0.2711211  0.27106002 0.27084386 0.2706087  0.2705649
 0.27076766 0.2710433  0.27118552 0.27124098 0.271302   0.27149892
 0.27180186 0.27200574 0.27198595 0.2718458  0.27179092 0.27185753
 0.2719482  0.27186018 0.27164182 0.27143443 0.2713187  0.27124608
 0.27099606 0.2706936  0.27053997 0.2705523  0.27065966 0.2708237
 0.27092558 0.27098975 0.27112913 0.2714946  0.27181938 0.27193755
 0.2719156  0.27192172 0.27204043 0.27215746 0.27206263 0.27170062
 0.27148688 0.27133653 0.27118716 0.27102053 0.27091005 0.27091217
 0.27095148 0.2711245  0.2713077  0.27131656 0.2712781  0.27138463
 0.27164847 0.2719078  0.27203217 0.27203187 0.2720018  0.2721381
 0.27242118 0.27264407 0.27266273 0.27253482 0.27240908 0.27225235
 0.27229503 0.2721962  0.27196726 0.2717397  0.27162844 0.27165568
 0.27171046 0.2716491  0.27163333 0.2717711  0.27202615 0.2722293
 0.27226838 0.27219364 0.2721444  0.27224568 0.272442   0.2726297
 0.27271876 0.27277872 0.27293894 0.27317223 0.2732923  0.2731248
 0.27285787 0.27259877 0.2724425  0.27238655 0.27232796 0.2724598
 0.2727195  0.2724477  0.27217123 0.27217707 0.27233496 0.27253196
 0.27268505 0.27277285 0.27281645 0.27288052 0.2729952  0.27317774
 0.27337503 0.2734725  0.27345207 0.2733474  0.27325794 0.27322257
 0.27314404 0.27292946 0.27260345 0.27227804 0.27203557 0.27219635
 0.27263072 0.2726299  0.27255157 0.27269378 0.27292928 0.27308375
 0.27311876 0.27310917 0.2731443  0.27320662 0.27319804 0.27315637
 0.27315745 0.27322194 0.27330956 0.27332804 0.27320892 0.27297974
 0.27268043 0.27239427 0.27220216 0.27207068 0.2719875  0.2721259
 0.2723569  0.27238894 0.27232292 0.2722812  0.27237877 0.27266362
 0.27300152 0.2731584  0.2730482  0.2728037  0.27261934 0.27262658
 0.2727809  0.2728121  0.27263218 0.2723432  0.27212662 0.27203482
 0.2719401  0.27175468 0.27154496 0.27146626 0.27153677 0.27170742
 0.2717576  0.2716924  0.27168536 0.2718024  0.27193615 0.27195236
 0.27185956 0.2717947  0.2719253  0.27220264 0.27231807 0.27217016
 0.2719746  0.2719126  0.272027   0.27217722 0.27219412 0.2720251
 0.2718017  0.27151695 0.27127716 0.27116284 0.2710701  0.27104816
 0.2709781  0.27097386 0.27101076 0.27109978 0.2712635  0.27154228
 0.27187014 0.27210537 0.27214798 0.27207416 0.27203375 0.27209383
 0.2721915  0.27217835 0.27201292 0.27180663 0.27162984 0.2714883
 0.27145785 0.27122888 0.27091143 0.27074555 0.27089942 0.2712153
 0.2713289  0.2713409  0.27142146 0.27158278 0.27179363 0.2719142
 0.27190655 0.27190575 0.27205852 0.27234167 0.27257094 0.27258578
 0.27241838 0.2722678  0.27226302 0.27229884 0.27216414 0.27183977
 0.2715299  0.2713431  0.2713649  0.27152056 0.27169383 0.27189267
 0.27209708 0.27218506 0.27222216 0.27227005 0.27240124 0.27262938
 0.27286854 0.27298114 0.2728866  0.27264178 0.27244034 0.27242345
 0.27253804 0.272599   0.2725065  0.27240315 0.27242088 0.27251345
 0.27248055 0.2722253  0.27193218 0.27181008 0.27189288 0.2722076
 0.2724822  0.27214238 0.2719385  0.27222714 0.2726703  0.2728868
 0.27277267 0.27256536 0.27255356 0.27276477 0.2729345  0.27287987
 0.27262345 0.27236173 0.2723014  0.27232298 0.2721952  0.2718313
 0.27137667 0.27111226 0.27108654 0.2711041  0.2710763  0.27128637
 0.2717369  0.27185145 0.27180988 0.27181515 0.27194452 0.27226657
 0.27266562 0.27287254 0.2727246  0.2723422  0.2719999  0.27191687
 0.27197313 0.2718702  0.2715155  0.27111426 0.27093714 0.2709826
 0.27095458 0.27065924 0.27028042 0.27016577 0.27039203 0.2708291
 0.27109376 0.27102536 0.27107558 0.27147734 0.2719718  0.27217966
 0.27200615 0.27172655 0.27165204 0.27174902 0.27168778 0.27132702
 0.27085543 0.27051327 0.27038306 0.27028257 0.270017   0.26968133
 0.26954103 0.2697526  0.27016756 0.27047727 0.27052072 0.2705045
 0.27056652 0.2707084  0.2708555  0.27087232 0.270833   0.2708847
 0.27097794 0.2709019  0.27056396 0.2701203  0.2698121  0.26977372
 0.26987094 0.26976645 0.26934886 0.26888123 0.26870456 0.26884893
 0.26896986 0.2688097  0.26853067 0.2685183  0.26886395 0.26930663
 0.26944274 0.2694202  0.26958618 0.26998186 0.2703926  0.2704809
 0.27014983 0.26972595 0.2696009  0.26973957 0.26967847 0.26913258
 0.268396   0.26800463 0.26810086 0.268289   0.2681761  0.26787385
 0.26778644 0.26804495 0.26845595 0.26867634 0.26869497 0.26873627
 0.26898614 0.2695624  0.270111   0.2702256  0.27006832 0.27001193
 0.27014166 0.2701934  0.26989317 0.26932102 0.26889765 0.26888263
 0.26901287 0.26884234 0.2683717  0.26806998 0.26824677 0.26872748
 0.26898247 0.26885676 0.26878688 0.26917723 0.26990497 0.2705102
 0.27065924 0.2704792  0.27052727 0.270925   0.2712296  0.2709313
 0.27009228 0.26934767 0.26918858 0.26938722 0.26930827 0.26871228
 0.26808512 0.26804176 0.26853415 0.26891774 0.26883215 0.26862362
 0.26887998 0.26968396 0.2704349  0.2706243  0.27040067 0.27063224
 0.27146372 0.2717568  0.2715799  0.27126214 0.27109453 0.27121753
 0.27129218 0.2707717  0.26960933 0.26856187 0.26844692 0.26919225
 0.26977462 0.2694605  0.26885077 0.26902834 0.2701869  0.27128026
 0.27133414 0.2706999  0.27052733 0.27124444 0.272197   0.27269766
 0.2725657  0.27209076 0.27216518 0.27244234 0.27174094 0.26990774
 0.26839364 0.26851597 0.26952225 0.26952428 0.2682117  0.26764235
 0.26952028 0.27245873 0.27390125 0.2732272  0.2724966  0.2735332
 0.2753714  0.2744597  0.2680535  0.25990215 0.26238728 0.26746735]
