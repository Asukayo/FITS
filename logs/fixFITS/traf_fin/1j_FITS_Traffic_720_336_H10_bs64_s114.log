Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16559226880.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 77.70747923851013
Epoch: 1, Steps: 87 | Train Loss: 0.7964075 Vali Loss: 0.6942279 Test Loss: 0.7998554
Validation loss decreased (inf --> 0.694228).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 72.19562697410583
Epoch: 2, Steps: 87 | Train Loss: 0.4510764 Vali Loss: 0.4954154 Test Loss: 0.5750287
Validation loss decreased (0.694228 --> 0.495415).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 77.02777123451233
Epoch: 3, Steps: 87 | Train Loss: 0.3348819 Vali Loss: 0.4077104 Test Loss: 0.4797657
Validation loss decreased (0.495415 --> 0.407710).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 74.49225854873657
Epoch: 4, Steps: 87 | Train Loss: 0.2844705 Vali Loss: 0.3702969 Test Loss: 0.4390370
Validation loss decreased (0.407710 --> 0.370297).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 77.70103716850281
Epoch: 5, Steps: 87 | Train Loss: 0.2631204 Vali Loss: 0.3536645 Test Loss: 0.4232580
Validation loss decreased (0.370297 --> 0.353664).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 82.9429714679718
Epoch: 6, Steps: 87 | Train Loss: 0.2543903 Vali Loss: 0.3473550 Test Loss: 0.4174311
Validation loss decreased (0.353664 --> 0.347355).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 82.82831597328186
Epoch: 7, Steps: 87 | Train Loss: 0.2509312 Vali Loss: 0.3443252 Test Loss: 0.4151622
Validation loss decreased (0.347355 --> 0.344325).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 70.95109534263611
Epoch: 8, Steps: 87 | Train Loss: 0.2494771 Vali Loss: 0.3430965 Test Loss: 0.4146166
Validation loss decreased (0.344325 --> 0.343096).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 72.30184841156006
Epoch: 9, Steps: 87 | Train Loss: 0.2487837 Vali Loss: 0.3420026 Test Loss: 0.4139658
Validation loss decreased (0.343096 --> 0.342003).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 76.1381938457489
Epoch: 10, Steps: 87 | Train Loss: 0.2484616 Vali Loss: 0.3413160 Test Loss: 0.4137368
Validation loss decreased (0.342003 --> 0.341316).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 73.60367965698242
Epoch: 11, Steps: 87 | Train Loss: 0.2483375 Vali Loss: 0.3412708 Test Loss: 0.4136856
Validation loss decreased (0.341316 --> 0.341271).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 71.77056956291199
Epoch: 12, Steps: 87 | Train Loss: 0.2481166 Vali Loss: 0.3409836 Test Loss: 0.4135589
Validation loss decreased (0.341271 --> 0.340984).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 74.06433343887329
Epoch: 13, Steps: 87 | Train Loss: 0.2479947 Vali Loss: 0.3407953 Test Loss: 0.4132506
Validation loss decreased (0.340984 --> 0.340795).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 74.87749552726746
Epoch: 14, Steps: 87 | Train Loss: 0.2479041 Vali Loss: 0.3401939 Test Loss: 0.4133377
Validation loss decreased (0.340795 --> 0.340194).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 74.75980806350708
Epoch: 15, Steps: 87 | Train Loss: 0.2478781 Vali Loss: 0.3403396 Test Loss: 0.4131581
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 76.93134760856628
Epoch: 16, Steps: 87 | Train Loss: 0.2477794 Vali Loss: 0.3401717 Test Loss: 0.4131616
Validation loss decreased (0.340194 --> 0.340172).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 76.33646607398987
Epoch: 17, Steps: 87 | Train Loss: 0.2477625 Vali Loss: 0.3398911 Test Loss: 0.4130392
Validation loss decreased (0.340172 --> 0.339891).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 73.77508807182312
Epoch: 18, Steps: 87 | Train Loss: 0.2476612 Vali Loss: 0.3401456 Test Loss: 0.4132132
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 65.5392439365387
Epoch: 19, Steps: 87 | Train Loss: 0.2477094 Vali Loss: 0.3397559 Test Loss: 0.4131813
Validation loss decreased (0.339891 --> 0.339756).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 72.97303628921509
Epoch: 20, Steps: 87 | Train Loss: 0.2476856 Vali Loss: 0.3398758 Test Loss: 0.4129715
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 75.15083575248718
Epoch: 21, Steps: 87 | Train Loss: 0.2475724 Vali Loss: 0.3394703 Test Loss: 0.4129268
Validation loss decreased (0.339756 --> 0.339470).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 75.48684501647949
Epoch: 22, Steps: 87 | Train Loss: 0.2475740 Vali Loss: 0.3396489 Test Loss: 0.4127039
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 80.65471076965332
Epoch: 23, Steps: 87 | Train Loss: 0.2475149 Vali Loss: 0.3397007 Test Loss: 0.4129169
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 69.92943811416626
Epoch: 24, Steps: 87 | Train Loss: 0.2474868 Vali Loss: 0.3395779 Test Loss: 0.4127724
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.41078460216522217, mae:0.27920135855674744, rse:0.5267521739006042, corr:[0.2712411  0.28383356 0.28174412 0.28288105 0.28397995 0.2837863
 0.28564146 0.2842354  0.28538775 0.284672   0.28471342 0.28477117
 0.2834542  0.28420162 0.28351164 0.28367656 0.28347668 0.28286982
 0.2835651  0.2833706  0.28385907 0.28380767 0.28368697 0.28379405
 0.28483608 0.28578568 0.28532472 0.28514034 0.28513792 0.28438613
 0.28466824 0.28399402 0.2840678  0.28411525 0.28358737 0.28414336
 0.2835712  0.28405657 0.2843576  0.2837915  0.28419495 0.28392532
 0.28441837 0.28452143 0.2842135  0.28464663 0.28418648 0.28424302
 0.28465542 0.28470123 0.2850294  0.28460434 0.28465918 0.28411397
 0.2837279  0.2839832  0.28383604 0.2843619  0.2838538  0.2836234
 0.28420943 0.28420138 0.28472245 0.28417137 0.2840039  0.28441152
 0.28406817 0.28431877 0.2840632  0.28382418 0.28413817 0.28407288
 0.28394815 0.28361222 0.28387478 0.2839993  0.28389046 0.28419623
 0.28370816 0.2835501  0.28381294 0.28379568 0.28373173 0.28324366
 0.28345594 0.2836196  0.28377798 0.28404757 0.28359494 0.28397968
 0.28413498 0.2837112  0.2833472  0.2829462  0.28374287 0.2839767
 0.2836662  0.28374046 0.2831504  0.2832254  0.28333536 0.28351092
 0.2833586  0.28280059 0.2834335  0.28360707 0.28379673 0.2840476
 0.2837751  0.284116   0.28351045 0.28317025 0.28342173 0.2829546
 0.28331116 0.28331822 0.28309092 0.2831461  0.28294182 0.28319424
 0.2833363  0.28385836 0.28411913 0.2840632  0.2844055  0.2840643
 0.28387734 0.28358233 0.28312153 0.28347367 0.28333017 0.28360462
 0.2841708  0.2843647  0.2845166  0.28379023 0.28395918 0.28455114
 0.28425843 0.2843525  0.2842023  0.28403437 0.28394938 0.28384247
 0.28424096 0.28431278 0.28456232 0.2845721  0.2845827  0.2845816
 0.28410152 0.28428248 0.28414956 0.28387868 0.28398317 0.28382024
 0.28409338 0.28418526 0.28430167 0.28436804 0.28402618 0.28429088
 0.28413805 0.283975   0.2841378  0.2840217  0.2843499  0.28419665
 0.28517854 0.28550804 0.28531978 0.28542247 0.28486428 0.28495926
 0.28525475 0.28504214 0.28554985 0.2853531  0.2849722  0.2851108
 0.28541133 0.2858232  0.28540283 0.28522977 0.28502327 0.28474787
 0.28510803 0.28463015 0.28436005 0.2844677  0.28441128 0.2844971
 0.28474292 0.28519228 0.28533307 0.28499213 0.28510237 0.2849832
 0.28513342 0.28467536 0.28437907 0.28462806 0.2843174  0.28471148
 0.28480107 0.284619   0.2846732  0.28415534 0.284126   0.2840309
 0.28407753 0.28400823 0.28335235 0.28386828 0.2839806  0.28358808
 0.2839918  0.2839077  0.28389105 0.2837863  0.28416604 0.28455594
 0.28422418 0.28404662 0.28357592 0.2838557  0.28398833 0.28361174
 0.28404465 0.283857   0.2839969  0.2840039  0.28357476 0.28392443
 0.28369638 0.28375357 0.28355107 0.28296593 0.2832223  0.2830014
 0.28332224 0.28357667 0.2835606  0.2841312  0.28414422 0.28460056
 0.28443384 0.28380677 0.28416723 0.28404382 0.28414178 0.28384972
 0.2835367  0.283898   0.28357664 0.283828   0.28364488 0.28325582
 0.28362104 0.28346026 0.2834362  0.28299153 0.28307578 0.28345975
 0.28292546 0.28341198 0.28336456 0.28283182 0.28288743 0.28282213
 0.2829576  0.2823604  0.28243753 0.28289083 0.28248814 0.2824786
 0.2820704  0.28232038 0.28268546 0.28242707 0.28275856 0.28244993
 0.2825722  0.28267273 0.28240967 0.28285685 0.28283232 0.28313226
 0.2831293  0.28330106 0.28356537 0.2829833  0.28341272 0.28342688
 0.28302723 0.28279296 0.28212118 0.2825815  0.28199643 0.28188333
 0.28270045 0.28242233 0.28328478 0.28291434 0.28253883 0.2828311
 0.28239906 0.28337204 0.28323132 0.28333145 0.2837207  0.28329653
 0.28409928 0.2836766  0.28383124 0.28367022 0.28326082 0.2835903
 0.28237185 0.28284144 0.28263    0.28261635 0.283557   0.28277853
 0.2841466  0.28401658 0.2845946  0.28520516 0.2838405  0.28507102
 0.28296295 0.2835757  0.28166106 0.28083855 0.28164378 0.28563136]
