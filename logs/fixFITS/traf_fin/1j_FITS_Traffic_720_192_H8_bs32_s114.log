Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H8', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j192_H8_FITS_custom_ftM_sl720_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=258, out_features=326, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4640070144.0
params:  84434.0
Trainable parameters:  84434
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4369889
	speed: 0.6451s/iter; left time: 5645.5902s
Epoch: 1 cost time: 109.95093512535095
Epoch: 1, Steps: 177 | Train Loss: 0.5781983 Vali Loss: 0.4125755 Test Loss: 0.4824006
Validation loss decreased (inf --> 0.412575).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2463671
	speed: 1.5190s/iter; left time: 13024.1600s
Epoch: 2 cost time: 87.64222812652588
Epoch: 2, Steps: 177 | Train Loss: 0.2625826 Vali Loss: 0.3373644 Test Loss: 0.4061356
Validation loss decreased (0.412575 --> 0.337364).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2530241
	speed: 1.4696s/iter; left time: 12339.9499s
Epoch: 3 cost time: 91.19955611228943
Epoch: 3, Steps: 177 | Train Loss: 0.2413402 Vali Loss: 0.3323010 Test Loss: 0.4020723
Validation loss decreased (0.337364 --> 0.332301).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2388966
	speed: 1.4086s/iter; left time: 11578.4398s
Epoch: 4 cost time: 84.32910203933716
Epoch: 4, Steps: 177 | Train Loss: 0.2398456 Vali Loss: 0.3306552 Test Loss: 0.4016046
Validation loss decreased (0.332301 --> 0.330655).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2431901
	speed: 1.3896s/iter; left time: 11176.3308s
Epoch: 5 cost time: 93.9514548778534
Epoch: 5, Steps: 177 | Train Loss: 0.2392821 Vali Loss: 0.3303849 Test Loss: 0.4009171
Validation loss decreased (0.330655 --> 0.330385).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2336900
	speed: 1.5915s/iter; left time: 12518.7618s
Epoch: 6 cost time: 103.69188594818115
Epoch: 6, Steps: 177 | Train Loss: 0.2390328 Vali Loss: 0.3292543 Test Loss: 0.4014027
Validation loss decreased (0.330385 --> 0.329254).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2310423
	speed: 1.6003s/iter; left time: 12305.0648s
Epoch: 7 cost time: 89.78704023361206
Epoch: 7, Steps: 177 | Train Loss: 0.2388643 Vali Loss: 0.3300663 Test Loss: 0.4007377
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2495384
	speed: 1.1682s/iter; left time: 8775.3630s
Epoch: 8 cost time: 85.94767570495605
Epoch: 8, Steps: 177 | Train Loss: 0.2387316 Vali Loss: 0.3296470 Test Loss: 0.4007927
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2300648
	speed: 1.6538s/iter; left time: 12130.8670s
Epoch: 9 cost time: 105.25979614257812
Epoch: 9, Steps: 177 | Train Loss: 0.2386926 Vali Loss: 0.3289076 Test Loss: 0.4007958
Validation loss decreased (0.329254 --> 0.328908).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2332662
	speed: 1.6781s/iter; left time: 12012.0989s
Epoch: 10 cost time: 107.6472737789154
Epoch: 10, Steps: 177 | Train Loss: 0.2385395 Vali Loss: 0.3290075 Test Loss: 0.4004420
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2342680
	speed: 1.6914s/iter; left time: 11807.4363s
Epoch: 11 cost time: 102.09237360954285
Epoch: 11, Steps: 177 | Train Loss: 0.2385417 Vali Loss: 0.3293167 Test Loss: 0.4004802
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2369095
	speed: 1.6345s/iter; left time: 11120.8292s
Epoch: 12 cost time: 93.06519842147827
Epoch: 12, Steps: 177 | Train Loss: 0.2385166 Vali Loss: 0.3291460 Test Loss: 0.4001807
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j192_H8_FITS_custom_ftM_sl720_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.39991605281829834, mae:0.2764512300491333, rse:0.5219314098358154, corr:[0.2803007  0.2881846  0.29092947 0.28873324 0.2903739  0.29201272
 0.29139367 0.29226854 0.2920268  0.29032013 0.29048467 0.29078713
 0.29008532 0.28991827 0.28961906 0.28907844 0.28929964 0.28935316
 0.28901088 0.28947142 0.2901283  0.29023984 0.29017714 0.2899009
 0.29063076 0.29158846 0.29211327 0.2911698  0.29092026 0.29180834
 0.29169005 0.29130003 0.2913473  0.29047945 0.2897697  0.29038146
 0.2903156  0.2894807  0.28973544 0.2900797  0.2894054  0.2888851
 0.2889377  0.2889357  0.28926352 0.29013634 0.29046962 0.29001155
 0.28980765 0.28989398 0.29030973 0.29019186 0.28950384 0.28932706
 0.28980064 0.28985652 0.2893122  0.28877038 0.2882611  0.28804237
 0.2884558  0.28890494 0.28908455 0.2892145  0.28909287 0.2885785
 0.28807715 0.28788677 0.2879524  0.2881789  0.28839526 0.28866377
 0.28879234 0.28883994 0.28924158 0.28963366 0.28942925 0.28904188
 0.28913707 0.28941187 0.28930467 0.28892964 0.28864527 0.2885146
 0.28846502 0.28840786 0.28855106 0.28902215 0.2892789  0.28885192
 0.28835323 0.2883106  0.28835794 0.28849733 0.2889289  0.2892126
 0.28880137 0.288511   0.2883767  0.28804514 0.2876952  0.28731588
 0.2872446  0.2877736  0.28821442 0.28793865 0.28733286 0.2868133
 0.2864127  0.28630528 0.28686786 0.28781012 0.28826773 0.28805256
 0.2876075  0.28695747 0.28626367 0.28624612 0.2869283  0.2873137
 0.28732756 0.28776538 0.28832698 0.28841394 0.28797573 0.28744107
 0.28738594 0.28778273 0.2879784  0.28768882 0.28736082 0.28740126
 0.2876114  0.28777218 0.28807247 0.2886404  0.28918016 0.28922012
 0.2887624  0.2884142  0.28869018 0.28924215 0.28930056 0.2889228
 0.2890062  0.2894417  0.289758   0.28963166 0.28908518 0.28832906
 0.2877561  0.28772494 0.28796148 0.28792083 0.28781033 0.28802556
 0.28815466 0.288013   0.28807932 0.2883901  0.28887543 0.28948328
 0.28959057 0.28916287 0.28917584 0.28942245 0.28869542 0.2879097
 0.2890815  0.28983605 0.29012993 0.29006603 0.28939742 0.28901005
 0.28934753 0.28911206 0.2882897  0.28819615 0.28837195 0.28839794
 0.28886077 0.28897026 0.28913134 0.29024643 0.29042768 0.29006812
 0.29056782 0.28899652 0.28705856 0.28829885 0.2859058  0.29024386]
