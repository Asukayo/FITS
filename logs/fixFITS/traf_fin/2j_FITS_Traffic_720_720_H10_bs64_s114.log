Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  22596812800.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 89.43051600456238
Epoch: 1, Steps: 84 | Train Loss: 1.1541284 Vali Loss: 1.1525716 Test Loss: 1.3612790
Validation loss decreased (inf --> 1.152572).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 93.77391576766968
Epoch: 2, Steps: 84 | Train Loss: 0.8190015 Vali Loss: 1.0223701 Test Loss: 1.2016526
Validation loss decreased (1.152572 --> 1.022370).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 88.83084487915039
Epoch: 3, Steps: 84 | Train Loss: 0.7249222 Vali Loss: 0.9576455 Test Loss: 1.1219072
Validation loss decreased (1.022370 --> 0.957646).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 90.39949774742126
Epoch: 4, Steps: 84 | Train Loss: 0.6615982 Vali Loss: 0.9067124 Test Loss: 1.0614625
Validation loss decreased (0.957646 --> 0.906712).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 91.23413515090942
Epoch: 5, Steps: 84 | Train Loss: 0.6102500 Vali Loss: 0.8612068 Test Loss: 1.0078365
Validation loss decreased (0.906712 --> 0.861207).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 88.97245955467224
Epoch: 6, Steps: 84 | Train Loss: 0.5669533 Vali Loss: 0.8224156 Test Loss: 0.9620183
Validation loss decreased (0.861207 --> 0.822416).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 97.18148827552795
Epoch: 7, Steps: 84 | Train Loss: 0.5297813 Vali Loss: 0.7880266 Test Loss: 0.9225865
Validation loss decreased (0.822416 --> 0.788027).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 97.17395615577698
Epoch: 8, Steps: 84 | Train Loss: 0.4974921 Vali Loss: 0.7583114 Test Loss: 0.8869866
Validation loss decreased (0.788027 --> 0.758311).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 95.37078619003296
Epoch: 9, Steps: 84 | Train Loss: 0.4691781 Vali Loss: 0.7316039 Test Loss: 0.8562752
Validation loss decreased (0.758311 --> 0.731604).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 99.23756194114685
Epoch: 10, Steps: 84 | Train Loss: 0.4442583 Vali Loss: 0.7080278 Test Loss: 0.8277010
Validation loss decreased (0.731604 --> 0.708028).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 98.49208545684814
Epoch: 11, Steps: 84 | Train Loss: 0.4219817 Vali Loss: 0.6857574 Test Loss: 0.8028866
Validation loss decreased (0.708028 --> 0.685757).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 98.67169332504272
Epoch: 12, Steps: 84 | Train Loss: 0.4022480 Vali Loss: 0.6664073 Test Loss: 0.7789512
Validation loss decreased (0.685757 --> 0.666407).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 93.95632076263428
Epoch: 13, Steps: 84 | Train Loss: 0.3844482 Vali Loss: 0.6469392 Test Loss: 0.7570715
Validation loss decreased (0.666407 --> 0.646939).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 88.11580395698547
Epoch: 14, Steps: 84 | Train Loss: 0.3685272 Vali Loss: 0.6316743 Test Loss: 0.7383350
Validation loss decreased (0.646939 --> 0.631674).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 90.09291648864746
Epoch: 15, Steps: 84 | Train Loss: 0.3540285 Vali Loss: 0.6157385 Test Loss: 0.7206360
Validation loss decreased (0.631674 --> 0.615739).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 106.91706728935242
Epoch: 16, Steps: 84 | Train Loss: 0.3409630 Vali Loss: 0.6030437 Test Loss: 0.7053470
Validation loss decreased (0.615739 --> 0.603044).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 91.2178168296814
Epoch: 17, Steps: 84 | Train Loss: 0.3290499 Vali Loss: 0.5897688 Test Loss: 0.6897718
Validation loss decreased (0.603044 --> 0.589769).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 88.1241672039032
Epoch: 18, Steps: 84 | Train Loss: 0.3182446 Vali Loss: 0.5785890 Test Loss: 0.6764713
Validation loss decreased (0.589769 --> 0.578589).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 92.20283102989197
Epoch: 19, Steps: 84 | Train Loss: 0.3084427 Vali Loss: 0.5686386 Test Loss: 0.6644666
Validation loss decreased (0.578589 --> 0.568639).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 97.6595447063446
Epoch: 20, Steps: 84 | Train Loss: 0.2993133 Vali Loss: 0.5589274 Test Loss: 0.6532278
Validation loss decreased (0.568639 --> 0.558927).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 93.55236840248108
Epoch: 21, Steps: 84 | Train Loss: 0.2911266 Vali Loss: 0.5506034 Test Loss: 0.6429043
Validation loss decreased (0.558927 --> 0.550603).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 102.80033755302429
Epoch: 22, Steps: 84 | Train Loss: 0.2834926 Vali Loss: 0.5426720 Test Loss: 0.6335883
Validation loss decreased (0.550603 --> 0.542672).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 94.41395974159241
Epoch: 23, Steps: 84 | Train Loss: 0.2764852 Vali Loss: 0.5337124 Test Loss: 0.6240213
Validation loss decreased (0.542672 --> 0.533712).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 109.59733939170837
Epoch: 24, Steps: 84 | Train Loss: 0.2700033 Vali Loss: 0.5279161 Test Loss: 0.6164323
Validation loss decreased (0.533712 --> 0.527916).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 99.277428150177
Epoch: 25, Steps: 84 | Train Loss: 0.2640301 Vali Loss: 0.5210756 Test Loss: 0.6088829
Validation loss decreased (0.527916 --> 0.521076).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 97.97092986106873
Epoch: 26, Steps: 84 | Train Loss: 0.2585918 Vali Loss: 0.5157208 Test Loss: 0.6017826
Validation loss decreased (0.521076 --> 0.515721).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 99.112051486969
Epoch: 27, Steps: 84 | Train Loss: 0.2534523 Vali Loss: 0.5095639 Test Loss: 0.5953743
Validation loss decreased (0.515721 --> 0.509564).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 110.37913179397583
Epoch: 28, Steps: 84 | Train Loss: 0.2487612 Vali Loss: 0.5046062 Test Loss: 0.5893397
Validation loss decreased (0.509564 --> 0.504606).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 96.35539841651917
Epoch: 29, Steps: 84 | Train Loss: 0.2443924 Vali Loss: 0.4998848 Test Loss: 0.5832720
Validation loss decreased (0.504606 --> 0.499885).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 95.54677653312683
Epoch: 30, Steps: 84 | Train Loss: 0.2402218 Vali Loss: 0.4957689 Test Loss: 0.5786283
Validation loss decreased (0.499885 --> 0.495769).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 92.4381639957428
Epoch: 31, Steps: 84 | Train Loss: 0.2364602 Vali Loss: 0.4906533 Test Loss: 0.5731317
Validation loss decreased (0.495769 --> 0.490653).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 98.06936883926392
Epoch: 32, Steps: 84 | Train Loss: 0.2329092 Vali Loss: 0.4878839 Test Loss: 0.5694340
Validation loss decreased (0.490653 --> 0.487884).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 96.91534733772278
Epoch: 33, Steps: 84 | Train Loss: 0.2296148 Vali Loss: 0.4840047 Test Loss: 0.5647889
Validation loss decreased (0.487884 --> 0.484005).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 98.4685525894165
Epoch: 34, Steps: 84 | Train Loss: 0.2265033 Vali Loss: 0.4807133 Test Loss: 0.5609623
Validation loss decreased (0.484005 --> 0.480713).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 94.70479273796082
Epoch: 35, Steps: 84 | Train Loss: 0.2236859 Vali Loss: 0.4776618 Test Loss: 0.5570969
Validation loss decreased (0.480713 --> 0.477662).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 113.28285431861877
Epoch: 36, Steps: 84 | Train Loss: 0.2208919 Vali Loss: 0.4748837 Test Loss: 0.5538008
Validation loss decreased (0.477662 --> 0.474884).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 100.0007836818695
Epoch: 37, Steps: 84 | Train Loss: 0.2183852 Vali Loss: 0.4723469 Test Loss: 0.5505995
Validation loss decreased (0.474884 --> 0.472347).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 92.77252435684204
Epoch: 38, Steps: 84 | Train Loss: 0.2160412 Vali Loss: 0.4692125 Test Loss: 0.5475002
Validation loss decreased (0.472347 --> 0.469212).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 100.56219983100891
Epoch: 39, Steps: 84 | Train Loss: 0.2137790 Vali Loss: 0.4668030 Test Loss: 0.5445414
Validation loss decreased (0.469212 --> 0.466803).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 103.91235566139221
Epoch: 40, Steps: 84 | Train Loss: 0.2117111 Vali Loss: 0.4639856 Test Loss: 0.5418150
Validation loss decreased (0.466803 --> 0.463986).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 97.02145600318909
Epoch: 41, Steps: 84 | Train Loss: 0.2097338 Vali Loss: 0.4620792 Test Loss: 0.5393637
Validation loss decreased (0.463986 --> 0.462079).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 100.91036343574524
Epoch: 42, Steps: 84 | Train Loss: 0.2079253 Vali Loss: 0.4601904 Test Loss: 0.5369653
Validation loss decreased (0.462079 --> 0.460190).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 94.5653645992279
Epoch: 43, Steps: 84 | Train Loss: 0.2061566 Vali Loss: 0.4587097 Test Loss: 0.5346656
Validation loss decreased (0.460190 --> 0.458710).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 91.89152908325195
Epoch: 44, Steps: 84 | Train Loss: 0.2045372 Vali Loss: 0.4560134 Test Loss: 0.5327540
Validation loss decreased (0.458710 --> 0.456013).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 101.74237990379333
Epoch: 45, Steps: 84 | Train Loss: 0.2029699 Vali Loss: 0.4550487 Test Loss: 0.5307962
Validation loss decreased (0.456013 --> 0.455049).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 98.83743095397949
Epoch: 46, Steps: 84 | Train Loss: 0.2015043 Vali Loss: 0.4537737 Test Loss: 0.5288956
Validation loss decreased (0.455049 --> 0.453774).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 94.20810341835022
Epoch: 47, Steps: 84 | Train Loss: 0.2001354 Vali Loss: 0.4524008 Test Loss: 0.5271115
Validation loss decreased (0.453774 --> 0.452401).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 104.23614621162415
Epoch: 48, Steps: 84 | Train Loss: 0.1988820 Vali Loss: 0.4505396 Test Loss: 0.5254225
Validation loss decreased (0.452401 --> 0.450540).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 103.97917985916138
Epoch: 49, Steps: 84 | Train Loss: 0.1976243 Vali Loss: 0.4492539 Test Loss: 0.5238762
Validation loss decreased (0.450540 --> 0.449254).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 94.28744387626648
Epoch: 50, Steps: 84 | Train Loss: 0.1964879 Vali Loss: 0.4479783 Test Loss: 0.5223362
Validation loss decreased (0.449254 --> 0.447978).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  22596812800.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 94.34173154830933
Epoch: 1, Steps: 84 | Train Loss: 0.3035146 Vali Loss: 0.4012399 Test Loss: 0.4682858
Validation loss decreased (inf --> 0.401240).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 104.65447807312012
Epoch: 2, Steps: 84 | Train Loss: 0.2782098 Vali Loss: 0.3882056 Test Loss: 0.4522948
Validation loss decreased (0.401240 --> 0.388206).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 102.9421603679657
Epoch: 3, Steps: 84 | Train Loss: 0.2718170 Vali Loss: 0.3856397 Test Loss: 0.4504973
Validation loss decreased (0.388206 --> 0.385640).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 104.26110649108887
Epoch: 4, Steps: 84 | Train Loss: 0.2704892 Vali Loss: 0.3849170 Test Loss: 0.4493097
Validation loss decreased (0.385640 --> 0.384917).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 106.70566201210022
Epoch: 5, Steps: 84 | Train Loss: 0.2702665 Vali Loss: 0.3846783 Test Loss: 0.4499212
Validation loss decreased (0.384917 --> 0.384678).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 110.97695994377136
Epoch: 6, Steps: 84 | Train Loss: 0.2702006 Vali Loss: 0.3847530 Test Loss: 0.4498729
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 106.96984314918518
Epoch: 7, Steps: 84 | Train Loss: 0.2701150 Vali Loss: 0.3857919 Test Loss: 0.4500767
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 105.77934002876282
Epoch: 8, Steps: 84 | Train Loss: 0.2701632 Vali Loss: 0.3847643 Test Loss: 0.4494468
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4493961036205292, mae:0.30024993419647217, rse:0.548170268535614, corr:[0.25698578 0.26664007 0.26590064 0.2664961  0.26822928 0.2683758
 0.26939064 0.2678934  0.26900488 0.2683334  0.26827806 0.2684349
 0.26792645 0.26839864 0.26712063 0.26737317 0.26706582 0.26707008
 0.26769644 0.2672349  0.26788047 0.26744324 0.2677965  0.26836327
 0.26902014 0.26988807 0.26931903 0.2691347  0.2686775  0.26803324
 0.26840612 0.26795453 0.2685104  0.2683705  0.26786953 0.26842162
 0.26775    0.26797974 0.2677679  0.26682493 0.26692492 0.26714107
 0.26775837 0.26766244 0.26767007 0.26804712 0.2680516  0.26846388
 0.26872483 0.26929134 0.2690262  0.2684298  0.2687089  0.26795623
 0.2683405  0.26869342 0.2681491  0.26848295 0.26769888 0.26714277
 0.26671258 0.26664284 0.26779118 0.2672715  0.2671555  0.26752728
 0.26752183 0.26778874 0.26738405 0.26763117 0.2674956  0.2672112
 0.26748708 0.26735073 0.2676931  0.2673756  0.26715088 0.2673691
 0.2670872  0.26743922 0.26726222 0.26691312 0.2668029  0.26626143
 0.2659564  0.26535577 0.26569816 0.26606026 0.2658835  0.2667074
 0.2665225  0.2663577  0.26689926 0.26655576 0.26668158 0.26687592
 0.26696163 0.26689672 0.26628566 0.26617402 0.26554045 0.26579815
 0.26635233 0.2661541  0.26656982 0.26638764 0.26615456 0.2663912
 0.26632482 0.2664908  0.26637638 0.26690087 0.26734158 0.26661074
 0.26657125 0.26664856 0.267242   0.26776755 0.26749298 0.2673664
 0.26696917 0.26687595 0.26677293 0.2665746  0.26667956 0.26661757
 0.2670222  0.26732394 0.26719067 0.26680598 0.26653647 0.26673147
 0.26651254 0.26681656 0.26729992 0.26718122 0.26728743 0.2669092
 0.26712084 0.2670967  0.2669377  0.26729614 0.26713756 0.2675249
 0.26856488 0.26872817 0.2687126  0.26834133 0.26820555 0.26836947
 0.26835218 0.26838964 0.2680264  0.26847512 0.26840374 0.26727715
 0.26725385 0.26729754 0.26755434 0.26790005 0.26789424 0.2683486
 0.26814264 0.26808643 0.2689148  0.26920345 0.26916564 0.26922822
 0.27043012 0.27063847 0.26961762 0.26855102 0.26883313 0.26995677
 0.26984254 0.2693423  0.26945156 0.26914915 0.2692914  0.26921672
 0.26885983 0.26908907 0.26881456 0.26880255 0.2691919  0.26956165
 0.26972774 0.26981097 0.27021053 0.269889   0.269824   0.27009365
 0.27061242 0.2713558  0.27080873 0.26948798 0.2692388  0.26915336
 0.26917905 0.2693123  0.26966605 0.27020895 0.27043465 0.27030745
 0.2694638  0.2692385  0.2695249  0.2691427  0.26900885 0.26866803
 0.2686766  0.26931238 0.26944008 0.2694582  0.26941827 0.26958725
 0.26985437 0.27001274 0.2702503  0.26970413 0.26927936 0.26896185
 0.26877627 0.26936945 0.2692149  0.26914057 0.26948977 0.26876688
 0.26819983 0.2680757  0.2682776  0.26881477 0.26867062 0.26852718
 0.26901352 0.2693897  0.26920953 0.26886645 0.26843566 0.26824248
 0.26877382 0.2690085  0.2692426  0.2695554  0.26946804 0.2692729
 0.26843023 0.2681005  0.268343   0.26810452 0.268292   0.2682512
 0.26780108 0.26800582 0.2682445  0.2686412  0.2692048  0.26948783
 0.2695407  0.26898694 0.2683598  0.26793212 0.26788542 0.26848838
 0.26886082 0.26871154 0.26844904 0.2688427  0.2691234  0.2690012
 0.26914054 0.26868373 0.26831105 0.26870126 0.26868302 0.26845494
 0.2683502  0.26882127 0.2691962  0.2688277  0.2683842  0.2679933
 0.26818055 0.2678218  0.26741952 0.26774788 0.26735494 0.2673868
 0.26789734 0.26791173 0.2681372  0.26859578 0.269018   0.26882735
 0.2684657  0.26843157 0.2683031  0.26867715 0.26911446 0.2696415
 0.2698512  0.26917428 0.2690068  0.26898184 0.26897457 0.26894206
 0.26885107 0.2691886  0.26900586 0.26885134 0.26931238 0.26903015
 0.2690415  0.26921216 0.2694787  0.26946115 0.26882413 0.26854566
 0.26817706 0.26880917 0.2695264  0.2691399  0.27002782 0.27059704
 0.2699468  0.269721   0.26962093 0.26966098 0.26913986 0.2687261
 0.26919648 0.26960215 0.269864   0.26974896 0.27015772 0.27050626
 0.27041218 0.2704799  0.2702153  0.26955488 0.26948488 0.2690563
 0.2686228  0.26875532 0.26938906 0.27013236 0.27040437 0.27058423
 0.270707   0.2705822  0.27027956 0.27052888 0.2708588  0.2704604
 0.270424   0.27076653 0.27119976 0.27143967 0.2711439  0.27093264
 0.27067    0.27044564 0.27107018 0.27110338 0.2705112  0.26990253
 0.26988813 0.27059206 0.2707473  0.27077097 0.27092674 0.27089545
 0.2708181  0.27017617 0.26995853 0.2704193  0.27054197 0.27051073
 0.27040127 0.27057967 0.27055743 0.27028042 0.27039254 0.27031398
 0.2701629  0.2701653  0.27011243 0.2699309  0.2698122  0.2700846
 0.2704722  0.27052942 0.270098   0.2696796  0.26997134 0.26992306
 0.26942205 0.26939678 0.2693772  0.26960266 0.27011994 0.2703647
 0.27021566 0.26973498 0.26937142 0.2692484  0.2695756  0.27023473
 0.27021864 0.26972368 0.26971078 0.27016973 0.2703893  0.27045172
 0.27021176 0.2696153  0.26918018 0.2687728  0.2689182  0.26944983
 0.26928377 0.26946086 0.27005082 0.2699653  0.26958716 0.26962844
 0.2697854  0.26913735 0.26868975 0.26896575 0.26901627 0.2694538
 0.26978558 0.2693615  0.26905006 0.26943934 0.2699948  0.26993948
 0.26945058 0.26881102 0.26863825 0.2685148  0.2679486  0.2680293
 0.26811537 0.2682869  0.26890633 0.2690067  0.26945248 0.26938212
 0.26875702 0.2687747  0.26878616 0.26904908 0.26914534 0.26914713
 0.2699631  0.26982665 0.26897088 0.26879665 0.26855838 0.2685425
 0.26833168 0.26814643 0.26854664 0.26907605 0.26942605 0.26894665
 0.2690344  0.26971194 0.26951596 0.26952615 0.26939297 0.26891872
 0.26898336 0.26904318 0.2691967  0.26940945 0.26948118 0.26990694
 0.27051383 0.27018076 0.2696164  0.2694032  0.2691436  0.2693252
 0.26946265 0.26960602 0.26968506 0.2691625  0.26918852 0.26928318
 0.2691681  0.26916555 0.26903263 0.26966414 0.26946834 0.26863304
 0.26923922 0.2695053  0.2692631  0.26937896 0.26938695 0.26914927
 0.26917422 0.2697651  0.27010828 0.26909998 0.26929945 0.2699515
 0.2695249  0.26940528 0.26918676 0.26899356 0.2693772  0.26945698
 0.269004   0.2689106  0.26923922 0.26976067 0.27017745 0.2694882
 0.26909775 0.26975438 0.27031162 0.27103117 0.27133334 0.2711891
 0.2708316  0.27011174 0.27018955 0.26990044 0.26939467 0.26948452
 0.26958695 0.27028593 0.2705441  0.27013752 0.27003422 0.26958328
 0.26956367 0.26940247 0.26909253 0.2696282  0.26963443 0.2697155
 0.2699443  0.26964328 0.26965827 0.26998892 0.27017465 0.2702009
 0.27006623 0.26912105 0.26844046 0.2687431  0.26863873 0.2685786
 0.26877293 0.26878962 0.2692155  0.26924103 0.26910755 0.26903707
 0.2683281  0.26823992 0.26860204 0.2689077  0.26881382 0.26844242
 0.26840723 0.2677197  0.26770133 0.268148   0.26816225 0.26938105
 0.26958114 0.269098   0.26932934 0.26879665 0.26890162 0.26921558
 0.26856914 0.26830533 0.26864654 0.2686704  0.26823217 0.26792592
 0.26769912 0.26779917 0.26818985 0.2680881  0.26841852 0.2688515
 0.26850048 0.26786697 0.2670971  0.26705584 0.26742604 0.26807797
 0.26878798 0.26861015 0.2683747  0.26808113 0.26798695 0.26768342
 0.2668533  0.26672888 0.26668882 0.26692212 0.266878   0.26648492
 0.26672047 0.2667963  0.26728046 0.2674635  0.26726428 0.26740012
 0.26673248 0.26663348 0.266837   0.26611245 0.26614785 0.26715526
 0.26817852 0.26800105 0.26714697 0.26697388 0.26707417 0.26771045
 0.26721513 0.26617703 0.2662098  0.26612264 0.26650998 0.2665954
 0.26644802 0.26680738 0.2670148  0.2676981  0.26772675 0.26748922
 0.2674877  0.26714212 0.2675289  0.26759106 0.26751193 0.2677385
 0.26786965 0.26819873 0.26758996 0.26727337 0.26792246 0.26799822
 0.26826268 0.26797178 0.26778817 0.2679039  0.267462   0.26789364
 0.26821965 0.2682288  0.26790905 0.26775542 0.26800513 0.26706067
 0.26692793 0.26732883 0.26754096 0.26817957 0.2678788  0.26802742
 0.26891488 0.26872492 0.26874176 0.26826203 0.26780382 0.26790002
 0.26874024 0.26939413 0.2685454  0.26825556 0.2677892  0.26779246
 0.26829642 0.26780275 0.2688543  0.26936057 0.26935163 0.2689903
 0.26823902 0.2696688  0.269846   0.27048483 0.2707923  0.26897523
 0.2695426  0.26945898 0.2689806  0.2686011  0.2681675  0.2686921
 0.26804107 0.2687704  0.26851937 0.26818332 0.26905236 0.26764342
 0.26859108 0.26899213 0.27018616 0.27199572 0.27181998 0.27280602
 0.2691138  0.2692237  0.2670103  0.2666108  0.26684737 0.2722873 ]
