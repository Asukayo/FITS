Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j96_H8', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j96_H8_FITS_custom_ftM_sl720_ll48_pl96_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=258, out_features=292, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4156136448.0
params:  75628.0
Trainable parameters:  75628
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2990848
	speed: 0.3553s/iter; left time: 3144.3765s
Epoch: 1 cost time: 66.42499446868896
Epoch: 1, Steps: 179 | Train Loss: 0.4400996 Vali Loss: 0.3369492 Test Loss: 0.4003488
Validation loss decreased (inf --> 0.336949).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2478900
	speed: 1.1231s/iter; left time: 9739.2509s
Epoch: 2 cost time: 77.34503984451294
Epoch: 2, Steps: 179 | Train Loss: 0.2358246 Vali Loss: 0.3299257 Test Loss: 0.3918768
Validation loss decreased (0.336949 --> 0.329926).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2305614
	speed: 1.3189s/iter; left time: 11201.3112s
Epoch: 3 cost time: 81.92468619346619
Epoch: 3, Steps: 179 | Train Loss: 0.2335909 Vali Loss: 0.3273822 Test Loss: 0.3912778
Validation loss decreased (0.329926 --> 0.327382).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2298960
	speed: 1.2553s/iter; left time: 10436.8476s
Epoch: 4 cost time: 83.49203300476074
Epoch: 4, Steps: 179 | Train Loss: 0.2330828 Vali Loss: 0.3275754 Test Loss: 0.3901514
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2291816
	speed: 1.3401s/iter; left time: 10901.5339s
Epoch: 5 cost time: 82.76412105560303
Epoch: 5, Steps: 179 | Train Loss: 0.2327674 Vali Loss: 0.3264828 Test Loss: 0.3896790
Validation loss decreased (0.327382 --> 0.326483).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2421815
	speed: 1.2647s/iter; left time: 10061.8566s
Epoch: 6 cost time: 79.1957516670227
Epoch: 6, Steps: 179 | Train Loss: 0.2326666 Vali Loss: 0.3273529 Test Loss: 0.3890503
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2360751
	speed: 1.2472s/iter; left time: 9699.3328s
Epoch: 7 cost time: 82.04953122138977
Epoch: 7, Steps: 179 | Train Loss: 0.2325270 Vali Loss: 0.3267234 Test Loss: 0.3891504
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2304506
	speed: 1.2904s/iter; left time: 9804.4927s
Epoch: 8 cost time: 82.01249384880066
Epoch: 8, Steps: 179 | Train Loss: 0.2325038 Vali Loss: 0.3266617 Test Loss: 0.3897431
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j96_H8_FITS_custom_ftM_sl720_ll48_pl96_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.3893107771873474, mae:0.27444273233413696, rse:0.516656219959259, corr:[0.28303105 0.29078597 0.2930822  0.29123202 0.29256588 0.29447296
 0.29416716 0.2950671  0.29497385 0.293225   0.29293787 0.29304424
 0.29287815 0.2932598  0.29269347 0.29161835 0.29190835 0.29241055
 0.29231185 0.2927661  0.2932612  0.29315236 0.29298422 0.2927123
 0.29324788 0.29383814 0.29435453 0.29412884 0.29422486 0.2941889
 0.29325053 0.2931861  0.2934586  0.29224205 0.29173175 0.2926793
 0.29271048 0.2924468  0.29287833 0.29273704 0.2926788  0.29346228
 0.2934833  0.29272798 0.2926051  0.29277575 0.29279327 0.2929552
 0.29295462 0.29251713 0.29270348 0.29263666 0.29236835 0.29303032
 0.2933862  0.2925441  0.29207703 0.2917264  0.2906857  0.2907586
 0.29174194 0.29186606 0.29230997 0.29312554 0.29293236 0.2928179
 0.29301098 0.2924759  0.29261324 0.2935522  0.2934673  0.29357427
 0.2940716  0.292832   0.29124624 0.29121256 0.29131645 0.29169828
 0.29268616 0.2922995  0.29133722 0.29154968 0.29145342 0.2913544
 0.2922699  0.29198208 0.29134175 0.29234028 0.2923325  0.29188368
 0.2931613  0.29209748 0.29017058 0.29080334 0.2884155  0.2943453 ]
