Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11298406400.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7045551
	speed: 0.8073s/iter; left time: 6741.7930s
Epoch: 1 cost time: 133.62411832809448
Epoch: 1, Steps: 169 | Train Loss: 0.8291066 Vali Loss: 0.7311894 Test Loss: 0.8523243
Validation loss decreased (inf --> 0.731189).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4479605
	speed: 1.9350s/iter; left time: 15831.8070s
Epoch: 2 cost time: 111.6508457660675
Epoch: 2, Steps: 169 | Train Loss: 0.4759238 Vali Loss: 0.5284963 Test Loss: 0.6131829
Validation loss decreased (0.731189 --> 0.528496).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3440392
	speed: 1.8765s/iter; left time: 15036.3581s
Epoch: 3 cost time: 117.94222831726074
Epoch: 3, Steps: 169 | Train Loss: 0.3543075 Vali Loss: 0.4422483 Test Loss: 0.5134428
Validation loss decreased (0.528496 --> 0.442248).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2940448
	speed: 1.8717s/iter; left time: 14681.6321s
Epoch: 4 cost time: 100.06563115119934
Epoch: 4, Steps: 169 | Train Loss: 0.3030971 Vali Loss: 0.4072965 Test Loss: 0.4717836
Validation loss decreased (0.442248 --> 0.407296).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2840138
	speed: 1.8996s/iter; left time: 14579.0739s
Epoch: 5 cost time: 126.02304124832153
Epoch: 5, Steps: 169 | Train Loss: 0.2827552 Vali Loss: 0.3943235 Test Loss: 0.4578343
Validation loss decreased (0.407296 --> 0.394324).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2671305
	speed: 2.0950s/iter; left time: 15724.7042s
Epoch: 6 cost time: 123.77419829368591
Epoch: 6, Steps: 169 | Train Loss: 0.2751674 Vali Loss: 0.3892765 Test Loss: 0.4531910
Validation loss decreased (0.394324 --> 0.389277).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2721292
	speed: 2.0346s/iter; left time: 14927.9355s
Epoch: 7 cost time: 124.96611428260803
Epoch: 7, Steps: 169 | Train Loss: 0.2724319 Vali Loss: 0.3871353 Test Loss: 0.4510687
Validation loss decreased (0.389277 --> 0.387135).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2748135
	speed: 2.0814s/iter; left time: 14919.2933s
Epoch: 8 cost time: 123.5513801574707
Epoch: 8, Steps: 169 | Train Loss: 0.2714354 Vali Loss: 0.3867238 Test Loss: 0.4507032
Validation loss decreased (0.387135 --> 0.386724).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2720343
	speed: 1.9285s/iter; left time: 13497.3611s
Epoch: 9 cost time: 105.14816880226135
Epoch: 9, Steps: 169 | Train Loss: 0.2710183 Vali Loss: 0.3866754 Test Loss: 0.4498680
Validation loss decreased (0.386724 --> 0.386675).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2700196
	speed: 1.7013s/iter; left time: 11620.0239s
Epoch: 10 cost time: 109.19549298286438
Epoch: 10, Steps: 169 | Train Loss: 0.2707849 Vali Loss: 0.3865164 Test Loss: 0.4504977
Validation loss decreased (0.386675 --> 0.386516).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2587762
	speed: 1.6516s/iter; left time: 11001.2309s
Epoch: 11 cost time: 85.92353749275208
Epoch: 11, Steps: 169 | Train Loss: 0.2706656 Vali Loss: 0.3864623 Test Loss: 0.4506707
Validation loss decreased (0.386516 --> 0.386462).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2788401
	speed: 1.5871s/iter; left time: 10303.2045s
Epoch: 12 cost time: 111.57564902305603
Epoch: 12, Steps: 169 | Train Loss: 0.2705553 Vali Loss: 0.3863026 Test Loss: 0.4494655
Validation loss decreased (0.386462 --> 0.386303).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2665837
	speed: 1.6415s/iter; left time: 10379.0067s
Epoch: 13 cost time: 97.23540449142456
Epoch: 13, Steps: 169 | Train Loss: 0.2704393 Vali Loss: 0.3851461 Test Loss: 0.4496147
Validation loss decreased (0.386303 --> 0.385146).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2777795
	speed: 1.7099s/iter; left time: 10522.5883s
Epoch: 14 cost time: 102.67253494262695
Epoch: 14, Steps: 169 | Train Loss: 0.2703990 Vali Loss: 0.3854006 Test Loss: 0.4497431
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2765620
	speed: 1.7868s/iter; left time: 10693.7951s
Epoch: 15 cost time: 120.9207329750061
Epoch: 15, Steps: 169 | Train Loss: 0.2703872 Vali Loss: 0.3856692 Test Loss: 0.4494192
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2718492
	speed: 2.0669s/iter; left time: 12021.0302s
Epoch: 16 cost time: 123.74573802947998
Epoch: 16, Steps: 169 | Train Loss: 0.2702760 Vali Loss: 0.3851053 Test Loss: 0.4499353
Validation loss decreased (0.385146 --> 0.385105).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2643470
	speed: 1.8994s/iter; left time: 10725.7619s
Epoch: 17 cost time: 121.84563255310059
Epoch: 17, Steps: 169 | Train Loss: 0.2702371 Vali Loss: 0.3857076 Test Loss: 0.4493143
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2732089
	speed: 1.8024s/iter; left time: 9873.4171s
Epoch: 18 cost time: 107.88609385490417
Epoch: 18, Steps: 169 | Train Loss: 0.2701810 Vali Loss: 0.3853480 Test Loss: 0.4500044
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2717248
	speed: 1.8905s/iter; left time: 10036.7243s
Epoch: 19 cost time: 111.19590330123901
Epoch: 19, Steps: 169 | Train Loss: 0.2701884 Vali Loss: 0.3852333 Test Loss: 0.4491002
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4494101405143738, mae:0.300204336643219, rse:0.5481787919998169, corr:[0.25600615 0.26554906 0.2632882  0.26566184 0.26753968 0.2681341
 0.27078325 0.26874807 0.27029255 0.26949507 0.26916692 0.26851913
 0.26698473 0.26767686 0.265957   0.26642874 0.2666042  0.26645795
 0.2673893  0.2669951  0.26787174 0.26779243 0.26785448 0.2676983
 0.26879656 0.2703864  0.26957533 0.2693461  0.26927468 0.26855376
 0.2694086  0.26874903 0.26869136 0.2684818  0.26757956 0.26766375
 0.26655865 0.26673192 0.26676548 0.2663438  0.2669503  0.26709056
 0.26774833 0.26757285 0.2676316  0.26821005 0.26777548 0.2680976
 0.26868197 0.26905173 0.26894587 0.26848587 0.2689499  0.26831612
 0.26820332 0.26823416 0.26733637 0.26735184 0.2666766  0.26650923
 0.26695347 0.2670068  0.26772395 0.26760632 0.2676861  0.26786736
 0.26782778 0.26840454 0.26825425 0.26805916 0.2677365  0.26758656
 0.26788712 0.267718   0.26808435 0.26780397 0.2672307  0.26723105
 0.26693237 0.26715195 0.26710114 0.2672591  0.26733562 0.26661682
 0.26701438 0.2671052  0.26686993 0.26711047 0.2673868  0.26848677
 0.26891243 0.26837078 0.26788336 0.2675002  0.2674848  0.26735327
 0.26761654 0.26756558 0.26723287 0.2676502  0.26726198 0.26703247
 0.26696938 0.26699722 0.26730093 0.26690388 0.26710638 0.26724237
 0.26712063 0.26753098 0.26721135 0.26736325 0.267623   0.26744384
 0.26815903 0.26812106 0.26758417 0.2669942  0.2667116  0.26703545
 0.26705292 0.26755932 0.26783156 0.2675751  0.2673821  0.2670294
 0.26714417 0.26706526 0.26678637 0.26683915 0.2668414  0.26701775
 0.2669736  0.2672192  0.2675875  0.2676039  0.2679396  0.2674151
 0.26718673 0.26723015 0.26723167 0.26734146 0.26681373 0.26684812
 0.2673489  0.26722428 0.26753944 0.26727456 0.26711938 0.26730117
 0.26692733 0.2668181  0.2667682  0.26739097 0.26789385 0.26782382
 0.26825917 0.26842546 0.26841938 0.26846567 0.2684499  0.26856428
 0.26863772 0.26902184 0.26909608 0.26848525 0.268154   0.26825806
 0.26990274 0.27032346 0.2700668  0.26986176 0.2693672  0.2692595
 0.26910576 0.26919985 0.26972896 0.2694978  0.26947224 0.26934725
 0.26905796 0.26928225 0.26934123 0.26960716 0.26983348 0.27020547
 0.2702884  0.26967192 0.26965672 0.26949492 0.26932606 0.26943514
 0.2699516  0.27029943 0.26984605 0.26910207 0.2689541  0.26894346
 0.2691698  0.26872987 0.2683849  0.26881894 0.26914594 0.26933384
 0.26905957 0.2692061  0.26950565 0.2692826  0.26961318 0.26973948
 0.2696512  0.26957273 0.26924103 0.26924115 0.269094   0.26887053
 0.2687692  0.2689364  0.26914567 0.26857203 0.2687982  0.26902896
 0.26868296 0.2686542  0.26820606 0.2683967  0.2691271  0.2690756
 0.26862296 0.26838627 0.2687636  0.2686614  0.26871046 0.26926452
 0.26876962 0.26862946 0.26906314 0.26882845 0.2686953  0.2687003
 0.26860893 0.26838732 0.26825717 0.26796606 0.2674391  0.2673094
 0.26752728 0.26810497 0.2684061  0.26885298 0.2693059  0.26870888
 0.2688986  0.26926932 0.26895595 0.26931903 0.2693188  0.26932448
 0.2692672  0.26862565 0.26842552 0.26796332 0.26782116 0.26786938
 0.26753396 0.26780882 0.26799548 0.26816908 0.26810524 0.2678127
 0.26806077 0.26825196 0.2684727  0.26869345 0.2687915  0.26909924
 0.2690227  0.26896164 0.26919642 0.26952857 0.26970676 0.26940596
 0.26951358 0.26932326 0.2684815  0.267679   0.26739952 0.26783675
 0.26801887 0.26868975 0.26931024 0.26873347 0.26832855 0.26821536
 0.268597   0.26914737 0.2685912  0.26852342 0.26887724 0.2689542
 0.269189   0.26921606 0.2693775  0.2693204  0.26938412 0.2697952
 0.26988712 0.2697695  0.26924798 0.26869944 0.26860318 0.26832741
 0.2685773  0.2685532  0.26865858 0.26888356 0.26840082 0.26863945
 0.26902416 0.2689208  0.26887137 0.26909307 0.2695848  0.26959234
 0.26980472 0.26992136 0.26962373 0.26983666 0.2698631  0.27019188
 0.27031526 0.2695677  0.26968095 0.2694453  0.268838   0.2690866
 0.27002817 0.2701529  0.2700667  0.27006406 0.2697399  0.26966715
 0.27011114 0.26988164 0.26995453 0.2702379  0.27052385 0.27124882
 0.27113947 0.27076596 0.27035105 0.27011693 0.27054444 0.27072918
 0.27095217 0.27082688 0.27037326 0.26998457 0.2695554  0.26989478
 0.2704608  0.2707083  0.27080515 0.2703283  0.2703465  0.27042007
 0.2704858  0.27025574 0.26962614 0.2701314  0.27072924 0.27103385
 0.2712697  0.27104056 0.2709703  0.2704529  0.27062994 0.2712221
 0.27054957 0.27037418 0.27051896 0.27052557 0.27061462 0.27025297
 0.27013114 0.2699983  0.27008992 0.26990888 0.26966614 0.27018383
 0.2705923  0.2710621  0.27089977 0.27012688 0.27029648 0.2705949
 0.27056214 0.27011865 0.26981714 0.2699165  0.2695457  0.2698117
 0.26984447 0.26938322 0.26985022 0.26979855 0.26940155 0.2691424
 0.26894858 0.2691183  0.2690858  0.26945135 0.26961    0.26998702
 0.2705995  0.2700507  0.26977128 0.26983848 0.2700104  0.2704508
 0.2703332  0.2699215  0.26916113 0.2689961  0.26953223 0.26965046
 0.26969266 0.2687796  0.26831686 0.26923525 0.26936495 0.26925856
 0.26912817 0.26905018 0.2693717  0.26932737 0.26927269 0.26938257
 0.27013436 0.2707356  0.27035585 0.27062765 0.27056554 0.27018458
 0.27029487 0.26990268 0.26993594 0.26992765 0.26960897 0.26928526
 0.26860327 0.26839173 0.2680712  0.26790208 0.26830435 0.26816767
 0.26857135 0.2692069  0.26922458 0.2690574  0.26875636 0.26925457
 0.26978034 0.2697987  0.26946965 0.26937428 0.27002022 0.27000642
 0.27008596 0.2703967  0.27029455 0.27046683 0.27002162 0.26954266
 0.26935014 0.26895675 0.26902628 0.2688918  0.2688399  0.26921728
 0.2697123  0.27000618 0.2700294  0.2701635  0.27006665 0.2700471
 0.27017856 0.26969624 0.26934522 0.26945156 0.2696709  0.2693437
 0.26904008 0.26979506 0.27001867 0.26991406 0.26985836 0.26954195
 0.2699309  0.26981676 0.26943225 0.2695552  0.26966333 0.2701615
 0.27106762 0.2712975  0.2709699  0.2703706  0.27037    0.2702048
 0.27015528 0.26986858 0.2695232  0.270217   0.27044597 0.27053955
 0.2707129  0.27032727 0.2703843  0.27025357 0.27020234 0.2701377
 0.26961455 0.26971844 0.2696946  0.26944146 0.26894027 0.26851654
 0.26970664 0.2703021  0.27018872 0.27039573 0.27060935 0.27056643
 0.2704628  0.27035046 0.26961866 0.2695326  0.27000973 0.2697468
 0.2699734  0.26978442 0.26910856 0.26894435 0.26881814 0.26929376
 0.26875687 0.26806006 0.26895726 0.2692815  0.26912203 0.26872614
 0.26851586 0.26867127 0.26879936 0.26961976 0.27000734 0.27021408
 0.2703021  0.26988629 0.27004084 0.2697539  0.26979375 0.26986128
 0.2690474  0.26897013 0.26848945 0.26803902 0.26799288 0.26775676
 0.26802903 0.2677482  0.26800078 0.26843455 0.26784757 0.2677886
 0.26776296 0.26803035 0.26826367 0.26829448 0.2687084  0.2682678
 0.26861045 0.26898247 0.2681542  0.26802954 0.26786307 0.2681022
 0.26824418 0.26750562 0.2673471  0.26722786 0.26761597 0.2676866
 0.26705977 0.26693356 0.26656374 0.26675585 0.26670998 0.2666761
 0.26740074 0.26734206 0.26741064 0.26715758 0.2670474  0.2677215
 0.26804218 0.26841348 0.26783344 0.26734263 0.2673627  0.26658937
 0.26659527 0.26614442 0.2657529  0.26625988 0.26634666 0.2668638
 0.2668057  0.26689848 0.2670214  0.26653516 0.26709515 0.26699513
 0.2669921  0.26753584 0.26734605 0.26760903 0.2671803  0.26704037
 0.2672739  0.26713917 0.26733047 0.26697832 0.26721597 0.26731396
 0.26706356 0.26724923 0.2665177  0.26681584 0.267207   0.26717812
 0.26772293 0.2670634  0.26696333 0.26711842 0.26720038 0.26792705
 0.2681096  0.2686426  0.26869527 0.26849607 0.2687155  0.26823005
 0.26860112 0.26828334 0.2675377  0.26771414 0.2675535  0.2676567
 0.2668148  0.26673546 0.26700488 0.2661952  0.2670595  0.2674386
 0.26765755 0.26763117 0.2673129  0.2682566  0.26785    0.26816198
 0.2696385  0.26983517 0.27045625 0.26992968 0.2701486  0.27008682
 0.2693561  0.2693005  0.2679279  0.2683828  0.26856726 0.2683343
 0.269164   0.26762584 0.26789644 0.26837844 0.26873273 0.2696381
 0.26900104 0.27048206 0.27063587 0.27030823 0.27062303 0.2694397
 0.27043524 0.2704619  0.2707294  0.270748   0.26978648 0.27044478
 0.26929495 0.26907614 0.2675582  0.2667708  0.26844713 0.26742575
 0.26967674 0.26947507 0.27008277 0.27150908 0.27073234 0.2734328
 0.27057692 0.27125502 0.2679473  0.26542863 0.26397282 0.27110603]
