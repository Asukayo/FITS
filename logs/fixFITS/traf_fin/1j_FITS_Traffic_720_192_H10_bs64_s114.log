Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14299545600.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 71.51240420341492
Epoch: 1, Steps: 88 | Train Loss: 0.7650930 Vali Loss: 0.6210802 Test Loss: 0.7114629
Validation loss decreased (inf --> 0.621080).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 76.59621810913086
Epoch: 2, Steps: 88 | Train Loss: 0.3832807 Vali Loss: 0.4140675 Test Loss: 0.4842838
Validation loss decreased (0.621080 --> 0.414067).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 76.51742005348206
Epoch: 3, Steps: 88 | Train Loss: 0.2780552 Vali Loss: 0.3527867 Test Loss: 0.4202823
Validation loss decreased (0.414067 --> 0.352787).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 84.09463930130005
Epoch: 4, Steps: 88 | Train Loss: 0.2486451 Vali Loss: 0.3364838 Test Loss: 0.4048944
Validation loss decreased (0.352787 --> 0.336484).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 74.3937726020813
Epoch: 5, Steps: 88 | Train Loss: 0.2413256 Vali Loss: 0.3322016 Test Loss: 0.4014718
Validation loss decreased (0.336484 --> 0.332202).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 75.90224838256836
Epoch: 6, Steps: 88 | Train Loss: 0.2393368 Vali Loss: 0.3303794 Test Loss: 0.4003264
Validation loss decreased (0.332202 --> 0.330379).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 73.72045350074768
Epoch: 7, Steps: 88 | Train Loss: 0.2386087 Vali Loss: 0.3298269 Test Loss: 0.3998908
Validation loss decreased (0.330379 --> 0.329827).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 69.45634198188782
Epoch: 8, Steps: 88 | Train Loss: 0.2382587 Vali Loss: 0.3293386 Test Loss: 0.3998444
Validation loss decreased (0.329827 --> 0.329339).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 71.02899765968323
Epoch: 9, Steps: 88 | Train Loss: 0.2379727 Vali Loss: 0.3289347 Test Loss: 0.3993053
Validation loss decreased (0.329339 --> 0.328935).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 68.98070478439331
Epoch: 10, Steps: 88 | Train Loss: 0.2378464 Vali Loss: 0.3284768 Test Loss: 0.3992384
Validation loss decreased (0.328935 --> 0.328477).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 65.38446021080017
Epoch: 11, Steps: 88 | Train Loss: 0.2377803 Vali Loss: 0.3285660 Test Loss: 0.3991546
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 73.94257473945618
Epoch: 12, Steps: 88 | Train Loss: 0.2375997 Vali Loss: 0.3275823 Test Loss: 0.3987817
Validation loss decreased (0.328477 --> 0.327582).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 68.81032395362854
Epoch: 13, Steps: 88 | Train Loss: 0.2375380 Vali Loss: 0.3284574 Test Loss: 0.3988953
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 64.10965013504028
Epoch: 14, Steps: 88 | Train Loss: 0.2375046 Vali Loss: 0.3280170 Test Loss: 0.3988211
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 67.34620904922485
Epoch: 15, Steps: 88 | Train Loss: 0.2372694 Vali Loss: 0.3277798 Test Loss: 0.3984677
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.39783015847206116, mae:0.2742597162723541, rse:0.5205685496330261, corr:[0.27732024 0.28821895 0.28705978 0.28725538 0.28957418 0.2892489
 0.29215446 0.2908898  0.29211852 0.2910567  0.2903904  0.29059416
 0.2890082  0.29006168 0.28892568 0.2886751  0.28868923 0.28778037
 0.28825808 0.28771076 0.28834578 0.28845328 0.28839022 0.28874135
 0.29018265 0.29159278 0.2909992  0.29099432 0.2915123  0.2906952
 0.2912321  0.2903042  0.29002145 0.29012492 0.28885588 0.28903323
 0.2884509  0.28863102 0.28892058 0.28845662 0.2890833  0.28909573
 0.28940398 0.28919432 0.28865847 0.28861845 0.28824845 0.28932762
 0.28979504 0.28935122 0.28970978 0.28974578 0.29058793 0.2901944
 0.28972706 0.28969973 0.28877234 0.2889111  0.28793502 0.2872824
 0.28773755 0.28768182 0.28860518 0.2885717  0.28885296 0.28929824
 0.28875676 0.28885943 0.28844687 0.28856045 0.28860447 0.28838116
 0.28867492 0.28796268 0.28803802 0.28822923 0.2883862  0.28910813
 0.2886215  0.28866857 0.2883125  0.28748927 0.28735572 0.28641897
 0.28660887 0.28698757 0.28697228 0.2869633  0.2865659  0.28754205
 0.28772363 0.2875601  0.2880859  0.2879081  0.2882011  0.28823823
 0.28817993 0.2881156  0.28748187 0.28798622 0.28784713 0.2880941
 0.2880954  0.28700143 0.2871574  0.28688392 0.28689474 0.28690213
 0.2864798  0.28728166 0.2871914  0.28716046 0.28716013 0.28692988
 0.28748608 0.28693572 0.2869999  0.28738257 0.28746837 0.288389
 0.28848094 0.28866595 0.28820902 0.28788623 0.28816584 0.28771675
 0.28823292 0.28760195 0.2868643  0.28740627 0.28674603 0.28696564
 0.28721482 0.2876944  0.28850555 0.2877245  0.28740475 0.28701535
 0.28706923 0.28748754 0.2875534  0.28791043 0.28738844 0.28834343
 0.28921103 0.28830183 0.28823605 0.28758428 0.28785098 0.2881375
 0.2879631  0.28863245 0.28773138 0.2879819  0.28808567 0.2876176
 0.28845096 0.28842002 0.28948358 0.28937528 0.28858665 0.28886902
 0.2880973  0.2885008  0.28841507 0.28824908 0.28815907 0.28743702
 0.29004797 0.2900783  0.28926632 0.28917533 0.28882352 0.2892685
 0.28788075 0.28833586 0.28830385 0.28766134 0.2892879  0.2889813
 0.29067826 0.29063216 0.29038632 0.29091024 0.28903633 0.289999
 0.2873621  0.28802145 0.2865148  0.28551248 0.28659913 0.2918672 ]
