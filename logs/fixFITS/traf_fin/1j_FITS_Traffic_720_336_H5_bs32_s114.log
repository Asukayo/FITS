Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j336_H5_FITS_custom_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2193755520.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5781810
	speed: 0.6591s/iter; left time: 5702.0834s
Epoch: 1 cost time: 114.51216292381287
Epoch: 1, Steps: 175 | Train Loss: 0.6978949 Vali Loss: 0.5472774 Test Loss: 0.6327562
Validation loss decreased (inf --> 0.547277).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3217266
	speed: 1.8059s/iter; left time: 15306.4750s
Epoch: 2 cost time: 108.4588611125946
Epoch: 2, Steps: 175 | Train Loss: 0.3364729 Vali Loss: 0.3882455 Test Loss: 0.4594589
Validation loss decreased (0.547277 --> 0.388245).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2681628
	speed: 1.7178s/iter; left time: 14259.3857s
Epoch: 3 cost time: 105.97806715965271
Epoch: 3, Steps: 175 | Train Loss: 0.2686349 Vali Loss: 0.3570501 Test Loss: 0.4284405
Validation loss decreased (0.388245 --> 0.357050).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2605514
	speed: 1.7018s/iter; left time: 13828.8117s
Epoch: 4 cost time: 100.71646738052368
Epoch: 4, Steps: 175 | Train Loss: 0.2567737 Vali Loss: 0.3510800 Test Loss: 0.4243438
Validation loss decreased (0.357050 --> 0.351080).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2499421
	speed: 1.7731s/iter; left time: 14098.2525s
Epoch: 5 cost time: 112.03006052970886
Epoch: 5, Steps: 175 | Train Loss: 0.2549123 Vali Loss: 0.3495606 Test Loss: 0.4239659
Validation loss decreased (0.351080 --> 0.349561).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2516398
	speed: 1.7429s/iter; left time: 13552.6610s
Epoch: 6 cost time: 107.99109697341919
Epoch: 6, Steps: 175 | Train Loss: 0.2544545 Vali Loss: 0.3485633 Test Loss: 0.4236046
Validation loss decreased (0.349561 --> 0.348563).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2488954
	speed: 1.7542s/iter; left time: 13333.9168s
Epoch: 7 cost time: 103.9847526550293
Epoch: 7, Steps: 175 | Train Loss: 0.2543077 Vali Loss: 0.3485707 Test Loss: 0.4235243
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2628812
	speed: 1.6486s/iter; left time: 12242.4677s
Epoch: 8 cost time: 104.83392143249512
Epoch: 8, Steps: 175 | Train Loss: 0.2542220 Vali Loss: 0.3481196 Test Loss: 0.4234618
Validation loss decreased (0.348563 --> 0.348120).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2528131
	speed: 1.6764s/iter; left time: 12155.2975s
Epoch: 9 cost time: 97.44887018203735
Epoch: 9, Steps: 175 | Train Loss: 0.2541376 Vali Loss: 0.3484960 Test Loss: 0.4233384
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2673835
	speed: 1.5648s/iter; left time: 11072.2029s
Epoch: 10 cost time: 103.02676439285278
Epoch: 10, Steps: 175 | Train Loss: 0.2540702 Vali Loss: 0.3479926 Test Loss: 0.4230436
Validation loss decreased (0.348120 --> 0.347993).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2470795
	speed: 1.6725s/iter; left time: 11542.2105s
Epoch: 11 cost time: 87.0684564113617
Epoch: 11, Steps: 175 | Train Loss: 0.2540516 Vali Loss: 0.3481374 Test Loss: 0.4232871
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2584491
	speed: 1.1864s/iter; left time: 7979.8598s
Epoch: 12 cost time: 66.15303134918213
Epoch: 12, Steps: 175 | Train Loss: 0.2539962 Vali Loss: 0.3481042 Test Loss: 0.4232021
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2537412
	speed: 1.2359s/iter; left time: 8096.0692s
Epoch: 13 cost time: 98.91215014457703
Epoch: 13, Steps: 175 | Train Loss: 0.2540121 Vali Loss: 0.3480667 Test Loss: 0.4231824
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j336_H5_FITS_custom_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4207296371459961, mae:0.29318419098854065, rse:0.533090353012085, corr:[0.28457987 0.2861832  0.28584927 0.28828102 0.28970233 0.28914735
 0.2882895  0.28836504 0.2889033  0.28875875 0.2877698  0.28689176
 0.28689873 0.2873736  0.28743404 0.28684393 0.2862565  0.28632572
 0.2869248  0.2873588  0.28713527 0.28648728 0.2862084  0.2870994
 0.2886064  0.28889477 0.28855333 0.28838307 0.2884994  0.2886236
 0.28846958 0.2880171  0.28762376 0.28754735 0.28752422 0.28729984
 0.28703898 0.28705692 0.28749084 0.28807256 0.28839204 0.288304
 0.28810918 0.2880446  0.28820452 0.28835323 0.2883329  0.28843424
 0.28868496 0.28867245 0.28856394 0.2883373  0.28801835 0.28784978
 0.28798142 0.28823858 0.2882811  0.28798112 0.2875228  0.28728685
 0.28750414 0.28790691 0.28812847 0.28800473 0.28759202 0.28711554
 0.28688574 0.28664082 0.28636768 0.28623608 0.28632632 0.28663033
 0.28685972 0.28697938 0.2871548  0.2873915  0.2875717  0.28755477
 0.28736952 0.2871976  0.2871656  0.28715703 0.28694794 0.2865739
 0.28643256 0.28683916 0.28770205 0.2884849  0.28868443 0.2882342
 0.28752482 0.28688076 0.28655028 0.28651482 0.28660405 0.28675964
 0.2868589  0.2869952  0.2869778  0.28667775 0.28632307 0.28618565
 0.28630656 0.2865247  0.28668448 0.28677163 0.28687966 0.2870385
 0.2872224  0.2873815  0.2875018  0.28761557 0.28768015 0.28759846
 0.2873138  0.2868338  0.28638342 0.2861225  0.28618047 0.28632727
 0.28631318 0.28629628 0.28645036 0.28675187 0.28717914 0.28749526
 0.28753582 0.28739235 0.28725448 0.28718954 0.28714833 0.28711247
 0.28715426 0.28738692 0.28780276 0.28818884 0.28830194 0.2880565
 0.28763986 0.2872902  0.28711966 0.28708884 0.28706056 0.28710073
 0.28732386 0.28760123 0.28789824 0.28807348 0.28803918 0.2878597
 0.28767967 0.28757313 0.28744224 0.28721106 0.28699848 0.28703374
 0.28734735 0.28767437 0.28774357 0.28761217 0.28757685 0.28770465
 0.28778163 0.2875704  0.2871581  0.2869085  0.28710347 0.28797537
 0.28910875 0.28917494 0.28886524 0.28871825 0.28867587 0.2886215
 0.28854993 0.28859252 0.2888188  0.289058   0.2890083  0.28862724
 0.2881443  0.2877226  0.28745535 0.28730908 0.28726137 0.2873353
 0.28747118 0.28761005 0.28756922 0.28723437 0.2868606  0.28706113
 0.2877795  0.28795186 0.2877806  0.28755432 0.2873964  0.28744397
 0.28769967 0.28802013 0.28827208 0.28840843 0.28839698 0.28832427
 0.28825268 0.28808004 0.2877742  0.28739527 0.28709936 0.28702512
 0.2871292  0.2872508  0.2872478  0.28711608 0.28699988 0.28710374
 0.2872421  0.28715056 0.28711462 0.28733224 0.28767067 0.28782085
 0.2876219  0.28728744 0.2871935  0.28739902 0.28753063 0.28733483
 0.28701428 0.28686732 0.28697813 0.2870758  0.28688002 0.2864209
 0.28597748 0.28578764 0.28588265 0.2858594  0.28563544 0.28542897
 0.28546864 0.28579566 0.28623152 0.2865101  0.28658515 0.28659943
 0.28668237 0.2868345  0.28693685 0.286854   0.28656358 0.28618476
 0.28586826 0.28564176 0.28551006 0.28550464 0.28563547 0.28584337
 0.28594786 0.2859235  0.28596815 0.28620908 0.2864994  0.28657442
 0.2862862  0.2859722  0.2859681  0.28623265 0.2865281  0.28660822
 0.28644195 0.28628978 0.2863856  0.286645   0.28674382 0.2864506
 0.2859327  0.28555214 0.2855544  0.2858519  0.28618994 0.28640753
 0.28650078 0.28656694 0.28659183 0.28645957 0.28615183 0.28575602
 0.2854645  0.2856559  0.28613505 0.28637066 0.28622422 0.28588915
 0.28563967 0.2855716  0.28553745 0.28540775 0.28530923 0.28539452
 0.28552347 0.28543207 0.2851491  0.2850535  0.28545597 0.2861461
 0.2866246  0.28667986 0.28660828 0.28671333 0.28691715 0.28691494
 0.28657025 0.28613782 0.2861909  0.2866199  0.2868015  0.28637317
 0.285721   0.28551942 0.2858244  0.2859808  0.28554243 0.28499067
 0.28521067 0.28626978 0.2871642  0.2870013  0.2862291  0.28614187
 0.28709683 0.28760868 0.28578246 0.28230134 0.28197414 0.28821847]
