Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j720_H5_FITS_custom_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3003897600.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.9882638
	speed: 0.6922s/iter; left time: 5780.2063s
Epoch: 1 cost time: 114.7764093875885
Epoch: 1, Steps: 169 | Train Loss: 1.1036080 Vali Loss: 1.1067333 Test Loss: 1.3002410
Validation loss decreased (inf --> 1.106733).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7391626
	speed: 1.7499s/iter; left time: 14318.0625s
Epoch: 2 cost time: 103.66040992736816
Epoch: 2, Steps: 169 | Train Loss: 0.7669668 Vali Loss: 0.9619141 Test Loss: 1.1254334
Validation loss decreased (1.106733 --> 0.961914).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6335943
	speed: 1.6300s/iter; left time: 13061.0648s
Epoch: 3 cost time: 98.97828531265259
Epoch: 3, Steps: 169 | Train Loss: 0.6468961 Vali Loss: 0.8650201 Test Loss: 1.0113270
Validation loss decreased (0.961914 --> 0.865020).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5484116
	speed: 1.6929s/iter; left time: 13278.8827s
Epoch: 4 cost time: 112.17904758453369
Epoch: 4, Steps: 169 | Train Loss: 0.5600735 Vali Loss: 0.7869042 Test Loss: 0.9199911
Validation loss decreased (0.865020 --> 0.786904).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4843269
	speed: 1.8749s/iter; left time: 14390.2250s
Epoch: 5 cost time: 107.8670265674591
Epoch: 5, Steps: 169 | Train Loss: 0.4918806 Vali Loss: 0.7250069 Test Loss: 0.8473044
Validation loss decreased (0.786904 --> 0.725007).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4310335
	speed: 1.6894s/iter; left time: 12680.6636s
Epoch: 6 cost time: 103.98751616477966
Epoch: 6, Steps: 169 | Train Loss: 0.4367859 Vali Loss: 0.6721832 Test Loss: 0.7852305
Validation loss decreased (0.725007 --> 0.672183).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3943975
	speed: 1.7238s/iter; left time: 12647.2840s
Epoch: 7 cost time: 103.920574426651
Epoch: 7, Steps: 169 | Train Loss: 0.3917753 Vali Loss: 0.6264715 Test Loss: 0.7325544
Validation loss decreased (0.672183 --> 0.626472).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3552146
	speed: 1.8065s/iter; left time: 12949.0292s
Epoch: 8 cost time: 115.60032510757446
Epoch: 8, Steps: 169 | Train Loss: 0.3544961 Vali Loss: 0.5891675 Test Loss: 0.6880554
Validation loss decreased (0.626472 --> 0.589168).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3209094
	speed: 1.7105s/iter; left time: 11971.7583s
Epoch: 9 cost time: 101.16754269599915
Epoch: 9, Steps: 169 | Train Loss: 0.3235098 Vali Loss: 0.5579772 Test Loss: 0.6514613
Validation loss decreased (0.589168 --> 0.557977).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2912416
	speed: 1.7119s/iter; left time: 11691.9912s
Epoch: 10 cost time: 110.81256675720215
Epoch: 10, Steps: 169 | Train Loss: 0.2976198 Vali Loss: 0.5313171 Test Loss: 0.6189722
Validation loss decreased (0.557977 --> 0.531317).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2830912
	speed: 1.7078s/iter; left time: 11375.5094s
Epoch: 11 cost time: 100.9438545703888
Epoch: 11, Steps: 169 | Train Loss: 0.2758648 Vali Loss: 0.5085406 Test Loss: 0.5924503
Validation loss decreased (0.531317 --> 0.508541).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2525971
	speed: 1.7944s/iter; left time: 11649.4227s
Epoch: 12 cost time: 112.57984495162964
Epoch: 12, Steps: 169 | Train Loss: 0.2575818 Vali Loss: 0.4895617 Test Loss: 0.5701869
Validation loss decreased (0.508541 --> 0.489562).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2331940
	speed: 1.6866s/iter; left time: 10664.2866s
Epoch: 13 cost time: 109.94706439971924
Epoch: 13, Steps: 169 | Train Loss: 0.2421990 Vali Loss: 0.4742267 Test Loss: 0.5532265
Validation loss decreased (0.489562 --> 0.474227).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2274618
	speed: 1.8364s/iter; left time: 11301.1203s
Epoch: 14 cost time: 109.90157389640808
Epoch: 14, Steps: 169 | Train Loss: 0.2291994 Vali Loss: 0.4612273 Test Loss: 0.5370439
Validation loss decreased (0.474227 --> 0.461227).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2171411
	speed: 1.7290s/iter; left time: 10347.9738s
Epoch: 15 cost time: 105.37535405158997
Epoch: 15, Steps: 169 | Train Loss: 0.2182173 Vali Loss: 0.4501214 Test Loss: 0.5241074
Validation loss decreased (0.461227 --> 0.450121).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2122943
	speed: 1.8781s/iter; left time: 10922.7694s
Epoch: 16 cost time: 109.1493628025055
Epoch: 16, Steps: 169 | Train Loss: 0.2089663 Vali Loss: 0.4403811 Test Loss: 0.5127210
Validation loss decreased (0.450121 --> 0.440381).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2006196
	speed: 1.7980s/iter; left time: 10153.2650s
Epoch: 17 cost time: 115.94752979278564
Epoch: 17, Steps: 169 | Train Loss: 0.2011352 Vali Loss: 0.4326339 Test Loss: 0.5038400
Validation loss decreased (0.440381 --> 0.432634).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1909840
	speed: 1.9012s/iter; left time: 10414.5680s
Epoch: 18 cost time: 113.78881907463074
Epoch: 18, Steps: 169 | Train Loss: 0.1945030 Vali Loss: 0.4254939 Test Loss: 0.4958024
Validation loss decreased (0.432634 --> 0.425494).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1809738
	speed: 1.8966s/iter; left time: 10069.1012s
Epoch: 19 cost time: 113.64626097679138
Epoch: 19, Steps: 169 | Train Loss: 0.1889291 Vali Loss: 0.4199933 Test Loss: 0.4892882
Validation loss decreased (0.425494 --> 0.419993).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1829248
	speed: 1.8163s/iter; left time: 9335.6466s
Epoch: 20 cost time: 109.55397582054138
Epoch: 20, Steps: 169 | Train Loss: 0.1841800 Vali Loss: 0.4152419 Test Loss: 0.4834775
Validation loss decreased (0.419993 --> 0.415242).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1802534
	speed: 1.7796s/iter; left time: 8846.2739s
Epoch: 21 cost time: 106.5082802772522
Epoch: 21, Steps: 169 | Train Loss: 0.1802114 Vali Loss: 0.4108391 Test Loss: 0.4785926
Validation loss decreased (0.415242 --> 0.410839).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1762307
	speed: 1.7251s/iter; left time: 8284.1602s
Epoch: 22 cost time: 109.15182447433472
Epoch: 22, Steps: 169 | Train Loss: 0.1768380 Vali Loss: 0.4083601 Test Loss: 0.4751214
Validation loss decreased (0.410839 --> 0.408360).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1748458
	speed: 1.7855s/iter; left time: 8272.1373s
Epoch: 23 cost time: 102.44795489311218
Epoch: 23, Steps: 169 | Train Loss: 0.1739945 Vali Loss: 0.4053920 Test Loss: 0.4719987
Validation loss decreased (0.408360 --> 0.405392).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1741854
	speed: 1.9573s/iter; left time: 8737.4290s
Epoch: 24 cost time: 127.48327088356018
Epoch: 24, Steps: 169 | Train Loss: 0.1715916 Vali Loss: 0.4028961 Test Loss: 0.4687667
Validation loss decreased (0.405392 --> 0.402896).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1645778
	speed: 1.8142s/iter; left time: 7791.8022s
Epoch: 25 cost time: 107.34176659584045
Epoch: 25, Steps: 169 | Train Loss: 0.1696293 Vali Loss: 0.4008584 Test Loss: 0.4666524
Validation loss decreased (0.402896 --> 0.400858).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1687487
	speed: 1.8884s/iter; left time: 7791.3534s
Epoch: 26 cost time: 118.64114904403687
Epoch: 26, Steps: 169 | Train Loss: 0.1679335 Vali Loss: 0.3994689 Test Loss: 0.4652387
Validation loss decreased (0.400858 --> 0.399469).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1686858
	speed: 1.7435s/iter; left time: 6899.1682s
Epoch: 27 cost time: 103.08060479164124
Epoch: 27, Steps: 169 | Train Loss: 0.1665332 Vali Loss: 0.3975166 Test Loss: 0.4634030
Validation loss decreased (0.399469 --> 0.397517).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1625693
	speed: 1.7861s/iter; left time: 6765.8397s
Epoch: 28 cost time: 109.26094436645508
Epoch: 28, Steps: 169 | Train Loss: 0.1653782 Vali Loss: 0.3972053 Test Loss: 0.4621809
Validation loss decreased (0.397517 --> 0.397205).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1604763
	speed: 1.8514s/iter; left time: 6700.0425s
Epoch: 29 cost time: 116.04455208778381
Epoch: 29, Steps: 169 | Train Loss: 0.1644177 Vali Loss: 0.3958758 Test Loss: 0.4611382
Validation loss decreased (0.397205 --> 0.395876).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1618359
	speed: 1.8289s/iter; left time: 6309.8610s
Epoch: 30 cost time: 111.01161122322083
Epoch: 30, Steps: 169 | Train Loss: 0.1635895 Vali Loss: 0.3952143 Test Loss: 0.4605256
Validation loss decreased (0.395876 --> 0.395214).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1618587
	speed: 1.9174s/iter; left time: 6290.8758s
Epoch: 31 cost time: 117.75704789161682
Epoch: 31, Steps: 169 | Train Loss: 0.1628978 Vali Loss: 0.3947343 Test Loss: 0.4596091
Validation loss decreased (0.395214 --> 0.394734).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1652926
	speed: 1.8449s/iter; left time: 5741.4220s
Epoch: 32 cost time: 111.86711478233337
Epoch: 32, Steps: 169 | Train Loss: 0.1623653 Vali Loss: 0.3944811 Test Loss: 0.4591080
Validation loss decreased (0.394734 --> 0.394481).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1607720
	speed: 1.7668s/iter; left time: 5199.6047s
Epoch: 33 cost time: 102.28398513793945
Epoch: 33, Steps: 169 | Train Loss: 0.1619305 Vali Loss: 0.3937953 Test Loss: 0.4589527
Validation loss decreased (0.394481 --> 0.393795).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1607618
	speed: 1.6240s/iter; left time: 4504.9894s
Epoch: 34 cost time: 109.5270631313324
Epoch: 34, Steps: 169 | Train Loss: 0.1615441 Vali Loss: 0.3933977 Test Loss: 0.4584066
Validation loss decreased (0.393795 --> 0.393398).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1615975
	speed: 1.9302s/iter; left time: 5028.1538s
Epoch: 35 cost time: 110.69957375526428
Epoch: 35, Steps: 169 | Train Loss: 0.1612499 Vali Loss: 0.3934044 Test Loss: 0.4579702
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1606779
	speed: 1.5511s/iter; left time: 3778.5644s
Epoch: 36 cost time: 94.70175313949585
Epoch: 36, Steps: 169 | Train Loss: 0.1610029 Vali Loss: 0.3928922 Test Loss: 0.4580760
Validation loss decreased (0.393398 --> 0.392892).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1599589
	speed: 1.6667s/iter; left time: 3778.5171s
Epoch: 37 cost time: 111.1851909160614
Epoch: 37, Steps: 169 | Train Loss: 0.1608020 Vali Loss: 0.3931040 Test Loss: 0.4577033
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1613707
	speed: 1.8222s/iter; left time: 3822.8829s
Epoch: 38 cost time: 103.5075752735138
Epoch: 38, Steps: 169 | Train Loss: 0.1606264 Vali Loss: 0.3925867 Test Loss: 0.4576048
Validation loss decreased (0.392892 --> 0.392587).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1592128
	speed: 1.8147s/iter; left time: 3500.4821s
Epoch: 39 cost time: 117.82236123085022
Epoch: 39, Steps: 169 | Train Loss: 0.1604986 Vali Loss: 0.3923220 Test Loss: 0.4574286
Validation loss decreased (0.392587 --> 0.392322).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1634560
	speed: 1.7824s/iter; left time: 3136.9844s
Epoch: 40 cost time: 111.47375917434692
Epoch: 40, Steps: 169 | Train Loss: 0.1604025 Vali Loss: 0.3927476 Test Loss: 0.4575787
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1593932
	speed: 1.9495s/iter; left time: 3101.6752s
Epoch: 41 cost time: 111.33774828910828
Epoch: 41, Steps: 169 | Train Loss: 0.1603390 Vali Loss: 0.3931206 Test Loss: 0.4574751
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1549825
	speed: 1.6835s/iter; left time: 2393.9556s
Epoch: 42 cost time: 104.819491147995
Epoch: 42, Steps: 169 | Train Loss: 0.1602521 Vali Loss: 0.3927873 Test Loss: 0.4574326
EarlyStopping counter: 3 out of 3
Early stopping
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3003897600.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2785806
	speed: 0.6304s/iter; left time: 5264.2143s
Epoch: 1 cost time: 111.38237690925598
Epoch: 1, Steps: 169 | Train Loss: 0.2755513 Vali Loss: 0.3926669 Test Loss: 0.4575621
Validation loss decreased (inf --> 0.392667).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2643604
	speed: 1.9417s/iter; left time: 15887.2625s
Epoch: 2 cost time: 118.09921002388
Epoch: 2, Steps: 169 | Train Loss: 0.2753377 Vali Loss: 0.3924961 Test Loss: 0.4570968
Validation loss decreased (0.392667 --> 0.392496).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2807958
	speed: 1.8919s/iter; left time: 15159.8082s
Epoch: 3 cost time: 122.68108248710632
Epoch: 3, Steps: 169 | Train Loss: 0.2751582 Vali Loss: 0.3924074 Test Loss: 0.4566609
Validation loss decreased (0.392496 --> 0.392407).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2789841
	speed: 1.9759s/iter; left time: 15498.5838s
Epoch: 4 cost time: 119.83384251594543
Epoch: 4, Steps: 169 | Train Loss: 0.2751450 Vali Loss: 0.3926654 Test Loss: 0.4580045
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2813611
	speed: 1.9176s/iter; left time: 14717.4080s
Epoch: 5 cost time: 114.9973726272583
Epoch: 5, Steps: 169 | Train Loss: 0.2751040 Vali Loss: 0.3920354 Test Loss: 0.4572978
Validation loss decreased (0.392407 --> 0.392035).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2737526
	speed: 1.8442s/iter; left time: 13842.9314s
Epoch: 6 cost time: 112.80705690383911
Epoch: 6, Steps: 169 | Train Loss: 0.2750283 Vali Loss: 0.3928436 Test Loss: 0.4567222
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2707776
	speed: 1.7461s/iter; left time: 12811.0736s
Epoch: 7 cost time: 110.43766021728516
Epoch: 7, Steps: 169 | Train Loss: 0.2750220 Vali Loss: 0.3923095 Test Loss: 0.4568990
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2721266
	speed: 1.8133s/iter; left time: 12997.6788s
Epoch: 8 cost time: 113.88128709793091
Epoch: 8, Steps: 169 | Train Loss: 0.2749813 Vali Loss: 0.3922879 Test Loss: 0.4566245
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j720_H5_FITS_custom_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4568037986755371, mae:0.31074148416519165, rse:0.5526697039604187, corr:[0.26491624 0.2708159  0.27054858 0.2702457  0.27009085 0.26986206
 0.26971814 0.26989344 0.2703042  0.27065438 0.27074975 0.27061436
 0.27041173 0.27023843 0.27019322 0.27026257 0.27038157 0.27052075
 0.27069023 0.27087015 0.27099368 0.27092353 0.2708226  0.2713174
 0.27221936 0.2721641  0.27173504 0.27144465 0.27137548 0.27148768
 0.27164784 0.2717046  0.27164775 0.27158108 0.27154803 0.27152747
 0.2715539  0.27157953 0.27157477 0.27152798 0.27149102 0.271506
 0.27154168 0.2715494  0.2714809  0.27126744 0.27099356 0.27105984
 0.27136156 0.27131832 0.27114657 0.27098408 0.27093273 0.27107272
 0.27130923 0.27146882 0.27154708 0.27168074 0.2719154  0.27210602
 0.27210838 0.2718224  0.27145582 0.27120373 0.2710651  0.27092323
 0.27070794 0.27051553 0.27045065 0.2705144  0.2706164  0.2707544
 0.27063772 0.27021962 0.2697197  0.2693231  0.2692742  0.26963347
 0.27016434 0.27056625 0.2707646  0.27091253 0.27093408 0.2707394
 0.27034613 0.269932   0.26980686 0.2701549  0.27083996 0.27141833
 0.2717766  0.27168417 0.27133563 0.27099684 0.2707205  0.2704937
 0.27012053 0.26980773 0.26965746 0.26964352 0.2697805  0.27
 0.27018154 0.2702375  0.2702224  0.2703425  0.27053314 0.2707757
 0.27103537 0.27121857 0.2712188  0.27101263 0.2706657  0.27030867
 0.27035028 0.2704954  0.27061754 0.27069524 0.27094656 0.27126944
 0.27134353 0.2711505  0.27076605 0.2703172  0.27018467 0.2704854
 0.27103415 0.27144858 0.27157918 0.2715397  0.27139792 0.27129132
 0.27124235 0.2712591  0.27134696 0.27144402 0.27133814 0.27066728
 0.27021515 0.26986605 0.26978397 0.27000394 0.27030393 0.27057505
 0.27075052 0.27068    0.27059203 0.27056894 0.27062097 0.2707354
 0.27087972 0.27102348 0.27114773 0.2713621  0.2715925  0.27183995
 0.27210444 0.2723304  0.27242178 0.2722604  0.27182955 0.27135348
 0.27116987 0.2713027  0.27159965 0.27181208 0.27183148 0.27207208
 0.2725642  0.27227813 0.2717425  0.27122504 0.27079797 0.2706626
 0.27086195 0.27123818 0.27156407 0.27176213 0.27189317 0.2720504
 0.27219433 0.2721259  0.27184954 0.27158573 0.2715996  0.2719377
 0.27228302 0.2723827  0.27216554 0.27177653 0.2715693  0.2719527
 0.272638   0.27262804 0.27226582 0.27208304 0.27227762 0.27275988
 0.2731853  0.27327168 0.2730721  0.2728231  0.27260727 0.27239922
 0.27216065 0.27185377 0.2716206  0.27157956 0.27171263 0.27189818
 0.27201447 0.27205896 0.27208033 0.27203172 0.2718729  0.27184978
 0.2719939  0.27199644 0.2720406  0.27221918 0.27240437 0.2724588
 0.2723477  0.2721883  0.2721681  0.2723154  0.27240035 0.27219725
 0.27177387 0.27132067 0.27112326 0.27131674 0.27176788 0.27217272
 0.27229965 0.27218702 0.2721031  0.27200082 0.27188483 0.27175328
 0.2715565  0.27137613 0.27136463 0.27147242 0.27159578 0.27167913
 0.27172008 0.27170438 0.2716057  0.2715103  0.27144927 0.2715325
 0.2718088  0.2721114  0.27217346 0.2718669  0.27132633 0.27080104
 0.27053967 0.27053505 0.27074736 0.27104643 0.2713002  0.27147818
 0.27145165 0.2713484  0.2713621  0.27145123 0.27153218 0.27167198
 0.27178735 0.27178687 0.27164647 0.2715395  0.2714381  0.27143273
 0.27159262 0.27181605 0.27194786 0.2718974  0.27167463 0.2713858
 0.2713922  0.27146488 0.27154186 0.27168512 0.2718887  0.2719368
 0.271576   0.27109915 0.27074608 0.27060044 0.27079186 0.27128017
 0.2718729  0.27235243 0.2726064  0.2726424  0.27253708 0.27245474
 0.27238557 0.2722591  0.27211332 0.2720529  0.27204928 0.2718316
 0.27160433 0.27129593 0.2712365  0.27155238 0.2720266  0.27236512
 0.2723634  0.2720501  0.27182677 0.27174452 0.27164268 0.27143732
 0.27126408 0.27129695 0.27150425 0.27173    0.27185747 0.2720001
 0.27227667 0.2726936  0.27313134 0.27341923 0.27351817 0.27352312
 0.27367127 0.27376446 0.2735223  0.27291605 0.27227738 0.27235588
 0.2731148  0.2732748  0.27289328 0.2722937  0.2718677  0.2720016
 0.2725786  0.27309525 0.27317902 0.27296486 0.27282813 0.27295995
 0.27314457 0.2730626  0.27276033 0.2725534  0.27272308 0.27316806
 0.2734614  0.2732882  0.27271137 0.27207693 0.27172354 0.272054
 0.27277032 0.2729239  0.27277714 0.2727575  0.2729362  0.27316806
 0.273219   0.27297154 0.2725817  0.2723184  0.2722789  0.2724285
 0.27261946 0.27274776 0.2728728  0.2730857  0.27335918 0.27357358
 0.27358732 0.27342135 0.2732214  0.27301037 0.27275428 0.27264142
 0.27266905 0.2726155  0.2726159  0.27267033 0.2726524  0.27253628
 0.27242038 0.27238774 0.27242535 0.27242196 0.27223733 0.2719268
 0.27174398 0.27180386 0.27211964 0.27252918 0.2728479  0.27296323
 0.272868   0.2727033  0.27261025 0.27258274 0.27246904 0.27229866
 0.27209267 0.2720219  0.2721738  0.27235287 0.2723618  0.27221155
 0.27203014 0.2718784  0.27180746 0.27185956 0.27194592 0.27201557
 0.27205658 0.2720047  0.27196252 0.27209887 0.2724067  0.27264482
 0.27265188 0.2724004  0.27229458 0.2726061  0.27299988 0.27306727
 0.27248356 0.2716792  0.2711229  0.27101794 0.27118826 0.27136937
 0.27144185 0.2715205  0.27175814 0.272158   0.2725059  0.2725838
 0.272423   0.27225563 0.2723422  0.27271318 0.2730569  0.2730851
 0.27292934 0.27256107 0.2722544  0.2721648  0.27230173 0.27247295
 0.27238992 0.2722673  0.27228388 0.27232918 0.27238223 0.27243757
 0.27251625 0.27264604 0.27279165 0.27284002 0.2727095  0.2724723
 0.2723476  0.27251822 0.2728982  0.27313632 0.2729325  0.2723961
 0.27197272 0.27189493 0.2721286  0.2723402  0.27225283 0.27196598
 0.27168655 0.27146134 0.2713696  0.2712952  0.27117354 0.27110323
 0.2712498  0.27164626 0.272124   0.27245757 0.27258217 0.2726097
 0.2727033  0.2729023  0.2730761  0.27309838 0.27293545 0.2727229
 0.2726282  0.2726878  0.2727842  0.2726981  0.2724302  0.2725004
 0.272963   0.27282536 0.27243966 0.2720242  0.27160445 0.2714319
 0.27167723 0.2722064  0.27265164 0.27272454 0.27239642 0.2719933
 0.27180275 0.27182788 0.27191693 0.2718689  0.27172226 0.27167252
 0.2718101  0.27207497 0.27222922 0.27206323 0.2717127  0.27172256
 0.27212107 0.2722298  0.27213737 0.27199605 0.2718919  0.27194437
 0.27213606 0.27231815 0.27237982 0.2723378  0.2722511  0.2722294
 0.27226582 0.27227262 0.27221093 0.27211505 0.27203676 0.27195504
 0.2717911  0.271545   0.27133062 0.27122203 0.2711401  0.27109918
 0.2709114  0.27040312 0.26994354 0.26982602 0.27001855 0.27035567
 0.27071676 0.27106798 0.27136922 0.2715075  0.27136114 0.2710484
 0.2708398  0.27087697 0.27115253 0.27149922 0.2717082  0.2716739
 0.2713638  0.2708751  0.27040377 0.2700946  0.26992205 0.26987347
 0.26987576 0.27000803 0.27032006 0.2705524  0.27047163 0.27012137
 0.26980075 0.26980636 0.2701482  0.27051175 0.2705422  0.2702591
 0.2700035  0.27003366 0.2703466  0.27069685 0.27081922 0.2706041
 0.2700924  0.2694989  0.2691218  0.26911637 0.2693172  0.26947546
 0.26937836 0.26921883 0.269111   0.26898813 0.2689104  0.26893362
 0.26905638 0.2692958  0.26964563 0.26998684 0.27011424 0.26997098
 0.26976988 0.26977718 0.26996297 0.2699729  0.26951408 0.26871598
 0.2680246  0.2678072  0.26818085 0.26882127 0.26931804 0.2694079
 0.26919416 0.2691271  0.2691855  0.26911068 0.26893854 0.26888043
 0.2691375  0.26970938 0.27032912 0.2706997  0.27078125 0.27072638
 0.27067077 0.2706311  0.2705243  0.27022448 0.26965642 0.2690247
 0.26864287 0.2688016  0.2695379  0.2704503  0.27098027 0.27092108
 0.27047032 0.26992485 0.2696407  0.26955807 0.26943466 0.26924774
 0.26925504 0.26963732 0.27018183 0.27046284 0.2703023  0.26991594
 0.2697005  0.26991013 0.27046606 0.2710378  0.27133888 0.2712869
 0.2710473  0.2708559  0.2707083  0.27047026 0.27012756 0.27011588
 0.27040383 0.27032918 0.2703209  0.270399   0.2702696  0.26993638
 0.26967913 0.26976597 0.27019963 0.270705   0.2709713  0.27097145
 0.27086398 0.27083477 0.27103743 0.2713434  0.27151006 0.27132082
 0.27078104 0.2702007  0.26992056 0.2699858  0.27025837 0.27072677
 0.27103844 0.270736   0.27048698 0.2707008  0.27126515 0.27191976
 0.27241737 0.27264062 0.2726704  0.27258393 0.27226925 0.2716809
 0.27101022 0.27056167 0.270639   0.27103332 0.27133015 0.2713301
 0.2711737  0.27103934 0.27095583 0.2709989  0.27137983 0.27183154]
