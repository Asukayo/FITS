Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H8', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j192_H8_FITS_custom_ftM_sl720_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=258, out_features=326, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9280140288.0
params:  84434.0
Trainable parameters:  84434
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 71.98866176605225
Epoch: 1, Steps: 88 | Train Loss: 1.0522008 Vali Loss: 1.1144255 Test Loss: 1.2839990
Validation loss decreased (inf --> 1.114426).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 75.63691997528076
Epoch: 2, Steps: 88 | Train Loss: 0.7865706 Vali Loss: 0.9845249 Test Loss: 1.1334008
Validation loss decreased (1.114426 --> 0.984525).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 82.36027455329895
Epoch: 3, Steps: 88 | Train Loss: 0.6870537 Vali Loss: 0.9196377 Test Loss: 1.0584286
Validation loss decreased (0.984525 --> 0.919638).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 77.55239987373352
Epoch: 4, Steps: 88 | Train Loss: 0.6177072 Vali Loss: 0.8754579 Test Loss: 1.0075614
Validation loss decreased (0.919638 --> 0.875458).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 70.11150121688843
Epoch: 5, Steps: 88 | Train Loss: 0.5615694 Vali Loss: 0.8300891 Test Loss: 0.9565169
Validation loss decreased (0.875458 --> 0.830089).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 72.03736996650696
Epoch: 6, Steps: 88 | Train Loss: 0.5144151 Vali Loss: 0.7901347 Test Loss: 0.9119132
Validation loss decreased (0.830089 --> 0.790135).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 71.56521916389465
Epoch: 7, Steps: 88 | Train Loss: 0.4738081 Vali Loss: 0.7582349 Test Loss: 0.8751740
Validation loss decreased (0.790135 --> 0.758235).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 68.08018898963928
Epoch: 8, Steps: 88 | Train Loss: 0.4385315 Vali Loss: 0.7285274 Test Loss: 0.8410251
Validation loss decreased (0.758235 --> 0.728527).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 68.11266732215881
Epoch: 9, Steps: 88 | Train Loss: 0.4077260 Vali Loss: 0.6958749 Test Loss: 0.8040025
Validation loss decreased (0.728527 --> 0.695875).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 68.46785926818848
Epoch: 10, Steps: 88 | Train Loss: 0.3804183 Vali Loss: 0.6733811 Test Loss: 0.7777857
Validation loss decreased (0.695875 --> 0.673381).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 68.49188542366028
Epoch: 11, Steps: 88 | Train Loss: 0.3561936 Vali Loss: 0.6496080 Test Loss: 0.7511883
Validation loss decreased (0.673381 --> 0.649608).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 70.08480215072632
Epoch: 12, Steps: 88 | Train Loss: 0.3347260 Vali Loss: 0.6292385 Test Loss: 0.7279176
Validation loss decreased (0.649608 --> 0.629239).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 64.63096714019775
Epoch: 13, Steps: 88 | Train Loss: 0.3153610 Vali Loss: 0.6112363 Test Loss: 0.7075686
Validation loss decreased (0.629239 --> 0.611236).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 71.47532176971436
Epoch: 14, Steps: 88 | Train Loss: 0.2980057 Vali Loss: 0.5948117 Test Loss: 0.6883388
Validation loss decreased (0.611236 --> 0.594812).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 71.94324088096619
Epoch: 15, Steps: 88 | Train Loss: 0.2823409 Vali Loss: 0.5765361 Test Loss: 0.6672631
Validation loss decreased (0.594812 --> 0.576536).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 68.46776032447815
Epoch: 16, Steps: 88 | Train Loss: 0.2682011 Vali Loss: 0.5609559 Test Loss: 0.6509688
Validation loss decreased (0.576536 --> 0.560956).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 64.5653669834137
Epoch: 17, Steps: 88 | Train Loss: 0.2554198 Vali Loss: 0.5498796 Test Loss: 0.6369638
Validation loss decreased (0.560956 --> 0.549880).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 69.18474650382996
Epoch: 18, Steps: 88 | Train Loss: 0.2436942 Vali Loss: 0.5379754 Test Loss: 0.6234167
Validation loss decreased (0.549880 --> 0.537975).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 68.53733325004578
Epoch: 19, Steps: 88 | Train Loss: 0.2330753 Vali Loss: 0.5265632 Test Loss: 0.6105605
Validation loss decreased (0.537975 --> 0.526563).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 70.37651991844177
Epoch: 20, Steps: 88 | Train Loss: 0.2233795 Vali Loss: 0.5163770 Test Loss: 0.5990188
Validation loss decreased (0.526563 --> 0.516377).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 75.66133260726929
Epoch: 21, Steps: 88 | Train Loss: 0.2145148 Vali Loss: 0.5053968 Test Loss: 0.5865143
Validation loss decreased (0.516377 --> 0.505397).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 67.8630862236023
Epoch: 22, Steps: 88 | Train Loss: 0.2063959 Vali Loss: 0.4966075 Test Loss: 0.5767707
Validation loss decreased (0.505397 --> 0.496608).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 70.01722526550293
Epoch: 23, Steps: 88 | Train Loss: 0.1988739 Vali Loss: 0.4899801 Test Loss: 0.5687779
Validation loss decreased (0.496608 --> 0.489980).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 63.67732334136963
Epoch: 24, Steps: 88 | Train Loss: 0.1920475 Vali Loss: 0.4829611 Test Loss: 0.5611361
Validation loss decreased (0.489980 --> 0.482961).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 68.94506430625916
Epoch: 25, Steps: 88 | Train Loss: 0.1857458 Vali Loss: 0.4749446 Test Loss: 0.5520929
Validation loss decreased (0.482961 --> 0.474945).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 73.4550302028656
Epoch: 26, Steps: 88 | Train Loss: 0.1798877 Vali Loss: 0.4687924 Test Loss: 0.5453764
Validation loss decreased (0.474945 --> 0.468792).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 74.18169927597046
Epoch: 27, Steps: 88 | Train Loss: 0.1744925 Vali Loss: 0.4629105 Test Loss: 0.5386717
Validation loss decreased (0.468792 --> 0.462911).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 66.02248549461365
Epoch: 28, Steps: 88 | Train Loss: 0.1694945 Vali Loss: 0.4573361 Test Loss: 0.5328805
Validation loss decreased (0.462911 --> 0.457336).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 66.87736225128174
Epoch: 29, Steps: 88 | Train Loss: 0.1648861 Vali Loss: 0.4518846 Test Loss: 0.5263142
Validation loss decreased (0.457336 --> 0.451885).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 72.16692662239075
Epoch: 30, Steps: 88 | Train Loss: 0.1605936 Vali Loss: 0.4476075 Test Loss: 0.5213535
Validation loss decreased (0.451885 --> 0.447607).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 68.12607073783875
Epoch: 31, Steps: 88 | Train Loss: 0.1565955 Vali Loss: 0.4423276 Test Loss: 0.5161403
Validation loss decreased (0.447607 --> 0.442328).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 67.0420081615448
Epoch: 32, Steps: 88 | Train Loss: 0.1528786 Vali Loss: 0.4381960 Test Loss: 0.5115584
Validation loss decreased (0.442328 --> 0.438196).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 71.33693647384644
Epoch: 33, Steps: 88 | Train Loss: 0.1494469 Vali Loss: 0.4344441 Test Loss: 0.5068959
Validation loss decreased (0.438196 --> 0.434444).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 65.73112297058105
Epoch: 34, Steps: 88 | Train Loss: 0.1462343 Vali Loss: 0.4304638 Test Loss: 0.5030791
Validation loss decreased (0.434444 --> 0.430464).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 68.46601438522339
Epoch: 35, Steps: 88 | Train Loss: 0.1432197 Vali Loss: 0.4274752 Test Loss: 0.4990774
Validation loss decreased (0.430464 --> 0.427475).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 65.08096504211426
Epoch: 36, Steps: 88 | Train Loss: 0.1404106 Vali Loss: 0.4245874 Test Loss: 0.4957579
Validation loss decreased (0.427475 --> 0.424587).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 70.09478807449341
Epoch: 37, Steps: 88 | Train Loss: 0.1377753 Vali Loss: 0.4210946 Test Loss: 0.4926632
Validation loss decreased (0.424587 --> 0.421095).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 67.10698461532593
Epoch: 38, Steps: 88 | Train Loss: 0.1353273 Vali Loss: 0.4186766 Test Loss: 0.4895306
Validation loss decreased (0.421095 --> 0.418677).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 78.37843155860901
Epoch: 39, Steps: 88 | Train Loss: 0.1329885 Vali Loss: 0.4165660 Test Loss: 0.4872549
Validation loss decreased (0.418677 --> 0.416566).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 64.68113231658936
Epoch: 40, Steps: 88 | Train Loss: 0.1308543 Vali Loss: 0.4136417 Test Loss: 0.4840170
Validation loss decreased (0.416566 --> 0.413642).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 71.48944044113159
Epoch: 41, Steps: 88 | Train Loss: 0.1288297 Vali Loss: 0.4106829 Test Loss: 0.4811910
Validation loss decreased (0.413642 --> 0.410683).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 71.7251238822937
Epoch: 42, Steps: 88 | Train Loss: 0.1269277 Vali Loss: 0.4091029 Test Loss: 0.4789906
Validation loss decreased (0.410683 --> 0.409103).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 69.93109011650085
Epoch: 43, Steps: 88 | Train Loss: 0.1251406 Vali Loss: 0.4072467 Test Loss: 0.4768260
Validation loss decreased (0.409103 --> 0.407247).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 67.53574395179749
Epoch: 44, Steps: 88 | Train Loss: 0.1234446 Vali Loss: 0.4048382 Test Loss: 0.4744469
Validation loss decreased (0.407247 --> 0.404838).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 68.91106033325195
Epoch: 45, Steps: 88 | Train Loss: 0.1218778 Vali Loss: 0.4029383 Test Loss: 0.4725533
Validation loss decreased (0.404838 --> 0.402938).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 72.33187246322632
Epoch: 46, Steps: 88 | Train Loss: 0.1203768 Vali Loss: 0.4011290 Test Loss: 0.4708331
Validation loss decreased (0.402938 --> 0.401129).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 72.13545989990234
Epoch: 47, Steps: 88 | Train Loss: 0.1189609 Vali Loss: 0.3997863 Test Loss: 0.4688670
Validation loss decreased (0.401129 --> 0.399786).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 76.56751823425293
Epoch: 48, Steps: 88 | Train Loss: 0.1176623 Vali Loss: 0.3984997 Test Loss: 0.4673057
Validation loss decreased (0.399786 --> 0.398500).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 66.86734437942505
Epoch: 49, Steps: 88 | Train Loss: 0.1164075 Vali Loss: 0.3966949 Test Loss: 0.4659147
Validation loss decreased (0.398500 --> 0.396695).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 72.21901679039001
Epoch: 50, Steps: 88 | Train Loss: 0.1152258 Vali Loss: 0.3954047 Test Loss: 0.4645331
Validation loss decreased (0.396695 --> 0.395405).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=258, out_features=326, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9280140288.0
params:  84434.0
Trainable parameters:  84434
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 63.09602975845337
Epoch: 1, Steps: 88 | Train Loss: 0.2521070 Vali Loss: 0.3307237 Test Loss: 0.4012991
Validation loss decreased (inf --> 0.330724).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 66.34910798072815
Epoch: 2, Steps: 88 | Train Loss: 0.2391214 Vali Loss: 0.3303460 Test Loss: 0.4011220
Validation loss decreased (0.330724 --> 0.330346).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 64.18894934654236
Epoch: 3, Steps: 88 | Train Loss: 0.2387625 Vali Loss: 0.3293024 Test Loss: 0.4008758
Validation loss decreased (0.330346 --> 0.329302).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 66.90730118751526
Epoch: 4, Steps: 88 | Train Loss: 0.2386026 Vali Loss: 0.3294424 Test Loss: 0.4005299
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 66.94010305404663
Epoch: 5, Steps: 88 | Train Loss: 0.2385460 Vali Loss: 0.3297964 Test Loss: 0.4004209
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 72.30204319953918
Epoch: 6, Steps: 88 | Train Loss: 0.2384151 Vali Loss: 0.3292234 Test Loss: 0.4006755
Validation loss decreased (0.329302 --> 0.329223).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 68.5089704990387
Epoch: 7, Steps: 88 | Train Loss: 0.2384308 Vali Loss: 0.3295951 Test Loss: 0.3999537
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 61.413100242614746
Epoch: 8, Steps: 88 | Train Loss: 0.2383567 Vali Loss: 0.3288830 Test Loss: 0.4001602
Validation loss decreased (0.329223 --> 0.328883).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 60.39918780326843
Epoch: 9, Steps: 88 | Train Loss: 0.2382968 Vali Loss: 0.3287503 Test Loss: 0.4002534
Validation loss decreased (0.328883 --> 0.328750).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 64.07384705543518
Epoch: 10, Steps: 88 | Train Loss: 0.2382621 Vali Loss: 0.3286856 Test Loss: 0.3997245
Validation loss decreased (0.328750 --> 0.328686).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 62.51657462120056
Epoch: 11, Steps: 88 | Train Loss: 0.2381576 Vali Loss: 0.3285215 Test Loss: 0.3997719
Validation loss decreased (0.328686 --> 0.328521).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 64.92527675628662
Epoch: 12, Steps: 88 | Train Loss: 0.2382081 Vali Loss: 0.3282625 Test Loss: 0.3996612
Validation loss decreased (0.328521 --> 0.328263).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 67.92591953277588
Epoch: 13, Steps: 88 | Train Loss: 0.2381738 Vali Loss: 0.3289652 Test Loss: 0.3997289
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 64.54805397987366
Epoch: 14, Steps: 88 | Train Loss: 0.2381681 Vali Loss: 0.3285097 Test Loss: 0.3998632
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 67.59792113304138
Epoch: 15, Steps: 88 | Train Loss: 0.2381784 Vali Loss: 0.3284832 Test Loss: 0.3998097
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j192_H8_FITS_custom_ftM_sl720_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.39870548248291016, mae:0.275120347738266, rse:0.5211408734321594, corr:[0.2779541  0.29051265 0.2911988  0.29058412 0.29107046 0.2909964
 0.29067263 0.29147568 0.29187045 0.29148003 0.29107144 0.2905654
 0.29056308 0.29051173 0.28977042 0.289752   0.29034552 0.29023108
 0.29017195 0.2905618  0.29033247 0.28991967 0.28989467 0.28976786
 0.29042333 0.29077983 0.2910015  0.29062623 0.2904669  0.29058778
 0.29043218 0.2905135  0.2908657  0.29097092 0.29106992 0.29117957
 0.29098466 0.29072413 0.29053566 0.29070792 0.2913703  0.29149714
 0.29096702 0.29085076 0.29087517 0.29037985 0.28999662 0.29013276
 0.29014742 0.28972876 0.29009858 0.2903094  0.28957844 0.28894117
 0.28885394 0.28885657 0.2891291  0.28962094 0.28964046 0.28942674
 0.28949288 0.28937954 0.288881   0.28854322 0.2888217  0.28932074
 0.28930828 0.28897494 0.28883344 0.28875485 0.28878495 0.28913006
 0.28885368 0.28802648 0.28795636 0.2884469  0.28852385 0.28833163
 0.28819296 0.28815737 0.28855243 0.2890413  0.28885514 0.28828362
 0.287973   0.28785163 0.28780186 0.28800118 0.2883688  0.2884409
 0.28820744 0.28816187 0.28831163 0.28823656 0.28812823 0.28845638
 0.2884378  0.28798044 0.28770253 0.28779146 0.287714   0.28731883
 0.2871249  0.28704697 0.2869271  0.2872067  0.287759   0.28791922
 0.28779188 0.28767174 0.28742477 0.2873219  0.28767413 0.28793475
 0.287717   0.28758162 0.2877926  0.28797114 0.28806263 0.2880607
 0.28793988 0.28788158 0.28761262 0.28746957 0.28772604 0.28767964
 0.28737867 0.28765082 0.28827822 0.28851497 0.2885188  0.28852624
 0.28845778 0.28842327 0.28851527 0.28864458 0.28873253 0.2885637
 0.28808722 0.2876988  0.2876547  0.2876009  0.2874     0.28771058
 0.288714   0.28913492 0.2888655  0.2887272  0.2890538  0.289344
 0.28906524 0.2882944  0.28777656 0.28785673 0.28801003 0.28810376
 0.2883731  0.28863102 0.2889072  0.2892743  0.28934586 0.2891003
 0.28877777 0.28835997 0.28823125 0.28863135 0.28890684 0.28934595
 0.2908445  0.2909169  0.29048505 0.2902381  0.29003587 0.2900596
 0.29029545 0.29012442 0.2897992  0.29011804 0.29035577 0.28993756
 0.28979665 0.2896758  0.2893862  0.28997645 0.29022184 0.2892333
 0.28922054 0.28950012 0.28876877 0.28918916 0.28876844 0.28944817]
