Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7149772800.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4632013
	speed: 0.4544s/iter; left time: 3976.3591s
Epoch: 1 cost time: 78.22462844848633
Epoch: 1, Steps: 177 | Train Loss: 0.5765818 Vali Loss: 0.4141167 Test Loss: 0.4844957
Validation loss decreased (inf --> 0.414117).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2390003
	speed: 1.1950s/iter; left time: 10245.6696s
Epoch: 2 cost time: 75.53006482124329
Epoch: 2, Steps: 177 | Train Loss: 0.2627363 Vali Loss: 0.3365055 Test Loss: 0.4043534
Validation loss decreased (0.414117 --> 0.336506).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2519696
	speed: 1.2218s/iter; left time: 10259.3557s
Epoch: 3 cost time: 75.88673543930054
Epoch: 3, Steps: 177 | Train Loss: 0.2403588 Vali Loss: 0.3314621 Test Loss: 0.4012780
Validation loss decreased (0.336506 --> 0.331462).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2458528
	speed: 1.2768s/iter; left time: 10495.4283s
Epoch: 4 cost time: 77.14706492424011
Epoch: 4, Steps: 177 | Train Loss: 0.2387880 Vali Loss: 0.3296986 Test Loss: 0.4002364
Validation loss decreased (0.331462 --> 0.329699).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2307007
	speed: 1.2071s/iter; left time: 9708.9325s
Epoch: 5 cost time: 77.20143032073975
Epoch: 5, Steps: 177 | Train Loss: 0.2383362 Vali Loss: 0.3293718 Test Loss: 0.3996509
Validation loss decreased (0.329699 --> 0.329372).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2281806
	speed: 1.2065s/iter; left time: 9490.0095s
Epoch: 6 cost time: 79.74896597862244
Epoch: 6, Steps: 177 | Train Loss: 0.2381486 Vali Loss: 0.3283527 Test Loss: 0.3997698
Validation loss decreased (0.329372 --> 0.328353).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2332138
	speed: 1.5263s/iter; left time: 11735.9574s
Epoch: 7 cost time: 99.79407119750977
Epoch: 7, Steps: 177 | Train Loss: 0.2379903 Vali Loss: 0.3281344 Test Loss: 0.3992220
Validation loss decreased (0.328353 --> 0.328134).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2329792
	speed: 1.6622s/iter; left time: 12486.3114s
Epoch: 8 cost time: 110.89373803138733
Epoch: 8, Steps: 177 | Train Loss: 0.2379113 Vali Loss: 0.3282386 Test Loss: 0.3996683
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2388185
	speed: 1.7273s/iter; left time: 12669.8174s
Epoch: 9 cost time: 102.17813515663147
Epoch: 9, Steps: 177 | Train Loss: 0.2377494 Vali Loss: 0.3279355 Test Loss: 0.3992249
Validation loss decreased (0.328134 --> 0.327935).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2430404
	speed: 1.6417s/iter; left time: 11751.1753s
Epoch: 10 cost time: 100.317453622818
Epoch: 10, Steps: 177 | Train Loss: 0.2376959 Vali Loss: 0.3278152 Test Loss: 0.3988684
Validation loss decreased (0.327935 --> 0.327815).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2453781
	speed: 1.5540s/iter; left time: 10848.7199s
Epoch: 11 cost time: 85.0696439743042
Epoch: 11, Steps: 177 | Train Loss: 0.2376820 Vali Loss: 0.3281740 Test Loss: 0.3993391
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2209958
	speed: 1.2909s/iter; left time: 8783.3500s
Epoch: 12 cost time: 93.64490532875061
Epoch: 12, Steps: 177 | Train Loss: 0.2375604 Vali Loss: 0.3277493 Test Loss: 0.3987450
Validation loss decreased (0.327815 --> 0.327749).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2429018
	speed: 1.6663s/iter; left time: 11042.8981s
Epoch: 13 cost time: 83.79811429977417
Epoch: 13, Steps: 177 | Train Loss: 0.2375110 Vali Loss: 0.3281755 Test Loss: 0.3991496
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2353089
	speed: 1.0698s/iter; left time: 6900.0178s
Epoch: 14 cost time: 74.08731365203857
Epoch: 14, Steps: 177 | Train Loss: 0.2374996 Vali Loss: 0.3284810 Test Loss: 0.3992397
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2266957
	speed: 1.2577s/iter; left time: 7889.2778s
Epoch: 15 cost time: 75.87086534500122
Epoch: 15, Steps: 177 | Train Loss: 0.2374835 Vali Loss: 0.3278340 Test Loss: 0.3985871
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.39789262413978577, mae:0.2740307152271271, rse:0.5206093788146973, corr:[0.27460444 0.2894595  0.2875962  0.2892233  0.2892524  0.29009598
 0.2917314  0.2910839  0.29148474 0.29054558 0.29018438 0.29005972
 0.29002878 0.2899245  0.28909793 0.28939655 0.28952938 0.28930292
 0.28916603 0.28892103 0.28928086 0.2896432  0.28944987 0.2892884
 0.29118502 0.29198712 0.29126963 0.29060555 0.2899668  0.28990662
 0.2904874  0.29004666 0.29000813 0.29001775 0.2894463  0.28926829
 0.28932014 0.28953296 0.28944212 0.28944853 0.2899427  0.29016393
 0.28969964 0.28927585 0.28943178 0.2899253  0.28974813 0.2891022
 0.28939572 0.2895197  0.28966665 0.2897539  0.2893526  0.2891421
 0.28899646 0.2887778  0.28919327 0.2893539  0.2885683  0.288064
 0.28808013 0.28831637 0.2886184  0.28855512 0.2884239  0.28825873
 0.28845623 0.28889343 0.2887112  0.28840968 0.28838697 0.28839493
 0.28838637 0.28829587 0.2880891  0.28789666 0.2878768  0.28848788
 0.2887108  0.28847644 0.28885874 0.28874904 0.28818166 0.28797436
 0.2880417  0.2883122  0.28850016 0.2882645  0.28794095 0.2880726
 0.28810823 0.28769904 0.2873296  0.28725561 0.28713974 0.2872043
 0.28757176 0.28750244 0.28712034 0.28707296 0.28697354 0.28712177
 0.28753072 0.28764176 0.2876566  0.28758115 0.28749642 0.2873295
 0.28713492 0.28789872 0.28874758 0.2888568  0.2888952  0.28871962
 0.288376   0.28776363 0.28730556 0.28729326 0.28710955 0.2872114
 0.28792468 0.28860018 0.28869757 0.2883211  0.28765288 0.28761673
 0.28791875 0.28788644 0.28838035 0.2887317  0.28800613 0.2876552
 0.28763744 0.28717905 0.28707066 0.2872334  0.28773493 0.28784767
 0.28742102 0.2873939  0.287032   0.28690657 0.28731257 0.28811103
 0.28917393 0.28900313 0.28899097 0.28881797 0.28841048 0.28842673
 0.28788173 0.28779414 0.28776124 0.28769812 0.28789738 0.28750017
 0.28756458 0.28761134 0.28766122 0.28775948 0.2875323  0.2881479
 0.28831488 0.28811413 0.288097   0.2878169  0.2879238  0.28837284
 0.2903361  0.2900608  0.28880513 0.2889682  0.28893587 0.2889226
 0.2893527  0.2899548  0.29011723 0.29001898 0.29026815 0.288446
 0.28844374 0.289345   0.28844562 0.2887388  0.28792092 0.28888565
 0.28851765 0.28815553 0.28791085 0.2861857  0.2879704  0.28940296]
