Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14299545600.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 63.12861728668213
Epoch: 1, Steps: 88 | Train Loss: 1.0595264 Vali Loss: 1.1102067 Test Loss: 1.2780925
Validation loss decreased (inf --> 1.110207).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 64.9677472114563
Epoch: 2, Steps: 88 | Train Loss: 0.7968651 Vali Loss: 0.9965819 Test Loss: 1.1458968
Validation loss decreased (1.110207 --> 0.996582).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 65.26273322105408
Epoch: 3, Steps: 88 | Train Loss: 0.6991418 Vali Loss: 0.9373901 Test Loss: 1.0773299
Validation loss decreased (0.996582 --> 0.937390).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 63.798197507858276
Epoch: 4, Steps: 88 | Train Loss: 0.6286066 Vali Loss: 0.8877959 Test Loss: 1.0217334
Validation loss decreased (0.937390 --> 0.887796).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 63.10328388214111
Epoch: 5, Steps: 88 | Train Loss: 0.5710385 Vali Loss: 0.8454659 Test Loss: 0.9733010
Validation loss decreased (0.887796 --> 0.845466).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 62.352752923965454
Epoch: 6, Steps: 88 | Train Loss: 0.5223810 Vali Loss: 0.8053078 Test Loss: 0.9274560
Validation loss decreased (0.845466 --> 0.805308).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 63.809301137924194
Epoch: 7, Steps: 88 | Train Loss: 0.4807109 Vali Loss: 0.7717505 Test Loss: 0.8888556
Validation loss decreased (0.805308 --> 0.771751).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 62.876853942871094
Epoch: 8, Steps: 88 | Train Loss: 0.4444199 Vali Loss: 0.7392086 Test Loss: 0.8517929
Validation loss decreased (0.771751 --> 0.739209).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 65.31508326530457
Epoch: 9, Steps: 88 | Train Loss: 0.4126822 Vali Loss: 0.7098824 Test Loss: 0.8182570
Validation loss decreased (0.739209 --> 0.709882).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 67.36408257484436
Epoch: 10, Steps: 88 | Train Loss: 0.3845794 Vali Loss: 0.6827552 Test Loss: 0.7875454
Validation loss decreased (0.709882 --> 0.682755).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 77.13716316223145
Epoch: 11, Steps: 88 | Train Loss: 0.3597357 Vali Loss: 0.6621239 Test Loss: 0.7636393
Validation loss decreased (0.682755 --> 0.662124).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 61.21641659736633
Epoch: 12, Steps: 88 | Train Loss: 0.3375705 Vali Loss: 0.6388891 Test Loss: 0.7376529
Validation loss decreased (0.662124 --> 0.638889).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 65.1737654209137
Epoch: 13, Steps: 88 | Train Loss: 0.3177032 Vali Loss: 0.6185258 Test Loss: 0.7138448
Validation loss decreased (0.638889 --> 0.618526).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 65.16830611228943
Epoch: 14, Steps: 88 | Train Loss: 0.2998265 Vali Loss: 0.6018557 Test Loss: 0.6953173
Validation loss decreased (0.618526 --> 0.601856).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 66.0843563079834
Epoch: 15, Steps: 88 | Train Loss: 0.2836469 Vali Loss: 0.5866647 Test Loss: 0.6777062
Validation loss decreased (0.601856 --> 0.586665).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 65.00684547424316
Epoch: 16, Steps: 88 | Train Loss: 0.2690888 Vali Loss: 0.5723346 Test Loss: 0.6616193
Validation loss decreased (0.586665 --> 0.572335).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 66.4358298778534
Epoch: 17, Steps: 88 | Train Loss: 0.2559056 Vali Loss: 0.5577240 Test Loss: 0.6455637
Validation loss decreased (0.572335 --> 0.557724).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 64.78343963623047
Epoch: 18, Steps: 88 | Train Loss: 0.2439021 Vali Loss: 0.5459109 Test Loss: 0.6319386
Validation loss decreased (0.557724 --> 0.545911).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 61.070054054260254
Epoch: 19, Steps: 88 | Train Loss: 0.2329410 Vali Loss: 0.5345419 Test Loss: 0.6187083
Validation loss decreased (0.545911 --> 0.534542).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 63.12359118461609
Epoch: 20, Steps: 88 | Train Loss: 0.2229567 Vali Loss: 0.5232728 Test Loss: 0.6058726
Validation loss decreased (0.534542 --> 0.523273).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 63.33893013000488
Epoch: 21, Steps: 88 | Train Loss: 0.2137847 Vali Loss: 0.5127134 Test Loss: 0.5949210
Validation loss decreased (0.523273 --> 0.512713).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 67.3733274936676
Epoch: 22, Steps: 88 | Train Loss: 0.2054194 Vali Loss: 0.5039057 Test Loss: 0.5844988
Validation loss decreased (0.512713 --> 0.503906).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 66.48648285865784
Epoch: 23, Steps: 88 | Train Loss: 0.1976543 Vali Loss: 0.4951524 Test Loss: 0.5747374
Validation loss decreased (0.503906 --> 0.495152).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 63.9315927028656
Epoch: 24, Steps: 88 | Train Loss: 0.1906101 Vali Loss: 0.4888556 Test Loss: 0.5675031
Validation loss decreased (0.495152 --> 0.488856).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 62.77103042602539
Epoch: 25, Steps: 88 | Train Loss: 0.1840659 Vali Loss: 0.4813556 Test Loss: 0.5587267
Validation loss decreased (0.488856 --> 0.481356).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 63.17715501785278
Epoch: 26, Steps: 88 | Train Loss: 0.1780377 Vali Loss: 0.4753840 Test Loss: 0.5515438
Validation loss decreased (0.481356 --> 0.475384).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 63.95716381072998
Epoch: 27, Steps: 88 | Train Loss: 0.1724799 Vali Loss: 0.4686346 Test Loss: 0.5443988
Validation loss decreased (0.475384 --> 0.468635).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 69.94180154800415
Epoch: 28, Steps: 88 | Train Loss: 0.1673142 Vali Loss: 0.4628659 Test Loss: 0.5382566
Validation loss decreased (0.468635 --> 0.462866).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 68.94171571731567
Epoch: 29, Steps: 88 | Train Loss: 0.1624946 Vali Loss: 0.4567499 Test Loss: 0.5315762
Validation loss decreased (0.462866 --> 0.456750).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 68.83318042755127
Epoch: 30, Steps: 88 | Train Loss: 0.1580692 Vali Loss: 0.4533197 Test Loss: 0.5271095
Validation loss decreased (0.456750 --> 0.453320).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 69.8872287273407
Epoch: 31, Steps: 88 | Train Loss: 0.1539289 Vali Loss: 0.4479555 Test Loss: 0.5210847
Validation loss decreased (0.453320 --> 0.447956).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 69.40442967414856
Epoch: 32, Steps: 88 | Train Loss: 0.1500977 Vali Loss: 0.4442734 Test Loss: 0.5168399
Validation loss decreased (0.447956 --> 0.444273).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 69.47560167312622
Epoch: 33, Steps: 88 | Train Loss: 0.1464918 Vali Loss: 0.4390130 Test Loss: 0.5117676
Validation loss decreased (0.444273 --> 0.439013).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 72.5184428691864
Epoch: 34, Steps: 88 | Train Loss: 0.1431702 Vali Loss: 0.4360432 Test Loss: 0.5075379
Validation loss decreased (0.439013 --> 0.436043).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 70.0311951637268
Epoch: 35, Steps: 88 | Train Loss: 0.1400801 Vali Loss: 0.4322303 Test Loss: 0.5040315
Validation loss decreased (0.436043 --> 0.432230).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 66.31128668785095
Epoch: 36, Steps: 88 | Train Loss: 0.1371289 Vali Loss: 0.4282937 Test Loss: 0.5001156
Validation loss decreased (0.432230 --> 0.428294).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 69.63626527786255
Epoch: 37, Steps: 88 | Train Loss: 0.1344248 Vali Loss: 0.4254874 Test Loss: 0.4969541
Validation loss decreased (0.428294 --> 0.425487).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 70.10699224472046
Epoch: 38, Steps: 88 | Train Loss: 0.1318618 Vali Loss: 0.4231133 Test Loss: 0.4940948
Validation loss decreased (0.425487 --> 0.423113).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 73.70032000541687
Epoch: 39, Steps: 88 | Train Loss: 0.1294354 Vali Loss: 0.4195137 Test Loss: 0.4904556
Validation loss decreased (0.423113 --> 0.419514).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 73.66486287117004
Epoch: 40, Steps: 88 | Train Loss: 0.1272126 Vali Loss: 0.4171655 Test Loss: 0.4878631
Validation loss decreased (0.419514 --> 0.417166).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 73.09943509101868
Epoch: 41, Steps: 88 | Train Loss: 0.1250971 Vali Loss: 0.4146992 Test Loss: 0.4849029
Validation loss decreased (0.417166 --> 0.414699).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 69.86135935783386
Epoch: 42, Steps: 88 | Train Loss: 0.1231176 Vali Loss: 0.4126409 Test Loss: 0.4827259
Validation loss decreased (0.414699 --> 0.412641).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 62.566301345825195
Epoch: 43, Steps: 88 | Train Loss: 0.1212752 Vali Loss: 0.4101284 Test Loss: 0.4802363
Validation loss decreased (0.412641 --> 0.410128).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 69.56831693649292
Epoch: 44, Steps: 88 | Train Loss: 0.1195124 Vali Loss: 0.4075381 Test Loss: 0.4776408
Validation loss decreased (0.410128 --> 0.407538).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 64.08917188644409
Epoch: 45, Steps: 88 | Train Loss: 0.1178464 Vali Loss: 0.4069255 Test Loss: 0.4762340
Validation loss decreased (0.407538 --> 0.406926).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 72.19602608680725
Epoch: 46, Steps: 88 | Train Loss: 0.1162997 Vali Loss: 0.4044968 Test Loss: 0.4743147
Validation loss decreased (0.406926 --> 0.404497).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 66.76194715499878
Epoch: 47, Steps: 88 | Train Loss: 0.1148469 Vali Loss: 0.4024334 Test Loss: 0.4720527
Validation loss decreased (0.404497 --> 0.402433).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 70.91142582893372
Epoch: 48, Steps: 88 | Train Loss: 0.1134741 Vali Loss: 0.4014128 Test Loss: 0.4707606
Validation loss decreased (0.402433 --> 0.401413).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 70.88671398162842
Epoch: 49, Steps: 88 | Train Loss: 0.1121627 Vali Loss: 0.3992332 Test Loss: 0.4683666
Validation loss decreased (0.401413 --> 0.399233).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 72.32929182052612
Epoch: 50, Steps: 88 | Train Loss: 0.1109280 Vali Loss: 0.3980486 Test Loss: 0.4671329
Validation loss decreased (0.399233 --> 0.398049).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14299545600.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 67.54950094223022
Epoch: 1, Steps: 88 | Train Loss: 0.2524158 Vali Loss: 0.3301638 Test Loss: 0.4001403
Validation loss decreased (inf --> 0.330164).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 69.02646327018738
Epoch: 2, Steps: 88 | Train Loss: 0.2382621 Vali Loss: 0.3289583 Test Loss: 0.3995208
Validation loss decreased (0.330164 --> 0.328958).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 64.94626092910767
Epoch: 3, Steps: 88 | Train Loss: 0.2379231 Vali Loss: 0.3275299 Test Loss: 0.3994280
Validation loss decreased (0.328958 --> 0.327530).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 65.63662433624268
Epoch: 4, Steps: 88 | Train Loss: 0.2376899 Vali Loss: 0.3278432 Test Loss: 0.3990603
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 61.91336226463318
Epoch: 5, Steps: 88 | Train Loss: 0.2376617 Vali Loss: 0.3280251 Test Loss: 0.3988065
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 65.85917258262634
Epoch: 6, Steps: 88 | Train Loss: 0.2376142 Vali Loss: 0.3283439 Test Loss: 0.3985993
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.39847758412361145, mae:0.27493008971214294, rse:0.5209919214248657, corr:[0.2760015  0.2894011  0.2901389  0.28923887 0.29002914 0.28983608
 0.2902723  0.28968555 0.28986338 0.2893486  0.28896713 0.2891257
 0.28916222 0.28967577 0.28896338 0.28879994 0.2891051  0.28958565
 0.28983173 0.28953102 0.28923482 0.28892288 0.28909233 0.2883872
 0.28933558 0.290418   0.2901182  0.29007712 0.29061884 0.2906468
 0.29045954 0.29003623 0.2900469  0.2900259  0.2900322  0.2902001
 0.2896283  0.28966144 0.29011297 0.2903371  0.28998187 0.28983662
 0.2905058  0.28950316 0.28845355 0.2887823  0.28897917 0.28950444
 0.28991055 0.28961948 0.28972775 0.29013616 0.2902856  0.2896515
 0.28936294 0.28915706 0.2890228  0.28946567 0.28904745 0.28871843
 0.28869846 0.28812176 0.2878853  0.28771123 0.28792325 0.28814426
 0.28869605 0.2896877  0.2896909  0.2893724  0.28865907 0.2885219
 0.28842807 0.28763342 0.28812382 0.287735   0.28755796 0.28875983
 0.28841302 0.28811148 0.28839424 0.28837174 0.28793967 0.28733194
 0.2873005  0.28644636 0.28615597 0.28708082 0.28747243 0.28781998
 0.28766236 0.28782573 0.28841084 0.28836194 0.2887213  0.2887622
 0.28846952 0.28850546 0.2879856  0.28758597 0.28731313 0.28790808
 0.28789958 0.28724363 0.2872206  0.28708097 0.28736255 0.2873016
 0.28735983 0.2874431  0.28681746 0.28715518 0.2873458  0.28739
 0.28737518 0.2869354  0.28760344 0.28748894 0.2874664  0.28818977
 0.28751382 0.2877588  0.28783265 0.2872021  0.28726277 0.28690273
 0.28702772 0.28763023 0.2880305  0.2882166  0.28794172 0.28805968
 0.2878234  0.2873808  0.28710598 0.28674325 0.2867323  0.28655344
 0.28709626 0.28718445 0.28663033 0.28739467 0.28718594 0.28677067
 0.2878884  0.28831047 0.28873107 0.28843462 0.28687993 0.28676912
 0.28693226 0.28611133 0.28590283 0.2866572  0.28718835 0.28798565
 0.28846285 0.28784543 0.2878473  0.28789416 0.28833744 0.28855085
 0.28783026 0.28813648 0.28805962 0.28755078 0.28730682 0.2876396
 0.28966615 0.28997973 0.28975222 0.28964362 0.28972304 0.28977153
 0.28813913 0.28695878 0.28637585 0.28629798 0.28765288 0.2883684
 0.2891386  0.28862917 0.2884612  0.28867537 0.28830206 0.28925794
 0.28834954 0.28903708 0.28778353 0.28766096 0.28789043 0.28939345]
