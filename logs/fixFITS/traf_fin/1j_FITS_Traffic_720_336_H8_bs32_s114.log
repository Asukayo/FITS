Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H8', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j336_H8_FITS_custom_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=258, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5380204032.0
params:  97902.0
Trainable parameters:  97902
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5398353
	speed: 0.6920s/iter; left time: 5986.3716s
Epoch: 1 cost time: 120.75273895263672
Epoch: 1, Steps: 175 | Train Loss: 0.6484452 Vali Loss: 0.5105313 Test Loss: 0.5910930
Validation loss decreased (inf --> 0.510531).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2993281
	speed: 1.7797s/iter; left time: 15084.7063s
Epoch: 2 cost time: 99.18121361732483
Epoch: 2, Steps: 175 | Train Loss: 0.3153716 Vali Loss: 0.3724452 Test Loss: 0.4414861
Validation loss decreased (0.510531 --> 0.372445).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2505473
	speed: 1.5317s/iter; left time: 12714.4159s
Epoch: 3 cost time: 98.34418749809265
Epoch: 3, Steps: 175 | Train Loss: 0.2594699 Vali Loss: 0.3480695 Test Loss: 0.4188440
Validation loss decreased (0.372445 --> 0.348070).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2395471
	speed: 1.7619s/iter; left time: 14317.4855s
Epoch: 4 cost time: 107.34454727172852
Epoch: 4, Steps: 175 | Train Loss: 0.2510435 Vali Loss: 0.3439692 Test Loss: 0.4159178
Validation loss decreased (0.348070 --> 0.343969).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2552713
	speed: 1.7042s/iter; left time: 13550.4491s
Epoch: 5 cost time: 114.93597221374512
Epoch: 5, Steps: 175 | Train Loss: 0.2496801 Vali Loss: 0.3426951 Test Loss: 0.4150890
Validation loss decreased (0.343969 --> 0.342695).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2499275
	speed: 1.6585s/iter; left time: 12896.8731s
Epoch: 6 cost time: 85.6865770816803
Epoch: 6, Steps: 175 | Train Loss: 0.2492118 Vali Loss: 0.3421639 Test Loss: 0.4150480
Validation loss decreased (0.342695 --> 0.342164).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2505938
	speed: 1.7066s/iter; left time: 12971.9542s
Epoch: 7 cost time: 114.51190519332886
Epoch: 7, Steps: 175 | Train Loss: 0.2490852 Vali Loss: 0.3415782 Test Loss: 0.4149616
Validation loss decreased (0.342164 --> 0.341578).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2449787
	speed: 1.6359s/iter; left time: 12148.0883s
Epoch: 8 cost time: 90.98479914665222
Epoch: 8, Steps: 175 | Train Loss: 0.2489928 Vali Loss: 0.3413182 Test Loss: 0.4153532
Validation loss decreased (0.341578 --> 0.341318).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2512897
	speed: 1.4688s/iter; left time: 10650.5054s
Epoch: 9 cost time: 94.69761681556702
Epoch: 9, Steps: 175 | Train Loss: 0.2488896 Vali Loss: 0.3412916 Test Loss: 0.4152152
Validation loss decreased (0.341318 --> 0.341292).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2425325
	speed: 1.4737s/iter; left time: 10427.6729s
Epoch: 10 cost time: 88.3895537853241
Epoch: 10, Steps: 175 | Train Loss: 0.2488501 Vali Loss: 0.3413329 Test Loss: 0.4150164
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2633122
	speed: 1.4470s/iter; left time: 9986.0702s
Epoch: 11 cost time: 91.73339748382568
Epoch: 11, Steps: 175 | Train Loss: 0.2487825 Vali Loss: 0.3408863 Test Loss: 0.4144518
Validation loss decreased (0.341292 --> 0.340886).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2547136
	speed: 1.4636s/iter; left time: 9843.8824s
Epoch: 12 cost time: 92.27081227302551
Epoch: 12, Steps: 175 | Train Loss: 0.2487678 Vali Loss: 0.3409342 Test Loss: 0.4147828
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2492845
	speed: 1.5515s/iter; left time: 10163.8739s
Epoch: 13 cost time: 99.78315353393555
Epoch: 13, Steps: 175 | Train Loss: 0.2487755 Vali Loss: 0.3408618 Test Loss: 0.4149874
Validation loss decreased (0.340886 --> 0.340862).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2420771
	speed: 1.6865s/iter; left time: 10753.3431s
Epoch: 14 cost time: 90.73097825050354
Epoch: 14, Steps: 175 | Train Loss: 0.2487048 Vali Loss: 0.3405868 Test Loss: 0.4144818
Validation loss decreased (0.340862 --> 0.340587).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2445199
	speed: 1.4224s/iter; left time: 8820.4052s
Epoch: 15 cost time: 89.59428215026855
Epoch: 15, Steps: 175 | Train Loss: 0.2486423 Vali Loss: 0.3409736 Test Loss: 0.4147548
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2399606
	speed: 1.4261s/iter; left time: 8593.7697s
Epoch: 16 cost time: 89.06929326057434
Epoch: 16, Steps: 175 | Train Loss: 0.2486258 Vali Loss: 0.3408289 Test Loss: 0.4145893
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2536022
	speed: 1.6499s/iter; left time: 9653.8480s
Epoch: 17 cost time: 106.5348813533783
Epoch: 17, Steps: 175 | Train Loss: 0.2485343 Vali Loss: 0.3406469 Test Loss: 0.4142693
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j336_H8_FITS_custom_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.41204801201820374, mae:0.2813867926597595, rse:0.527561604976654, corr:[0.27475956 0.28309986 0.28508484 0.2838341  0.28494295 0.2855477
 0.28506342 0.28571025 0.2855477  0.28500378 0.2856653  0.28546318
 0.28450823 0.28426775 0.28372547 0.2831562  0.28375357 0.28433275
 0.28419942 0.2843216  0.28455248 0.28456923 0.28492677 0.285428
 0.28599197 0.28575927 0.2860289  0.286178   0.28610465 0.285899
 0.28511828 0.2844957  0.28459013 0.28462952 0.28457853 0.28496456
 0.2851986  0.28516433 0.28542116 0.2856703  0.28565672 0.28573403
 0.28601888 0.2859845  0.28552917 0.2850042  0.2847567  0.28522858
 0.2860994  0.28602186 0.28563592 0.28541562 0.28511596 0.28475925
 0.28465202 0.28447726 0.2841315  0.28426665 0.28479338 0.28500506
 0.2847803  0.28470576 0.28531012 0.28588167 0.2854605  0.28456962
 0.2844749  0.28466013 0.28443354 0.28406686 0.2839476  0.28408864
 0.28409997 0.28411973 0.2843565  0.28435656 0.28394365 0.2836881
 0.28389093 0.28404793 0.2840132  0.28414637 0.28416476 0.2836168
 0.2832304  0.2836806  0.28438306 0.28475434 0.28499377 0.28501618
 0.28483972 0.28464308 0.28435782 0.28401873 0.2840162  0.28430575
 0.28413132 0.28372493 0.28341275 0.28327185 0.28343365 0.2836766
 0.28358015 0.2831962  0.28304002 0.2832604  0.2836003  0.28379416
 0.28374916 0.2836549  0.28398368 0.28461742 0.28487325 0.2847264
 0.28465664 0.2847496  0.2848817  0.28464958 0.2840803  0.28371868
 0.2836737  0.28366157 0.28352985 0.2836868  0.2841799  0.2845833
 0.28470194 0.28456464 0.2844536  0.28454322 0.28452247 0.28417674
 0.28402725 0.28439945 0.28485042 0.28518084 0.28538907 0.28516743
 0.28489572 0.28504643 0.28500846 0.2845943  0.28440574 0.2844723
 0.284577   0.28460827 0.28454658 0.28428736 0.2843713  0.28483105
 0.28503662 0.28500402 0.28503504 0.2849464  0.28469777 0.2846311
 0.28478524 0.28486174 0.2847475  0.28454557 0.2843241  0.2841732
 0.28421742 0.28431755 0.28439003 0.28463474 0.28491884 0.28524458
 0.28637516 0.28658575 0.2863222  0.28569302 0.28540447 0.28584975
 0.28612298 0.2860055  0.28616238 0.28611428 0.28529772 0.28470704
 0.2849299  0.28516024 0.285269   0.28547356 0.2851485  0.28436387
 0.28426588 0.2847907  0.2849139  0.28496116 0.28527805 0.28525856
 0.28518838 0.2850042  0.28503963 0.28507957 0.28514928 0.28519762
 0.2852946  0.2855711  0.28566468 0.28547105 0.2853646  0.28544065
 0.28542194 0.285241   0.2852181  0.28545684 0.28558606 0.2852582
 0.2847275  0.2845006  0.28466082 0.2849376  0.28500086 0.28500214
 0.28517577 0.28516775 0.2849293  0.28466558 0.2845709  0.284407
 0.28430122 0.28465468 0.28493026 0.2847812  0.28487483 0.2849436
 0.28418663 0.28342968 0.2835611  0.28362557 0.28321862 0.28311354
 0.28327605 0.28337252 0.28366277 0.28382006 0.283752   0.2839087
 0.28407654 0.28408    0.28430986 0.28451896 0.28443682 0.28447294
 0.28456226 0.2843281  0.2842791  0.28450072 0.28427517 0.28396556
 0.28400332 0.28363895 0.2831718  0.28346092 0.283549   0.28285196
 0.28261656 0.28300202 0.28303045 0.28297415 0.28327665 0.28352684
 0.28342035 0.2831909  0.28282115 0.28279018 0.28321335 0.28335965
 0.2831954  0.28318915 0.28314635 0.28303277 0.28323737 0.28337446
 0.28305098 0.28264984 0.28249398 0.28254572 0.2826122  0.2823839
 0.28234097 0.28314358 0.2837903  0.28344035 0.283173   0.283427
 0.28339857 0.28346244 0.28360277 0.28337222 0.28326973 0.2835099
 0.28332794 0.2831243  0.2836343  0.2839484  0.28383824 0.28385785
 0.2832308  0.28223494 0.28248274 0.28293037 0.28238684 0.28264365
 0.28387308 0.2842144  0.28420848 0.28446007 0.2839595  0.28331816
 0.2834118  0.28314492 0.28315696 0.28380978 0.28393942 0.28391287
 0.2840644  0.28338936 0.28300765 0.28369406 0.28347662 0.28318563
 0.28434858 0.28429648 0.28339812 0.2846736  0.28521413 0.28376427
 0.28381136 0.28307626 0.28130606 0.2830382  0.28156355 0.28654563]
