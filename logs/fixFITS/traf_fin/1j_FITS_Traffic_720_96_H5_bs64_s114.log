Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j96_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j96_H5_FITS_custom_ftM_sl720_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3404417280.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 70.68210506439209
Epoch: 1, Steps: 89 | Train Loss: 0.5617995 Vali Loss: 0.4115498 Test Loss: 0.4829199
Validation loss decreased (inf --> 0.411550).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 69.12401700019836
Epoch: 2, Steps: 89 | Train Loss: 0.2637743 Vali Loss: 0.3426934 Test Loss: 0.4105693
Validation loss decreased (0.411550 --> 0.342693).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 68.93944787979126
Epoch: 3, Steps: 89 | Train Loss: 0.2423541 Vali Loss: 0.3380122 Test Loss: 0.4051965
Validation loss decreased (0.342693 --> 0.338012).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 72.59703135490417
Epoch: 4, Steps: 89 | Train Loss: 0.2404676 Vali Loss: 0.3367392 Test Loss: 0.4033867
Validation loss decreased (0.338012 --> 0.336739).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 68.20525646209717
Epoch: 5, Steps: 89 | Train Loss: 0.2397790 Vali Loss: 0.3345133 Test Loss: 0.4028394
Validation loss decreased (0.336739 --> 0.334513).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 70.05505657196045
Epoch: 6, Steps: 89 | Train Loss: 0.2392495 Vali Loss: 0.3348810 Test Loss: 0.4020836
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 66.52948474884033
Epoch: 7, Steps: 89 | Train Loss: 0.2389789 Vali Loss: 0.3346834 Test Loss: 0.4016735
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 66.44934630393982
Epoch: 8, Steps: 89 | Train Loss: 0.2386796 Vali Loss: 0.3331281 Test Loss: 0.4013871
Validation loss decreased (0.334513 --> 0.333128).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 60.740747690200806
Epoch: 9, Steps: 89 | Train Loss: 0.2385237 Vali Loss: 0.3341535 Test Loss: 0.4010681
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 62.111151695251465
Epoch: 10, Steps: 89 | Train Loss: 0.2384955 Vali Loss: 0.3338808 Test Loss: 0.4006556
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 55.25857710838318
Epoch: 11, Steps: 89 | Train Loss: 0.2384742 Vali Loss: 0.3335991 Test Loss: 0.4008199
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j96_H5_FITS_custom_ftM_sl720_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.3986215889453888, mae:0.2864854037761688, rse:0.5227979421615601, corr:[0.2904532  0.29465514 0.29305652 0.29524726 0.29792956 0.29823676
 0.297212   0.29701385 0.29797187 0.29851246 0.29752105 0.29582605
 0.29505917 0.29549068 0.2959034  0.29531813 0.29428634 0.29404575
 0.29479888 0.29557854 0.29559502 0.2951905  0.29536432 0.2967755
 0.29845294 0.29820454 0.29710686 0.2966687  0.2970572  0.2973706
 0.2968231  0.29571503 0.2950993  0.29545557 0.29605517 0.29594114
 0.29519165 0.29464924 0.2948645  0.29542422 0.29560837 0.29534164
 0.29526308 0.2958993  0.29693043 0.29754928 0.29746732 0.29728597
 0.29737023 0.29732254 0.29703963 0.29637027 0.29558414 0.29520553
 0.29534763 0.29553142 0.29528534 0.29476434 0.29459485 0.29507178
 0.29575628 0.29589036 0.2954649  0.29519993 0.29560375 0.29630607
 0.29652575 0.29613736 0.2958432  0.29620564 0.29679555 0.29679537
 0.29584742 0.29482144 0.29466522 0.29498813 0.2947979  0.29380003
 0.2928128  0.29282284 0.2937297  0.29444733 0.2943427  0.2940471
 0.29453364 0.29564863 0.2962757  0.29579633 0.29501557 0.29516625
 0.296036   0.29577526 0.2933088  0.29077876 0.29220536 0.2961988 ]
