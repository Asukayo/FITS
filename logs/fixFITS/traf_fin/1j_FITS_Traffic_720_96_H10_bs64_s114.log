Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j96_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=320, out_features=362, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12781322240.0
params:  116202.0
Trainable parameters:  116202
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 67.60464096069336
Epoch: 1, Steps: 89 | Train Loss: 0.5814861 Vali Loss: 0.4210800 Test Loss: 0.4909311
Validation loss decreased (inf --> 0.421080).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 72.24865531921387
Epoch: 2, Steps: 89 | Train Loss: 0.2622747 Vali Loss: 0.3339008 Test Loss: 0.3994757
Validation loss decreased (0.421080 --> 0.333901).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 74.29304075241089
Epoch: 3, Steps: 89 | Train Loss: 0.2346836 Vali Loss: 0.3294279 Test Loss: 0.3931476
Validation loss decreased (0.333901 --> 0.329428).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 63.015177726745605
Epoch: 4, Steps: 89 | Train Loss: 0.2324134 Vali Loss: 0.3273665 Test Loss: 0.3921518
Validation loss decreased (0.329428 --> 0.327366).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 63.08688473701477
Epoch: 5, Steps: 89 | Train Loss: 0.2319767 Vali Loss: 0.3269015 Test Loss: 0.3912126
Validation loss decreased (0.327366 --> 0.326902).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 54.299277544021606
Epoch: 6, Steps: 89 | Train Loss: 0.2317064 Vali Loss: 0.3250308 Test Loss: 0.3909990
Validation loss decreased (0.326902 --> 0.325031).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 53.19243550300598
Epoch: 7, Steps: 89 | Train Loss: 0.2316379 Vali Loss: 0.3256717 Test Loss: 0.3903193
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 70.78680610656738
Epoch: 8, Steps: 89 | Train Loss: 0.2314621 Vali Loss: 0.3251551 Test Loss: 0.3900631
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 64.6714141368866
Epoch: 9, Steps: 89 | Train Loss: 0.2312230 Vali Loss: 0.3251684 Test Loss: 0.3899755
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.3879580795764923, mae:0.27213573455810547, rse:0.5157578587532043, corr:[0.2787524  0.2886197  0.28985402 0.28947878 0.29244408 0.29170516
 0.2945891  0.29323113 0.2938872  0.2933779  0.29283515 0.29360756
 0.2916832  0.29220954 0.290571   0.2905058  0.29166928 0.2912334
 0.2924164  0.29203346 0.29222435 0.29199886 0.29154024 0.29218373
 0.29279307 0.29399076 0.29431123 0.29409915 0.2943688  0.2935274
 0.29430738 0.29314578 0.2919678  0.2916939  0.29034978 0.29119986
 0.2908137  0.2912562  0.29156357 0.29068652 0.2920682  0.2924865
 0.29301983 0.29271528 0.29204974 0.29277867 0.29217058 0.29255578
 0.29323027 0.29276338 0.2929146  0.29290408 0.29317868 0.29181072
 0.29116008 0.2916567  0.2909964  0.29115456 0.29020402 0.29079196
 0.29185116 0.2912898  0.29190627 0.2911139  0.29196444 0.29309884
 0.29308107 0.29404142 0.29323584 0.29318127 0.29277828 0.29207388
 0.29211104 0.29084927 0.29127365 0.29091337 0.29024756 0.2901143
 0.28920883 0.29016504 0.2897011  0.29003304 0.29078618 0.29070577
 0.29229105 0.29173383 0.29287797 0.29275286 0.29224056 0.29365027
 0.2914223  0.2917958  0.2880685  0.2882593  0.28702533 0.29356235]
