Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  22596812800.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 94.74436163902283
Epoch: 1, Steps: 84 | Train Loss: 0.9944615 Vali Loss: 0.9167346 Test Loss: 1.0750664
Validation loss decreased (inf --> 0.916735).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 88.58544659614563
Epoch: 2, Steps: 84 | Train Loss: 0.6525001 Vali Loss: 0.7296011 Test Loss: 0.8508936
Validation loss decreased (0.916735 --> 0.729601).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 95.34413123130798
Epoch: 3, Steps: 84 | Train Loss: 0.5216120 Vali Loss: 0.6143773 Test Loss: 0.7131962
Validation loss decreased (0.729601 --> 0.614377).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 93.8186309337616
Epoch: 4, Steps: 84 | Train Loss: 0.4376706 Vali Loss: 0.5383013 Test Loss: 0.6239496
Validation loss decreased (0.614377 --> 0.538301).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 101.09653425216675
Epoch: 5, Steps: 84 | Train Loss: 0.3820750 Vali Loss: 0.4876119 Test Loss: 0.5650613
Validation loss decreased (0.538301 --> 0.487612).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 100.71674251556396
Epoch: 6, Steps: 84 | Train Loss: 0.3448420 Vali Loss: 0.4532082 Test Loss: 0.5248563
Validation loss decreased (0.487612 --> 0.453208).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 94.93260836601257
Epoch: 7, Steps: 84 | Train Loss: 0.3199483 Vali Loss: 0.4303369 Test Loss: 0.4994445
Validation loss decreased (0.453208 --> 0.430337).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 89.19680452346802
Epoch: 8, Steps: 84 | Train Loss: 0.3033137 Vali Loss: 0.4156530 Test Loss: 0.4816999
Validation loss decreased (0.430337 --> 0.415653).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 94.44480895996094
Epoch: 9, Steps: 84 | Train Loss: 0.2922921 Vali Loss: 0.4054735 Test Loss: 0.4708293
Validation loss decreased (0.415653 --> 0.405474).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 99.9114899635315
Epoch: 10, Steps: 84 | Train Loss: 0.2850858 Vali Loss: 0.3994463 Test Loss: 0.4632075
Validation loss decreased (0.405474 --> 0.399446).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 99.28192162513733
Epoch: 11, Steps: 84 | Train Loss: 0.2801473 Vali Loss: 0.3950722 Test Loss: 0.4594010
Validation loss decreased (0.399446 --> 0.395072).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 100.13538718223572
Epoch: 12, Steps: 84 | Train Loss: 0.2770658 Vali Loss: 0.3927721 Test Loss: 0.4561671
Validation loss decreased (0.395072 --> 0.392772).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 102.4995539188385
Epoch: 13, Steps: 84 | Train Loss: 0.2748458 Vali Loss: 0.3898956 Test Loss: 0.4534192
Validation loss decreased (0.392772 --> 0.389896).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 103.83610510826111
Epoch: 14, Steps: 84 | Train Loss: 0.2734966 Vali Loss: 0.3892936 Test Loss: 0.4522955
Validation loss decreased (0.389896 --> 0.389294).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 102.88844466209412
Epoch: 15, Steps: 84 | Train Loss: 0.2725357 Vali Loss: 0.3877844 Test Loss: 0.4518891
Validation loss decreased (0.389294 --> 0.387784).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 110.02696681022644
Epoch: 16, Steps: 84 | Train Loss: 0.2718996 Vali Loss: 0.3873171 Test Loss: 0.4511054
Validation loss decreased (0.387784 --> 0.387317).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 106.16726756095886
Epoch: 17, Steps: 84 | Train Loss: 0.2713798 Vali Loss: 0.3867863 Test Loss: 0.4505496
Validation loss decreased (0.387317 --> 0.386786).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 109.46992230415344
Epoch: 18, Steps: 84 | Train Loss: 0.2710444 Vali Loss: 0.3867022 Test Loss: 0.4504502
Validation loss decreased (0.386786 --> 0.386702).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 135.1154932975769
Epoch: 19, Steps: 84 | Train Loss: 0.2709473 Vali Loss: 0.3868518 Test Loss: 0.4504121
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 110.14216017723083
Epoch: 20, Steps: 84 | Train Loss: 0.2706079 Vali Loss: 0.3863474 Test Loss: 0.4500019
Validation loss decreased (0.386702 --> 0.386347).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 88.83762264251709
Epoch: 21, Steps: 84 | Train Loss: 0.2706111 Vali Loss: 0.3866068 Test Loss: 0.4500573
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 84.67949175834656
Epoch: 22, Steps: 84 | Train Loss: 0.2704794 Vali Loss: 0.3865161 Test Loss: 0.4499379
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 85.97324705123901
Epoch: 23, Steps: 84 | Train Loss: 0.2703745 Vali Loss: 0.3856570 Test Loss: 0.4498142
Validation loss decreased (0.386347 --> 0.385657).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 88.25436997413635
Epoch: 24, Steps: 84 | Train Loss: 0.2702628 Vali Loss: 0.3861719 Test Loss: 0.4496500
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 93.44345474243164
Epoch: 25, Steps: 84 | Train Loss: 0.2701761 Vali Loss: 0.3857719 Test Loss: 0.4497350
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 89.86444091796875
Epoch: 26, Steps: 84 | Train Loss: 0.2702181 Vali Loss: 0.3861913 Test Loss: 0.4499650
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.44928544759750366, mae:0.300457626581192, rse:0.5481027364730835, corr:[0.2577359  0.26257944 0.26260173 0.26377818 0.26838356 0.26725766
 0.2712613  0.26962563 0.27081686 0.26997098 0.2689173  0.2692079
 0.2666089  0.26707113 0.26550105 0.2656771  0.2662625  0.26552388
 0.2670673  0.26678586 0.26754665 0.26741245 0.26708958 0.26753026
 0.26847878 0.27010182 0.26966262 0.26922244 0.2694557  0.26828352
 0.26878655 0.2681498  0.26780152 0.26782435 0.2668411  0.26726884
 0.2664545  0.2667102  0.26721355 0.26699916 0.26788858 0.26759914
 0.26806495 0.26825726 0.2680728  0.26822466 0.26760814 0.2683463
 0.2688406  0.26893634 0.26941234 0.26847443 0.26833677 0.26780707
 0.2674795  0.26776904 0.2669519  0.26695514 0.26657858 0.26650378
 0.26698855 0.2666718  0.26725182 0.26714128 0.26731002 0.26783574
 0.2676168  0.26806757 0.26808426 0.2681498  0.26784858 0.2676805
 0.26807415 0.26751947 0.26771715 0.26756185 0.2670935  0.26744938
 0.26696858 0.2668685  0.26660046 0.2663164  0.26662895 0.2662847
 0.26660034 0.26660562 0.26667434 0.26748455 0.26754287 0.2679483
 0.26791775 0.26749146 0.26739767 0.26692885 0.26722267 0.26724282
 0.26727083 0.2676636  0.26732275 0.267419   0.26687935 0.2667266
 0.26693875 0.26651686 0.26672655 0.2665584  0.26655746 0.2666864
 0.2663081  0.26657477 0.26672548 0.26722184 0.26749066 0.26710442
 0.26757067 0.26729685 0.26712227 0.2671927  0.26695916 0.2673656
 0.2673635  0.26770005 0.26772714 0.2673159  0.26749086 0.26726365
 0.26750395 0.26753    0.26699847 0.26702663 0.26703194 0.26721904
 0.2669823  0.26704538 0.26770052 0.26785445 0.26816982 0.26771334
 0.26746598 0.26749066 0.26734057 0.26751563 0.26740247 0.2675031
 0.26806146 0.26776907 0.26768032 0.2674271  0.26716584 0.26717424
 0.26671135 0.2669214  0.267014   0.26705045 0.26735196 0.26743647
 0.26784745 0.26775175 0.2679262  0.26842603 0.26824617 0.26831982
 0.26835716 0.2682372  0.26805598 0.26775038 0.26793328 0.26799518
 0.26932195 0.2696314  0.26930276 0.26934177 0.26885077 0.26840183
 0.2683736  0.26841918 0.26881033 0.26885635 0.26901087 0.26897264
 0.2688205  0.26916707 0.26935795 0.26962772 0.2693462  0.2690243
 0.26935655 0.26928696 0.2690744  0.26878783 0.26887715 0.2691036
 0.26940596 0.26975837 0.26960564 0.26921982 0.26917526 0.26867273
 0.26879236 0.26892713 0.26886418 0.26905426 0.2689633  0.2693312
 0.26931834 0.269131   0.26940766 0.26947832 0.26953715 0.26939154
 0.2695498  0.26961917 0.26938233 0.26919752 0.26847675 0.26841924
 0.26873577 0.26872846 0.26901704 0.26865402 0.26860282 0.26867694
 0.268385   0.2686901  0.26886937 0.26910147 0.2691762  0.26898608
 0.2692454  0.26909608 0.26903737 0.26897395 0.26882982 0.26904365
 0.26900175 0.2690407  0.26885095 0.26876092 0.26876405 0.2683364
 0.26845774 0.2682385  0.26817396 0.26853332 0.26829135 0.26839527
 0.26827946 0.26819855 0.2685311  0.26851878 0.2687424  0.26879495
 0.26877233 0.26877722 0.26858687 0.2691459  0.26938865 0.26923674
 0.269092   0.26859847 0.2686728  0.26844186 0.26839072 0.26845765
 0.26780003 0.2681562  0.2683344  0.26821804 0.2681821  0.26791093
 0.2683073  0.2683629  0.26818886 0.26840505 0.26849496 0.26874164
 0.2686     0.26854092 0.2689135  0.26911524 0.26905134 0.26857832
 0.26850605 0.2683548  0.2681031  0.26794168 0.26763836 0.26801455
 0.26803696 0.26803425 0.26855895 0.26843908 0.26859474 0.2687033
 0.26855496 0.2688492  0.2686333  0.26870766 0.26872584 0.26860338
 0.26885033 0.26881972 0.26926264 0.2695955  0.2696293  0.26956946
 0.2690303  0.26883888 0.26886597 0.2688295  0.2690157  0.26880404
 0.2690702  0.2691728  0.26928777 0.26960856 0.26928446 0.2692513
 0.26915923 0.2689788  0.2690098  0.2688286  0.2692044  0.26962024
 0.26975235 0.26976684 0.26972115 0.27009657 0.27002704 0.26972666
 0.26959342 0.2693078  0.26950246 0.26943952 0.2693046  0.2694239
 0.2703412  0.27060738 0.2701377  0.26999697 0.2700988  0.26974344
 0.26973945 0.26970017 0.26983547 0.27011073 0.2701895  0.27043068
 0.27040583 0.27066448 0.27095526 0.27082667 0.27065945 0.27043608
 0.27046248 0.27018726 0.26992208 0.26984656 0.2695416  0.2699614
 0.27061874 0.270613   0.27062497 0.2702442  0.27050868 0.27067062
 0.2705916  0.2706883  0.27032146 0.27059132 0.27081835 0.27081513
 0.27100378 0.2705738  0.27059242 0.2706984  0.27052182 0.27061734
 0.2704274  0.27039737 0.27033722 0.27014816 0.2696876  0.26920962
 0.26969227 0.26982096 0.26985222 0.27005488 0.26996842 0.27028033
 0.27046645 0.27057272 0.27084282 0.27080625 0.2708114  0.27056184
 0.27034807 0.27021933 0.27012053 0.27026847 0.270029   0.2700429
 0.2698757  0.269416   0.26955393 0.26946405 0.26945868 0.26936677
 0.26913407 0.26938894 0.269075   0.26910236 0.26941958 0.26936617
 0.26960003 0.2695311  0.26961187 0.26982495 0.26969075 0.26969662
 0.26950654 0.26956022 0.2696753  0.26966482 0.2696367  0.26930213
 0.26954594 0.2692931  0.2687813  0.2689055  0.2686624  0.26847956
 0.26793858 0.26823986 0.26903966 0.26899385 0.2692989  0.26947662
 0.26968265 0.26998314 0.26970276 0.26967546 0.2695004  0.2694812
 0.26950547 0.26930857 0.26958463 0.26950577 0.26948798 0.26933867
 0.26895085 0.26905113 0.26862374 0.26829964 0.26834294 0.26815268
 0.2685042  0.2686774  0.26896793 0.26943806 0.26943544 0.26983136
 0.27001503 0.27017552 0.27011636 0.2698781  0.27009162 0.2698229
 0.26962528 0.26957634 0.26965043 0.27006197 0.26985875 0.26966664
 0.2693729  0.26905996 0.26943514 0.26932496 0.26903594 0.26889825
 0.26922998 0.26965073 0.26941618 0.26958212 0.26975697 0.26998702
 0.27048686 0.27040726 0.27054602 0.27053133 0.27039436 0.27044198
 0.27014083 0.27003556 0.27003124 0.27024832 0.270183   0.26981756
 0.26975533 0.269505   0.26977584 0.26972395 0.26908657 0.26928595
 0.26995134 0.270261   0.27049652 0.2704071  0.27072197 0.27040038
 0.2701591  0.27039155 0.2702834  0.2705054  0.27036652 0.27037114
 0.27034187 0.26988333 0.27005908 0.2699235  0.2698352  0.26979786
 0.26936403 0.26931256 0.2691557  0.2692664  0.26926434 0.2691658
 0.2697961  0.2697759  0.26994717 0.27010062 0.27028286 0.2706603
 0.27037907 0.2704004  0.27047625 0.27058995 0.2708333  0.27024004
 0.26985976 0.2694435  0.26918814 0.26943037 0.26935008 0.26962
 0.2693241  0.26894593 0.26891392 0.26864776 0.26893073 0.2688863
 0.2690525  0.26948425 0.2696137  0.26978478 0.26963076 0.27000126
 0.2702399  0.269694   0.26989436 0.26983047 0.26976687 0.26984113
 0.2691132  0.26891336 0.26883507 0.26906687 0.26913917 0.268836
 0.26885754 0.267972   0.2679258  0.26824644 0.26807493 0.26853976
 0.26816532 0.26855296 0.26921815 0.2691402  0.26953238 0.2692565
 0.26917994 0.2688753  0.26856163 0.26893312 0.26832995 0.26795956
 0.26753175 0.26721463 0.26759526 0.26705983 0.26722002 0.2673059
 0.26692697 0.26700908 0.26651543 0.2668651  0.26686907 0.26663378
 0.267039   0.26713008 0.2680401  0.2681238  0.26799637 0.26830754
 0.26781142 0.26796168 0.26756945 0.2674094  0.26763117 0.26697254
 0.26684067 0.26618874 0.2659587  0.26612148 0.26587534 0.26643515
 0.26602033 0.26602277 0.26635447 0.26600257 0.26686552 0.26708236
 0.26740015 0.26812437 0.2680256  0.26840317 0.2680944  0.26821718
 0.26812193 0.26752856 0.26769122 0.2672649  0.26752752 0.2673674
 0.2665788  0.26650596 0.2660043  0.26617977 0.2661498  0.2663136
 0.26680133 0.26654333 0.26728696 0.2673196  0.26723057 0.26779148
 0.26780862 0.2686034  0.26887074 0.26885232 0.26893422 0.26820973
 0.2683503  0.26789728 0.26767936 0.267651   0.26700738 0.26758847
 0.2669781  0.26663274 0.26652983 0.26616594 0.2670423  0.26682115
 0.26722533 0.26753426 0.26762587 0.2682927  0.26763818 0.26855132
 0.2699912  0.27021107 0.27085596 0.27017823 0.27012306 0.26951265
 0.2689046  0.2691041  0.26776463 0.2682647  0.26816747 0.2678698
 0.26827466 0.26732588 0.26835784 0.2681303  0.2686534  0.26966187
 0.26910168 0.27019373 0.26964945 0.27008805 0.2703398  0.26928282
 0.27087134 0.27086505 0.27119097 0.27045727 0.26941347 0.26928782
 0.2670179  0.26775843 0.26676035 0.26691106 0.26863188 0.2677151
 0.269815   0.26870197 0.27084264 0.27210718 0.27143878 0.2735655
 0.2692023  0.27035877 0.2642921  0.2636791  0.2585601  0.27192023]
