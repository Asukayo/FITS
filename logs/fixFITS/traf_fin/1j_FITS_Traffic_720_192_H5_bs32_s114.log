Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j192_H5_FITS_custom_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1902468480.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4779579
	speed: 0.5263s/iter; left time: 4605.7327s
Epoch: 1 cost time: 90.73746657371521
Epoch: 1, Steps: 177 | Train Loss: 0.5925531 Vali Loss: 0.4243416 Test Loss: 0.4968550
Validation loss decreased (inf --> 0.424342).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2592517
	speed: 1.4622s/iter; left time: 12536.9195s
Epoch: 2 cost time: 86.90501141548157
Epoch: 2, Steps: 177 | Train Loss: 0.2702748 Vali Loss: 0.3455192 Test Loss: 0.4154442
Validation loss decreased (0.424342 --> 0.345519).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2492907
	speed: 1.3840s/iter; left time: 11621.4032s
Epoch: 3 cost time: 80.73918318748474
Epoch: 3, Steps: 177 | Train Loss: 0.2474525 Vali Loss: 0.3402441 Test Loss: 0.4115935
Validation loss decreased (0.345519 --> 0.340244).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2431902
	speed: 1.3785s/iter; left time: 11331.1687s
Epoch: 4 cost time: 91.19355130195618
Epoch: 4, Steps: 177 | Train Loss: 0.2455883 Vali Loss: 0.3381767 Test Loss: 0.4102454
Validation loss decreased (0.340244 --> 0.338177).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2445061
	speed: 1.5519s/iter; left time: 12481.9861s
Epoch: 5 cost time: 93.9914116859436
Epoch: 5, Steps: 177 | Train Loss: 0.2450412 Vali Loss: 0.3374003 Test Loss: 0.4105707
Validation loss decreased (0.338177 --> 0.337400).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2341420
	speed: 1.3637s/iter; left time: 10726.8312s
Epoch: 6 cost time: 66.43898367881775
Epoch: 6, Steps: 177 | Train Loss: 0.2447872 Vali Loss: 0.3371537 Test Loss: 0.4091207
Validation loss decreased (0.337400 --> 0.337154).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2459964
	speed: 1.2054s/iter; left time: 9268.1843s
Epoch: 7 cost time: 94.60968446731567
Epoch: 7, Steps: 177 | Train Loss: 0.2445204 Vali Loss: 0.3367427 Test Loss: 0.4096559
Validation loss decreased (0.337154 --> 0.336743).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2394894
	speed: 1.5847s/iter; left time: 11904.1124s
Epoch: 8 cost time: 97.84677505493164
Epoch: 8, Steps: 177 | Train Loss: 0.2444444 Vali Loss: 0.3366049 Test Loss: 0.4089996
Validation loss decreased (0.336743 --> 0.336605).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2493555
	speed: 1.6202s/iter; left time: 11883.9950s
Epoch: 9 cost time: 100.75387597084045
Epoch: 9, Steps: 177 | Train Loss: 0.2443874 Vali Loss: 0.3368548 Test Loss: 0.4097526
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2326177
	speed: 1.6017s/iter; left time: 11464.7221s
Epoch: 10 cost time: 97.27127194404602
Epoch: 10, Steps: 177 | Train Loss: 0.2442748 Vali Loss: 0.3365726 Test Loss: 0.4096117
Validation loss decreased (0.336605 --> 0.336573).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2359158
	speed: 1.5755s/iter; left time: 10998.3227s
Epoch: 11 cost time: 93.47863721847534
Epoch: 11, Steps: 177 | Train Loss: 0.2443343 Vali Loss: 0.3361036 Test Loss: 0.4099410
Validation loss decreased (0.336573 --> 0.336104).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2456654
	speed: 1.4678s/iter; left time: 9987.1767s
Epoch: 12 cost time: 94.41954517364502
Epoch: 12, Steps: 177 | Train Loss: 0.2442650 Vali Loss: 0.3359891 Test Loss: 0.4093157
Validation loss decreased (0.336104 --> 0.335989).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2560356
	speed: 1.6569s/iter; left time: 10980.2126s
Epoch: 13 cost time: 108.05318093299866
Epoch: 13, Steps: 177 | Train Loss: 0.2441851 Vali Loss: 0.3361870 Test Loss: 0.4092587
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2607661
	speed: 1.6310s/iter; left time: 10520.2576s
Epoch: 14 cost time: 100.31134510040283
Epoch: 14, Steps: 177 | Train Loss: 0.2442035 Vali Loss: 0.3357593 Test Loss: 0.4090964
Validation loss decreased (0.335989 --> 0.335759).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2606838
	speed: 1.6075s/iter; left time: 10083.5760s
Epoch: 15 cost time: 102.19092559814453
Epoch: 15, Steps: 177 | Train Loss: 0.2440916 Vali Loss: 0.3361359 Test Loss: 0.4093351
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2306938
	speed: 1.5886s/iter; left time: 9683.9540s
Epoch: 16 cost time: 96.27943158149719
Epoch: 16, Steps: 177 | Train Loss: 0.2441124 Vali Loss: 0.3356784 Test Loss: 0.4089531
Validation loss decreased (0.335759 --> 0.335678).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2372067
	speed: 1.5194s/iter; left time: 8993.1269s
Epoch: 17 cost time: 105.36946058273315
Epoch: 17, Steps: 177 | Train Loss: 0.2440481 Vali Loss: 0.3360945 Test Loss: 0.4090729
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2623554
	speed: 1.5343s/iter; left time: 8810.0404s
Epoch: 18 cost time: 78.53108143806458
Epoch: 18, Steps: 177 | Train Loss: 0.2440618 Vali Loss: 0.3362153 Test Loss: 0.4091817
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2571713
	speed: 1.4229s/iter; left time: 7918.5890s
Epoch: 19 cost time: 103.11317133903503
Epoch: 19, Steps: 177 | Train Loss: 0.2440673 Vali Loss: 0.3363167 Test Loss: 0.4090613
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j192_H5_FITS_custom_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.4081217050552368, mae:0.2877907156944275, rse:0.5272588729858398, corr:[0.2881571  0.2924516  0.29310232 0.2948652  0.2954754  0.29488868
 0.29442695 0.29443997 0.29423556 0.2934332  0.29259375 0.29241663
 0.29283375 0.29314855 0.2930724  0.2929367  0.29313532 0.29357517
 0.29383582 0.29372868 0.2934207  0.29313895 0.29301745 0.29335335
 0.29385832 0.29338154 0.2929284  0.2930335  0.29331005 0.29337218
 0.2932471  0.29317406 0.29323235 0.2932065  0.2929088  0.2925532
 0.29256377 0.29293284 0.29329985 0.29335177 0.2931548  0.2929795
 0.29293862 0.29289615 0.29265234 0.29221594 0.2918996  0.29210585
 0.29258382 0.2926386  0.29246372 0.2923023  0.29227304 0.29231596
 0.29224482 0.29199305 0.2917527  0.29172957 0.29185435 0.29192924
 0.2919308  0.29191923 0.29201785 0.29219007 0.2922933  0.2922528
 0.29211205 0.29198796 0.29188415 0.29169837 0.29137185 0.29110992
 0.29098767 0.29109442 0.2913456  0.29152167 0.2916659  0.291931
 0.29229537 0.29254252 0.292483   0.29215026 0.29179937 0.29170197
 0.2918774  0.29201669 0.2918864  0.29156265 0.29134652 0.29134452
 0.2913245  0.29105383 0.29069293 0.2905853  0.29074907 0.29085514
 0.2904024  0.28969532 0.2893364  0.28955847 0.29018664 0.290787
 0.29099393 0.29081672 0.290549   0.29042917 0.29051206 0.2907449
 0.2910711  0.2914322  0.29177335 0.29197603 0.29193082 0.2916637
 0.2912528  0.2908414  0.29056686 0.29040614 0.29043406 0.2905598
 0.29067257 0.29088455 0.29106894 0.29102746 0.29096535 0.29104158
 0.29121834 0.2913321  0.2913016  0.2912295  0.29127645 0.29141825
 0.29146412 0.2913264  0.29115972 0.2911631  0.2913215  0.29142046
 0.29139614 0.29136613 0.29148078 0.2917009  0.291752   0.29156888
 0.2913537  0.29123402 0.29138735 0.29167515 0.29187828 0.29192036
 0.2918591  0.29175922 0.2915567  0.29118758 0.29076836 0.29059196
 0.29079804 0.2911844  0.29145202 0.29153    0.29163504 0.29187208
 0.292139   0.29229334 0.29232955 0.2923423  0.29246697 0.29304665
 0.2938231  0.29370463 0.29344964 0.29350376 0.2936654  0.2935805
 0.2929803  0.29200995 0.29116243 0.2907898  0.29076067 0.2908323
 0.29115525 0.29199553 0.29320008 0.2939968  0.29384992 0.29322085
 0.29288018 0.292844   0.29210111 0.29028502 0.28939444 0.2920825 ]
