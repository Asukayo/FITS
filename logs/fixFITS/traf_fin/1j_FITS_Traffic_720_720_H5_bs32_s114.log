Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j720_H5_FITS_custom_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3003897600.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7748879
	speed: 0.6255s/iter; left time: 5223.9216s
Epoch: 1 cost time: 107.40208125114441
Epoch: 1, Steps: 169 | Train Loss: 0.9130414 Vali Loss: 0.7944483 Test Loss: 0.9266356
Validation loss decreased (inf --> 0.794448).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4882201
	speed: 1.6908s/iter; left time: 13834.1689s
Epoch: 2 cost time: 99.46167325973511
Epoch: 2, Steps: 169 | Train Loss: 0.5200386 Vali Loss: 0.5686207 Test Loss: 0.6598306
Validation loss decreased (0.794448 --> 0.568621).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3669176
	speed: 1.4580s/iter; left time: 11683.3288s
Epoch: 3 cost time: 91.18297219276428
Epoch: 3, Steps: 169 | Train Loss: 0.3808150 Vali Loss: 0.4663053 Test Loss: 0.5405167
Validation loss decreased (0.568621 --> 0.466305).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3092836
	speed: 1.7209s/iter; left time: 13498.9398s
Epoch: 4 cost time: 106.76596689224243
Epoch: 4, Steps: 169 | Train Loss: 0.3184937 Vali Loss: 0.4219648 Test Loss: 0.4895383
Validation loss decreased (0.466305 --> 0.421965).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2901940
	speed: 1.8193s/iter; left time: 13962.7470s
Epoch: 5 cost time: 120.82073879241943
Epoch: 5, Steps: 169 | Train Loss: 0.2922191 Vali Loss: 0.4045485 Test Loss: 0.4695883
Validation loss decreased (0.421965 --> 0.404548).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2780450
	speed: 2.0061s/iter; left time: 15057.8445s
Epoch: 6 cost time: 121.83348345756531
Epoch: 6, Steps: 169 | Train Loss: 0.2818420 Vali Loss: 0.3976790 Test Loss: 0.4616151
Validation loss decreased (0.404548 --> 0.397679).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2810885
	speed: 1.8455s/iter; left time: 13540.5826s
Epoch: 7 cost time: 113.31700372695923
Epoch: 7, Steps: 169 | Train Loss: 0.2780771 Vali Loss: 0.3949132 Test Loss: 0.4596603
Validation loss decreased (0.397679 --> 0.394913).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2830709
	speed: 1.7433s/iter; left time: 12496.3311s
Epoch: 8 cost time: 99.88672280311584
Epoch: 8, Steps: 169 | Train Loss: 0.2765973 Vali Loss: 0.3943848 Test Loss: 0.4584175
Validation loss decreased (0.394913 --> 0.394385).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2800048
	speed: 1.6683s/iter; left time: 11676.3762s
Epoch: 9 cost time: 107.87003135681152
Epoch: 9, Steps: 169 | Train Loss: 0.2760397 Vali Loss: 0.3938664 Test Loss: 0.4582347
Validation loss decreased (0.394385 --> 0.393866).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2693282
	speed: 1.6677s/iter; left time: 11390.5662s
Epoch: 10 cost time: 95.13497495651245
Epoch: 10, Steps: 169 | Train Loss: 0.2757532 Vali Loss: 0.3940871 Test Loss: 0.4575174
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2929018
	speed: 1.7955s/iter; left time: 11959.5287s
Epoch: 11 cost time: 126.79079675674438
Epoch: 11, Steps: 169 | Train Loss: 0.2755026 Vali Loss: 0.3935766 Test Loss: 0.4570767
Validation loss decreased (0.393866 --> 0.393577).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2689892
	speed: 1.7539s/iter; left time: 11386.5064s
Epoch: 12 cost time: 110.56836557388306
Epoch: 12, Steps: 169 | Train Loss: 0.2753769 Vali Loss: 0.3933517 Test Loss: 0.4573760
Validation loss decreased (0.393577 --> 0.393352).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2646650
	speed: 1.9684s/iter; left time: 12446.3959s
Epoch: 13 cost time: 116.41303062438965
Epoch: 13, Steps: 169 | Train Loss: 0.2752972 Vali Loss: 0.3926549 Test Loss: 0.4576402
Validation loss decreased (0.393352 --> 0.392655).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2728810
	speed: 2.0196s/iter; left time: 12428.7134s
Epoch: 14 cost time: 131.3231861591339
Epoch: 14, Steps: 169 | Train Loss: 0.2751973 Vali Loss: 0.3929654 Test Loss: 0.4570648
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2749866
	speed: 2.1284s/iter; left time: 12738.5181s
Epoch: 15 cost time: 125.16900157928467
Epoch: 15, Steps: 169 | Train Loss: 0.2751122 Vali Loss: 0.3929636 Test Loss: 0.4572800
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2831805
	speed: 2.0641s/iter; left time: 12004.5888s
Epoch: 16 cost time: 122.16615033149719
Epoch: 16, Steps: 169 | Train Loss: 0.2750938 Vali Loss: 0.3925471 Test Loss: 0.4570094
Validation loss decreased (0.392655 --> 0.392547).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2761697
	speed: 1.7612s/iter; left time: 9945.3404s
Epoch: 17 cost time: 119.79457759857178
Epoch: 17, Steps: 169 | Train Loss: 0.2750434 Vali Loss: 0.3927428 Test Loss: 0.4574594
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2711660
	speed: 2.0538s/iter; left time: 11250.4474s
Epoch: 18 cost time: 134.09595274925232
Epoch: 18, Steps: 169 | Train Loss: 0.2749842 Vali Loss: 0.3923194 Test Loss: 0.4573631
Validation loss decreased (0.392547 --> 0.392319).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2610672
	speed: 1.8854s/iter; left time: 10009.4596s
Epoch: 19 cost time: 116.67815494537354
Epoch: 19, Steps: 169 | Train Loss: 0.2749961 Vali Loss: 0.3923706 Test Loss: 0.4572873
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2742930
	speed: 1.9377s/iter; left time: 9959.9241s
Epoch: 20 cost time: 125.75462317466736
Epoch: 20, Steps: 169 | Train Loss: 0.2749016 Vali Loss: 0.3922803 Test Loss: 0.4569453
Validation loss decreased (0.392319 --> 0.392280).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2755114
	speed: 1.9450s/iter; left time: 9668.8271s
Epoch: 21 cost time: 100.80296802520752
Epoch: 21, Steps: 169 | Train Loss: 0.2749171 Vali Loss: 0.3921394 Test Loss: 0.4570974
Validation loss decreased (0.392280 --> 0.392139).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2734309
	speed: 1.4871s/iter; left time: 7140.9312s
Epoch: 22 cost time: 93.23496913909912
Epoch: 22, Steps: 169 | Train Loss: 0.2748746 Vali Loss: 0.3925680 Test Loss: 0.4571085
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2768963
	speed: 1.4830s/iter; left time: 6870.5390s
Epoch: 23 cost time: 88.35395336151123
Epoch: 23, Steps: 169 | Train Loss: 0.2748187 Vali Loss: 0.3925245 Test Loss: 0.4574449
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2782224
	speed: 1.6025s/iter; left time: 7153.4554s
Epoch: 24 cost time: 120.23065114021301
Epoch: 24, Steps: 169 | Train Loss: 0.2747622 Vali Loss: 0.3923001 Test Loss: 0.4567325
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j720_H5_FITS_custom_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4565654695034027, mae:0.310331255197525, rse:0.552525520324707, corr:[0.26905257 0.26517567 0.26568434 0.27085152 0.27311897 0.2725065
 0.27205503 0.27289498 0.2736872  0.27299452 0.2712305  0.26996952
 0.26991045 0.27008343 0.26956347 0.2687565  0.26875073 0.26969624
 0.27055827 0.27055153 0.27012542 0.2701769  0.2710124  0.2724859
 0.2737793  0.27352044 0.2729436  0.2729264  0.273033   0.272587
 0.2716132  0.2708159  0.2707567  0.2711347  0.27113995 0.27051932
 0.26993513 0.2700146  0.27066064 0.27123067 0.27138758 0.27139837
 0.27162766 0.27201843 0.2721499  0.27177167 0.27125978 0.27139565
 0.2721242  0.27250138 0.27231875 0.2717182  0.27111554 0.27083573
 0.27072886 0.27044111 0.26998806 0.26976168 0.2699717  0.27031252
 0.27037752 0.27006    0.26986673 0.2702294  0.27096167 0.27143967
 0.2713121  0.2709411  0.27090025 0.27129123 0.2716447  0.2716471
 0.27123845 0.2709021  0.27098906 0.27112257 0.27092588 0.27045393
 0.27004763 0.26993018 0.2700106  0.27007028 0.2699236  0.26979506
 0.2699707  0.27040717 0.27082708 0.27101794 0.27101287 0.27091387
 0.27096003 0.270859   0.27055243 0.27025712 0.27020833 0.2704878
 0.27066645 0.27061325 0.2703701  0.2701221  0.27011782 0.27027032
 0.27024174 0.26987964 0.269481   0.2695277  0.26987284 0.27012205
 0.2700345  0.2697755  0.26975596 0.2701228  0.2705008  0.27042845
 0.2702027  0.26995248 0.26994598 0.27007368 0.27018243 0.27008066
 0.26983085 0.26983327 0.27012402 0.27029213 0.27026188 0.27019894
 0.27028984 0.27042392 0.2704645  0.27045888 0.270527   0.2708744
 0.27136466 0.27168715 0.2717092  0.27158484 0.27147534 0.2711779
 0.27126572 0.27122417 0.2710016  0.270771   0.2706338  0.27065414
 0.27073547 0.2706016  0.27032977 0.27008343 0.26999664 0.270057
 0.2701345  0.27013165 0.2700978  0.27026576 0.27055842 0.2708467
 0.27103636 0.2711593  0.2713322  0.27155903 0.27168417 0.27160776
 0.27147606 0.271423   0.27152318 0.2716432  0.2716264  0.27188027
 0.27258888 0.27265322 0.27247778 0.27217472 0.2716789  0.27128154
 0.2712196  0.27141225 0.27157587 0.2716075  0.27166432 0.27193892
 0.27235425 0.27257025 0.27247548 0.27226523 0.27221474 0.27237016
 0.27245426 0.27234727 0.2721217  0.2719058  0.27185437 0.27219027
 0.2726431  0.27242047 0.2719883  0.27177942 0.27179354 0.27188465
 0.27192262 0.2719377  0.27210248 0.27248573 0.27287337 0.27303627
 0.27294913 0.2727229  0.27258545 0.27258778 0.2726005  0.27251542
 0.27236983 0.27230668 0.27234447 0.2722836  0.27198872 0.2717402
 0.2717035  0.27168933 0.2716756  0.27155814 0.2713467  0.27125034
 0.27139357 0.2716489  0.2718007  0.27176183 0.27164266 0.27164122
 0.2718398  0.27199522 0.27194104 0.2717868  0.2717544  0.271852
 0.27183747 0.2715462  0.27116722 0.27084464 0.2708003  0.2709683
 0.2710191  0.2708295  0.2706061  0.2705144  0.2705903  0.27077362
 0.27104223 0.27142406 0.2718758  0.27226216 0.27227685 0.27187464
 0.27135324 0.27107844 0.27118427 0.2714741  0.27162144 0.27146456
 0.27121475 0.2710767  0.27115303 0.27127615 0.2712665  0.2711835
 0.2711328  0.27129433 0.27145725 0.2712839  0.27087408 0.27070162
 0.2709321  0.27135774 0.2716379  0.27177566 0.27179542 0.2718239
 0.27185667 0.27176842 0.27154678 0.27134344 0.27127558 0.27127534
 0.27139574 0.27127072 0.27091688 0.2706138  0.27058867 0.27077132
 0.2708951  0.27102193 0.27118373 0.2713082  0.27141693 0.2715223
 0.27163517 0.2718024  0.2720821  0.27240428 0.27255318 0.27250788
 0.27237907 0.2723164  0.27237496 0.27244845 0.27237773 0.27206025
 0.27195257 0.2719349  0.27192694 0.27178097 0.2715258  0.27142495
 0.2716529  0.27196333 0.2721031  0.27194443 0.27169803 0.27167538
 0.27194676 0.27228388 0.27244186 0.27245504 0.27247685 0.27259138
 0.27265105 0.27253348 0.2723304  0.2722266  0.27227923 0.2723262
 0.27235013 0.2722664  0.27216637 0.27215868 0.27220476 0.27249336
 0.27292302 0.27277243 0.27264202 0.27282476 0.27304554 0.2731134
 0.2730751  0.27313802 0.27339652 0.27371648 0.27382278 0.27363312
 0.27332303 0.27313402 0.27319577 0.27336228 0.27344474 0.27339977
 0.27331263 0.2732643  0.27318767 0.27292213 0.27245897 0.27239975
 0.27292332 0.27322012 0.27332297 0.27332938 0.27324978 0.2732357
 0.2733666  0.2735062  0.273519   0.27346465 0.27350473 0.27373934
 0.27395508 0.27383968 0.2733896  0.27294806 0.2728499  0.27307364
 0.2732326  0.27306515 0.27270374 0.27240425 0.2723107  0.27250096
 0.27272025 0.27270338 0.2727067  0.27284965 0.2729637  0.27285722
 0.27257702 0.27240345 0.27258837 0.27305418 0.2734088  0.27336878
 0.27301228 0.27253264 0.2721501  0.27188292 0.2716769  0.2715406
 0.27150077 0.27153403 0.27152938 0.27142036 0.27126545 0.27131993
 0.27155867 0.27183148 0.27199098 0.2719961  0.27199164 0.27212596
 0.2723303  0.2723757  0.27223116 0.27208537 0.27204737 0.2721195
 0.27218217 0.27204478 0.27178732 0.27167758 0.27183703 0.2720614
 0.27207974 0.27168232 0.2711851  0.27100095 0.27108654 0.2712414
 0.27118227 0.2711632  0.2713303  0.27169243 0.27207777 0.27232233
 0.2724124  0.27248427 0.27258846 0.27261323 0.2724037  0.27197334
 0.27153677 0.27128246 0.27120695 0.27115318 0.27099866 0.2708604
 0.2710201  0.27110416 0.27089727 0.27046522 0.2702256  0.2704586
 0.27090678 0.27133223 0.27154028 0.27153656 0.27164376 0.27197444
 0.27232513 0.27245975 0.2723708  0.2722597  0.2722914  0.2724064
 0.27240133 0.27217695 0.2718532  0.27163884 0.27161324 0.27169278
 0.27172583 0.27155274 0.27131733 0.27120036 0.27123275 0.2713692
 0.27153546 0.27168387 0.27195317 0.27229592 0.27255026 0.2726491
 0.2727027  0.27281877 0.2729141  0.27278262 0.27239183 0.27195543
 0.27173758 0.2717855  0.27189693 0.2719101  0.2718199  0.27175996
 0.2717871  0.27180174 0.27170214 0.27148506 0.27136657 0.27181384
 0.27270165 0.273002   0.27297997 0.2728992  0.27279449 0.27275068
 0.27276427 0.27277726 0.27277154 0.2727786  0.272767   0.27267858
 0.272397   0.2719411  0.27159324 0.27154133 0.27173707 0.2719014
 0.2718029  0.27155748 0.27139792 0.27138567 0.2714688  0.27176705
 0.272168   0.27221853 0.2723028  0.272577   0.27281386 0.27289328
 0.27290505 0.27298564 0.2731071  0.27306983 0.27272725 0.2722582
 0.27193925 0.27184772 0.2717964  0.27155432 0.27115822 0.27087596
 0.27084836 0.270931   0.27090442 0.27074876 0.2706459  0.27090254
 0.2713556  0.27153066 0.27147794 0.2714465  0.27158266 0.2718194
 0.2719346  0.2718063  0.27157038 0.27147278 0.27156165 0.2716773
 0.27157825 0.27116367 0.27070644 0.27055258 0.2707142  0.27084962
 0.27061608 0.2701239  0.26983395 0.270026   0.270471   0.27078497
 0.2706999  0.27041388 0.27032477 0.27046728 0.27066854 0.27080774
 0.27090505 0.27103272 0.2711642  0.27113277 0.2707924  0.27028522
 0.2699069  0.26970023 0.26946473 0.26906258 0.26864603 0.2684854
 0.26855636 0.2685513  0.26826632 0.26791376 0.26788595 0.26833776
 0.26886263 0.26921093 0.26934126 0.26937863 0.26962277 0.2700372
 0.27023342 0.2699892  0.26950377 0.2691224  0.26890662 0.26866835
 0.2682811  0.26786166 0.26767164 0.26782578 0.26819026 0.2685304
 0.26867008 0.26857883 0.26848394 0.26854894 0.26880217 0.26909316
 0.2692964  0.2696615  0.2701746  0.27046993 0.27048406 0.27042472
 0.27049914 0.27068576 0.2706866  0.2702158  0.26941913 0.26879182
 0.2686244  0.26869008 0.26857725 0.26820686 0.26792166 0.2681514
 0.26875433 0.26919365 0.2691535  0.26882342 0.26871657 0.26917464
 0.26994067 0.27041122 0.2704872  0.27044562 0.2705901  0.27086285
 0.2709349  0.27065113 0.27021503 0.2699003  0.26969737 0.26933616
 0.26870766 0.26811612 0.2679673  0.26826054 0.26861972 0.26876542
 0.26887676 0.26934037 0.27008995 0.27065584 0.2707297  0.2708556
 0.2713878  0.27163953 0.27178943 0.27176854 0.27144375 0.27106678
 0.2708804  0.27076373 0.27038193 0.26969403 0.26914048 0.26923972
 0.26985624 0.27027693 0.27016273 0.26991373 0.27023572 0.27113974
 0.2718949  0.27197114 0.27164206 0.2715616  0.27204192 0.27287784
 0.27327695 0.2726305  0.27193576 0.27186406 0.27192286 0.27132526
 0.27010164 0.26920268 0.2692964  0.2698132  0.2697811  0.26940167
 0.2699208  0.27164772 0.2733582  0.27363345 0.27283654 0.27262136
 0.27356997 0.27384028 0.2706186  0.2646777  0.26412785 0.27200255]
