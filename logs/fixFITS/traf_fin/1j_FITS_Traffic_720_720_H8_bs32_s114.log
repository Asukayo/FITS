Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H8', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7344405504.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7617814
	speed: 0.7647s/iter; left time: 6386.1596s
Epoch: 1 cost time: 127.24720621109009
Epoch: 1, Steps: 169 | Train Loss: 0.8837522 Vali Loss: 0.7750231 Test Loss: 0.9019638
Validation loss decreased (inf --> 0.775023).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4831104
	speed: 1.7755s/iter; left time: 14527.3665s
Epoch: 2 cost time: 91.21049976348877
Epoch: 2, Steps: 169 | Train Loss: 0.5047148 Vali Loss: 0.5525568 Test Loss: 0.6404202
Validation loss decreased (0.775023 --> 0.552557).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3642411
	speed: 1.5990s/iter; left time: 12812.7178s
Epoch: 3 cost time: 124.58446049690247
Epoch: 3, Steps: 169 | Train Loss: 0.3693302 Vali Loss: 0.4540467 Test Loss: 0.5266294
Validation loss decreased (0.552557 --> 0.454047).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3195015
	speed: 2.1174s/iter; left time: 16608.6712s
Epoch: 4 cost time: 119.60811400413513
Epoch: 4, Steps: 169 | Train Loss: 0.3103867 Vali Loss: 0.4128371 Test Loss: 0.4791493
Validation loss decreased (0.454047 --> 0.412837).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2821354
	speed: 1.9585s/iter; left time: 15031.7243s
Epoch: 5 cost time: 128.6961853504181
Epoch: 5, Steps: 169 | Train Loss: 0.2862204 Vali Loss: 0.3972077 Test Loss: 0.4607983
Validation loss decreased (0.412837 --> 0.397208).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2688344
	speed: 2.0982s/iter; left time: 15749.1940s
Epoch: 6 cost time: 114.35758113861084
Epoch: 6, Steps: 169 | Train Loss: 0.2769631 Vali Loss: 0.3911099 Test Loss: 0.4539953
Validation loss decreased (0.397208 --> 0.391110).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2729627
	speed: 1.9529s/iter; left time: 14328.6264s
Epoch: 7 cost time: 136.0786590576172
Epoch: 7, Steps: 169 | Train Loss: 0.2735185 Vali Loss: 0.3895500 Test Loss: 0.4526558
Validation loss decreased (0.391110 --> 0.389550).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2778459
	speed: 2.2331s/iter; left time: 16006.5605s
Epoch: 8 cost time: 133.57280898094177
Epoch: 8, Steps: 169 | Train Loss: 0.2722028 Vali Loss: 0.3877705 Test Loss: 0.4517520
Validation loss decreased (0.389550 --> 0.387771).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2746038
	speed: 2.1820s/iter; left time: 15271.6619s
Epoch: 9 cost time: 139.09875392913818
Epoch: 9, Steps: 169 | Train Loss: 0.2716744 Vali Loss: 0.3875147 Test Loss: 0.4519431
Validation loss decreased (0.387771 --> 0.387515).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2772563
	speed: 2.1681s/iter; left time: 14807.8545s
Epoch: 10 cost time: 132.35405611991882
Epoch: 10, Steps: 169 | Train Loss: 0.2713946 Vali Loss: 0.3868726 Test Loss: 0.4500562
Validation loss decreased (0.387515 --> 0.386873).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2706195
	speed: 1.9967s/iter; left time: 13299.7050s
Epoch: 11 cost time: 106.40255284309387
Epoch: 11, Steps: 169 | Train Loss: 0.2712843 Vali Loss: 0.3870217 Test Loss: 0.4509452
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2658037
	speed: 1.6356s/iter; left time: 10618.5794s
Epoch: 12 cost time: 104.86293172836304
Epoch: 12, Steps: 169 | Train Loss: 0.2711811 Vali Loss: 0.3870429 Test Loss: 0.4511857
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2689116
	speed: 1.5679s/iter; left time: 9913.6705s
Epoch: 13 cost time: 91.5485508441925
Epoch: 13, Steps: 169 | Train Loss: 0.2710881 Vali Loss: 0.3869881 Test Loss: 0.4506047
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.44951584935188293, mae:0.3016308844089508, rse:0.5482432842254639, corr:[0.26253197 0.2582351  0.2666466  0.26356342 0.26698732 0.27114007
 0.26979142 0.27096862 0.27178082 0.26927388 0.2689193  0.2687906
 0.26637006 0.26632163 0.26717952 0.26593652 0.26603118 0.2668993
 0.2664508  0.26717764 0.26834923 0.26816452 0.2685239  0.26935622
 0.26998404 0.2704819  0.2712421  0.27036208 0.26983708 0.27075318
 0.27044827 0.2693933  0.26906726 0.26811472 0.2674117  0.26808685
 0.26783162 0.2671031  0.26789215 0.2681604  0.26747254 0.26792547
 0.2685056  0.26831886 0.26869282 0.26902315 0.26861164 0.26901114
 0.26968002 0.2689541  0.26869106 0.26902124 0.26842093 0.26798993
 0.26843736 0.26818216 0.26768008 0.26785564 0.2674119  0.26675147
 0.26717138 0.267516   0.26769882 0.26841643 0.26847076 0.26798782
 0.26823875 0.26823002 0.26765504 0.26785418 0.26806375 0.267817
 0.2681811  0.2685092  0.2677494  0.26722172 0.26751617 0.26764125
 0.2677543  0.26792422 0.26748446 0.2671979  0.26743788 0.2673068
 0.26712114 0.26760614 0.26804435 0.2681914  0.26845953 0.26825923
 0.26789787 0.2678668  0.26770064 0.26749876 0.2679899  0.26843587
 0.26794648 0.26775938 0.26778886 0.26718867 0.26687574 0.26697934
 0.26657444 0.2661216  0.26620135 0.2663681  0.26656866 0.26706687
 0.26729742 0.26725787 0.26745355 0.26747572 0.26725203 0.26729432
 0.2672874  0.2666315  0.26656014 0.26706198 0.267181   0.26736826
 0.26792443 0.26822913 0.2681532  0.26788932 0.2673717  0.267214
 0.2674664  0.2670223  0.26645157 0.26670483 0.267066   0.26750338
 0.26835117 0.26877868 0.26866412 0.26878834 0.26867932 0.26781136
 0.26795158 0.2682416  0.26786992 0.26764986 0.26758334 0.2670709
 0.26732045 0.26820403 0.26822612 0.26774555 0.2679967  0.26819167
 0.2679017  0.2678619  0.26771557 0.26748708 0.26787674 0.26835644
 0.26830104 0.26842654 0.26876608 0.26878592 0.2688996  0.26915404
 0.26917294 0.26922476 0.26958323 0.2698122  0.26968703 0.26957178
 0.27008078 0.27001458 0.26988646 0.2689982  0.26813573 0.26846525
 0.26887795 0.26882666 0.2692913  0.26973248 0.26944372 0.26938036
 0.269667   0.26961932 0.2696552  0.26990062 0.2699729  0.27010128
 0.2700153  0.26938272 0.26896626 0.26907796 0.26890814 0.2688086
 0.269714   0.27019614 0.27025872 0.27023885 0.2700484  0.26977
 0.26964787 0.26936266 0.26911327 0.2697717  0.27059546 0.27045506
 0.2701005  0.27004227 0.26979977 0.26978564 0.27019465 0.27007744
 0.26951334 0.26908603 0.26853096 0.26830322 0.26878992 0.26901832
 0.26874456 0.26865193 0.26884484 0.2688542  0.26885536 0.26884705
 0.26879495 0.26890782 0.26883474 0.26859623 0.2690327  0.26971406
 0.2698537  0.27020407 0.27082878 0.27050745 0.26978296 0.2697014
 0.26940462 0.26868346 0.26869297 0.26906198 0.26918897 0.26944697
 0.2694546  0.26881385 0.2683379  0.26826897 0.26836196 0.26856634
 0.26845568 0.26807642 0.26829365 0.26895788 0.26903462 0.26880652
 0.26888254 0.26914605 0.26968977 0.27024028 0.27009383 0.26962915
 0.26948154 0.26906145 0.26852623 0.26869467 0.26901913 0.26888362
 0.2686286  0.26833308 0.2678887  0.2679392  0.26839092 0.2687141
 0.26897797 0.26917362 0.2691032  0.26926512 0.26945868 0.26925898
 0.26924887 0.26982245 0.27024883 0.27016485 0.26992324 0.26932487
 0.2685495  0.26796982 0.2676068  0.2673476  0.26771724 0.2685068
 0.26873347 0.26890033 0.26913255 0.26875272 0.26819807 0.268377
 0.2689125  0.26926106 0.2694883  0.26950777 0.2695108  0.26980218
 0.27002075 0.27023035 0.2707688  0.27101204 0.2706307  0.27004966
 0.2695678  0.2687919  0.26861632 0.26918867 0.26948446 0.2695472
 0.2699953  0.27012032 0.26965234 0.26916164 0.26897356 0.26914433
 0.26967725 0.27000552 0.26981112 0.26964685 0.2698507  0.27016747
 0.27036974 0.27043948 0.2704441  0.27048168 0.27048066 0.2703586
 0.27043703 0.27035975 0.2700696  0.27027294 0.27066764 0.27068493
 0.27122918 0.271508   0.2709728  0.26990664 0.2697362  0.270325
 0.27060574 0.27068245 0.2707885  0.2708543  0.27108014 0.2712272
 0.27110425 0.27126038 0.27155215 0.27138662 0.27133065 0.27156952
 0.27128792 0.27073592 0.27054268 0.27017686 0.26933905 0.26924604
 0.27036315 0.27092314 0.2709439  0.27056494 0.2704131  0.27107382
 0.27150127 0.2710028  0.27068585 0.27099478 0.27106705 0.2710712
 0.27135208 0.2713282  0.2710467  0.27100173 0.27102455 0.27100846
 0.27082923 0.2702093  0.2697663  0.26995477 0.2698084  0.26942933
 0.26978034 0.27019438 0.27014107 0.2700919  0.2702094  0.2702088
 0.27028364 0.27022803 0.26978156 0.26962048 0.26990837 0.2699816
 0.2701189  0.2707468  0.27122766 0.27105978 0.270604   0.27017063
 0.26975462 0.26935452 0.2690102  0.2691632  0.2698253  0.27017063
 0.26983663 0.26956078 0.26935506 0.26905164 0.26933125 0.26993203
 0.2699229  0.2697134  0.26983213 0.26984212 0.26993838 0.27045104
 0.2704877  0.27015665 0.27056804 0.27094623 0.27041748 0.26987028
 0.269667   0.26928064 0.26923403 0.26967856 0.26985523 0.269911
 0.26985177 0.26974273 0.2698286  0.269865   0.26971862 0.26988763
 0.27004546 0.26972148 0.26959372 0.26997253 0.27029845 0.27053854
 0.2705564  0.27002877 0.26962134 0.2697166  0.26940584 0.2686865
 0.26860765 0.2684679  0.26800072 0.2682202  0.26893234 0.26908916
 0.2691036  0.2697501  0.27023688 0.27047718 0.2704808  0.27003056
 0.26984715 0.27020246 0.2701324  0.26967603 0.26972678 0.2699541
 0.26996744 0.2701187  0.27023757 0.2701206  0.27008682 0.2698443
 0.2692151  0.2690059  0.26942223 0.26957843 0.26946682 0.26954883
 0.2696831  0.269708   0.26993468 0.27001798 0.27012026 0.27060893
 0.27096692 0.27111244 0.27138987 0.2712084  0.27052307 0.270207
 0.27012    0.26989412 0.26996967 0.2700729  0.2697279  0.26977885
 0.2703781  0.2704345  0.27021182 0.27037603 0.27026877 0.27013004
 0.2708192  0.27075705 0.27028805 0.27043244 0.27088803 0.2710265
 0.27110085 0.27114004 0.27098167 0.27095002 0.27088457 0.27059957
 0.2706051  0.2708257  0.2708373  0.27080008 0.27065852 0.27020636
 0.26985884 0.26965153 0.2692971  0.2693054  0.2695551  0.26937437
 0.26946527 0.26990956 0.27030137 0.27016646 0.27031818 0.27086204
 0.270863   0.27046457 0.27024403 0.27018172 0.2703313  0.27049848
 0.27034235 0.27050024 0.27097073 0.27078286 0.27047703 0.27056453
 0.26992908 0.26885283 0.26870772 0.26906747 0.26919055 0.2695403
 0.2696521  0.26921996 0.2693693  0.26991484 0.2701006  0.2702465
 0.2702665  0.26971012 0.26922724 0.26900604 0.26839623 0.26813456
 0.26885834 0.2694098  0.26934657 0.26918462 0.26867023 0.26815164
 0.26825216 0.2681695  0.2679179  0.26848158 0.26888987 0.2685511
 0.2686072  0.26887932 0.2685735  0.26862404 0.2691105  0.26899943
 0.26882067 0.26891023 0.26850903 0.26800215 0.26801017 0.2680366
 0.2680436  0.2682352  0.26813233 0.26792356 0.26801875 0.26780006
 0.26738498 0.26752347 0.2674408  0.2669859  0.26741284 0.26807436
 0.2675778  0.2673342  0.26780468 0.2679803  0.26831633 0.2686619
 0.2682125  0.26796755 0.26835194 0.2679782  0.2674334  0.26796925
 0.26824114 0.2675205  0.26694486 0.2664375  0.2657119  0.26574
 0.26623392 0.26638427 0.26682982 0.2673277  0.26720497 0.26746264
 0.26829413 0.26856434 0.2683988  0.26855686 0.26847318 0.26841787
 0.26860982 0.26813912 0.26761723 0.267751   0.267408   0.2667922
 0.2670106  0.2671809  0.26700783 0.26722446 0.26706347 0.26668787
 0.2671864  0.26749864 0.26710925 0.2676174  0.26853496 0.26844817
 0.26863623 0.2692753  0.26921114 0.26904806 0.26945332 0.2693241
 0.26897722 0.26925167 0.2687717  0.26760948 0.26734084 0.2671002
 0.26645994 0.26660192 0.26668116 0.26640904 0.2671241  0.26779696
 0.26781982 0.2686836  0.26961374 0.26947987 0.2694493  0.26981822
 0.26998478 0.26984805 0.27006412 0.26959983 0.2695487  0.2705848
 0.2706094  0.26993188 0.2701007  0.2694503  0.26803058 0.26813415
 0.2682996  0.26794618 0.26888043 0.26931182 0.2689125  0.27012983
 0.2710368  0.27037072 0.27102968 0.27179688 0.27021024 0.26945665
 0.2704902  0.27027586 0.2699556  0.26997507 0.26913473 0.26929727
 0.26977968 0.26787373 0.26707184 0.2683303  0.26757702 0.2680354
 0.2708555  0.27085322 0.27132368 0.27398503 0.27314913 0.27244818
 0.2740547  0.26867512 0.26417157 0.26671413 0.25470167 0.27246875]
