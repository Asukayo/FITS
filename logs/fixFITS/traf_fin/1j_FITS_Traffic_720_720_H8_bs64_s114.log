Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H8', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14688811008.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 88.24150323867798
Epoch: 1, Steps: 84 | Train Loss: 1.0610943 Vali Loss: 0.9756333 Test Loss: 1.1439925
Validation loss decreased (inf --> 0.975633).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 103.93771505355835
Epoch: 2, Steps: 84 | Train Loss: 0.6967174 Vali Loss: 0.7735915 Test Loss: 0.9012291
Validation loss decreased (0.975633 --> 0.773592).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 102.94454979896545
Epoch: 3, Steps: 84 | Train Loss: 0.5550577 Vali Loss: 0.6470969 Test Loss: 0.7511761
Validation loss decreased (0.773592 --> 0.647097).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 97.77096009254456
Epoch: 4, Steps: 84 | Train Loss: 0.4629562 Vali Loss: 0.5628924 Test Loss: 0.6524122
Validation loss decreased (0.647097 --> 0.562892).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 97.35329723358154
Epoch: 5, Steps: 84 | Train Loss: 0.4009368 Vali Loss: 0.5056076 Test Loss: 0.5861242
Validation loss decreased (0.562892 --> 0.505608).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 96.25062298774719
Epoch: 6, Steps: 84 | Train Loss: 0.3587719 Vali Loss: 0.4671417 Test Loss: 0.5414873
Validation loss decreased (0.505608 --> 0.467142).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 98.48375701904297
Epoch: 7, Steps: 84 | Train Loss: 0.3300709 Vali Loss: 0.4402201 Test Loss: 0.5106707
Validation loss decreased (0.467142 --> 0.440220).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 95.56347274780273
Epoch: 8, Steps: 84 | Train Loss: 0.3108880 Vali Loss: 0.4224573 Test Loss: 0.4902346
Validation loss decreased (0.440220 --> 0.422457).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 92.50485849380493
Epoch: 9, Steps: 84 | Train Loss: 0.2978618 Vali Loss: 0.4112458 Test Loss: 0.4771841
Validation loss decreased (0.422457 --> 0.411246).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 105.76292705535889
Epoch: 10, Steps: 84 | Train Loss: 0.2890381 Vali Loss: 0.4038023 Test Loss: 0.4683968
Validation loss decreased (0.411246 --> 0.403802).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 96.19703078269958
Epoch: 11, Steps: 84 | Train Loss: 0.2832821 Vali Loss: 0.3980021 Test Loss: 0.4623715
Validation loss decreased (0.403802 --> 0.398002).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 96.11344742774963
Epoch: 12, Steps: 84 | Train Loss: 0.2793525 Vali Loss: 0.3946096 Test Loss: 0.4586096
Validation loss decreased (0.398002 --> 0.394610).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 88.21495985984802
Epoch: 13, Steps: 84 | Train Loss: 0.2767071 Vali Loss: 0.3919190 Test Loss: 0.4556629
Validation loss decreased (0.394610 --> 0.391919).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 95.8264491558075
Epoch: 14, Steps: 84 | Train Loss: 0.2749326 Vali Loss: 0.3907284 Test Loss: 0.4545244
Validation loss decreased (0.391919 --> 0.390728).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 95.80039834976196
Epoch: 15, Steps: 84 | Train Loss: 0.2738226 Vali Loss: 0.3894663 Test Loss: 0.4528832
Validation loss decreased (0.390728 --> 0.389466).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 100.00889492034912
Epoch: 16, Steps: 84 | Train Loss: 0.2729591 Vali Loss: 0.3891093 Test Loss: 0.4527359
Validation loss decreased (0.389466 --> 0.389109).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 98.39890360832214
Epoch: 17, Steps: 84 | Train Loss: 0.2723567 Vali Loss: 0.3887199 Test Loss: 0.4520841
Validation loss decreased (0.389109 --> 0.388720).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 101.27356243133545
Epoch: 18, Steps: 84 | Train Loss: 0.2720021 Vali Loss: 0.3877088 Test Loss: 0.4514708
Validation loss decreased (0.388720 --> 0.387709).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 105.31992936134338
Epoch: 19, Steps: 84 | Train Loss: 0.2716911 Vali Loss: 0.3884680 Test Loss: 0.4518762
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 98.83072710037231
Epoch: 20, Steps: 84 | Train Loss: 0.2715290 Vali Loss: 0.3875313 Test Loss: 0.4508855
Validation loss decreased (0.387709 --> 0.387531).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 111.97519755363464
Epoch: 21, Steps: 84 | Train Loss: 0.2713294 Vali Loss: 0.3870361 Test Loss: 0.4508163
Validation loss decreased (0.387531 --> 0.387036).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 104.14407348632812
Epoch: 22, Steps: 84 | Train Loss: 0.2712474 Vali Loss: 0.3873690 Test Loss: 0.4508661
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 111.79465699195862
Epoch: 23, Steps: 84 | Train Loss: 0.2711129 Vali Loss: 0.3873408 Test Loss: 0.4505698
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 128.6863875389099
Epoch: 24, Steps: 84 | Train Loss: 0.2709766 Vali Loss: 0.3867410 Test Loss: 0.4505921
Validation loss decreased (0.387036 --> 0.386741).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 124.21265149116516
Epoch: 25, Steps: 84 | Train Loss: 0.2709793 Vali Loss: 0.3871503 Test Loss: 0.4504489
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 106.26209354400635
Epoch: 26, Steps: 84 | Train Loss: 0.2708687 Vali Loss: 0.3878494 Test Loss: 0.4507918
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 93.24607229232788
Epoch: 27, Steps: 84 | Train Loss: 0.2708060 Vali Loss: 0.3869242 Test Loss: 0.4507081
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.45004746317863464, mae:0.30182579159736633, rse:0.5485673546791077, corr:[0.26295012 0.25728607 0.26645148 0.26352125 0.26697567 0.27130783
 0.2697997  0.27095813 0.2721736  0.26982203 0.26946297 0.2694814
 0.2667677  0.26611042 0.26662448 0.26517668 0.26553038 0.26702088
 0.26657546 0.2669561  0.26809576 0.26772195 0.26782128 0.26881814
 0.26959696 0.2699984  0.27092344 0.27034852 0.26969424 0.27018228
 0.2695898  0.2685437  0.26860836 0.2680647  0.26723394 0.26759937
 0.26741326 0.26677978 0.2674923  0.26803178 0.26786205 0.26846415
 0.26892012 0.26867932 0.268927   0.26902342 0.26854658 0.26891768
 0.2696049  0.26913047 0.26904887 0.269281   0.2685919  0.26810977
 0.26824868 0.26756716 0.26680583 0.26696858 0.26675162 0.2664484
 0.26708096 0.26726684 0.26702997 0.2675969  0.26807472 0.26799676
 0.2682513  0.268384   0.2681371  0.26834372 0.26837936 0.26800898
 0.26813817 0.26830882 0.26770228 0.26746032 0.26777574 0.26731858
 0.26670465 0.26690057 0.26696506 0.26678672 0.26694903 0.26696447
 0.26691914 0.26740524 0.26762584 0.2674573  0.26795018 0.26836768
 0.26816452 0.26801965 0.26803687 0.26784593 0.26788533 0.26801556
 0.26745924 0.26731148 0.26751015 0.2670836  0.26679835 0.26691887
 0.2666718  0.2665216  0.26689565 0.26701286 0.26685727 0.2671557
 0.26733798 0.2671071  0.26720363 0.2674828  0.2675568  0.26767954
 0.26787674 0.26763228 0.2675848  0.267533   0.26719344 0.26727962
 0.26759106 0.26747453 0.26730782 0.2674054  0.26728374 0.26725256
 0.267557   0.26735952 0.26709852 0.26752615 0.26781067 0.26772156
 0.26789314 0.26813474 0.26814198 0.26832974 0.26854426 0.26808587
 0.26811478 0.26821628 0.2679856  0.267745   0.26765352 0.26761436
 0.26807016 0.26854718 0.26823375 0.2676008  0.26750013 0.26747224
 0.26729044 0.26735267 0.2673773  0.26747462 0.26794618 0.268244
 0.2681864  0.2684687  0.26886433 0.26886103 0.26894018 0.26917368
 0.2691583  0.26901606 0.2689117  0.2686132  0.26840618 0.26871374
 0.2696353  0.26958352 0.26964045 0.26949608 0.269055   0.2690963
 0.26916388 0.26884744 0.26888457 0.26920593 0.26929513 0.26955435
 0.26988265 0.26981637 0.2699134  0.2702826  0.27034563 0.2702986
 0.2702643  0.26990676 0.26949674 0.26936182 0.26906353 0.26889578
 0.2697838  0.270165   0.26997855 0.26987246 0.2698363  0.26964223
 0.269555   0.26947355 0.2692547  0.26950476 0.27001613 0.2700609
 0.27009332 0.27041438 0.27054396 0.27053967 0.2705187  0.270161
 0.26980782 0.2697343  0.2694322  0.26912287 0.26912272 0.26898432
 0.26882038 0.26897064 0.26913851 0.26875055 0.26859877 0.2689232
 0.26904848 0.269127   0.26936498 0.2693999  0.2695285  0.26983467
 0.26969123 0.26938093 0.26949736 0.2694354  0.26907817 0.26903668
 0.26906794 0.26894638 0.26905307 0.2690022  0.2687074  0.2686896
 0.26866308 0.26834384 0.26823637 0.26820263 0.26799393 0.26807463
 0.26844463 0.26861328 0.26881665 0.26925275 0.26926845 0.26895568
 0.2688469  0.26878288 0.2688133  0.26914352 0.26927423 0.269108
 0.26921755 0.26921296 0.2688789  0.2688152  0.26889378 0.26870745
 0.26853922 0.26866892 0.26854947 0.26840916 0.26840365 0.26839843
 0.26864165 0.26907334 0.26919767 0.26931122 0.2694093  0.2692185
 0.2691121  0.2692969  0.26924813 0.2690353  0.26908192 0.2689125
 0.2686386  0.26859024 0.2685119  0.26811713 0.268079   0.2683843
 0.26824665 0.26830932 0.26871523 0.26876467 0.26856488 0.26860783
 0.26871395 0.2688386  0.26919106 0.26946598 0.26952577 0.26969883
 0.26986748 0.2699332  0.2700693  0.27002326 0.26973397 0.26952115
 0.26958534 0.2692707  0.2689918  0.2689158  0.26870006 0.26870283
 0.26913634 0.26924583 0.26925665 0.26958403 0.26979363 0.26979446
 0.26990584 0.26985106 0.26970434 0.26993006 0.27015248 0.270129
 0.27020678 0.2703323  0.2703232  0.27031615 0.27019462 0.26983497
 0.26985148 0.27004933 0.2698732  0.2696717  0.26968533 0.26972818
 0.27042824 0.27074486 0.27069503 0.2703989  0.27052048 0.27086237
 0.2708039  0.27065963 0.2706747  0.27079335 0.27112374 0.27127242
 0.270994   0.27086985 0.27090052 0.2706292  0.2705098  0.27065852
 0.27048275 0.27016762 0.27003872 0.26982063 0.26964507 0.2700723
 0.27076522 0.27053878 0.2704521  0.27063316 0.27058798 0.2707453
 0.27108237 0.27105734 0.27100042 0.2712432  0.2713763  0.27143466
 0.2714232  0.2710273  0.27070057 0.270781   0.27061653 0.27023354
 0.27026674 0.27042755 0.27030864 0.27013138 0.26984835 0.2697349
 0.27008137 0.270265   0.27016237 0.27016214 0.27039513 0.27047083
 0.2703639  0.27029145 0.27032655 0.27057305 0.27083966 0.27073422
 0.27049118 0.2703618  0.2702012  0.27009726 0.27014202 0.27000612
 0.26982537 0.26983783 0.26962808 0.26940444 0.2695472  0.2695052
 0.2690919  0.26921624 0.26969004 0.26973525 0.26974612 0.2698706
 0.2698446  0.26998037 0.2703078  0.27037415 0.27030125 0.27037317
 0.2702863  0.27005506 0.27001783 0.26994058 0.26980573 0.2698259
 0.2697638  0.26951593 0.26948738 0.26931232 0.26878342 0.26871523
 0.26875925 0.2685876  0.26875865 0.2692529  0.2693106  0.2693599
 0.26967245 0.26972294 0.2698275  0.2702877  0.2703619  0.2700331
 0.26991403 0.26974395 0.26942265 0.26945686 0.26946476 0.26912948
 0.2692929  0.26946807 0.26905683 0.26875338 0.26897955 0.26910037
 0.26909092 0.2693927  0.2694183  0.2693822  0.26968643 0.26987422
 0.26998165 0.27028912 0.27038544 0.27032158 0.2704982  0.27049914
 0.27018002 0.2700239  0.26992103 0.26964545 0.26954737 0.2696164
 0.26960227 0.26955098 0.26945734 0.26929986 0.26954418 0.27006623
 0.27017015 0.2699573  0.27010354 0.2702471  0.27021247 0.27041075
 0.27059618 0.2704719  0.2704118  0.27042732 0.27043116 0.2705864
 0.27053913 0.27013168 0.2698896  0.26983044 0.26968265 0.2697672
 0.27002028 0.2700232  0.270092   0.2702935  0.2701001  0.27007538
 0.2709604  0.27100542 0.2705771  0.27046585 0.2706552  0.27087083
 0.27093637 0.2706291  0.27029625 0.27049398 0.2706177  0.27024826
 0.27010494 0.27012083 0.26979598 0.2696659  0.26973963 0.26938906
 0.26931354 0.2698019  0.2696684  0.26921222 0.26942307 0.2697533
 0.27001756 0.27015862 0.2703942  0.27025828 0.27022305 0.2705452
 0.27068192 0.27067393 0.27059966 0.27036974 0.27042043 0.27062368
 0.2702288  0.26974827 0.26970783 0.2693878  0.2688925  0.26879856
 0.26865557 0.26850078 0.26875257 0.2688014  0.26848936 0.26879364
 0.2693939  0.2693814  0.2693836  0.2695135  0.2694948  0.26976424
 0.2701914  0.27011877 0.26981184 0.26966473 0.26940432 0.269197
 0.2691262  0.26869595 0.2683443  0.268488   0.2683343  0.2680003
 0.26825896 0.2684764  0.26835215 0.26866555 0.26891    0.26864266
 0.26860946 0.26878214 0.26861477 0.26868922 0.2690403  0.26897717
 0.26893225 0.2691801  0.2691262  0.2688439  0.26858887 0.26814142
 0.26778227 0.26767877 0.26736313 0.26714534 0.26743412 0.26741067
 0.2670608  0.26728934 0.26745406 0.26710236 0.26722786 0.26759863
 0.26725426 0.2672367  0.2676893  0.2675386  0.26743937 0.26782754
 0.26788124 0.2678848  0.26819488 0.26795644 0.26738244 0.26722547
 0.26694676 0.26645654 0.26636034 0.26635313 0.26641658 0.26697117
 0.26717833 0.2667775  0.26692855 0.26729593 0.26713055 0.2672932
 0.2678685  0.26801285 0.26799744 0.26836443 0.26836148 0.26814643
 0.2681488  0.26779595 0.26753977 0.26785666 0.26767784 0.26715165
 0.26719195 0.26700088 0.26648462 0.26655275 0.266606   0.26655552
 0.26709476 0.26733145 0.26703015 0.26753208 0.26822555 0.2679751
 0.2680587  0.26862565 0.2685184  0.26837543 0.26891586 0.26896375
 0.26849827 0.26831686 0.2677008  0.266936   0.26705018 0.26697496
 0.26662236 0.26706564 0.26716498 0.26673216 0.26720798 0.2675553
 0.267294   0.26794025 0.26856127 0.26822647 0.26846948 0.26931748
 0.2697733  0.26973715 0.27010918 0.26985267 0.26960206 0.26989186
 0.2692505  0.26820508 0.26805526 0.26755825 0.26701972 0.26776838
 0.2679297  0.26741517 0.26815256 0.2685481  0.2684703  0.26986328
 0.2706393  0.2699834  0.27057904 0.2711785  0.27004236 0.2701144
 0.27145118 0.2709578  0.27037767 0.27003247 0.26864392 0.2683819
 0.26858935 0.26681742 0.26657078 0.2679702  0.26713496 0.26773825
 0.27027607 0.2700204  0.27092442 0.2737109  0.2726309  0.2720385
 0.27369797 0.26829568 0.264059   0.26641572 0.2542543  0.27354252]
