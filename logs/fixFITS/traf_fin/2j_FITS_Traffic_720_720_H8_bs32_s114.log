Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H8', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7344405504.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.9372174
	speed: 0.7355s/iter; left time: 6142.1154s
Epoch: 1 cost time: 125.6988914012909
Epoch: 1, Steps: 169 | Train Loss: 1.0640275 Vali Loss: 1.0862088 Test Loss: 1.2733480
Validation loss decreased (inf --> 1.086209).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7306885
	speed: 1.9780s/iter; left time: 16183.6942s
Epoch: 2 cost time: 118.37538242340088
Epoch: 2, Steps: 169 | Train Loss: 0.7444796 Vali Loss: 0.9501553 Test Loss: 1.1097580
Validation loss decreased (1.086209 --> 0.950155).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6185073
	speed: 1.8844s/iter; left time: 15099.6574s
Epoch: 3 cost time: 115.34665822982788
Epoch: 3, Steps: 169 | Train Loss: 0.6251425 Vali Loss: 0.8536780 Test Loss: 0.9966525
Validation loss decreased (0.950155 --> 0.853678).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5366829
	speed: 1.9265s/iter; left time: 15111.6692s
Epoch: 4 cost time: 121.49004578590393
Epoch: 4, Steps: 169 | Train Loss: 0.5375567 Vali Loss: 0.7759612 Test Loss: 0.9057348
Validation loss decreased (0.853678 --> 0.775961).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4625620
	speed: 1.9285s/iter; left time: 14801.4345s
Epoch: 5 cost time: 119.4955644607544
Epoch: 5, Steps: 169 | Train Loss: 0.4691595 Vali Loss: 0.7122542 Test Loss: 0.8308132
Validation loss decreased (0.775961 --> 0.712254).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4041735
	speed: 1.9090s/iter; left time: 14329.0817s
Epoch: 6 cost time: 107.44207668304443
Epoch: 6, Steps: 169 | Train Loss: 0.4141559 Vali Loss: 0.6590962 Test Loss: 0.7686186
Validation loss decreased (0.712254 --> 0.659096).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3737272
	speed: 1.9012s/iter; left time: 13949.1813s
Epoch: 7 cost time: 124.41457223892212
Epoch: 7, Steps: 169 | Train Loss: 0.3693550 Vali Loss: 0.6150142 Test Loss: 0.7172406
Validation loss decreased (0.659096 --> 0.615014).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3328132
	speed: 1.8377s/iter; left time: 13172.5386s
Epoch: 8 cost time: 105.2482168674469
Epoch: 8, Steps: 169 | Train Loss: 0.3324413 Vali Loss: 0.5765029 Test Loss: 0.6728693
Validation loss decreased (0.615014 --> 0.576503).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3001163
	speed: 1.7388s/iter; left time: 12169.5773s
Epoch: 9 cost time: 109.39139819145203
Epoch: 9, Steps: 169 | Train Loss: 0.3018391 Vali Loss: 0.5466697 Test Loss: 0.6380996
Validation loss decreased (0.576503 --> 0.546670).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2772293
	speed: 1.9180s/iter; left time: 13099.8802s
Epoch: 10 cost time: 119.9286196231842
Epoch: 10, Steps: 169 | Train Loss: 0.2763509 Vali Loss: 0.5197707 Test Loss: 0.6056638
Validation loss decreased (0.546670 --> 0.519771).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2532168
	speed: 1.9844s/iter; left time: 13218.0581s
Epoch: 11 cost time: 121.67674994468689
Epoch: 11, Steps: 169 | Train Loss: 0.2550521 Vali Loss: 0.4983019 Test Loss: 0.5808172
Validation loss decreased (0.519771 --> 0.498302).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2341323
	speed: 1.8178s/iter; left time: 11801.2884s
Epoch: 12 cost time: 114.13966107368469
Epoch: 12, Steps: 169 | Train Loss: 0.2371876 Vali Loss: 0.4802375 Test Loss: 0.5594462
Validation loss decreased (0.498302 --> 0.480238).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2202641
	speed: 1.8367s/iter; left time: 11613.6414s
Epoch: 13 cost time: 104.22440910339355
Epoch: 13, Steps: 169 | Train Loss: 0.2221935 Vali Loss: 0.4645886 Test Loss: 0.5406622
Validation loss decreased (0.480238 --> 0.464589).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2084106
	speed: 1.8007s/iter; left time: 11081.3783s
Epoch: 14 cost time: 115.15440607070923
Epoch: 14, Steps: 169 | Train Loss: 0.2095525 Vali Loss: 0.4521201 Test Loss: 0.5266110
Validation loss decreased (0.464589 --> 0.452120).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1989030
	speed: 1.8190s/iter; left time: 10886.8997s
Epoch: 15 cost time: 118.46796989440918
Epoch: 15, Steps: 169 | Train Loss: 0.1989236 Vali Loss: 0.4409422 Test Loss: 0.5135353
Validation loss decreased (0.452120 --> 0.440942).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1842385
	speed: 1.9184s/iter; left time: 11157.6515s
Epoch: 16 cost time: 113.52889776229858
Epoch: 16, Steps: 169 | Train Loss: 0.1899724 Vali Loss: 0.4318774 Test Loss: 0.5032240
Validation loss decreased (0.440942 --> 0.431877).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1896019
	speed: 1.8349s/iter; left time: 10361.9235s
Epoch: 17 cost time: 114.12275266647339
Epoch: 17, Steps: 169 | Train Loss: 0.1824169 Vali Loss: 0.4238732 Test Loss: 0.4939474
Validation loss decreased (0.431877 --> 0.423873).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1687956
	speed: 1.9463s/iter; left time: 10661.7006s
Epoch: 18 cost time: 120.40534472465515
Epoch: 18, Steps: 169 | Train Loss: 0.1760553 Vali Loss: 0.4177790 Test Loss: 0.4867574
Validation loss decreased (0.423873 --> 0.417779).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1731557
	speed: 1.8845s/iter; left time: 10004.6090s
Epoch: 19 cost time: 109.58445239067078
Epoch: 19, Steps: 169 | Train Loss: 0.1706951 Vali Loss: 0.4118090 Test Loss: 0.4801654
Validation loss decreased (0.417779 --> 0.411809).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1630287
	speed: 1.7948s/iter; left time: 9225.4577s
Epoch: 20 cost time: 111.8769280910492
Epoch: 20, Steps: 169 | Train Loss: 0.1661981 Vali Loss: 0.4074622 Test Loss: 0.4747798
Validation loss decreased (0.411809 --> 0.407462).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1644294
	speed: 1.7477s/iter; left time: 8688.0072s
Epoch: 21 cost time: 116.34902429580688
Epoch: 21, Steps: 169 | Train Loss: 0.1624252 Vali Loss: 0.4040128 Test Loss: 0.4707116
Validation loss decreased (0.407462 --> 0.404013).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1569208
	speed: 1.9605s/iter; left time: 9414.2632s
Epoch: 22 cost time: 115.95884466171265
Epoch: 22, Steps: 169 | Train Loss: 0.1592049 Vali Loss: 0.4003639 Test Loss: 0.4664942
Validation loss decreased (0.404013 --> 0.400364).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1574089
	speed: 1.7901s/iter; left time: 8293.7201s
Epoch: 23 cost time: 107.02806305885315
Epoch: 23, Steps: 169 | Train Loss: 0.1565278 Vali Loss: 0.3980109 Test Loss: 0.4632753
Validation loss decreased (0.400364 --> 0.398011).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1563780
	speed: 1.9120s/iter; left time: 8535.2798s
Epoch: 24 cost time: 119.8773889541626
Epoch: 24, Steps: 169 | Train Loss: 0.1543028 Vali Loss: 0.3953012 Test Loss: 0.4609339
Validation loss decreased (0.398011 --> 0.395301).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1480303
	speed: 1.8016s/iter; left time: 7737.7526s
Epoch: 25 cost time: 110.56574058532715
Epoch: 25, Steps: 169 | Train Loss: 0.1523813 Vali Loss: 0.3938471 Test Loss: 0.4592545
Validation loss decreased (0.395301 --> 0.393847).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1490737
	speed: 1.8397s/iter; left time: 7590.6274s
Epoch: 26 cost time: 115.48381161689758
Epoch: 26, Steps: 169 | Train Loss: 0.1508194 Vali Loss: 0.3932528 Test Loss: 0.4572616
Validation loss decreased (0.393847 --> 0.393253).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1503692
	speed: 1.8241s/iter; left time: 7218.1295s
Epoch: 27 cost time: 115.35985064506531
Epoch: 27, Steps: 169 | Train Loss: 0.1495258 Vali Loss: 0.3912081 Test Loss: 0.4560557
Validation loss decreased (0.393253 --> 0.391208).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1460820
	speed: 1.9815s/iter; left time: 7505.7742s
Epoch: 28 cost time: 120.55936002731323
Epoch: 28, Steps: 169 | Train Loss: 0.1484376 Vali Loss: 0.3902207 Test Loss: 0.4549292
Validation loss decreased (0.391208 --> 0.390221).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1462212
	speed: 1.7843s/iter; left time: 6457.5179s
Epoch: 29 cost time: 109.76419878005981
Epoch: 29, Steps: 169 | Train Loss: 0.1475166 Vali Loss: 0.3894121 Test Loss: 0.4536289
Validation loss decreased (0.390221 --> 0.389412).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1415934
	speed: 1.8610s/iter; left time: 6420.3260s
Epoch: 30 cost time: 110.68277525901794
Epoch: 30, Steps: 169 | Train Loss: 0.1468000 Vali Loss: 0.3889197 Test Loss: 0.4530167
Validation loss decreased (0.389412 --> 0.388920).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1483741
	speed: 1.7718s/iter; left time: 5813.2768s
Epoch: 31 cost time: 118.05202078819275
Epoch: 31, Steps: 169 | Train Loss: 0.1461420 Vali Loss: 0.3885079 Test Loss: 0.4529047
Validation loss decreased (0.388920 --> 0.388508).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1407541
	speed: 1.9229s/iter; left time: 5984.1030s
Epoch: 32 cost time: 109.70759057998657
Epoch: 32, Steps: 169 | Train Loss: 0.1456800 Vali Loss: 0.3880944 Test Loss: 0.4519731
Validation loss decreased (0.388508 --> 0.388094).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1420307
	speed: 1.9361s/iter; left time: 5697.9986s
Epoch: 33 cost time: 120.93396496772766
Epoch: 33, Steps: 169 | Train Loss: 0.1452468 Vali Loss: 0.3878515 Test Loss: 0.4517572
Validation loss decreased (0.388094 --> 0.387851).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1469574
	speed: 1.8878s/iter; left time: 5236.7842s
Epoch: 34 cost time: 114.21581673622131
Epoch: 34, Steps: 169 | Train Loss: 0.1449161 Vali Loss: 0.3873761 Test Loss: 0.4514882
Validation loss decreased (0.387851 --> 0.387376).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1516126
	speed: 1.8931s/iter; left time: 4931.5251s
Epoch: 35 cost time: 121.39842534065247
Epoch: 35, Steps: 169 | Train Loss: 0.1446383 Vali Loss: 0.3873921 Test Loss: 0.4510332
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1401322
	speed: 1.7969s/iter; left time: 4377.1883s
Epoch: 36 cost time: 114.53785514831543
Epoch: 36, Steps: 169 | Train Loss: 0.1444005 Vali Loss: 0.3868095 Test Loss: 0.4511635
Validation loss decreased (0.387376 --> 0.386809).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1442169
	speed: 1.7480s/iter; left time: 3962.7718s
Epoch: 37 cost time: 101.68024754524231
Epoch: 37, Steps: 169 | Train Loss: 0.1442169 Vali Loss: 0.3866443 Test Loss: 0.4510757
Validation loss decreased (0.386809 --> 0.386644).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1450295
	speed: 1.5938s/iter; left time: 3343.8144s
Epoch: 38 cost time: 102.43050837516785
Epoch: 38, Steps: 169 | Train Loss: 0.1441054 Vali Loss: 0.3869968 Test Loss: 0.4508808
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1470746
	speed: 1.7219s/iter; left time: 3321.6389s
Epoch: 39 cost time: 108.7895724773407
Epoch: 39, Steps: 169 | Train Loss: 0.1439782 Vali Loss: 0.3864782 Test Loss: 0.4510653
Validation loss decreased (0.386644 --> 0.386478).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1450917
	speed: 1.8492s/iter; left time: 3254.6741s
Epoch: 40 cost time: 113.0315580368042
Epoch: 40, Steps: 169 | Train Loss: 0.1438935 Vali Loss: 0.3870711 Test Loss: 0.4510814
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1444194
	speed: 1.7507s/iter; left time: 2785.4314s
Epoch: 41 cost time: 104.43623065948486
Epoch: 41, Steps: 169 | Train Loss: 0.1438130 Vali Loss: 0.3865064 Test Loss: 0.4507159
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1424716
	speed: 1.7606s/iter; left time: 2503.6152s
Epoch: 42 cost time: 116.67305493354797
Epoch: 42, Steps: 169 | Train Loss: 0.1437457 Vali Loss: 0.3869619 Test Loss: 0.4509405
EarlyStopping counter: 3 out of 3
Early stopping
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7344405504.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2646660
	speed: 0.7044s/iter; left time: 5882.6700s
Epoch: 1 cost time: 121.2831482887268
Epoch: 1, Steps: 169 | Train Loss: 0.2715667 Vali Loss: 0.3871095 Test Loss: 0.4526778
Validation loss decreased (inf --> 0.387109).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2554266
	speed: 1.9626s/iter; left time: 16058.2244s
Epoch: 2 cost time: 105.93474674224854
Epoch: 2, Steps: 169 | Train Loss: 0.2713051 Vali Loss: 0.3867037 Test Loss: 0.4519652
Validation loss decreased (0.387109 --> 0.386704).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2774507
	speed: 1.5676s/iter; left time: 12561.0044s
Epoch: 3 cost time: 102.31094193458557
Epoch: 3, Steps: 169 | Train Loss: 0.2711773 Vali Loss: 0.3865345 Test Loss: 0.4500656
Validation loss decreased (0.386704 --> 0.386534).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2733205
	speed: 1.7656s/iter; left time: 13849.6795s
Epoch: 4 cost time: 108.71549701690674
Epoch: 4, Steps: 169 | Train Loss: 0.2711265 Vali Loss: 0.3863774 Test Loss: 0.4506292
Validation loss decreased (0.386534 --> 0.386377).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2707691
	speed: 1.7364s/iter; left time: 13326.7900s
Epoch: 5 cost time: 106.01598358154297
Epoch: 5, Steps: 169 | Train Loss: 0.2710780 Vali Loss: 0.3864206 Test Loss: 0.4512399
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2766251
	speed: 1.7610s/iter; left time: 13218.3933s
Epoch: 6 cost time: 106.91840291023254
Epoch: 6, Steps: 169 | Train Loss: 0.2709802 Vali Loss: 0.3862803 Test Loss: 0.4500445
Validation loss decreased (0.386377 --> 0.386280).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2847601
	speed: 1.8595s/iter; left time: 13642.8925s
Epoch: 7 cost time: 111.8312554359436
Epoch: 7, Steps: 169 | Train Loss: 0.2709565 Vali Loss: 0.3865169 Test Loss: 0.4506366
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2723513
	speed: 1.8192s/iter; left time: 13039.7234s
Epoch: 8 cost time: 115.6180031299591
Epoch: 8, Steps: 169 | Train Loss: 0.2708736 Vali Loss: 0.3864204 Test Loss: 0.4505206
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2818278
	speed: 1.9067s/iter; left time: 13345.0243s
Epoch: 9 cost time: 111.52731966972351
Epoch: 9, Steps: 169 | Train Loss: 0.2709303 Vali Loss: 0.3856629 Test Loss: 0.4507576
Validation loss decreased (0.386280 --> 0.385663).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2586237
	speed: 1.8171s/iter; left time: 12410.7866s
Epoch: 10 cost time: 112.18198251724243
Epoch: 10, Steps: 169 | Train Loss: 0.2708401 Vali Loss: 0.3865769 Test Loss: 0.4507566
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2723542
	speed: 1.7100s/iter; left time: 11390.3230s
Epoch: 11 cost time: 94.09775018692017
Epoch: 11, Steps: 169 | Train Loss: 0.2707636 Vali Loss: 0.3862879 Test Loss: 0.4507917
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2839456
	speed: 1.7475s/iter; left time: 11344.6248s
Epoch: 12 cost time: 117.36408185958862
Epoch: 12, Steps: 169 | Train Loss: 0.2707785 Vali Loss: 0.3857691 Test Loss: 0.4501083
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.45025548338890076, mae:0.30128082633018494, rse:0.5486941337585449, corr:[0.25609547 0.26838243 0.26906276 0.2686562  0.26806805 0.26803997
 0.26807734 0.26784325 0.26766786 0.26791823 0.26807186 0.2677399
 0.2678689  0.26829335 0.26816985 0.2680717  0.268452   0.26876196
 0.26878154 0.26852152 0.26794523 0.2674719  0.26738825 0.26759678
 0.2686579  0.26890266 0.26882765 0.26865813 0.26866767 0.26870787
 0.2683625  0.26821473 0.26877823 0.26936013 0.26927108 0.26875252
 0.26845393 0.26870432 0.26901418 0.26898    0.26902172 0.26895759
 0.26828185 0.26773342 0.2678728  0.26784047 0.26732394 0.2674572
 0.2683304  0.26856652 0.2684664  0.26834118 0.26837632 0.2685137
 0.26848128 0.26847285 0.26878622 0.2691151  0.26921067 0.2692336
 0.26902503 0.26831782 0.2678164  0.267883   0.26792383 0.26770627
 0.2675716  0.26749963 0.26733533 0.2674372  0.2676953  0.26772133
 0.26771727 0.26820177 0.26861763 0.2684072  0.26798943 0.26757866
 0.26708445 0.26679763 0.26696047 0.26743174 0.2678708  0.26807082
 0.26800954 0.267888   0.2677615  0.2675722  0.26754874 0.26749024
 0.2671501  0.2667632  0.26720667 0.26797053 0.26783162 0.2671522
 0.2668059  0.26725107 0.2678065  0.2679401  0.2677202  0.2672656
 0.26688975 0.2669317  0.2673124  0.26772627 0.26775563 0.2673498
 0.26680118 0.26668102 0.2672385  0.26774728 0.26749086 0.2668928
 0.26686415 0.26702082 0.2672455  0.26724884 0.26690066 0.26677322
 0.2671743  0.26775438 0.2681602  0.26856002 0.26853013 0.2679267
 0.2676307  0.26774356 0.26775298 0.26781482 0.26805374 0.26807112
 0.26769733 0.2675092  0.2678342  0.26820925 0.26813668 0.26739597
 0.26712805 0.2670834  0.2672777  0.2675808  0.2676786  0.26766554
 0.26791704 0.26812243 0.2682159  0.26814234 0.26783514 0.26761696
 0.26800427 0.26871037 0.26883438 0.26851484 0.2684112  0.26842454
 0.2683119  0.2685612  0.26891056 0.26848873 0.26784796 0.26799324
 0.2685455  0.2686933  0.26880872 0.26926088 0.2695392  0.26956618
 0.27023694 0.27037114 0.27031687 0.26969254 0.26889712 0.26896736
 0.26963815 0.26982662 0.26948422 0.2692917  0.26934943 0.26945794
 0.269554   0.26948258 0.26944754 0.26962355 0.26967987 0.26959214
 0.26960847 0.26939246 0.26869607 0.2682972  0.26853085 0.2690533
 0.2700979  0.27057934 0.27060848 0.27017683 0.26971045 0.26975945
 0.2700822  0.2701104  0.2698302  0.26967853 0.2697032  0.2695568
 0.26915392 0.26891848 0.2692433  0.26963118 0.26950663 0.26912442
 0.2688304  0.26866758 0.26869658 0.2687429  0.26851013 0.2684574
 0.26892063 0.26925573 0.26951265 0.26970664 0.2696728  0.26967013
 0.26983088 0.2698335  0.2697125  0.2698223  0.2700936  0.2702136
 0.27002335 0.26945516 0.2690368  0.26932877 0.26986122 0.26973626
 0.2691012  0.26880753 0.26895413 0.26892415 0.2688185  0.268838
 0.2687229  0.26870367 0.269182   0.26960105 0.26952618 0.26922032
 0.2689687  0.26878077 0.26862302 0.26856524 0.2686171  0.26893556
 0.26944613 0.26958257 0.26922038 0.26909095 0.2694132  0.26925287
 0.2685444  0.26823303 0.26851556 0.26872954 0.26873413 0.26860496
 0.26831934 0.26860616 0.2692394  0.2691793  0.2685938  0.2687091
 0.2692507  0.26917294 0.2688679  0.2692255  0.26954094 0.26933163
 0.26909983 0.26897618 0.26874924 0.2686346  0.26880243 0.2689488
 0.2691046  0.26888368 0.26819375 0.26758143 0.26779538 0.26854146
 0.26897278 0.26939762 0.2698831  0.26986727 0.26939955 0.269157
 0.2692333  0.26936108 0.2697068  0.2701049  0.27006495 0.26968703
 0.2692911  0.2690263  0.26904118 0.26941535 0.2699314  0.27008626
 0.2700031  0.26965937 0.2696939  0.27002528 0.26968712 0.26879805
 0.268756   0.26949564 0.27012554 0.27033207 0.2702585  0.2700475
 0.26989743 0.26976493 0.26980278 0.2703531  0.27070814 0.270183
 0.26966277 0.26977226 0.26958573 0.2688761  0.26862878 0.26882353
 0.2690663  0.26957962 0.27054632 0.2710052  0.2704232  0.27012482
 0.27093527 0.27074113 0.27017784 0.26982588 0.26960537 0.2696217
 0.2698138  0.26984942 0.26987737 0.2700054  0.2698892  0.2697737
 0.27023098 0.2709314  0.27114806 0.27080405 0.27045375 0.27042794
 0.2704832  0.2703261  0.26997334 0.26947832 0.26881352 0.26879767
 0.27012825 0.27092138 0.27075133 0.2704542  0.27077967 0.27121153
 0.27091566 0.27026653 0.27006784 0.27032298 0.27043906 0.27013543
 0.26981857 0.27003247 0.27051395 0.27070874 0.27069342 0.27055624
 0.27024692 0.27016532 0.2703743  0.27029726 0.27000996 0.27037323
 0.2710037  0.27080172 0.27053317 0.27057666 0.27022874 0.2696371
 0.26966667 0.26986244 0.26966855 0.2699186  0.27071387 0.2707221
 0.26998326 0.2698136  0.2702927  0.27047917 0.27021775 0.26975814
 0.26909733 0.2686408  0.2688573  0.26950046 0.27000672 0.27017206
 0.2698993  0.26968703 0.2698965  0.26970693 0.2690362  0.26899296
 0.26955414 0.26967004 0.2694305  0.26946375 0.26946855 0.26935446
 0.26981032 0.27057552 0.27055943 0.26992333 0.26963022 0.2693157
 0.26858452 0.26822153 0.26876184 0.2696097  0.27007326 0.26995108
 0.2692734  0.2692484  0.26981235 0.26964855 0.26890895 0.2689493
 0.26960877 0.26979038 0.26948506 0.2692181  0.26896316 0.2687316
 0.26884916 0.26940918 0.27008867 0.27035555 0.26994887 0.2692508
 0.26885945 0.2684403  0.2681291  0.26838094 0.26903924 0.2693886
 0.26915205 0.26904187 0.2693118  0.26961622 0.26946864 0.26906475
 0.26902515 0.2695456  0.27011052 0.27009684 0.26976937 0.26958573
 0.26944602 0.26950935 0.2701369  0.27076513 0.27083996 0.2708562
 0.27088407 0.27031046 0.26984522 0.2702324  0.2706344  0.27050993
 0.27061123 0.27092335 0.27105084 0.27079064 0.2702696  0.26969466
 0.26947746 0.2697279  0.27009287 0.2703136  0.27032134 0.2698027
 0.26895577 0.26869985 0.2692068  0.26977563 0.2700636  0.27015984
 0.27008885 0.27003887 0.2702416  0.2703029  0.26984358 0.26973
 0.27082065 0.2715068  0.2718098  0.27169192 0.27107218 0.2704933
 0.27032515 0.27026054 0.27024674 0.27033684 0.2698297  0.268759
 0.26855758 0.2695694  0.27038214 0.27035087 0.27021924 0.27025187
 0.2701478  0.2699957  0.26972863 0.2691627  0.26873213 0.2689483
 0.2697671  0.27019766 0.27050498 0.27044678 0.26997918 0.2698327
 0.26997006 0.26983628 0.2695835  0.26954037 0.2694153  0.2691224
 0.2689724  0.26891214 0.26880312 0.26883233 0.26892206 0.26870325
 0.2682592  0.26795027 0.26788893 0.26808235 0.26818943 0.26815814
 0.26855832 0.26893738 0.2690326  0.26887172 0.2685625  0.26812273
 0.26816386 0.26891842 0.26936734 0.26913586 0.268928   0.26889247
 0.26874235 0.26863053 0.2688188  0.2691668  0.26921442 0.26875308
 0.2682336  0.26806065 0.2679132  0.2676496  0.26767752 0.26809698
 0.26826692 0.26809967 0.26817793 0.26844004 0.26860633 0.2687082
 0.2687104  0.2684288  0.26812968 0.26818162 0.26835334 0.26838973
 0.26843682 0.268433   0.26813996 0.2678519  0.26800093 0.26803008
 0.26722977 0.2663283  0.26628113 0.26684475 0.26733604 0.2674332
 0.2668825  0.2666382  0.26735196 0.26742527 0.2663482  0.26597014
 0.26689336 0.267463   0.267204   0.26715678 0.26733577 0.26716176
 0.26700166 0.2673089  0.26785374 0.26836503 0.26873073 0.26876795
 0.26841483 0.26789472 0.26754194 0.2673686  0.26715755 0.26681644
 0.26665252 0.26719084 0.2679932  0.26823908 0.2677185  0.26726547
 0.26758012 0.2678493  0.26740542 0.26713565 0.26749164 0.26748455
 0.26710284 0.2674852  0.26811007 0.26776296 0.2675153  0.26859996
 0.26916835 0.26809603 0.26738966 0.26775223 0.2673436  0.2663861
 0.2665814  0.26739475 0.2678084  0.26778117 0.26759392 0.26761287
 0.26805    0.26830205 0.2680272  0.26783893 0.2678879  0.26760027
 0.26743504 0.26813057 0.2688386  0.26871237 0.26841584 0.2684533
 0.26842117 0.2679443  0.26727182 0.26719257 0.26753905 0.26753873
 0.26772198 0.2681362  0.26907277 0.26940998 0.26901287 0.26884565
 0.26930866 0.269592   0.26898262 0.2681079  0.26778254 0.26783404
 0.2678811  0.26824766 0.2691388  0.26971823 0.26956993 0.2693288
 0.2692493  0.26918966 0.26915583 0.268682   0.2678379  0.26796433
 0.2691708  0.26942644 0.2695684  0.2698839  0.269527   0.26909825
 0.26944873 0.26988444 0.2695905  0.2691823  0.26929334 0.26929492
 0.2690947  0.2693395  0.26991785 0.2703086  0.27027756 0.26964903
 0.26892743 0.268207   0.26736438 0.26755476 0.26831922 0.26891673]
