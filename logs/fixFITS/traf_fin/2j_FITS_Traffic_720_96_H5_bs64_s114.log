Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j96_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j96_H5_FITS_custom_ftM_sl720_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3404417280.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 62.06658339500427
Epoch: 1, Steps: 89 | Train Loss: 0.9827894 Vali Loss: 1.0768026 Test Loss: 1.2365526
Validation loss decreased (inf --> 1.076803).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 65.77213716506958
Epoch: 2, Steps: 89 | Train Loss: 0.7510298 Vali Loss: 0.9543495 Test Loss: 1.0973817
Validation loss decreased (1.076803 --> 0.954349).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 66.5805184841156
Epoch: 3, Steps: 89 | Train Loss: 0.6513732 Vali Loss: 0.8920423 Test Loss: 1.0264353
Validation loss decreased (0.954349 --> 0.892042).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 64.52333879470825
Epoch: 4, Steps: 89 | Train Loss: 0.5826819 Vali Loss: 0.8377681 Test Loss: 0.9669018
Validation loss decreased (0.892042 --> 0.837768).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 64.5086190700531
Epoch: 5, Steps: 89 | Train Loss: 0.5271102 Vali Loss: 0.7962687 Test Loss: 0.9208556
Validation loss decreased (0.837768 --> 0.796269).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 64.00540900230408
Epoch: 6, Steps: 89 | Train Loss: 0.4801376 Vali Loss: 0.7590171 Test Loss: 0.8755954
Validation loss decreased (0.796269 --> 0.759017).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 64.13175392150879
Epoch: 7, Steps: 89 | Train Loss: 0.4400156 Vali Loss: 0.7244573 Test Loss: 0.8357049
Validation loss decreased (0.759017 --> 0.724457).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 64.06748294830322
Epoch: 8, Steps: 89 | Train Loss: 0.4051832 Vali Loss: 0.6892456 Test Loss: 0.7986988
Validation loss decreased (0.724457 --> 0.689246).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 65.09910559654236
Epoch: 9, Steps: 89 | Train Loss: 0.3748666 Vali Loss: 0.6647367 Test Loss: 0.7678392
Validation loss decreased (0.689246 --> 0.664737).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 61.96766448020935
Epoch: 10, Steps: 89 | Train Loss: 0.3482478 Vali Loss: 0.6392254 Test Loss: 0.7400265
Validation loss decreased (0.664737 --> 0.639225).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 61.416935443878174
Epoch: 11, Steps: 89 | Train Loss: 0.3247820 Vali Loss: 0.6173111 Test Loss: 0.7144973
Validation loss decreased (0.639225 --> 0.617311).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 60.98369789123535
Epoch: 12, Steps: 89 | Train Loss: 0.3040014 Vali Loss: 0.5997251 Test Loss: 0.6920299
Validation loss decreased (0.617311 --> 0.599725).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 60.357019662857056
Epoch: 13, Steps: 89 | Train Loss: 0.2854584 Vali Loss: 0.5789078 Test Loss: 0.6683940
Validation loss decreased (0.599725 --> 0.578908).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 53.70250844955444
Epoch: 14, Steps: 89 | Train Loss: 0.2689612 Vali Loss: 0.5605038 Test Loss: 0.6483347
Validation loss decreased (0.578908 --> 0.560504).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 60.024489641189575
Epoch: 15, Steps: 89 | Train Loss: 0.2542010 Vali Loss: 0.5449885 Test Loss: 0.6320476
Validation loss decreased (0.560504 --> 0.544989).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 60.076074838638306
Epoch: 16, Steps: 89 | Train Loss: 0.2408906 Vali Loss: 0.5319182 Test Loss: 0.6163056
Validation loss decreased (0.544989 --> 0.531918).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 60.93845248222351
Epoch: 17, Steps: 89 | Train Loss: 0.2289290 Vali Loss: 0.5212973 Test Loss: 0.6037821
Validation loss decreased (0.531918 --> 0.521297).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 59.87347149848938
Epoch: 18, Steps: 89 | Train Loss: 0.2181437 Vali Loss: 0.5100445 Test Loss: 0.5894870
Validation loss decreased (0.521297 --> 0.510045).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 59.529295206069946
Epoch: 19, Steps: 89 | Train Loss: 0.2083178 Vali Loss: 0.4986735 Test Loss: 0.5780418
Validation loss decreased (0.510045 --> 0.498673).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 60.75642275810242
Epoch: 20, Steps: 89 | Train Loss: 0.1994567 Vali Loss: 0.4893778 Test Loss: 0.5677687
Validation loss decreased (0.498673 --> 0.489378).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 60.59831714630127
Epoch: 21, Steps: 89 | Train Loss: 0.1913755 Vali Loss: 0.4812517 Test Loss: 0.5566235
Validation loss decreased (0.489378 --> 0.481252).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 60.17831635475159
Epoch: 22, Steps: 89 | Train Loss: 0.1840011 Vali Loss: 0.4725565 Test Loss: 0.5476198
Validation loss decreased (0.481252 --> 0.472557).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 60.92685508728027
Epoch: 23, Steps: 89 | Train Loss: 0.1772757 Vali Loss: 0.4661337 Test Loss: 0.5402203
Validation loss decreased (0.472557 --> 0.466134).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 60.34986424446106
Epoch: 24, Steps: 89 | Train Loss: 0.1711320 Vali Loss: 0.4596376 Test Loss: 0.5326537
Validation loss decreased (0.466134 --> 0.459638).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 57.64712405204773
Epoch: 25, Steps: 89 | Train Loss: 0.1654608 Vali Loss: 0.4532740 Test Loss: 0.5247300
Validation loss decreased (0.459638 --> 0.453274).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 57.5395233631134
Epoch: 26, Steps: 89 | Train Loss: 0.1602821 Vali Loss: 0.4458522 Test Loss: 0.5178300
Validation loss decreased (0.453274 --> 0.445852).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 55.75826668739319
Epoch: 27, Steps: 89 | Train Loss: 0.1554831 Vali Loss: 0.4408772 Test Loss: 0.5123813
Validation loss decreased (0.445852 --> 0.440877).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 60.35864806175232
Epoch: 28, Steps: 89 | Train Loss: 0.1511095 Vali Loss: 0.4367758 Test Loss: 0.5066655
Validation loss decreased (0.440877 --> 0.436776).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 60.712220907211304
Epoch: 29, Steps: 89 | Train Loss: 0.1470447 Vali Loss: 0.4327984 Test Loss: 0.5025416
Validation loss decreased (0.436776 --> 0.432798).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 60.20173668861389
Epoch: 30, Steps: 89 | Train Loss: 0.1432985 Vali Loss: 0.4284465 Test Loss: 0.4975420
Validation loss decreased (0.432798 --> 0.428447).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 61.24826431274414
Epoch: 31, Steps: 89 | Train Loss: 0.1398288 Vali Loss: 0.4244386 Test Loss: 0.4924997
Validation loss decreased (0.428447 --> 0.424439).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 60.28585982322693
Epoch: 32, Steps: 89 | Train Loss: 0.1365969 Vali Loss: 0.4204558 Test Loss: 0.4885859
Validation loss decreased (0.424439 --> 0.420456).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 60.87209630012512
Epoch: 33, Steps: 89 | Train Loss: 0.1336322 Vali Loss: 0.4167119 Test Loss: 0.4853045
Validation loss decreased (0.420456 --> 0.416712).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 54.721890926361084
Epoch: 34, Steps: 89 | Train Loss: 0.1308556 Vali Loss: 0.4130205 Test Loss: 0.4801715
Validation loss decreased (0.416712 --> 0.413020).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 60.761396408081055
Epoch: 35, Steps: 89 | Train Loss: 0.1282818 Vali Loss: 0.4094318 Test Loss: 0.4774738
Validation loss decreased (0.413020 --> 0.409432).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 60.05631494522095
Epoch: 36, Steps: 89 | Train Loss: 0.1258609 Vali Loss: 0.4066591 Test Loss: 0.4736246
Validation loss decreased (0.409432 --> 0.406659).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 57.83289170265198
Epoch: 37, Steps: 89 | Train Loss: 0.1236273 Vali Loss: 0.4060214 Test Loss: 0.4723314
Validation loss decreased (0.406659 --> 0.406021).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 61.70579934120178
Epoch: 38, Steps: 89 | Train Loss: 0.1215403 Vali Loss: 0.4021007 Test Loss: 0.4690772
Validation loss decreased (0.406021 --> 0.402101).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 62.77594017982483
Epoch: 39, Steps: 89 | Train Loss: 0.1195579 Vali Loss: 0.3984571 Test Loss: 0.4658648
Validation loss decreased (0.402101 --> 0.398457).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 59.92872929573059
Epoch: 40, Steps: 89 | Train Loss: 0.1177581 Vali Loss: 0.4003472 Test Loss: 0.4646561
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 58.73736095428467
Epoch: 41, Steps: 89 | Train Loss: 0.1160321 Vali Loss: 0.3972703 Test Loss: 0.4623668
Validation loss decreased (0.398457 --> 0.397270).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 58.85233950614929
Epoch: 42, Steps: 89 | Train Loss: 0.1144509 Vali Loss: 0.3939720 Test Loss: 0.4603032
Validation loss decreased (0.397270 --> 0.393972).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 60.85092806816101
Epoch: 43, Steps: 89 | Train Loss: 0.1129422 Vali Loss: 0.3944097 Test Loss: 0.4583338
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 62.59267807006836
Epoch: 44, Steps: 89 | Train Loss: 0.1115236 Vali Loss: 0.3916353 Test Loss: 0.4564790
Validation loss decreased (0.393972 --> 0.391635).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 60.83679962158203
Epoch: 45, Steps: 89 | Train Loss: 0.1102066 Vali Loss: 0.3887527 Test Loss: 0.4547263
Validation loss decreased (0.391635 --> 0.388753).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 62.57884931564331
Epoch: 46, Steps: 89 | Train Loss: 0.1089576 Vali Loss: 0.3877883 Test Loss: 0.4532993
Validation loss decreased (0.388753 --> 0.387788).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 62.355042457580566
Epoch: 47, Steps: 89 | Train Loss: 0.1077859 Vali Loss: 0.3872814 Test Loss: 0.4516802
Validation loss decreased (0.387788 --> 0.387281).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 63.06000781059265
Epoch: 48, Steps: 89 | Train Loss: 0.1066790 Vali Loss: 0.3858769 Test Loss: 0.4501835
Validation loss decreased (0.387281 --> 0.385877).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 63.252355098724365
Epoch: 49, Steps: 89 | Train Loss: 0.1056524 Vali Loss: 0.3841198 Test Loss: 0.4486204
Validation loss decreased (0.385877 --> 0.384120).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 62.95173192024231
Epoch: 50, Steps: 89 | Train Loss: 0.1046627 Vali Loss: 0.3838011 Test Loss: 0.4473043
Validation loss decreased (0.384120 --> 0.383801).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3404417280.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 66.22358536720276
Epoch: 1, Steps: 89 | Train Loss: 0.2450962 Vali Loss: 0.3345196 Test Loss: 0.4019564
Validation loss decreased (inf --> 0.334520).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 66.57053995132446
Epoch: 2, Steps: 89 | Train Loss: 0.2390429 Vali Loss: 0.3353862 Test Loss: 0.4006990
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
Epoch: 3 cost time: 64.26223373413086
Epoch: 3, Steps: 89 | Train Loss: 0.2387553 Vali Loss: 0.3341853 Test Loss: 0.4010063
Validation loss decreased (0.334520 --> 0.334185).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 63.86047863960266
Epoch: 4, Steps: 89 | Train Loss: 0.2384265 Vali Loss: 0.3332810 Test Loss: 0.4006778
Validation loss decreased (0.334185 --> 0.333281).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 66.18154644966125
Epoch: 5, Steps: 89 | Train Loss: 0.2383818 Vali Loss: 0.3354655 Test Loss: 0.4008825
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 60.8540415763855
Epoch: 6, Steps: 89 | Train Loss: 0.2382687 Vali Loss: 0.3333251 Test Loss: 0.4008019
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 65.55517244338989
Epoch: 7, Steps: 89 | Train Loss: 0.2381949 Vali Loss: 0.3327849 Test Loss: 0.4007683
Validation loss decreased (0.333281 --> 0.332785).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 70.78735280036926
Epoch: 8, Steps: 89 | Train Loss: 0.2381597 Vali Loss: 0.3340712 Test Loss: 0.4001854
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 65.719722032547
Epoch: 9, Steps: 89 | Train Loss: 0.2380443 Vali Loss: 0.3342112 Test Loss: 0.4000174
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 64.85163950920105
Epoch: 10, Steps: 89 | Train Loss: 0.2380609 Vali Loss: 0.3343118 Test Loss: 0.4008677
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j96_H5_FITS_custom_ftM_sl720_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.39802470803260803, mae:0.28529080748558044, rse:0.5224063992500305, corr:[0.28977323 0.29626086 0.2963613  0.29654345 0.29658502 0.29623434
 0.2959327  0.29597855 0.29624298 0.2964504  0.2965233  0.29654005
 0.2964976  0.29622757 0.2957723  0.2953964  0.2954152  0.29592907
 0.29661697 0.29697552 0.2966867  0.29589054 0.29518887 0.295397
 0.296349   0.2965854  0.296538   0.29658622 0.29666194 0.29666823
 0.29656008 0.29632425 0.29605094 0.29590765 0.2959903  0.29618227
 0.29625326 0.29597813 0.29550695 0.2952609  0.2955269  0.29611164
 0.29651922 0.29642966 0.29590696 0.29528475 0.2949337  0.29519013
 0.29578617 0.2959482  0.29568636 0.29523736 0.29497325 0.29519132
 0.2958256  0.2964696  0.29672736 0.29649916 0.295979   0.29543263
 0.29504624 0.2947519  0.29455176 0.29457104 0.29493746 0.29554313
 0.2959642  0.2958265  0.29516083 0.29439467 0.29398003 0.29418862
 0.29473257 0.29532236 0.2957288  0.2956597  0.2951259  0.29448837
 0.29419988 0.29447743 0.29515103 0.29579467 0.29607615 0.29599783
 0.29582798 0.29574245 0.29570153 0.29546294 0.2949044  0.29430315
 0.29411098 0.29428858 0.29407924 0.2929566  0.29179573 0.29256088]
