Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j192_H5_FITS_custom_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3804936960.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 67.53966426849365
Epoch: 1, Steps: 88 | Train Loss: 0.7861964 Vali Loss: 0.6314918 Test Loss: 0.7253131
Validation loss decreased (inf --> 0.631492).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 67.85971975326538
Epoch: 2, Steps: 88 | Train Loss: 0.3910853 Vali Loss: 0.4233439 Test Loss: 0.4958225
Validation loss decreased (0.631492 --> 0.423344).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 69.33268404006958
Epoch: 3, Steps: 88 | Train Loss: 0.2856180 Vali Loss: 0.3626218 Test Loss: 0.4316581
Validation loss decreased (0.423344 --> 0.362622).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 72.46913170814514
Epoch: 4, Steps: 88 | Train Loss: 0.2561083 Vali Loss: 0.3461613 Test Loss: 0.4158424
Validation loss decreased (0.362622 --> 0.346161).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 61.04060339927673
Epoch: 5, Steps: 88 | Train Loss: 0.2485532 Vali Loss: 0.3411809 Test Loss: 0.4122653
Validation loss decreased (0.346161 --> 0.341181).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 70.11851525306702
Epoch: 6, Steps: 88 | Train Loss: 0.2462664 Vali Loss: 0.3393867 Test Loss: 0.4111826
Validation loss decreased (0.341181 --> 0.339387).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 67.44723129272461
Epoch: 7, Steps: 88 | Train Loss: 0.2455486 Vali Loss: 0.3385250 Test Loss: 0.4104581
Validation loss decreased (0.339387 --> 0.338525).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 68.48093152046204
Epoch: 8, Steps: 88 | Train Loss: 0.2451036 Vali Loss: 0.3379565 Test Loss: 0.4101149
Validation loss decreased (0.338525 --> 0.337956).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 70.76018500328064
Epoch: 9, Steps: 88 | Train Loss: 0.2448091 Vali Loss: 0.3374020 Test Loss: 0.4100839
Validation loss decreased (0.337956 --> 0.337402).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 69.152010679245
Epoch: 10, Steps: 88 | Train Loss: 0.2446928 Vali Loss: 0.3368699 Test Loss: 0.4095590
Validation loss decreased (0.337402 --> 0.336870).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 71.7765884399414
Epoch: 11, Steps: 88 | Train Loss: 0.2444905 Vali Loss: 0.3369382 Test Loss: 0.4094733
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 76.29017901420593
Epoch: 12, Steps: 88 | Train Loss: 0.2444378 Vali Loss: 0.3368626 Test Loss: 0.4094467
Validation loss decreased (0.336870 --> 0.336863).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 71.89272809028625
Epoch: 13, Steps: 88 | Train Loss: 0.2443474 Vali Loss: 0.3363848 Test Loss: 0.4091531
Validation loss decreased (0.336863 --> 0.336385).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 63.77261757850647
Epoch: 14, Steps: 88 | Train Loss: 0.2443004 Vali Loss: 0.3360286 Test Loss: 0.4093812
Validation loss decreased (0.336385 --> 0.336029).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 66.87082433700562
Epoch: 15, Steps: 88 | Train Loss: 0.2441132 Vali Loss: 0.3364666 Test Loss: 0.4092000
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 72.35537195205688
Epoch: 16, Steps: 88 | Train Loss: 0.2441232 Vali Loss: 0.3365616 Test Loss: 0.4091504
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 66.00778293609619
Epoch: 17, Steps: 88 | Train Loss: 0.2440681 Vali Loss: 0.3362763 Test Loss: 0.4090740
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j192_H5_FITS_custom_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.4084683060646057, mae:0.28834253549575806, rse:0.5274827480316162, corr:[0.28912345 0.2907444  0.2899151  0.29299408 0.29532674 0.295145
 0.29425856 0.294368   0.29511753 0.29508093 0.29388672 0.2926539
 0.29253557 0.29324055 0.29357326 0.2929216  0.29194403 0.29166457
 0.29225174 0.292953   0.29304844 0.29264632 0.2925125  0.2933692
 0.2946166  0.29449028 0.2938062  0.29364133 0.29413733 0.29467124
 0.29456547 0.2938192  0.29307482 0.2928237  0.29290453 0.29288733
 0.29269734 0.29254666 0.29267642 0.29299662 0.29320115 0.2931115
 0.29286674 0.29276198 0.2929012  0.29308823 0.29311833 0.29315394
 0.29324648 0.29310718 0.29301265 0.29295158 0.29285464 0.29283512
 0.29296616 0.2930669  0.292872   0.29234302 0.29177105 0.29154652
 0.29176417 0.29202232 0.2920216  0.2918659  0.2918974  0.2922383
 0.29260352 0.2926781  0.29246584 0.29226986 0.2922661  0.29237774
 0.2922251  0.2918301  0.2915456  0.29155287 0.2917443  0.29179856
 0.29154813 0.29120392 0.29109046 0.29120654 0.29123878 0.29103553
 0.2908597  0.29101267 0.29148707 0.29191157 0.2919944  0.29181615
 0.2916464  0.29156908 0.29144606 0.2911372  0.2907276  0.29057845
 0.2907224  0.29113212 0.29151633 0.29157034 0.2914212  0.29135215
 0.29135418 0.29118198 0.29074436 0.29028603 0.2901703  0.29045683
 0.29080918 0.29081526 0.2904661  0.2901409  0.29015943 0.2904515
 0.2906151  0.2904764  0.29029173 0.29031733 0.2906158  0.29078668
 0.29056355 0.29037815 0.29064035 0.2911702  0.29160267 0.29160893
 0.291297   0.29108623 0.2912102  0.29143357 0.29137772 0.2910084
 0.29071215 0.29085657 0.2913474  0.2917251  0.29168108 0.29135558
 0.29119805 0.29137412 0.2916459  0.29172018 0.29155636 0.29150233
 0.29176185 0.29197702 0.2919512  0.29174033 0.29169098 0.29200125
 0.29238585 0.2923594  0.29177046 0.29107064 0.29088628 0.29133856
 0.29184252 0.29178664 0.29130363 0.2911205  0.29168308 0.29251736
 0.29285172 0.29253533 0.29211777 0.2920693  0.29228652 0.2926586
 0.29297003 0.29261437 0.29257783 0.29289016 0.2928199  0.29224926
 0.2918062  0.29203054 0.29258448 0.29262462 0.29191175 0.29129544
 0.29175097 0.29295123 0.29360038 0.29296702 0.29191315 0.29191056
 0.29289266 0.2930562  0.29099095 0.2883573  0.28948003 0.29441595]
