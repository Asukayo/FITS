Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j96_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=320, out_features=362, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12781322240.0
params:  116202.0
Trainable parameters:  116202
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 61.78047704696655
Epoch: 1, Steps: 89 | Train Loss: 0.9944167 Vali Loss: 1.0758046 Test Loss: 1.2322633
Validation loss decreased (inf --> 1.075805).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 57.652122020721436
Epoch: 2, Steps: 89 | Train Loss: 0.7575378 Vali Loss: 0.9860241 Test Loss: 1.1284417
Validation loss decreased (1.075805 --> 0.986024).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 64.69339370727539
Epoch: 3, Steps: 89 | Train Loss: 0.6628629 Vali Loss: 0.9256117 Test Loss: 1.0626554
Validation loss decreased (0.986024 --> 0.925612).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 66.97589039802551
Epoch: 4, Steps: 89 | Train Loss: 0.5919768 Vali Loss: 0.8799038 Test Loss: 1.0114622
Validation loss decreased (0.925612 --> 0.879904).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 66.85362815856934
Epoch: 5, Steps: 89 | Train Loss: 0.5333248 Vali Loss: 0.8345073 Test Loss: 0.9569717
Validation loss decreased (0.879904 --> 0.834507).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 71.50250005722046
Epoch: 6, Steps: 89 | Train Loss: 0.4834410 Vali Loss: 0.7893216 Test Loss: 0.9078155
Validation loss decreased (0.834507 --> 0.789322).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 70.64668607711792
Epoch: 7, Steps: 89 | Train Loss: 0.4405540 Vali Loss: 0.7538607 Test Loss: 0.8674932
Validation loss decreased (0.789322 --> 0.753861).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 69.96419191360474
Epoch: 8, Steps: 89 | Train Loss: 0.4032486 Vali Loss: 0.7228000 Test Loss: 0.8307551
Validation loss decreased (0.753861 --> 0.722800).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 72.99357843399048
Epoch: 9, Steps: 89 | Train Loss: 0.3707359 Vali Loss: 0.6889377 Test Loss: 0.7929401
Validation loss decreased (0.722800 --> 0.688938).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 72.10193085670471
Epoch: 10, Steps: 89 | Train Loss: 0.3420739 Vali Loss: 0.6645387 Test Loss: 0.7651642
Validation loss decreased (0.688938 --> 0.664539).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 69.85995316505432
Epoch: 11, Steps: 89 | Train Loss: 0.3167928 Vali Loss: 0.6364143 Test Loss: 0.7336221
Validation loss decreased (0.664539 --> 0.636414).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 70.22430467605591
Epoch: 12, Steps: 89 | Train Loss: 0.2943910 Vali Loss: 0.6173673 Test Loss: 0.7107609
Validation loss decreased (0.636414 --> 0.617367).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 66.17121648788452
Epoch: 13, Steps: 89 | Train Loss: 0.2744265 Vali Loss: 0.5970914 Test Loss: 0.6873900
Validation loss decreased (0.617367 --> 0.597091).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 59.53934049606323
Epoch: 14, Steps: 89 | Train Loss: 0.2565401 Vali Loss: 0.5782555 Test Loss: 0.6664745
Validation loss decreased (0.597091 --> 0.578256).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 69.52938079833984
Epoch: 15, Steps: 89 | Train Loss: 0.2405075 Vali Loss: 0.5631337 Test Loss: 0.6494590
Validation loss decreased (0.578256 --> 0.563134).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 65.9461350440979
Epoch: 16, Steps: 89 | Train Loss: 0.2261201 Vali Loss: 0.5461304 Test Loss: 0.6301259
Validation loss decreased (0.563134 --> 0.546130).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 60.15054726600647
Epoch: 17, Steps: 89 | Train Loss: 0.2131035 Vali Loss: 0.5341896 Test Loss: 0.6154418
Validation loss decreased (0.546130 --> 0.534190).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 59.53420567512512
Epoch: 18, Steps: 89 | Train Loss: 0.2013717 Vali Loss: 0.5211236 Test Loss: 0.6006696
Validation loss decreased (0.534190 --> 0.521124).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 63.883875370025635
Epoch: 19, Steps: 89 | Train Loss: 0.1907073 Vali Loss: 0.5094666 Test Loss: 0.5878996
Validation loss decreased (0.521124 --> 0.509467).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 63.763859272003174
Epoch: 20, Steps: 89 | Train Loss: 0.1810424 Vali Loss: 0.4997289 Test Loss: 0.5762048
Validation loss decreased (0.509467 --> 0.499729).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 67.4030122756958
Epoch: 21, Steps: 89 | Train Loss: 0.1722096 Vali Loss: 0.4907289 Test Loss: 0.5672063
Validation loss decreased (0.499729 --> 0.490729).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 64.53470802307129
Epoch: 22, Steps: 89 | Train Loss: 0.1641480 Vali Loss: 0.4824068 Test Loss: 0.5569999
Validation loss decreased (0.490729 --> 0.482407).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 68.04524183273315
Epoch: 23, Steps: 89 | Train Loss: 0.1567863 Vali Loss: 0.4737318 Test Loss: 0.5471068
Validation loss decreased (0.482407 --> 0.473732).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 64.42570233345032
Epoch: 24, Steps: 89 | Train Loss: 0.1500435 Vali Loss: 0.4660740 Test Loss: 0.5377603
Validation loss decreased (0.473732 --> 0.466074).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 63.8848819732666
Epoch: 25, Steps: 89 | Train Loss: 0.1438481 Vali Loss: 0.4584180 Test Loss: 0.5298563
Validation loss decreased (0.466074 --> 0.458418).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 74.47220396995544
Epoch: 26, Steps: 89 | Train Loss: 0.1381490 Vali Loss: 0.4528709 Test Loss: 0.5238602
Validation loss decreased (0.458418 --> 0.452871).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 60.92842745780945
Epoch: 27, Steps: 89 | Train Loss: 0.1329208 Vali Loss: 0.4456275 Test Loss: 0.5165959
Validation loss decreased (0.452871 --> 0.445627).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 60.840041637420654
Epoch: 28, Steps: 89 | Train Loss: 0.1280541 Vali Loss: 0.4408987 Test Loss: 0.5110577
Validation loss decreased (0.445627 --> 0.440899).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 65.93116068840027
Epoch: 29, Steps: 89 | Train Loss: 0.1235928 Vali Loss: 0.4375125 Test Loss: 0.5059919
Validation loss decreased (0.440899 --> 0.437513).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 64.16858172416687
Epoch: 30, Steps: 89 | Train Loss: 0.1194703 Vali Loss: 0.4321323 Test Loss: 0.5001524
Validation loss decreased (0.437513 --> 0.432132).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 70.112877368927
Epoch: 31, Steps: 89 | Train Loss: 0.1156114 Vali Loss: 0.4272864 Test Loss: 0.4948302
Validation loss decreased (0.432132 --> 0.427286).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 72.12701845169067
Epoch: 32, Steps: 89 | Train Loss: 0.1120604 Vali Loss: 0.4241733 Test Loss: 0.4910376
Validation loss decreased (0.427286 --> 0.424173).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 68.26319742202759
Epoch: 33, Steps: 89 | Train Loss: 0.1087648 Vali Loss: 0.4206103 Test Loss: 0.4861248
Validation loss decreased (0.424173 --> 0.420610).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 70.36428928375244
Epoch: 34, Steps: 89 | Train Loss: 0.1056868 Vali Loss: 0.4148706 Test Loss: 0.4817568
Validation loss decreased (0.420610 --> 0.414871).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 72.9819233417511
Epoch: 35, Steps: 89 | Train Loss: 0.1028108 Vali Loss: 0.4118141 Test Loss: 0.4781189
Validation loss decreased (0.414871 --> 0.411814).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 69.87484669685364
Epoch: 36, Steps: 89 | Train Loss: 0.1001508 Vali Loss: 0.4091116 Test Loss: 0.4748445
Validation loss decreased (0.411814 --> 0.409112).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 67.12340307235718
Epoch: 37, Steps: 89 | Train Loss: 0.0976685 Vali Loss: 0.4059887 Test Loss: 0.4717169
Validation loss decreased (0.409112 --> 0.405989).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 64.96596121788025
Epoch: 38, Steps: 89 | Train Loss: 0.0953369 Vali Loss: 0.4032878 Test Loss: 0.4682798
Validation loss decreased (0.405989 --> 0.403288).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 67.36755609512329
Epoch: 39, Steps: 89 | Train Loss: 0.0931494 Vali Loss: 0.4019323 Test Loss: 0.4656755
Validation loss decreased (0.403288 --> 0.401932).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 65.4612832069397
Epoch: 40, Steps: 89 | Train Loss: 0.0911299 Vali Loss: 0.3993276 Test Loss: 0.4637160
Validation loss decreased (0.401932 --> 0.399328).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 63.83884406089783
Epoch: 41, Steps: 89 | Train Loss: 0.0892111 Vali Loss: 0.3971983 Test Loss: 0.4608260
Validation loss decreased (0.399328 --> 0.397198).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 66.10099029541016
Epoch: 42, Steps: 89 | Train Loss: 0.0874331 Vali Loss: 0.3953432 Test Loss: 0.4585897
Validation loss decreased (0.397198 --> 0.395343).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 68.63394618034363
Epoch: 43, Steps: 89 | Train Loss: 0.0857489 Vali Loss: 0.3932126 Test Loss: 0.4567128
Validation loss decreased (0.395343 --> 0.393213).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 65.02790522575378
Epoch: 44, Steps: 89 | Train Loss: 0.0841637 Vali Loss: 0.3913969 Test Loss: 0.4542825
Validation loss decreased (0.393213 --> 0.391397).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 58.97231483459473
Epoch: 45, Steps: 89 | Train Loss: 0.0826776 Vali Loss: 0.3903128 Test Loss: 0.4522238
Validation loss decreased (0.391397 --> 0.390313).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 54.49200677871704
Epoch: 46, Steps: 89 | Train Loss: 0.0812819 Vali Loss: 0.3877372 Test Loss: 0.4508788
Validation loss decreased (0.390313 --> 0.387737).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 63.19002652168274
Epoch: 47, Steps: 89 | Train Loss: 0.0799755 Vali Loss: 0.3862208 Test Loss: 0.4490976
Validation loss decreased (0.387737 --> 0.386221).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 65.43612742424011
Epoch: 48, Steps: 89 | Train Loss: 0.0787426 Vali Loss: 0.3849409 Test Loss: 0.4477426
Validation loss decreased (0.386221 --> 0.384941).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 65.26515817642212
Epoch: 49, Steps: 89 | Train Loss: 0.0775906 Vali Loss: 0.3829535 Test Loss: 0.4462831
Validation loss decreased (0.384941 --> 0.382954).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 62.47314476966858
Epoch: 50, Steps: 89 | Train Loss: 0.0764972 Vali Loss: 0.3816696 Test Loss: 0.4444829
Validation loss decreased (0.382954 --> 0.381670).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=320, out_features=362, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12781322240.0
params:  116202.0
Trainable parameters:  116202
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 61.63689923286438
Epoch: 1, Steps: 89 | Train Loss: 0.2390000 Vali Loss: 0.3269678 Test Loss: 0.3907243
Validation loss decreased (inf --> 0.326968).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 60.018210649490356
Epoch: 2, Steps: 89 | Train Loss: 0.2320581 Vali Loss: 0.3261440 Test Loss: 0.3904837
Validation loss decreased (0.326968 --> 0.326144).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 62.33289885520935
Epoch: 3, Steps: 89 | Train Loss: 0.2316141 Vali Loss: 0.3251761 Test Loss: 0.3898333
Validation loss decreased (0.326144 --> 0.325176).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 65.51452445983887
Epoch: 4, Steps: 89 | Train Loss: 0.2315170 Vali Loss: 0.3253069 Test Loss: 0.3904969
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 65.57929801940918
Epoch: 5, Steps: 89 | Train Loss: 0.2312396 Vali Loss: 0.3246429 Test Loss: 0.3897375
Validation loss decreased (0.325176 --> 0.324643).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 63.18899488449097
Epoch: 6, Steps: 89 | Train Loss: 0.2311818 Vali Loss: 0.3251652 Test Loss: 0.3896893
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 61.696189403533936
Epoch: 7, Steps: 89 | Train Loss: 0.2311319 Vali Loss: 0.3250997 Test Loss: 0.3892187
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 60.75164198875427
Epoch: 8, Steps: 89 | Train Loss: 0.2311191 Vali Loss: 0.3258883 Test Loss: 0.3899507
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.38683757185935974, mae:0.27103158831596375, rse:0.5150125026702881, corr:[0.27761242 0.29315364 0.2932329  0.29330122 0.29294962 0.2928985
 0.29293624 0.2926963  0.29249915 0.29176056 0.29214606 0.2923264
 0.29201052 0.2921105  0.29223213 0.29270557 0.29211697 0.29164192
 0.2923214  0.29281148 0.29289082 0.29260385 0.29213163 0.29120946
 0.29171598 0.29177067 0.29150677 0.29180408 0.29183573 0.29213625
 0.29278347 0.29224342 0.29176363 0.291772   0.29187828 0.29173973
 0.29196522 0.29280657 0.29244354 0.2924134  0.29289842 0.2928977
 0.2936492  0.2938421  0.29358563 0.29354602 0.29266354 0.29244226
 0.29305154 0.29318398 0.29271334 0.29239398 0.29372498 0.29424325
 0.2939851  0.29354843 0.29211    0.29158813 0.2913197  0.29085538
 0.2914833  0.29178134 0.2920516  0.2928079  0.29316396 0.29342267
 0.2928933  0.29236954 0.2924601  0.29191867 0.29082283 0.29023424
 0.29065582 0.29093346 0.29140428 0.29165184 0.2912968  0.2914556
 0.29209265 0.29253343 0.29199475 0.29179233 0.29185376 0.29154986
 0.2921024  0.29143476 0.2911421  0.29263595 0.29222584 0.29230827
 0.2922058  0.29158217 0.2907289  0.28937078 0.29058522 0.29140738]
