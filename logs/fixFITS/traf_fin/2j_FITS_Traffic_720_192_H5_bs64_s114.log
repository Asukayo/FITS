Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j192_H5_FITS_custom_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3804936960.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 64.39645910263062
Epoch: 1, Steps: 88 | Train Loss: 1.0941230 Vali Loss: 1.1615063 Test Loss: 1.3376925
Validation loss decreased (inf --> 1.161506).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 65.22567105293274
Epoch: 2, Steps: 88 | Train Loss: 0.8299339 Vali Loss: 1.0184571 Test Loss: 1.1693546
Validation loss decreased (1.161506 --> 1.018457).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 68.47008204460144
Epoch: 3, Steps: 88 | Train Loss: 0.7228389 Vali Loss: 0.9469884 Test Loss: 1.0869209
Validation loss decreased (1.018457 --> 0.946988).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 66.73761367797852
Epoch: 4, Steps: 88 | Train Loss: 0.6523189 Vali Loss: 0.8966615 Test Loss: 1.0285939
Validation loss decreased (0.946988 --> 0.896662).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 74.21947169303894
Epoch: 5, Steps: 88 | Train Loss: 0.5961937 Vali Loss: 0.8531993 Test Loss: 0.9799102
Validation loss decreased (0.896662 --> 0.853199).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 74.50421667098999
Epoch: 6, Steps: 88 | Train Loss: 0.5490794 Vali Loss: 0.8168195 Test Loss: 0.9383513
Validation loss decreased (0.853199 --> 0.816819).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 73.46163368225098
Epoch: 7, Steps: 88 | Train Loss: 0.5086244 Vali Loss: 0.7783625 Test Loss: 0.8947350
Validation loss decreased (0.816819 --> 0.778363).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 71.93017578125
Epoch: 8, Steps: 88 | Train Loss: 0.4734746 Vali Loss: 0.7476102 Test Loss: 0.8588685
Validation loss decreased (0.778363 --> 0.747610).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 74.11355757713318
Epoch: 9, Steps: 88 | Train Loss: 0.4425923 Vali Loss: 0.7218261 Test Loss: 0.8309244
Validation loss decreased (0.747610 --> 0.721826).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 63.54389810562134
Epoch: 10, Steps: 88 | Train Loss: 0.4152791 Vali Loss: 0.6945994 Test Loss: 0.7992995
Validation loss decreased (0.721826 --> 0.694599).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 63.50956058502197
Epoch: 11, Steps: 88 | Train Loss: 0.3909517 Vali Loss: 0.6700192 Test Loss: 0.7708814
Validation loss decreased (0.694599 --> 0.670019).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 62.928650856018066
Epoch: 12, Steps: 88 | Train Loss: 0.3692355 Vali Loss: 0.6492231 Test Loss: 0.7471839
Validation loss decreased (0.670019 --> 0.649223).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 64.24088668823242
Epoch: 13, Steps: 88 | Train Loss: 0.3497688 Vali Loss: 0.6308222 Test Loss: 0.7268699
Validation loss decreased (0.649223 --> 0.630822).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 61.58078670501709
Epoch: 14, Steps: 88 | Train Loss: 0.3322185 Vali Loss: 0.6138872 Test Loss: 0.7078493
Validation loss decreased (0.630822 --> 0.613887).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 61.23567724227905
Epoch: 15, Steps: 88 | Train Loss: 0.3163622 Vali Loss: 0.5984154 Test Loss: 0.6896769
Validation loss decreased (0.613887 --> 0.598415).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 67.87016105651855
Epoch: 16, Steps: 88 | Train Loss: 0.3020373 Vali Loss: 0.5823988 Test Loss: 0.6711619
Validation loss decreased (0.598415 --> 0.582399).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 67.61977958679199
Epoch: 17, Steps: 88 | Train Loss: 0.2889597 Vali Loss: 0.5694180 Test Loss: 0.6571528
Validation loss decreased (0.582399 --> 0.569418).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 72.09253239631653
Epoch: 18, Steps: 88 | Train Loss: 0.2771057 Vali Loss: 0.5573534 Test Loss: 0.6434571
Validation loss decreased (0.569418 --> 0.557353).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 71.64262819290161
Epoch: 19, Steps: 88 | Train Loss: 0.2662752 Vali Loss: 0.5449229 Test Loss: 0.6292662
Validation loss decreased (0.557353 --> 0.544923).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 71.41906428337097
Epoch: 20, Steps: 88 | Train Loss: 0.2563977 Vali Loss: 0.5345418 Test Loss: 0.6177558
Validation loss decreased (0.544923 --> 0.534542).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 72.43820428848267
Epoch: 21, Steps: 88 | Train Loss: 0.2473143 Vali Loss: 0.5249779 Test Loss: 0.6068070
Validation loss decreased (0.534542 --> 0.524978).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 71.499440908432
Epoch: 22, Steps: 88 | Train Loss: 0.2390044 Vali Loss: 0.5149688 Test Loss: 0.5959235
Validation loss decreased (0.524978 --> 0.514969).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 73.54601240158081
Epoch: 23, Steps: 88 | Train Loss: 0.2313786 Vali Loss: 0.5066312 Test Loss: 0.5867333
Validation loss decreased (0.514969 --> 0.506631).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 76.32630729675293
Epoch: 24, Steps: 88 | Train Loss: 0.2243256 Vali Loss: 0.4993761 Test Loss: 0.5778230
Validation loss decreased (0.506631 --> 0.499376).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 73.79677653312683
Epoch: 25, Steps: 88 | Train Loss: 0.2178569 Vali Loss: 0.4938776 Test Loss: 0.5717350
Validation loss decreased (0.499376 --> 0.493878).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 70.1708996295929
Epoch: 26, Steps: 88 | Train Loss: 0.2118488 Vali Loss: 0.4862336 Test Loss: 0.5636340
Validation loss decreased (0.493878 --> 0.486234).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 74.1141426563263
Epoch: 27, Steps: 88 | Train Loss: 0.2062874 Vali Loss: 0.4800652 Test Loss: 0.5568319
Validation loss decreased (0.486234 --> 0.480065).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 69.39153385162354
Epoch: 28, Steps: 88 | Train Loss: 0.2011274 Vali Loss: 0.4742670 Test Loss: 0.5503778
Validation loss decreased (0.480065 --> 0.474267).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 68.93271493911743
Epoch: 29, Steps: 88 | Train Loss: 0.1963710 Vali Loss: 0.4694609 Test Loss: 0.5447987
Validation loss decreased (0.474267 --> 0.469461).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 69.0372896194458
Epoch: 30, Steps: 88 | Train Loss: 0.1919339 Vali Loss: 0.4637239 Test Loss: 0.5387692
Validation loss decreased (0.469461 --> 0.463724).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 69.69126152992249
Epoch: 31, Steps: 88 | Train Loss: 0.1878084 Vali Loss: 0.4590826 Test Loss: 0.5336016
Validation loss decreased (0.463724 --> 0.459083).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 69.52803325653076
Epoch: 32, Steps: 88 | Train Loss: 0.1839801 Vali Loss: 0.4551221 Test Loss: 0.5294551
Validation loss decreased (0.459083 --> 0.455122).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 77.795889377594
Epoch: 33, Steps: 88 | Train Loss: 0.1803765 Vali Loss: 0.4504108 Test Loss: 0.5245662
Validation loss decreased (0.455122 --> 0.450411).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 63.31155967712402
Epoch: 34, Steps: 88 | Train Loss: 0.1770775 Vali Loss: 0.4468523 Test Loss: 0.5205446
Validation loss decreased (0.450411 --> 0.446852).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 66.68228197097778
Epoch: 35, Steps: 88 | Train Loss: 0.1739380 Vali Loss: 0.4436239 Test Loss: 0.5169072
Validation loss decreased (0.446852 --> 0.443624).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 65.89655637741089
Epoch: 36, Steps: 88 | Train Loss: 0.1710055 Vali Loss: 0.4403260 Test Loss: 0.5130566
Validation loss decreased (0.443624 --> 0.440326).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 68.09590291976929
Epoch: 37, Steps: 88 | Train Loss: 0.1682609 Vali Loss: 0.4370509 Test Loss: 0.5097237
Validation loss decreased (0.440326 --> 0.437051).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 68.59017896652222
Epoch: 38, Steps: 88 | Train Loss: 0.1657185 Vali Loss: 0.4343953 Test Loss: 0.5065518
Validation loss decreased (0.437051 --> 0.434395).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 71.2725441455841
Epoch: 39, Steps: 88 | Train Loss: 0.1633195 Vali Loss: 0.4316381 Test Loss: 0.5036306
Validation loss decreased (0.434395 --> 0.431638).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 69.56387138366699
Epoch: 40, Steps: 88 | Train Loss: 0.1610831 Vali Loss: 0.4284327 Test Loss: 0.5004947
Validation loss decreased (0.431638 --> 0.428433).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 68.42744326591492
Epoch: 41, Steps: 88 | Train Loss: 0.1590029 Vali Loss: 0.4269228 Test Loss: 0.4984699
Validation loss decreased (0.428433 --> 0.426923).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 67.64074683189392
Epoch: 42, Steps: 88 | Train Loss: 0.1569835 Vali Loss: 0.4242916 Test Loss: 0.4956005
Validation loss decreased (0.426923 --> 0.424292).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 65.50216174125671
Epoch: 43, Steps: 88 | Train Loss: 0.1550966 Vali Loss: 0.4218661 Test Loss: 0.4930989
Validation loss decreased (0.424292 --> 0.421866).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 68.33120059967041
Epoch: 44, Steps: 88 | Train Loss: 0.1533732 Vali Loss: 0.4200474 Test Loss: 0.4911655
Validation loss decreased (0.421866 --> 0.420047).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 75.33276915550232
Epoch: 45, Steps: 88 | Train Loss: 0.1516863 Vali Loss: 0.4180142 Test Loss: 0.4889401
Validation loss decreased (0.420047 --> 0.418014).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 65.61767220497131
Epoch: 46, Steps: 88 | Train Loss: 0.1501600 Vali Loss: 0.4159782 Test Loss: 0.4868909
Validation loss decreased (0.418014 --> 0.415978).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 68.91279220581055
Epoch: 47, Steps: 88 | Train Loss: 0.1486891 Vali Loss: 0.4150439 Test Loss: 0.4849704
Validation loss decreased (0.415978 --> 0.415044).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 69.19151306152344
Epoch: 48, Steps: 88 | Train Loss: 0.1472731 Vali Loss: 0.4126930 Test Loss: 0.4832573
Validation loss decreased (0.415044 --> 0.412693).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 67.44988465309143
Epoch: 49, Steps: 88 | Train Loss: 0.1459834 Vali Loss: 0.4120545 Test Loss: 0.4817802
Validation loss decreased (0.412693 --> 0.412055).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 71.43929839134216
Epoch: 50, Steps: 88 | Train Loss: 0.1447558 Vali Loss: 0.4095716 Test Loss: 0.4800228
Validation loss decreased (0.412055 --> 0.409572).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3804936960.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 74.44169640541077
Epoch: 1, Steps: 88 | Train Loss: 0.2599219 Vali Loss: 0.3388144 Test Loss: 0.4106608
Validation loss decreased (inf --> 0.338814).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 67.34554505348206
Epoch: 2, Steps: 88 | Train Loss: 0.2448502 Vali Loss: 0.3372643 Test Loss: 0.4101426
Validation loss decreased (0.338814 --> 0.337264).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 61.55591440200806
Epoch: 3, Steps: 88 | Train Loss: 0.2444969 Vali Loss: 0.3368877 Test Loss: 0.4099169
Validation loss decreased (0.337264 --> 0.336888).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 63.717931270599365
Epoch: 4, Steps: 88 | Train Loss: 0.2443150 Vali Loss: 0.3362107 Test Loss: 0.4093114
Validation loss decreased (0.336888 --> 0.336211).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 69.92298126220703
Epoch: 5, Steps: 88 | Train Loss: 0.2443278 Vali Loss: 0.3360879 Test Loss: 0.4092442
Validation loss decreased (0.336211 --> 0.336088).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 68.22975206375122
Epoch: 6, Steps: 88 | Train Loss: 0.2441525 Vali Loss: 0.3361649 Test Loss: 0.4094403
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 73.72610855102539
Epoch: 7, Steps: 88 | Train Loss: 0.2441717 Vali Loss: 0.3360467 Test Loss: 0.4093365
Validation loss decreased (0.336088 --> 0.336047).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 68.46862602233887
Epoch: 8, Steps: 88 | Train Loss: 0.2440718 Vali Loss: 0.3361051 Test Loss: 0.4090962
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 67.29829907417297
Epoch: 9, Steps: 88 | Train Loss: 0.2440962 Vali Loss: 0.3359008 Test Loss: 0.4092402
Validation loss decreased (0.336047 --> 0.335901).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 67.02292227745056
Epoch: 10, Steps: 88 | Train Loss: 0.2439509 Vali Loss: 0.3357512 Test Loss: 0.4088629
Validation loss decreased (0.335901 --> 0.335751).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 66.74329447746277
Epoch: 11, Steps: 88 | Train Loss: 0.2440031 Vali Loss: 0.3356841 Test Loss: 0.4091608
Validation loss decreased (0.335751 --> 0.335684).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 74.46245408058167
Epoch: 12, Steps: 88 | Train Loss: 0.2440476 Vali Loss: 0.3356418 Test Loss: 0.4093310
Validation loss decreased (0.335684 --> 0.335642).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 71.19071936607361
Epoch: 13, Steps: 88 | Train Loss: 0.2439147 Vali Loss: 0.3355120 Test Loss: 0.4089190
Validation loss decreased (0.335642 --> 0.335512).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 68.96711134910583
Epoch: 14, Steps: 88 | Train Loss: 0.2438753 Vali Loss: 0.3358569 Test Loss: 0.4091114
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 68.92654252052307
Epoch: 15, Steps: 88 | Train Loss: 0.2438398 Vali Loss: 0.3360370 Test Loss: 0.4094183
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 70.22734665870667
Epoch: 16, Steps: 88 | Train Loss: 0.2439075 Vali Loss: 0.3359765 Test Loss: 0.4090182
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j192_H5_FITS_custom_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.4080033600330353, mae:0.2875829339027405, rse:0.527182400226593, corr:[0.2875409  0.2933455  0.2933799  0.29374024 0.29398745 0.29388362
 0.29384148 0.29399312 0.2940662  0.2939021  0.29371327 0.29369432
 0.29369524 0.2934354  0.29298973 0.29264164 0.292581   0.29273736
 0.29295748 0.29318377 0.29331297 0.29316708 0.29287377 0.29323325
 0.2943353  0.29470098 0.2948201  0.29491776 0.29478326 0.29442492
 0.29402116 0.29368564 0.2934402  0.29324958 0.2930893  0.2930868
 0.293378   0.29374924 0.29391295 0.29379338 0.2935986  0.29353097
 0.29356417 0.2935254  0.2933089  0.29297596 0.29276752 0.29300922
 0.29338798 0.29322496 0.29289502 0.29270932 0.29278296 0.29306027
 0.29333118 0.29338604 0.293205   0.2929348  0.29273278 0.29269254
 0.29280815 0.29285774 0.29270887 0.2924056  0.29213017 0.29201412
 0.29201275 0.29203603 0.29204366 0.29207456 0.2921387  0.29225987
 0.2922158  0.2920455  0.29186934 0.29162833 0.29142794 0.29141173
 0.29155514 0.29172164 0.29180998 0.2918119  0.2918222  0.29198456
 0.2923129  0.29255715 0.292489   0.292109   0.29166663 0.29137862
 0.29123867 0.29111975 0.29102868 0.29106835 0.29121333 0.29130304
 0.29104033 0.29074398 0.2907208  0.29093233 0.29121283 0.29132575
 0.29110602 0.29066712 0.29033497 0.2903152  0.29053363 0.29077557
 0.290901   0.29085785 0.29069802 0.29048985 0.29032972 0.2903657
 0.29057425 0.2907913  0.29085118 0.29066175 0.2904367  0.29022926
 0.28997698 0.28989872 0.29009515 0.2904364  0.29088214 0.29117513
 0.29109278 0.29075572 0.290528   0.29064247 0.2910311  0.29139355
 0.29147932 0.29132622 0.29117817 0.2911992  0.29133046 0.29137343
 0.2912911  0.2911895  0.29120186 0.29132882 0.29136065 0.29126203
 0.29115978 0.2910793  0.29127142 0.29167205 0.29198214 0.29196176
 0.29165456 0.29136142 0.29127494 0.29131472 0.29130545 0.2912612
 0.29132608 0.29152173 0.29170612 0.29174525 0.29171848 0.29174477
 0.29185247 0.29194278 0.2918905  0.29168117 0.29149133 0.2919289
 0.2929743  0.29321024 0.29308167 0.29293358 0.29276404 0.29266346
 0.29261205 0.29252535 0.2924232  0.29241404 0.29247746 0.2925205
 0.29257694 0.29274333 0.29305652 0.29322457 0.292968   0.2925146
 0.2923455  0.29261497 0.29266906 0.2919943  0.2915403  0.2932028 ]
