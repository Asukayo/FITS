Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H8', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14688811008.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 89.14350152015686
Epoch: 1, Steps: 84 | Train Loss: 1.2349386 Vali Loss: 1.2358446 Test Loss: 1.4577866
Validation loss decreased (inf --> 1.235845).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 104.44389152526855
Epoch: 2, Steps: 84 | Train Loss: 0.8852041 Vali Loss: 1.0838842 Test Loss: 1.2711059
Validation loss decreased (1.235845 --> 1.083884).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 99.37511825561523
Epoch: 3, Steps: 84 | Train Loss: 0.7813253 Vali Loss: 1.0133507 Test Loss: 1.1853892
Validation loss decreased (1.083884 --> 1.013351).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 88.11353611946106
Epoch: 4, Steps: 84 | Train Loss: 0.7148151 Vali Loss: 0.9596008 Test Loss: 1.1210743
Validation loss decreased (1.013351 --> 0.959601).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 94.413400888443
Epoch: 5, Steps: 84 | Train Loss: 0.6612529 Vali Loss: 0.9123209 Test Loss: 1.0658907
Validation loss decreased (0.959601 --> 0.912321).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 96.73023414611816
Epoch: 6, Steps: 84 | Train Loss: 0.6160599 Vali Loss: 0.8707352 Test Loss: 1.0168538
Validation loss decreased (0.912321 --> 0.870735).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 101.90977811813354
Epoch: 7, Steps: 84 | Train Loss: 0.5770479 Vali Loss: 0.8358532 Test Loss: 0.9766434
Validation loss decreased (0.870735 --> 0.835853).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 102.82178950309753
Epoch: 8, Steps: 84 | Train Loss: 0.5432950 Vali Loss: 0.8035051 Test Loss: 0.9386217
Validation loss decreased (0.835853 --> 0.803505).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 96.8862829208374
Epoch: 9, Steps: 84 | Train Loss: 0.5134732 Vali Loss: 0.7759553 Test Loss: 0.9060497
Validation loss decreased (0.803505 --> 0.775955).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 102.96096277236938
Epoch: 10, Steps: 84 | Train Loss: 0.4870061 Vali Loss: 0.7504182 Test Loss: 0.8757338
Validation loss decreased (0.775955 --> 0.750418).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 97.87391901016235
Epoch: 11, Steps: 84 | Train Loss: 0.4634337 Vali Loss: 0.7258206 Test Loss: 0.8476492
Validation loss decreased (0.750418 --> 0.725821).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 98.40765166282654
Epoch: 12, Steps: 84 | Train Loss: 0.4423372 Vali Loss: 0.7051505 Test Loss: 0.8233104
Validation loss decreased (0.725821 --> 0.705150).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 103.15758490562439
Epoch: 13, Steps: 84 | Train Loss: 0.4233895 Vali Loss: 0.6857107 Test Loss: 0.8009964
Validation loss decreased (0.705150 --> 0.685711).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 100.29721236228943
Epoch: 14, Steps: 84 | Train Loss: 0.4062314 Vali Loss: 0.6685792 Test Loss: 0.7807775
Validation loss decreased (0.685711 --> 0.668579).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 105.69941711425781
Epoch: 15, Steps: 84 | Train Loss: 0.3907135 Vali Loss: 0.6523905 Test Loss: 0.7614567
Validation loss decreased (0.668579 --> 0.652390).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 98.43106150627136
Epoch: 16, Steps: 84 | Train Loss: 0.3765887 Vali Loss: 0.6380541 Test Loss: 0.7446685
Validation loss decreased (0.652390 --> 0.638054).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 93.08789563179016
Epoch: 17, Steps: 84 | Train Loss: 0.3637395 Vali Loss: 0.6240755 Test Loss: 0.7279825
Validation loss decreased (0.638054 --> 0.624075).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 96.11638140678406
Epoch: 18, Steps: 84 | Train Loss: 0.3520268 Vali Loss: 0.6115648 Test Loss: 0.7139695
Validation loss decreased (0.624075 --> 0.611565).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 101.89490365982056
Epoch: 19, Steps: 84 | Train Loss: 0.3412603 Vali Loss: 0.6010360 Test Loss: 0.7011511
Validation loss decreased (0.611565 --> 0.601036).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 99.24051761627197
Epoch: 20, Steps: 84 | Train Loss: 0.3314229 Vali Loss: 0.5900193 Test Loss: 0.6882120
Validation loss decreased (0.601036 --> 0.590019).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 91.21038722991943
Epoch: 21, Steps: 84 | Train Loss: 0.3223797 Vali Loss: 0.5796272 Test Loss: 0.6765059
Validation loss decreased (0.590019 --> 0.579627).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 105.32861638069153
Epoch: 22, Steps: 84 | Train Loss: 0.3140434 Vali Loss: 0.5710337 Test Loss: 0.6662387
Validation loss decreased (0.579627 --> 0.571034).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 107.59341549873352
Epoch: 23, Steps: 84 | Train Loss: 0.3063655 Vali Loss: 0.5630375 Test Loss: 0.6563736
Validation loss decreased (0.571034 --> 0.563037).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 94.24329018592834
Epoch: 24, Steps: 84 | Train Loss: 0.2992446 Vali Loss: 0.5545742 Test Loss: 0.6471282
Validation loss decreased (0.563037 --> 0.554574).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 99.45378112792969
Epoch: 25, Steps: 84 | Train Loss: 0.2927144 Vali Loss: 0.5483317 Test Loss: 0.6390897
Validation loss decreased (0.554574 --> 0.548332).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 103.69852662086487
Epoch: 26, Steps: 84 | Train Loss: 0.2866017 Vali Loss: 0.5424019 Test Loss: 0.6315515
Validation loss decreased (0.548332 --> 0.542402).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 97.60152006149292
Epoch: 27, Steps: 84 | Train Loss: 0.2809111 Vali Loss: 0.5358431 Test Loss: 0.6247734
Validation loss decreased (0.542402 --> 0.535843).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 103.36908960342407
Epoch: 28, Steps: 84 | Train Loss: 0.2756966 Vali Loss: 0.5298480 Test Loss: 0.6177874
Validation loss decreased (0.535843 --> 0.529848).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 94.71497321128845
Epoch: 29, Steps: 84 | Train Loss: 0.2707836 Vali Loss: 0.5246553 Test Loss: 0.6112517
Validation loss decreased (0.529848 --> 0.524655).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 93.35141968727112
Epoch: 30, Steps: 84 | Train Loss: 0.2662671 Vali Loss: 0.5198586 Test Loss: 0.6052889
Validation loss decreased (0.524655 --> 0.519859).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 102.27262902259827
Epoch: 31, Steps: 84 | Train Loss: 0.2620274 Vali Loss: 0.5147102 Test Loss: 0.6000814
Validation loss decreased (0.519859 --> 0.514710).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 106.79982995986938
Epoch: 32, Steps: 84 | Train Loss: 0.2580738 Vali Loss: 0.5105966 Test Loss: 0.5948565
Validation loss decreased (0.514710 --> 0.510597).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 94.25763988494873
Epoch: 33, Steps: 84 | Train Loss: 0.2543514 Vali Loss: 0.5064741 Test Loss: 0.5901362
Validation loss decreased (0.510597 --> 0.506474).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 101.00067448616028
Epoch: 34, Steps: 84 | Train Loss: 0.2508704 Vali Loss: 0.5032327 Test Loss: 0.5860339
Validation loss decreased (0.506474 --> 0.503233).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 106.22024345397949
Epoch: 35, Steps: 84 | Train Loss: 0.2476812 Vali Loss: 0.4992186 Test Loss: 0.5817885
Validation loss decreased (0.503233 --> 0.499219).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 95.03777766227722
Epoch: 36, Steps: 84 | Train Loss: 0.2446095 Vali Loss: 0.4961508 Test Loss: 0.5778563
Validation loss decreased (0.499219 --> 0.496151).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 93.12251114845276
Epoch: 37, Steps: 84 | Train Loss: 0.2417469 Vali Loss: 0.4929366 Test Loss: 0.5739993
Validation loss decreased (0.496151 --> 0.492937).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 102.70492148399353
Epoch: 38, Steps: 84 | Train Loss: 0.2391159 Vali Loss: 0.4903928 Test Loss: 0.5709532
Validation loss decreased (0.492937 --> 0.490393).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 101.11892771720886
Epoch: 39, Steps: 84 | Train Loss: 0.2366120 Vali Loss: 0.4869471 Test Loss: 0.5674395
Validation loss decreased (0.490393 --> 0.486947).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 103.5137710571289
Epoch: 40, Steps: 84 | Train Loss: 0.2342585 Vali Loss: 0.4841338 Test Loss: 0.5644427
Validation loss decreased (0.486947 --> 0.484134).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 104.29605054855347
Epoch: 41, Steps: 84 | Train Loss: 0.2320076 Vali Loss: 0.4825373 Test Loss: 0.5617489
Validation loss decreased (0.484134 --> 0.482537).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 121.96733117103577
Epoch: 42, Steps: 84 | Train Loss: 0.2299078 Vali Loss: 0.4799818 Test Loss: 0.5590870
Validation loss decreased (0.482537 --> 0.479982).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 109.78521180152893
Epoch: 43, Steps: 84 | Train Loss: 0.2279244 Vali Loss: 0.4777058 Test Loss: 0.5565000
Validation loss decreased (0.479982 --> 0.477706).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 109.45477390289307
Epoch: 44, Steps: 84 | Train Loss: 0.2260846 Vali Loss: 0.4756874 Test Loss: 0.5541620
Validation loss decreased (0.477706 --> 0.475687).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 101.58792972564697
Epoch: 45, Steps: 84 | Train Loss: 0.2242737 Vali Loss: 0.4738647 Test Loss: 0.5519350
Validation loss decreased (0.475687 --> 0.473865).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 104.02320337295532
Epoch: 46, Steps: 84 | Train Loss: 0.2226631 Vali Loss: 0.4720328 Test Loss: 0.5497617
Validation loss decreased (0.473865 --> 0.472033).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 92.90504217147827
Epoch: 47, Steps: 84 | Train Loss: 0.2211353 Vali Loss: 0.4702073 Test Loss: 0.5478246
Validation loss decreased (0.472033 --> 0.470207).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 92.94257092475891
Epoch: 48, Steps: 84 | Train Loss: 0.2196470 Vali Loss: 0.4686756 Test Loss: 0.5459992
Validation loss decreased (0.470207 --> 0.468676).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 99.52752947807312
Epoch: 49, Steps: 84 | Train Loss: 0.2182020 Vali Loss: 0.4671867 Test Loss: 0.5440765
Validation loss decreased (0.468676 --> 0.467187).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 97.00215244293213
Epoch: 50, Steps: 84 | Train Loss: 0.2168747 Vali Loss: 0.4658083 Test Loss: 0.5423445
Validation loss decreased (0.467187 --> 0.465808).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14688811008.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 101.7393114566803
Epoch: 1, Steps: 84 | Train Loss: 0.3150331 Vali Loss: 0.4099253 Test Loss: 0.4779594
Validation loss decreased (inf --> 0.409925).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 98.7431366443634
Epoch: 2, Steps: 84 | Train Loss: 0.2835839 Vali Loss: 0.3917124 Test Loss: 0.4571894
Validation loss decreased (0.409925 --> 0.391712).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 95.378977060318
Epoch: 3, Steps: 84 | Train Loss: 0.2739001 Vali Loss: 0.3876294 Test Loss: 0.4525015
Validation loss decreased (0.391712 --> 0.387629).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 95.5599434375763
Epoch: 4, Steps: 84 | Train Loss: 0.2714715 Vali Loss: 0.3865350 Test Loss: 0.4508921
Validation loss decreased (0.387629 --> 0.386535).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 112.33393669128418
Epoch: 5, Steps: 84 | Train Loss: 0.2708891 Vali Loss: 0.3862522 Test Loss: 0.4508137
Validation loss decreased (0.386535 --> 0.386252).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 111.70108795166016
Epoch: 6, Steps: 84 | Train Loss: 0.2708184 Vali Loss: 0.3856919 Test Loss: 0.4515340
Validation loss decreased (0.386252 --> 0.385692).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 112.03654193878174
Epoch: 7, Steps: 84 | Train Loss: 0.2707527 Vali Loss: 0.3859454 Test Loss: 0.4506973
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 120.98027777671814
Epoch: 8, Steps: 84 | Train Loss: 0.2706777 Vali Loss: 0.3857456 Test Loss: 0.4499702
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 107.53694772720337
Epoch: 9, Steps: 84 | Train Loss: 0.2705817 Vali Loss: 0.3859166 Test Loss: 0.4507608
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4510067105293274, mae:0.3017489016056061, rse:0.5491516590118408, corr:[0.2608835  0.26416484 0.26791513 0.26562533 0.26722097 0.2685724
 0.26749632 0.26897404 0.26972225 0.26860023 0.2690313  0.26899606
 0.26806757 0.26850706 0.26805484 0.26654518 0.26697975 0.2677756
 0.26748335 0.2678539  0.26821187 0.26774886 0.26784843 0.2682812
 0.26914048 0.2696892  0.27011937 0.2695916  0.26912823 0.2691321
 0.26885182 0.2689287  0.26920077 0.2686713  0.26808843 0.2678842
 0.26746866 0.26740444 0.26756507 0.2671399  0.26727477 0.26809424
 0.26798406 0.26749364 0.26772228 0.26792002 0.26799643 0.26860923
 0.26901114 0.26857838 0.26855615 0.26854035 0.26816082 0.26831552
 0.26870367 0.26850715 0.2682951  0.26839936 0.2683651  0.2682339
 0.2679428  0.26737332 0.26751292 0.26805753 0.26787734 0.26763636
 0.26773474 0.26735646 0.26690015 0.26713297 0.2673236  0.2673999
 0.26768747 0.267751   0.26768726 0.26806182 0.26849043 0.2685365
 0.26854494 0.26829633 0.2677775  0.26788196 0.26814812 0.2677713
 0.26761094 0.26804015 0.26798505 0.26757547 0.267621   0.2675037
 0.26726145 0.2672193  0.26709658 0.26698327 0.26726454 0.26737642
 0.26689225 0.2669774  0.26739645 0.2673551  0.26741618 0.26763493
 0.26766357 0.26757947 0.26715088 0.2667672  0.26708308 0.26725888
 0.26657    0.26635244 0.26701784 0.2673616  0.26738247 0.2673308
 0.2670662  0.2667975  0.26684666 0.2663288  0.26595658 0.2667966
 0.2675107  0.26760474 0.2679503  0.26796845 0.26751566 0.2679815
 0.2686998  0.26792654 0.2670903  0.26753923 0.26784754 0.2677785
 0.26802972 0.26792872 0.26757395 0.2678055  0.26783913 0.26711217
 0.26742226 0.26762277 0.26703855 0.26702183 0.26751634 0.267431
 0.26759923 0.26817748 0.26823217 0.26805678 0.26827368 0.26837757
 0.26844952 0.26862097 0.26821816 0.2677011  0.26758888 0.2672976
 0.26698747 0.2674724  0.2681825  0.2685406  0.2689019  0.2690379
 0.26885194 0.26864338 0.26838124 0.26804835 0.26803416 0.26836187
 0.2693352  0.26966655 0.26982963 0.26957223 0.2693996  0.2695802
 0.269411   0.26918262 0.26937968 0.2692741  0.2688251  0.2689599
 0.26936984 0.26948252 0.26982117 0.27023628 0.2701108  0.2697565
 0.2694501  0.2692922  0.2694069  0.26931494 0.26867917 0.26859885
 0.26983237 0.27002898 0.2697749  0.2697985  0.2696856  0.2695785
 0.2699492  0.27017725 0.26983926 0.26954114 0.26919305 0.26876903
 0.26908538 0.26976496 0.26982176 0.2696074  0.26958853 0.26930395
 0.26887694 0.2689745  0.26940995 0.26960433 0.2694033  0.26936868
 0.2698403  0.27003628 0.26982757 0.26965407 0.2696727  0.2696145
 0.2697284  0.27002436 0.2696595  0.2689372  0.26898184 0.26919636
 0.26887792 0.26891005 0.26933554 0.26917544 0.26897544 0.269139
 0.2689109  0.26867726 0.26904982 0.2691465  0.26888704 0.2688476
 0.26873675 0.2687059  0.26907843 0.26920176 0.26927388 0.26963928
 0.26968157 0.26943725 0.26942942 0.26939765 0.26925465 0.2695071
 0.26955634 0.26897228 0.26885596 0.26929757 0.26910153 0.2686232
 0.26885596 0.26896456 0.26856026 0.2686819  0.26917192 0.26904187
 0.26863655 0.26886067 0.2692144  0.2691336  0.26892632 0.26902246
 0.26903188 0.268424   0.2678078  0.26821992 0.26870072 0.26838335
 0.2682547  0.26867682 0.2688719  0.26900432 0.2691683  0.26847124
 0.2677471  0.26798934 0.26839978 0.26829132 0.26845184 0.2688076
 0.26872656 0.26912245 0.26980972 0.2698345  0.2698373  0.27029118
 0.2702433  0.26949733 0.26871434 0.2680089  0.26778463 0.2684221
 0.26893312 0.26871663 0.2684532  0.26844406 0.26857573 0.2687378
 0.26887733 0.26866558 0.2688862  0.2694812  0.26968566 0.26970187
 0.2699848  0.27024266 0.27062762 0.27074254 0.2703918  0.27055326
 0.27104178 0.27052504 0.26983202 0.27006653 0.2699808  0.26944238
 0.26960978 0.2698538  0.26973355 0.27018842 0.27067223 0.2703118
 0.27027777 0.27059755 0.2703013  0.27001128 0.2701597  0.2701638
 0.27082217 0.27139467 0.27150387 0.27091143 0.27063212 0.2708046
 0.27070838 0.27076176 0.27107272 0.2707857  0.27008376 0.26988304
 0.2701648  0.2703657  0.2702675  0.27019253 0.27052942 0.2707803
 0.27062422 0.27074012 0.27104363 0.27094772 0.27077293 0.2710359
 0.27165815 0.2716501  0.27146986 0.2711721  0.27100867 0.27119505
 0.27112326 0.27084035 0.27104995 0.27133682 0.2709942  0.2707401
 0.270972   0.27090758 0.2706179  0.27062115 0.27052465 0.27025247
 0.270218   0.27035007 0.27056405 0.27072954 0.27041626 0.27018008
 0.27051175 0.2706586  0.2707136  0.27097782 0.27101576 0.2707185
 0.27079308 0.27136174 0.27186084 0.27182794 0.27096957 0.2698518
 0.26953614 0.26953918 0.2691562  0.26918158 0.26971266 0.2696249
 0.26931867 0.26958135 0.26963165 0.2694184  0.26953653 0.26950383
 0.26926032 0.26963374 0.27002442 0.26988852 0.2699602  0.2700097
 0.26970008 0.26991618 0.27051324 0.27055484 0.27047455 0.2705191
 0.26972032 0.2684306  0.26800773 0.26817542 0.26845372 0.2690963
 0.2694332  0.26873446 0.26826096 0.2688362  0.26934558 0.26931685
 0.26905543 0.26899135 0.26923367 0.26950338 0.26940787 0.2694131
 0.26956558 0.26939833 0.26947832 0.27013966 0.27040827 0.27016032
 0.27005693 0.26989737 0.26970783 0.26989067 0.26984847 0.2693579
 0.26940563 0.26944706 0.2687508  0.26802605 0.26789904 0.26805806
 0.2687476  0.2701006  0.27062407 0.27031815 0.27050853 0.27085498
 0.27035215 0.26966062 0.26964658 0.2698175  0.2695845  0.26930815
 0.26944378 0.26959696 0.26931486 0.2690674  0.2692164  0.2692537
 0.2691711  0.26939568 0.2696053  0.2693765  0.26928833 0.26975623
 0.27032802 0.2706586  0.27080342 0.27048585 0.27019498 0.27041328
 0.27045417 0.27015826 0.2702676  0.27042162 0.27009076 0.26972649
 0.26942095 0.26915002 0.26936695 0.26967987 0.26953316 0.26955292
 0.26982412 0.26980323 0.27003947 0.27061728 0.27046368 0.27017
 0.27109578 0.27129647 0.27074593 0.27040255 0.27022645 0.26996872
 0.27008858 0.27030268 0.27006942 0.26992148 0.2698917  0.26972342
 0.26996893 0.27026093 0.26986197 0.26976448 0.27038214 0.27038035
 0.2698996  0.2700318  0.27029106 0.27014235 0.2698457  0.2697815
 0.27049023 0.2707514  0.27010858 0.2693997  0.26945433 0.26954487
 0.2693149  0.2696939  0.27033076 0.27020708 0.26986367 0.2698818
 0.26970288 0.2694484  0.26949704 0.2694793  0.26957968 0.26971996
 0.26906428 0.2684123  0.26881278 0.2690377  0.26857367 0.2690546
 0.27005428 0.269898   0.269447   0.2696391  0.2700457  0.270302
 0.270275   0.27010584 0.27027464 0.27030596 0.26953396 0.26884446
 0.2686822  0.2680333  0.26744306 0.2681185  0.26889867 0.26871046
 0.26843187 0.26827928 0.2679399  0.26797363 0.26807877 0.26785696
 0.268152   0.2689793  0.26928565 0.2693369  0.26937473 0.2687845
 0.26815668 0.2681479  0.26820454 0.26815972 0.26817253 0.2678375
 0.26723787 0.2668809  0.26684123 0.2671506  0.26763156 0.26775682
 0.26768634 0.26776877 0.26751852 0.26693282 0.2666809  0.26650482
 0.26624176 0.26689792 0.267959   0.26832464 0.26859894 0.26870653
 0.26788193 0.26731375 0.2677335  0.26776075 0.2673467  0.26734376
 0.2669098  0.26602614 0.26611754 0.26667497 0.26668358 0.26687604
 0.26731938 0.26727986 0.26705444 0.26676986 0.2664368  0.26667812
 0.26725113 0.2676434  0.26807505 0.26853397 0.26830944 0.26802158
 0.2681216  0.26775798 0.26725167 0.26723418 0.26686728 0.2664316
 0.26677668 0.26673335 0.2663134  0.26688978 0.2672586  0.2666551
 0.26673144 0.26713774 0.26663834 0.26659042 0.26725176 0.2672033
 0.26728976 0.2680867  0.2682366  0.26795527 0.26833645 0.2684228
 0.2679625  0.26790965 0.26756305 0.26676565 0.2666683  0.2665904
 0.26613593 0.26657465 0.2672286  0.26716387 0.26766023 0.26843566
 0.2682615  0.26820418 0.26856238 0.2683691  0.2683513  0.26909757
 0.26978207 0.2697381  0.2700004  0.2695818  0.26880586 0.2686737
 0.26833668 0.26801783 0.26825666 0.26750085 0.26652047 0.26759416
 0.2687979  0.26861772 0.26895592 0.26928565 0.26884678 0.269413
 0.26991886 0.26901025 0.26899835 0.26983514 0.26937512 0.26930892
 0.2705162  0.2701727  0.26948968 0.26981577 0.26939481 0.2690405
 0.2699794  0.26982757 0.2688708  0.26890454 0.26820487 0.26776844
 0.26930276 0.2696306  0.26960513 0.27156535 0.27171576 0.2707634
 0.2714618  0.26895556 0.26651067 0.26750687 0.26237366 0.27184424]
