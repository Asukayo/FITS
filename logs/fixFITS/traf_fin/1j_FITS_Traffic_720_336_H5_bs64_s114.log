Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j336_H5_FITS_custom_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4387511040.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 81.28996682167053
Epoch: 1, Steps: 87 | Train Loss: 0.8830347 Vali Loss: 0.7670490 Test Loss: 0.8811066
Validation loss decreased (inf --> 0.767049).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 81.41247701644897
Epoch: 2, Steps: 87 | Train Loss: 0.5032688 Vali Loss: 0.5465044 Test Loss: 0.6316547
Validation loss decreased (0.767049 --> 0.546504).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 81.745521068573
Epoch: 3, Steps: 87 | Train Loss: 0.3707424 Vali Loss: 0.4428075 Test Loss: 0.5175101
Validation loss decreased (0.546504 --> 0.442807).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 84.40696096420288
Epoch: 4, Steps: 87 | Train Loss: 0.3075282 Vali Loss: 0.3929361 Test Loss: 0.4639447
Validation loss decreased (0.442807 --> 0.392936).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 77.92158269882202
Epoch: 5, Steps: 87 | Train Loss: 0.2780577 Vali Loss: 0.3696687 Test Loss: 0.4400008
Validation loss decreased (0.392936 --> 0.369669).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 67.66184067726135
Epoch: 6, Steps: 87 | Train Loss: 0.2647565 Vali Loss: 0.3589728 Test Loss: 0.4300348
Validation loss decreased (0.369669 --> 0.358973).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 65.24704122543335
Epoch: 7, Steps: 87 | Train Loss: 0.2588943 Vali Loss: 0.3543128 Test Loss: 0.4259919
Validation loss decreased (0.358973 --> 0.354313).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 68.10382437705994
Epoch: 8, Steps: 87 | Train Loss: 0.2563131 Vali Loss: 0.3518174 Test Loss: 0.4242113
Validation loss decreased (0.354313 --> 0.351817).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 73.86669611930847
Epoch: 9, Steps: 87 | Train Loss: 0.2552459 Vali Loss: 0.3503191 Test Loss: 0.4234639
Validation loss decreased (0.351817 --> 0.350319).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 71.49908518791199
Epoch: 10, Steps: 87 | Train Loss: 0.2547378 Vali Loss: 0.3498795 Test Loss: 0.4232655
Validation loss decreased (0.350319 --> 0.349880).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 71.72357892990112
Epoch: 11, Steps: 87 | Train Loss: 0.2543952 Vali Loss: 0.3489736 Test Loss: 0.4231046
Validation loss decreased (0.349880 --> 0.348974).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 68.88660979270935
Epoch: 12, Steps: 87 | Train Loss: 0.2541934 Vali Loss: 0.3486861 Test Loss: 0.4228959
Validation loss decreased (0.348974 --> 0.348686).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 75.82096242904663
Epoch: 13, Steps: 87 | Train Loss: 0.2541224 Vali Loss: 0.3485525 Test Loss: 0.4228858
Validation loss decreased (0.348686 --> 0.348552).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 72.13540887832642
Epoch: 14, Steps: 87 | Train Loss: 0.2540499 Vali Loss: 0.3484323 Test Loss: 0.4227832
Validation loss decreased (0.348552 --> 0.348432).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 70.53419303894043
Epoch: 15, Steps: 87 | Train Loss: 0.2539194 Vali Loss: 0.3484594 Test Loss: 0.4226671
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 73.21249055862427
Epoch: 16, Steps: 87 | Train Loss: 0.2539680 Vali Loss: 0.3485245 Test Loss: 0.4229590
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 70.68526458740234
Epoch: 17, Steps: 87 | Train Loss: 0.2538692 Vali Loss: 0.3478865 Test Loss: 0.4226715
Validation loss decreased (0.348432 --> 0.347887).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 66.71444582939148
Epoch: 18, Steps: 87 | Train Loss: 0.2538661 Vali Loss: 0.3480826 Test Loss: 0.4227725
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 67.58002090454102
Epoch: 19, Steps: 87 | Train Loss: 0.2538670 Vali Loss: 0.3476343 Test Loss: 0.4225160
Validation loss decreased (0.347887 --> 0.347634).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 79.40994453430176
Epoch: 20, Steps: 87 | Train Loss: 0.2538494 Vali Loss: 0.3478041 Test Loss: 0.4226280
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 72.99573183059692
Epoch: 21, Steps: 87 | Train Loss: 0.2537770 Vali Loss: 0.3477989 Test Loss: 0.4227089
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 70.7833046913147
Epoch: 22, Steps: 87 | Train Loss: 0.2536865 Vali Loss: 0.3477430 Test Loss: 0.4226710
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j336_H5_FITS_custom_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.420474648475647, mae:0.29283255338668823, rse:0.5329287648200989, corr:[0.2845415  0.28485548 0.28429976 0.28749406 0.28959328 0.2892026
 0.28827563 0.28845784 0.2893334  0.2894308  0.2882992  0.28699225
 0.2866561  0.28708908 0.28726164 0.28669465 0.28602168 0.28607747
 0.28683648 0.2875286  0.28755385 0.28712922 0.2870262  0.28793857
 0.2892828  0.28925887 0.28863645 0.28837237 0.28850067 0.2885281
 0.28811365 0.2874479  0.28712478 0.28735706 0.2875724  0.28724942
 0.2866103  0.28628764 0.28667632 0.2874973  0.2881527  0.28833726
 0.28829384 0.288265   0.28833845 0.28827986 0.28799468 0.28791663
 0.28819963 0.28834927 0.2883702  0.28811905 0.2876445  0.28732422
 0.2873995  0.28765538 0.28767103 0.28733864 0.28694117 0.28685048
 0.28714037 0.28745535 0.28760853 0.2877547  0.28803614 0.28838202
 0.28868213 0.2885118  0.28806174 0.28780282 0.28791684 0.2882347
 0.2882158  0.28777626 0.28735575 0.28721085 0.28726617 0.28726223
 0.2871008  0.2869595  0.28705847 0.2873276  0.28742695 0.2871685
 0.28680286 0.2866928  0.28697792 0.28738606 0.2875616  0.28743362
 0.28725085 0.28711346 0.2870704  0.2870269  0.28690907 0.28684598
 0.28682318 0.28700078 0.28719807 0.2871189  0.2868681  0.2867381
 0.28683367 0.28698742 0.28698662 0.2868032  0.28661892 0.28659964
 0.28676036 0.28695357 0.28705412 0.28708407 0.28710526 0.28712532
 0.28708586 0.2869129  0.28674778 0.28670645 0.28690863 0.28712946
 0.28711548 0.28705618 0.2871349  0.28724894 0.2873559  0.2873289
 0.28717992 0.28712252 0.2873113  0.28761557 0.28776768 0.2876701
 0.2874657  0.28740147 0.28758103 0.2878328  0.28791055 0.28773296
 0.2874986  0.28741112 0.2874834  0.28757408 0.28750795 0.2873861
 0.28736454 0.28734055 0.28733873 0.28732622 0.2873326  0.2874264
 0.2875993  0.28776026 0.28777215 0.28759837 0.2873512  0.28722435
 0.28731653 0.28755936 0.2878232  0.28805938 0.28829172 0.28843746
 0.2884131  0.2882236  0.28800428 0.28788534 0.28786835 0.28817195
 0.28865483 0.2882751  0.28794104 0.28813833 0.28852877 0.2887112
 0.28857172 0.28836    0.2883604  0.28852603 0.2885214  0.28824377
 0.28796366 0.2879261  0.2881791  0.2884244  0.28834608 0.2879237
 0.28739458 0.2871551  0.28726366 0.28742054 0.28743908 0.28764683
 0.2881179  0.28812876 0.28803268 0.2879956  0.28792918 0.2878778
 0.28789595 0.28791875 0.28785232 0.28770122 0.2875364  0.28752136
 0.28768203 0.28778806 0.28773102 0.28759596 0.28752628 0.2874971
 0.287297   0.2868576  0.2863976  0.28620586 0.2863207  0.28662676
 0.28678054 0.28650242 0.28625897 0.286385   0.28674978 0.28703737
 0.28709278 0.28706068 0.28717244 0.28742617 0.2875438  0.28739995
 0.2872264  0.28724608 0.28746754 0.2876091  0.2874187  0.28695154
 0.28650653 0.2863487  0.2865443  0.2866803  0.28661057 0.28647894
 0.28646642 0.2866407  0.28687418 0.28691128 0.28678635 0.28676242
 0.286987   0.28731552 0.28744948 0.28721875 0.28673235 0.28625706
 0.28595945 0.28577912 0.28565738 0.28564307 0.28577486 0.2859449
 0.28587973 0.28555226 0.28531116 0.28547776 0.2859762  0.28643033
 0.28646398 0.28631544 0.28639248 0.28663492 0.28675058 0.28655022
 0.2861506  0.28592327 0.28605795 0.28629008 0.28619036 0.28566876
 0.28515187 0.28506574 0.28539726 0.2857022  0.28562027 0.28525788
 0.2850095  0.28514117 0.28550336 0.28575718 0.2858043  0.2857868
 0.28586802 0.28627804 0.28673768 0.28681472 0.28659806 0.2864178
 0.28641453 0.28641298 0.28616267 0.28569815 0.28537932 0.2854232
 0.2855736  0.28544703 0.28508738 0.28496864 0.28539777 0.28603107
 0.2863023  0.28613102 0.28603825 0.28641567 0.28702247 0.2872919
 0.2869684  0.28638127 0.28629813 0.28669247 0.28682005 0.28621116
 0.28533402 0.28507042 0.28555882 0.28597796 0.28566924 0.28511354
 0.2853752  0.28662905 0.2877756  0.28776234 0.28700534 0.28685057
 0.287664   0.28795454 0.28592277 0.28253165 0.2829025  0.28980315]
