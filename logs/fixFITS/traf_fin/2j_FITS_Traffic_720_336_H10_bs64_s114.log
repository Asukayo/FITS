Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16559226880.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 76.6455340385437
Epoch: 1, Steps: 87 | Train Loss: 1.0291503 Vali Loss: 1.0951320 Test Loss: 1.2763714
Validation loss decreased (inf --> 1.095132).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 76.68358969688416
Epoch: 2, Steps: 87 | Train Loss: 0.7572469 Vali Loss: 0.9750941 Test Loss: 1.1348143
Validation loss decreased (1.095132 --> 0.975094).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 78.2894697189331
Epoch: 3, Steps: 87 | Train Loss: 0.6648705 Vali Loss: 0.9099098 Test Loss: 1.0600784
Validation loss decreased (0.975094 --> 0.909910).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 67.9459056854248
Epoch: 4, Steps: 87 | Train Loss: 0.5987974 Vali Loss: 0.8579246 Test Loss: 0.9986879
Validation loss decreased (0.909910 --> 0.857925).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 70.50140142440796
Epoch: 5, Steps: 87 | Train Loss: 0.5449042 Vali Loss: 0.8135107 Test Loss: 0.9482515
Validation loss decreased (0.857925 --> 0.813511).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 76.84854197502136
Epoch: 6, Steps: 87 | Train Loss: 0.4994966 Vali Loss: 0.7752937 Test Loss: 0.9035776
Validation loss decreased (0.813511 --> 0.775294).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 72.881112575531
Epoch: 7, Steps: 87 | Train Loss: 0.4606157 Vali Loss: 0.7383648 Test Loss: 0.8606384
Validation loss decreased (0.775294 --> 0.738365).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 72.8341703414917
Epoch: 8, Steps: 87 | Train Loss: 0.4269883 Vali Loss: 0.7076118 Test Loss: 0.8254312
Validation loss decreased (0.738365 --> 0.707612).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 72.93067789077759
Epoch: 9, Steps: 87 | Train Loss: 0.3975394 Vali Loss: 0.6794654 Test Loss: 0.7927753
Validation loss decreased (0.707612 --> 0.679465).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 72.36007618904114
Epoch: 10, Steps: 87 | Train Loss: 0.3718050 Vali Loss: 0.6551141 Test Loss: 0.7650236
Validation loss decreased (0.679465 --> 0.655114).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 84.38313937187195
Epoch: 11, Steps: 87 | Train Loss: 0.3489972 Vali Loss: 0.6332963 Test Loss: 0.7396129
Validation loss decreased (0.655114 --> 0.633296).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 77.25177597999573
Epoch: 12, Steps: 87 | Train Loss: 0.3287436 Vali Loss: 0.6115575 Test Loss: 0.7142751
Validation loss decreased (0.633296 --> 0.611557).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 77.22556686401367
Epoch: 13, Steps: 87 | Train Loss: 0.3106820 Vali Loss: 0.5939855 Test Loss: 0.6940547
Validation loss decreased (0.611557 --> 0.593985).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 75.27819108963013
Epoch: 14, Steps: 87 | Train Loss: 0.2944856 Vali Loss: 0.5760299 Test Loss: 0.6736684
Validation loss decreased (0.593985 --> 0.576030).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 76.86704993247986
Epoch: 15, Steps: 87 | Train Loss: 0.2799908 Vali Loss: 0.5619362 Test Loss: 0.6568344
Validation loss decreased (0.576030 --> 0.561936).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 78.49811100959778
Epoch: 16, Steps: 87 | Train Loss: 0.2668844 Vali Loss: 0.5481313 Test Loss: 0.6414776
Validation loss decreased (0.561936 --> 0.548131).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 83.55727863311768
Epoch: 17, Steps: 87 | Train Loss: 0.2550416 Vali Loss: 0.5361082 Test Loss: 0.6275387
Validation loss decreased (0.548131 --> 0.536108).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 71.2614336013794
Epoch: 18, Steps: 87 | Train Loss: 0.2442885 Vali Loss: 0.5251675 Test Loss: 0.6148019
Validation loss decreased (0.536108 --> 0.525167).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 73.0618348121643
Epoch: 19, Steps: 87 | Train Loss: 0.2345592 Vali Loss: 0.5148314 Test Loss: 0.6032402
Validation loss decreased (0.525167 --> 0.514831).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 76.49464154243469
Epoch: 20, Steps: 87 | Train Loss: 0.2256801 Vali Loss: 0.5046325 Test Loss: 0.5911576
Validation loss decreased (0.514831 --> 0.504633).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 76.994624376297
Epoch: 21, Steps: 87 | Train Loss: 0.2175448 Vali Loss: 0.4955123 Test Loss: 0.5811349
Validation loss decreased (0.504633 --> 0.495512).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 79.48113226890564
Epoch: 22, Steps: 87 | Train Loss: 0.2101398 Vali Loss: 0.4883177 Test Loss: 0.5721247
Validation loss decreased (0.495512 --> 0.488318).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 75.03874039649963
Epoch: 23, Steps: 87 | Train Loss: 0.2033472 Vali Loss: 0.4810532 Test Loss: 0.5640455
Validation loss decreased (0.488318 --> 0.481053).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 76.87272715568542
Epoch: 24, Steps: 87 | Train Loss: 0.1970814 Vali Loss: 0.4738527 Test Loss: 0.5560376
Validation loss decreased (0.481053 --> 0.473853).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 76.05464005470276
Epoch: 25, Steps: 87 | Train Loss: 0.1913780 Vali Loss: 0.4679210 Test Loss: 0.5486828
Validation loss decreased (0.473853 --> 0.467921).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 76.87705993652344
Epoch: 26, Steps: 87 | Train Loss: 0.1860599 Vali Loss: 0.4618816 Test Loss: 0.5427394
Validation loss decreased (0.467921 --> 0.461882).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 80.59197163581848
Epoch: 27, Steps: 87 | Train Loss: 0.1811803 Vali Loss: 0.4562162 Test Loss: 0.5363944
Validation loss decreased (0.461882 --> 0.456216).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 76.33127951622009
Epoch: 28, Steps: 87 | Train Loss: 0.1766855 Vali Loss: 0.4523219 Test Loss: 0.5308166
Validation loss decreased (0.456216 --> 0.452322).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 71.18963050842285
Epoch: 29, Steps: 87 | Train Loss: 0.1724935 Vali Loss: 0.4464966 Test Loss: 0.5247061
Validation loss decreased (0.452322 --> 0.446497).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 70.79539585113525
Epoch: 30, Steps: 87 | Train Loss: 0.1686154 Vali Loss: 0.4432745 Test Loss: 0.5209034
Validation loss decreased (0.446497 --> 0.443274).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 66.93663811683655
Epoch: 31, Steps: 87 | Train Loss: 0.1650057 Vali Loss: 0.4390404 Test Loss: 0.5160943
Validation loss decreased (0.443274 --> 0.439040).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 71.74125671386719
Epoch: 32, Steps: 87 | Train Loss: 0.1616370 Vali Loss: 0.4353889 Test Loss: 0.5121219
Validation loss decreased (0.439040 --> 0.435389).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 78.64410710334778
Epoch: 33, Steps: 87 | Train Loss: 0.1585585 Vali Loss: 0.4313807 Test Loss: 0.5078113
Validation loss decreased (0.435389 --> 0.431381).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 77.79068541526794
Epoch: 34, Steps: 87 | Train Loss: 0.1556519 Vali Loss: 0.4281690 Test Loss: 0.5039286
Validation loss decreased (0.431381 --> 0.428169).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 74.67949843406677
Epoch: 35, Steps: 87 | Train Loss: 0.1529506 Vali Loss: 0.4252537 Test Loss: 0.5004578
Validation loss decreased (0.428169 --> 0.425254).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 76.24110317230225
Epoch: 36, Steps: 87 | Train Loss: 0.1504015 Vali Loss: 0.4220551 Test Loss: 0.4976868
Validation loss decreased (0.425254 --> 0.422055).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 79.67587614059448
Epoch: 37, Steps: 87 | Train Loss: 0.1480676 Vali Loss: 0.4199986 Test Loss: 0.4946044
Validation loss decreased (0.422055 --> 0.419999).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 78.08246660232544
Epoch: 38, Steps: 87 | Train Loss: 0.1458606 Vali Loss: 0.4168125 Test Loss: 0.4915303
Validation loss decreased (0.419999 --> 0.416813).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 81.26275324821472
Epoch: 39, Steps: 87 | Train Loss: 0.1437673 Vali Loss: 0.4144808 Test Loss: 0.4888386
Validation loss decreased (0.416813 --> 0.414481).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 81.27405118942261
Epoch: 40, Steps: 87 | Train Loss: 0.1418258 Vali Loss: 0.4128119 Test Loss: 0.4867317
Validation loss decreased (0.414481 --> 0.412812).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 81.68849754333496
Epoch: 41, Steps: 87 | Train Loss: 0.1400355 Vali Loss: 0.4106388 Test Loss: 0.4844090
Validation loss decreased (0.412812 --> 0.410639).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 76.71225309371948
Epoch: 42, Steps: 87 | Train Loss: 0.1383349 Vali Loss: 0.4084129 Test Loss: 0.4820172
Validation loss decreased (0.410639 --> 0.408413).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 81.97471714019775
Epoch: 43, Steps: 87 | Train Loss: 0.1367266 Vali Loss: 0.4066434 Test Loss: 0.4803084
Validation loss decreased (0.408413 --> 0.406643).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 74.54690933227539
Epoch: 44, Steps: 87 | Train Loss: 0.1351922 Vali Loss: 0.4048732 Test Loss: 0.4782700
Validation loss decreased (0.406643 --> 0.404873).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 78.54336309432983
Epoch: 45, Steps: 87 | Train Loss: 0.1337470 Vali Loss: 0.4033397 Test Loss: 0.4765722
Validation loss decreased (0.404873 --> 0.403340).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 74.05255031585693
Epoch: 46, Steps: 87 | Train Loss: 0.1324351 Vali Loss: 0.4017937 Test Loss: 0.4748516
Validation loss decreased (0.403340 --> 0.401794).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 65.17448687553406
Epoch: 47, Steps: 87 | Train Loss: 0.1311916 Vali Loss: 0.4002377 Test Loss: 0.4731218
Validation loss decreased (0.401794 --> 0.400238).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 80.32629823684692
Epoch: 48, Steps: 87 | Train Loss: 0.1299751 Vali Loss: 0.3990747 Test Loss: 0.4718120
Validation loss decreased (0.400238 --> 0.399075).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 75.24520444869995
Epoch: 49, Steps: 87 | Train Loss: 0.1288856 Vali Loss: 0.3977848 Test Loss: 0.4702467
Validation loss decreased (0.399075 --> 0.397785).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 74.66564846038818
Epoch: 50, Steps: 87 | Train Loss: 0.1278274 Vali Loss: 0.3966267 Test Loss: 0.4688239
Validation loss decreased (0.397785 --> 0.396627).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16559226880.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 72.87797546386719
Epoch: 1, Steps: 87 | Train Loss: 0.2631731 Vali Loss: 0.3440052 Test Loss: 0.4158301
Validation loss decreased (inf --> 0.344005).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 80.82808375358582
Epoch: 2, Steps: 87 | Train Loss: 0.2488091 Vali Loss: 0.3413184 Test Loss: 0.4143634
Validation loss decreased (0.344005 --> 0.341318).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 83.61598753929138
Epoch: 3, Steps: 87 | Train Loss: 0.2482506 Vali Loss: 0.3404187 Test Loss: 0.4138022
Validation loss decreased (0.341318 --> 0.340419).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 60.4951868057251
Epoch: 4, Steps: 87 | Train Loss: 0.2480329 Vali Loss: 0.3402278 Test Loss: 0.4139110
Validation loss decreased (0.340419 --> 0.340228).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 62.02858829498291
Epoch: 5, Steps: 87 | Train Loss: 0.2480824 Vali Loss: 0.3397719 Test Loss: 0.4139060
Validation loss decreased (0.340228 --> 0.339772).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 69.90060067176819
Epoch: 6, Steps: 87 | Train Loss: 0.2478993 Vali Loss: 0.3396294 Test Loss: 0.4132486
Validation loss decreased (0.339772 --> 0.339629).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 65.7758584022522
Epoch: 7, Steps: 87 | Train Loss: 0.2479736 Vali Loss: 0.3399545 Test Loss: 0.4133516
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 66.08144235610962
Epoch: 8, Steps: 87 | Train Loss: 0.2478173 Vali Loss: 0.3395669 Test Loss: 0.4133085
Validation loss decreased (0.339629 --> 0.339567).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 72.02967715263367
Epoch: 9, Steps: 87 | Train Loss: 0.2478513 Vali Loss: 0.3395768 Test Loss: 0.4129941
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 65.9068877696991
Epoch: 10, Steps: 87 | Train Loss: 0.2477270 Vali Loss: 0.3399849 Test Loss: 0.4131415
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 73.16140079498291
Epoch: 11, Steps: 87 | Train Loss: 0.2477031 Vali Loss: 0.3393087 Test Loss: 0.4128765
Validation loss decreased (0.339567 --> 0.339309).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 67.07564520835876
Epoch: 12, Steps: 87 | Train Loss: 0.2477588 Vali Loss: 0.3393274 Test Loss: 0.4128324
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 67.45472025871277
Epoch: 13, Steps: 87 | Train Loss: 0.2476786 Vali Loss: 0.3394375 Test Loss: 0.4131604
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 69.54978513717651
Epoch: 14, Steps: 87 | Train Loss: 0.2476381 Vali Loss: 0.3392797 Test Loss: 0.4128010
Validation loss decreased (0.339309 --> 0.339280).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 73.91751742362976
Epoch: 15, Steps: 87 | Train Loss: 0.2476970 Vali Loss: 0.3394198 Test Loss: 0.4131118
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 70.2565906047821
Epoch: 16, Steps: 87 | Train Loss: 0.2475971 Vali Loss: 0.3393231 Test Loss: 0.4129286
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 71.70985674858093
Epoch: 17, Steps: 87 | Train Loss: 0.2475941 Vali Loss: 0.3388745 Test Loss: 0.4126996
Validation loss decreased (0.339280 --> 0.338874).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 66.75694298744202
Epoch: 18, Steps: 87 | Train Loss: 0.2475209 Vali Loss: 0.3394274 Test Loss: 0.4131536
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 68.908438205719
Epoch: 19, Steps: 87 | Train Loss: 0.2475038 Vali Loss: 0.3394223 Test Loss: 0.4126175
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 73.97273683547974
Epoch: 20, Steps: 87 | Train Loss: 0.2474725 Vali Loss: 0.3394804 Test Loss: 0.4131677
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4105226993560791, mae:0.2786667048931122, rse:0.526584267616272, corr:[0.26876566 0.28395155 0.28382042 0.2840256  0.28374624 0.2840962
 0.2840741  0.2839352  0.28417966 0.28366676 0.28392503 0.28404495
 0.28409186 0.28391966 0.28331518 0.2829735  0.28295842 0.28345495
 0.2836319  0.2835944  0.28346193 0.2836257  0.2835943  0.28309146
 0.28455788 0.28474468 0.28454655 0.28477776 0.28456163 0.2845502
 0.28446352 0.28428835 0.28442633 0.28395268 0.28366    0.28384405
 0.28401357 0.2839501  0.2839546  0.2842768  0.28416342 0.28445864
 0.2847349  0.28445795 0.2844224  0.28419346 0.28382123 0.28338882
 0.28359267 0.2839529  0.28413147 0.28424278 0.28439116 0.2845745
 0.2845718  0.28415257 0.28396657 0.2836921  0.28328922 0.2832316
 0.2831356  0.28345135 0.28359652 0.28348726 0.2838739  0.28389314
 0.28417876 0.2842521  0.28387883 0.28419656 0.2845068  0.28458327
 0.28448606 0.28407565 0.2838697  0.28356844 0.2833519  0.2835165
 0.28344697 0.28336227 0.28335193 0.28312913 0.28326905 0.2832363
 0.28302193 0.2832794  0.28333056 0.28356907 0.28421026 0.28415447
 0.28389573 0.28366116 0.283448   0.2837535  0.28371584 0.2834549
 0.28340194 0.28315693 0.28304616 0.28320032 0.28317955 0.28333566
 0.28356674 0.28357476 0.28351986 0.2835155  0.2836134  0.28361532
 0.28354666 0.2836197  0.2833959  0.28328738 0.2837561  0.28391135
 0.28389072 0.28373    0.28340775 0.28342125 0.28337783 0.28346992
 0.28369376 0.28381172 0.28414324 0.28414017 0.28366756 0.283855
 0.28395686 0.283622   0.2836513  0.28369257 0.28365356 0.2839154
 0.28430235 0.28418532 0.28370214 0.28360343 0.28375953 0.2839499
 0.28395796 0.2839452  0.2841239  0.2840072  0.28394943 0.28402093
 0.2839497  0.28384608 0.2841448  0.28436944 0.28413278 0.28414902
 0.2841727  0.28394097 0.28403938 0.28391683 0.28366607 0.28393975
 0.2840929  0.2839686  0.28358713 0.28355828 0.28413704 0.2844745
 0.2848057  0.28480297 0.2845759  0.28466147 0.2842414  0.28369257
 0.2846986  0.28497893 0.28508526 0.28508267 0.28494647 0.28501645
 0.28540808 0.28552744 0.28522548 0.28535274 0.285452   0.28546026
 0.28564268 0.285268   0.28495222 0.28474137 0.2845766  0.28485626
 0.28501332 0.28485596 0.28439125 0.28442675 0.2845643  0.28375095
 0.28420767 0.28439596 0.2841945  0.28468287 0.28519186 0.28542125
 0.2851917  0.28466928 0.28457326 0.28430107 0.28457376 0.28494194
 0.28451726 0.28460637 0.28445822 0.28396904 0.28387937 0.28379962
 0.28409737 0.28426033 0.28387138 0.2837346  0.28343132 0.282888
 0.28302515 0.2832868  0.2833666  0.28335527 0.28366616 0.2841179
 0.2840192  0.28334782 0.28272814 0.2827592  0.2830228  0.28301093
 0.28317657 0.28331873 0.28345877 0.28374967 0.28360802 0.28350684
 0.2838485  0.2840092  0.28407952 0.28386858 0.28378496 0.28384188
 0.28354043 0.28339458 0.283257   0.2834413  0.28395796 0.2841205
 0.2840378  0.2834397  0.2826847  0.28233784 0.282613   0.2829414
 0.28276995 0.28278354 0.2830481  0.28336042 0.2836706  0.2836572
 0.28354326 0.28328592 0.28319895 0.28366417 0.28363603 0.28333068
 0.28332907 0.283325   0.28332812 0.28327313 0.28346518 0.28356442
 0.28330028 0.28296953 0.28252578 0.2824046  0.28253013 0.28279352
 0.28300923 0.28298903 0.28299466 0.2827699  0.28287736 0.28306386
 0.2825427  0.2821382  0.28220582 0.28266832 0.28289768 0.2825674
 0.2825553  0.2830723  0.28354597 0.28366563 0.28306732 0.28301415
 0.28316265 0.2828709  0.28269753 0.2829352  0.28313896 0.28295943
 0.28299174 0.28318757 0.28331894 0.28285968 0.28259245 0.28324944
 0.2831328  0.28271767 0.2828044  0.28290412 0.28276446 0.28229883
 0.28265533 0.28280884 0.28285158 0.28343964 0.28329834 0.2830043
 0.28272676 0.28254774 0.28291363 0.28277683 0.2830305  0.28303555
 0.28304338 0.28358504 0.2830031  0.28245795 0.282176   0.28294033
 0.28259584 0.2820404  0.28253028 0.28211355 0.2835901  0.2832713 ]
