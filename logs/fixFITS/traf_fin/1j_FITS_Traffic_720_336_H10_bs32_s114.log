Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8279613440.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5183504
	speed: 0.6378s/iter; left time: 5517.5712s
Epoch: 1 cost time: 108.64265727996826
Epoch: 1, Steps: 175 | Train Loss: 0.6279354 Vali Loss: 0.4957942 Test Loss: 0.5761248
Validation loss decreased (inf --> 0.495794).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3018279
	speed: 1.7570s/iter; left time: 14892.2935s
Epoch: 2 cost time: 106.16040229797363
Epoch: 2, Steps: 175 | Train Loss: 0.3079612 Vali Loss: 0.3670822 Test Loss: 0.4361879
Validation loss decreased (0.495794 --> 0.367082).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2674870
	speed: 1.7249s/iter; left time: 14318.0341s
Epoch: 3 cost time: 105.97135972976685
Epoch: 3, Steps: 175 | Train Loss: 0.2572687 Vali Loss: 0.3466565 Test Loss: 0.4174319
Validation loss decreased (0.367082 --> 0.346656).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2505256
	speed: 1.7217s/iter; left time: 13990.3791s
Epoch: 4 cost time: 107.51192569732666
Epoch: 4, Steps: 175 | Train Loss: 0.2500375 Vali Loss: 0.3426661 Test Loss: 0.4151914
Validation loss decreased (0.346656 --> 0.342666).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2481294
	speed: 1.7977s/iter; left time: 14293.7008s
Epoch: 5 cost time: 111.73355627059937
Epoch: 5, Steps: 175 | Train Loss: 0.2488808 Vali Loss: 0.3417666 Test Loss: 0.4146785
Validation loss decreased (0.342666 --> 0.341767).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2579808
	speed: 1.7454s/iter; left time: 13571.8707s
Epoch: 6 cost time: 108.90806937217712
Epoch: 6, Steps: 175 | Train Loss: 0.2484901 Vali Loss: 0.3411890 Test Loss: 0.4144338
Validation loss decreased (0.341767 --> 0.341189).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2477874
	speed: 1.7499s/iter; left time: 13301.3567s
Epoch: 7 cost time: 106.07550835609436
Epoch: 7, Steps: 175 | Train Loss: 0.2483263 Vali Loss: 0.3407360 Test Loss: 0.4143751
Validation loss decreased (0.341189 --> 0.340736).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2423423
	speed: 1.6720s/iter; left time: 12416.5007s
Epoch: 8 cost time: 103.07901263237
Epoch: 8, Steps: 175 | Train Loss: 0.2482487 Vali Loss: 0.3399244 Test Loss: 0.4136754
Validation loss decreased (0.340736 --> 0.339924).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2427510
	speed: 1.6732s/iter; left time: 12132.2877s
Epoch: 9 cost time: 98.46639561653137
Epoch: 9, Steps: 175 | Train Loss: 0.2480875 Vali Loss: 0.3400007 Test Loss: 0.4143630
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2419101
	speed: 1.5433s/iter; left time: 10920.3812s
Epoch: 10 cost time: 100.03030180931091
Epoch: 10, Steps: 175 | Train Loss: 0.2480648 Vali Loss: 0.3399558 Test Loss: 0.4136218
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2491984
	speed: 1.6612s/iter; left time: 11463.7964s
Epoch: 11 cost time: 88.86867785453796
Epoch: 11, Steps: 175 | Train Loss: 0.2480053 Vali Loss: 0.3400095 Test Loss: 0.4138741
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.41122567653656006, mae:0.28028297424316406, rse:0.5270348787307739, corr:[0.2705734  0.28278327 0.28179047 0.2828395  0.28390032 0.2837271
 0.28560424 0.28423756 0.285557   0.28454268 0.28409398 0.2842844
 0.2832345  0.2838982  0.28306925 0.28368533 0.28387487 0.28334582
 0.2838572  0.28265664 0.2824841  0.2823166  0.28210443 0.28302252
 0.28452218 0.28550273 0.28500652 0.2850429  0.2854391  0.28439513
 0.28462762 0.28417942 0.2841657  0.28425032 0.283391   0.2835115
 0.28295898 0.283641   0.28443423 0.2844109  0.28479582 0.28447342
 0.28503186 0.2849882  0.2846336  0.28488603 0.2843149  0.2847325
 0.28535795 0.28512627 0.2852078  0.28485715 0.28521335 0.2849489
 0.28432745 0.28407162 0.28369847 0.28438473 0.28451425 0.284425
 0.28407001 0.28363013 0.28463507 0.28447407 0.2841298  0.28421727
 0.28437704 0.2849081  0.28419423 0.28409085 0.28425312 0.2837791
 0.28411108 0.28385016 0.2840829  0.2846595  0.2841839  0.2836264
 0.28284007 0.28310654 0.28351688 0.28321674 0.28373536 0.2839976
 0.28408596 0.28392982 0.28424117 0.2844364  0.2833033  0.28341675
 0.28380826 0.28381318 0.28404623 0.28381523 0.28397784 0.28358966
 0.28288037 0.28304565 0.28352925 0.28443047 0.28414446 0.28359216
 0.28333268 0.2830751  0.28353813 0.28337237 0.2836068  0.28427798
 0.28401107 0.2838275  0.2834496  0.2834625  0.28351298 0.28353158
 0.28428552 0.2839272  0.28370044 0.2835534  0.2831193  0.2833413
 0.28266755 0.28257656 0.28283325 0.28386366 0.2852504  0.28415677
 0.28341872 0.28347492 0.28357184 0.28428558 0.28366318 0.28338796
 0.2836382  0.28367677 0.28437945 0.28436974 0.2841726  0.28406966
 0.28383845 0.2840486  0.28395072 0.28371271 0.28352594 0.28375652
 0.28412274 0.28385058 0.28427586 0.28460672 0.28505868 0.28493017
 0.283659   0.28407237 0.28472668 0.28482184 0.28470242 0.28398812
 0.28415698 0.28426963 0.28447604 0.28423083 0.28340122 0.2840037
 0.28411746 0.2838764  0.28434038 0.28452015 0.28464863 0.28465685
 0.28601855 0.2860001  0.28527126 0.2855568  0.28517994 0.2846535
 0.28427926 0.284106   0.2849286  0.2851871  0.28527334 0.28476742
 0.28445148 0.2850123  0.2844399  0.28458384 0.28500152 0.2849799
 0.28527382 0.28441972 0.28391612 0.2834398  0.28292677 0.28333196
 0.28408584 0.28460133 0.28436399 0.2842785  0.285259   0.28511187
 0.28513744 0.28546515 0.28554127 0.2853156  0.28413275 0.2838825
 0.2839531  0.2840059  0.28431386 0.28429368 0.28455365 0.28411987
 0.2844238  0.28452137 0.28318974 0.2834495  0.28409883 0.28460187
 0.28482908 0.28439498 0.28473908 0.2837988  0.28345868 0.28427434
 0.2840306  0.28401572 0.283668   0.28382075 0.28393623 0.28324997
 0.28350648 0.2830764  0.28307626 0.2832338  0.28266978 0.28331402
 0.28363663 0.28373063 0.28352925 0.28300828 0.2834136  0.28353098
 0.28377905 0.28342998 0.2831656  0.28396994 0.28402758 0.2843896
 0.28391314 0.28298983 0.28356093 0.28376672 0.28400874 0.28337628
 0.28228766 0.28249642 0.28256714 0.28283697 0.28264397 0.2826436
 0.28319046 0.28232276 0.28223702 0.28306368 0.28325006 0.28298512
 0.28237647 0.28254363 0.28283018 0.28382373 0.28408682 0.2832317
 0.283454   0.28328028 0.28347436 0.28337455 0.28276908 0.28296986
 0.28198484 0.2821337  0.28258723 0.2820913  0.28257287 0.28238493
 0.28285524 0.28286582 0.2824714  0.28327686 0.28277355 0.2827362
 0.28213    0.28126556 0.28202078 0.28272265 0.28372622 0.28316212
 0.28236324 0.2829878  0.2824801  0.28294107 0.28276414 0.28201482
 0.28224653 0.2821057  0.28342253 0.28358305 0.28306448 0.2832354
 0.2827787  0.28372562 0.28414837 0.28452304 0.28437573 0.28321734
 0.2836738  0.2828411  0.28259665 0.28262928 0.2825466  0.28274542
 0.2813946  0.28254098 0.2825916  0.28216836 0.28299716 0.2820376
 0.28371406 0.2840555  0.28463683 0.28504536 0.28370866 0.28476116
 0.28223187 0.28321353 0.28142592 0.28074542 0.2814944  0.28708547]
