Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H5', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j336_H5_FITS_custom_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4387511040.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 72.9694504737854
Epoch: 1, Steps: 87 | Train Loss: 1.1081832 Vali Loss: 1.1830826 Test Loss: 1.3732862
Validation loss decreased (inf --> 1.183083).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 69.42810392379761
Epoch: 2, Steps: 87 | Train Loss: 0.8277516 Vali Loss: 1.0307078 Test Loss: 1.1914116
Validation loss decreased (1.183083 --> 1.030708).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 73.7681028842926
Epoch: 3, Steps: 87 | Train Loss: 0.7208235 Vali Loss: 0.9562986 Test Loss: 1.1047215
Validation loss decreased (1.030708 --> 0.956299).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 72.75395369529724
Epoch: 4, Steps: 87 | Train Loss: 0.6521669 Vali Loss: 0.9013519 Test Loss: 1.0412264
Validation loss decreased (0.956299 --> 0.901352).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 76.39722514152527
Epoch: 5, Steps: 87 | Train Loss: 0.5976702 Vali Loss: 0.8549337 Test Loss: 0.9876904
Validation loss decreased (0.901352 --> 0.854934).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 79.5048770904541
Epoch: 6, Steps: 87 | Train Loss: 0.5518760 Vali Loss: 0.8140030 Test Loss: 0.9410774
Validation loss decreased (0.854934 --> 0.814003).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 74.71249461174011
Epoch: 7, Steps: 87 | Train Loss: 0.5126414 Vali Loss: 0.7785726 Test Loss: 0.9002786
Validation loss decreased (0.814003 --> 0.778573).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 74.56068158149719
Epoch: 8, Steps: 87 | Train Loss: 0.4785171 Vali Loss: 0.7448906 Test Loss: 0.8621485
Validation loss decreased (0.778573 --> 0.744891).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 71.3455011844635
Epoch: 9, Steps: 87 | Train Loss: 0.4486767 Vali Loss: 0.7164157 Test Loss: 0.8300264
Validation loss decreased (0.744891 --> 0.716416).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 73.17857551574707
Epoch: 10, Steps: 87 | Train Loss: 0.4223798 Vali Loss: 0.6916252 Test Loss: 0.8014719
Validation loss decreased (0.716416 --> 0.691625).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 73.77829813957214
Epoch: 11, Steps: 87 | Train Loss: 0.3989637 Vali Loss: 0.6667538 Test Loss: 0.7735394
Validation loss decreased (0.691625 --> 0.666754).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 76.09162211418152
Epoch: 12, Steps: 87 | Train Loss: 0.3781493 Vali Loss: 0.6458585 Test Loss: 0.7491235
Validation loss decreased (0.666754 --> 0.645858).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 71.79988837242126
Epoch: 13, Steps: 87 | Train Loss: 0.3595505 Vali Loss: 0.6261529 Test Loss: 0.7269900
Validation loss decreased (0.645858 --> 0.626153).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 66.51093316078186
Epoch: 14, Steps: 87 | Train Loss: 0.3428373 Vali Loss: 0.6091918 Test Loss: 0.7075467
Validation loss decreased (0.626153 --> 0.609192).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 69.87458157539368
Epoch: 15, Steps: 87 | Train Loss: 0.3277069 Vali Loss: 0.5931814 Test Loss: 0.6889093
Validation loss decreased (0.609192 --> 0.593181).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 66.47538042068481
Epoch: 16, Steps: 87 | Train Loss: 0.3141209 Vali Loss: 0.5794719 Test Loss: 0.6733946
Validation loss decreased (0.593181 --> 0.579472).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 69.39246773719788
Epoch: 17, Steps: 87 | Train Loss: 0.3017547 Vali Loss: 0.5650389 Test Loss: 0.6572025
Validation loss decreased (0.579472 --> 0.565039).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 67.92272710800171
Epoch: 18, Steps: 87 | Train Loss: 0.2905123 Vali Loss: 0.5546870 Test Loss: 0.6453251
Validation loss decreased (0.565039 --> 0.554687).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 65.35549449920654
Epoch: 19, Steps: 87 | Train Loss: 0.2802925 Vali Loss: 0.5431457 Test Loss: 0.6326926
Validation loss decreased (0.554687 --> 0.543146).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 69.85048985481262
Epoch: 20, Steps: 87 | Train Loss: 0.2709503 Vali Loss: 0.5331382 Test Loss: 0.6210189
Validation loss decreased (0.543146 --> 0.533138).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 69.06373357772827
Epoch: 21, Steps: 87 | Train Loss: 0.2623478 Vali Loss: 0.5240268 Test Loss: 0.6109095
Validation loss decreased (0.533138 --> 0.524027).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 72.99668169021606
Epoch: 22, Steps: 87 | Train Loss: 0.2544827 Vali Loss: 0.5152626 Test Loss: 0.6005129
Validation loss decreased (0.524027 --> 0.515263).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 77.59299397468567
Epoch: 23, Steps: 87 | Train Loss: 0.2472724 Vali Loss: 0.5079952 Test Loss: 0.5918656
Validation loss decreased (0.515263 --> 0.507995).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 69.3975396156311
Epoch: 24, Steps: 87 | Train Loss: 0.2406281 Vali Loss: 0.5007631 Test Loss: 0.5838441
Validation loss decreased (0.507995 --> 0.500763).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 71.35121631622314
Epoch: 25, Steps: 87 | Train Loss: 0.2345323 Vali Loss: 0.4933459 Test Loss: 0.5760929
Validation loss decreased (0.500763 --> 0.493346).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 71.2278778553009
Epoch: 26, Steps: 87 | Train Loss: 0.2288595 Vali Loss: 0.4868611 Test Loss: 0.5683185
Validation loss decreased (0.493346 --> 0.486861).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 75.00392699241638
Epoch: 27, Steps: 87 | Train Loss: 0.2236428 Vali Loss: 0.4809123 Test Loss: 0.5619490
Validation loss decreased (0.486861 --> 0.480912).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 80.16974687576294
Epoch: 28, Steps: 87 | Train Loss: 0.2188137 Vali Loss: 0.4759595 Test Loss: 0.5562147
Validation loss decreased (0.480912 --> 0.475960).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 77.83278775215149
Epoch: 29, Steps: 87 | Train Loss: 0.2143144 Vali Loss: 0.4714242 Test Loss: 0.5505001
Validation loss decreased (0.475960 --> 0.471424).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 77.28948616981506
Epoch: 30, Steps: 87 | Train Loss: 0.2101265 Vali Loss: 0.4669546 Test Loss: 0.5456222
Validation loss decreased (0.471424 --> 0.466955).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 75.59875583648682
Epoch: 31, Steps: 87 | Train Loss: 0.2062687 Vali Loss: 0.4625537 Test Loss: 0.5412039
Validation loss decreased (0.466955 --> 0.462554).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 77.82177472114563
Epoch: 32, Steps: 87 | Train Loss: 0.2026422 Vali Loss: 0.4582868 Test Loss: 0.5362409
Validation loss decreased (0.462554 --> 0.458287).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 72.07215762138367
Epoch: 33, Steps: 87 | Train Loss: 0.1992589 Vali Loss: 0.4543375 Test Loss: 0.5318431
Validation loss decreased (0.458287 --> 0.454337).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 81.66450881958008
Epoch: 34, Steps: 87 | Train Loss: 0.1960978 Vali Loss: 0.4513272 Test Loss: 0.5279655
Validation loss decreased (0.454337 --> 0.451327).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 74.0214991569519
Epoch: 35, Steps: 87 | Train Loss: 0.1931369 Vali Loss: 0.4472103 Test Loss: 0.5242693
Validation loss decreased (0.451327 --> 0.447210).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 78.0579354763031
Epoch: 36, Steps: 87 | Train Loss: 0.1904165 Vali Loss: 0.4445880 Test Loss: 0.5207905
Validation loss decreased (0.447210 --> 0.444588).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 75.38827300071716
Epoch: 37, Steps: 87 | Train Loss: 0.1878670 Vali Loss: 0.4412280 Test Loss: 0.5177917
Validation loss decreased (0.444588 --> 0.441228).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 78.71930694580078
Epoch: 38, Steps: 87 | Train Loss: 0.1854557 Vali Loss: 0.4383762 Test Loss: 0.5144594
Validation loss decreased (0.441228 --> 0.438376).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 80.34171938896179
Epoch: 39, Steps: 87 | Train Loss: 0.1831859 Vali Loss: 0.4360410 Test Loss: 0.5115680
Validation loss decreased (0.438376 --> 0.436041).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 75.75038766860962
Epoch: 40, Steps: 87 | Train Loss: 0.1810359 Vali Loss: 0.4337087 Test Loss: 0.5091486
Validation loss decreased (0.436041 --> 0.433709).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 67.65242457389832
Epoch: 41, Steps: 87 | Train Loss: 0.1790720 Vali Loss: 0.4320005 Test Loss: 0.5067083
Validation loss decreased (0.433709 --> 0.432001).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 71.89804744720459
Epoch: 42, Steps: 87 | Train Loss: 0.1771754 Vali Loss: 0.4294138 Test Loss: 0.5041988
Validation loss decreased (0.432001 --> 0.429414).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 78.1075005531311
Epoch: 43, Steps: 87 | Train Loss: 0.1754215 Vali Loss: 0.4270650 Test Loss: 0.5018713
Validation loss decreased (0.429414 --> 0.427065).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 79.6349368095398
Epoch: 44, Steps: 87 | Train Loss: 0.1737835 Vali Loss: 0.4259751 Test Loss: 0.5000613
Validation loss decreased (0.427065 --> 0.425975).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 74.99854683876038
Epoch: 45, Steps: 87 | Train Loss: 0.1721814 Vali Loss: 0.4239028 Test Loss: 0.4979288
Validation loss decreased (0.425975 --> 0.423903).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 74.53365063667297
Epoch: 46, Steps: 87 | Train Loss: 0.1707135 Vali Loss: 0.4221913 Test Loss: 0.4962589
Validation loss decreased (0.423903 --> 0.422191).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 67.3988766670227
Epoch: 47, Steps: 87 | Train Loss: 0.1693286 Vali Loss: 0.4203987 Test Loss: 0.4941444
Validation loss decreased (0.422191 --> 0.420399).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 72.30962133407593
Epoch: 48, Steps: 87 | Train Loss: 0.1679872 Vali Loss: 0.4192024 Test Loss: 0.4926164
Validation loss decreased (0.420399 --> 0.419202).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 78.7858784198761
Epoch: 49, Steps: 87 | Train Loss: 0.1667777 Vali Loss: 0.4175529 Test Loss: 0.4911398
Validation loss decreased (0.419202 --> 0.417553).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 80.1018431186676
Epoch: 50, Steps: 87 | Train Loss: 0.1656145 Vali Loss: 0.4162190 Test Loss: 0.4897269
Validation loss decreased (0.417553 --> 0.416219).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4387511040.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 78.832937002182
Epoch: 1, Steps: 87 | Train Loss: 0.2738319 Vali Loss: 0.3539342 Test Loss: 0.4267935
Validation loss decreased (inf --> 0.353934).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 78.85316514968872
Epoch: 2, Steps: 87 | Train Loss: 0.2551497 Vali Loss: 0.3489382 Test Loss: 0.4234442
Validation loss decreased (0.353934 --> 0.348938).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 75.28674268722534
Epoch: 3, Steps: 87 | Train Loss: 0.2542281 Vali Loss: 0.3480844 Test Loss: 0.4230601
Validation loss decreased (0.348938 --> 0.348084).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 84.37521529197693
Epoch: 4, Steps: 87 | Train Loss: 0.2541564 Vali Loss: 0.3473426 Test Loss: 0.4228618
Validation loss decreased (0.348084 --> 0.347343).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 81.15631890296936
Epoch: 5, Steps: 87 | Train Loss: 0.2539691 Vali Loss: 0.3478689 Test Loss: 0.4233254
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 79.01007986068726
Epoch: 6, Steps: 87 | Train Loss: 0.2539573 Vali Loss: 0.3478675 Test Loss: 0.4225374
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 80.33982634544373
Epoch: 7, Steps: 87 | Train Loss: 0.2538554 Vali Loss: 0.3479226 Test Loss: 0.4226576
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j336_H5_FITS_custom_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.42077144980430603, mae:0.2930110692977905, rse:0.53311687707901, corr:[0.28393465 0.28716892 0.2865973  0.28744164 0.2881892  0.2880146
 0.28763586 0.28776392 0.28817585 0.28818282 0.28762695 0.28710946
 0.28715816 0.28746897 0.28744635 0.28697217 0.28654772 0.28657627
 0.2868685  0.28698516 0.2867727  0.28654316 0.28680232 0.28795537
 0.28930962 0.2890984  0.2881437  0.28747168 0.28734615 0.28754845
 0.2877234  0.28768614 0.28764743 0.28784296 0.28809693 0.2881416
 0.28797933 0.28782374 0.28793433 0.28827003 0.28851253 0.2883776
 0.28798845 0.2876148  0.2875504  0.28766873 0.28771344 0.2878736
 0.28822586 0.28830278 0.2882849  0.2880792  0.2876329  0.28727818
 0.2873549  0.28776044 0.28806686 0.28804424 0.28783914 0.28779027
 0.2880388  0.2882895  0.2883287  0.28823337 0.28812593 0.28804436
 0.28807184 0.28788292 0.28755337 0.28731397 0.28723648 0.28730956
 0.28725487 0.2870843  0.28704977 0.2871563  0.28728247 0.2872751
 0.28706527 0.2867327  0.2864897  0.2865053  0.28672945 0.28694615
 0.28701013 0.286921   0.2868633  0.2869389  0.28709808 0.2872603
 0.28747168 0.28768975 0.2879302  0.28804755 0.2878495  0.28742236
 0.2869112  0.28677866 0.287018   0.28719616 0.28716487 0.28705472
 0.28698313 0.286957   0.28695858 0.28699318 0.28705323 0.28705657
 0.28694466 0.28672093 0.28647906 0.28635868 0.28642002 0.2866475
 0.28697202 0.28723314 0.28737047 0.28733763 0.287346   0.28745985
 0.28757775 0.2877121  0.2877717  0.28754702 0.28720006 0.2869191
 0.28676197 0.28669763 0.28668985 0.28669146 0.28665996 0.28661177
 0.28659195 0.28665194 0.2868153  0.28700277 0.2870938  0.28703293
 0.28694952 0.28699252 0.28716335 0.2873111  0.2872471  0.28710234
 0.28713122 0.28735554 0.2877641  0.28805062 0.28801537 0.28780296
 0.28769594 0.28775668 0.28769982 0.28730375 0.28678256 0.2866172
 0.28698012 0.28750393 0.28769082 0.2874589  0.2871619  0.2870623
 0.28715882 0.28730172 0.2874409  0.28762192 0.28787094 0.28853795
 0.28955466 0.28958824 0.2892242  0.28888193 0.28856555 0.28841323
 0.28850192 0.28872162 0.28887063 0.2888384  0.28865832 0.28850123
 0.28841543 0.2882087  0.287871   0.2875526  0.28743643 0.28753844
 0.28763384 0.28760776 0.28742987 0.28718743 0.28718987 0.28789136
 0.28897545 0.28928393 0.28909168 0.2887695  0.28847238 0.2883325
 0.2883411  0.28834885 0.28823423 0.2880025  0.28771242 0.2875154
 0.28743094 0.28727645 0.28705406 0.2869349  0.2870313  0.28718364
 0.28707317 0.2866562  0.286274   0.28631467 0.28682372 0.28757346
 0.28800794 0.28765792 0.28702244 0.28661242 0.2865607  0.28672597
 0.2868968  0.28700066 0.28712177 0.2873172  0.28747314 0.28749317
 0.28742242 0.287299   0.28721896 0.28721875 0.287239   0.28717068
 0.28696826 0.28673115 0.2866727  0.28665516 0.28663382 0.28658667
 0.28650486 0.28650615 0.28667602 0.28686392 0.28700596 0.2871258
 0.28721085 0.2872204  0.2871452  0.28701326 0.28686324 0.2867195
 0.2865811  0.28638425 0.2861457  0.2859704  0.28595683 0.28610963
 0.28625265 0.28627542 0.28621998 0.28611496 0.285897   0.28550047
 0.2849454  0.28470007 0.28503013 0.28563222 0.2861014  0.2861796
 0.2859165  0.28567818 0.28575355 0.28603926 0.28618193 0.285964
 0.28559935 0.28543088 0.28553322 0.2856243  0.2854544  0.28511882
 0.2849045  0.28498793 0.28520843 0.2852873  0.28522727 0.28522748
 0.28542373 0.28590146 0.2863471  0.28640252 0.2861831  0.28593954
 0.2857373  0.2855011  0.2852768  0.2852903  0.28563905 0.28605935
 0.2861493  0.28582984 0.2854921  0.28556496 0.28602457 0.2863996
 0.28638428 0.28618112 0.2862078  0.28646967 0.28655854 0.28621212
 0.2856742  0.28541952 0.2858432  0.2865687  0.2869106  0.28660938
 0.2860275  0.28570032 0.28573212 0.28575674 0.28550795 0.28524485
 0.28542453 0.28604275 0.2865224  0.286401   0.28600335 0.28601107
 0.28647307 0.28657633 0.28567886 0.28457093 0.2852342  0.28842795]
