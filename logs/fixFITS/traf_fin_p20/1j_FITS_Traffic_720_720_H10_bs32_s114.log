Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11298406400.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7045551
	speed: 0.5639s/iter; left time: 4709.1486s
Epoch: 1 cost time: 93.97315883636475
Epoch: 1, Steps: 169 | Train Loss: 0.8291066 Vali Loss: 0.7311894 Test Loss: 0.8523243
Validation loss decreased (inf --> 0.731189).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4479605
	speed: 1.7831s/iter; left time: 14589.1385s
Epoch: 2 cost time: 116.19982695579529
Epoch: 2, Steps: 169 | Train Loss: 0.4759238 Vali Loss: 0.5284963 Test Loss: 0.6131829
Validation loss decreased (0.731189 --> 0.528496).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3440392
	speed: 1.6585s/iter; left time: 13289.8783s
Epoch: 3 cost time: 97.0899806022644
Epoch: 3, Steps: 169 | Train Loss: 0.3543075 Vali Loss: 0.4422483 Test Loss: 0.5134428
Validation loss decreased (0.528496 --> 0.442248).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2940448
	speed: 1.5984s/iter; left time: 12537.8283s
Epoch: 4 cost time: 97.78358888626099
Epoch: 4, Steps: 169 | Train Loss: 0.3030971 Vali Loss: 0.4072965 Test Loss: 0.4717836
Validation loss decreased (0.442248 --> 0.407296).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2840138
	speed: 1.6585s/iter; left time: 12729.1324s
Epoch: 5 cost time: 110.3174774646759
Epoch: 5, Steps: 169 | Train Loss: 0.2827552 Vali Loss: 0.3943235 Test Loss: 0.4578343
Validation loss decreased (0.407296 --> 0.394324).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2671305
	speed: 1.8590s/iter; left time: 13953.9743s
Epoch: 6 cost time: 107.93983912467957
Epoch: 6, Steps: 169 | Train Loss: 0.2751674 Vali Loss: 0.3892765 Test Loss: 0.4531910
Validation loss decreased (0.394324 --> 0.389277).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2721292
	speed: 1.6580s/iter; left time: 12164.5388s
Epoch: 7 cost time: 102.00686073303223
Epoch: 7, Steps: 169 | Train Loss: 0.2724319 Vali Loss: 0.3871353 Test Loss: 0.4510687
Validation loss decreased (0.389277 --> 0.387135).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2748135
	speed: 1.7695s/iter; left time: 12683.6066s
Epoch: 8 cost time: 106.74735260009766
Epoch: 8, Steps: 169 | Train Loss: 0.2714354 Vali Loss: 0.3867238 Test Loss: 0.4507032
Validation loss decreased (0.387135 --> 0.386724).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2720343
	speed: 1.8183s/iter; left time: 12726.5531s
Epoch: 9 cost time: 115.34288048744202
Epoch: 9, Steps: 169 | Train Loss: 0.2710183 Vali Loss: 0.3866754 Test Loss: 0.4498680
Validation loss decreased (0.386724 --> 0.386675).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2700196
	speed: 1.8730s/iter; left time: 12792.5310s
Epoch: 10 cost time: 110.29907035827637
Epoch: 10, Steps: 169 | Train Loss: 0.2707849 Vali Loss: 0.3865164 Test Loss: 0.4504977
Validation loss decreased (0.386675 --> 0.386516).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2587762
	speed: 1.6647s/iter; left time: 11088.3601s
Epoch: 11 cost time: 100.61843633651733
Epoch: 11, Steps: 169 | Train Loss: 0.2706656 Vali Loss: 0.3864623 Test Loss: 0.4506707
Validation loss decreased (0.386516 --> 0.386462).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2788401
	speed: 1.7345s/iter; left time: 11260.3465s
Epoch: 12 cost time: 109.84810972213745
Epoch: 12, Steps: 169 | Train Loss: 0.2705553 Vali Loss: 0.3863026 Test Loss: 0.4494655
Validation loss decreased (0.386462 --> 0.386303).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2665837
	speed: 1.8056s/iter; left time: 11416.9298s
Epoch: 13 cost time: 109.71169376373291
Epoch: 13, Steps: 169 | Train Loss: 0.2704393 Vali Loss: 0.3851461 Test Loss: 0.4496147
Validation loss decreased (0.386303 --> 0.385146).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2777795
	speed: 1.8313s/iter; left time: 11270.0174s
Epoch: 14 cost time: 97.73912048339844
Epoch: 14, Steps: 169 | Train Loss: 0.2703990 Vali Loss: 0.3854006 Test Loss: 0.4497431
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2765620
	speed: 1.5313s/iter; left time: 9165.0543s
Epoch: 15 cost time: 104.52221727371216
Epoch: 15, Steps: 169 | Train Loss: 0.2703872 Vali Loss: 0.3856692 Test Loss: 0.4494192
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2718492
	speed: 1.8723s/iter; left time: 10889.4713s
Epoch: 16 cost time: 115.02690482139587
Epoch: 16, Steps: 169 | Train Loss: 0.2702760 Vali Loss: 0.3851053 Test Loss: 0.4499353
Validation loss decreased (0.385146 --> 0.385105).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2643470
	speed: 1.7591s/iter; left time: 9933.6497s
Epoch: 17 cost time: 113.54718399047852
Epoch: 17, Steps: 169 | Train Loss: 0.2702371 Vali Loss: 0.3857076 Test Loss: 0.4493143
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2732089
	speed: 1.8895s/iter; left time: 10350.7161s
Epoch: 18 cost time: 114.36752414703369
Epoch: 18, Steps: 169 | Train Loss: 0.2701810 Vali Loss: 0.3853480 Test Loss: 0.4500044
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2717248
	speed: 1.9187s/iter; left time: 10186.3340s
Epoch: 19 cost time: 115.84211540222168
Epoch: 19, Steps: 169 | Train Loss: 0.2701884 Vali Loss: 0.3852333 Test Loss: 0.4491002
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2676351
	speed: 1.7502s/iter; left time: 8995.8247s
Epoch: 20 cost time: 106.67347359657288
Epoch: 20, Steps: 169 | Train Loss: 0.2701425 Vali Loss: 0.3854635 Test Loss: 0.4497721
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2655399
	speed: 1.9093s/iter; left time: 9491.3264s
Epoch: 21 cost time: 116.31132507324219
Epoch: 21, Steps: 169 | Train Loss: 0.2701245 Vali Loss: 0.3852698 Test Loss: 0.4497112
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2672794
	speed: 1.8720s/iter; left time: 8989.1699s
Epoch: 22 cost time: 115.47964787483215
Epoch: 22, Steps: 169 | Train Loss: 0.2700454 Vali Loss: 0.3858472 Test Loss: 0.4500047
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2742574
	speed: 1.7530s/iter; left time: 8121.7837s
Epoch: 23 cost time: 106.7562005519867
Epoch: 23, Steps: 169 | Train Loss: 0.2700055 Vali Loss: 0.3853144 Test Loss: 0.4497982
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2768877
	speed: 1.7522s/iter; left time: 7821.9065s
Epoch: 24 cost time: 118.72585558891296
Epoch: 24, Steps: 169 | Train Loss: 0.2700238 Vali Loss: 0.3844303 Test Loss: 0.4495100
Validation loss decreased (0.385105 --> 0.384430).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2716439
	speed: 2.0145s/iter; left time: 8652.4006s
Epoch: 25 cost time: 110.34408664703369
Epoch: 25, Steps: 169 | Train Loss: 0.2699641 Vali Loss: 0.3851027 Test Loss: 0.4501103
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2715019
	speed: 1.7186s/iter; left time: 7090.8976s
Epoch: 26 cost time: 103.4329981803894
Epoch: 26, Steps: 169 | Train Loss: 0.2699527 Vali Loss: 0.3851960 Test Loss: 0.4498682
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2841609
	speed: 1.7826s/iter; left time: 7053.8680s
Epoch: 27 cost time: 116.1684353351593
Epoch: 27, Steps: 169 | Train Loss: 0.2699086 Vali Loss: 0.3849730 Test Loss: 0.4496115
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2673766
	speed: 1.8748s/iter; left time: 7101.7422s
Epoch: 28 cost time: 119.57764053344727
Epoch: 28, Steps: 169 | Train Loss: 0.2699029 Vali Loss: 0.3849995 Test Loss: 0.4497638
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2734873
	speed: 1.8579s/iter; left time: 6723.8794s
Epoch: 29 cost time: 106.2799391746521
Epoch: 29, Steps: 169 | Train Loss: 0.2698620 Vali Loss: 0.3850398 Test Loss: 0.4492058
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2699429
	speed: 1.8359s/iter; left time: 6333.9114s
Epoch: 30 cost time: 119.71277570724487
Epoch: 30, Steps: 169 | Train Loss: 0.2698389 Vali Loss: 0.3849572 Test Loss: 0.4493704
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2697479
	speed: 1.8348s/iter; left time: 6019.9304s
Epoch: 31 cost time: 117.15736818313599
Epoch: 31, Steps: 169 | Train Loss: 0.2698271 Vali Loss: 0.3854647 Test Loss: 0.4495990
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2657213
	speed: 1.9018s/iter; left time: 5918.4978s
Epoch: 32 cost time: 107.92211484909058
Epoch: 32, Steps: 169 | Train Loss: 0.2698348 Vali Loss: 0.3849631 Test Loss: 0.4493512
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2669871
	speed: 1.7601s/iter; left time: 5180.0714s
Epoch: 33 cost time: 115.57608771324158
Epoch: 33, Steps: 169 | Train Loss: 0.2698436 Vali Loss: 0.3852559 Test Loss: 0.4492663
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2688866
	speed: 1.8266s/iter; left time: 5067.0684s
Epoch: 34 cost time: 111.4179675579071
Epoch: 34, Steps: 169 | Train Loss: 0.2698225 Vali Loss: 0.3856498 Test Loss: 0.4496833
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2709217
	speed: 1.7254s/iter; left time: 4494.5745s
Epoch: 35 cost time: 92.1594033241272
Epoch: 35, Steps: 169 | Train Loss: 0.2697180 Vali Loss: 0.3847191 Test Loss: 0.4496028
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2673407
	speed: 1.5497s/iter; left time: 3775.0492s
Epoch: 36 cost time: 88.93717455863953
Epoch: 36, Steps: 169 | Train Loss: 0.2697512 Vali Loss: 0.3846596 Test Loss: 0.4492016
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2881170
	speed: 1.4758s/iter; left time: 3345.6108s
Epoch: 37 cost time: 87.26077461242676
Epoch: 37, Steps: 169 | Train Loss: 0.2697500 Vali Loss: 0.3852335 Test Loss: 0.4496351
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2642430
	speed: 1.4093s/iter; left time: 2956.7910s
Epoch: 38 cost time: 85.41720008850098
Epoch: 38, Steps: 169 | Train Loss: 0.2697468 Vali Loss: 0.3844740 Test Loss: 0.4492615
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2690754
	speed: 1.4130s/iter; left time: 2725.7059s
Epoch: 39 cost time: 86.57793974876404
Epoch: 39, Steps: 169 | Train Loss: 0.2696778 Vali Loss: 0.3849850 Test Loss: 0.4495174
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2696907
	speed: 1.4475s/iter; left time: 2547.6687s
Epoch: 40 cost time: 89.78677129745483
Epoch: 40, Steps: 169 | Train Loss: 0.2696922 Vali Loss: 0.3843377 Test Loss: 0.4495215
Validation loss decreased (0.384430 --> 0.384338).  Saving model ...
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2787073
	speed: 1.4982s/iter; left time: 2383.5732s
Epoch: 41 cost time: 90.09959697723389
Epoch: 41, Steps: 169 | Train Loss: 0.2697219 Vali Loss: 0.3851852 Test Loss: 0.4492241
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2827543
	speed: 1.4986s/iter; left time: 2131.0433s
Epoch: 42 cost time: 94.12022995948792
Epoch: 42, Steps: 169 | Train Loss: 0.2696783 Vali Loss: 0.3843636 Test Loss: 0.4489891
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2747006
	speed: 1.5221s/iter; left time: 1907.1562s
Epoch: 43 cost time: 90.92343139648438
Epoch: 43, Steps: 169 | Train Loss: 0.2696612 Vali Loss: 0.3851430 Test Loss: 0.4490342
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2775558
	speed: 1.5882s/iter; left time: 1721.5934s
Epoch: 44 cost time: 104.71738147735596
Epoch: 44, Steps: 169 | Train Loss: 0.2696495 Vali Loss: 0.3844261 Test Loss: 0.4492191
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2698359
	speed: 1.7221s/iter; left time: 1575.7219s
Epoch: 45 cost time: 107.50341010093689
Epoch: 45, Steps: 169 | Train Loss: 0.2696120 Vali Loss: 0.3846888 Test Loss: 0.4491436
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2665194
	speed: 1.7587s/iter; left time: 1311.9553s
Epoch: 46 cost time: 102.96962833404541
Epoch: 46, Steps: 169 | Train Loss: 0.2696221 Vali Loss: 0.3850268 Test Loss: 0.4491991
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2714317
	speed: 1.6904s/iter; left time: 975.3663s
Epoch: 47 cost time: 110.64402651786804
Epoch: 47, Steps: 169 | Train Loss: 0.2696329 Vali Loss: 0.3849530 Test Loss: 0.4492734
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2639114
	speed: 1.6399s/iter; left time: 669.0801s
Epoch: 48 cost time: 97.66452383995056
Epoch: 48, Steps: 169 | Train Loss: 0.2695756 Vali Loss: 0.3850452 Test Loss: 0.4493317
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2683009
	speed: 1.5139s/iter; left time: 361.8317s
Epoch: 49 cost time: 91.00294494628906
Epoch: 49, Steps: 169 | Train Loss: 0.2695884 Vali Loss: 0.3847834 Test Loss: 0.4491322
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2765169
	speed: 1.4628s/iter; left time: 102.3952s
Epoch: 50 cost time: 89.94974279403687
Epoch: 50, Steps: 169 | Train Loss: 0.2695609 Vali Loss: 0.3845060 Test Loss: 0.4492341
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.0497355408796396e-05
>>>>>>>testing : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4490029513835907, mae:0.2993033230304718, rse:0.5479304194450378, corr:[0.25441548 0.2670181  0.26392952 0.26659924 0.26690564 0.26826355
 0.2689986  0.2685875  0.26938763 0.26850745 0.26864654 0.26769254
 0.26724657 0.26674795 0.26635092 0.26699257 0.2665507  0.2669499
 0.26707217 0.26715308 0.267509   0.2675809  0.26786098 0.26764983
 0.26922607 0.26957747 0.26934654 0.26951915 0.2689659  0.26896042
 0.26871872 0.2682171  0.26827833 0.2677696  0.26777306 0.26751518
 0.26726246 0.26739606 0.26730332 0.26780817 0.26786545 0.26795092
 0.26818514 0.26824448 0.2682992  0.2681796  0.2684265  0.26829067
 0.26873145 0.26892415 0.2682054  0.26826215 0.26831788 0.26773518
 0.2675783  0.26722538 0.26726344 0.26747674 0.26726562 0.26730326
 0.26732314 0.2673894  0.26754558 0.26761946 0.2676523  0.2678133
 0.26802593 0.26782796 0.26792306 0.26773718 0.2672767  0.26766747
 0.26758778 0.2673876  0.2673445  0.26692984 0.26687944 0.26676923
 0.26670483 0.26661626 0.26659548 0.26691124 0.26684418 0.26692384
 0.2670098  0.26699835 0.26726204 0.26720774 0.26746675 0.2677865
 0.26782995 0.26791424 0.26784855 0.2675467  0.26721162 0.26721632
 0.2669724  0.26674712 0.2667197  0.26665205 0.26681837 0.26693332
 0.2668754  0.26677108 0.2664005  0.26649946 0.2668063  0.26686886
 0.26684788 0.26679105 0.26709056 0.26732272 0.2672384  0.26722267
 0.2674108  0.26752198 0.2674055  0.26715967 0.26718423 0.2671015
 0.2669274  0.267024   0.26727355 0.26727015 0.2669965  0.2670843
 0.2669098  0.26658174 0.26664317 0.26700088 0.26736003 0.26729774
 0.26736847 0.26766205 0.26779315 0.26794827 0.2680901  0.2679488
 0.26807046 0.2678737  0.26764068 0.26736137 0.26739073 0.26751956
 0.2676173  0.26767415 0.26750788 0.26735324 0.26733798 0.26740456
 0.26768962 0.26770914 0.26755604 0.26768225 0.26770422 0.26759416
 0.2676584  0.2679001  0.26793444 0.26799512 0.26802254 0.26793522
 0.26817232 0.2682046  0.2681322  0.26789856 0.26783007 0.2684569
 0.2693719  0.26921064 0.26932806 0.26894554 0.26865003 0.26879245
 0.26892135 0.2689986  0.26892024 0.2690737  0.26942214 0.26941863
 0.269299   0.26965272 0.27000406 0.26995564 0.2700597  0.26991665
 0.26949003 0.2693353  0.26919678 0.269294   0.269364   0.26902735
 0.269515   0.26958132 0.26950282 0.26933244 0.2691723  0.2693098
 0.26939029 0.2692707  0.26930672 0.2696446  0.26957887 0.26917955
 0.26938716 0.2696029  0.2696786  0.26980144 0.26977852 0.26977235
 0.2695069  0.2696017  0.2696292  0.26906142 0.2689873  0.2688133
 0.26866877 0.26886863 0.26878017 0.26873118 0.26874962 0.26890612
 0.26925725 0.26915824 0.26922348 0.26924294 0.2690076  0.2689651
 0.26884338 0.2688702  0.26885813 0.26871184 0.26878402 0.26880953
 0.26880717 0.26884001 0.26901692 0.26882666 0.26842386 0.26857892
 0.2688251  0.2689214  0.26863858 0.2682371  0.2684106  0.26862976
 0.268635   0.26869935 0.26881966 0.2689025  0.2689952  0.26921552
 0.2690417  0.26894993 0.26916894 0.26897287 0.26887682 0.2688504
 0.26874015 0.26883572 0.26905298 0.26920584 0.26898614 0.26880565
 0.26868063 0.2687149  0.2687571  0.26863682 0.26858735 0.26866117
 0.26875195 0.26890647 0.26887625 0.26880488 0.26865512 0.26870495
 0.2689886  0.2690297  0.26888752 0.268833   0.26881242 0.2684918
 0.2684352  0.2687421  0.2684925  0.26825872 0.26861757 0.26839882
 0.26825428 0.26874915 0.26883608 0.26889145 0.26871198 0.26876307
 0.2690976  0.26909503 0.26892582 0.26881713 0.2688282  0.2688801
 0.26908457 0.26925093 0.26931688 0.269405   0.2692677  0.26914263
 0.26922676 0.26899287 0.26902777 0.2691594  0.26925114 0.2691598
 0.26910305 0.26944703 0.26956627 0.26923725 0.2690685  0.26909634
 0.26918817 0.26912728 0.26943696 0.26982966 0.26982546 0.26992086
 0.26980016 0.2699272  0.27013537 0.26997298 0.2700303  0.26979312
 0.26962024 0.2696811  0.26967293 0.26964173 0.2694221  0.2694729
 0.27042815 0.2706437  0.2707475  0.270531   0.27031305 0.27037293
 0.27037808 0.27032626 0.27017352 0.27033913 0.27062386 0.27039376
 0.27021223 0.27034083 0.27050993 0.27056664 0.27052462 0.2705672
 0.2705582  0.27055618 0.2704892  0.27030888 0.27022207 0.2703661
 0.27095982 0.27081317 0.27064776 0.27080017 0.27066025 0.2706187
 0.27084988 0.27078456 0.270467   0.27041352 0.27057233 0.2704636
 0.27052304 0.27052325 0.27037156 0.2705774  0.27043825 0.2704674
 0.2705792  0.27035424 0.2704003  0.2700319  0.26978207 0.27007294
 0.27016118 0.27021194 0.27040946 0.27051142 0.2704486  0.2701946
 0.2703109  0.2704038  0.27021286 0.2702989  0.27047345 0.27056012
 0.27045837 0.2703003  0.2703775  0.27022737 0.26993895 0.26990056
 0.27007243 0.26994663 0.26953056 0.2693925  0.269287   0.26939562
 0.26953048 0.26958558 0.26965386 0.26952943 0.26956442 0.26958975
 0.269402   0.26936066 0.26954022 0.26975068 0.26965132 0.2696927
 0.26991066 0.26969674 0.26948312 0.2694206  0.26929334 0.26922733
 0.26932278 0.26942015 0.26915625 0.26895726 0.26909128 0.26901123
 0.2689835  0.26935717 0.26936886 0.26932147 0.26945585 0.2695466
 0.2695963  0.2696747  0.26953518 0.26964357 0.2697954  0.2695738
 0.26961935 0.2695599  0.2693623  0.26956597 0.26942053 0.269088
 0.2693339  0.26917478 0.2687235  0.26876107 0.26897618 0.26882586
 0.26870105 0.26906836 0.26931933 0.26952076 0.26947573 0.26956886
 0.26984024 0.26954556 0.26938248 0.26956627 0.2694562  0.26922825
 0.26943725 0.26983643 0.26977903 0.26961103 0.26954728 0.26965922
 0.2698059  0.2695438  0.26950288 0.2694764  0.2691681  0.26915136
 0.2694587  0.27000928 0.27042776 0.27025494 0.2703413  0.27037135
 0.2701852  0.27024    0.2700455  0.26994127 0.2699131  0.26964492
 0.26971093 0.2701617  0.27031374 0.27002952 0.26993367 0.26994598
 0.26984015 0.2697831  0.26979426 0.27001095 0.27012464 0.2700698
 0.27085453 0.2709428  0.27052027 0.2704127  0.2707447  0.27072117
 0.27025992 0.27013865 0.27003312 0.2699662  0.2701048  0.26976535
 0.26957455 0.26968783 0.26976773 0.26994553 0.26990405 0.26979876
 0.26962    0.26970175 0.26959524 0.26908022 0.26927006 0.26954222
 0.2701004  0.27033517 0.2702087  0.27015302 0.27020666 0.27056494
 0.27076167 0.27048808 0.2706311  0.27065372 0.27021408 0.27002364
 0.26992524 0.2697877  0.26957855 0.26948696 0.2694617  0.26936576
 0.26927865 0.2691435  0.2692847  0.26909637 0.2687318  0.26896822
 0.26912218 0.26917833 0.26944396 0.26974154 0.26992175 0.2697263
 0.26985836 0.26983666 0.26969555 0.27000332 0.2697272  0.26924127
 0.26898912 0.2687095  0.2686672  0.26841137 0.26834482 0.26853552
 0.26859862 0.26862633 0.2680776  0.26777545 0.26788154 0.26799268
 0.26845786 0.26877037 0.26873314 0.2685827  0.2686145  0.26851776
 0.26844835 0.2688403  0.26879546 0.26849064 0.26832488 0.26809362
 0.2679741  0.26776883 0.26762897 0.26750252 0.2674248  0.2676039
 0.26749742 0.26742554 0.26734963 0.26700506 0.26697347 0.26702833
 0.26727614 0.26779896 0.26783022 0.26781386 0.26763517 0.26757646
 0.26766187 0.26751295 0.26737052 0.26718056 0.26709503 0.26689944
 0.26674017 0.2664856  0.2662774  0.2667076  0.26661736 0.26649508
 0.26689982 0.2668766  0.2667754  0.26650903 0.26663908 0.2671694
 0.26722354 0.26758105 0.2678549  0.26806396 0.26806375 0.26782945
 0.2680591  0.2679499  0.26780632 0.26766565 0.26705524 0.2666996
 0.26653    0.26699114 0.2670042  0.26639277 0.266537   0.26666626
 0.26703453 0.26712853 0.26690635 0.26733387 0.26719195 0.26716396
 0.26791352 0.2684745  0.26893082 0.268858   0.26884574 0.26885122
 0.26866344 0.268368   0.26761064 0.26735583 0.2671923  0.26707456
 0.26734704 0.2671487  0.26686826 0.2666292  0.26694274 0.26724526
 0.2674565  0.2683099  0.26836628 0.26844904 0.2687061  0.2687013
 0.2698078  0.27006736 0.27003703 0.2696843  0.26960215 0.26982865
 0.26919195 0.26887217 0.26854363 0.26817885 0.2680288  0.2676361
 0.26813838 0.26802665 0.26808214 0.268455   0.2682288  0.26903185
 0.26901835 0.2696165  0.2704868  0.2699584  0.27012143 0.26972735
 0.270401   0.2706995  0.27028248 0.27035904 0.26970318 0.2694899
 0.2690237  0.26897883 0.26910248 0.2683173  0.2690771  0.2688057
 0.2693027  0.2697204  0.26990828 0.27141878 0.27033314 0.27111933
 0.26991552 0.26945147 0.26878235 0.26532277 0.26692122 0.2705713 ]
