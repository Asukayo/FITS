Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  22596812800.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 64.27080416679382
Epoch: 1, Steps: 84 | Train Loss: 0.9944615 Vali Loss: 0.9167346 Test Loss: 1.0750664
Validation loss decreased (inf --> 0.916735).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 64.53798747062683
Epoch: 2, Steps: 84 | Train Loss: 0.6525001 Vali Loss: 0.7296011 Test Loss: 0.8508936
Validation loss decreased (0.916735 --> 0.729601).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 64.63413095474243
Epoch: 3, Steps: 84 | Train Loss: 0.5216120 Vali Loss: 0.6143773 Test Loss: 0.7131962
Validation loss decreased (0.729601 --> 0.614377).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 65.89616131782532
Epoch: 4, Steps: 84 | Train Loss: 0.4376706 Vali Loss: 0.5383013 Test Loss: 0.6239496
Validation loss decreased (0.614377 --> 0.538301).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 67.72205018997192
Epoch: 5, Steps: 84 | Train Loss: 0.3820750 Vali Loss: 0.4876119 Test Loss: 0.5650613
Validation loss decreased (0.538301 --> 0.487612).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 65.87652635574341
Epoch: 6, Steps: 84 | Train Loss: 0.3448420 Vali Loss: 0.4532082 Test Loss: 0.5248563
Validation loss decreased (0.487612 --> 0.453208).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 67.57183241844177
Epoch: 7, Steps: 84 | Train Loss: 0.3199483 Vali Loss: 0.4303369 Test Loss: 0.4994445
Validation loss decreased (0.453208 --> 0.430337).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 69.11888647079468
Epoch: 8, Steps: 84 | Train Loss: 0.3033137 Vali Loss: 0.4156530 Test Loss: 0.4816999
Validation loss decreased (0.430337 --> 0.415653).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 70.0529272556305
Epoch: 9, Steps: 84 | Train Loss: 0.2922921 Vali Loss: 0.4054735 Test Loss: 0.4708293
Validation loss decreased (0.415653 --> 0.405474).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 65.8425178527832
Epoch: 10, Steps: 84 | Train Loss: 0.2850858 Vali Loss: 0.3994463 Test Loss: 0.4632075
Validation loss decreased (0.405474 --> 0.399446).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 65.95338463783264
Epoch: 11, Steps: 84 | Train Loss: 0.2801473 Vali Loss: 0.3950722 Test Loss: 0.4594010
Validation loss decreased (0.399446 --> 0.395072).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 65.15155124664307
Epoch: 12, Steps: 84 | Train Loss: 0.2770658 Vali Loss: 0.3927721 Test Loss: 0.4561671
Validation loss decreased (0.395072 --> 0.392772).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 64.2741768360138
Epoch: 13, Steps: 84 | Train Loss: 0.2748458 Vali Loss: 0.3898956 Test Loss: 0.4534192
Validation loss decreased (0.392772 --> 0.389896).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 67.71643900871277
Epoch: 14, Steps: 84 | Train Loss: 0.2734966 Vali Loss: 0.3892936 Test Loss: 0.4522955
Validation loss decreased (0.389896 --> 0.389294).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 66.55494928359985
Epoch: 15, Steps: 84 | Train Loss: 0.2725357 Vali Loss: 0.3877844 Test Loss: 0.4518891
Validation loss decreased (0.389294 --> 0.387784).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 64.84853482246399
Epoch: 16, Steps: 84 | Train Loss: 0.2718996 Vali Loss: 0.3873171 Test Loss: 0.4511054
Validation loss decreased (0.387784 --> 0.387317).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 65.9306287765503
Epoch: 17, Steps: 84 | Train Loss: 0.2713798 Vali Loss: 0.3867863 Test Loss: 0.4505496
Validation loss decreased (0.387317 --> 0.386786).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 67.00620412826538
Epoch: 18, Steps: 84 | Train Loss: 0.2710444 Vali Loss: 0.3867022 Test Loss: 0.4504502
Validation loss decreased (0.386786 --> 0.386702).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 69.31434965133667
Epoch: 19, Steps: 84 | Train Loss: 0.2709473 Vali Loss: 0.3868518 Test Loss: 0.4504121
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 66.12611055374146
Epoch: 20, Steps: 84 | Train Loss: 0.2706079 Vali Loss: 0.3863474 Test Loss: 0.4500019
Validation loss decreased (0.386702 --> 0.386347).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 66.66778826713562
Epoch: 21, Steps: 84 | Train Loss: 0.2706111 Vali Loss: 0.3866068 Test Loss: 0.4500573
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 67.13576650619507
Epoch: 22, Steps: 84 | Train Loss: 0.2704794 Vali Loss: 0.3865161 Test Loss: 0.4499379
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 66.44509840011597
Epoch: 23, Steps: 84 | Train Loss: 0.2703745 Vali Loss: 0.3856570 Test Loss: 0.4498142
Validation loss decreased (0.386347 --> 0.385657).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 67.15379691123962
Epoch: 24, Steps: 84 | Train Loss: 0.2702628 Vali Loss: 0.3861719 Test Loss: 0.4496500
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 66.16956806182861
Epoch: 25, Steps: 84 | Train Loss: 0.2701761 Vali Loss: 0.3857719 Test Loss: 0.4497350
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 67.39823889732361
Epoch: 26, Steps: 84 | Train Loss: 0.2702181 Vali Loss: 0.3861913 Test Loss: 0.4499650
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 61.05335521697998
Epoch: 27, Steps: 84 | Train Loss: 0.2701358 Vali Loss: 0.3857264 Test Loss: 0.4497821
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 69.07010674476624
Epoch: 28, Steps: 84 | Train Loss: 0.2701807 Vali Loss: 0.3856065 Test Loss: 0.4497689
Validation loss decreased (0.385657 --> 0.385607).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 67.27068829536438
Epoch: 29, Steps: 84 | Train Loss: 0.2701723 Vali Loss: 0.3860469 Test Loss: 0.4496726
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 62.955315828323364
Epoch: 30, Steps: 84 | Train Loss: 0.2699354 Vali Loss: 0.3855258 Test Loss: 0.4495581
Validation loss decreased (0.385607 --> 0.385526).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 66.1587142944336
Epoch: 31, Steps: 84 | Train Loss: 0.2700494 Vali Loss: 0.3851029 Test Loss: 0.4496050
Validation loss decreased (0.385526 --> 0.385103).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 58.92464303970337
Epoch: 32, Steps: 84 | Train Loss: 0.2699796 Vali Loss: 0.3855911 Test Loss: 0.4498717
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 62.499948501586914
Epoch: 33, Steps: 84 | Train Loss: 0.2699982 Vali Loss: 0.3853828 Test Loss: 0.4495631
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 67.7499372959137
Epoch: 34, Steps: 84 | Train Loss: 0.2699534 Vali Loss: 0.3854409 Test Loss: 0.4494879
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 66.17721438407898
Epoch: 35, Steps: 84 | Train Loss: 0.2699775 Vali Loss: 0.3857471 Test Loss: 0.4495523
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 67.186940908432
Epoch: 36, Steps: 84 | Train Loss: 0.2698284 Vali Loss: 0.3856158 Test Loss: 0.4495801
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 67.65304255485535
Epoch: 37, Steps: 84 | Train Loss: 0.2698359 Vali Loss: 0.3859092 Test Loss: 0.4495499
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 70.82091069221497
Epoch: 38, Steps: 84 | Train Loss: 0.2698714 Vali Loss: 0.3851809 Test Loss: 0.4494978
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 66.76233911514282
Epoch: 39, Steps: 84 | Train Loss: 0.2697918 Vali Loss: 0.3854685 Test Loss: 0.4495219
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 67.62283253669739
Epoch: 40, Steps: 84 | Train Loss: 0.2698257 Vali Loss: 0.3850034 Test Loss: 0.4494479
Validation loss decreased (0.385103 --> 0.385003).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 52.93130660057068
Epoch: 41, Steps: 84 | Train Loss: 0.2697968 Vali Loss: 0.3848988 Test Loss: 0.4493576
Validation loss decreased (0.385003 --> 0.384899).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 58.029778242111206
Epoch: 42, Steps: 84 | Train Loss: 0.2698369 Vali Loss: 0.3850331 Test Loss: 0.4493870
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 67.62115550041199
Epoch: 43, Steps: 84 | Train Loss: 0.2697847 Vali Loss: 0.3855392 Test Loss: 0.4493544
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 64.4006814956665
Epoch: 44, Steps: 84 | Train Loss: 0.2698153 Vali Loss: 0.3845953 Test Loss: 0.4495802
Validation loss decreased (0.384899 --> 0.384595).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 64.49737000465393
Epoch: 45, Steps: 84 | Train Loss: 0.2697637 Vali Loss: 0.3851886 Test Loss: 0.4494585
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 58.257856369018555
Epoch: 46, Steps: 84 | Train Loss: 0.2697209 Vali Loss: 0.3856452 Test Loss: 0.4495120
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 50.20789313316345
Epoch: 47, Steps: 84 | Train Loss: 0.2697063 Vali Loss: 0.3855301 Test Loss: 0.4493590
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 51.49694299697876
Epoch: 48, Steps: 84 | Train Loss: 0.2697827 Vali Loss: 0.3851696 Test Loss: 0.4494493
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 50.69584131240845
Epoch: 49, Steps: 84 | Train Loss: 0.2696731 Vali Loss: 0.3851336 Test Loss: 0.4493405
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 50.62522029876709
Epoch: 50, Steps: 84 | Train Loss: 0.2697299 Vali Loss: 0.3852214 Test Loss: 0.4493399
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
>>>>>>>testing : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4490545988082886, mae:0.299572616815567, rse:0.5479618906974792, corr:[0.25601712 0.2646311  0.26298094 0.2651263  0.2674662  0.26764697
 0.27042025 0.26892334 0.27036768 0.26888227 0.26858196 0.26848304
 0.26679406 0.26731673 0.26566184 0.26627216 0.26613042 0.26582515
 0.2670768  0.2666731  0.26768816 0.26752028 0.26757824 0.26797915
 0.26900774 0.2702645  0.26983973 0.26994294 0.26966202 0.2688138
 0.2691856  0.2680644  0.26809132 0.2678914  0.26714733 0.26761246
 0.2669626  0.26728365 0.26720807 0.2672023  0.26793382 0.26767647
 0.26823956 0.2680631  0.26797011 0.26829016 0.26795158 0.2684136
 0.26857403 0.26891327 0.26905632 0.26817188 0.26834515 0.26770002
 0.26752028 0.26754242 0.26684603 0.26718807 0.26677063 0.26686326
 0.26706558 0.2667876  0.2674169  0.2672791  0.26780176 0.26812965
 0.26784202 0.26820555 0.26789203 0.2679187  0.26777115 0.26784262
 0.2681499  0.26754662 0.26771995 0.26739568 0.26699263 0.26713493
 0.2666037  0.26687932 0.26677486 0.2666686  0.26701334 0.26675487
 0.26714155 0.26696503 0.26692966 0.26746708 0.26752192 0.2679827
 0.2677916  0.2675497  0.26768172 0.26740506 0.26766953 0.26752517
 0.26730436 0.2672472  0.2667559  0.266992   0.26662323 0.26649016
 0.26657405 0.26628098 0.2665618  0.26642537 0.26656157 0.26676252
 0.26667348 0.26697996 0.26682281 0.26720524 0.26744637 0.26708364
 0.26740178 0.26713312 0.2670545  0.26696202 0.26686352 0.2673394
 0.26731613 0.2675574  0.26748657 0.26737428 0.26741466 0.26693815
 0.2671071  0.2670109  0.26693082 0.26724404 0.26702034 0.2672531
 0.26726985 0.26760203 0.26808038 0.26792476 0.26841182 0.26794153
 0.26779768 0.26796502 0.26760232 0.26773408 0.26757786 0.26766512
 0.26801565 0.26751223 0.2676086  0.2674311  0.26728138 0.26741585
 0.26721257 0.26754108 0.26757368 0.26770037 0.26770213 0.26744622
 0.26789203 0.2680809  0.26835737 0.26854703 0.2683907  0.268573
 0.26848942 0.26853886 0.26853117 0.2683126  0.26843497 0.2683328
 0.269562   0.26963928 0.26924685 0.26918703 0.26877406 0.26892468
 0.2689067  0.26880726 0.26921442 0.26899746 0.26911795 0.2691351
 0.26898074 0.26928663 0.26933655 0.26970685 0.26975563 0.2696435
 0.2696881  0.2694144  0.269605   0.26933005 0.26895583 0.26902306
 0.26940703 0.26971018 0.26964843 0.2694625  0.26941034 0.26912683
 0.2693078  0.26921293 0.2693334  0.2695029  0.2693072  0.26961577
 0.26944503 0.26938295 0.26955497 0.26941705 0.2697063  0.269675
 0.2696008  0.26945344 0.2691544  0.26924622 0.26889265 0.26886392
 0.2690926  0.26884317 0.26893514 0.26867023 0.26872402 0.26880497
 0.26865816 0.26898375 0.26892778 0.26913145 0.26932    0.26907465
 0.26908514 0.26902765 0.2693234  0.26919267 0.26905856 0.26934892
 0.2688727  0.2686071  0.2684428  0.26829574 0.26846907 0.26833126
 0.2685116  0.26842272 0.2683729  0.26845384 0.2681219  0.26838297
 0.26839077 0.26840207 0.2686731  0.26857796 0.2687956  0.26866493
 0.26859263 0.26880717 0.268743   0.26909393 0.26910615 0.26905093
 0.26903307 0.26866695 0.26879802 0.26854032 0.2684037  0.26854342
 0.26819527 0.26833948 0.26823336 0.26833165 0.2683971  0.26817912
 0.26850528 0.2684187  0.26845583 0.26875395 0.26871985 0.26891127
 0.26876107 0.26887444 0.26900008 0.26886222 0.26910025 0.26867935
 0.26860577 0.2687832  0.26861846 0.2686045  0.26835465 0.2684873
 0.26849595 0.26849926 0.2687498  0.26849028 0.26855543 0.2686143
 0.26865202 0.26901925 0.2688681  0.26902443 0.26908308 0.2691358
 0.26941708 0.26929733 0.26953855 0.2695544  0.2694935  0.26943907
 0.2691236  0.26912865 0.2690155  0.2690105  0.26923177 0.2690931
 0.26942647 0.2692049  0.26908574 0.26929218 0.26904055 0.26932988
 0.26944306 0.26953182 0.2697327  0.26951003 0.26980466 0.26987126
 0.26983884 0.2700675  0.27003843 0.2703207  0.27016082 0.26984957
 0.2697911  0.26954016 0.2697943  0.26966923 0.2695347  0.26958716
 0.27020976 0.2705499  0.2702041  0.27003437 0.27005023 0.26991224
 0.2702132  0.27024677 0.2704507  0.2705413  0.27038378 0.2706343
 0.27048606 0.2704803  0.27062926 0.27067843 0.27078417 0.27042967
 0.2704609  0.2705551  0.27042627 0.27040157 0.26997736 0.2699109
 0.27034867 0.27049753 0.27057776 0.2701853  0.27043703 0.2704629
 0.27046221 0.27090588 0.2707647  0.2708771  0.27084053 0.27071047
 0.27085716 0.2706914  0.27090842 0.27092713 0.27076906 0.2707024
 0.27022502 0.27014574 0.26996446 0.26979336 0.2698254  0.2697192
 0.27001685 0.26998907 0.2701054  0.27008104 0.2699392  0.2703142
 0.27021506 0.2703147  0.27050552 0.2704525  0.27069253 0.27037454
 0.27031502 0.27038854 0.27029553 0.2704338  0.27003357 0.27006882
 0.26998526 0.26948124 0.26947135 0.26926166 0.26930186 0.26926455
 0.26918957 0.26944104 0.26923186 0.26944062 0.2694505  0.2694514
 0.2697982  0.26961458 0.26998863 0.27012274 0.2698626  0.26996133
 0.2696796  0.26985434 0.26987502 0.26964468 0.26969048 0.26932856
 0.26951727 0.26938617 0.26910338 0.26921168 0.26889274 0.26925865
 0.2692094  0.26908338 0.2692888  0.26899856 0.26921135 0.2691981
 0.269268   0.26966265 0.26957804 0.26993084 0.26985884 0.26968566
 0.2698162  0.2696559  0.26994935 0.26977205 0.2695053  0.2693257
 0.26883137 0.2688024  0.26863596 0.26861137 0.26873085 0.26871195
 0.2690474  0.26895234 0.26917046 0.26947466 0.26929227 0.26970664
 0.26972303 0.26982674 0.2698781  0.269558   0.26990548 0.26986632
 0.26991585 0.27006045 0.26977623 0.26992488 0.26978144 0.26973668
 0.26967606 0.2693515  0.26951    0.26925796 0.26923048 0.269372
 0.26951072 0.26989326 0.2699328  0.27009848 0.27007845 0.2701426
 0.2705296  0.27037054 0.27055666 0.27039567 0.2700677  0.27033517
 0.27014092 0.2701779  0.27011994 0.27000806 0.26999253 0.2695829
 0.269888   0.26995078 0.26989272 0.27008164 0.26965234 0.26969633
 0.27046221 0.27065676 0.2706091  0.27020323 0.27044266 0.2705653
 0.27071    0.27076122 0.2703879  0.2706222  0.2703516  0.2701759
 0.27031222 0.26986673 0.2699609  0.26986516 0.26977113 0.2697948
 0.26949605 0.26972845 0.26954088 0.2693197  0.26922026 0.2692326
 0.27008712 0.27002868 0.27031368 0.27046487 0.27022848 0.2705428
 0.27045888 0.2706755  0.27067924 0.27037418 0.27058193 0.27014846
 0.26992485 0.26972347 0.26953295 0.26981047 0.2692398  0.26915774
 0.26917487 0.2688854  0.26897305 0.26868054 0.2688328  0.26878902
 0.26892063 0.26927593 0.2691623  0.26946476 0.2694233  0.26960006
 0.27000186 0.26975256 0.27003762 0.26981357 0.26962864 0.2697086
 0.26909825 0.26915175 0.26888004 0.26861516 0.26864895 0.2682879
 0.26850817 0.26830134 0.26824588 0.26833826 0.26803818 0.2684655
 0.26842988 0.2685811  0.2688057  0.26866046 0.26912382 0.26888627
 0.26872554 0.26877305 0.26865104 0.26884097 0.26825958 0.26808304
 0.26799276 0.26757714 0.2675535  0.26695362 0.2671289  0.2672377
 0.2668703  0.26707953 0.26680556 0.26714253 0.26734203 0.26732725
 0.26748925 0.26728004 0.26778516 0.2677347  0.26764432 0.26799017
 0.26762262 0.26789117 0.26762995 0.26734167 0.26747587 0.26694515
 0.2670374  0.26668772 0.26660872 0.2668731  0.2664023  0.26673922
 0.26649138 0.26650175 0.26704848 0.26688704 0.267384   0.26732647
 0.2674156  0.26779127 0.26754004 0.2680665  0.26788685 0.26800585
 0.2680769  0.2675427  0.26778826 0.26704693 0.26686075 0.2669427
 0.2665082  0.26692906 0.26649207 0.26672968 0.2667838  0.26638168
 0.2670647  0.26680952 0.26731747 0.26773253 0.26746142 0.26802772
 0.26817426 0.26855138 0.2685249  0.26847997 0.26884896 0.26837975
 0.26873854 0.26839593 0.26780716 0.2677408  0.26685578 0.26714158
 0.26685822 0.2667745  0.2672295  0.26681775 0.26751727 0.26731429
 0.26758754 0.26813838 0.2677794  0.26854384 0.26827353 0.26860604
 0.26987952 0.26977515 0.27011925 0.2696062  0.26977718 0.26940146
 0.2687084  0.26904723 0.26775354 0.26779497 0.26766753 0.26733884
 0.26823252 0.2673926  0.2682043  0.26835498 0.26847827 0.26945066
 0.26882574 0.270193   0.27034104 0.27036607 0.27063432 0.26947564
 0.27078885 0.2707138  0.27092564 0.27032754 0.269155   0.2697153
 0.26771107 0.26825318 0.26770052 0.26701298 0.26860946 0.2674104
 0.2698417  0.26973766 0.2707821  0.27221322 0.27114064 0.27348313
 0.26993957 0.27044752 0.26674005 0.2644819  0.2628797  0.27150655]
