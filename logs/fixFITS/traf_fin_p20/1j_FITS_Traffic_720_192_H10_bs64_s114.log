Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14299545600.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 54.15810775756836
Epoch: 1, Steps: 88 | Train Loss: 0.7650930 Vali Loss: 0.6210802 Test Loss: 0.7114629
Validation loss decreased (inf --> 0.621080).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 57.17959547042847
Epoch: 2, Steps: 88 | Train Loss: 0.3832807 Vali Loss: 0.4140675 Test Loss: 0.4842838
Validation loss decreased (0.621080 --> 0.414067).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 56.254358530044556
Epoch: 3, Steps: 88 | Train Loss: 0.2780552 Vali Loss: 0.3527867 Test Loss: 0.4202823
Validation loss decreased (0.414067 --> 0.352787).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 58.157013177871704
Epoch: 4, Steps: 88 | Train Loss: 0.2486451 Vali Loss: 0.3364838 Test Loss: 0.4048944
Validation loss decreased (0.352787 --> 0.336484).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 54.38221621513367
Epoch: 5, Steps: 88 | Train Loss: 0.2413256 Vali Loss: 0.3322016 Test Loss: 0.4014718
Validation loss decreased (0.336484 --> 0.332202).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 57.14781641960144
Epoch: 6, Steps: 88 | Train Loss: 0.2393368 Vali Loss: 0.3303794 Test Loss: 0.4003264
Validation loss decreased (0.332202 --> 0.330379).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 57.22951102256775
Epoch: 7, Steps: 88 | Train Loss: 0.2386087 Vali Loss: 0.3298269 Test Loss: 0.3998908
Validation loss decreased (0.330379 --> 0.329827).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 56.32199001312256
Epoch: 8, Steps: 88 | Train Loss: 0.2382587 Vali Loss: 0.3293386 Test Loss: 0.3998444
Validation loss decreased (0.329827 --> 0.329339).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 56.89510989189148
Epoch: 9, Steps: 88 | Train Loss: 0.2379727 Vali Loss: 0.3289347 Test Loss: 0.3993053
Validation loss decreased (0.329339 --> 0.328935).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 56.11368465423584
Epoch: 10, Steps: 88 | Train Loss: 0.2378464 Vali Loss: 0.3284768 Test Loss: 0.3992384
Validation loss decreased (0.328935 --> 0.328477).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 51.691338777542114
Epoch: 11, Steps: 88 | Train Loss: 0.2377803 Vali Loss: 0.3285660 Test Loss: 0.3991546
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 58.51880741119385
Epoch: 12, Steps: 88 | Train Loss: 0.2375997 Vali Loss: 0.3275823 Test Loss: 0.3987817
Validation loss decreased (0.328477 --> 0.327582).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 59.21102166175842
Epoch: 13, Steps: 88 | Train Loss: 0.2375380 Vali Loss: 0.3284574 Test Loss: 0.3988953
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 55.835795402526855
Epoch: 14, Steps: 88 | Train Loss: 0.2375046 Vali Loss: 0.3280170 Test Loss: 0.3988211
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 61.04734134674072
Epoch: 15, Steps: 88 | Train Loss: 0.2372694 Vali Loss: 0.3277798 Test Loss: 0.3984677
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 59.24238324165344
Epoch: 16, Steps: 88 | Train Loss: 0.2373646 Vali Loss: 0.3279714 Test Loss: 0.3985915
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 58.86887526512146
Epoch: 17, Steps: 88 | Train Loss: 0.2374551 Vali Loss: 0.3275899 Test Loss: 0.3986707
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 58.06348276138306
Epoch: 18, Steps: 88 | Train Loss: 0.2372845 Vali Loss: 0.3276171 Test Loss: 0.3986673
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 56.03257775306702
Epoch: 19, Steps: 88 | Train Loss: 0.2372847 Vali Loss: 0.3280762 Test Loss: 0.3985843
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 57.4750497341156
Epoch: 20, Steps: 88 | Train Loss: 0.2371736 Vali Loss: 0.3275447 Test Loss: 0.3983713
Validation loss decreased (0.327582 --> 0.327545).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 58.65269589424133
Epoch: 21, Steps: 88 | Train Loss: 0.2370548 Vali Loss: 0.3271056 Test Loss: 0.3983524
Validation loss decreased (0.327545 --> 0.327106).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 53.923482179641724
Epoch: 22, Steps: 88 | Train Loss: 0.2371056 Vali Loss: 0.3272915 Test Loss: 0.3984385
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 55.037110805511475
Epoch: 23, Steps: 88 | Train Loss: 0.2370223 Vali Loss: 0.3270931 Test Loss: 0.3982524
Validation loss decreased (0.327106 --> 0.327093).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 52.41185665130615
Epoch: 24, Steps: 88 | Train Loss: 0.2371007 Vali Loss: 0.3274604 Test Loss: 0.3985050
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 50.38460898399353
Epoch: 25, Steps: 88 | Train Loss: 0.2371288 Vali Loss: 0.3272772 Test Loss: 0.3981152
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 52.04793977737427
Epoch: 26, Steps: 88 | Train Loss: 0.2369979 Vali Loss: 0.3281309 Test Loss: 0.3983927
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 51.704915046691895
Epoch: 27, Steps: 88 | Train Loss: 0.2370684 Vali Loss: 0.3275402 Test Loss: 0.3983433
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 51.87997603416443
Epoch: 28, Steps: 88 | Train Loss: 0.2370442 Vali Loss: 0.3272639 Test Loss: 0.3983681
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 53.740476846694946
Epoch: 29, Steps: 88 | Train Loss: 0.2369203 Vali Loss: 0.3272056 Test Loss: 0.3982173
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 55.50158452987671
Epoch: 30, Steps: 88 | Train Loss: 0.2369519 Vali Loss: 0.3276366 Test Loss: 0.3981838
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 52.375195026397705
Epoch: 31, Steps: 88 | Train Loss: 0.2368569 Vali Loss: 0.3276980 Test Loss: 0.3982581
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 52.753546476364136
Epoch: 32, Steps: 88 | Train Loss: 0.2369456 Vali Loss: 0.3277383 Test Loss: 0.3981459
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 52.48315715789795
Epoch: 33, Steps: 88 | Train Loss: 0.2368054 Vali Loss: 0.3272014 Test Loss: 0.3981838
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 51.62543344497681
Epoch: 34, Steps: 88 | Train Loss: 0.2369024 Vali Loss: 0.3279792 Test Loss: 0.3981011
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 51.677178382873535
Epoch: 35, Steps: 88 | Train Loss: 0.2369426 Vali Loss: 0.3273781 Test Loss: 0.3981467
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 55.63004159927368
Epoch: 36, Steps: 88 | Train Loss: 0.2368240 Vali Loss: 0.3269751 Test Loss: 0.3981045
Validation loss decreased (0.327093 --> 0.326975).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 52.23935842514038
Epoch: 37, Steps: 88 | Train Loss: 0.2368997 Vali Loss: 0.3272356 Test Loss: 0.3981843
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 52.96549415588379
Epoch: 38, Steps: 88 | Train Loss: 0.2368454 Vali Loss: 0.3271644 Test Loss: 0.3981042
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 51.60193181037903
Epoch: 39, Steps: 88 | Train Loss: 0.2367122 Vali Loss: 0.3271633 Test Loss: 0.3982134
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 51.78797459602356
Epoch: 40, Steps: 88 | Train Loss: 0.2368674 Vali Loss: 0.3269950 Test Loss: 0.3980835
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 53.93377232551575
Epoch: 41, Steps: 88 | Train Loss: 0.2367444 Vali Loss: 0.3274504 Test Loss: 0.3982155
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 56.84366035461426
Epoch: 42, Steps: 88 | Train Loss: 0.2367817 Vali Loss: 0.3272122 Test Loss: 0.3981637
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 55.311359167099
Epoch: 43, Steps: 88 | Train Loss: 0.2367771 Vali Loss: 0.3268823 Test Loss: 0.3980899
Validation loss decreased (0.326975 --> 0.326882).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 55.52653908729553
Epoch: 44, Steps: 88 | Train Loss: 0.2367705 Vali Loss: 0.3263966 Test Loss: 0.3980084
Validation loss decreased (0.326882 --> 0.326397).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 55.133689880371094
Epoch: 45, Steps: 88 | Train Loss: 0.2366883 Vali Loss: 0.3273556 Test Loss: 0.3980173
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 55.38611602783203
Epoch: 46, Steps: 88 | Train Loss: 0.2366878 Vali Loss: 0.3267449 Test Loss: 0.3980250
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 53.50963377952576
Epoch: 47, Steps: 88 | Train Loss: 0.2367246 Vali Loss: 0.3266903 Test Loss: 0.3980325
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 57.836984157562256
Epoch: 48, Steps: 88 | Train Loss: 0.2367384 Vali Loss: 0.3269561 Test Loss: 0.3980153
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 55.63962697982788
Epoch: 49, Steps: 88 | Train Loss: 0.2367409 Vali Loss: 0.3269819 Test Loss: 0.3980158
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 55.40095901489258
Epoch: 50, Steps: 88 | Train Loss: 0.2366873 Vali Loss: 0.3268099 Test Loss: 0.3980215
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
>>>>>>>testing : Traffic_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.3970426917076111, mae:0.27230918407440186, rse:0.5200530290603638, corr:[0.27426985 0.29009175 0.2883881  0.28960106 0.28945976 0.29034477
 0.29091978 0.2907416  0.29106697 0.29030648 0.29052964 0.2900758
 0.28978878 0.2894917  0.28917113 0.28930685 0.28892082 0.28893107
 0.2890503  0.28932694 0.28939688 0.28928015 0.28923947 0.2889866
 0.29050538 0.29081392 0.29044846 0.29029354 0.29003674 0.29007268
 0.29001564 0.2898645  0.28986105 0.289655   0.28986278 0.28965324
 0.28919166 0.28915113 0.28914028 0.2893345  0.28936294 0.28930104
 0.28912672 0.28923416 0.28955045 0.2895245  0.2894349  0.28928024
 0.28974405 0.28976744 0.2894666  0.28941137 0.2891263  0.28875172
 0.28874072 0.28856322 0.28855145 0.28860396 0.28853485 0.2884873
 0.28828847 0.28847864 0.2887394  0.28862822 0.2885914  0.28853768
 0.28836876 0.28831017 0.28848532 0.28840342 0.288267   0.28844315
 0.2883558  0.28815982 0.28791654 0.28795126 0.28811666 0.2880512
 0.28815195 0.28796828 0.28776002 0.28787628 0.2878015  0.28768957
 0.2875867  0.2875316  0.2876392  0.2875621  0.28752926 0.2877018
 0.287893   0.28790486 0.2878012  0.28761613 0.28743002 0.28765938
 0.28739893 0.28717265 0.2872064  0.2872237  0.28754908 0.28763163
 0.28752157 0.28751582 0.2873879  0.28732    0.2873172  0.2874401
 0.28741774 0.28735194 0.28735027 0.28729236 0.28728247 0.28685313
 0.28678286 0.2871477  0.28696755 0.2869747  0.28708667 0.2871347
 0.2873352  0.28737107 0.28756887 0.28773704 0.28771046 0.28790316
 0.2877005  0.28742006 0.28741932 0.28755522 0.28745842 0.28734705
 0.28740886 0.28745908 0.2878267  0.28789034 0.28791386 0.28809834
 0.28808573 0.28817675 0.28789303 0.28782135 0.28799212 0.28777942
 0.2880783  0.2881663  0.28827426 0.28838456 0.2880831  0.2881158
 0.28799608 0.2879323  0.2878054  0.287539   0.28768948 0.2876752
 0.28820452 0.2883172  0.28813973 0.28851902 0.28827652 0.28843537
 0.2884977  0.28850174 0.288723   0.28846747 0.28873962 0.2886573
 0.28978622 0.29006714 0.2895012  0.28928667 0.28912854 0.28918886
 0.28899384 0.28888842 0.28925192 0.28882933 0.28912774 0.28911942
 0.28926098 0.28959322 0.2890291  0.2897669  0.28900278 0.28905666
 0.28859413 0.28806493 0.28836575 0.2868415  0.28874648 0.2891535 ]
