Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16559226880.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 58.99082565307617
Epoch: 1, Steps: 87 | Train Loss: 0.7964075 Vali Loss: 0.6942279 Test Loss: 0.7998554
Validation loss decreased (inf --> 0.694228).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 63.06558346748352
Epoch: 2, Steps: 87 | Train Loss: 0.4510764 Vali Loss: 0.4954154 Test Loss: 0.5750287
Validation loss decreased (0.694228 --> 0.495415).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 60.1121551990509
Epoch: 3, Steps: 87 | Train Loss: 0.3348819 Vali Loss: 0.4077104 Test Loss: 0.4797657
Validation loss decreased (0.495415 --> 0.407710).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 54.39025568962097
Epoch: 4, Steps: 87 | Train Loss: 0.2844705 Vali Loss: 0.3702969 Test Loss: 0.4390370
Validation loss decreased (0.407710 --> 0.370297).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 54.01977753639221
Epoch: 5, Steps: 87 | Train Loss: 0.2631204 Vali Loss: 0.3536645 Test Loss: 0.4232580
Validation loss decreased (0.370297 --> 0.353664).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 57.011351346969604
Epoch: 6, Steps: 87 | Train Loss: 0.2543903 Vali Loss: 0.3473550 Test Loss: 0.4174311
Validation loss decreased (0.353664 --> 0.347355).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 56.060869455337524
Epoch: 7, Steps: 87 | Train Loss: 0.2509312 Vali Loss: 0.3443252 Test Loss: 0.4151622
Validation loss decreased (0.347355 --> 0.344325).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 56.22360825538635
Epoch: 8, Steps: 87 | Train Loss: 0.2494771 Vali Loss: 0.3430965 Test Loss: 0.4146166
Validation loss decreased (0.344325 --> 0.343096).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 57.26536583900452
Epoch: 9, Steps: 87 | Train Loss: 0.2487837 Vali Loss: 0.3420026 Test Loss: 0.4139658
Validation loss decreased (0.343096 --> 0.342003).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 54.96250891685486
Epoch: 10, Steps: 87 | Train Loss: 0.2484616 Vali Loss: 0.3413160 Test Loss: 0.4137368
Validation loss decreased (0.342003 --> 0.341316).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 55.176607847213745
Epoch: 11, Steps: 87 | Train Loss: 0.2483375 Vali Loss: 0.3412708 Test Loss: 0.4136856
Validation loss decreased (0.341316 --> 0.341271).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 55.12862229347229
Epoch: 12, Steps: 87 | Train Loss: 0.2481166 Vali Loss: 0.3409836 Test Loss: 0.4135589
Validation loss decreased (0.341271 --> 0.340984).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 54.93192100524902
Epoch: 13, Steps: 87 | Train Loss: 0.2479947 Vali Loss: 0.3407953 Test Loss: 0.4132506
Validation loss decreased (0.340984 --> 0.340795).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 55.07894444465637
Epoch: 14, Steps: 87 | Train Loss: 0.2479041 Vali Loss: 0.3401939 Test Loss: 0.4133377
Validation loss decreased (0.340795 --> 0.340194).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 55.681705951690674
Epoch: 15, Steps: 87 | Train Loss: 0.2478781 Vali Loss: 0.3403396 Test Loss: 0.4131581
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 54.04287075996399
Epoch: 16, Steps: 87 | Train Loss: 0.2477794 Vali Loss: 0.3401717 Test Loss: 0.4131616
Validation loss decreased (0.340194 --> 0.340172).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 55.01388907432556
Epoch: 17, Steps: 87 | Train Loss: 0.2477625 Vali Loss: 0.3398911 Test Loss: 0.4130392
Validation loss decreased (0.340172 --> 0.339891).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 52.28766345977783
Epoch: 18, Steps: 87 | Train Loss: 0.2476612 Vali Loss: 0.3401456 Test Loss: 0.4132132
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 50.66601586341858
Epoch: 19, Steps: 87 | Train Loss: 0.2477094 Vali Loss: 0.3397559 Test Loss: 0.4131813
Validation loss decreased (0.339891 --> 0.339756).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 53.12551546096802
Epoch: 20, Steps: 87 | Train Loss: 0.2476856 Vali Loss: 0.3398758 Test Loss: 0.4129715
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 52.06354212760925
Epoch: 21, Steps: 87 | Train Loss: 0.2475724 Vali Loss: 0.3394703 Test Loss: 0.4129268
Validation loss decreased (0.339756 --> 0.339470).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 49.970948457717896
Epoch: 22, Steps: 87 | Train Loss: 0.2475740 Vali Loss: 0.3396489 Test Loss: 0.4127039
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 50.43337821960449
Epoch: 23, Steps: 87 | Train Loss: 0.2475149 Vali Loss: 0.3397007 Test Loss: 0.4129169
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 50.86687231063843
Epoch: 24, Steps: 87 | Train Loss: 0.2474868 Vali Loss: 0.3395779 Test Loss: 0.4127724
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 50.755703926086426
Epoch: 25, Steps: 87 | Train Loss: 0.2475153 Vali Loss: 0.3399472 Test Loss: 0.4127879
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 50.902196645736694
Epoch: 26, Steps: 87 | Train Loss: 0.2475018 Vali Loss: 0.3393019 Test Loss: 0.4128769
Validation loss decreased (0.339470 --> 0.339302).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 51.66936230659485
Epoch: 27, Steps: 87 | Train Loss: 0.2474765 Vali Loss: 0.3391834 Test Loss: 0.4128489
Validation loss decreased (0.339302 --> 0.339183).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 51.11211585998535
Epoch: 28, Steps: 87 | Train Loss: 0.2475370 Vali Loss: 0.3399389 Test Loss: 0.4129221
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 51.76525592803955
Epoch: 29, Steps: 87 | Train Loss: 0.2474700 Vali Loss: 0.3394624 Test Loss: 0.4127702
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 51.04503393173218
Epoch: 30, Steps: 87 | Train Loss: 0.2474543 Vali Loss: 0.3396021 Test Loss: 0.4127125
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 50.079034090042114
Epoch: 31, Steps: 87 | Train Loss: 0.2473779 Vali Loss: 0.3397510 Test Loss: 0.4127609
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 50.85120129585266
Epoch: 32, Steps: 87 | Train Loss: 0.2473611 Vali Loss: 0.3395307 Test Loss: 0.4127532
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 52.9696581363678
Epoch: 33, Steps: 87 | Train Loss: 0.2474231 Vali Loss: 0.3392682 Test Loss: 0.4126585
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 50.335195541381836
Epoch: 34, Steps: 87 | Train Loss: 0.2473588 Vali Loss: 0.3395516 Test Loss: 0.4126820
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 50.146392583847046
Epoch: 35, Steps: 87 | Train Loss: 0.2473275 Vali Loss: 0.3397606 Test Loss: 0.4126877
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 51.5474169254303
Epoch: 36, Steps: 87 | Train Loss: 0.2472788 Vali Loss: 0.3391630 Test Loss: 0.4127557
Validation loss decreased (0.339183 --> 0.339163).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 50.8854124546051
Epoch: 37, Steps: 87 | Train Loss: 0.2473532 Vali Loss: 0.3397082 Test Loss: 0.4126626
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 51.31574749946594
Epoch: 38, Steps: 87 | Train Loss: 0.2473379 Vali Loss: 0.3393642 Test Loss: 0.4127405
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 53.532323360443115
Epoch: 39, Steps: 87 | Train Loss: 0.2472503 Vali Loss: 0.3392758 Test Loss: 0.4126782
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 51.987370014190674
Epoch: 40, Steps: 87 | Train Loss: 0.2472584 Vali Loss: 0.3394971 Test Loss: 0.4126333
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 49.7934684753418
Epoch: 41, Steps: 87 | Train Loss: 0.2473021 Vali Loss: 0.3392783 Test Loss: 0.4125788
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 50.67451453208923
Epoch: 42, Steps: 87 | Train Loss: 0.2473282 Vali Loss: 0.3391952 Test Loss: 0.4126130
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 67.09436845779419
Epoch: 43, Steps: 87 | Train Loss: 0.2473123 Vali Loss: 0.3391207 Test Loss: 0.4126100
Validation loss decreased (0.339163 --> 0.339121).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 54.96985340118408
Epoch: 44, Steps: 87 | Train Loss: 0.2472362 Vali Loss: 0.3391381 Test Loss: 0.4126311
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 58.30475878715515
Epoch: 45, Steps: 87 | Train Loss: 0.2471388 Vali Loss: 0.3391034 Test Loss: 0.4126500
Validation loss decreased (0.339121 --> 0.339103).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 53.98722815513611
Epoch: 46, Steps: 87 | Train Loss: 0.2472378 Vali Loss: 0.3391263 Test Loss: 0.4126210
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 54.27522373199463
Epoch: 47, Steps: 87 | Train Loss: 0.2472974 Vali Loss: 0.3390372 Test Loss: 0.4126040
Validation loss decreased (0.339103 --> 0.339037).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 67.61321878433228
Epoch: 48, Steps: 87 | Train Loss: 0.2471912 Vali Loss: 0.3391649 Test Loss: 0.4126357
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 52.87515926361084
Epoch: 49, Steps: 87 | Train Loss: 0.2472441 Vali Loss: 0.3391513 Test Loss: 0.4125942
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 52.80212092399597
Epoch: 50, Steps: 87 | Train Loss: 0.2472423 Vali Loss: 0.3393432 Test Loss: 0.4125336
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
>>>>>>>testing : Traffic_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4104542136192322, mae:0.2785550653934479, rse:0.5265403389930725, corr:[0.2699958  0.2842026  0.2825701  0.2837632  0.28375617 0.2842747
 0.2850477  0.28479835 0.28521323 0.28432104 0.28460938 0.284194
 0.2836734  0.2838235  0.28336892 0.28359583 0.28351426 0.2835115
 0.2837737  0.28379035 0.28387082 0.28364617 0.2832751  0.283308
 0.28490007 0.28503546 0.2847336  0.28496695 0.28475636 0.28443518
 0.28453547 0.2843559  0.28430557 0.2842877  0.28422892 0.28412774
 0.2841579  0.28426042 0.28397128 0.2840864  0.2843437  0.28432977
 0.28438774 0.28447577 0.28449702 0.28423378 0.2841607  0.28433448
 0.28473043 0.28488466 0.28460434 0.2841726  0.2840886  0.28402066
 0.28412953 0.2841254  0.28401655 0.28417462 0.2841512  0.28401455
 0.2839997  0.28420007 0.28436294 0.28439593 0.2844817  0.2842015
 0.2841541  0.284228   0.28409144 0.2840735  0.28409198 0.28415358
 0.28409213 0.28393736 0.28389862 0.28374666 0.2835186  0.2834594
 0.2835009  0.2835079  0.28367805 0.28387883 0.28383055 0.2837927
 0.28383404 0.2838326  0.2840384  0.28410938 0.28408664 0.28417668
 0.28414923 0.28411552 0.28401557 0.2837402  0.283514   0.2835182
 0.28346714 0.28341603 0.2834131  0.2835213  0.28359243 0.2836454
 0.28360862 0.28350297 0.28350207 0.28358626 0.2836561  0.28365353
 0.28362417 0.28359696 0.28348124 0.28349057 0.28352445 0.28341264
 0.28346282 0.28339624 0.28327724 0.28328368 0.28328446 0.2833852
 0.2834972  0.28373402 0.28391677 0.28395936 0.28397217 0.2840273
 0.2838938  0.28374752 0.28379616 0.28395775 0.28400236 0.283954
 0.2841598  0.28436884 0.2843123  0.28419444 0.2841673  0.28404057
 0.28388166 0.2839059  0.2837761  0.28376514 0.28388909 0.28387538
 0.28425106 0.28434056 0.28419188 0.28417006 0.284257   0.2843282
 0.28433844 0.2844895  0.28451115 0.2843638  0.28423396 0.28415638
 0.28423068 0.28441343 0.28444386 0.2843992  0.28432122 0.2842618
 0.2842197  0.28398272 0.2839542  0.28411224 0.28407395 0.28414452
 0.28543013 0.28561053 0.28537685 0.28533736 0.2852083  0.28530592
 0.28545818 0.28540283 0.28546166 0.28530788 0.28515705 0.28511396
 0.28503123 0.28509802 0.2850102  0.2848439  0.28480363 0.28481376
 0.28489655 0.28472996 0.28442916 0.28420717 0.28401455 0.28422928
 0.28503564 0.28505406 0.28494018 0.28476983 0.28482264 0.28493705
 0.28501508 0.2849778  0.28486606 0.28485024 0.2846329  0.28441876
 0.28441483 0.28436822 0.2842666  0.28414124 0.28405827 0.28390375
 0.2837843  0.28387576 0.28371167 0.2835614  0.28354278 0.28352705
 0.28390476 0.2838739  0.2837929  0.2838919  0.28393704 0.28396127
 0.28392056 0.2839876  0.284019   0.2840003  0.2840353  0.2839279
 0.2838988  0.28385147 0.28369856 0.2836902  0.2837107  0.28364438
 0.28348777 0.28340346 0.2834652  0.28332818 0.28332114 0.2834232
 0.28349453 0.28360525 0.28354833 0.28355712 0.28358313 0.28367957
 0.28373995 0.28356135 0.28371996 0.2836586  0.2834367  0.2835037
 0.283375   0.2832093  0.28295815 0.28272426 0.2828382  0.28296566
 0.28297064 0.28293744 0.282953   0.28299427 0.282973   0.28301668
 0.28291142 0.28291187 0.2830259  0.28318754 0.28331336 0.2834121
 0.28348988 0.28318805 0.28301832 0.28311545 0.2828636  0.28271127
 0.28254482 0.28243774 0.28256735 0.2825656  0.28264368 0.28245455
 0.28240138 0.28258836 0.28250775 0.2826713  0.28275174 0.28284398
 0.28302693 0.28315645 0.28332123 0.28345683 0.28353772 0.28353387
 0.2832574  0.2830944  0.28277683 0.282709   0.28277725 0.28253016
 0.28265017 0.28271496 0.2827159  0.28254557 0.28247616 0.28276026
 0.28270826 0.2829936  0.2830985  0.28318942 0.28353536 0.28334174
 0.28364664 0.2838094  0.28394458 0.2837189  0.28346163 0.28360656
 0.28291106 0.2827745  0.2826629  0.28236306 0.28276265 0.28243932
 0.2829107  0.28323323 0.2834145  0.284171   0.2836555  0.28427893
 0.28357556 0.28331414 0.28254676 0.28136647 0.2824316  0.28453597]
