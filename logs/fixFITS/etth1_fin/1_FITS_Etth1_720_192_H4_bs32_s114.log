Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10145408.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4480709
	speed: 0.1393s/iter; left time: 821.7584s
Epoch: 1 cost time: 16.713746547698975
Epoch: 1, Steps: 120 | Train Loss: 0.5933840 Vali Loss: 1.1194017 Test Loss: 0.5007890
Validation loss decreased (inf --> 1.119402).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3855556
	speed: 0.3345s/iter; left time: 1933.7231s
Epoch: 2 cost time: 13.83118486404419
Epoch: 2, Steps: 120 | Train Loss: 0.4341438 Vali Loss: 1.0137179 Test Loss: 0.4320078
Validation loss decreased (1.119402 --> 1.013718).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4004704
	speed: 0.2717s/iter; left time: 1538.0017s
Epoch: 3 cost time: 14.215232372283936
Epoch: 3, Steps: 120 | Train Loss: 0.4055929 Vali Loss: 0.9878923 Test Loss: 0.4202685
Validation loss decreased (1.013718 --> 0.987892).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3613069
	speed: 0.4085s/iter; left time: 2263.5001s
Epoch: 4 cost time: 19.420680284500122
Epoch: 4, Steps: 120 | Train Loss: 0.3969933 Vali Loss: 0.9794576 Test Loss: 0.4187230
Validation loss decreased (0.987892 --> 0.979458).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4151114
	speed: 0.4125s/iter; left time: 2236.1126s
Epoch: 5 cost time: 19.114445209503174
Epoch: 5, Steps: 120 | Train Loss: 0.3936461 Vali Loss: 0.9752618 Test Loss: 0.4188519
Validation loss decreased (0.979458 --> 0.975262).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4030890
	speed: 0.4106s/iter; left time: 2176.6850s
Epoch: 6 cost time: 19.026350021362305
Epoch: 6, Steps: 120 | Train Loss: 0.3910304 Vali Loss: 0.9720925 Test Loss: 0.4186566
Validation loss decreased (0.975262 --> 0.972093).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3885817
	speed: 0.4425s/iter; left time: 2292.5770s
Epoch: 7 cost time: 20.813435316085815
Epoch: 7, Steps: 120 | Train Loss: 0.3891871 Vali Loss: 0.9710104 Test Loss: 0.4189343
Validation loss decreased (0.972093 --> 0.971010).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3708926
	speed: 0.4441s/iter; left time: 2247.4743s
Epoch: 8 cost time: 20.42093825340271
Epoch: 8, Steps: 120 | Train Loss: 0.3881985 Vali Loss: 0.9712339 Test Loss: 0.4191359
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3790642
	speed: 0.4212s/iter; left time: 2080.9543s
Epoch: 9 cost time: 19.255043745040894
Epoch: 9, Steps: 120 | Train Loss: 0.3870226 Vali Loss: 0.9703861 Test Loss: 0.4194732
Validation loss decreased (0.971010 --> 0.970386).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3859807
	speed: 0.4100s/iter; left time: 1976.4299s
Epoch: 10 cost time: 18.9375741481781
Epoch: 10, Steps: 120 | Train Loss: 0.3863419 Vali Loss: 0.9706185 Test Loss: 0.4198445
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3686770
	speed: 0.4186s/iter; left time: 1967.8356s
Epoch: 11 cost time: 19.322290897369385
Epoch: 11, Steps: 120 | Train Loss: 0.3858676 Vali Loss: 0.9693834 Test Loss: 0.4197037
Validation loss decreased (0.970386 --> 0.969383).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3859385
	speed: 0.4007s/iter; left time: 1835.7101s
Epoch: 12 cost time: 19.25579857826233
Epoch: 12, Steps: 120 | Train Loss: 0.3847803 Vali Loss: 0.9687316 Test Loss: 0.4198504
Validation loss decreased (0.969383 --> 0.968732).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4068973
	speed: 0.3579s/iter; left time: 1596.7055s
Epoch: 13 cost time: 14.789196968078613
Epoch: 13, Steps: 120 | Train Loss: 0.3847087 Vali Loss: 0.9682784 Test Loss: 0.4198896
Validation loss decreased (0.968732 --> 0.968278).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3883656
	speed: 0.3596s/iter; left time: 1560.9403s
Epoch: 14 cost time: 19.361762285232544
Epoch: 14, Steps: 120 | Train Loss: 0.3844232 Vali Loss: 0.9682888 Test Loss: 0.4200003
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3363499
	speed: 0.3593s/iter; left time: 1516.6448s
Epoch: 15 cost time: 18.15708637237549
Epoch: 15, Steps: 120 | Train Loss: 0.3838680 Vali Loss: 0.9667197 Test Loss: 0.4199848
Validation loss decreased (0.968278 --> 0.966720).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3363059
	speed: 0.4743s/iter; left time: 1945.0303s
Epoch: 16 cost time: 22.833835124969482
Epoch: 16, Steps: 120 | Train Loss: 0.3837363 Vali Loss: 0.9668189 Test Loss: 0.4200472
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3308350
	speed: 0.4846s/iter; left time: 1929.2285s
Epoch: 17 cost time: 22.355120182037354
Epoch: 17, Steps: 120 | Train Loss: 0.3829850 Vali Loss: 0.9676139 Test Loss: 0.4200212
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3811996
	speed: 0.4435s/iter; left time: 1712.2637s
Epoch: 18 cost time: 19.776402711868286
Epoch: 18, Steps: 120 | Train Loss: 0.3833142 Vali Loss: 0.9660134 Test Loss: 0.4200118
Validation loss decreased (0.966720 --> 0.966013).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4312033
	speed: 0.4213s/iter; left time: 1576.2470s
Epoch: 19 cost time: 19.270596742630005
Epoch: 19, Steps: 120 | Train Loss: 0.3829180 Vali Loss: 0.9663989 Test Loss: 0.4198478
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3964373
	speed: 0.4241s/iter; left time: 1535.7021s
Epoch: 20 cost time: 19.701894998550415
Epoch: 20, Steps: 120 | Train Loss: 0.3825024 Vali Loss: 0.9669160 Test Loss: 0.4199650
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3413635
	speed: 0.4152s/iter; left time: 1453.4481s
Epoch: 21 cost time: 19.111627101898193
Epoch: 21, Steps: 120 | Train Loss: 0.3826054 Vali Loss: 0.9661440 Test Loss: 0.4199637
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.41819047927856445, mae:0.4259757399559021, rse:0.6141074895858765, corr:[0.25769404 0.26806524 0.26923758 0.26645523 0.26424533 0.2632039
 0.26232317 0.26138958 0.26055074 0.26041886 0.26087603 0.26114544
 0.26104185 0.26082486 0.26081085 0.26106277 0.26107216 0.2607327
 0.26010224 0.25945887 0.25916445 0.25928488 0.2592403  0.25907877
 0.2589306  0.25896433 0.2589994  0.2587356  0.25822228 0.25764495
 0.25721696 0.2569163  0.25686023 0.25701272 0.2571344  0.25729817
 0.25755003 0.25784186 0.25801155 0.25805458 0.25814164 0.25823644
 0.25825927 0.25831455 0.25848302 0.2586505  0.25882608 0.25888714
 0.25840452 0.2576497  0.2567108  0.2556873  0.25460383 0.25338057
 0.25241512 0.25199467 0.25192204 0.25196734 0.25177154 0.25142843
 0.25114146 0.2510799  0.25125045 0.25155285 0.2516843  0.25155434
 0.25132546 0.25106457 0.2510495  0.2512859  0.25154164 0.2514632
 0.25102398 0.25030765 0.24946426 0.24877562 0.2482431  0.24773982
 0.24725835 0.24687093 0.2465904  0.24638075 0.246171   0.24598943
 0.24595216 0.2459492  0.24587674 0.24569914 0.24549815 0.24542199
 0.24541973 0.24545823 0.24548163 0.24548654 0.24556278 0.24577536
 0.2461236  0.24644955 0.2465593  0.24647626 0.24617521 0.24565527
 0.2452063  0.24509853 0.24537519 0.24584794 0.24601345 0.24561563
 0.24499471 0.24453595 0.2445569  0.24502634 0.2453696  0.24523969
 0.24478862 0.24430482 0.24409236 0.2442339  0.24447152 0.244425
 0.24398834 0.24315143 0.24212158 0.24128985 0.2408045  0.24049777
 0.24034154 0.24020892 0.23981333 0.23929368 0.23872802 0.23820806
 0.23779315 0.23754366 0.23743926 0.23742542 0.23763429 0.23795223
 0.23818254 0.23808843 0.23780316 0.23743795 0.2370554  0.23651521
 0.23570171 0.23453675 0.2331749  0.2319384  0.23131718 0.23117451
 0.23122865 0.23129    0.23139119 0.2316371  0.23172879 0.23144099
 0.23070298 0.22982626 0.22931515 0.22933008 0.22951487 0.22965026
 0.22938733 0.22898644 0.2290061  0.22939925 0.22957803 0.22883593
 0.22756971 0.22677068 0.22681564 0.22730576 0.22728929 0.22653426
 0.22554849 0.22506863 0.2249278  0.2248209  0.22421575 0.22338544
 0.22291799 0.22281796 0.22230299 0.22114742 0.22050622 0.22175962
 0.22420149 0.2247006  0.22223775 0.22011758 0.22395846 0.2318796 ]
