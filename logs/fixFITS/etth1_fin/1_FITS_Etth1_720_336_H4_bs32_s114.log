Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11766272.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5720987
	speed: 0.1605s/iter; left time: 931.2493s
Epoch: 1 cost time: 18.603246688842773
Epoch: 1, Steps: 118 | Train Loss: 0.6743263 Vali Loss: 1.3276600 Test Loss: 0.5408791
Validation loss decreased (inf --> 1.327660).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5455502
	speed: 0.3815s/iter; left time: 2168.0252s
Epoch: 2 cost time: 18.475857496261597
Epoch: 2, Steps: 118 | Train Loss: 0.5117320 Vali Loss: 1.2234763 Test Loss: 0.4635989
Validation loss decreased (1.327660 --> 1.223476).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4466553
	speed: 0.4156s/iter; left time: 2312.5510s
Epoch: 3 cost time: 20.521607160568237
Epoch: 3, Steps: 118 | Train Loss: 0.4724138 Vali Loss: 1.1975071 Test Loss: 0.4421826
Validation loss decreased (1.223476 --> 1.197507).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4558567
	speed: 0.4135s/iter; left time: 2252.5546s
Epoch: 4 cost time: 18.476576566696167
Epoch: 4, Steps: 118 | Train Loss: 0.4587197 Vali Loss: 1.1888000 Test Loss: 0.4374712
Validation loss decreased (1.197507 --> 1.188800).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4573297
	speed: 0.3543s/iter; left time: 1888.0582s
Epoch: 5 cost time: 16.868276357650757
Epoch: 5, Steps: 118 | Train Loss: 0.4521974 Vali Loss: 1.1925051 Test Loss: 0.4376649
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4947010
	speed: 0.3475s/iter; left time: 1811.0000s
Epoch: 6 cost time: 16.574543952941895
Epoch: 6, Steps: 118 | Train Loss: 0.4488242 Vali Loss: 1.1932229 Test Loss: 0.4388525
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4527198
	speed: 0.3580s/iter; left time: 1823.4625s
Epoch: 7 cost time: 17.141178369522095
Epoch: 7, Steps: 118 | Train Loss: 0.4464372 Vali Loss: 1.1976931 Test Loss: 0.4394783
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4365027844905853, mae:0.44135183095932007, rse:0.6289926171302795, corr:[0.24854955 0.259728   0.25749785 0.25173873 0.2506839  0.25220802
 0.25250563 0.25161606 0.2503482  0.2499312  0.25014517 0.25003824
 0.24961182 0.24926996 0.24941884 0.24978144 0.24957086 0.24856557
 0.24740298 0.24685907 0.24711728 0.24725416 0.24701673 0.24645603
 0.24617389 0.24645717 0.24684873 0.24680759 0.24637572 0.24596684
 0.246113   0.24680053 0.24749248 0.24763131 0.24711572 0.24648653
 0.24630801 0.24650614 0.24691235 0.2473552  0.2477248  0.24803968
 0.2482308  0.24838819 0.24857508 0.24896677 0.24953485 0.24991262
 0.24980395 0.24908786 0.24790029 0.24684191 0.2461296  0.24565385
 0.24537441 0.24518749 0.24493177 0.24457186 0.24421419 0.24393193
 0.24377519 0.24378721 0.2439055  0.24413875 0.24428844 0.2443016
 0.24419037 0.24401402 0.24414991 0.24436946 0.24437368 0.24400578
 0.24338892 0.24271612 0.24216792 0.24173586 0.24119933 0.24058793
 0.24027416 0.24049333 0.24095081 0.24120657 0.24097459 0.24039236
 0.23974569 0.23927385 0.23895995 0.23871969 0.23836915 0.23807782
 0.23797849 0.23815648 0.23829196 0.23814489 0.23770055 0.23750299
 0.23779061 0.2383785  0.23897856 0.2394423  0.23979442 0.24001251
 0.24015649 0.24026506 0.24032642 0.24032392 0.24038883 0.24058488
 0.24074678 0.24072735 0.2403994  0.2399794  0.23969075 0.23969226
 0.23987378 0.23997082 0.23995028 0.23985118 0.23971608 0.23955083
 0.23920368 0.23860034 0.2380409  0.23776515 0.2376274  0.23727421
 0.23674917 0.23642306 0.23640354 0.23666075 0.23686916 0.23673749
 0.23633115 0.2359097  0.23581128 0.23600163 0.23607945 0.23590271
 0.2355924  0.23552363 0.23577534 0.23585108 0.2352847  0.23435089
 0.23345406 0.23290187 0.23272654 0.23258743 0.23238376 0.232059
 0.23188107 0.23210743 0.23255438 0.23277399 0.23258144 0.23217699
 0.23171698 0.23130605 0.23101135 0.23094212 0.23098055 0.23120868
 0.23128544 0.23097019 0.23029949 0.22962682 0.22916538 0.22912686
 0.22931485 0.2295166  0.22966845 0.22998555 0.23043218 0.23073955
 0.23061292 0.23021774 0.22981551 0.22973922 0.22985029 0.22976898
 0.22922274 0.22855672 0.22842759 0.2290753  0.2299728  0.23043725
 0.23029615 0.23005512 0.23029375 0.2309038  0.23123495 0.23082864
 0.2297696  0.2286841  0.22809692 0.22794077 0.2278753  0.22770967
 0.22760686 0.22784284 0.22818607 0.22810197 0.22746822 0.22677206
 0.22643705 0.22661917 0.22693506 0.22698791 0.22687115 0.22689769
 0.2271452  0.22716233 0.22667024 0.22575776 0.22488847 0.2245791
 0.22472684 0.2248406  0.2246107  0.2242294  0.22412784 0.22427791
 0.22439453 0.22430047 0.22406514 0.22395802 0.22402817 0.22422372
 0.22417262 0.22388074 0.22386026 0.22441137 0.22491717 0.22475442
 0.22374693 0.22245595 0.22185832 0.22234203 0.2233256  0.22375688
 0.22351116 0.22299083 0.22294952 0.22351201 0.22404855 0.22415178
 0.22399427 0.22405931 0.2243023  0.2242524  0.22363691 0.22280584
 0.22225372 0.22210203 0.22216313 0.2219114  0.22133623 0.22090012
 0.22107466 0.22154553 0.22190996 0.22180633 0.22137724 0.22089599
 0.22066641 0.22047424 0.22003244 0.21952699 0.21912973 0.2190097
 0.2189607  0.21846986 0.21760562 0.2171182  0.21723117 0.21752577
 0.21725214 0.21634287 0.21550567 0.21567652 0.21665944 0.21733302
 0.21658632 0.21460119 0.21298029 0.21279949 0.21369317 0.21427019
 0.21372522 0.21283534 0.21279262 0.2139296  0.21482354 0.21464002
 0.21372114 0.21319355 0.2135931  0.2140834  0.2133629  0.21162586
 0.21045548 0.2110622  0.2130386  0.21430111 0.21396387 0.21279617
 0.21261011 0.2134143  0.21411264 0.21317285 0.21088165 0.20896244
 0.20894393 0.20995586 0.20975232 0.2078045  0.20564716 0.20472474
 0.2050568  0.20488024 0.20358516 0.20229544 0.20234135 0.20352352
 0.20379145 0.20154656 0.198748   0.19886987 0.20131898 0.20237821
 0.19816814 0.19011392 0.18557665 0.18914434 0.19130266 0.16640314]
