Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16088576.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5955833
	speed: 0.1285s/iter; left time: 707.0246s
Epoch: 1 cost time: 14.56146502494812
Epoch: 1, Steps: 112 | Train Loss: 0.7489361 Vali Loss: 1.9080446 Test Loss: 0.7857121
Validation loss decreased (inf --> 1.908045).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5259688
	speed: 0.3184s/iter; left time: 1716.0698s
Epoch: 2 cost time: 16.032538890838623
Epoch: 2, Steps: 112 | Train Loss: 0.5556169 Vali Loss: 1.7623098 Test Loss: 0.6976725
Validation loss decreased (1.908045 --> 1.762310).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4536111
	speed: 0.3176s/iter; left time: 1675.8414s
Epoch: 3 cost time: 16.097543239593506
Epoch: 3, Steps: 112 | Train Loss: 0.4913628 Vali Loss: 1.7051120 Test Loss: 0.6615211
Validation loss decreased (1.762310 --> 1.705112).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4410161
	speed: 0.2505s/iter; left time: 1293.9644s
Epoch: 4 cost time: 8.914403200149536
Epoch: 4, Steps: 112 | Train Loss: 0.4551065 Vali Loss: 1.6653267 Test Loss: 0.6347145
Validation loss decreased (1.705112 --> 1.665327).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3874029
	speed: 0.1796s/iter; left time: 907.5150s
Epoch: 5 cost time: 12.833521842956543
Epoch: 5, Steps: 112 | Train Loss: 0.4287116 Vali Loss: 1.6407700 Test Loss: 0.6123024
Validation loss decreased (1.665327 --> 1.640770).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3968870
	speed: 0.2981s/iter; left time: 1472.8480s
Epoch: 6 cost time: 14.953911781311035
Epoch: 6, Steps: 112 | Train Loss: 0.4081781 Vali Loss: 1.6111982 Test Loss: 0.5935604
Validation loss decreased (1.640770 --> 1.611198).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4036186
	speed: 0.2958s/iter; left time: 1428.5026s
Epoch: 7 cost time: 14.732620000839233
Epoch: 7, Steps: 112 | Train Loss: 0.3920840 Vali Loss: 1.5923511 Test Loss: 0.5757790
Validation loss decreased (1.611198 --> 1.592351).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3844520
	speed: 0.2948s/iter; left time: 1390.4414s
Epoch: 8 cost time: 14.706386804580688
Epoch: 8, Steps: 112 | Train Loss: 0.3786314 Vali Loss: 1.5737956 Test Loss: 0.5616491
Validation loss decreased (1.592351 --> 1.573796).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3570656
	speed: 0.2845s/iter; left time: 1310.3436s
Epoch: 9 cost time: 14.620289325714111
Epoch: 9, Steps: 112 | Train Loss: 0.3679278 Vali Loss: 1.5585333 Test Loss: 0.5474588
Validation loss decreased (1.573796 --> 1.558533).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3509681
	speed: 0.2937s/iter; left time: 1319.6870s
Epoch: 10 cost time: 14.640016794204712
Epoch: 10, Steps: 112 | Train Loss: 0.3589564 Vali Loss: 1.5460974 Test Loss: 0.5363773
Validation loss decreased (1.558533 --> 1.546097).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3268884
	speed: 0.2886s/iter; left time: 1264.4567s
Epoch: 11 cost time: 14.247411012649536
Epoch: 11, Steps: 112 | Train Loss: 0.3512053 Vali Loss: 1.5307502 Test Loss: 0.5264516
Validation loss decreased (1.546097 --> 1.530750).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3399445
	speed: 0.2868s/iter; left time: 1224.2437s
Epoch: 12 cost time: 14.580203294754028
Epoch: 12, Steps: 112 | Train Loss: 0.3449261 Vali Loss: 1.5258009 Test Loss: 0.5170337
Validation loss decreased (1.530750 --> 1.525801).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3349012
	speed: 0.2875s/iter; left time: 1194.9882s
Epoch: 13 cost time: 14.5263671875
Epoch: 13, Steps: 112 | Train Loss: 0.3393034 Vali Loss: 1.5111341 Test Loss: 0.5091045
Validation loss decreased (1.525801 --> 1.511134).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3244770
	speed: 0.3033s/iter; left time: 1226.6464s
Epoch: 14 cost time: 15.771239519119263
Epoch: 14, Steps: 112 | Train Loss: 0.3347969 Vali Loss: 1.5031146 Test Loss: 0.5014604
Validation loss decreased (1.511134 --> 1.503115).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3133758
	speed: 0.2248s/iter; left time: 884.2683s
Epoch: 15 cost time: 8.296614646911621
Epoch: 15, Steps: 112 | Train Loss: 0.3305935 Vali Loss: 1.4982474 Test Loss: 0.4958845
Validation loss decreased (1.503115 --> 1.498247).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3323597
	speed: 0.1590s/iter; left time: 607.6735s
Epoch: 16 cost time: 7.818225383758545
Epoch: 16, Steps: 112 | Train Loss: 0.3269333 Vali Loss: 1.4937601 Test Loss: 0.4903983
Validation loss decreased (1.498247 --> 1.493760).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3220665
	speed: 0.2743s/iter; left time: 1017.4108s
Epoch: 17 cost time: 14.635184049606323
Epoch: 17, Steps: 112 | Train Loss: 0.3236917 Vali Loss: 1.4898372 Test Loss: 0.4854560
Validation loss decreased (1.493760 --> 1.489837).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3233037
	speed: 0.2750s/iter; left time: 989.0077s
Epoch: 18 cost time: 13.737561464309692
Epoch: 18, Steps: 112 | Train Loss: 0.3211038 Vali Loss: 1.4823558 Test Loss: 0.4811729
Validation loss decreased (1.489837 --> 1.482356).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3108592
	speed: 0.2798s/iter; left time: 974.9376s
Epoch: 19 cost time: 13.866084575653076
Epoch: 19, Steps: 112 | Train Loss: 0.3186132 Vali Loss: 1.4787600 Test Loss: 0.4772158
Validation loss decreased (1.482356 --> 1.478760).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3411830
	speed: 0.2693s/iter; left time: 908.4658s
Epoch: 20 cost time: 13.620497703552246
Epoch: 20, Steps: 112 | Train Loss: 0.3163995 Vali Loss: 1.4738923 Test Loss: 0.4737188
Validation loss decreased (1.478760 --> 1.473892).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3372481
	speed: 0.2632s/iter; left time: 858.2761s
Epoch: 21 cost time: 13.48793911933899
Epoch: 21, Steps: 112 | Train Loss: 0.3145767 Vali Loss: 1.4705760 Test Loss: 0.4704137
Validation loss decreased (1.473892 --> 1.470576).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3133945
	speed: 0.2675s/iter; left time: 842.2257s
Epoch: 22 cost time: 13.069676160812378
Epoch: 22, Steps: 112 | Train Loss: 0.3127450 Vali Loss: 1.4716974 Test Loss: 0.4677093
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3249013
	speed: 0.2718s/iter; left time: 825.5594s
Epoch: 23 cost time: 13.5488760471344
Epoch: 23, Steps: 112 | Train Loss: 0.3111469 Vali Loss: 1.4662884 Test Loss: 0.4654088
Validation loss decreased (1.470576 --> 1.466288).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3170854
	speed: 0.2289s/iter; left time: 669.6534s
Epoch: 24 cost time: 10.236963987350464
Epoch: 24, Steps: 112 | Train Loss: 0.3098302 Vali Loss: 1.4693325 Test Loss: 0.4629701
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3107190
	speed: 0.2258s/iter; left time: 635.2768s
Epoch: 25 cost time: 13.51477575302124
Epoch: 25, Steps: 112 | Train Loss: 0.3084514 Vali Loss: 1.4662923 Test Loss: 0.4608502
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2945723
	speed: 0.2684s/iter; left time: 724.8948s
Epoch: 26 cost time: 13.445005655288696
Epoch: 26, Steps: 112 | Train Loss: 0.3074855 Vali Loss: 1.4646900 Test Loss: 0.4590824
Validation loss decreased (1.466288 --> 1.464690).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2861999
	speed: 0.2652s/iter; left time: 686.6088s
Epoch: 27 cost time: 13.338820457458496
Epoch: 27, Steps: 112 | Train Loss: 0.3064593 Vali Loss: 1.4624944 Test Loss: 0.4575044
Validation loss decreased (1.464690 --> 1.462494).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2924762
	speed: 0.2293s/iter; left time: 568.0936s
Epoch: 28 cost time: 11.943938493728638
Epoch: 28, Steps: 112 | Train Loss: 0.3054157 Vali Loss: 1.4587504 Test Loss: 0.4559594
Validation loss decreased (1.462494 --> 1.458750).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3107890
	speed: 0.2494s/iter; left time: 589.7213s
Epoch: 29 cost time: 14.59060525894165
Epoch: 29, Steps: 112 | Train Loss: 0.3046777 Vali Loss: 1.4573046 Test Loss: 0.4546463
Validation loss decreased (1.458750 --> 1.457305).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3060926
	speed: 0.2940s/iter; left time: 662.4572s
Epoch: 30 cost time: 13.582279682159424
Epoch: 30, Steps: 112 | Train Loss: 0.3037284 Vali Loss: 1.4597070 Test Loss: 0.4534856
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2921967
	speed: 0.2850s/iter; left time: 610.1692s
Epoch: 31 cost time: 13.76705813407898
Epoch: 31, Steps: 112 | Train Loss: 0.3030174 Vali Loss: 1.4578816 Test Loss: 0.4522632
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3138577
	speed: 0.2623s/iter; left time: 532.2341s
Epoch: 32 cost time: 13.330504179000854
Epoch: 32, Steps: 112 | Train Loss: 0.3024332 Vali Loss: 1.4554241 Test Loss: 0.4512490
Validation loss decreased (1.457305 --> 1.455424).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2754685
	speed: 0.2504s/iter; left time: 479.9739s
Epoch: 33 cost time: 12.55695128440857
Epoch: 33, Steps: 112 | Train Loss: 0.3018961 Vali Loss: 1.4542266 Test Loss: 0.4502634
Validation loss decreased (1.455424 --> 1.454227).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3127320
	speed: 0.2662s/iter; left time: 480.5466s
Epoch: 34 cost time: 13.27211880683899
Epoch: 34, Steps: 112 | Train Loss: 0.3014475 Vali Loss: 1.4580097 Test Loss: 0.4494111
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3156471
	speed: 0.2618s/iter; left time: 443.2635s
Epoch: 35 cost time: 13.148359775543213
Epoch: 35, Steps: 112 | Train Loss: 0.3008238 Vali Loss: 1.4508913 Test Loss: 0.4486811
Validation loss decreased (1.454227 --> 1.450891).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2881724
	speed: 0.2658s/iter; left time: 420.1653s
Epoch: 36 cost time: 13.313528537750244
Epoch: 36, Steps: 112 | Train Loss: 0.3003186 Vali Loss: 1.4528233 Test Loss: 0.4480288
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2936630
	speed: 0.2470s/iter; left time: 362.8906s
Epoch: 37 cost time: 11.67075800895691
Epoch: 37, Steps: 112 | Train Loss: 0.3000336 Vali Loss: 1.4533730 Test Loss: 0.4474077
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3120084
	speed: 0.2523s/iter; left time: 342.3598s
Epoch: 38 cost time: 11.862578392028809
Epoch: 38, Steps: 112 | Train Loss: 0.2994769 Vali Loss: 1.4526228 Test Loss: 0.4468334
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16088576.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5614779
	speed: 0.0924s/iter; left time: 508.4639s
Epoch: 1 cost time: 10.32803463935852
Epoch: 1, Steps: 112 | Train Loss: 0.5570540 Vali Loss: 1.4413786 Test Loss: 0.4384743
Validation loss decreased (inf --> 1.441379).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5576450
	speed: 0.2249s/iter; left time: 1212.0516s
Epoch: 2 cost time: 13.310892343521118
Epoch: 2, Steps: 112 | Train Loss: 0.5533568 Vali Loss: 1.4390465 Test Loss: 0.4379940
Validation loss decreased (1.441379 --> 1.439047).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5465168
	speed: 0.2921s/iter; left time: 1541.3143s
Epoch: 3 cost time: 14.795461177825928
Epoch: 3, Steps: 112 | Train Loss: 0.5526817 Vali Loss: 1.4420931 Test Loss: 0.4382595
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5449497
	speed: 0.2952s/iter; left time: 1524.7336s
Epoch: 4 cost time: 14.179987907409668
Epoch: 4, Steps: 112 | Train Loss: 0.5519228 Vali Loss: 1.4387672 Test Loss: 0.4384634
Validation loss decreased (1.439047 --> 1.438767).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5255132
	speed: 0.2433s/iter; left time: 1229.5466s
Epoch: 5 cost time: 11.280619144439697
Epoch: 5, Steps: 112 | Train Loss: 0.5514722 Vali Loss: 1.4450958 Test Loss: 0.4383859
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5172067
	speed: 0.2646s/iter; left time: 1307.4491s
Epoch: 6 cost time: 15.485851764678955
Epoch: 6, Steps: 112 | Train Loss: 0.5513439 Vali Loss: 1.4448617 Test Loss: 0.4386205
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5640573
	speed: 0.3078s/iter; left time: 1486.6030s
Epoch: 7 cost time: 14.750112533569336
Epoch: 7, Steps: 112 | Train Loss: 0.5513334 Vali Loss: 1.4456631 Test Loss: 0.4388812
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43728959560394287, mae:0.45862719416618347, rse:0.633047878742218, corr:[0.22772968 0.2346789  0.23570581 0.23250839 0.22991708 0.2289977
 0.22913921 0.22930734 0.22893487 0.22886913 0.22920518 0.22964452
 0.22994576 0.22993429 0.22973233 0.22949563 0.22911242 0.2285164
 0.22777866 0.22730926 0.22738282 0.22796437 0.22848253 0.22876061
 0.22878887 0.22879744 0.22890459 0.22891612 0.22888379 0.22884369
 0.2288634  0.22881366 0.22870609 0.22847581 0.22813253 0.22788218
 0.2278902  0.22791383 0.22786164 0.22777493 0.22780542 0.2277905
 0.22774278 0.22783752 0.2281888  0.22875354 0.2294084  0.22976065
 0.22947448 0.2288677  0.22806986 0.22729543 0.2266727  0.22589557
 0.22498083 0.22408502 0.22365367 0.22347811 0.22314443 0.22265886
 0.22204973 0.22146586 0.22105592 0.22104065 0.22121307 0.22135046
 0.22138524 0.22132751 0.22123535 0.22105806 0.22061335 0.22012517
 0.21961075 0.21921447 0.21897924 0.21890076 0.2187836  0.21845125
 0.21802618 0.21766214 0.21731944 0.21697828 0.21654767 0.21609937
 0.21582809 0.21572907 0.21572244 0.21571788 0.21567613 0.21573184
 0.21589594 0.2163091  0.21673231 0.21707402 0.21746588 0.21814844
 0.21917292 0.22024532 0.2209628  0.22126567 0.22111908 0.22070912
 0.2203175  0.22023752 0.22042254 0.22056723 0.22037983 0.2199034
 0.2194618  0.21915329 0.21903789 0.21917999 0.21941534 0.21946518
 0.21935113 0.21914001 0.21894439 0.21881022 0.21878229 0.21882531
 0.21872145 0.21817966 0.21722086 0.21635784 0.2157877  0.21559942
 0.21574551 0.21609974 0.21623755 0.21607046 0.21565351 0.21509878
 0.21461652 0.21431364 0.21435012 0.21471591 0.2152444  0.21568482
 0.21591705 0.21594061 0.2158858  0.21574841 0.21545543 0.21508108
 0.2145369  0.21372133 0.21288165 0.2122771  0.21214403 0.21219905
 0.21210483 0.21188559 0.21160279 0.21138868 0.2112738  0.21118793
 0.21101314 0.2107422  0.21065646 0.21096972 0.21143717 0.21186468
 0.21198668 0.21195537 0.21196729 0.21217899 0.21237123 0.21251756
 0.21260779 0.21270482 0.21290079 0.21336488 0.21373098 0.21363282
 0.21304995 0.21237768 0.21188506 0.21169385 0.21154997 0.211311
 0.21104862 0.21095927 0.2111767  0.2116011  0.21195704 0.2120871
 0.2121284  0.21234567 0.21282423 0.21315119 0.21307339 0.21261007
 0.21199076 0.21153684 0.21134451 0.21130979 0.21120153 0.21090414
 0.2105298  0.21037157 0.21036081 0.210275   0.21002363 0.20984708
 0.20996724 0.21045803 0.21105734 0.21137659 0.21124929 0.21074487
 0.21009456 0.20950805 0.2092252  0.20913376 0.20918696 0.2093192
 0.20927575 0.208931   0.20826292 0.20761336 0.20733891 0.20737903
 0.20754257 0.20747969 0.20701036 0.20633915 0.20588629 0.20593458
 0.2063182  0.20654501 0.20644224 0.20613085 0.20581314 0.20570415
 0.20576552 0.20578155 0.20567773 0.20549932 0.20555538 0.20588909
 0.20643099 0.20684801 0.20695172 0.20681192 0.206574   0.20642063
 0.20650288 0.20670837 0.20681165 0.20669831 0.20643924 0.20611513
 0.20575884 0.2052408  0.2047637  0.20454465 0.20478256 0.20537262
 0.20596813 0.20617045 0.20614217 0.20594433 0.20566088 0.20546481
 0.20540632 0.20536672 0.20517248 0.20473841 0.20407492 0.2035054
 0.20327404 0.20333308 0.20338033 0.20342433 0.20343135 0.20346352
 0.20350565 0.20355204 0.20347089 0.20331997 0.20322612 0.20335262
 0.20355943 0.20375308 0.20384823 0.2037936  0.20375143 0.20395947
 0.20440061 0.20492268 0.20528045 0.20540863 0.20527217 0.20513219
 0.2052126  0.20559217 0.20602114 0.206288   0.2062966  0.20609957
 0.20577501 0.20530866 0.20494553 0.20486873 0.20517224 0.20580441
 0.20649964 0.20675693 0.20655963 0.2061669  0.20592679 0.20619816
 0.20678174 0.2070691  0.20660913 0.20566641 0.20471795 0.20411342
 0.20385934 0.20372637 0.20343201 0.20297806 0.2026906  0.2028985
 0.20347181 0.20403658 0.20435095 0.20463619 0.20476411 0.2048691
 0.20489837 0.20477986 0.20464556 0.20458099 0.20459683 0.20454438
 0.20437394 0.20391066 0.20313229 0.20218134 0.20128591 0.20064855
 0.20044382 0.20057891 0.20081384 0.20110247 0.20141767 0.20177476
 0.20199402 0.2020515  0.20188506 0.20149104 0.20091599 0.20069885
 0.20099396 0.20156786 0.2021116  0.20240831 0.20224987 0.20195535
 0.20171359 0.20137888 0.20079568 0.19995502 0.19905345 0.19841099
 0.19825897 0.1985734  0.19898267 0.19916652 0.1990828  0.19886789
 0.19845821 0.197758   0.19691195 0.19620983 0.19611749 0.19668484
 0.19745602 0.19774288 0.19734909 0.19668472 0.19620311 0.19648868
 0.19736713 0.19823977 0.1986362  0.19860066 0.19839689 0.19800983
 0.19755782 0.19712164 0.19677539 0.19665147 0.19667049 0.196873
 0.19705795 0.19705796 0.19699964 0.19690616 0.19679782 0.19690916
 0.19727518 0.19785617 0.19856693 0.19908567 0.19908364 0.19880733
 0.19869892 0.19902784 0.19961338 0.19995233 0.1996448  0.19869563
 0.19790979 0.19794254 0.19881868 0.19985904 0.20043974 0.20034471
 0.19975986 0.19905777 0.19827172 0.19759566 0.19719382 0.1972977
 0.19788885 0.19872689 0.19933198 0.19996032 0.20042944 0.20062071
 0.20037019 0.19943619 0.19808954 0.19723505 0.19732696 0.19811721
 0.19890735 0.19922638 0.19887945 0.19845235 0.1984711  0.19889553
 0.19905996 0.19847412 0.19742028 0.1965899  0.19650461 0.1971835
 0.19812898 0.19878851 0.19917049 0.19962133 0.2002824  0.2011375
 0.20164615 0.20149942 0.2008254  0.20010588 0.19958329 0.19912344
 0.19872096 0.19844706 0.19840448 0.19879505 0.19939706 0.19992305
 0.20007049 0.19980739 0.19931543 0.19886358 0.19860996 0.19868302
 0.19896404 0.19925375 0.19932434 0.19900782 0.19872212 0.19857354
 0.19857141 0.19863851 0.19855297 0.19840632 0.19821523 0.19786489
 0.19738112 0.19693302 0.19647403 0.19635105 0.19673565 0.19749908
 0.1981774  0.19819686 0.19753511 0.19664164 0.19618906 0.1964953
 0.1973833  0.19830425 0.19888163 0.1991276  0.19932874 0.19965692
 0.19989224 0.1998934  0.1995014  0.19898437 0.19864284 0.1984046
 0.19820957 0.19800718 0.19752264 0.19675022 0.19619505 0.19639105
 0.19722316 0.19817339 0.1988835  0.19894548 0.19859752 0.19824311
 0.19831827 0.198906   0.19983748 0.20038822 0.2007161  0.20037611
 0.19981334 0.19907852 0.19829184 0.19786836 0.19770414 0.19787931
 0.1981917  0.1984378  0.19819166 0.19768049 0.19736817 0.19759314
 0.19801198 0.19799486 0.19756444 0.19714199 0.19746777 0.1985446
 0.19971572 0.20016046 0.19960509 0.19856632 0.19785644 0.19788742
 0.19825728 0.19828923 0.19735043 0.19570391 0.19422641 0.19366638
 0.19401728 0.19463466 0.19468078 0.1939542  0.19280736 0.19182317
 0.19110264 0.1903512  0.18959424 0.18869008 0.18814763 0.18819268
 0.18883017 0.18955547 0.19038188 0.19123134 0.19217642 0.19295587
 0.1930797  0.19221653 0.19036894 0.18874542 0.18811588 0.18842275
 0.18923251 0.18965514 0.18937927 0.1886936  0.18830922 0.18816175
 0.18784182 0.18672837 0.18542583 0.18473116 0.18505083 0.18578054
 0.18604298 0.18564074 0.18522827 0.18545675 0.18611042 0.1863533
 0.18561217 0.18400982 0.18241018 0.1816418  0.1817227  0.18189275
 0.18166691 0.18112779 0.18055078 0.18029125 0.18034826 0.18054451
 0.18060881 0.18029025 0.1795741  0.1785311  0.17761537 0.17740376
 0.17818274 0.17959535 0.18094441 0.18163648 0.18174703 0.18161006
 0.18134522 0.18073876 0.1795213  0.17825319 0.17759153 0.17779699
 0.17858234 0.179144   0.17888075 0.17833018 0.17846431 0.17960994
 0.18063606 0.1800459  0.17786962 0.17538601 0.17390946 0.17378838
 0.17429651 0.17483175 0.17530407 0.17600755 0.17701922 0.17762062
 0.1768498  0.17497273 0.1732233  0.17293368 0.17366704 0.17428052
 0.17404605 0.17323568 0.17258611 0.17205639 0.17132884 0.17024265
 0.16920204 0.16866486 0.16890173 0.16929068 0.16933374 0.16914026
 0.1694372  0.17066424 0.17212996 0.17274073 0.1718084  0.16993633
 0.16799313 0.1663841  0.16467431 0.16265121 0.16085209 0.15970764
 0.15993704 0.16068481 0.16083714 0.15991716 0.1592944  0.15979096
 0.16041456 0.15955235 0.15757264 0.15602526 0.15643121 0.15874796
 0.16136615 0.16262136 0.1623323  0.16151083 0.1610214  0.16085874
 0.16033249 0.15965782 0.15895958 0.1588979  0.15876596 0.15794404
 0.15697163 0.15644833 0.15644358 0.15623532 0.15545706 0.15463457
 0.15503861 0.15624735 0.15687199 0.15540078 0.1534524  0.15464608
 0.16060607 0.16682254 0.16698724 0.16228423 0.16368994 0.17877828]
