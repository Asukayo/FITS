Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  128708608.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.537452220916748
Epoch: 1, Steps: 14 | Train Loss: 0.9256462 Vali Loss: 2.4495986 Test Loss: 1.0916512
Validation loss decreased (inf --> 2.449599).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.3734920024871826
Epoch: 2, Steps: 14 | Train Loss: 0.8456289 Vali Loss: 2.3124969 Test Loss: 1.0183041
Validation loss decreased (2.449599 --> 2.312497).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.5275914669036865
Epoch: 3, Steps: 14 | Train Loss: 0.7834414 Vali Loss: 2.2002532 Test Loss: 0.9603240
Validation loss decreased (2.312497 --> 2.200253).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.527308702468872
Epoch: 4, Steps: 14 | Train Loss: 0.7346484 Vali Loss: 2.1184695 Test Loss: 0.9150004
Validation loss decreased (2.200253 --> 2.118469).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.718017339706421
Epoch: 5, Steps: 14 | Train Loss: 0.6969472 Vali Loss: 2.0519989 Test Loss: 0.8790377
Validation loss decreased (2.118469 --> 2.051999).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.4610631465911865
Epoch: 6, Steps: 14 | Train Loss: 0.6669503 Vali Loss: 2.0006101 Test Loss: 0.8503541
Validation loss decreased (2.051999 --> 2.000610).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.607633113861084
Epoch: 7, Steps: 14 | Train Loss: 0.6428027 Vali Loss: 1.9617451 Test Loss: 0.8275563
Validation loss decreased (2.000610 --> 1.961745).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.292741537094116
Epoch: 8, Steps: 14 | Train Loss: 0.6229234 Vali Loss: 1.9260308 Test Loss: 0.8093136
Validation loss decreased (1.961745 --> 1.926031).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.3738064765930176
Epoch: 9, Steps: 14 | Train Loss: 0.6065858 Vali Loss: 1.9003458 Test Loss: 0.7944342
Validation loss decreased (1.926031 --> 1.900346).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.298316240310669
Epoch: 10, Steps: 14 | Train Loss: 0.5929705 Vali Loss: 1.8764508 Test Loss: 0.7822585
Validation loss decreased (1.900346 --> 1.876451).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.277362585067749
Epoch: 11, Steps: 14 | Train Loss: 0.5815042 Vali Loss: 1.8550632 Test Loss: 0.7721924
Validation loss decreased (1.876451 --> 1.855063).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.054027795791626
Epoch: 12, Steps: 14 | Train Loss: 0.5715660 Vali Loss: 1.8396087 Test Loss: 0.7637655
Validation loss decreased (1.855063 --> 1.839609).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.973844289779663
Epoch: 13, Steps: 14 | Train Loss: 0.5631446 Vali Loss: 1.8327250 Test Loss: 0.7564629
Validation loss decreased (1.839609 --> 1.832725).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.821350336074829
Epoch: 14, Steps: 14 | Train Loss: 0.5558377 Vali Loss: 1.8238869 Test Loss: 0.7502521
Validation loss decreased (1.832725 --> 1.823887).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.230614423751831
Epoch: 15, Steps: 14 | Train Loss: 0.5491750 Vali Loss: 1.8109043 Test Loss: 0.7450787
Validation loss decreased (1.823887 --> 1.810904).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.8469197750091553
Epoch: 16, Steps: 14 | Train Loss: 0.5434261 Vali Loss: 1.7998384 Test Loss: 0.7403554
Validation loss decreased (1.810904 --> 1.799838).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.7418246269226074
Epoch: 17, Steps: 14 | Train Loss: 0.5384078 Vali Loss: 1.7903210 Test Loss: 0.7364804
Validation loss decreased (1.799838 --> 1.790321).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.8432424068450928
Epoch: 18, Steps: 14 | Train Loss: 0.5338403 Vali Loss: 1.7854548 Test Loss: 0.7331231
Validation loss decreased (1.790321 --> 1.785455).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.474038600921631
Epoch: 19, Steps: 14 | Train Loss: 0.5297734 Vali Loss: 1.7808677 Test Loss: 0.7298166
Validation loss decreased (1.785455 --> 1.780868).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.8373448848724365
Epoch: 20, Steps: 14 | Train Loss: 0.5258633 Vali Loss: 1.7708139 Test Loss: 0.7270769
Validation loss decreased (1.780868 --> 1.770814).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.7630672454833984
Epoch: 21, Steps: 14 | Train Loss: 0.5225967 Vali Loss: 1.7704411 Test Loss: 0.7245610
Validation loss decreased (1.770814 --> 1.770441).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.668708086013794
Epoch: 22, Steps: 14 | Train Loss: 0.5194758 Vali Loss: 1.7678515 Test Loss: 0.7223327
Validation loss decreased (1.770441 --> 1.767851).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.3923797607421875
Epoch: 23, Steps: 14 | Train Loss: 0.5168879 Vali Loss: 1.7654369 Test Loss: 0.7203762
Validation loss decreased (1.767851 --> 1.765437).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.204643487930298
Epoch: 24, Steps: 14 | Train Loss: 0.5142455 Vali Loss: 1.7576638 Test Loss: 0.7185093
Validation loss decreased (1.765437 --> 1.757664).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.269599199295044
Epoch: 25, Steps: 14 | Train Loss: 0.5118014 Vali Loss: 1.7540379 Test Loss: 0.7166970
Validation loss decreased (1.757664 --> 1.754038).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.348391056060791
Epoch: 26, Steps: 14 | Train Loss: 0.5095720 Vali Loss: 1.7533174 Test Loss: 0.7151580
Validation loss decreased (1.754038 --> 1.753317).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.5426032543182373
Epoch: 27, Steps: 14 | Train Loss: 0.5078939 Vali Loss: 1.7522593 Test Loss: 0.7137197
Validation loss decreased (1.753317 --> 1.752259).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.4442615509033203
Epoch: 28, Steps: 14 | Train Loss: 0.5056469 Vali Loss: 1.7427311 Test Loss: 0.7123383
Validation loss decreased (1.752259 --> 1.742731).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.416564464569092
Epoch: 29, Steps: 14 | Train Loss: 0.5037966 Vali Loss: 1.7431610 Test Loss: 0.7111899
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.8268697261810303
Epoch: 30, Steps: 14 | Train Loss: 0.5023553 Vali Loss: 1.7450645 Test Loss: 0.7101153
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.8934614658355713
Epoch: 31, Steps: 14 | Train Loss: 0.5008689 Vali Loss: 1.7388117 Test Loss: 0.7089249
Validation loss decreased (1.742731 --> 1.738812).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.9594430923461914
Epoch: 32, Steps: 14 | Train Loss: 0.4995574 Vali Loss: 1.7417153 Test Loss: 0.7078501
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.9177002906799316
Epoch: 33, Steps: 14 | Train Loss: 0.4980026 Vali Loss: 1.7378886 Test Loss: 0.7069290
Validation loss decreased (1.738812 --> 1.737889).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.549320697784424
Epoch: 34, Steps: 14 | Train Loss: 0.4968442 Vali Loss: 1.7356374 Test Loss: 0.7060552
Validation loss decreased (1.737889 --> 1.735637).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.5383546352386475
Epoch: 35, Steps: 14 | Train Loss: 0.4956497 Vali Loss: 1.7301445 Test Loss: 0.7052592
Validation loss decreased (1.735637 --> 1.730145).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.669060230255127
Epoch: 36, Steps: 14 | Train Loss: 0.4944892 Vali Loss: 1.7260690 Test Loss: 0.7044454
Validation loss decreased (1.730145 --> 1.726069).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.420210599899292
Epoch: 37, Steps: 14 | Train Loss: 0.4933204 Vali Loss: 1.7344711 Test Loss: 0.7037132
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.4882285594940186
Epoch: 38, Steps: 14 | Train Loss: 0.4925451 Vali Loss: 1.7289507 Test Loss: 0.7030480
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.513596773147583
Epoch: 39, Steps: 14 | Train Loss: 0.4916066 Vali Loss: 1.7303078 Test Loss: 0.7023573
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  128708608.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.450127124786377
Epoch: 1, Steps: 14 | Train Loss: 0.7053651 Vali Loss: 1.7019327 Test Loss: 0.6839398
Validation loss decreased (inf --> 1.701933).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.385110378265381
Epoch: 2, Steps: 14 | Train Loss: 0.6909394 Vali Loss: 1.6784232 Test Loss: 0.6664567
Validation loss decreased (1.701933 --> 1.678423).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.032092332839966
Epoch: 3, Steps: 14 | Train Loss: 0.6801565 Vali Loss: 1.6595902 Test Loss: 0.6513061
Validation loss decreased (1.678423 --> 1.659590).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.96440052986145
Epoch: 4, Steps: 14 | Train Loss: 0.6703077 Vali Loss: 1.6390357 Test Loss: 0.6380190
Validation loss decreased (1.659590 --> 1.639036).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.8578507900238037
Epoch: 5, Steps: 14 | Train Loss: 0.6623988 Vali Loss: 1.6302675 Test Loss: 0.6262836
Validation loss decreased (1.639036 --> 1.630268).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.729205846786499
Epoch: 6, Steps: 14 | Train Loss: 0.6556059 Vali Loss: 1.6174657 Test Loss: 0.6157382
Validation loss decreased (1.630268 --> 1.617466).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.964365243911743
Epoch: 7, Steps: 14 | Train Loss: 0.6493293 Vali Loss: 1.6040967 Test Loss: 0.6062273
Validation loss decreased (1.617466 --> 1.604097).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.960439443588257
Epoch: 8, Steps: 14 | Train Loss: 0.6439409 Vali Loss: 1.5955429 Test Loss: 0.5976048
Validation loss decreased (1.604097 --> 1.595543).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.778010129928589
Epoch: 9, Steps: 14 | Train Loss: 0.6390857 Vali Loss: 1.5869428 Test Loss: 0.5894231
Validation loss decreased (1.595543 --> 1.586943).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.7172417640686035
Epoch: 10, Steps: 14 | Train Loss: 0.6347892 Vali Loss: 1.5796562 Test Loss: 0.5822817
Validation loss decreased (1.586943 --> 1.579656).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.7939116954803467
Epoch: 11, Steps: 14 | Train Loss: 0.6307877 Vali Loss: 1.5697043 Test Loss: 0.5756938
Validation loss decreased (1.579656 --> 1.569704).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.358565330505371
Epoch: 12, Steps: 14 | Train Loss: 0.6272930 Vali Loss: 1.5696815 Test Loss: 0.5696033
Validation loss decreased (1.569704 --> 1.569682).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.4093477725982666
Epoch: 13, Steps: 14 | Train Loss: 0.6242479 Vali Loss: 1.5552195 Test Loss: 0.5641755
Validation loss decreased (1.569682 --> 1.555220).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.6812760829925537
Epoch: 14, Steps: 14 | Train Loss: 0.6209298 Vali Loss: 1.5532066 Test Loss: 0.5591336
Validation loss decreased (1.555220 --> 1.553207).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.5860507488250732
Epoch: 15, Steps: 14 | Train Loss: 0.6188458 Vali Loss: 1.5432760 Test Loss: 0.5544272
Validation loss decreased (1.553207 --> 1.543276).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.644562244415283
Epoch: 16, Steps: 14 | Train Loss: 0.6162639 Vali Loss: 1.5433316 Test Loss: 0.5501761
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.210157155990601
Epoch: 17, Steps: 14 | Train Loss: 0.6133631 Vali Loss: 1.5336235 Test Loss: 0.5462562
Validation loss decreased (1.543276 --> 1.533623).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.076730251312256
Epoch: 18, Steps: 14 | Train Loss: 0.6116179 Vali Loss: 1.5364815 Test Loss: 0.5427133
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.165337085723877
Epoch: 19, Steps: 14 | Train Loss: 0.6097070 Vali Loss: 1.5299050 Test Loss: 0.5393296
Validation loss decreased (1.533623 --> 1.529905).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.20673942565918
Epoch: 20, Steps: 14 | Train Loss: 0.6075581 Vali Loss: 1.5272157 Test Loss: 0.5363058
Validation loss decreased (1.529905 --> 1.527216).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.934704303741455
Epoch: 21, Steps: 14 | Train Loss: 0.6060772 Vali Loss: 1.5229315 Test Loss: 0.5334017
Validation loss decreased (1.527216 --> 1.522931).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.651207208633423
Epoch: 22, Steps: 14 | Train Loss: 0.6048480 Vali Loss: 1.5183139 Test Loss: 0.5307035
Validation loss decreased (1.522931 --> 1.518314).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.171008825302124
Epoch: 23, Steps: 14 | Train Loss: 0.6032218 Vali Loss: 1.5194739 Test Loss: 0.5282640
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.065735340118408
Epoch: 24, Steps: 14 | Train Loss: 0.6018417 Vali Loss: 1.5137360 Test Loss: 0.5259563
Validation loss decreased (1.518314 --> 1.513736).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.6091320514678955
Epoch: 25, Steps: 14 | Train Loss: 0.6009772 Vali Loss: 1.5137746 Test Loss: 0.5238690
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.234649896621704
Epoch: 26, Steps: 14 | Train Loss: 0.5995317 Vali Loss: 1.5070313 Test Loss: 0.5218596
Validation loss decreased (1.513736 --> 1.507031).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.127195596694946
Epoch: 27, Steps: 14 | Train Loss: 0.5984317 Vali Loss: 1.5045996 Test Loss: 0.5200340
Validation loss decreased (1.507031 --> 1.504600).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.110124826431274
Epoch: 28, Steps: 14 | Train Loss: 0.5977445 Vali Loss: 1.5056307 Test Loss: 0.5183188
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.035001516342163
Epoch: 29, Steps: 14 | Train Loss: 0.5962533 Vali Loss: 1.5028338 Test Loss: 0.5166749
Validation loss decreased (1.504600 --> 1.502834).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.256648302078247
Epoch: 30, Steps: 14 | Train Loss: 0.5953475 Vali Loss: 1.4998331 Test Loss: 0.5151710
Validation loss decreased (1.502834 --> 1.499833).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.158373594284058
Epoch: 31, Steps: 14 | Train Loss: 0.5952758 Vali Loss: 1.5030649 Test Loss: 0.5138069
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.9859962463378906
Epoch: 32, Steps: 14 | Train Loss: 0.5943582 Vali Loss: 1.4987707 Test Loss: 0.5124809
Validation loss decreased (1.499833 --> 1.498771).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.8541805744171143
Epoch: 33, Steps: 14 | Train Loss: 0.5933480 Vali Loss: 1.4921596 Test Loss: 0.5112520
Validation loss decreased (1.498771 --> 1.492160).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.272105932235718
Epoch: 34, Steps: 14 | Train Loss: 0.5928725 Vali Loss: 1.4972439 Test Loss: 0.5101339
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.849844455718994
Epoch: 35, Steps: 14 | Train Loss: 0.5922475 Vali Loss: 1.4989587 Test Loss: 0.5089933
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.03588080406189
Epoch: 36, Steps: 14 | Train Loss: 0.5913689 Vali Loss: 1.4958255 Test Loss: 0.5080196
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4891160726547241, mae:0.4970810115337372, rse:0.6695113182067871, corr:[0.20146854 0.22863361 0.22701605 0.21966884 0.22125408 0.22610183
 0.22880422 0.22862963 0.22733893 0.226732   0.22642739 0.22609435
 0.22583221 0.22540113 0.22455072 0.22280233 0.22062221 0.21919915
 0.2192551  0.22003405 0.22004297 0.21887699 0.21720539 0.21675868
 0.21807514 0.22012864 0.2213609  0.22125204 0.22056134 0.22024871
 0.22071028 0.22130534 0.22155881 0.22115503 0.22019292 0.21915823
 0.2184855  0.21809731 0.21808846 0.2182623  0.21845697 0.21810842
 0.21719074 0.21650676 0.21675676 0.2180819  0.21971405 0.22076085
 0.22079736 0.22045949 0.22017092 0.22009659 0.22006471 0.21943045
 0.2182742  0.21686183 0.2159804  0.21558793 0.21536604 0.21525104
 0.2150556  0.21450086 0.21368559 0.21310848 0.21297489 0.21334091
 0.21397488 0.21432236 0.21423581 0.21381877 0.21335758 0.21343
 0.21368302 0.21380138 0.21346864 0.21287033 0.212185   0.21167782
 0.21152493 0.21159343 0.21143359 0.21086907 0.20998123 0.20917138
 0.2086087  0.20820646 0.20782928 0.20747277 0.20702055 0.20645376
 0.2058492  0.20553589 0.20559543 0.20605136 0.20663925 0.20740665
 0.20831874 0.20936987 0.21052115 0.21168806 0.21252233 0.21275426
 0.21243057 0.21182068 0.21133831 0.21102431 0.21082307 0.21056764
 0.21018535 0.20971599 0.20918165 0.20889483 0.20893389 0.20917787
 0.20949155 0.20956373 0.2093172  0.20892787 0.20878841 0.20923725
 0.20991609 0.21013616 0.20956533 0.20875019 0.20803094 0.2076149
 0.20751315 0.20755468 0.20733684 0.20667785 0.20581824 0.2051332
 0.20491062 0.20495975 0.20503992 0.20497392 0.204643   0.2041193
 0.20358951 0.20331424 0.20334055 0.2034105  0.20321643 0.20294021
 0.20263018 0.20228285 0.20194791 0.20162944 0.20140378 0.20112008
 0.2007348  0.200436   0.20036456 0.2003999  0.20041046 0.2003561
 0.19997571 0.19936292 0.1988531  0.19879547 0.19904967 0.19948423
 0.19981027 0.19979572 0.19940242 0.19891797 0.1982657  0.19798526
 0.19820748 0.1988584  0.19939962 0.19972903 0.19971693 0.19960064
 0.1996276  0.19997822 0.20018522 0.20000866 0.19940005 0.19872096
 0.19835262 0.19844249 0.19889906 0.19938472 0.19967225 0.19966196
 0.19953144 0.19958727 0.1999147  0.20016222 0.20015854 0.19991338
 0.19954494 0.19909991 0.19864003 0.19825684 0.19801632 0.19772238
 0.19720465 0.19665729 0.19622476 0.19601019 0.19621839 0.19676374
 0.19722599 0.19752729 0.19752918 0.19739452 0.197455   0.19773446
 0.19794998 0.19778565 0.19718027 0.19635452 0.19567043 0.19552667
 0.19584616 0.19631352 0.19651498 0.19646053 0.19636516 0.1961391
 0.19592041 0.1958497  0.19573607 0.1953959  0.19459201 0.19379617
 0.19325058 0.19313699 0.19340122 0.19378172 0.19375502 0.19336164
 0.1928972  0.19258952 0.19245332 0.19243424 0.19249319 0.19263302
 0.19297116 0.19344974 0.19415447 0.19490995 0.19538961 0.19546914
 0.19528967 0.19486728 0.19414508 0.19328812 0.19269742 0.19267043
 0.19283071 0.19277665 0.19250791 0.19226561 0.19237867 0.1928838
 0.19346538 0.19366537 0.19360763 0.19336186 0.19315845 0.19307177
 0.19326343 0.19378795 0.19433399 0.19462812 0.19433206 0.19370633
 0.19330715 0.19327569 0.19335172 0.19343065 0.19324535 0.19277942
 0.19228888 0.19234012 0.19255601 0.19286588 0.19290884 0.19276464
 0.19237335 0.19210024 0.1918964  0.19174205 0.19168727 0.19195728
 0.19254345 0.19327837 0.19393778 0.1944995  0.19488154 0.19515537
 0.19532423 0.19531886 0.19493909 0.19451883 0.19443952 0.19488466
 0.1954681  0.19569005 0.1955726  0.19545734 0.19556966 0.19596887
 0.19662248 0.1968156  0.19648601 0.19596145 0.19569474 0.19607025
 0.19700705 0.19789173 0.1982452  0.19810194 0.19760554 0.19685604
 0.19598798 0.19521047 0.19472952 0.19446163 0.19427167 0.19400358
 0.19379364 0.19380876 0.19424325 0.19513546 0.19584548 0.19613679
 0.1960658  0.19584751 0.1957916  0.1959204  0.19596872 0.19590473
 0.19576196 0.19555129 0.19530416 0.19515719 0.19498904 0.19467367
 0.1943106  0.19378515 0.1929551  0.19217882 0.19185215 0.19214618
 0.19270205 0.19308072 0.1930253  0.19278152 0.19273281 0.19322895
 0.19385025 0.19414657 0.19396822 0.193547   0.19311117 0.19297668
 0.19324438 0.193586   0.19380723 0.19383727 0.19363308 0.19295542
 0.19199291 0.1911902  0.19080469 0.19070674 0.19057712 0.19014053
 0.18922195 0.1881691  0.18760999 0.18766652 0.18803082 0.18824701
 0.18804504 0.187478   0.18694499 0.18684247 0.18718687 0.1880928
 0.18926512 0.19038303 0.19115278 0.19157152 0.19182596 0.19168656
 0.1912348  0.19042727 0.18922563 0.1879451  0.18697034 0.18681186
 0.18727967 0.18764988 0.18763973 0.18732393 0.1871607  0.18760999
 0.18843593 0.18902065 0.18911219 0.18883745 0.1884573  0.18844934
 0.1890048  0.18999617 0.19110464 0.19202927 0.19254299 0.19226766
 0.19154663 0.19073375 0.19023325 0.19015819 0.19030362 0.19031279
 0.18990128 0.18933415 0.18875355 0.18867746 0.18901703 0.1892932
 0.18899898 0.18837345 0.187784   0.18794656 0.1887105  0.1896919
 0.19059937 0.19109623 0.1911662  0.19137752 0.19185749 0.19222088
 0.19241682 0.19247457 0.19209774 0.19154154 0.19105633 0.1909876
 0.19124341 0.19136739 0.19124693 0.1906621  0.18993445 0.18946935
 0.18950063 0.18977526 0.189994   0.19001389 0.19007403 0.19057357
 0.19150591 0.19255625 0.19322722 0.1933201  0.19297858 0.19214107
 0.1912686  0.19055997 0.19022119 0.19059858 0.1911791  0.19157709
 0.19142698 0.19102614 0.19096398 0.1915594  0.19239141 0.19287936
 0.19269006 0.19206548 0.19149286 0.19123583 0.19132693 0.19144103
 0.191492   0.19150117 0.19140264 0.19141412 0.19154173 0.19138403
 0.19114101 0.19117628 0.19100247 0.19049545 0.18970342 0.18914224
 0.18926738 0.18975826 0.19014095 0.19005243 0.18957098 0.18909414
 0.18893173 0.18891445 0.18880758 0.18858774 0.18853037 0.18896665
 0.18987802 0.19110544 0.19211471 0.19272794 0.19302328 0.19275925
 0.19196402 0.19105853 0.19021562 0.18971173 0.189874   0.190479
 0.19084604 0.19073787 0.19076464 0.19109811 0.19193365 0.19265865
 0.19286285 0.19267675 0.19287854 0.19316925 0.19389589 0.19401245
 0.19406703 0.19396684 0.19376811 0.19380696 0.19355777 0.1930637
 0.19263427 0.1925386  0.19246827 0.19216357 0.19146991 0.19109344
 0.1914075  0.19214605 0.19304265 0.19348021 0.19353077 0.1933576
 0.19336273 0.19358575 0.19371217 0.193426   0.1927356  0.19189535
 0.19114658 0.19084682 0.19072525 0.19054763 0.1901438  0.18937948
 0.18839969 0.18741786 0.18659438 0.18629879 0.18653637 0.18696345
 0.18675548 0.18567926 0.18493824 0.18472743 0.18543048 0.18619478
 0.18657358 0.1861563  0.18556656 0.18508647 0.18467511 0.18410078
 0.18354915 0.1834521  0.18346116 0.18384491 0.18393102 0.18333316
 0.18285656 0.18261978 0.1827412  0.1826424  0.1823846  0.18212254
 0.18235351 0.18279813 0.18328695 0.18338734 0.18296596 0.18232776
 0.18205445 0.18235806 0.18289275 0.18310338 0.18264255 0.18169019
 0.18084806 0.18050674 0.1804795  0.18048099 0.18011804 0.1790965
 0.17778975 0.17681178 0.17628385 0.17640638 0.17681214 0.1772955
 0.17736226 0.17685287 0.17623803 0.17602882 0.1763448  0.17689501
 0.17727166 0.17729549 0.17721565 0.17728761 0.17760068 0.17779358
 0.17765264 0.17724353 0.1765482  0.17604023 0.17574205 0.17544986
 0.17547064 0.17566773 0.1756019  0.17518431 0.17455904 0.17429212
 0.17460659 0.17494215 0.17527576 0.1754368  0.1752568  0.17481852
 0.17436787 0.17424795 0.17447536 0.17470853 0.174685   0.17430325
 0.1736207  0.17304607 0.17282541 0.17339449 0.17372683 0.17307997
 0.17158842 0.17008394 0.16949321 0.16985275 0.17053857 0.17097397
 0.1707956  0.16996455 0.16924204 0.1691542  0.16980991 0.17070074
 0.17105627 0.17075415 0.16993195 0.16921987 0.1688122  0.16864595
 0.16799518 0.16670705 0.16471668 0.16248155 0.16088663 0.15997955
 0.16032293 0.16105178 0.16134106 0.16049409 0.15964316 0.15971595
 0.16049176 0.16111194 0.16166893 0.16193978 0.1618752  0.1615683
 0.16126035 0.16143902 0.1620422  0.16208202 0.16075987 0.15830678
 0.155935   0.15565519 0.15714213 0.15911974 0.15894903 0.15631242
 0.15405732 0.154239   0.15658088 0.1585788  0.15894362 0.15782328
 0.15633975 0.15449199 0.15284172 0.15180779 0.15233633 0.15353762
 0.15223117 0.14468466 0.13556516 0.13617769 0.1446971  0.10665838]
