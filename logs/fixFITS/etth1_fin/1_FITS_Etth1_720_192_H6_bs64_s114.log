Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.585785627365112
Epoch: 1, Steps: 60 | Train Loss: 0.6519894 Vali Loss: 1.1970724 Test Loss: 0.5668868
Validation loss decreased (inf --> 1.197072).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.04281234741211
Epoch: 2, Steps: 60 | Train Loss: 0.4844575 Vali Loss: 1.0596355 Test Loss: 0.4745445
Validation loss decreased (1.197072 --> 1.059636).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.082607984542847
Epoch: 3, Steps: 60 | Train Loss: 0.4352209 Vali Loss: 1.0059483 Test Loss: 0.4382724
Validation loss decreased (1.059636 --> 1.005948).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.215550899505615
Epoch: 4, Steps: 60 | Train Loss: 0.4133068 Vali Loss: 0.9826912 Test Loss: 0.4236814
Validation loss decreased (1.005948 --> 0.982691).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.835548162460327
Epoch: 5, Steps: 60 | Train Loss: 0.4029478 Vali Loss: 0.9735072 Test Loss: 0.4186169
Validation loss decreased (0.982691 --> 0.973507).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.931129693984985
Epoch: 6, Steps: 60 | Train Loss: 0.3975338 Vali Loss: 0.9684221 Test Loss: 0.4169523
Validation loss decreased (0.973507 --> 0.968422).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.66757607460022
Epoch: 7, Steps: 60 | Train Loss: 0.3938341 Vali Loss: 0.9667459 Test Loss: 0.4162206
Validation loss decreased (0.968422 --> 0.966746).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.694591999053955
Epoch: 8, Steps: 60 | Train Loss: 0.3917195 Vali Loss: 0.9651375 Test Loss: 0.4164197
Validation loss decreased (0.966746 --> 0.965137).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.11333417892456
Epoch: 9, Steps: 60 | Train Loss: 0.3895564 Vali Loss: 0.9644850 Test Loss: 0.4167304
Validation loss decreased (0.965137 --> 0.964485).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.19814395904541
Epoch: 10, Steps: 60 | Train Loss: 0.3885685 Vali Loss: 0.9634023 Test Loss: 0.4165831
Validation loss decreased (0.964485 --> 0.963402).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.986143589019775
Epoch: 11, Steps: 60 | Train Loss: 0.3872037 Vali Loss: 0.9630748 Test Loss: 0.4166215
Validation loss decreased (0.963402 --> 0.963075).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.017707109451294
Epoch: 12, Steps: 60 | Train Loss: 0.3861598 Vali Loss: 0.9625325 Test Loss: 0.4166940
Validation loss decreased (0.963075 --> 0.962533).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.030821800231934
Epoch: 13, Steps: 60 | Train Loss: 0.3854372 Vali Loss: 0.9631698 Test Loss: 0.4169256
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 7.631755590438843
Epoch: 14, Steps: 60 | Train Loss: 0.3850550 Vali Loss: 0.9623151 Test Loss: 0.4169821
Validation loss decreased (0.962533 --> 0.962315).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.881823539733887
Epoch: 15, Steps: 60 | Train Loss: 0.3845197 Vali Loss: 0.9626672 Test Loss: 0.4172417
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.337213039398193
Epoch: 16, Steps: 60 | Train Loss: 0.3840096 Vali Loss: 0.9625006 Test Loss: 0.4172169
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.118255376815796
Epoch: 17, Steps: 60 | Train Loss: 0.3834892 Vali Loss: 0.9627578 Test Loss: 0.4172983
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.41324979066848755, mae:0.4224551320075989, rse:0.6104690432548523, corr:[0.25894445 0.27175432 0.26987776 0.2709485  0.26988468 0.26654908
 0.26533812 0.26578903 0.26539364 0.26447955 0.26412034 0.264212
 0.26410586 0.26379886 0.26344576 0.2631828  0.2632365  0.26325193
 0.26283404 0.26222628 0.26200563 0.26189837 0.26170275 0.2616197
 0.2616505  0.26158053 0.2616417  0.26190436 0.2619019  0.26148129
 0.26094285 0.26053858 0.26038712 0.26034346 0.26032078 0.26023775
 0.26012284 0.26018134 0.26030302 0.26019642 0.2600228  0.26014498
 0.26059517 0.26099792 0.2611118  0.26129472 0.26177052 0.26217982
 0.2620697  0.26125988 0.2601488  0.2591995  0.25820497 0.25704145
 0.25625253 0.25579616 0.25533313 0.2549968  0.25488323 0.2548678
 0.2546521  0.25447613 0.2545076  0.2545616  0.25448054 0.25472564
 0.2553374  0.25550255 0.25521085 0.2551098  0.25535554 0.25526613
 0.25460836 0.25386426 0.25340483 0.25303814 0.25241366 0.25174546
 0.2514436  0.2514     0.25129196 0.2509086  0.25051966 0.25040644
 0.25040713 0.2502269  0.24991733 0.24976213 0.2498108  0.24976309
 0.24940778 0.24917528 0.24937871 0.2496287  0.24942768 0.24966808
 0.25078207 0.25156254 0.25145584 0.25109756 0.25092703 0.25068206
 0.25028887 0.24998897 0.24991028 0.24987043 0.24965143 0.2496235
 0.24978912 0.24947831 0.24911706 0.24952483 0.25021288 0.2502856
 0.25012848 0.2503546  0.25063193 0.25018528 0.24948749 0.24931346
 0.24931854 0.24866098 0.24759118 0.24682729 0.245941   0.24475983
 0.24422605 0.2442095  0.24373941 0.24321397 0.24306805 0.2430626
 0.24280304 0.24271734 0.24269465 0.24222201 0.24201411 0.2425979
 0.24322812 0.24288343 0.24248737 0.24287328 0.24281728 0.2417516
 0.24081807 0.2404051  0.23965554 0.23828167 0.23714429 0.23623745
 0.23520198 0.23473877 0.23518962 0.23523977 0.23424327 0.23406112
 0.2350209  0.23519482 0.23457289 0.23422384 0.23394756 0.23350906
 0.23362237 0.23414108 0.23348352 0.2320574  0.23174655 0.23167787
 0.23052979 0.22998539 0.23105825 0.23128298 0.22929636 0.22776401
 0.22812623 0.22793423 0.226703   0.2267807  0.22679067 0.22517656
 0.22464186 0.22576368 0.22438855 0.22209199 0.22309838 0.22323073
 0.21935648 0.21921174 0.2215019  0.21364786 0.20972452 0.22546543]
