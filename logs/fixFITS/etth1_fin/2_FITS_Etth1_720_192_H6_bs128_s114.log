Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  87105536.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.592596530914307
Epoch: 1, Steps: 30 | Train Loss: 0.6842898 Vali Loss: 1.6671846 Test Loss: 0.8777332
Validation loss decreased (inf --> 1.667185).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.386240482330322
Epoch: 2, Steps: 30 | Train Loss: 0.5774715 Vali Loss: 1.5143375 Test Loss: 0.8026388
Validation loss decreased (1.667185 --> 1.514338).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.649860143661499
Epoch: 3, Steps: 30 | Train Loss: 0.5123169 Vali Loss: 1.4283056 Test Loss: 0.7602976
Validation loss decreased (1.514338 --> 1.428306).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.3930723667144775
Epoch: 4, Steps: 30 | Train Loss: 0.4711925 Vali Loss: 1.3791291 Test Loss: 0.7399663
Validation loss decreased (1.428306 --> 1.379129).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.08224081993103
Epoch: 5, Steps: 30 | Train Loss: 0.4429472 Vali Loss: 1.3439986 Test Loss: 0.7283247
Validation loss decreased (1.379129 --> 1.343999).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.56914496421814
Epoch: 6, Steps: 30 | Train Loss: 0.4222713 Vali Loss: 1.3320745 Test Loss: 0.7213528
Validation loss decreased (1.343999 --> 1.332075).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.175288915634155
Epoch: 7, Steps: 30 | Train Loss: 0.4057098 Vali Loss: 1.3177884 Test Loss: 0.7172400
Validation loss decreased (1.332075 --> 1.317788).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.568056583404541
Epoch: 8, Steps: 30 | Train Loss: 0.3920306 Vali Loss: 1.3063495 Test Loss: 0.7131231
Validation loss decreased (1.317788 --> 1.306350).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.556799650192261
Epoch: 9, Steps: 30 | Train Loss: 0.3802917 Vali Loss: 1.2948002 Test Loss: 0.7095636
Validation loss decreased (1.306350 --> 1.294800).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.694799423217773
Epoch: 10, Steps: 30 | Train Loss: 0.3701057 Vali Loss: 1.2892170 Test Loss: 0.7063470
Validation loss decreased (1.294800 --> 1.289217).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.493112802505493
Epoch: 11, Steps: 30 | Train Loss: 0.3607296 Vali Loss: 1.2845228 Test Loss: 0.7030305
Validation loss decreased (1.289217 --> 1.284523).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.334615707397461
Epoch: 12, Steps: 30 | Train Loss: 0.3525700 Vali Loss: 1.2745502 Test Loss: 0.7005844
Validation loss decreased (1.284523 --> 1.274550).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.062814474105835
Epoch: 13, Steps: 30 | Train Loss: 0.3449651 Vali Loss: 1.2722306 Test Loss: 0.6973960
Validation loss decreased (1.274550 --> 1.272231).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.171140432357788
Epoch: 14, Steps: 30 | Train Loss: 0.3379066 Vali Loss: 1.2665906 Test Loss: 0.6940797
Validation loss decreased (1.272231 --> 1.266591).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.690184593200684
Epoch: 15, Steps: 30 | Train Loss: 0.3316545 Vali Loss: 1.2635586 Test Loss: 0.6916779
Validation loss decreased (1.266591 --> 1.263559).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.616023778915405
Epoch: 16, Steps: 30 | Train Loss: 0.3256439 Vali Loss: 1.2593092 Test Loss: 0.6887451
Validation loss decreased (1.263559 --> 1.259309).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.421349287033081
Epoch: 17, Steps: 30 | Train Loss: 0.3200924 Vali Loss: 1.2553797 Test Loss: 0.6856942
Validation loss decreased (1.259309 --> 1.255380).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.561624526977539
Epoch: 18, Steps: 30 | Train Loss: 0.3153676 Vali Loss: 1.2486048 Test Loss: 0.6836123
Validation loss decreased (1.255380 --> 1.248605).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.558119058609009
Epoch: 19, Steps: 30 | Train Loss: 0.3106576 Vali Loss: 1.2382494 Test Loss: 0.6805534
Validation loss decreased (1.248605 --> 1.238249).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.422561883926392
Epoch: 20, Steps: 30 | Train Loss: 0.3062679 Vali Loss: 1.2389132 Test Loss: 0.6779181
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.51063346862793
Epoch: 21, Steps: 30 | Train Loss: 0.3021408 Vali Loss: 1.2410872 Test Loss: 0.6758481
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.149026393890381
Epoch: 22, Steps: 30 | Train Loss: 0.2984216 Vali Loss: 1.2386286 Test Loss: 0.6736198
EarlyStopping counter: 3 out of 3
Early stopping
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  87105536.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.878610610961914
Epoch: 1, Steps: 30 | Train Loss: 0.5126930 Vali Loss: 1.1257662 Test Loss: 0.5817430
Validation loss decreased (inf --> 1.125766).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.509944677352905
Epoch: 2, Steps: 30 | Train Loss: 0.4635280 Vali Loss: 1.0571709 Test Loss: 0.5192481
Validation loss decreased (1.125766 --> 1.057171).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.777319669723511
Epoch: 3, Steps: 30 | Train Loss: 0.4334057 Vali Loss: 1.0154388 Test Loss: 0.4794301
Validation loss decreased (1.057171 --> 1.015439).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.035221338272095
Epoch: 4, Steps: 30 | Train Loss: 0.4149763 Vali Loss: 0.9866832 Test Loss: 0.4549214
Validation loss decreased (1.015439 --> 0.986683).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.550207138061523
Epoch: 5, Steps: 30 | Train Loss: 0.4029993 Vali Loss: 0.9681069 Test Loss: 0.4395928
Validation loss decreased (0.986683 --> 0.968107).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.708524227142334
Epoch: 6, Steps: 30 | Train Loss: 0.3959789 Vali Loss: 0.9626274 Test Loss: 0.4304826
Validation loss decreased (0.968107 --> 0.962627).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.311314582824707
Epoch: 7, Steps: 30 | Train Loss: 0.3912190 Vali Loss: 0.9536303 Test Loss: 0.4251379
Validation loss decreased (0.962627 --> 0.953630).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.885970115661621
Epoch: 8, Steps: 30 | Train Loss: 0.3884022 Vali Loss: 0.9548663 Test Loss: 0.4220470
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.688063383102417
Epoch: 9, Steps: 30 | Train Loss: 0.3861772 Vali Loss: 0.9570156 Test Loss: 0.4200606
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.727161884307861
Epoch: 10, Steps: 30 | Train Loss: 0.3848861 Vali Loss: 0.9538225 Test Loss: 0.4191170
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4188474416732788, mae:0.42684492468833923, rse:0.6145896911621094, corr:[0.2627737  0.270308   0.26660892 0.27015057 0.26924837 0.26524326
 0.26423126 0.265469   0.26508924 0.26383886 0.2637602  0.26414767
 0.26393262 0.2631012  0.26231885 0.26206875 0.2620082  0.26178786
 0.26091322 0.26023856 0.26021972 0.25990507 0.25872752 0.2583729
 0.25912556 0.25942937 0.25890732 0.25870472 0.25910062 0.25914243
 0.2586479  0.25823817 0.25842714 0.25864807 0.25821134 0.25769898
 0.2575157  0.25759163 0.2575847  0.25755152 0.2579706  0.258375
 0.25825536 0.25770968 0.2574268  0.257899   0.25891525 0.2595616
 0.259464   0.25921604 0.25864604 0.25753477 0.25601327 0.25464523
 0.254185   0.25388247 0.25303036 0.25245142 0.25237665 0.25238588
 0.25179234 0.2515065  0.2523358  0.25325468 0.2529934  0.25245312
 0.25300083 0.2536791  0.25350755 0.25295392 0.25288197 0.25327486
 0.2531953  0.25233507 0.25135162 0.2511618  0.25097498 0.2502387
 0.24936803 0.24902065 0.24920017 0.2491695  0.24849771 0.24786936
 0.24777989 0.24774791 0.24750121 0.24724834 0.24728012 0.24742675
 0.24723084 0.24674286 0.2465183  0.2467317  0.24697539 0.24720274
 0.24772684 0.24837083 0.24875554 0.24886502 0.24878892 0.24869841
 0.24878591 0.24860804 0.2481268  0.247915   0.24772073 0.24717641
 0.24680355 0.24692443 0.2474355  0.24780811 0.24757175 0.2474133
 0.24770066 0.24781895 0.2475298  0.24727267 0.24740621 0.2478744
 0.24802317 0.24721043 0.24580428 0.24495803 0.2444482  0.24344584
 0.2426265  0.24245052 0.24217446 0.2417745  0.24153349 0.24160735
 0.2413973  0.24088657 0.2407997  0.24119876 0.24148479 0.24109663
 0.24077976 0.24078214 0.24084613 0.24088384 0.2407666  0.24021606
 0.23969269 0.23905103 0.23818225 0.2371581  0.23638946 0.2356917
 0.23496829 0.23419298 0.23407277 0.23484373 0.23514318 0.2347033
 0.23439871 0.23472767 0.23491864 0.23419566 0.23334661 0.2333781
 0.2337999  0.23372848 0.23326585 0.23308155 0.23344088 0.23303135
 0.23182918 0.23118667 0.23125574 0.23115025 0.2298412  0.22847226
 0.22857611 0.2296676  0.22963135 0.22919188 0.22921693 0.22911654
 0.2279777  0.2266568  0.22650044 0.22726856 0.22701982 0.2251051
 0.22412978 0.22460498 0.22141668 0.21391429 0.21547389 0.21746625]
