Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  142517760.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.5092933177948
Epoch: 1, Steps: 14 | Train Loss: 0.8919902 Vali Loss: 1.8672184 Test Loss: 0.8581282
Validation loss decreased (inf --> 1.867218).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.9794299602508545
Epoch: 2, Steps: 14 | Train Loss: 0.7662228 Vali Loss: 1.6610293 Test Loss: 0.7427771
Validation loss decreased (1.867218 --> 1.661029).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.644920587539673
Epoch: 3, Steps: 14 | Train Loss: 0.6887539 Vali Loss: 1.5365324 Test Loss: 0.6734394
Validation loss decreased (1.661029 --> 1.536532).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.617114543914795
Epoch: 4, Steps: 14 | Train Loss: 0.6410426 Vali Loss: 1.4581138 Test Loss: 0.6308645
Validation loss decreased (1.536532 --> 1.458114).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.7425520420074463
Epoch: 5, Steps: 14 | Train Loss: 0.6117776 Vali Loss: 1.4133450 Test Loss: 0.6029485
Validation loss decreased (1.458114 --> 1.413345).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.0314972400665283
Epoch: 6, Steps: 14 | Train Loss: 0.5913875 Vali Loss: 1.3922043 Test Loss: 0.5829611
Validation loss decreased (1.413345 --> 1.392204).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.617400646209717
Epoch: 7, Steps: 14 | Train Loss: 0.5756305 Vali Loss: 1.3579948 Test Loss: 0.5674674
Validation loss decreased (1.392204 --> 1.357995).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.609071969985962
Epoch: 8, Steps: 14 | Train Loss: 0.5639564 Vali Loss: 1.3449702 Test Loss: 0.5549625
Validation loss decreased (1.357995 --> 1.344970).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.6014370918273926
Epoch: 9, Steps: 14 | Train Loss: 0.5535171 Vali Loss: 1.3217192 Test Loss: 0.5444816
Validation loss decreased (1.344970 --> 1.321719).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.5902085304260254
Epoch: 10, Steps: 14 | Train Loss: 0.5443599 Vali Loss: 1.3171244 Test Loss: 0.5354216
Validation loss decreased (1.321719 --> 1.317124).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.630542039871216
Epoch: 11, Steps: 14 | Train Loss: 0.5380246 Vali Loss: 1.3104643 Test Loss: 0.5275606
Validation loss decreased (1.317124 --> 1.310464).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.6097099781036377
Epoch: 12, Steps: 14 | Train Loss: 0.5301916 Vali Loss: 1.2819405 Test Loss: 0.5205590
Validation loss decreased (1.310464 --> 1.281940).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.506866693496704
Epoch: 13, Steps: 14 | Train Loss: 0.5246481 Vali Loss: 1.2838672 Test Loss: 0.5143708
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.5966808795928955
Epoch: 14, Steps: 14 | Train Loss: 0.5205874 Vali Loss: 1.2711167 Test Loss: 0.5088748
Validation loss decreased (1.281940 --> 1.271117).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.58473539352417
Epoch: 15, Steps: 14 | Train Loss: 0.5161163 Vali Loss: 1.2597853 Test Loss: 0.5039742
Validation loss decreased (1.271117 --> 1.259785).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.8041486740112305
Epoch: 16, Steps: 14 | Train Loss: 0.5122130 Vali Loss: 1.2556790 Test Loss: 0.4995600
Validation loss decreased (1.259785 --> 1.255679).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.7713959217071533
Epoch: 17, Steps: 14 | Train Loss: 0.5082867 Vali Loss: 1.2569864 Test Loss: 0.4955971
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.6362483501434326
Epoch: 18, Steps: 14 | Train Loss: 0.5052418 Vali Loss: 1.2506548 Test Loss: 0.4920231
Validation loss decreased (1.255679 --> 1.250655).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.666071653366089
Epoch: 19, Steps: 14 | Train Loss: 0.5022104 Vali Loss: 1.2462208 Test Loss: 0.4887633
Validation loss decreased (1.250655 --> 1.246221).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.2656285762786865
Epoch: 20, Steps: 14 | Train Loss: 0.4994412 Vali Loss: 1.2395024 Test Loss: 0.4857526
Validation loss decreased (1.246221 --> 1.239502).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.4660143852233887
Epoch: 21, Steps: 14 | Train Loss: 0.4970587 Vali Loss: 1.2507234 Test Loss: 0.4830933
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.786585569381714
Epoch: 22, Steps: 14 | Train Loss: 0.4950664 Vali Loss: 1.2296863 Test Loss: 0.4806052
Validation loss decreased (1.239502 --> 1.229686).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.5712084770202637
Epoch: 23, Steps: 14 | Train Loss: 0.4917378 Vali Loss: 1.2267262 Test Loss: 0.4783299
Validation loss decreased (1.229686 --> 1.226726).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.4496469497680664
Epoch: 24, Steps: 14 | Train Loss: 0.4915639 Vali Loss: 1.2195115 Test Loss: 0.4763084
Validation loss decreased (1.226726 --> 1.219512).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.666517972946167
Epoch: 25, Steps: 14 | Train Loss: 0.4888705 Vali Loss: 1.2199496 Test Loss: 0.4744159
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.1349072456359863
Epoch: 26, Steps: 14 | Train Loss: 0.4877778 Vali Loss: 1.2236354 Test Loss: 0.4727006
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.7184500694274902
Epoch: 27, Steps: 14 | Train Loss: 0.4858457 Vali Loss: 1.2226660 Test Loss: 0.4711171
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.47543996572494507, mae:0.46997350454330444, rse:0.6564473509788513, corr:[0.23524667 0.25407624 0.24653102 0.24264461 0.24750002 0.2499785
 0.24921644 0.24839282 0.24929409 0.24995379 0.24959509 0.2491387
 0.24881835 0.24803601 0.24690351 0.24633595 0.24627721 0.24529691
 0.24308322 0.24141438 0.24150258 0.24171886 0.24066548 0.23854934
 0.23759945 0.23841837 0.23960312 0.23991702 0.2397791  0.24021341
 0.24127541 0.24227703 0.24290119 0.24283293 0.24238567 0.24200365
 0.24198355 0.24197906 0.24159059 0.24093403 0.24046347 0.24053995
 0.24081609 0.24078201 0.24027608 0.24019134 0.24082968 0.24137034
 0.24139188 0.24090783 0.24042217 0.24033508 0.24017198 0.23961401
 0.23890965 0.2382767  0.23783979 0.23747696 0.23727491 0.23697866
 0.23636676 0.23572977 0.23563135 0.23608434 0.23628114 0.23594879
 0.2354327  0.23522186 0.23566727 0.23584294 0.23514424 0.23386031
 0.23302357 0.23287249 0.23299465 0.23296154 0.23248772 0.2320136
 0.23192535 0.23232926 0.23259982 0.23230706 0.23173155 0.23166631
 0.23174137 0.23117062 0.22972988 0.22845733 0.22826742 0.22863786
 0.22828759 0.22687936 0.22523622 0.22439346 0.22417583 0.22435907
 0.22469586 0.22528464 0.22613637 0.2270293  0.22749464 0.22741169
 0.22718982 0.22710633 0.22721879 0.22738327 0.22729644 0.22684844
 0.22606252 0.22570571 0.22611667 0.22655493 0.22593799 0.2246697
 0.2240216  0.22442065 0.2248212  0.22400965 0.22226205 0.22124322
 0.22165865 0.2222801  0.22194639 0.22100048 0.22018129 0.21978527
 0.21998855 0.22051412 0.22074598 0.22012578 0.21895163 0.21832529
 0.2187135  0.2191377  0.21876663 0.21810931 0.21802856 0.21854936
 0.21834743 0.21713102 0.21602422 0.21590815 0.21610202 0.21582578
 0.21496016 0.21419689 0.21390794 0.21388595 0.21370344 0.21330133
 0.21317612 0.21325304 0.2133544  0.21325214 0.21286276 0.2124083
 0.21162769 0.21124512 0.21164209 0.21241973 0.21249196 0.21191742
 0.21170367 0.2122671  0.21272701 0.21194774 0.20980164 0.20814121
 0.20806547 0.20884596 0.20895678 0.20842737 0.20802903 0.20819946
 0.20875074 0.20939715 0.20954113 0.20913684 0.20856017 0.20857212
 0.20902175 0.20913762 0.20851846 0.20814884 0.20919007 0.21100026
 0.21166046 0.21065822 0.20950359 0.2097833  0.21097973 0.21135572
 0.21034132 0.20919198 0.20890705 0.20885311 0.2082716  0.20727472
 0.20682268 0.2073312  0.208296   0.20923139 0.20967211 0.20961775
 0.2090061  0.20890349 0.20979284 0.21106344 0.21145909 0.21061786
 0.20993364 0.21044558 0.21109557 0.21029885 0.20824061 0.20707977
 0.20784387 0.20924367 0.20952167 0.2090714  0.20918706 0.20969401
 0.21008737 0.21039025 0.21077946 0.21103077 0.21064056 0.21062909
 0.21130344 0.21215895 0.21224345 0.21166767 0.21145114 0.21240442
 0.21323214 0.21251781 0.21093267 0.21047667 0.21156277 0.21224317
 0.2117256  0.21111347 0.2119215  0.21331804 0.21378995 0.21338077
 0.2133194  0.2140653  0.21471986 0.21480206 0.21480076 0.2151605
 0.21529351 0.2148628  0.21497858 0.21581697 0.21650903 0.21619172
 0.21576692 0.21622777 0.21713933 0.21678905 0.2152952  0.21432245
 0.2151676  0.21667801 0.21688306 0.21597415 0.21514201 0.21501034
 0.21544354 0.21579745 0.216032   0.21630941 0.21607116 0.2157433
 0.21626255 0.21737808 0.21730076 0.21607226 0.2154997  0.21670169
 0.2177263  0.21659392 0.21458448 0.21415548 0.21559419 0.21639623
 0.21543935 0.21464036 0.21545704 0.21683131 0.2166919  0.21572256
 0.21571718 0.21674436 0.21716496 0.21691105 0.21703702 0.21776322
 0.21781091 0.21683179 0.21677725 0.21807985 0.21900783 0.21809988
 0.21723127 0.21792664 0.21946414 0.21883379 0.21633129 0.21523087
 0.21682385 0.21786438 0.21605937 0.21361825 0.21343859 0.21399127
 0.21297361 0.21085067 0.21015382 0.21075588 0.20983577 0.20777273
 0.20768377 0.20881183 0.20721765 0.20346822 0.20218047 0.20496215
 0.20441888 0.19489676 0.1853125  0.1883353  0.19146144 0.15417814]
