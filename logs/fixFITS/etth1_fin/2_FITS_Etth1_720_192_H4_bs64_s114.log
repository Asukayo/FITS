Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20290816.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.013286828994751
Epoch: 1, Steps: 60 | Train Loss: 0.6408295 Vali Loss: 1.5990659 Test Loss: 0.8324018
Validation loss decreased (inf --> 1.599066).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.597392320632935
Epoch: 2, Steps: 60 | Train Loss: 0.5058687 Vali Loss: 1.4245391 Test Loss: 0.7400550
Validation loss decreased (1.599066 --> 1.424539).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.790123224258423
Epoch: 3, Steps: 60 | Train Loss: 0.4370741 Vali Loss: 1.3495299 Test Loss: 0.7036313
Validation loss decreased (1.424539 --> 1.349530).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.547391414642334
Epoch: 4, Steps: 60 | Train Loss: 0.3964639 Vali Loss: 1.3098321 Test Loss: 0.6847059
Validation loss decreased (1.349530 --> 1.309832).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.943785429000854
Epoch: 5, Steps: 60 | Train Loss: 0.3677768 Vali Loss: 1.2854153 Test Loss: 0.6730330
Validation loss decreased (1.309832 --> 1.285415).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.841287612915039
Epoch: 6, Steps: 60 | Train Loss: 0.3456023 Vali Loss: 1.2687736 Test Loss: 0.6656913
Validation loss decreased (1.285415 --> 1.268774).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.530029773712158
Epoch: 7, Steps: 60 | Train Loss: 0.3267504 Vali Loss: 1.2533350 Test Loss: 0.6568530
Validation loss decreased (1.268774 --> 1.253335).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.822730302810669
Epoch: 8, Steps: 60 | Train Loss: 0.3106094 Vali Loss: 1.2378520 Test Loss: 0.6476460
Validation loss decreased (1.253335 --> 1.237852).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.488445043563843
Epoch: 9, Steps: 60 | Train Loss: 0.2967293 Vali Loss: 1.2233591 Test Loss: 0.6383215
Validation loss decreased (1.237852 --> 1.223359).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.558187007904053
Epoch: 10, Steps: 60 | Train Loss: 0.2843834 Vali Loss: 1.2112745 Test Loss: 0.6304650
Validation loss decreased (1.223359 --> 1.211275).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.047566890716553
Epoch: 11, Steps: 60 | Train Loss: 0.2733854 Vali Loss: 1.1995440 Test Loss: 0.6221380
Validation loss decreased (1.211275 --> 1.199544).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.66698932647705
Epoch: 12, Steps: 60 | Train Loss: 0.2634359 Vali Loss: 1.1890010 Test Loss: 0.6149542
Validation loss decreased (1.199544 --> 1.189001).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.16907548904419
Epoch: 13, Steps: 60 | Train Loss: 0.2547466 Vali Loss: 1.1788458 Test Loss: 0.6075833
Validation loss decreased (1.189001 --> 1.178846).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.381459474563599
Epoch: 14, Steps: 60 | Train Loss: 0.2467219 Vali Loss: 1.1679678 Test Loss: 0.5995624
Validation loss decreased (1.178846 --> 1.167968).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.46927523612976
Epoch: 15, Steps: 60 | Train Loss: 0.2395245 Vali Loss: 1.1601729 Test Loss: 0.5935956
Validation loss decreased (1.167968 --> 1.160173).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.304159879684448
Epoch: 16, Steps: 60 | Train Loss: 0.2328678 Vali Loss: 1.1514610 Test Loss: 0.5875358
Validation loss decreased (1.160173 --> 1.151461).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.39855670928955
Epoch: 17, Steps: 60 | Train Loss: 0.2269256 Vali Loss: 1.1437565 Test Loss: 0.5815925
Validation loss decreased (1.151461 --> 1.143757).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.437517881393433
Epoch: 18, Steps: 60 | Train Loss: 0.2214800 Vali Loss: 1.1370314 Test Loss: 0.5765210
Validation loss decreased (1.143757 --> 1.137031).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.63370656967163
Epoch: 19, Steps: 60 | Train Loss: 0.2164832 Vali Loss: 1.1299261 Test Loss: 0.5710417
Validation loss decreased (1.137031 --> 1.129926).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.066688776016235
Epoch: 20, Steps: 60 | Train Loss: 0.2118493 Vali Loss: 1.1238215 Test Loss: 0.5661582
Validation loss decreased (1.129926 --> 1.123821).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.83499813079834
Epoch: 21, Steps: 60 | Train Loss: 0.2076839 Vali Loss: 1.1184411 Test Loss: 0.5619504
Validation loss decreased (1.123821 --> 1.118441).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.067809104919434
Epoch: 22, Steps: 60 | Train Loss: 0.2037152 Vali Loss: 1.1122885 Test Loss: 0.5574079
Validation loss decreased (1.118441 --> 1.112288).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.18699836730957
Epoch: 23, Steps: 60 | Train Loss: 0.2002086 Vali Loss: 1.1073091 Test Loss: 0.5533656
Validation loss decreased (1.112288 --> 1.107309).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.597198247909546
Epoch: 24, Steps: 60 | Train Loss: 0.1968467 Vali Loss: 1.1026950 Test Loss: 0.5494857
Validation loss decreased (1.107309 --> 1.102695).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 9.391436100006104
Epoch: 25, Steps: 60 | Train Loss: 0.1938462 Vali Loss: 1.0976588 Test Loss: 0.5456083
Validation loss decreased (1.102695 --> 1.097659).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 9.642676591873169
Epoch: 26, Steps: 60 | Train Loss: 0.1910095 Vali Loss: 1.0937208 Test Loss: 0.5424439
Validation loss decreased (1.097659 --> 1.093721).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 9.578659057617188
Epoch: 27, Steps: 60 | Train Loss: 0.1883893 Vali Loss: 1.0895971 Test Loss: 0.5390213
Validation loss decreased (1.093721 --> 1.089597).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.66913104057312
Epoch: 28, Steps: 60 | Train Loss: 0.1859738 Vali Loss: 1.0859466 Test Loss: 0.5362580
Validation loss decreased (1.089597 --> 1.085947).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.512399435043335
Epoch: 29, Steps: 60 | Train Loss: 0.1836536 Vali Loss: 1.0820563 Test Loss: 0.5333560
Validation loss decreased (1.085947 --> 1.082056).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.558980703353882
Epoch: 30, Steps: 60 | Train Loss: 0.1815474 Vali Loss: 1.0794352 Test Loss: 0.5310415
Validation loss decreased (1.082056 --> 1.079435).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 9.315186023712158
Epoch: 31, Steps: 60 | Train Loss: 0.1795858 Vali Loss: 1.0757718 Test Loss: 0.5282786
Validation loss decreased (1.079435 --> 1.075772).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.846046924591064
Epoch: 32, Steps: 60 | Train Loss: 0.1777376 Vali Loss: 1.0726588 Test Loss: 0.5261430
Validation loss decreased (1.075772 --> 1.072659).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.387460947036743
Epoch: 33, Steps: 60 | Train Loss: 0.1760441 Vali Loss: 1.0697863 Test Loss: 0.5236557
Validation loss decreased (1.072659 --> 1.069786).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 9.305741786956787
Epoch: 34, Steps: 60 | Train Loss: 0.1745023 Vali Loss: 1.0671332 Test Loss: 0.5216398
Validation loss decreased (1.069786 --> 1.067133).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 9.824479341506958
Epoch: 35, Steps: 60 | Train Loss: 0.1729440 Vali Loss: 1.0651474 Test Loss: 0.5195992
Validation loss decreased (1.067133 --> 1.065147).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 9.311818599700928
Epoch: 36, Steps: 60 | Train Loss: 0.1713826 Vali Loss: 1.0623319 Test Loss: 0.5178373
Validation loss decreased (1.065147 --> 1.062332).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 9.94326663017273
Epoch: 37, Steps: 60 | Train Loss: 0.1700229 Vali Loss: 1.0604261 Test Loss: 0.5160514
Validation loss decreased (1.062332 --> 1.060426).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 9.871057510375977
Epoch: 38, Steps: 60 | Train Loss: 0.1688524 Vali Loss: 1.0583191 Test Loss: 0.5141471
Validation loss decreased (1.060426 --> 1.058319).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 9.936302661895752
Epoch: 39, Steps: 60 | Train Loss: 0.1676597 Vali Loss: 1.0565468 Test Loss: 0.5127283
Validation loss decreased (1.058319 --> 1.056547).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 10.165203094482422
Epoch: 40, Steps: 60 | Train Loss: 0.1666026 Vali Loss: 1.0545347 Test Loss: 0.5110695
Validation loss decreased (1.056547 --> 1.054535).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 10.58683180809021
Epoch: 41, Steps: 60 | Train Loss: 0.1655642 Vali Loss: 1.0527856 Test Loss: 0.5100276
Validation loss decreased (1.054535 --> 1.052786).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 9.81108546257019
Epoch: 42, Steps: 60 | Train Loss: 0.1645364 Vali Loss: 1.0511893 Test Loss: 0.5083959
Validation loss decreased (1.052786 --> 1.051189).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 9.816896915435791
Epoch: 43, Steps: 60 | Train Loss: 0.1636601 Vali Loss: 1.0498941 Test Loss: 0.5072605
Validation loss decreased (1.051189 --> 1.049894).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 9.95631217956543
Epoch: 44, Steps: 60 | Train Loss: 0.1627342 Vali Loss: 1.0485142 Test Loss: 0.5060080
Validation loss decreased (1.049894 --> 1.048514).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 9.371796131134033
Epoch: 45, Steps: 60 | Train Loss: 0.1618458 Vali Loss: 1.0471866 Test Loss: 0.5049579
Validation loss decreased (1.048514 --> 1.047187).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.715168237686157
Epoch: 46, Steps: 60 | Train Loss: 0.1610937 Vali Loss: 1.0453035 Test Loss: 0.5039341
Validation loss decreased (1.047187 --> 1.045303).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 9.651547193527222
Epoch: 47, Steps: 60 | Train Loss: 0.1604145 Vali Loss: 1.0446066 Test Loss: 0.5028571
Validation loss decreased (1.045303 --> 1.044607).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 9.69924259185791
Epoch: 48, Steps: 60 | Train Loss: 0.1596781 Vali Loss: 1.0432605 Test Loss: 0.5018857
Validation loss decreased (1.044607 --> 1.043260).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 8.580300331115723
Epoch: 49, Steps: 60 | Train Loss: 0.1589928 Vali Loss: 1.0423536 Test Loss: 0.5010604
Validation loss decreased (1.043260 --> 1.042354).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 8.577146530151367
Epoch: 50, Steps: 60 | Train Loss: 0.1584175 Vali Loss: 1.0405359 Test Loss: 0.5000396
Validation loss decreased (1.042354 --> 1.040536).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20290816.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.068891763687134
Epoch: 1, Steps: 60 | Train Loss: 0.4134026 Vali Loss: 0.9654099 Test Loss: 0.4344722
Validation loss decreased (inf --> 0.965410).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.542371273040771
Epoch: 2, Steps: 60 | Train Loss: 0.3910408 Vali Loss: 0.9535760 Test Loss: 0.4220490
Validation loss decreased (0.965410 --> 0.953576).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.456848382949829
Epoch: 3, Steps: 60 | Train Loss: 0.3860776 Vali Loss: 0.9553304 Test Loss: 0.4210916
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.691455841064453
Epoch: 4, Steps: 60 | Train Loss: 0.3848721 Vali Loss: 0.9590922 Test Loss: 0.4215848
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.390000104904175
Epoch: 5, Steps: 60 | Train Loss: 0.3837662 Vali Loss: 0.9585747 Test Loss: 0.4221166
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.419347882270813, mae:0.42767614126205444, rse:0.6149567365646362, corr:[0.26413256 0.27197614 0.2711654  0.266963   0.26414707 0.26360887
 0.26390588 0.26416594 0.2637043  0.26298514 0.26237196 0.26187596
 0.2616484  0.26163435 0.26177052 0.26183763 0.2614458  0.26070407
 0.25980702 0.25916854 0.2589885  0.2591092  0.25905946 0.25887758
 0.25877574 0.25901747 0.25923592 0.25900725 0.2584055  0.25763136
 0.25709757 0.25698698 0.25738    0.2579753  0.2582545  0.25816372
 0.25790536 0.25776228 0.2578262  0.25806385 0.25847137 0.25878826
 0.25874096 0.2584408  0.2581179  0.2579145  0.25805902 0.25843856
 0.2584024  0.25799334 0.25708246 0.25582257 0.25447464 0.25309998
 0.25215384 0.25184998 0.2519036  0.25198972 0.25176707 0.25148076
 0.25132558 0.251385   0.25161237 0.25195011 0.2521384  0.2520995
 0.25196224 0.25172764 0.25170985 0.2519329  0.25208044 0.2517874
 0.25116414 0.25042483 0.24974343 0.2494127  0.24917725 0.24874973
 0.24815325 0.24756959 0.24711487 0.24675241 0.24625261 0.24554479
 0.24485406 0.24436471 0.24423806 0.24442688 0.24472721 0.24495819
 0.24492945 0.24472138 0.24454461 0.24460346 0.2450468  0.24577184
 0.24655363 0.24713771 0.24736513 0.24746521 0.24752346 0.24741548
 0.24716946 0.24674684 0.24617837 0.24563798 0.24506857 0.24446273
 0.24405749 0.24381994 0.24383168 0.24417633 0.24453883 0.24484822
 0.24517259 0.24546665 0.24571599 0.24588293 0.24585085 0.24547833
 0.24477243 0.24367772 0.2423607  0.24131376 0.24070004 0.2402869
 0.24007924 0.2399737  0.23970884 0.23946889 0.23912708 0.23863392
 0.23805697 0.2376613  0.23762827 0.2378842  0.23834991 0.2386363
 0.23850772 0.23794264 0.23740993 0.23723842 0.23742221 0.23742597
 0.2369064  0.2357689  0.23430146 0.23294218 0.23222055 0.23193121
 0.23186551 0.23186342 0.2318554  0.23181929 0.23140982 0.23069112
 0.23001285 0.22977182 0.23003629 0.23038587 0.23037417 0.23009622
 0.2295172  0.22897214 0.22886972 0.22900237 0.22903879 0.22840527
 0.22754925 0.22713606 0.22694418 0.22658736 0.22575685 0.22489548
 0.2246827  0.2253008  0.22570573 0.2256589  0.22503172 0.2243059
 0.22383665 0.22336704 0.22254832 0.22174045 0.22152208 0.22189146
 0.22179163 0.2201296  0.21822283 0.21824609 0.21935037 0.21380754]
