Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  24393600.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7369992
	speed: 0.1448s/iter; left time: 796.6204s
Epoch: 1 cost time: 16.34315013885498
Epoch: 1, Steps: 112 | Train Loss: 0.8456115 Vali Loss: 1.7370411 Test Loss: 0.6506353
Validation loss decreased (inf --> 1.737041).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6666198
	speed: 0.2921s/iter; left time: 1573.8841s
Epoch: 2 cost time: 14.023027181625366
Epoch: 2, Steps: 112 | Train Loss: 0.6762108 Vali Loss: 1.5981433 Test Loss: 0.5533469
Validation loss decreased (1.737041 --> 1.598143).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6270641
	speed: 0.3090s/iter; left time: 1630.3912s
Epoch: 3 cost time: 15.385463953018188
Epoch: 3, Steps: 112 | Train Loss: 0.6269495 Vali Loss: 1.5305178 Test Loss: 0.5005818
Validation loss decreased (1.598143 --> 1.530518).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5894634
	speed: 0.3205s/iter; left time: 1655.3445s
Epoch: 4 cost time: 16.256024599075317
Epoch: 4, Steps: 112 | Train Loss: 0.6000462 Vali Loss: 1.4898461 Test Loss: 0.4699840
Validation loss decreased (1.530518 --> 1.489846).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5632368
	speed: 0.3094s/iter; left time: 1563.2279s
Epoch: 5 cost time: 15.077006578445435
Epoch: 5, Steps: 112 | Train Loss: 0.5841758 Vali Loss: 1.4663774 Test Loss: 0.4523888
Validation loss decreased (1.489846 --> 1.466377).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5706789
	speed: 0.3085s/iter; left time: 1524.4193s
Epoch: 6 cost time: 15.445940017700195
Epoch: 6, Steps: 112 | Train Loss: 0.5742700 Vali Loss: 1.4535379 Test Loss: 0.4425505
Validation loss decreased (1.466377 --> 1.453538).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6306590
	speed: 0.3211s/iter; left time: 1550.7376s
Epoch: 7 cost time: 16.167907238006592
Epoch: 7, Steps: 112 | Train Loss: 0.5680290 Vali Loss: 1.4393269 Test Loss: 0.4371968
Validation loss decreased (1.453538 --> 1.439327).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5222335
	speed: 0.3127s/iter; left time: 1475.1226s
Epoch: 8 cost time: 15.348993062973022
Epoch: 8, Steps: 112 | Train Loss: 0.5641861 Vali Loss: 1.4380128 Test Loss: 0.4346102
Validation loss decreased (1.439327 --> 1.438013).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5662950
	speed: 0.3145s/iter; left time: 1448.2503s
Epoch: 9 cost time: 15.673635244369507
Epoch: 9, Steps: 112 | Train Loss: 0.5612781 Vali Loss: 1.4347193 Test Loss: 0.4332937
Validation loss decreased (1.438013 --> 1.434719).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5652139
	speed: 0.3250s/iter; left time: 1460.2378s
Epoch: 10 cost time: 16.454049110412598
Epoch: 10, Steps: 112 | Train Loss: 0.5588935 Vali Loss: 1.4356290 Test Loss: 0.4328055
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5675756
	speed: 0.3077s/iter; left time: 1348.2240s
Epoch: 11 cost time: 15.291220903396606
Epoch: 11, Steps: 112 | Train Loss: 0.5579948 Vali Loss: 1.4356780 Test Loss: 0.4329891
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4971849
	speed: 0.3015s/iter; left time: 1287.2893s
Epoch: 12 cost time: 14.209190845489502
Epoch: 12, Steps: 112 | Train Loss: 0.5571533 Vali Loss: 1.4321458 Test Loss: 0.4327816
Validation loss decreased (1.434719 --> 1.432146).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5460795
	speed: 0.2197s/iter; left time: 913.0901s
Epoch: 13 cost time: 11.041146039962769
Epoch: 13, Steps: 112 | Train Loss: 0.5562901 Vali Loss: 1.4357129 Test Loss: 0.4331295
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5374045
	speed: 0.2822s/iter; left time: 1141.4036s
Epoch: 14 cost time: 14.234135627746582
Epoch: 14, Steps: 112 | Train Loss: 0.5553150 Vali Loss: 1.4375418 Test Loss: 0.4332958
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4950143
	speed: 0.2931s/iter; left time: 1152.7005s
Epoch: 15 cost time: 15.164137840270996
Epoch: 15, Steps: 112 | Train Loss: 0.5546960 Vali Loss: 1.4368948 Test Loss: 0.4335015
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4315808415412903, mae:0.4555113911628723, rse:0.6289021372795105, corr:[0.21794102 0.23207858 0.23016849 0.23073405 0.23330486 0.23304118
 0.23136759 0.23140554 0.23264283 0.23340514 0.2326251  0.23144215
 0.23084177 0.23093288 0.23110433 0.2308299  0.23000677 0.22903988
 0.22862682 0.22909467 0.22973649 0.22989954 0.22977252 0.22984812
 0.23014334 0.23018889 0.22994046 0.22981149 0.23020336 0.23073225
 0.23078136 0.2302796  0.22978239 0.22954313 0.22941263 0.22916734
 0.22900882 0.22875187 0.22859327 0.22872943 0.22911151 0.22932346
 0.22917464 0.22902012 0.22937195 0.23025692 0.23099618 0.23100515
 0.23059732 0.23017877 0.22979015 0.22916993 0.2282554  0.22735702
 0.22689438 0.2264877  0.22610101 0.22529119 0.2244786  0.22416927
 0.2242976  0.22440685 0.22423615 0.22399926 0.22391953 0.22411007
 0.2243763  0.22439392 0.22412342 0.22375128 0.22357622 0.22384073
 0.22389287 0.22348435 0.22289784 0.22264566 0.22276719 0.22279125
 0.22241561 0.22182772 0.22144216 0.22127758 0.22103947 0.22058666
 0.22014585 0.21984945 0.2195873  0.2193242  0.21900637 0.2187597
 0.21865492 0.21891983 0.21937372 0.21991909 0.22031552 0.22100918
 0.22211964 0.22313106 0.22372922 0.2239857  0.22386357 0.22357145
 0.22347263 0.22361922 0.22360687 0.2231386  0.22262982 0.22257808
 0.22286065 0.22303483 0.22286698 0.22267015 0.22273168 0.22286987
 0.22288722 0.22276068 0.22280763 0.22304276 0.22314565 0.22302167
 0.22264603 0.22209686 0.2216205  0.22142713 0.22121301 0.22083402
 0.22033389 0.2199101  0.21967599 0.21959321 0.21952894 0.21928762
 0.21895418 0.21865048 0.21852702 0.21853529 0.21843985 0.2182377
 0.21816501 0.21830812 0.21848315 0.21843828 0.21808717 0.21803766
 0.21823421 0.21822803 0.2178853  0.21731064 0.21677987 0.216406
 0.21619247 0.2162137  0.21621948 0.21596181 0.21559648 0.21545582
 0.2154253  0.21524166 0.21487667 0.21478231 0.21506149 0.21559678
 0.2159839  0.21611781 0.21605805 0.21603177 0.21576822 0.21567565
 0.21581365 0.2161714  0.21656932 0.21695073 0.21710955 0.21707647
 0.21686222 0.21653748 0.21616973 0.21605879 0.2162569  0.21645801
 0.21637951 0.21614864 0.21610302 0.21624008 0.21631345 0.21619554
 0.21617933 0.21651872 0.21698056 0.21691883 0.21634321 0.21579835
 0.21563402 0.21559541 0.2153129  0.21472427 0.2141078  0.21381204
 0.21400216 0.21450724 0.21481903 0.21461566 0.21423765 0.21399663
 0.21396258 0.21395291 0.21382764 0.21365887 0.21361592 0.2137315
 0.21383381 0.21376958 0.21359706 0.21341291 0.21340285 0.21365367
 0.21382797 0.21370131 0.21346349 0.21347246 0.21368726 0.21366665
 0.21325731 0.21276025 0.21247682 0.21236253 0.2120657  0.21159165
 0.21118611 0.21096484 0.21075577 0.21043764 0.21011716 0.21008024
 0.21031703 0.21049574 0.21042435 0.21016122 0.20991163 0.20983826
 0.21009435 0.21056002 0.21123168 0.21169345 0.21146488 0.21091168
 0.21093668 0.21163625 0.2121017  0.21171999 0.2109694  0.21055603
 0.21062817 0.21055505 0.21026142 0.21008147 0.21040408 0.21099623
 0.21130678 0.21110256 0.21079366 0.21049549 0.21019857 0.21007329
 0.21014264 0.21011572 0.20977315 0.2093797  0.20929651 0.20951723
 0.20948038 0.20891207 0.20833202 0.20849316 0.20918119 0.20957364
 0.20941041 0.20922363 0.20918117 0.20909959 0.2087729  0.20857784
 0.2087356  0.20920496 0.20942317 0.20923848 0.20908403 0.20957854
 0.2103432  0.21070687 0.21076205 0.21095951 0.21118468 0.21099885
 0.21043614 0.21003875 0.21009694 0.21044336 0.21070687 0.21081251
 0.21084626 0.21080422 0.21070756 0.21061914 0.21062253 0.21088757
 0.21143432 0.21179716 0.2117579  0.21144485 0.21117334 0.21141157
 0.21211156 0.21261424 0.21245511 0.21189363 0.2113266  0.21102218
 0.21085137 0.21057078 0.21012269 0.20972565 0.20952561 0.20937409
 0.20915586 0.2089917  0.20903857 0.20941742 0.20975599 0.2098981
 0.20974514 0.20933764 0.20898655 0.20893022 0.20904195 0.2090914
 0.208897   0.20854539 0.20836431 0.20818792 0.20761226 0.20682703
 0.2064932  0.20658888 0.20657833 0.20641492 0.20645818 0.20693286
 0.20746817 0.20755517 0.20699355 0.20628887 0.2060406  0.20664564
 0.20748323 0.20787624 0.2076238  0.20707804 0.20647703 0.2063921
 0.20670652 0.20669489 0.2060797  0.20543182 0.2052827  0.20537397
 0.2051267  0.20446718 0.20393641 0.20395769 0.2042078  0.2040594
 0.20329852 0.20246677 0.20197016 0.2016867  0.20160165 0.20188904
 0.20249803 0.20280896 0.20261012 0.20244628 0.20266943 0.20345853
 0.2040742  0.20406164 0.20395397 0.20429224 0.20450558 0.20379108
 0.20264599 0.20182544 0.20137058 0.20085412 0.20019381 0.19998783
 0.20037183 0.20083481 0.20099252 0.20089749 0.20098735 0.2013199
 0.20150834 0.2014014  0.2014247  0.20171537 0.20188029 0.20196058
 0.20214804 0.20254268 0.20304818 0.20349452 0.2038535  0.20388018
 0.20354909 0.20280384 0.20205218 0.20169435 0.20166403 0.20148508
 0.2010676  0.20100139 0.20137024 0.20175883 0.20159602 0.20115723
 0.20108455 0.2015625  0.20185266 0.2020932  0.20239605 0.20313853
 0.20404059 0.20425327 0.2034984  0.20242245 0.20175798 0.20185763
 0.20252949 0.20304327 0.20264834 0.20172794 0.20115526 0.20151183
 0.2022664  0.20255263 0.20218915 0.20160611 0.20128362 0.20124146
 0.20119764 0.20107515 0.20118062 0.20162632 0.20222326 0.20290805
 0.20345606 0.2036488  0.20333958 0.20271854 0.202371   0.20238799
 0.20241015 0.20203021 0.20151626 0.20159224 0.20198536 0.20200908
 0.20143756 0.20111158 0.20163427 0.20237024 0.20221731 0.20133924
 0.20076092 0.20114996 0.20190695 0.20209178 0.20206769 0.20227557
 0.20278578 0.20318812 0.20315063 0.20282093 0.20233442 0.20171057
 0.20104307 0.20052455 0.20001385 0.1997021  0.19965525 0.19990718
 0.20031542 0.20048589 0.20011835 0.19925886 0.19858366 0.19872785
 0.19958831 0.20037603 0.20054744 0.20034833 0.20035997 0.20083879
 0.20132518 0.20141031 0.2012204  0.20135075 0.20175515 0.20162769
 0.20095672 0.20045324 0.20041044 0.20047912 0.20033641 0.20007429
 0.19993022 0.19998437 0.20017621 0.20021218 0.20056483 0.20140995
 0.20228092 0.2027395  0.2032521  0.20374449 0.2047037  0.20485155
 0.20426664 0.2033825  0.2029986  0.20314625 0.20276687 0.2018877
 0.2010834  0.20091632 0.20100757 0.20126478 0.20152324 0.20173244
 0.20157422 0.20114833 0.20133407 0.20218624 0.2030606  0.20302065
 0.20212442 0.20119192 0.20075275 0.20071152 0.20075195 0.20073818
 0.20053066 0.20003591 0.19914906 0.19827762 0.19773732 0.19727033
 0.1966132  0.19612956 0.19604455 0.19613989 0.19590455 0.19545515
 0.1952842  0.19541049 0.1955549  0.1949167  0.19434118 0.19444425
 0.19510977 0.19529952 0.19521907 0.19530895 0.19554067 0.19520052
 0.19408697 0.19327153 0.19333771 0.19395101 0.19378641 0.1926466
 0.19205965 0.19227155 0.1925671  0.19223346 0.19206554 0.19254224
 0.19326952 0.19317418 0.19258896 0.19215648 0.19215414 0.19224511
 0.19233182 0.19267495 0.19320033 0.19347703 0.19331892 0.1931479
 0.19319871 0.19265287 0.19082671 0.18862224 0.18735042 0.1866714
 0.18561344 0.18404397 0.18293935 0.18329626 0.1845084  0.18532912
 0.18508111 0.18429792 0.18383405 0.18356755 0.18330781 0.1834435
 0.18439928 0.18576737 0.18652706 0.18608907 0.18512073 0.18460229
 0.18476446 0.18513897 0.18504426 0.18441804 0.18340863 0.18229493
 0.18158643 0.18112607 0.18055345 0.18007877 0.1798337  0.17989475
 0.18004642 0.1799625  0.17975314 0.17926879 0.17876142 0.17892016
 0.179723   0.1803676  0.1799275  0.17886142 0.17870586 0.1797721
 0.18061161 0.1800989  0.17882873 0.17847967 0.1785158  0.17752187
 0.17563841 0.17449032 0.17493576 0.175693   0.17562345 0.17528425
 0.1754542  0.17555621 0.17496684 0.173957   0.17365418 0.17416342
 0.17445873 0.17417005 0.17363834 0.1734563  0.1733011  0.17294995
 0.17236647 0.17185238 0.17079015 0.16866161 0.16623089 0.1643473
 0.16384214 0.16364823 0.1634033  0.16302764 0.16309977 0.16330299
 0.16299601 0.16218673 0.16170013 0.16131136 0.16107361 0.16155334
 0.16245565 0.16278586 0.16221152 0.16201887 0.16309963 0.16397895
 0.16283174 0.16117679 0.16080938 0.16144615 0.15949555 0.15495592
 0.15261793 0.15428385 0.15651022 0.1558107  0.15424024 0.15452795
 0.1553372  0.15374899 0.15216418 0.15299685 0.15409477 0.15220287
 0.15079086 0.15202066 0.14958552 0.13897072 0.14390479 0.16769454]
