Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  27646080.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.541309356689453
Epoch: 1, Steps: 61 | Train Loss: 0.5983880 Vali Loss: 1.3500168 Test Loss: 0.7468351
Validation loss decreased (inf --> 1.350017).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.446215867996216
Epoch: 2, Steps: 61 | Train Loss: 0.4692130 Vali Loss: 1.2317387 Test Loss: 0.6984767
Validation loss decreased (1.350017 --> 1.231739).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.90931749343872
Epoch: 3, Steps: 61 | Train Loss: 0.4060148 Vali Loss: 1.1935625 Test Loss: 0.6853831
Validation loss decreased (1.231739 --> 1.193563).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.999355792999268
Epoch: 4, Steps: 61 | Train Loss: 0.3677587 Vali Loss: 1.1713991 Test Loss: 0.6774889
Validation loss decreased (1.193563 --> 1.171399).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.181716442108154
Epoch: 5, Steps: 61 | Train Loss: 0.3395220 Vali Loss: 1.1574404 Test Loss: 0.6737710
Validation loss decreased (1.171399 --> 1.157440).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.26473331451416
Epoch: 6, Steps: 61 | Train Loss: 0.3169202 Vali Loss: 1.1330647 Test Loss: 0.6618670
Validation loss decreased (1.157440 --> 1.133065).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.218351125717163
Epoch: 7, Steps: 61 | Train Loss: 0.2979201 Vali Loss: 1.1162878 Test Loss: 0.6547918
Validation loss decreased (1.133065 --> 1.116288).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.544481754302979
Epoch: 8, Steps: 61 | Train Loss: 0.2813107 Vali Loss: 1.0963378 Test Loss: 0.6434877
Validation loss decreased (1.116288 --> 1.096338).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.839447021484375
Epoch: 9, Steps: 61 | Train Loss: 0.2668176 Vali Loss: 1.0813286 Test Loss: 0.6347568
Validation loss decreased (1.096338 --> 1.081329).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.197223663330078
Epoch: 10, Steps: 61 | Train Loss: 0.2538603 Vali Loss: 1.0640347 Test Loss: 0.6264979
Validation loss decreased (1.081329 --> 1.064035).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.930818796157837
Epoch: 11, Steps: 61 | Train Loss: 0.2423913 Vali Loss: 1.0485573 Test Loss: 0.6154680
Validation loss decreased (1.064035 --> 1.048557).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 7.135715961456299
Epoch: 12, Steps: 61 | Train Loss: 0.2321480 Vali Loss: 1.0315580 Test Loss: 0.6059108
Validation loss decreased (1.048557 --> 1.031558).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 7.134525299072266
Epoch: 13, Steps: 61 | Train Loss: 0.2226594 Vali Loss: 1.0174198 Test Loss: 0.5970447
Validation loss decreased (1.031558 --> 1.017420).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 6.80929970741272
Epoch: 14, Steps: 61 | Train Loss: 0.2142845 Vali Loss: 1.0005988 Test Loss: 0.5864896
Validation loss decreased (1.017420 --> 1.000599).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 7.445788145065308
Epoch: 15, Steps: 61 | Train Loss: 0.2066189 Vali Loss: 0.9892694 Test Loss: 0.5781398
Validation loss decreased (1.000599 --> 0.989269).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 7.119274377822876
Epoch: 16, Steps: 61 | Train Loss: 0.1995289 Vali Loss: 0.9779493 Test Loss: 0.5712675
Validation loss decreased (0.989269 --> 0.977949).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.243099212646484
Epoch: 17, Steps: 61 | Train Loss: 0.1931523 Vali Loss: 0.9662877 Test Loss: 0.5625588
Validation loss decreased (0.977949 --> 0.966288).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.464559316635132
Epoch: 18, Steps: 61 | Train Loss: 0.1872637 Vali Loss: 0.9559886 Test Loss: 0.5570999
Validation loss decreased (0.966288 --> 0.955989).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 8.625051736831665
Epoch: 19, Steps: 61 | Train Loss: 0.1817945 Vali Loss: 0.9467747 Test Loss: 0.5502095
Validation loss decreased (0.955989 --> 0.946775).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.339965105056763
Epoch: 20, Steps: 61 | Train Loss: 0.1768862 Vali Loss: 0.9367561 Test Loss: 0.5439807
Validation loss decreased (0.946775 --> 0.936756).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.726831674575806
Epoch: 21, Steps: 61 | Train Loss: 0.1723035 Vali Loss: 0.9327853 Test Loss: 0.5385084
Validation loss decreased (0.936756 --> 0.932785).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.552417278289795
Epoch: 22, Steps: 61 | Train Loss: 0.1680629 Vali Loss: 0.9240215 Test Loss: 0.5333878
Validation loss decreased (0.932785 --> 0.924021).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.573511838912964
Epoch: 23, Steps: 61 | Train Loss: 0.1641544 Vali Loss: 0.9165374 Test Loss: 0.5288898
Validation loss decreased (0.924021 --> 0.916537).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.33066439628601
Epoch: 24, Steps: 61 | Train Loss: 0.1605003 Vali Loss: 0.9084802 Test Loss: 0.5239419
Validation loss decreased (0.916537 --> 0.908480).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.79661750793457
Epoch: 25, Steps: 61 | Train Loss: 0.1571331 Vali Loss: 0.8998850 Test Loss: 0.5191826
Validation loss decreased (0.908480 --> 0.899885).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 7.820326328277588
Epoch: 26, Steps: 61 | Train Loss: 0.1540477 Vali Loss: 0.8958783 Test Loss: 0.5152616
Validation loss decreased (0.899885 --> 0.895878).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.64570927619934
Epoch: 27, Steps: 61 | Train Loss: 0.1511249 Vali Loss: 0.8919579 Test Loss: 0.5112761
Validation loss decreased (0.895878 --> 0.891958).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.148189544677734
Epoch: 28, Steps: 61 | Train Loss: 0.1483658 Vali Loss: 0.8843735 Test Loss: 0.5073763
Validation loss decreased (0.891958 --> 0.884374).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.155384540557861
Epoch: 29, Steps: 61 | Train Loss: 0.1459123 Vali Loss: 0.8803997 Test Loss: 0.5043620
Validation loss decreased (0.884374 --> 0.880400).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.582373142242432
Epoch: 30, Steps: 61 | Train Loss: 0.1435296 Vali Loss: 0.8784289 Test Loss: 0.5014124
Validation loss decreased (0.880400 --> 0.878429).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 9.59316873550415
Epoch: 31, Steps: 61 | Train Loss: 0.1413251 Vali Loss: 0.8709051 Test Loss: 0.4981470
Validation loss decreased (0.878429 --> 0.870905).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 8.821226596832275
Epoch: 32, Steps: 61 | Train Loss: 0.1392714 Vali Loss: 0.8689341 Test Loss: 0.4955252
Validation loss decreased (0.870905 --> 0.868934).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 8.654129981994629
Epoch: 33, Steps: 61 | Train Loss: 0.1373412 Vali Loss: 0.8644628 Test Loss: 0.4932408
Validation loss decreased (0.868934 --> 0.864463).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 6.3170554637908936
Epoch: 34, Steps: 61 | Train Loss: 0.1355097 Vali Loss: 0.8561412 Test Loss: 0.4901371
Validation loss decreased (0.864463 --> 0.856141).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 7.495030641555786
Epoch: 35, Steps: 61 | Train Loss: 0.1338322 Vali Loss: 0.8567392 Test Loss: 0.4882850
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.948042392730713
Epoch: 36, Steps: 61 | Train Loss: 0.1322663 Vali Loss: 0.8533953 Test Loss: 0.4863719
Validation loss decreased (0.856141 --> 0.853395).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 9.009089946746826
Epoch: 37, Steps: 61 | Train Loss: 0.1307102 Vali Loss: 0.8537564 Test Loss: 0.4843755
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 8.990587949752808
Epoch: 38, Steps: 61 | Train Loss: 0.1293044 Vali Loss: 0.8480578 Test Loss: 0.4820489
Validation loss decreased (0.853395 --> 0.848058).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 9.575982809066772
Epoch: 39, Steps: 61 | Train Loss: 0.1279728 Vali Loss: 0.8452713 Test Loss: 0.4802694
Validation loss decreased (0.848058 --> 0.845271).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 9.711410999298096
Epoch: 40, Steps: 61 | Train Loss: 0.1266847 Vali Loss: 0.8452920 Test Loss: 0.4784598
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 8.693652153015137
Epoch: 41, Steps: 61 | Train Loss: 0.1255983 Vali Loss: 0.8422411 Test Loss: 0.4767190
Validation loss decreased (0.845271 --> 0.842241).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 9.087112665176392
Epoch: 42, Steps: 61 | Train Loss: 0.1244828 Vali Loss: 0.8371882 Test Loss: 0.4750628
Validation loss decreased (0.842241 --> 0.837188).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 8.738297700881958
Epoch: 43, Steps: 61 | Train Loss: 0.1234251 Vali Loss: 0.8330404 Test Loss: 0.4739187
Validation loss decreased (0.837188 --> 0.833040).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 7.847222805023193
Epoch: 44, Steps: 61 | Train Loss: 0.1224411 Vali Loss: 0.8328595 Test Loss: 0.4719855
Validation loss decreased (0.833040 --> 0.832860).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 7.674673795700073
Epoch: 45, Steps: 61 | Train Loss: 0.1215387 Vali Loss: 0.8321849 Test Loss: 0.4711571
Validation loss decreased (0.832860 --> 0.832185).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 7.293606519699097
Epoch: 46, Steps: 61 | Train Loss: 0.1206305 Vali Loss: 0.8244367 Test Loss: 0.4696625
Validation loss decreased (0.832185 --> 0.824437).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 9.507484197616577
Epoch: 47, Steps: 61 | Train Loss: 0.1197792 Vali Loss: 0.8302878 Test Loss: 0.4686533
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 8.828593969345093
Epoch: 48, Steps: 61 | Train Loss: 0.1190315 Vali Loss: 0.8259251 Test Loss: 0.4676332
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 10.227991819381714
Epoch: 49, Steps: 61 | Train Loss: 0.1182679 Vali Loss: 0.8246848 Test Loss: 0.4667397
EarlyStopping counter: 3 out of 3
Early stopping
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  27646080.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.94356656074524
Epoch: 1, Steps: 61 | Train Loss: 0.3615409 Vali Loss: 0.7139307 Test Loss: 0.3853066
Validation loss decreased (inf --> 0.713931).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.248666048049927
Epoch: 2, Steps: 61 | Train Loss: 0.3398331 Vali Loss: 0.7044565 Test Loss: 0.3825305
Validation loss decreased (0.713931 --> 0.704457).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.533355951309204
Epoch: 3, Steps: 61 | Train Loss: 0.3367987 Vali Loss: 0.6964650 Test Loss: 0.3823671
Validation loss decreased (0.704457 --> 0.696465).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.487305879592896
Epoch: 4, Steps: 61 | Train Loss: 0.3353376 Vali Loss: 0.6973996 Test Loss: 0.3817292
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.412293910980225
Epoch: 5, Steps: 61 | Train Loss: 0.3345703 Vali Loss: 0.6953666 Test Loss: 0.3818451
Validation loss decreased (0.696465 --> 0.695367).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.370354652404785
Epoch: 6, Steps: 61 | Train Loss: 0.3339775 Vali Loss: 0.6939607 Test Loss: 0.3820187
Validation loss decreased (0.695367 --> 0.693961).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.755634069442749
Epoch: 7, Steps: 61 | Train Loss: 0.3335610 Vali Loss: 0.6971003 Test Loss: 0.3821136
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.923205375671387
Epoch: 8, Steps: 61 | Train Loss: 0.3331076 Vali Loss: 0.6927449 Test Loss: 0.3821402
Validation loss decreased (0.693961 --> 0.692745).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.25104808807373
Epoch: 9, Steps: 61 | Train Loss: 0.3325890 Vali Loss: 0.6974427 Test Loss: 0.3823119
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.154735326766968
Epoch: 10, Steps: 61 | Train Loss: 0.3325115 Vali Loss: 0.6954284 Test Loss: 0.3820920
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.538532018661499
Epoch: 11, Steps: 61 | Train Loss: 0.3321212 Vali Loss: 0.6990541 Test Loss: 0.3824914
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.3817717134952545, mae:0.4044722020626068, rse:0.5868940353393555, corr:[0.27204752 0.27947277 0.2799699  0.27803692 0.2757892  0.2738087
 0.27232018 0.2716189  0.27125812 0.27130216 0.27135697 0.27125004
 0.2712918  0.27135575 0.271152   0.2709498  0.27078617 0.2707343
 0.27038363 0.26959866 0.26886454 0.2686812  0.26872128 0.2690328
 0.26909587 0.26886082 0.26855877 0.26842153 0.2684221  0.26823622
 0.26766124 0.2668269  0.26647985 0.26674575 0.26698825 0.266946
 0.2666806  0.26642388 0.2662042  0.26598385 0.26613232 0.26656932
 0.26690215 0.26690584 0.26673013 0.2665732  0.26673687 0.267107
 0.2667558  0.2658906  0.264732   0.2636075  0.26256853 0.2614841
 0.26079273 0.26051563 0.26020095 0.25982797 0.2591834  0.2588064
 0.25863662 0.2585973  0.25843447 0.25818637 0.25802454 0.25830215
 0.25913253 0.25959513 0.25923017 0.25855544 0.25834832 0.25839493
 0.2582133  0.25744504 0.25633648 0.2559269  0.2558552  0.25520045
 0.2539502  0.25283417 0.25218484 0.25171947 0.25085032 0.24987759
 0.24935682 0.24905102 0.24894668 0.24886051 0.24865264 0.24809663
 0.24733523 0.24669191 0.24616393 0.24528588 0.24611157 0.2495223 ]
