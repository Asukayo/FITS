Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30898560.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.958629608154297
Epoch: 1, Steps: 60 | Train Loss: 0.6655381 Vali Loss: 1.2275273 Test Loss: 0.5779789
Validation loss decreased (inf --> 1.227527).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.810835361480713
Epoch: 2, Steps: 60 | Train Loss: 0.4920808 Vali Loss: 1.0790782 Test Loss: 0.4819149
Validation loss decreased (1.227527 --> 1.079078).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.573214292526245
Epoch: 3, Steps: 60 | Train Loss: 0.4403676 Vali Loss: 1.0186995 Test Loss: 0.4430842
Validation loss decreased (1.079078 --> 1.018700).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.560004949569702
Epoch: 4, Steps: 60 | Train Loss: 0.4177092 Vali Loss: 0.9919847 Test Loss: 0.4267362
Validation loss decreased (1.018700 --> 0.991985).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.447709798812866
Epoch: 5, Steps: 60 | Train Loss: 0.4060380 Vali Loss: 0.9798768 Test Loss: 0.4202099
Validation loss decreased (0.991985 --> 0.979877).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 7.4572224617004395
Epoch: 6, Steps: 60 | Train Loss: 0.3997125 Vali Loss: 0.9737169 Test Loss: 0.4180034
Validation loss decreased (0.979877 --> 0.973717).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 6.204644441604614
Epoch: 7, Steps: 60 | Train Loss: 0.3962976 Vali Loss: 0.9710673 Test Loss: 0.4176112
Validation loss decreased (0.973717 --> 0.971067).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 6.271349191665649
Epoch: 8, Steps: 60 | Train Loss: 0.3937539 Vali Loss: 0.9693569 Test Loss: 0.4172578
Validation loss decreased (0.971067 --> 0.969357).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.603245973587036
Epoch: 9, Steps: 60 | Train Loss: 0.3921207 Vali Loss: 0.9678934 Test Loss: 0.4175183
Validation loss decreased (0.969357 --> 0.967893).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.690915584564209
Epoch: 10, Steps: 60 | Train Loss: 0.3899451 Vali Loss: 0.9672928 Test Loss: 0.4175478
Validation loss decreased (0.967893 --> 0.967293).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.874516725540161
Epoch: 11, Steps: 60 | Train Loss: 0.3889831 Vali Loss: 0.9669358 Test Loss: 0.4178580
Validation loss decreased (0.967293 --> 0.966936).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 6.145505428314209
Epoch: 12, Steps: 60 | Train Loss: 0.3882584 Vali Loss: 0.9665352 Test Loss: 0.4179288
Validation loss decreased (0.966936 --> 0.966535).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.7761969566345215
Epoch: 13, Steps: 60 | Train Loss: 0.3874881 Vali Loss: 0.9663764 Test Loss: 0.4180741
Validation loss decreased (0.966535 --> 0.966376).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.762349843978882
Epoch: 14, Steps: 60 | Train Loss: 0.3866891 Vali Loss: 0.9656244 Test Loss: 0.4180373
Validation loss decreased (0.966376 --> 0.965624).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.070707321166992
Epoch: 15, Steps: 60 | Train Loss: 0.3858703 Vali Loss: 0.9656535 Test Loss: 0.4185955
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.9149091243743896
Epoch: 16, Steps: 60 | Train Loss: 0.3853838 Vali Loss: 0.9657327 Test Loss: 0.4184775
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.7692344188690186
Epoch: 17, Steps: 60 | Train Loss: 0.3849124 Vali Loss: 0.9654961 Test Loss: 0.4185349
Validation loss decreased (0.965624 --> 0.965496).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.7813594341278076
Epoch: 18, Steps: 60 | Train Loss: 0.3845697 Vali Loss: 0.9647729 Test Loss: 0.4184176
Validation loss decreased (0.965496 --> 0.964773).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.320183277130127
Epoch: 19, Steps: 60 | Train Loss: 0.3838670 Vali Loss: 0.9649199 Test Loss: 0.4184723
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.47305965423584
Epoch: 20, Steps: 60 | Train Loss: 0.3836915 Vali Loss: 0.9647510 Test Loss: 0.4184307
Validation loss decreased (0.964773 --> 0.964751).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.4126906394958496
Epoch: 21, Steps: 60 | Train Loss: 0.3835811 Vali Loss: 0.9645139 Test Loss: 0.4186647
Validation loss decreased (0.964751 --> 0.964514).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.2869338989257812
Epoch: 22, Steps: 60 | Train Loss: 0.3834006 Vali Loss: 0.9645660 Test Loss: 0.4188206
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 7.148914098739624
Epoch: 23, Steps: 60 | Train Loss: 0.3828880 Vali Loss: 0.9648080 Test Loss: 0.4187119
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.408617496490479
Epoch: 24, Steps: 60 | Train Loss: 0.3824739 Vali Loss: 0.9641590 Test Loss: 0.4187404
Validation loss decreased (0.964514 --> 0.964159).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.63196611404419
Epoch: 25, Steps: 60 | Train Loss: 0.3822454 Vali Loss: 0.9641309 Test Loss: 0.4186991
Validation loss decreased (0.964159 --> 0.964131).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 8.521434307098389
Epoch: 26, Steps: 60 | Train Loss: 0.3821216 Vali Loss: 0.9648531 Test Loss: 0.4186716
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.370206356048584
Epoch: 27, Steps: 60 | Train Loss: 0.3823190 Vali Loss: 0.9646676 Test Loss: 0.4188627
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 8.477113962173462
Epoch: 28, Steps: 60 | Train Loss: 0.3820787 Vali Loss: 0.9644588 Test Loss: 0.4187843
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.41485363245010376, mae:0.4236116409301758, rse:0.6116524934768677, corr:[0.25728863 0.26988837 0.2698085  0.26870364 0.26797184 0.2659358
 0.26374522 0.26322082 0.26385134 0.26448658 0.264265   0.26357976
 0.2631645  0.26301086 0.26297942 0.26303735 0.26298562 0.26285356
 0.26256862 0.26232752 0.26212004 0.26194394 0.26167187 0.26162106
 0.26163283 0.26148257 0.2611866  0.26103547 0.26102003 0.26084593
 0.26043463 0.2599767  0.25986773 0.25994676 0.25983673 0.2596785
 0.259708   0.25982264 0.25986296 0.25992173 0.2602827  0.26068878
 0.26082137 0.26072985 0.26073244 0.26091498 0.2611391  0.2611473
 0.2606721  0.25993502 0.25904906 0.25802752 0.25681087 0.25557065
 0.25481877 0.2546352  0.25450373 0.25421464 0.2537968  0.25367823
 0.2538162  0.2538742  0.25368282 0.25348723 0.25346726 0.25368643
 0.25407824 0.2542575  0.2542712  0.2541966  0.2541081  0.25380877
 0.25326183 0.25249523 0.25169533 0.25134358 0.25129887 0.25118536
 0.2508296  0.25038013 0.25005326 0.24983093 0.2494603  0.24900757
 0.2488126  0.24888387 0.24897584 0.24887347 0.24861011 0.24838333
 0.2482133  0.24820736 0.24842916 0.24868397 0.24869539 0.24870676
 0.24905506 0.24957532 0.24991228 0.24985501 0.24942105 0.24897368
 0.24898484 0.24921931 0.24913329 0.24880771 0.2485477  0.24845402
 0.24835256 0.24809137 0.24794261 0.24809152 0.24823782 0.2483357
 0.24848934 0.24855131 0.24836434 0.24809793 0.24803066 0.24805221
 0.24769937 0.246535   0.24504794 0.2442584  0.24419157 0.24381559
 0.24293002 0.24208713 0.24181178 0.24213628 0.24227756 0.24198505
 0.24154012 0.24129175 0.24125734 0.24137338 0.24169448 0.2418431
 0.2416249  0.2412893  0.24129589 0.24143188 0.24096316 0.23999304
 0.23920886 0.23874947 0.23805377 0.23669529 0.2354371  0.23491998
 0.23494983 0.23463246 0.23388645 0.23349303 0.23365672 0.23384659
 0.23358011 0.23315015 0.23290038 0.23261386 0.23213504 0.23197648
 0.23219082 0.23222202 0.2318115  0.2314206  0.23143785 0.23129071
 0.23058254 0.2299225  0.22970875 0.22949304 0.22819135 0.22627197
 0.22541004 0.22614111 0.22634622 0.22543117 0.22451353 0.22462225
 0.22432224 0.22284834 0.2218581  0.22251648 0.22263752 0.22099178
 0.22041091 0.22214279 0.22077794 0.21521454 0.2173828  0.23004709]
