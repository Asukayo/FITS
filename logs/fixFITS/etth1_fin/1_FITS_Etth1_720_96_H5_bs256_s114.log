Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  110584320.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.513927698135376
Epoch: 1, Steps: 15 | Train Loss: 0.6814195 Vali Loss: 1.2578903 Test Loss: 0.6769393
Validation loss decreased (inf --> 1.257890).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.7737081050872803
Epoch: 2, Steps: 15 | Train Loss: 0.5281354 Vali Loss: 1.0482100 Test Loss: 0.5553309
Validation loss decreased (1.257890 --> 1.048210).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.9223618507385254
Epoch: 3, Steps: 15 | Train Loss: 0.4566052 Vali Loss: 0.9475045 Test Loss: 0.4973204
Validation loss decreased (1.048210 --> 0.947505).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.75746488571167
Epoch: 4, Steps: 15 | Train Loss: 0.4208689 Vali Loss: 0.8910972 Test Loss: 0.4636216
Validation loss decreased (0.947505 --> 0.891097).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.841228723526001
Epoch: 5, Steps: 15 | Train Loss: 0.4000553 Vali Loss: 0.8600671 Test Loss: 0.4417814
Validation loss decreased (0.891097 --> 0.860067).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.7245888710021973
Epoch: 6, Steps: 15 | Train Loss: 0.3866011 Vali Loss: 0.8374264 Test Loss: 0.4271169
Validation loss decreased (0.860067 --> 0.837426).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.487015962600708
Epoch: 7, Steps: 15 | Train Loss: 0.3766439 Vali Loss: 0.8216128 Test Loss: 0.4168951
Validation loss decreased (0.837426 --> 0.821613).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.8726439476013184
Epoch: 8, Steps: 15 | Train Loss: 0.3699242 Vali Loss: 0.8037705 Test Loss: 0.4096683
Validation loss decreased (0.821613 --> 0.803771).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.7574143409729004
Epoch: 9, Steps: 15 | Train Loss: 0.3644647 Vali Loss: 0.7907447 Test Loss: 0.4045292
Validation loss decreased (0.803771 --> 0.790745).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.637770175933838
Epoch: 10, Steps: 15 | Train Loss: 0.3617429 Vali Loss: 0.7813846 Test Loss: 0.4010281
Validation loss decreased (0.790745 --> 0.781385).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.6746280193328857
Epoch: 11, Steps: 15 | Train Loss: 0.3575285 Vali Loss: 0.7789768 Test Loss: 0.3983433
Validation loss decreased (0.781385 --> 0.778977).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.802157163619995
Epoch: 12, Steps: 15 | Train Loss: 0.3553146 Vali Loss: 0.7706897 Test Loss: 0.3964380
Validation loss decreased (0.778977 --> 0.770690).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.688568353652954
Epoch: 13, Steps: 15 | Train Loss: 0.3536706 Vali Loss: 0.7704243 Test Loss: 0.3949685
Validation loss decreased (0.770690 --> 0.770424).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.723919153213501
Epoch: 14, Steps: 15 | Train Loss: 0.3527160 Vali Loss: 0.7617210 Test Loss: 0.3938329
Validation loss decreased (0.770424 --> 0.761721).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.7710375785827637
Epoch: 15, Steps: 15 | Train Loss: 0.3503155 Vali Loss: 0.7622328 Test Loss: 0.3930173
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.8326096534729004
Epoch: 16, Steps: 15 | Train Loss: 0.3494385 Vali Loss: 0.7605244 Test Loss: 0.3923180
Validation loss decreased (0.761721 --> 0.760524).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.177009344100952
Epoch: 17, Steps: 15 | Train Loss: 0.3490886 Vali Loss: 0.7563868 Test Loss: 0.3918050
Validation loss decreased (0.760524 --> 0.756387).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.822145938873291
Epoch: 18, Steps: 15 | Train Loss: 0.3477023 Vali Loss: 0.7540554 Test Loss: 0.3914084
Validation loss decreased (0.756387 --> 0.754055).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.1000993251800537
Epoch: 19, Steps: 15 | Train Loss: 0.3474177 Vali Loss: 0.7515129 Test Loss: 0.3910013
Validation loss decreased (0.754055 --> 0.751513).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.0935041904449463
Epoch: 20, Steps: 15 | Train Loss: 0.3464102 Vali Loss: 0.7489286 Test Loss: 0.3906399
Validation loss decreased (0.751513 --> 0.748929).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.0479612350463867
Epoch: 21, Steps: 15 | Train Loss: 0.3457218 Vali Loss: 0.7493325 Test Loss: 0.3904843
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.0204079151153564
Epoch: 22, Steps: 15 | Train Loss: 0.3448239 Vali Loss: 0.7484595 Test Loss: 0.3901933
Validation loss decreased (0.748929 --> 0.748459).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.119114398956299
Epoch: 23, Steps: 15 | Train Loss: 0.3451694 Vali Loss: 0.7472836 Test Loss: 0.3900295
Validation loss decreased (0.748459 --> 0.747284).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.1875627040863037
Epoch: 24, Steps: 15 | Train Loss: 0.3445804 Vali Loss: 0.7419130 Test Loss: 0.3898124
Validation loss decreased (0.747284 --> 0.741913).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.149324417114258
Epoch: 25, Steps: 15 | Train Loss: 0.3440278 Vali Loss: 0.7385355 Test Loss: 0.3896960
Validation loss decreased (0.741913 --> 0.738536).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.2941646575927734
Epoch: 26, Steps: 15 | Train Loss: 0.3436845 Vali Loss: 0.7384683 Test Loss: 0.3895738
Validation loss decreased (0.738536 --> 0.738468).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.9865050315856934
Epoch: 27, Steps: 15 | Train Loss: 0.3435011 Vali Loss: 0.7429947 Test Loss: 0.3893938
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.8073251247406006
Epoch: 28, Steps: 15 | Train Loss: 0.3431270 Vali Loss: 0.7449723 Test Loss: 0.3892964
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.7601842880249023
Epoch: 29, Steps: 15 | Train Loss: 0.3429246 Vali Loss: 0.7369188 Test Loss: 0.3891238
Validation loss decreased (0.738468 --> 0.736919).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.8944015502929688
Epoch: 30, Steps: 15 | Train Loss: 0.3427592 Vali Loss: 0.7412080 Test Loss: 0.3890295
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.901726722717285
Epoch: 31, Steps: 15 | Train Loss: 0.3422514 Vali Loss: 0.7424399 Test Loss: 0.3889498
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.739686965942383
Epoch: 32, Steps: 15 | Train Loss: 0.3423273 Vali Loss: 0.7314013 Test Loss: 0.3888862
Validation loss decreased (0.736919 --> 0.731401).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.752248764038086
Epoch: 33, Steps: 15 | Train Loss: 0.3420930 Vali Loss: 0.7355227 Test Loss: 0.3888831
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.959843397140503
Epoch: 34, Steps: 15 | Train Loss: 0.3420450 Vali Loss: 0.7343289 Test Loss: 0.3887970
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.9519002437591553
Epoch: 35, Steps: 15 | Train Loss: 0.3416838 Vali Loss: 0.7352259 Test Loss: 0.3886990
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.3810359835624695, mae:0.40497979521751404, rse:0.5863282680511475, corr:[0.27049324 0.2818302  0.27876034 0.2768586  0.27689627 0.27529526
 0.27331924 0.27256876 0.27270994 0.27280885 0.27246743 0.27193785
 0.2715726  0.2713026  0.27132097 0.2714555  0.27140164 0.27085802
 0.27021945 0.26989004 0.26984698 0.26946524 0.26882824 0.268535
 0.2688346  0.26893035 0.2685623  0.26826704 0.26841614 0.26861435
 0.2683388  0.26765013 0.2672402  0.26734614 0.267478   0.26742497
 0.2673173  0.26737103 0.26749796 0.26764908 0.26792997 0.26830044
 0.26851836 0.26854098 0.26847342 0.26853058 0.26884827 0.26909655
 0.2688259  0.26818746 0.2673051  0.2662884  0.26519835 0.2638554
 0.26280102 0.2622793  0.26219484 0.26220128 0.2618684  0.26168522
 0.26185843 0.26213977 0.26195112 0.26159722 0.2615949  0.26209947
 0.262516   0.26223108 0.26179323 0.26184696 0.2619852  0.2615464
 0.2606793  0.2599034  0.25922057 0.25869825 0.25807652 0.25748265
 0.25702304 0.25624105 0.2552315  0.25462788 0.2544624  0.25386122
 0.2530442  0.25280547 0.2533309  0.25254118 0.2502936  0.24890046
 0.24916072 0.24727172 0.24154276 0.23719618 0.23966634 0.23224966]
