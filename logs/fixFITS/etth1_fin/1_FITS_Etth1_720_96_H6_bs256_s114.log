Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  155947008.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.2201671600341797
Epoch: 1, Steps: 15 | Train Loss: 0.7321932 Vali Loss: 1.3401892 Test Loss: 0.7536857
Validation loss decreased (inf --> 1.340189).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.5550003051757812
Epoch: 2, Steps: 15 | Train Loss: 0.5700388 Vali Loss: 1.1097687 Test Loss: 0.6202812
Validation loss decreased (1.340189 --> 1.109769).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.334089756011963
Epoch: 3, Steps: 15 | Train Loss: 0.4908208 Vali Loss: 1.0027223 Test Loss: 0.5525378
Validation loss decreased (1.109769 --> 1.002722).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.5401809215545654
Epoch: 4, Steps: 15 | Train Loss: 0.4483756 Vali Loss: 0.9393948 Test Loss: 0.5101526
Validation loss decreased (1.002722 --> 0.939395).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.032815456390381
Epoch: 5, Steps: 15 | Train Loss: 0.4227103 Vali Loss: 0.8912722 Test Loss: 0.4797577
Validation loss decreased (0.939395 --> 0.891272).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.5493357181549072
Epoch: 6, Steps: 15 | Train Loss: 0.4041391 Vali Loss: 0.8574384 Test Loss: 0.4576085
Validation loss decreased (0.891272 --> 0.857438).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.5285205841064453
Epoch: 7, Steps: 15 | Train Loss: 0.3907359 Vali Loss: 0.8317461 Test Loss: 0.4408863
Validation loss decreased (0.857438 --> 0.831746).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.126739978790283
Epoch: 8, Steps: 15 | Train Loss: 0.3809568 Vali Loss: 0.8079782 Test Loss: 0.4286357
Validation loss decreased (0.831746 --> 0.807978).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.203033685684204
Epoch: 9, Steps: 15 | Train Loss: 0.3723524 Vali Loss: 0.7913548 Test Loss: 0.4191202
Validation loss decreased (0.807978 --> 0.791355).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.222851514816284
Epoch: 10, Steps: 15 | Train Loss: 0.3662760 Vali Loss: 0.7836758 Test Loss: 0.4122219
Validation loss decreased (0.791355 --> 0.783676).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.2042980194091797
Epoch: 11, Steps: 15 | Train Loss: 0.3610780 Vali Loss: 0.7831609 Test Loss: 0.4068880
Validation loss decreased (0.783676 --> 0.783161).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.494192600250244
Epoch: 12, Steps: 15 | Train Loss: 0.3572603 Vali Loss: 0.7657551 Test Loss: 0.4027987
Validation loss decreased (0.783161 --> 0.765755).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.428874969482422
Epoch: 13, Steps: 15 | Train Loss: 0.3555737 Vali Loss: 0.7589252 Test Loss: 0.3995332
Validation loss decreased (0.765755 --> 0.758925).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.174142599105835
Epoch: 14, Steps: 15 | Train Loss: 0.3525145 Vali Loss: 0.7562186 Test Loss: 0.3971716
Validation loss decreased (0.758925 --> 0.756219).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.2530934810638428
Epoch: 15, Steps: 15 | Train Loss: 0.3511629 Vali Loss: 0.7547259 Test Loss: 0.3952559
Validation loss decreased (0.756219 --> 0.754726).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.9365265369415283
Epoch: 16, Steps: 15 | Train Loss: 0.3492024 Vali Loss: 0.7477762 Test Loss: 0.3937248
Validation loss decreased (0.754726 --> 0.747776).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.134251117706299
Epoch: 17, Steps: 15 | Train Loss: 0.3482968 Vali Loss: 0.7440231 Test Loss: 0.3925941
Validation loss decreased (0.747776 --> 0.744023).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.907470464706421
Epoch: 18, Steps: 15 | Train Loss: 0.3470475 Vali Loss: 0.7411829 Test Loss: 0.3916323
Validation loss decreased (0.744023 --> 0.741183).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.6006057262420654
Epoch: 19, Steps: 15 | Train Loss: 0.3468619 Vali Loss: 0.7412980 Test Loss: 0.3909051
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.4642093181610107
Epoch: 20, Steps: 15 | Train Loss: 0.3448316 Vali Loss: 0.7415212 Test Loss: 0.3902438
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.638540506362915
Epoch: 21, Steps: 15 | Train Loss: 0.3445639 Vali Loss: 0.7375939 Test Loss: 0.3897338
Validation loss decreased (0.741183 --> 0.737594).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.6595518589019775
Epoch: 22, Steps: 15 | Train Loss: 0.3442422 Vali Loss: 0.7383703 Test Loss: 0.3893411
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.3177218437194824
Epoch: 23, Steps: 15 | Train Loss: 0.3436211 Vali Loss: 0.7349202 Test Loss: 0.3889331
Validation loss decreased (0.737594 --> 0.734920).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.603487253189087
Epoch: 24, Steps: 15 | Train Loss: 0.3429810 Vali Loss: 0.7365605 Test Loss: 0.3887967
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.536545753479004
Epoch: 25, Steps: 15 | Train Loss: 0.3427172 Vali Loss: 0.7369202 Test Loss: 0.3885311
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.532052516937256
Epoch: 26, Steps: 15 | Train Loss: 0.3419587 Vali Loss: 0.7313157 Test Loss: 0.3882737
Validation loss decreased (0.734920 --> 0.731316).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.2862746715545654
Epoch: 27, Steps: 15 | Train Loss: 0.3418523 Vali Loss: 0.7320720 Test Loss: 0.3880368
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.3478565216064453
Epoch: 28, Steps: 15 | Train Loss: 0.3408394 Vali Loss: 0.7257351 Test Loss: 0.3879034
Validation loss decreased (0.731316 --> 0.725735).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.2683675289154053
Epoch: 29, Steps: 15 | Train Loss: 0.3415121 Vali Loss: 0.7326461 Test Loss: 0.3877712
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.1940038204193115
Epoch: 30, Steps: 15 | Train Loss: 0.3410699 Vali Loss: 0.7251196 Test Loss: 0.3876059
Validation loss decreased (0.725735 --> 0.725120).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.413740396499634
Epoch: 31, Steps: 15 | Train Loss: 0.3407236 Vali Loss: 0.7281739 Test Loss: 0.3874986
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.517343282699585
Epoch: 32, Steps: 15 | Train Loss: 0.3405521 Vali Loss: 0.7231929 Test Loss: 0.3873808
Validation loss decreased (0.725120 --> 0.723193).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.367326259613037
Epoch: 33, Steps: 15 | Train Loss: 0.3402579 Vali Loss: 0.7253643 Test Loss: 0.3873294
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.458435297012329
Epoch: 34, Steps: 15 | Train Loss: 0.3402488 Vali Loss: 0.7207170 Test Loss: 0.3872697
Validation loss decreased (0.723193 --> 0.720717).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.4421303272247314
Epoch: 35, Steps: 15 | Train Loss: 0.3400508 Vali Loss: 0.7260479 Test Loss: 0.3871559
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.2331056594848633
Epoch: 36, Steps: 15 | Train Loss: 0.3397341 Vali Loss: 0.7223434 Test Loss: 0.3871366
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.0248074531555176
Epoch: 37, Steps: 15 | Train Loss: 0.3394522 Vali Loss: 0.7253160 Test Loss: 0.3870817
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.3796766698360443, mae:0.4035658538341522, rse:0.5852815508842468, corr:[0.27093968 0.28115323 0.27769932 0.27912778 0.27802184 0.2746652
 0.27343968 0.2738481  0.27353764 0.27251148 0.27203736 0.27194422
 0.2716525  0.2711922  0.2709826  0.2711185  0.271156   0.27087152
 0.27022812 0.26975214 0.26946494 0.26887536 0.26827455 0.2682074
 0.26851836 0.268415   0.26817337 0.26830387 0.26871142 0.2686538
 0.26809642 0.26769742 0.2678725  0.26791674 0.26735562 0.2670164
 0.26730978 0.26768887 0.26757842 0.2674014  0.26782605 0.26837862
 0.2685032  0.26833627 0.26829457 0.26851174 0.26883504 0.269009
 0.26882884 0.2682716  0.26749408 0.26660708 0.26537097 0.2640781
 0.263394   0.26287445 0.26214308 0.26191488 0.26207596 0.26206496
 0.26147598 0.2614052  0.26200643 0.26204962 0.26142186 0.26160643
 0.2627185  0.26275173 0.26178992 0.26145887 0.26200342 0.26202607
 0.26121426 0.26017734 0.25917625 0.25879866 0.25855485 0.25785634
 0.25670594 0.25600168 0.2556546  0.25484082 0.25386122 0.25391456
 0.25427338 0.25308987 0.25195548 0.25232267 0.25185677 0.24882571
 0.24684228 0.24719095 0.24322155 0.23468189 0.23516801 0.23221849]
