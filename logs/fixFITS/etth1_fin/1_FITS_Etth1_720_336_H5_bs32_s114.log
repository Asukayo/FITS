Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17814720.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5467643
	speed: 0.1416s/iter; left time: 821.3485s
Epoch: 1 cost time: 16.7862331867218
Epoch: 1, Steps: 118 | Train Loss: 0.6744253 Vali Loss: 1.3295557 Test Loss: 0.5404910
Validation loss decreased (inf --> 1.329556).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5070196
	speed: 0.3506s/iter; left time: 1992.2726s
Epoch: 2 cost time: 15.924586057662964
Epoch: 2, Steps: 118 | Train Loss: 0.5151808 Vali Loss: 1.2229363 Test Loss: 0.4658342
Validation loss decreased (1.329556 --> 1.222936).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4712087
	speed: 0.3311s/iter; left time: 1842.4815s
Epoch: 3 cost time: 16.173298120498657
Epoch: 3, Steps: 118 | Train Loss: 0.4755633 Vali Loss: 1.1905540 Test Loss: 0.4424228
Validation loss decreased (1.222936 --> 1.190554).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4638021
	speed: 0.3577s/iter; left time: 1948.1914s
Epoch: 4 cost time: 16.77090048789978
Epoch: 4, Steps: 118 | Train Loss: 0.4596112 Vali Loss: 1.1852447 Test Loss: 0.4360753
Validation loss decreased (1.190554 --> 1.185245).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4589874
	speed: 0.3432s/iter; left time: 1828.8551s
Epoch: 5 cost time: 16.025176763534546
Epoch: 5, Steps: 118 | Train Loss: 0.4522631 Vali Loss: 1.1870329 Test Loss: 0.4357921
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4505641
	speed: 0.3410s/iter; left time: 1776.7300s
Epoch: 6 cost time: 16.743641138076782
Epoch: 6, Steps: 118 | Train Loss: 0.4477479 Vali Loss: 1.1876649 Test Loss: 0.4363504
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4603575
	speed: 0.3557s/iter; left time: 1811.3703s
Epoch: 7 cost time: 16.5647394657135
Epoch: 7, Steps: 118 | Train Loss: 0.4456148 Vali Loss: 1.1914606 Test Loss: 0.4373701
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.43511369824409485, mae:0.44102907180786133, rse:0.6279909610748291, corr:[0.2480858  0.26083055 0.25380963 0.2510068  0.2544416  0.254831
 0.25299582 0.25214925 0.2529614  0.2532251  0.25251845 0.2519159
 0.25172022 0.25122103 0.25069094 0.2506896  0.25091287 0.25000012
 0.24830751 0.24738574 0.24789095 0.24790786 0.24651718 0.2447773
 0.24470916 0.24597211 0.2467715  0.24651912 0.24607757 0.24634174
 0.24702667 0.24762346 0.24796113 0.24791172 0.24747771 0.24695522
 0.24682312 0.24704799 0.24716954 0.24702215 0.2468963  0.24719241
 0.24770682 0.24795546 0.24791645 0.24811953 0.24870601 0.24892129
 0.24856724 0.24796526 0.24749607 0.247067   0.24628954 0.2453243
 0.24470656 0.2445461  0.2445519  0.24451135 0.24444707 0.24431805
 0.2438769  0.24324875 0.24286449 0.24307966 0.2435892  0.24397919
 0.2441354  0.24410656 0.24432717 0.24447016 0.24414812 0.2434431
 0.24286924 0.24243887 0.24183267 0.24112552 0.24057907 0.24057022
 0.24101715 0.2413912  0.24119785 0.24062051 0.24028863 0.24063475
 0.24117449 0.24120843 0.240472   0.23972426 0.23955946 0.23976739
 0.2396943  0.2392047  0.23863871 0.23842727 0.23851383 0.23889756
 0.23953432 0.24031019 0.24108171 0.24163221 0.24179275 0.24157757
 0.24121423 0.24086216 0.24068594 0.24068788 0.24082872 0.24104956
 0.24118738 0.24128924 0.24133609 0.24129672 0.24108453 0.24101827
 0.2410852  0.24089374 0.24030176 0.23939    0.23870647 0.2388722
 0.23949483 0.23954614 0.2389195  0.23822834 0.23783495 0.2376517
 0.23765121 0.23775499 0.23762426 0.23711164 0.23636124 0.23580384
 0.23570871 0.23573248 0.2355716  0.23545374 0.23553546 0.2357073
 0.23547624 0.23492488 0.23459506 0.23458695 0.2344016  0.23373571
 0.23304287 0.23302764 0.23361105 0.23388617 0.23343806 0.23255263
 0.23192611 0.23177658 0.2317213  0.23135631 0.23082525 0.2307822
 0.23131694 0.23211657 0.23260039 0.23278743 0.23265028 0.2323495
 0.23192501 0.23145555 0.2310209  0.23052862 0.22976184 0.22917278
 0.22910585 0.22944522 0.22979015 0.22994903 0.22978163 0.22955026
 0.22962223 0.2301734  0.23057267 0.23050015 0.23019649 0.23009433
 0.2300927  0.22986068 0.22945814 0.229532   0.23046891 0.2317323
 0.23230405 0.23189971 0.23126361 0.23116772 0.23149551 0.23141368
 0.23049729 0.22917087 0.22796415 0.22709887 0.22669114 0.22655292
 0.22637402 0.2261499  0.22597238 0.22587521 0.22575366 0.22569856
 0.22574133 0.22613475 0.22677805 0.22743554 0.22776775 0.22755933
 0.2270338  0.2264694  0.22597401 0.22544414 0.22503543 0.22518018
 0.22570886 0.2259826  0.22572565 0.22545412 0.22554013 0.22563207
 0.2255578  0.22550337 0.22539017 0.2248798  0.22388273 0.22321369
 0.22322269 0.2234807  0.22334334 0.22276172 0.22221692 0.22233386
 0.2226539  0.22247896 0.22196506 0.22188295 0.22260827 0.22332868
 0.22358897 0.2235887  0.22386579 0.224134   0.22374514 0.22284617
 0.22212902 0.22188474 0.22161558 0.22105326 0.22059953 0.22074263
 0.22119707 0.22135457 0.22138506 0.22148095 0.22163188 0.2215513
 0.22143427 0.22151667 0.22188337 0.22191313 0.22143283 0.22083637
 0.22062366 0.22066854 0.22064318 0.22043577 0.21990123 0.21902965
 0.2183592  0.2182499  0.21853496 0.21875253 0.21819322 0.21745375
 0.21764834 0.21870206 0.21905622 0.21805704 0.21695726 0.21740286
 0.2186108  0.21849087 0.21682033 0.21524069 0.2152365  0.21614565
 0.21656564 0.21641503 0.2162697  0.21652952 0.21649408 0.21595049
 0.2152929  0.21497867 0.21486062 0.21476205 0.21467464 0.21470724
 0.2143836  0.21340741 0.21304765 0.21414743 0.21590106 0.21636699
 0.2156625  0.21511142 0.2159052  0.21626933 0.21482301 0.21288042
 0.21262735 0.21364254 0.21361399 0.21187639 0.21021636 0.20959623
 0.20917623 0.20762071 0.20594873 0.20564665 0.20569855 0.2045976
 0.20337118 0.20324218 0.20326214 0.20173343 0.19922803 0.19924922
 0.20008236 0.19514246 0.18501657 0.18169409 0.18858364 0.17546907]
