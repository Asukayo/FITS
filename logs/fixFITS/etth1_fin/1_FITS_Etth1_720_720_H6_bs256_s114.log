Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  275365888.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.9299330711364746
Epoch: 1, Steps: 14 | Train Loss: 1.0319634 Vali Loss: 2.2443833 Test Loss: 0.9752092
Validation loss decreased (inf --> 2.244383).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.8246049880981445
Epoch: 2, Steps: 14 | Train Loss: 0.8978184 Vali Loss: 2.0179422 Test Loss: 0.8417646
Validation loss decreased (2.244383 --> 2.017942).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.5388386249542236
Epoch: 3, Steps: 14 | Train Loss: 0.8164964 Vali Loss: 1.8911682 Test Loss: 0.7628134
Validation loss decreased (2.017942 --> 1.891168).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.7057230472564697
Epoch: 4, Steps: 14 | Train Loss: 0.7689171 Vali Loss: 1.8160977 Test Loss: 0.7169896
Validation loss decreased (1.891168 --> 1.816098).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.7298669815063477
Epoch: 5, Steps: 14 | Train Loss: 0.7396356 Vali Loss: 1.7716289 Test Loss: 0.6888530
Validation loss decreased (1.816098 --> 1.771629).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.267965793609619
Epoch: 6, Steps: 14 | Train Loss: 0.7205955 Vali Loss: 1.7412419 Test Loss: 0.6699640
Validation loss decreased (1.771629 --> 1.741242).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.6591923236846924
Epoch: 7, Steps: 14 | Train Loss: 0.7070616 Vali Loss: 1.7151885 Test Loss: 0.6558183
Validation loss decreased (1.741242 --> 1.715189).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.367832899093628
Epoch: 8, Steps: 14 | Train Loss: 0.6961922 Vali Loss: 1.7034431 Test Loss: 0.6445690
Validation loss decreased (1.715189 --> 1.703443).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.6279070377349854
Epoch: 9, Steps: 14 | Train Loss: 0.6876912 Vali Loss: 1.6871607 Test Loss: 0.6349790
Validation loss decreased (1.703443 --> 1.687161).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.475339889526367
Epoch: 10, Steps: 14 | Train Loss: 0.6804765 Vali Loss: 1.6689858 Test Loss: 0.6265444
Validation loss decreased (1.687161 --> 1.668986).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.5444869995117188
Epoch: 11, Steps: 14 | Train Loss: 0.6736889 Vali Loss: 1.6611837 Test Loss: 0.6191662
Validation loss decreased (1.668986 --> 1.661184).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.2522408962249756
Epoch: 12, Steps: 14 | Train Loss: 0.6684181 Vali Loss: 1.6511724 Test Loss: 0.6124992
Validation loss decreased (1.661184 --> 1.651172).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.5262718200683594
Epoch: 13, Steps: 14 | Train Loss: 0.6633916 Vali Loss: 1.6377485 Test Loss: 0.6063843
Validation loss decreased (1.651172 --> 1.637748).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.186335563659668
Epoch: 14, Steps: 14 | Train Loss: 0.6591302 Vali Loss: 1.6306041 Test Loss: 0.6007292
Validation loss decreased (1.637748 --> 1.630604).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.2430996894836426
Epoch: 15, Steps: 14 | Train Loss: 0.6547453 Vali Loss: 1.6258354 Test Loss: 0.5956303
Validation loss decreased (1.630604 --> 1.625835).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.6199588775634766
Epoch: 16, Steps: 14 | Train Loss: 0.6508593 Vali Loss: 1.6175838 Test Loss: 0.5908794
Validation loss decreased (1.625835 --> 1.617584).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.516627788543701
Epoch: 17, Steps: 14 | Train Loss: 0.6477413 Vali Loss: 1.6118984 Test Loss: 0.5865943
Validation loss decreased (1.617584 --> 1.611898).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.6322152614593506
Epoch: 18, Steps: 14 | Train Loss: 0.6448395 Vali Loss: 1.6101488 Test Loss: 0.5824920
Validation loss decreased (1.611898 --> 1.610149).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.4457459449768066
Epoch: 19, Steps: 14 | Train Loss: 0.6422417 Vali Loss: 1.6013867 Test Loss: 0.5787449
Validation loss decreased (1.610149 --> 1.601387).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.555166482925415
Epoch: 20, Steps: 14 | Train Loss: 0.6395670 Vali Loss: 1.5895916 Test Loss: 0.5752891
Validation loss decreased (1.601387 --> 1.589592).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.3165926933288574
Epoch: 21, Steps: 14 | Train Loss: 0.6369201 Vali Loss: 1.5899639 Test Loss: 0.5720202
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.4021332263946533
Epoch: 22, Steps: 14 | Train Loss: 0.6349038 Vali Loss: 1.5892606 Test Loss: 0.5689628
Validation loss decreased (1.589592 --> 1.589261).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.400808572769165
Epoch: 23, Steps: 14 | Train Loss: 0.6327224 Vali Loss: 1.5824459 Test Loss: 0.5661538
Validation loss decreased (1.589261 --> 1.582446).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.3536715507507324
Epoch: 24, Steps: 14 | Train Loss: 0.6312560 Vali Loss: 1.5807688 Test Loss: 0.5634879
Validation loss decreased (1.582446 --> 1.580769).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.5387067794799805
Epoch: 25, Steps: 14 | Train Loss: 0.6293824 Vali Loss: 1.5748178 Test Loss: 0.5609974
Validation loss decreased (1.580769 --> 1.574818).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.4538395404815674
Epoch: 26, Steps: 14 | Train Loss: 0.6274463 Vali Loss: 1.5741425 Test Loss: 0.5587097
Validation loss decreased (1.574818 --> 1.574142).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.4157707691192627
Epoch: 27, Steps: 14 | Train Loss: 0.6262097 Vali Loss: 1.5709698 Test Loss: 0.5565258
Validation loss decreased (1.574142 --> 1.570970).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.352907180786133
Epoch: 28, Steps: 14 | Train Loss: 0.6244659 Vali Loss: 1.5668824 Test Loss: 0.5545188
Validation loss decreased (1.570970 --> 1.566882).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.4789817333221436
Epoch: 29, Steps: 14 | Train Loss: 0.6232892 Vali Loss: 1.5626234 Test Loss: 0.5525543
Validation loss decreased (1.566882 --> 1.562623).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.672046184539795
Epoch: 30, Steps: 14 | Train Loss: 0.6222200 Vali Loss: 1.5642700 Test Loss: 0.5507916
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.769324779510498
Epoch: 31, Steps: 14 | Train Loss: 0.6210107 Vali Loss: 1.5604408 Test Loss: 0.5490880
Validation loss decreased (1.562623 --> 1.560441).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.1574785709381104
Epoch: 32, Steps: 14 | Train Loss: 0.6197125 Vali Loss: 1.5595893 Test Loss: 0.5474974
Validation loss decreased (1.560441 --> 1.559589).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.0831682682037354
Epoch: 33, Steps: 14 | Train Loss: 0.6188681 Vali Loss: 1.5530841 Test Loss: 0.5460210
Validation loss decreased (1.559589 --> 1.553084).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.540170907974243
Epoch: 34, Steps: 14 | Train Loss: 0.6183582 Vali Loss: 1.5591255 Test Loss: 0.5445977
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.851257801055908
Epoch: 35, Steps: 14 | Train Loss: 0.6173678 Vali Loss: 1.5581155 Test Loss: 0.5432650
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.6117782592773438
Epoch: 36, Steps: 14 | Train Loss: 0.6163210 Vali Loss: 1.5497842 Test Loss: 0.5419865
Validation loss decreased (1.553084 --> 1.549784).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.5563063621520996
Epoch: 37, Steps: 14 | Train Loss: 0.6156890 Vali Loss: 1.5528779 Test Loss: 0.5408131
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.586472749710083
Epoch: 38, Steps: 14 | Train Loss: 0.6149951 Vali Loss: 1.5516756 Test Loss: 0.5396944
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.2689640522003174
Epoch: 39, Steps: 14 | Train Loss: 0.6141270 Vali Loss: 1.5536906 Test Loss: 0.5386507
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.5153276324272156, mae:0.5140106081962585, rse:0.6872166395187378, corr:[0.2063872  0.22824076 0.21539624 0.22221592 0.2288698  0.22635446
 0.22576982 0.22926913 0.23099151 0.23001468 0.22934861 0.22923915
 0.2285227  0.22785668 0.22652642 0.22383422 0.22202118 0.2216009
 0.22039306 0.21812323 0.21761404 0.21825449 0.21743011 0.21535595
 0.21588993 0.21816732 0.21897155 0.21891952 0.22027856 0.22195162
 0.2219519  0.22129212 0.22176188 0.22236821 0.22190993 0.22093442
 0.22040838 0.21999742 0.21919872 0.21845333 0.2175546  0.21677178
 0.21646947 0.21635716 0.2158983  0.21632293 0.21730357 0.21753529
 0.21760568 0.21825854 0.21955281 0.21984911 0.21860301 0.2176004
 0.21763569 0.21716876 0.21634242 0.21545392 0.21547225 0.21556807
 0.21491385 0.2138969  0.21327318 0.2126747  0.21223566 0.21230976
 0.21229997 0.21189146 0.21176313 0.21178655 0.2114684  0.21053648
 0.20945768 0.2086756  0.20883362 0.20956588 0.20985763 0.2095143
 0.20921066 0.20948133 0.20974684 0.20923339 0.20828782 0.20786895
 0.20754638 0.20654498 0.20474286 0.20347187 0.20323226 0.20309244
 0.20245664 0.20196651 0.20172103 0.20154436 0.20069902 0.20045461
 0.20151132 0.20344293 0.20536682 0.20686929 0.20791453 0.20863192
 0.20888482 0.2087682  0.20839305 0.20804761 0.20779665 0.20722015
 0.20589447 0.20517515 0.20534971 0.20517446 0.20433092 0.20384613
 0.20410015 0.2039445  0.20308344 0.2025156  0.20269457 0.20310949
 0.20294817 0.20241362 0.20255467 0.20339425 0.20347951 0.20298435
 0.20293882 0.20331936 0.20356113 0.20323756 0.20271952 0.2024362
 0.20223585 0.2014827  0.20053928 0.20026119 0.20033826 0.20024124
 0.1996176  0.19911176 0.19914845 0.19906114 0.19785486 0.19687872
 0.19699128 0.19754909 0.19759588 0.19729532 0.19704452 0.19699839
 0.19697312 0.19674192 0.19644214 0.19633143 0.1963959  0.1962267
 0.19543436 0.19509403 0.19539186 0.19547494 0.19500561 0.19481409
 0.19509506 0.19485413 0.1939639  0.19359727 0.19255559 0.19147158
 0.19070531 0.19113801 0.19289842 0.19505787 0.19608614 0.1962952
 0.19644806 0.19664629 0.1966266  0.19652401 0.19647872 0.19662994
 0.19677912 0.19666155 0.1966079  0.19687234 0.19724868 0.19722608
 0.19706449 0.19724369 0.1974142  0.19699854 0.19645521 0.19645931
 0.19672443 0.1966367  0.19651866 0.1969478  0.19734119 0.19707179
 0.19672546 0.19681014 0.19691423 0.19669475 0.19661966 0.19672304
 0.19716825 0.197998   0.19864833 0.19857837 0.19827512 0.19806589
 0.19763514 0.19654895 0.19526026 0.19427112 0.19325429 0.19265254
 0.19280726 0.19360423 0.19457096 0.19534321 0.19586118 0.19636633
 0.19672006 0.1964422  0.19548969 0.19457212 0.19420564 0.19441995
 0.1941994  0.1938245  0.19378781 0.19379583 0.1934588  0.1929153
 0.1924918  0.1919243  0.19096117 0.19001287 0.18941283 0.18912686
 0.1894366  0.19013864 0.19121225 0.19219278 0.19278112 0.19303091
 0.19307394 0.19278324 0.19225141 0.19184391 0.19158922 0.1912262
 0.19085659 0.19103529 0.19126603 0.190879   0.19054522 0.19090988
 0.19119175 0.19051512 0.1894675  0.18872684 0.18813577 0.18755521
 0.18725704 0.18720785 0.18741526 0.18795283 0.1885367  0.1889858
 0.18879947 0.18794137 0.18739033 0.18731791 0.18715212 0.18700601
 0.18683106 0.18676467 0.18637115 0.18598434 0.18536721 0.18473895
 0.18400234 0.18358423 0.18265295 0.18161789 0.18079475 0.18071224
 0.18105459 0.18173654 0.18300556 0.18449405 0.18555127 0.18612775
 0.18648188 0.18655044 0.1862415  0.18612783 0.18633007 0.18655963
 0.18637453 0.18593872 0.18547983 0.18537395 0.18574667 0.18604793
 0.18565813 0.18449041 0.18346202 0.18305883 0.18279277 0.18263637
 0.18265803 0.182939   0.18405023 0.18529339 0.18542685 0.18475321
 0.18404514 0.1832997  0.18259887 0.1821732  0.18257539 0.18340938
 0.183732   0.18336965 0.18325166 0.18343614 0.18340757 0.18363002
 0.18426038 0.1842515  0.18335284 0.18285963 0.18280277 0.1826867
 0.1822095  0.1821443  0.1830279  0.18389101 0.18357164 0.18273441
 0.18207738 0.18118043 0.18029934 0.18060763 0.18095782 0.1805219
 0.17987718 0.18006238 0.18047588 0.18069582 0.18120937 0.18205415
 0.18200043 0.18116795 0.18063304 0.18034887 0.17918977 0.17831661
 0.17889853 0.17989086 0.18037976 0.18068403 0.1809305  0.18056962
 0.17981839 0.1792735  0.17894411 0.17851669 0.17806318 0.17765054
 0.17673002 0.17569926 0.17512448 0.17472339 0.17408806 0.17363162
 0.17373186 0.173259   0.17188305 0.17080747 0.16990854 0.16922405
 0.16943103 0.17099911 0.17371342 0.17622109 0.17740399 0.17743294
 0.17708261 0.17590168 0.17401548 0.17240506 0.17172432 0.17204046
 0.17221838 0.17201525 0.17194402 0.17202708 0.17212383 0.17179962
 0.17124994 0.17083703 0.17062657 0.17010379 0.16913833 0.16860335
 0.16920416 0.17052591 0.17223617 0.17388245 0.17515424 0.1754983
 0.17572744 0.17583044 0.17577076 0.17555559 0.17542316 0.1753662
 0.17502685 0.17479327 0.17468125 0.17465329 0.17444156 0.17411931
 0.17379141 0.1731596  0.17185837 0.1711492  0.17072925 0.17058127
 0.17132995 0.17288698 0.17481317 0.1767622  0.17800732 0.17850137
 0.17888223 0.17878658 0.17836082 0.17846368 0.17875823 0.17918383
 0.17939311 0.17918989 0.17908172 0.17903924 0.17882493 0.17812847
 0.17750794 0.17740045 0.17732197 0.17640331 0.17559595 0.17610075
 0.17721815 0.17834006 0.18011986 0.18227431 0.18325894 0.18267155
 0.18259887 0.18317871 0.18330978 0.1831839  0.18317197 0.183374
 0.1835112  0.18400219 0.18474333 0.18454237 0.18348236 0.18329526
 0.183692   0.18307254 0.18149255 0.18059073 0.18035413 0.17977548
 0.17943665 0.18001345 0.18090801 0.18158457 0.1822069  0.18267448
 0.18288882 0.18257158 0.18223034 0.18236774 0.18235043 0.18225582
 0.18224323 0.18173648 0.18102007 0.18086484 0.18106999 0.18074001
 0.17973733 0.17874594 0.17792109 0.17706089 0.17627275 0.17634246
 0.17709555 0.17787765 0.17890571 0.18068281 0.18224107 0.18229547
 0.18161763 0.18153152 0.18149112 0.18126468 0.18162867 0.18215486
 0.18193571 0.18170197 0.18250835 0.18332635 0.18353654 0.18354583
 0.18363434 0.18318449 0.18271995 0.18237267 0.18247621 0.18209104
 0.18241473 0.18306394 0.18302478 0.18292782 0.1831348  0.18368961
 0.18352388 0.18293871 0.18314089 0.18377346 0.18355775 0.18326174
 0.18332453 0.18326895 0.18365519 0.18454437 0.18517007 0.1844676
 0.18366918 0.1837422  0.18329766 0.18176562 0.18048735 0.17986412
 0.1789717  0.17828062 0.17885934 0.18002966 0.17991368 0.17885354
 0.17899415 0.1797955  0.17942172 0.17850462 0.17814612 0.17792262
 0.17725332 0.1767777  0.17690158 0.17606853 0.17550321 0.17575553
 0.17577153 0.17442861 0.17340271 0.17330423 0.17219591 0.17013876
 0.16930492 0.16994765 0.17011349 0.17065252 0.17199545 0.17300455
 0.17298031 0.17228992 0.17270465 0.17304161 0.17269687 0.17233346
 0.17223123 0.17145324 0.17128065 0.17216152 0.17266293 0.17205144
 0.17179523 0.17194417 0.17080964 0.16920261 0.16870584 0.16806504
 0.16640544 0.16512734 0.1659721  0.16717993 0.16657238 0.16515094
 0.16540334 0.16586925 0.16491573 0.16453546 0.16506256 0.16514315
 0.16466519 0.16473371 0.16500232 0.16473132 0.16481379 0.16522066
 0.1647282  0.1637228  0.16384856 0.16430105 0.16325055 0.16181684
 0.16205311 0.16293491 0.16272435 0.16296566 0.16434658 0.16469559
 0.1638332  0.16366217 0.16456828 0.16467777 0.16404423 0.1649067
 0.16626337 0.16618422 0.16589937 0.1660177  0.1654426  0.16468406
 0.16470596 0.16432565 0.16233678 0.16124073 0.16240709 0.16284609
 0.16135998 0.1610075  0.16296226 0.16455346 0.16374637 0.16315795
 0.16401564 0.16375248 0.16261213 0.1630035  0.16375837 0.16351125
 0.16351213 0.16407461 0.16385442 0.16318159 0.16383228 0.16465624
 0.16365133 0.16274774 0.1629458  0.16211614 0.16004224 0.16007525
 0.16121255 0.16002752 0.15770407 0.15815863 0.16005819 0.15824744
 0.15596634 0.15693435 0.15907393 0.15796518 0.15680684 0.15829727
 0.15950571 0.15874715 0.15871981 0.1591845  0.15869555 0.15862747
 0.15921631 0.15824056 0.15636727 0.1567463  0.15760708 0.15545593
 0.15325868 0.15562376 0.15781233 0.1566819  0.15533787 0.15666838
 0.1575828  0.15557665 0.15477583 0.15559384 0.15469517 0.15308608
 0.15293817 0.15094444 0.1483063  0.14841866 0.14763969 0.14161174
 0.1392418  0.14107981 0.13067126 0.11320272 0.12365692 0.10889   ]
