Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  97574400.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.850976228713989
Epoch: 1, Steps: 28 | Train Loss: 0.9148521 Vali Loss: 2.3674998 Test Loss: 1.0343897
Validation loss decreased (inf --> 2.367500).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.905212640762329
Epoch: 2, Steps: 28 | Train Loss: 0.7760606 Vali Loss: 2.1478844 Test Loss: 0.9184898
Validation loss decreased (2.367500 --> 2.147884).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.735860347747803
Epoch: 3, Steps: 28 | Train Loss: 0.6904239 Vali Loss: 2.0228522 Test Loss: 0.8489840
Validation loss decreased (2.147884 --> 2.022852).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.927413702011108
Epoch: 4, Steps: 28 | Train Loss: 0.6365249 Vali Loss: 1.9521424 Test Loss: 0.8063386
Validation loss decreased (2.022852 --> 1.952142).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.422408103942871
Epoch: 5, Steps: 28 | Train Loss: 0.6011908 Vali Loss: 1.9018265 Test Loss: 0.7798635
Validation loss decreased (1.952142 --> 1.901827).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.240885496139526
Epoch: 6, Steps: 28 | Train Loss: 0.5764069 Vali Loss: 1.8676217 Test Loss: 0.7619112
Validation loss decreased (1.901827 --> 1.867622).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.683557748794556
Epoch: 7, Steps: 28 | Train Loss: 0.5582355 Vali Loss: 1.8418713 Test Loss: 0.7494231
Validation loss decreased (1.867622 --> 1.841871).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.958845138549805
Epoch: 8, Steps: 28 | Train Loss: 0.5440243 Vali Loss: 1.8200741 Test Loss: 0.7400546
Validation loss decreased (1.841871 --> 1.820074).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.906992197036743
Epoch: 9, Steps: 28 | Train Loss: 0.5322897 Vali Loss: 1.8103714 Test Loss: 0.7324535
Validation loss decreased (1.820074 --> 1.810371).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.9496142864227295
Epoch: 10, Steps: 28 | Train Loss: 0.5226779 Vali Loss: 1.7964034 Test Loss: 0.7256898
Validation loss decreased (1.810371 --> 1.796403).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.200279235839844
Epoch: 11, Steps: 28 | Train Loss: 0.5141170 Vali Loss: 1.7888274 Test Loss: 0.7200067
Validation loss decreased (1.796403 --> 1.788827).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.816785573959351
Epoch: 12, Steps: 28 | Train Loss: 0.5068212 Vali Loss: 1.7802196 Test Loss: 0.7145057
Validation loss decreased (1.788827 --> 1.780220).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.236666917800903
Epoch: 13, Steps: 28 | Train Loss: 0.4999876 Vali Loss: 1.7731167 Test Loss: 0.7099555
Validation loss decreased (1.780220 --> 1.773117).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.237219333648682
Epoch: 14, Steps: 28 | Train Loss: 0.4941194 Vali Loss: 1.7665536 Test Loss: 0.7051913
Validation loss decreased (1.773117 --> 1.766554).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.550872564315796
Epoch: 15, Steps: 28 | Train Loss: 0.4884867 Vali Loss: 1.7604297 Test Loss: 0.7010524
Validation loss decreased (1.766554 --> 1.760430).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.955026149749756
Epoch: 16, Steps: 28 | Train Loss: 0.4834672 Vali Loss: 1.7572403 Test Loss: 0.6974679
Validation loss decreased (1.760430 --> 1.757240).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.861294269561768
Epoch: 17, Steps: 28 | Train Loss: 0.4790413 Vali Loss: 1.7485576 Test Loss: 0.6936544
Validation loss decreased (1.757240 --> 1.748558).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.862351894378662
Epoch: 18, Steps: 28 | Train Loss: 0.4744565 Vali Loss: 1.7437627 Test Loss: 0.6900517
Validation loss decreased (1.748558 --> 1.743763).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.8468217849731445
Epoch: 19, Steps: 28 | Train Loss: 0.4707297 Vali Loss: 1.7377646 Test Loss: 0.6870587
Validation loss decreased (1.743763 --> 1.737765).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.67734956741333
Epoch: 20, Steps: 28 | Train Loss: 0.4669363 Vali Loss: 1.7364686 Test Loss: 0.6838946
Validation loss decreased (1.737765 --> 1.736469).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.301331996917725
Epoch: 21, Steps: 28 | Train Loss: 0.4636943 Vali Loss: 1.7322965 Test Loss: 0.6809379
Validation loss decreased (1.736469 --> 1.732296).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.510685205459595
Epoch: 22, Steps: 28 | Train Loss: 0.4604004 Vali Loss: 1.7275814 Test Loss: 0.6783370
Validation loss decreased (1.732296 --> 1.727581).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.884759426116943
Epoch: 23, Steps: 28 | Train Loss: 0.4574046 Vali Loss: 1.7255654 Test Loss: 0.6756403
Validation loss decreased (1.727581 --> 1.725565).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.984212160110474
Epoch: 24, Steps: 28 | Train Loss: 0.4547063 Vali Loss: 1.7178099 Test Loss: 0.6731268
Validation loss decreased (1.725565 --> 1.717810).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.7406651973724365
Epoch: 25, Steps: 28 | Train Loss: 0.4520334 Vali Loss: 1.7116952 Test Loss: 0.6709114
Validation loss decreased (1.717810 --> 1.711695).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.906842231750488
Epoch: 26, Steps: 28 | Train Loss: 0.4496216 Vali Loss: 1.7100912 Test Loss: 0.6687288
Validation loss decreased (1.711695 --> 1.710091).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.813183307647705
Epoch: 27, Steps: 28 | Train Loss: 0.4472736 Vali Loss: 1.7098539 Test Loss: 0.6664488
Validation loss decreased (1.710091 --> 1.709854).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.825442314147949
Epoch: 28, Steps: 28 | Train Loss: 0.4453888 Vali Loss: 1.7078998 Test Loss: 0.6645854
Validation loss decreased (1.709854 --> 1.707900).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.89968466758728
Epoch: 29, Steps: 28 | Train Loss: 0.4432694 Vali Loss: 1.7018019 Test Loss: 0.6627668
Validation loss decreased (1.707900 --> 1.701802).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 5.259113550186157
Epoch: 30, Steps: 28 | Train Loss: 0.4412927 Vali Loss: 1.7016832 Test Loss: 0.6611211
Validation loss decreased (1.701802 --> 1.701683).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.823583126068115
Epoch: 31, Steps: 28 | Train Loss: 0.4396779 Vali Loss: 1.7018505 Test Loss: 0.6592771
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 5.0225605964660645
Epoch: 32, Steps: 28 | Train Loss: 0.4379970 Vali Loss: 1.6962306 Test Loss: 0.6577475
Validation loss decreased (1.701683 --> 1.696231).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.676230192184448
Epoch: 33, Steps: 28 | Train Loss: 0.4365408 Vali Loss: 1.6938877 Test Loss: 0.6563356
Validation loss decreased (1.696231 --> 1.693888).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.903216361999512
Epoch: 34, Steps: 28 | Train Loss: 0.4351686 Vali Loss: 1.6993492 Test Loss: 0.6548031
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 5.071125507354736
Epoch: 35, Steps: 28 | Train Loss: 0.4335234 Vali Loss: 1.6913564 Test Loss: 0.6537435
Validation loss decreased (1.693888 --> 1.691356).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.72678542137146
Epoch: 36, Steps: 28 | Train Loss: 0.4323351 Vali Loss: 1.6918566 Test Loss: 0.6521490
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 5.183246374130249
Epoch: 37, Steps: 28 | Train Loss: 0.4310550 Vali Loss: 1.6912385 Test Loss: 0.6508774
Validation loss decreased (1.691356 --> 1.691239).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 5.024452447891235
Epoch: 38, Steps: 28 | Train Loss: 0.4297473 Vali Loss: 1.6910534 Test Loss: 0.6500291
Validation loss decreased (1.691239 --> 1.691053).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 4.296416759490967
Epoch: 39, Steps: 28 | Train Loss: 0.4285929 Vali Loss: 1.6856053 Test Loss: 0.6488987
Validation loss decreased (1.691053 --> 1.685605).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.956723928451538
Epoch: 40, Steps: 28 | Train Loss: 0.4279150 Vali Loss: 1.6823589 Test Loss: 0.6478159
Validation loss decreased (1.685605 --> 1.682359).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 4.633805990219116
Epoch: 41, Steps: 28 | Train Loss: 0.4268231 Vali Loss: 1.6851161 Test Loss: 0.6468895
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.879560947418213
Epoch: 42, Steps: 28 | Train Loss: 0.4259007 Vali Loss: 1.6762861 Test Loss: 0.6458963
Validation loss decreased (1.682359 --> 1.676286).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.949181079864502
Epoch: 43, Steps: 28 | Train Loss: 0.4250144 Vali Loss: 1.6827807 Test Loss: 0.6448938
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.862516164779663
Epoch: 44, Steps: 28 | Train Loss: 0.4240890 Vali Loss: 1.6797953 Test Loss: 0.6441385
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.399203300476074
Epoch: 45, Steps: 28 | Train Loss: 0.4234025 Vali Loss: 1.6799911 Test Loss: 0.6433812
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  97574400.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.771009683609009
Epoch: 1, Steps: 28 | Train Loss: 0.6642082 Vali Loss: 1.6381605 Test Loss: 0.6111509
Validation loss decreased (inf --> 1.638160).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.267065048217773
Epoch: 2, Steps: 28 | Train Loss: 0.6451762 Vali Loss: 1.6012490 Test Loss: 0.5825775
Validation loss decreased (1.638160 --> 1.601249).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.4851977825164795
Epoch: 3, Steps: 28 | Train Loss: 0.6308657 Vali Loss: 1.5793035 Test Loss: 0.5601953
Validation loss decreased (1.601249 --> 1.579304).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.603744745254517
Epoch: 4, Steps: 28 | Train Loss: 0.6191727 Vali Loss: 1.5590334 Test Loss: 0.5415846
Validation loss decreased (1.579304 --> 1.559033).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.192033052444458
Epoch: 5, Steps: 28 | Train Loss: 0.6096978 Vali Loss: 1.5380554 Test Loss: 0.5262992
Validation loss decreased (1.559033 --> 1.538055).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.958545446395874
Epoch: 6, Steps: 28 | Train Loss: 0.6018066 Vali Loss: 1.5226135 Test Loss: 0.5134237
Validation loss decreased (1.538055 --> 1.522614).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.962047338485718
Epoch: 7, Steps: 28 | Train Loss: 0.5953597 Vali Loss: 1.5110697 Test Loss: 0.5025795
Validation loss decreased (1.522614 --> 1.511070).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.024850606918335
Epoch: 8, Steps: 28 | Train Loss: 0.5902085 Vali Loss: 1.5016644 Test Loss: 0.4937481
Validation loss decreased (1.511070 --> 1.501664).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.226676940917969
Epoch: 9, Steps: 28 | Train Loss: 0.5853885 Vali Loss: 1.4935232 Test Loss: 0.4860172
Validation loss decreased (1.501664 --> 1.493523).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.499204397201538
Epoch: 10, Steps: 28 | Train Loss: 0.5813211 Vali Loss: 1.4842213 Test Loss: 0.4797672
Validation loss decreased (1.493523 --> 1.484221).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.560358047485352
Epoch: 11, Steps: 28 | Train Loss: 0.5779140 Vali Loss: 1.4747809 Test Loss: 0.4741822
Validation loss decreased (1.484221 --> 1.474781).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.518477201461792
Epoch: 12, Steps: 28 | Train Loss: 0.5749703 Vali Loss: 1.4694568 Test Loss: 0.4695687
Validation loss decreased (1.474781 --> 1.469457).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.963833808898926
Epoch: 13, Steps: 28 | Train Loss: 0.5726249 Vali Loss: 1.4651651 Test Loss: 0.4655303
Validation loss decreased (1.469457 --> 1.465165).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.009729623794556
Epoch: 14, Steps: 28 | Train Loss: 0.5701621 Vali Loss: 1.4601688 Test Loss: 0.4621961
Validation loss decreased (1.465165 --> 1.460169).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.148071765899658
Epoch: 15, Steps: 28 | Train Loss: 0.5680695 Vali Loss: 1.4582095 Test Loss: 0.4592881
Validation loss decreased (1.460169 --> 1.458210).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.999416828155518
Epoch: 16, Steps: 28 | Train Loss: 0.5667663 Vali Loss: 1.4524868 Test Loss: 0.4567324
Validation loss decreased (1.458210 --> 1.452487).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.699281930923462
Epoch: 17, Steps: 28 | Train Loss: 0.5649910 Vali Loss: 1.4469101 Test Loss: 0.4546191
Validation loss decreased (1.452487 --> 1.446910).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.472604751586914
Epoch: 18, Steps: 28 | Train Loss: 0.5637155 Vali Loss: 1.4506378 Test Loss: 0.4528696
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.567196607589722
Epoch: 19, Steps: 28 | Train Loss: 0.5622471 Vali Loss: 1.4475129 Test Loss: 0.4512692
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.3192832469940186
Epoch: 20, Steps: 28 | Train Loss: 0.5615317 Vali Loss: 1.4456429 Test Loss: 0.4497832
Validation loss decreased (1.446910 --> 1.445643).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.0523316860198975
Epoch: 21, Steps: 28 | Train Loss: 0.5607073 Vali Loss: 1.4496436 Test Loss: 0.4486737
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.97822380065918
Epoch: 22, Steps: 28 | Train Loss: 0.5598328 Vali Loss: 1.4454446 Test Loss: 0.4474502
Validation loss decreased (1.445643 --> 1.445445).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.913180828094482
Epoch: 23, Steps: 28 | Train Loss: 0.5592520 Vali Loss: 1.4472029 Test Loss: 0.4466511
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 5.2978949546813965
Epoch: 24, Steps: 28 | Train Loss: 0.5584625 Vali Loss: 1.4414970 Test Loss: 0.4458038
Validation loss decreased (1.445445 --> 1.441497).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 5.001668214797974
Epoch: 25, Steps: 28 | Train Loss: 0.5582688 Vali Loss: 1.4405899 Test Loss: 0.4451441
Validation loss decreased (1.441497 --> 1.440590).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 5.373287916183472
Epoch: 26, Steps: 28 | Train Loss: 0.5575295 Vali Loss: 1.4402564 Test Loss: 0.4444507
Validation loss decreased (1.440590 --> 1.440256).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 5.413841485977173
Epoch: 27, Steps: 28 | Train Loss: 0.5572280 Vali Loss: 1.4401149 Test Loss: 0.4439864
Validation loss decreased (1.440256 --> 1.440115).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 5.548136949539185
Epoch: 28, Steps: 28 | Train Loss: 0.5567434 Vali Loss: 1.4370091 Test Loss: 0.4434808
Validation loss decreased (1.440115 --> 1.437009).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 5.155376672744751
Epoch: 29, Steps: 28 | Train Loss: 0.5563484 Vali Loss: 1.4381859 Test Loss: 0.4430099
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.09130859375
Epoch: 30, Steps: 28 | Train Loss: 0.5560318 Vali Loss: 1.4313614 Test Loss: 0.4426159
Validation loss decreased (1.437009 --> 1.431361).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.620563983917236
Epoch: 31, Steps: 28 | Train Loss: 0.5556272 Vali Loss: 1.4369087 Test Loss: 0.4422642
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.656476259231567
Epoch: 32, Steps: 28 | Train Loss: 0.5554062 Vali Loss: 1.4387832 Test Loss: 0.4420512
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.437177896499634
Epoch: 33, Steps: 28 | Train Loss: 0.5551694 Vali Loss: 1.4361193 Test Loss: 0.4417644
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4341912567615509, mae:0.46082383394241333, rse:0.6308012008666992, corr:[0.21651067 0.22745831 0.2243542  0.22732157 0.23083918 0.22994821
 0.22807169 0.22859178 0.23035488 0.23182741 0.23144716 0.22967355
 0.2283955  0.22824523 0.22863078 0.22842026 0.22713529 0.22589442
 0.2258171  0.22666332 0.22699583 0.2265912  0.22611544 0.22695923
 0.22835407 0.2290274  0.22871217 0.22831237 0.22858274 0.22907262
 0.22905391 0.22828504 0.22746758 0.22708742 0.22695817 0.22679342
 0.22652322 0.22583413 0.22517744 0.2250059  0.22553632 0.22590382
 0.22563738 0.22524667 0.225613   0.22665316 0.22761628 0.22802809
 0.22791456 0.22796687 0.22779469 0.22703226 0.22589779 0.22477676
 0.22419687 0.2239386  0.22366145 0.22302555 0.22199601 0.2213847
 0.22138159 0.22149892 0.2212449  0.22078978 0.22044809 0.22062744
 0.22137378 0.22190689 0.22179483 0.2214513  0.22146249 0.22205804
 0.22221838 0.22158983 0.22060734 0.22028986 0.22047877 0.22033125
 0.21960275 0.21874927 0.21825129 0.21819335 0.21800807 0.2174703
 0.21684131 0.21631224 0.21599247 0.21593103 0.21598357 0.21592723
 0.21554735 0.21527013 0.2155483  0.21631058 0.21700811 0.21764007
 0.21856236 0.2197375  0.22063173 0.22107346 0.22099993 0.22078997
 0.22084184 0.22106229 0.22094554 0.2203394  0.21961547 0.21923837
 0.21931623 0.21938339 0.2192549  0.21906167 0.2189932  0.21909145
 0.21937978 0.21955244 0.21946962 0.21926755 0.21933974 0.21984571
 0.22026041 0.21993692 0.21900152 0.21840662 0.21827851 0.21812168
 0.2177236  0.21735446 0.21708615 0.21694535 0.21666059 0.21611074
 0.21556452 0.21524519 0.2152332  0.2153848  0.21553136 0.21552846
 0.21544991 0.21543299 0.21564071 0.2158671  0.21573077 0.21543452
 0.21525683 0.21511869 0.21493176 0.21443187 0.21376058 0.21318643
 0.21298184 0.21316224 0.21320571 0.21285656 0.2123674  0.2121808
 0.212161   0.21190627 0.21148385 0.21132512 0.2115453  0.2119485
 0.21217492 0.21221288 0.21219018 0.2122475  0.21218567 0.21236318
 0.21272695 0.21304546 0.21318404 0.2134663  0.21371517 0.21366559
 0.21335292 0.21308309 0.21292175 0.21295412 0.21288973 0.21258953
 0.21222581 0.21205276 0.21216805 0.21241301 0.2126395  0.21281283
 0.21300569 0.21326473 0.2136408  0.21387857 0.21387143 0.21372354
 0.2135693  0.21336214 0.21291515 0.21220265 0.21145749 0.21094388
 0.21077879 0.21084304 0.21067348 0.21019888 0.20986007 0.21002883
 0.21038541 0.2105795  0.21046679 0.21031097 0.2104175  0.210701
 0.21088979 0.21078685 0.21050191 0.21016632 0.21001206 0.21015652
 0.21030577 0.21020974 0.20988911 0.20974591 0.20988376 0.2098483
 0.20953171 0.20912044 0.20882188 0.20860897 0.20824613 0.20786013
 0.20766753 0.20764019 0.20760852 0.2074566  0.2071911  0.20701848
 0.20700312 0.20705797 0.20716634 0.20725124 0.20728086 0.20723209
 0.20737825 0.20778844 0.20831701 0.20853814 0.2081837  0.20764726
 0.20760714 0.20794994 0.20800708 0.20745833 0.20679964 0.20652984
 0.20654848 0.2064665  0.20639886 0.20646738 0.20663236 0.20676821
 0.206799   0.20666377 0.20659353 0.20653746 0.20651655 0.20668477
 0.20688462 0.20681232 0.20649256 0.20632173 0.20630926 0.20616852
 0.20576015 0.20530267 0.20515878 0.20541681 0.20548679 0.20521004
 0.205038   0.20534465 0.20566349 0.2057052  0.20555176 0.20556287
 0.20562188 0.20567825 0.20575207 0.20590663 0.20606771 0.2063113
 0.20661028 0.20700073 0.20747575 0.20777011 0.20752792 0.20720676
 0.20736553 0.20779489 0.20774068 0.20723279 0.2069713  0.20734532
 0.20780805 0.20772977 0.20740108 0.20738186 0.20772706 0.207995
 0.20804428 0.20790924 0.20790628 0.2080457  0.20819412 0.20850706
 0.20889367 0.20896111 0.2086151  0.20825726 0.20795888 0.20750645
 0.20685357 0.20626663 0.20602182 0.2059973  0.20584598 0.20555528
 0.20542221 0.20560333 0.2057752  0.20585497 0.20578983 0.20590162
 0.20612714 0.20622589 0.20630847 0.20654204 0.20671478 0.20653814
 0.20616214 0.20587434 0.20577236 0.20551668 0.20479907 0.20399804
 0.20371418 0.20371369 0.20339349 0.20291568 0.20285922 0.20335907
 0.20379965 0.20384748 0.20370044 0.20378388 0.2040204  0.2043535
 0.20451951 0.20449606 0.20436083 0.20421377 0.20398858 0.20404942
 0.20427316 0.2040955  0.20344347 0.20279308 0.20244075 0.20215799
 0.20172434 0.20121336 0.20080581 0.2005586  0.20037502 0.2001613
 0.19991182 0.1996828  0.19943295 0.19909269 0.19892885 0.1990711
 0.19928539 0.19917989 0.19893458 0.19910611 0.1995087  0.20008594
 0.20073114 0.20150407 0.2022661  0.20255753 0.2020561  0.20104206
 0.20042062 0.20016183 0.19958825 0.1986774  0.19802356 0.19815964
 0.1985336  0.19848697 0.19834225 0.19852188 0.19890966 0.1991929
 0.19927457 0.1994429  0.199852   0.20015137 0.20012712 0.20031224
 0.20092715 0.20147337 0.2015885  0.20158608 0.20176919 0.201729
 0.20138265 0.20092304 0.20076105 0.200813   0.20066911 0.20020294
 0.19984363 0.19999969 0.20016766 0.20004804 0.19976223 0.19970563
 0.1998731  0.2000667  0.20003931 0.20041792 0.20092027 0.20124593
 0.20146038 0.20172551 0.20201336 0.20205534 0.20162445 0.2011405
 0.20121506 0.20151061 0.20108572 0.2002992  0.20005819 0.20057708
 0.20096308 0.20058626 0.20003954 0.19990171 0.19999914 0.19990438
 0.19973207 0.19984847 0.20028171 0.20058973 0.20067541 0.20115322
 0.20195681 0.20247407 0.20234837 0.20198329 0.20174347 0.20130154
 0.20059825 0.1999773  0.1999407  0.20044956 0.2006106  0.20027143
 0.20002376 0.20037836 0.2009054  0.20097703 0.2006335  0.20053121
 0.20067376 0.20067178 0.20048797 0.20036411 0.20059152 0.20066375
 0.20047723 0.20051421 0.20083557 0.2009526  0.20044355 0.19974844
 0.19966012 0.19991714 0.19949129 0.19871373 0.19849055 0.19902621
 0.19944158 0.1990576  0.19844273 0.19826467 0.1984133  0.19840717
 0.19824827 0.1982478  0.19843994 0.19862503 0.19893354 0.19982634
 0.20100087 0.20163819 0.20142144 0.20134197 0.2018654  0.20195988
 0.20114787 0.20022115 0.19998093 0.20011154 0.19995685 0.19957188
 0.19957583 0.20010187 0.20061953 0.20051166 0.20054194 0.2012225
 0.20193313 0.20193462 0.20181985 0.20203301 0.20291826 0.20299189
 0.20251222 0.20222364 0.20251264 0.20269403 0.2017962  0.2008133
 0.20082508 0.20139267 0.2010184  0.19992675 0.19939113 0.19992521
 0.20041882 0.20017955 0.20020758 0.2009897  0.20180248 0.20164455
 0.2010745  0.20113428 0.20154712 0.2012569  0.20031917 0.19984156
 0.20004964 0.19998159 0.19885936 0.19767836 0.1975099  0.19766611
 0.19693023 0.19561853 0.19502786 0.19537173 0.19546503 0.19477981
 0.19407201 0.19396724 0.1943292  0.19406907 0.19382228 0.19411373
 0.19464506 0.19433883 0.19378673 0.19381382 0.19426186 0.19406147
 0.1931683  0.19282937 0.19322336 0.19360898 0.1928449  0.19143938
 0.19104059 0.19128992 0.19112532 0.19032536 0.19015713 0.19074339
 0.1911655  0.19064318 0.19023164 0.1905329  0.1909886  0.19090493
 0.19072908 0.19115587 0.19177555 0.19172938 0.19111022 0.1907493
 0.19075222 0.1901532  0.18876567 0.18779388 0.18774347 0.18737203
 0.18603693 0.18476489 0.18457034 0.1851127  0.1850293  0.18429263
 0.1839947  0.1844396  0.18484    0.18458755 0.18437687 0.18481088
 0.18529876 0.1851319  0.18488973 0.18528189 0.18584806 0.1855353
 0.1845838  0.18428527 0.18461902 0.18446453 0.18332912 0.18237986
 0.18266803 0.18294752 0.18181378 0.18036903 0.1803857  0.18169096
 0.18228689 0.18146136 0.18090762 0.18148929 0.18209621 0.18173675
 0.18097578 0.18092701 0.18130465 0.1812244  0.181056   0.18145895
 0.18176502 0.18106733 0.179877   0.17999989 0.18076226 0.18020803
 0.17809795 0.17638877 0.17654283 0.17720991 0.17684639 0.17599075
 0.17594714 0.17634398 0.17621362 0.17561841 0.17571697 0.17646554
 0.17657542 0.17599821 0.1758647  0.17662112 0.17671074 0.17516913
 0.17303891 0.17201668 0.17136312 0.16956918 0.1673313  0.1663168
 0.16711545 0.1673063  0.16597886 0.16458322 0.16504098 0.16634417
 0.16638133 0.1653142  0.16516119 0.16583864 0.16606127 0.16586208
 0.16601436 0.16631089 0.16588163 0.16532072 0.16586442 0.16654328
 0.1651952  0.16292319 0.1625226  0.16476247 0.16530511 0.16240759
 0.16031441 0.16169694 0.16361713 0.16247734 0.16034359 0.1606346
 0.1623835  0.16161898 0.15986854 0.16014299 0.16120103 0.15929551
 0.15766038 0.15903369 0.15686981 0.14438638 0.14801516 0.16301838]
