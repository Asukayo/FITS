Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  174211072.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.214869976043701
Epoch: 1, Steps: 15 | Train Loss: 0.7170096 Vali Loss: 1.7797403 Test Loss: 0.9615984
Validation loss decreased (inf --> 1.779740).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.0785841941833496
Epoch: 2, Steps: 15 | Train Loss: 0.6480588 Vali Loss: 1.6647384 Test Loss: 0.9030519
Validation loss decreased (1.779740 --> 1.664738).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.051025152206421
Epoch: 3, Steps: 15 | Train Loss: 0.5959114 Vali Loss: 1.5759966 Test Loss: 0.8626234
Validation loss decreased (1.664738 --> 1.575997).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.9471065998077393
Epoch: 4, Steps: 15 | Train Loss: 0.5567543 Vali Loss: 1.5178803 Test Loss: 0.8325491
Validation loss decreased (1.575997 --> 1.517880).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.107638120651245
Epoch: 5, Steps: 15 | Train Loss: 0.5272786 Vali Loss: 1.4661760 Test Loss: 0.8101246
Validation loss decreased (1.517880 --> 1.466176).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.7024428844451904
Epoch: 6, Steps: 15 | Train Loss: 0.5038119 Vali Loss: 1.4321060 Test Loss: 0.7942184
Validation loss decreased (1.466176 --> 1.432106).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.8441686630249023
Epoch: 7, Steps: 15 | Train Loss: 0.4856121 Vali Loss: 1.4101406 Test Loss: 0.7835777
Validation loss decreased (1.432106 --> 1.410141).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.6110482215881348
Epoch: 8, Steps: 15 | Train Loss: 0.4703552 Vali Loss: 1.3919649 Test Loss: 0.7751634
Validation loss decreased (1.410141 --> 1.391965).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.3446383476257324
Epoch: 9, Steps: 15 | Train Loss: 0.4578389 Vali Loss: 1.3730482 Test Loss: 0.7686182
Validation loss decreased (1.391965 --> 1.373048).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.1234629154205322
Epoch: 10, Steps: 15 | Train Loss: 0.4473562 Vali Loss: 1.3628187 Test Loss: 0.7636717
Validation loss decreased (1.373048 --> 1.362819).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.090179443359375
Epoch: 11, Steps: 15 | Train Loss: 0.4384110 Vali Loss: 1.3535032 Test Loss: 0.7607473
Validation loss decreased (1.362819 --> 1.353503).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.0769124031066895
Epoch: 12, Steps: 15 | Train Loss: 0.4306721 Vali Loss: 1.3491919 Test Loss: 0.7575109
Validation loss decreased (1.353503 --> 1.349192).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.961759328842163
Epoch: 13, Steps: 15 | Train Loss: 0.4234934 Vali Loss: 1.3381779 Test Loss: 0.7555700
Validation loss decreased (1.349192 --> 1.338178).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.0585994720458984
Epoch: 14, Steps: 15 | Train Loss: 0.4174674 Vali Loss: 1.3295677 Test Loss: 0.7527531
Validation loss decreased (1.338178 --> 1.329568).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.221322774887085
Epoch: 15, Steps: 15 | Train Loss: 0.4119844 Vali Loss: 1.3240430 Test Loss: 0.7510276
Validation loss decreased (1.329568 --> 1.324043).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.2484986782073975
Epoch: 16, Steps: 15 | Train Loss: 0.4070158 Vali Loss: 1.3228613 Test Loss: 0.7500839
Validation loss decreased (1.324043 --> 1.322861).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.096825122833252
Epoch: 17, Steps: 15 | Train Loss: 0.4024281 Vali Loss: 1.3176200 Test Loss: 0.7486169
Validation loss decreased (1.322861 --> 1.317620).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.0524654388427734
Epoch: 18, Steps: 15 | Train Loss: 0.3983302 Vali Loss: 1.3184795 Test Loss: 0.7480747
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.8131327629089355
Epoch: 19, Steps: 15 | Train Loss: 0.3944912 Vali Loss: 1.3104315 Test Loss: 0.7466112
Validation loss decreased (1.317620 --> 1.310431).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.4860312938690186
Epoch: 20, Steps: 15 | Train Loss: 0.3908637 Vali Loss: 1.3056564 Test Loss: 0.7453821
Validation loss decreased (1.310431 --> 1.305656).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.51926326751709
Epoch: 21, Steps: 15 | Train Loss: 0.3876809 Vali Loss: 1.3094577 Test Loss: 0.7445638
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.6146063804626465
Epoch: 22, Steps: 15 | Train Loss: 0.3846585 Vali Loss: 1.3078209 Test Loss: 0.7437708
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.8541078567504883
Epoch: 23, Steps: 15 | Train Loss: 0.3819468 Vali Loss: 1.3016589 Test Loss: 0.7431345
Validation loss decreased (1.305656 --> 1.301659).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.7005045413970947
Epoch: 24, Steps: 15 | Train Loss: 0.3791215 Vali Loss: 1.3020848 Test Loss: 0.7422392
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.4754562377929688
Epoch: 25, Steps: 15 | Train Loss: 0.3767516 Vali Loss: 1.3017695 Test Loss: 0.7415918
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.494013786315918
Epoch: 26, Steps: 15 | Train Loss: 0.3744283 Vali Loss: 1.2984498 Test Loss: 0.7411730
Validation loss decreased (1.301659 --> 1.298450).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.1957736015319824
Epoch: 27, Steps: 15 | Train Loss: 0.3721220 Vali Loss: 1.2960060 Test Loss: 0.7404576
Validation loss decreased (1.298450 --> 1.296006).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.439063310623169
Epoch: 28, Steps: 15 | Train Loss: 0.3703189 Vali Loss: 1.2955393 Test Loss: 0.7395896
Validation loss decreased (1.296006 --> 1.295539).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.2018630504608154
Epoch: 29, Steps: 15 | Train Loss: 0.3682570 Vali Loss: 1.2905012 Test Loss: 0.7393809
Validation loss decreased (1.295539 --> 1.290501).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.743774175643921
Epoch: 30, Steps: 15 | Train Loss: 0.3663878 Vali Loss: 1.2947037 Test Loss: 0.7386220
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.8218166828155518
Epoch: 31, Steps: 15 | Train Loss: 0.3647965 Vali Loss: 1.2949040 Test Loss: 0.7381181
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.310612201690674
Epoch: 32, Steps: 15 | Train Loss: 0.3633578 Vali Loss: 1.2924315 Test Loss: 0.7377353
EarlyStopping counter: 3 out of 3
Early stopping
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  174211072.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.254420042037964
Epoch: 1, Steps: 15 | Train Loss: 0.5514513 Vali Loss: 1.2138716 Test Loss: 0.6754189
Validation loss decreased (inf --> 1.213872).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.429814577102661
Epoch: 2, Steps: 15 | Train Loss: 0.5159868 Vali Loss: 1.1603863 Test Loss: 0.6277068
Validation loss decreased (1.213872 --> 1.160386).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.3136401176452637
Epoch: 3, Steps: 15 | Train Loss: 0.4895383 Vali Loss: 1.1162910 Test Loss: 0.5904052
Validation loss decreased (1.160386 --> 1.116291).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.46108341217041
Epoch: 4, Steps: 15 | Train Loss: 0.4698896 Vali Loss: 1.0881901 Test Loss: 0.5606598
Validation loss decreased (1.116291 --> 1.088190).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.482405424118042
Epoch: 5, Steps: 15 | Train Loss: 0.4543798 Vali Loss: 1.0581429 Test Loss: 0.5370898
Validation loss decreased (1.088190 --> 1.058143).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.188610553741455
Epoch: 6, Steps: 15 | Train Loss: 0.4423707 Vali Loss: 1.0403935 Test Loss: 0.5180143
Validation loss decreased (1.058143 --> 1.040393).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.3180432319641113
Epoch: 7, Steps: 15 | Train Loss: 0.4327459 Vali Loss: 1.0236478 Test Loss: 0.5024757
Validation loss decreased (1.040393 --> 1.023648).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.3668577671051025
Epoch: 8, Steps: 15 | Train Loss: 0.4247832 Vali Loss: 1.0120913 Test Loss: 0.4898583
Validation loss decreased (1.023648 --> 1.012091).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.5731201171875
Epoch: 9, Steps: 15 | Train Loss: 0.4184589 Vali Loss: 1.0035238 Test Loss: 0.4795336
Validation loss decreased (1.012091 --> 1.003524).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.8505027294158936
Epoch: 10, Steps: 15 | Train Loss: 0.4131245 Vali Loss: 0.9925302 Test Loss: 0.4712110
Validation loss decreased (1.003524 --> 0.992530).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.2964766025543213
Epoch: 11, Steps: 15 | Train Loss: 0.4089873 Vali Loss: 0.9860476 Test Loss: 0.4642197
Validation loss decreased (0.992530 --> 0.986048).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.869809150695801
Epoch: 12, Steps: 15 | Train Loss: 0.4052164 Vali Loss: 0.9781744 Test Loss: 0.4585834
Validation loss decreased (0.986048 --> 0.978174).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.8772878646850586
Epoch: 13, Steps: 15 | Train Loss: 0.4027848 Vali Loss: 0.9745506 Test Loss: 0.4538810
Validation loss decreased (0.978174 --> 0.974551).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.1262426376342773
Epoch: 14, Steps: 15 | Train Loss: 0.4001199 Vali Loss: 0.9695549 Test Loss: 0.4499703
Validation loss decreased (0.974551 --> 0.969555).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.8245885372161865
Epoch: 15, Steps: 15 | Train Loss: 0.3975171 Vali Loss: 0.9668032 Test Loss: 0.4466940
Validation loss decreased (0.969555 --> 0.966803).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.7054080963134766
Epoch: 16, Steps: 15 | Train Loss: 0.3960248 Vali Loss: 0.9679070 Test Loss: 0.4439567
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.383678913116455
Epoch: 17, Steps: 15 | Train Loss: 0.3942990 Vali Loss: 0.9648207 Test Loss: 0.4416654
Validation loss decreased (0.966803 --> 0.964821).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.559669256210327
Epoch: 18, Steps: 15 | Train Loss: 0.3933782 Vali Loss: 0.9618190 Test Loss: 0.4397796
Validation loss decreased (0.964821 --> 0.961819).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.435826301574707
Epoch: 19, Steps: 15 | Train Loss: 0.3920265 Vali Loss: 0.9598536 Test Loss: 0.4381099
Validation loss decreased (0.961819 --> 0.959854).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.541017770767212
Epoch: 20, Steps: 15 | Train Loss: 0.3912573 Vali Loss: 0.9618268 Test Loss: 0.4367586
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.754096031188965
Epoch: 21, Steps: 15 | Train Loss: 0.3906508 Vali Loss: 0.9595081 Test Loss: 0.4354566
Validation loss decreased (0.959854 --> 0.959508).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.6498191356658936
Epoch: 22, Steps: 15 | Train Loss: 0.3898945 Vali Loss: 0.9566886 Test Loss: 0.4344277
Validation loss decreased (0.959508 --> 0.956689).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.563067674636841
Epoch: 23, Steps: 15 | Train Loss: 0.3891181 Vali Loss: 0.9560997 Test Loss: 0.4336256
Validation loss decreased (0.956689 --> 0.956100).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.4115307331085205
Epoch: 24, Steps: 15 | Train Loss: 0.3884227 Vali Loss: 0.9564462 Test Loss: 0.4328586
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.0957319736480713
Epoch: 25, Steps: 15 | Train Loss: 0.3876291 Vali Loss: 0.9537891 Test Loss: 0.4322897
Validation loss decreased (0.956100 --> 0.953789).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.400128126144409
Epoch: 26, Steps: 15 | Train Loss: 0.3875384 Vali Loss: 0.9518612 Test Loss: 0.4316929
Validation loss decreased (0.953789 --> 0.951861).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.1445071697235107
Epoch: 27, Steps: 15 | Train Loss: 0.3872560 Vali Loss: 0.9513736 Test Loss: 0.4311939
Validation loss decreased (0.951861 --> 0.951374).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.514512538909912
Epoch: 28, Steps: 15 | Train Loss: 0.3870427 Vali Loss: 0.9542883 Test Loss: 0.4307049
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.5976645946502686
Epoch: 29, Steps: 15 | Train Loss: 0.3863159 Vali Loss: 0.9535671 Test Loss: 0.4303766
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.383244752883911
Epoch: 30, Steps: 15 | Train Loss: 0.3865159 Vali Loss: 0.9543184 Test Loss: 0.4299817
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4152025878429413, mae:0.4245748817920685, rse:0.6119097471237183, corr:[0.26114497 0.26945713 0.26744482 0.27060115 0.2689556  0.26508364
 0.26446345 0.26545116 0.26498106 0.26398927 0.2636784  0.2636384
 0.26332867 0.2627135  0.26204604 0.2619808  0.2623582  0.26223734
 0.261222   0.260497   0.26050887 0.26009607 0.25918862 0.25937667
 0.2599907  0.2597115  0.2592554  0.25969198 0.26027104 0.2599996
 0.25929204 0.25895038 0.25905612 0.258996   0.25852033 0.25817445
 0.2581329  0.25825334 0.2583056  0.258129   0.25825018 0.25877726
 0.2592543  0.25921276 0.25897795 0.25924885 0.25995067 0.26042086
 0.26030633 0.25995162 0.25924084 0.25832903 0.25718775 0.2559597
 0.25528443 0.25477    0.25404906 0.2536865  0.25367996 0.2537869
 0.25347653 0.25314775 0.25322804 0.2534487  0.25337452 0.25343072
 0.2540945  0.25453532 0.25440943 0.25417906 0.25423115 0.254293
 0.25384974 0.2529796  0.25221694 0.25213698 0.25206515 0.25151026
 0.25081226 0.25043532 0.25025427 0.24982738 0.24924238 0.24900945
 0.24909198 0.24903768 0.24885482 0.24866651 0.2485554  0.2486008
 0.24857564 0.24829943 0.24801326 0.24805023 0.24820194 0.24852823
 0.24923274 0.24986042 0.25011355 0.25015828 0.25009927 0.24992153
 0.24976529 0.24943033 0.24906003 0.24909334 0.24904016 0.24856655
 0.24816263 0.24810185 0.24842088 0.24887954 0.24906342 0.24915634
 0.24942827 0.24961765 0.24946576 0.24911779 0.24901585 0.24914701
 0.2490685  0.24828486 0.24712934 0.24651167 0.2459512  0.24474166
 0.24397524 0.24411543 0.24394748 0.24322574 0.24261995 0.24260026
 0.24260023 0.2423981  0.24228881 0.24233069 0.24260515 0.2427982
 0.24294983 0.24282214 0.24261342 0.24259537 0.2423578  0.24170092
 0.24125127 0.24089974 0.24006486 0.23860438 0.23736031 0.23654835
 0.23595728 0.23550841 0.23563293 0.23613085 0.23603766 0.23568283
 0.23557189 0.23560704 0.23570715 0.23564301 0.23536417 0.23523742
 0.235384   0.23518108 0.23428202 0.23362638 0.23403528 0.23382859
 0.23237562 0.23156337 0.23218425 0.23260038 0.23126985 0.22987549
 0.2299851  0.2302091  0.22916882 0.22873287 0.2291678  0.2290798
 0.22848299 0.22807169 0.22721113 0.2266867  0.22696748 0.22560064
 0.22305287 0.2230989  0.22205408 0.21394604 0.21371835 0.22183898]
