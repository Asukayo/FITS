Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  48787200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 7.990622043609619
Epoch: 1, Steps: 56 | Train Loss: 0.8500241 Vali Loss: 2.1584811 Test Loss: 0.9179301
Validation loss decreased (inf --> 2.158481).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.8834593296051025
Epoch: 2, Steps: 56 | Train Loss: 0.6665536 Vali Loss: 1.9439707 Test Loss: 0.7988142
Validation loss decreased (2.158481 --> 1.943971).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.6381094455719
Epoch: 3, Steps: 56 | Train Loss: 0.5866258 Vali Loss: 1.8558016 Test Loss: 0.7517191
Validation loss decreased (1.943971 --> 1.855802).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.642462491989136
Epoch: 4, Steps: 56 | Train Loss: 0.5453304 Vali Loss: 1.8147415 Test Loss: 0.7266480
Validation loss decreased (1.855802 --> 1.814741).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.325685024261475
Epoch: 5, Steps: 56 | Train Loss: 0.5187674 Vali Loss: 1.7861726 Test Loss: 0.7092136
Validation loss decreased (1.814741 --> 1.786173).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.356481790542603
Epoch: 6, Steps: 56 | Train Loss: 0.4989087 Vali Loss: 1.7649906 Test Loss: 0.6955765
Validation loss decreased (1.786173 --> 1.764991).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.872902870178223
Epoch: 7, Steps: 56 | Train Loss: 0.4822639 Vali Loss: 1.7466089 Test Loss: 0.6832268
Validation loss decreased (1.764991 --> 1.746609).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.923670291900635
Epoch: 8, Steps: 56 | Train Loss: 0.4684833 Vali Loss: 1.7301509 Test Loss: 0.6718956
Validation loss decreased (1.746609 --> 1.730151).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.20189118385315
Epoch: 9, Steps: 56 | Train Loss: 0.4561546 Vali Loss: 1.7076516 Test Loss: 0.6606207
Validation loss decreased (1.730151 --> 1.707652).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.331717252731323
Epoch: 10, Steps: 56 | Train Loss: 0.4456720 Vali Loss: 1.7059996 Test Loss: 0.6514239
Validation loss decreased (1.707652 --> 1.706000).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.112741947174072
Epoch: 11, Steps: 56 | Train Loss: 0.4363164 Vali Loss: 1.6829022 Test Loss: 0.6420156
Validation loss decreased (1.706000 --> 1.682902).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.762978076934814
Epoch: 12, Steps: 56 | Train Loss: 0.4279712 Vali Loss: 1.6746235 Test Loss: 0.6343434
Validation loss decreased (1.682902 --> 1.674623).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.891627311706543
Epoch: 13, Steps: 56 | Train Loss: 0.4204547 Vali Loss: 1.6719127 Test Loss: 0.6265706
Validation loss decreased (1.674623 --> 1.671913).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.616877555847168
Epoch: 14, Steps: 56 | Train Loss: 0.4137388 Vali Loss: 1.6572130 Test Loss: 0.6195869
Validation loss decreased (1.671913 --> 1.657213).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.360783338546753
Epoch: 15, Steps: 56 | Train Loss: 0.4075470 Vali Loss: 1.6524770 Test Loss: 0.6136820
Validation loss decreased (1.657213 --> 1.652477).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.526834964752197
Epoch: 16, Steps: 56 | Train Loss: 0.4020772 Vali Loss: 1.6417239 Test Loss: 0.6064141
Validation loss decreased (1.652477 --> 1.641724).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.612495183944702
Epoch: 17, Steps: 56 | Train Loss: 0.3970805 Vali Loss: 1.6376116 Test Loss: 0.6006646
Validation loss decreased (1.641724 --> 1.637612).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.337320566177368
Epoch: 18, Steps: 56 | Train Loss: 0.3925948 Vali Loss: 1.6269882 Test Loss: 0.5955243
Validation loss decreased (1.637612 --> 1.626988).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 7.416833877563477
Epoch: 19, Steps: 56 | Train Loss: 0.3884750 Vali Loss: 1.6262512 Test Loss: 0.5907844
Validation loss decreased (1.626988 --> 1.626251).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 8.199235439300537
Epoch: 20, Steps: 56 | Train Loss: 0.3847317 Vali Loss: 1.6218792 Test Loss: 0.5862483
Validation loss decreased (1.626251 --> 1.621879).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.308798551559448
Epoch: 21, Steps: 56 | Train Loss: 0.3811335 Vali Loss: 1.6055795 Test Loss: 0.5818498
Validation loss decreased (1.621879 --> 1.605579).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.353206634521484
Epoch: 22, Steps: 56 | Train Loss: 0.3781207 Vali Loss: 1.6050489 Test Loss: 0.5776817
Validation loss decreased (1.605579 --> 1.605049).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.786175727844238
Epoch: 23, Steps: 56 | Train Loss: 0.3751916 Vali Loss: 1.6028415 Test Loss: 0.5738786
Validation loss decreased (1.605049 --> 1.602841).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.779421329498291
Epoch: 24, Steps: 56 | Train Loss: 0.3723664 Vali Loss: 1.5991690 Test Loss: 0.5701019
Validation loss decreased (1.602841 --> 1.599169).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.24463438987732
Epoch: 25, Steps: 56 | Train Loss: 0.3698410 Vali Loss: 1.5922637 Test Loss: 0.5668866
Validation loss decreased (1.599169 --> 1.592264).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 8.516231060028076
Epoch: 26, Steps: 56 | Train Loss: 0.3675767 Vali Loss: 1.5933378 Test Loss: 0.5639362
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.377973794937134
Epoch: 27, Steps: 56 | Train Loss: 0.3654212 Vali Loss: 1.5839248 Test Loss: 0.5609009
Validation loss decreased (1.592264 --> 1.583925).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 8.838238954544067
Epoch: 28, Steps: 56 | Train Loss: 0.3633202 Vali Loss: 1.5850468 Test Loss: 0.5581009
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.746204614639282
Epoch: 29, Steps: 56 | Train Loss: 0.3614348 Vali Loss: 1.5829923 Test Loss: 0.5554431
Validation loss decreased (1.583925 --> 1.582992).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 8.76041841506958
Epoch: 30, Steps: 56 | Train Loss: 0.3598087 Vali Loss: 1.5766075 Test Loss: 0.5530702
Validation loss decreased (1.582992 --> 1.576607).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 7.786303520202637
Epoch: 31, Steps: 56 | Train Loss: 0.3581089 Vali Loss: 1.5779357 Test Loss: 0.5506358
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 7.7479987144470215
Epoch: 32, Steps: 56 | Train Loss: 0.3567443 Vali Loss: 1.5725853 Test Loss: 0.5486899
Validation loss decreased (1.576607 --> 1.572585).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 7.975464105606079
Epoch: 33, Steps: 56 | Train Loss: 0.3552455 Vali Loss: 1.5698130 Test Loss: 0.5464835
Validation loss decreased (1.572585 --> 1.569813).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 7.7054762840271
Epoch: 34, Steps: 56 | Train Loss: 0.3540234 Vali Loss: 1.5665536 Test Loss: 0.5446452
Validation loss decreased (1.569813 --> 1.566554).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 8.143409967422485
Epoch: 35, Steps: 56 | Train Loss: 0.3528655 Vali Loss: 1.5607852 Test Loss: 0.5429721
Validation loss decreased (1.566554 --> 1.560785).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.22009825706482
Epoch: 36, Steps: 56 | Train Loss: 0.3514836 Vali Loss: 1.5705004 Test Loss: 0.5411806
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 8.870276927947998
Epoch: 37, Steps: 56 | Train Loss: 0.3503659 Vali Loss: 1.5598848 Test Loss: 0.5395753
Validation loss decreased (1.560785 --> 1.559885).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 9.128737211227417
Epoch: 38, Steps: 56 | Train Loss: 0.3492500 Vali Loss: 1.5601779 Test Loss: 0.5380366
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 8.489935159683228
Epoch: 39, Steps: 56 | Train Loss: 0.3484078 Vali Loss: 1.5591042 Test Loss: 0.5366703
Validation loss decreased (1.559885 --> 1.559104).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 8.358614921569824
Epoch: 40, Steps: 56 | Train Loss: 0.3475049 Vali Loss: 1.5540299 Test Loss: 0.5352942
Validation loss decreased (1.559104 --> 1.554030).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 8.388632774353027
Epoch: 41, Steps: 56 | Train Loss: 0.3466940 Vali Loss: 1.5530261 Test Loss: 0.5340066
Validation loss decreased (1.554030 --> 1.553026).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 8.625391244888306
Epoch: 42, Steps: 56 | Train Loss: 0.3458203 Vali Loss: 1.5545793 Test Loss: 0.5327566
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 8.986618757247925
Epoch: 43, Steps: 56 | Train Loss: 0.3450040 Vali Loss: 1.5562329 Test Loss: 0.5316504
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 8.17367672920227
Epoch: 44, Steps: 56 | Train Loss: 0.3445938 Vali Loss: 1.5455419 Test Loss: 0.5305610
Validation loss decreased (1.553026 --> 1.545542).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 8.457348585128784
Epoch: 45, Steps: 56 | Train Loss: 0.3437297 Vali Loss: 1.5405085 Test Loss: 0.5295728
Validation loss decreased (1.545542 --> 1.540509).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 7.018536567687988
Epoch: 46, Steps: 56 | Train Loss: 0.3430976 Vali Loss: 1.5498829 Test Loss: 0.5285464
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 8.536926984786987
Epoch: 47, Steps: 56 | Train Loss: 0.3423918 Vali Loss: 1.5468626 Test Loss: 0.5276146
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 8.673287153244019
Epoch: 48, Steps: 56 | Train Loss: 0.3418940 Vali Loss: 1.5415204 Test Loss: 0.5266384
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  48787200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.485381126403809
Epoch: 1, Steps: 56 | Train Loss: 0.5976244 Vali Loss: 1.5012836 Test Loss: 0.4914981
Validation loss decreased (inf --> 1.501284).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.253148317337036
Epoch: 2, Steps: 56 | Train Loss: 0.5798290 Vali Loss: 1.4793656 Test Loss: 0.4669895
Validation loss decreased (1.501284 --> 1.479366).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.459244012832642
Epoch: 3, Steps: 56 | Train Loss: 0.5687435 Vali Loss: 1.4578991 Test Loss: 0.4522624
Validation loss decreased (1.479366 --> 1.457899).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.464596033096313
Epoch: 4, Steps: 56 | Train Loss: 0.5624047 Vali Loss: 1.4496505 Test Loss: 0.4433474
Validation loss decreased (1.457899 --> 1.449651).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.224436283111572
Epoch: 5, Steps: 56 | Train Loss: 0.5579948 Vali Loss: 1.4413691 Test Loss: 0.4378701
Validation loss decreased (1.449651 --> 1.441369).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.440893650054932
Epoch: 6, Steps: 56 | Train Loss: 0.5547247 Vali Loss: 1.4395802 Test Loss: 0.4352378
Validation loss decreased (1.441369 --> 1.439580).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.05663013458252
Epoch: 7, Steps: 56 | Train Loss: 0.5531373 Vali Loss: 1.4379841 Test Loss: 0.4337327
Validation loss decreased (1.439580 --> 1.437984).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.660788536071777
Epoch: 8, Steps: 56 | Train Loss: 0.5519454 Vali Loss: 1.4373852 Test Loss: 0.4331351
Validation loss decreased (1.437984 --> 1.437385).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.199504852294922
Epoch: 9, Steps: 56 | Train Loss: 0.5509128 Vali Loss: 1.4383420 Test Loss: 0.4329550
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.155734777450562
Epoch: 10, Steps: 56 | Train Loss: 0.5504508 Vali Loss: 1.4410878 Test Loss: 0.4329502
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 7.28618311882019
Epoch: 11, Steps: 56 | Train Loss: 0.5499832 Vali Loss: 1.4380535 Test Loss: 0.4332106
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43219244480133057, mae:0.45771729946136475, rse:0.629347562789917, corr:[0.22217779 0.23224302 0.23090267 0.2304649  0.23182313 0.2316723
 0.23035097 0.22988084 0.23053996 0.2318336  0.23193231 0.23076916
 0.22979715 0.22949106 0.22941768 0.22913617 0.22857073 0.22814244
 0.22803512 0.2282559  0.2284802  0.22856149 0.22843689 0.22874959
 0.22952151 0.23016658 0.23015638 0.22961237 0.22929555 0.22937118
 0.22948164 0.22915971 0.22869089 0.22848123 0.22846808 0.22838664
 0.22815187 0.22764002 0.22725208 0.22726515 0.22767831 0.22779118
 0.22733025 0.22673848 0.22679214 0.22762729 0.2287363  0.22942422
 0.22938342 0.22915043 0.2287098  0.22787173 0.2267389  0.22557697
 0.22495593 0.2246932  0.22448412 0.22390863 0.22295953 0.2224186
 0.22235966 0.22226514 0.22181773 0.22143462 0.22139913 0.22178693
 0.22239824 0.22273196 0.22265254 0.22249807 0.22258148 0.22306335
 0.2232019  0.22255355 0.22128075 0.22039427 0.22032888 0.22062398
 0.22066772 0.22013631 0.21926302 0.2185761  0.21815059 0.21780197
 0.21740466 0.21691793 0.21651    0.21632503 0.21625881 0.21618822
 0.21597812 0.21597013 0.216303   0.21685362 0.21734479 0.21798946
 0.21902364 0.22020558 0.22104472 0.22150224 0.22157073 0.22146668
 0.22140375 0.22140336 0.22120164 0.22074156 0.22025737 0.22000521
 0.2199735  0.21988896 0.21975785 0.21972428 0.21979791 0.21978927
 0.21971008 0.21954489 0.21940719 0.2193516  0.21949646 0.21995349
 0.22022024 0.21974465 0.21884403 0.21835336 0.21822661 0.21803318
 0.21774533 0.21770078 0.21780314 0.21777041 0.2173122  0.21655808
 0.2160441  0.2160037  0.2162138  0.21621862 0.21593526 0.21558054
 0.21547277 0.2156585  0.21597025 0.21604146 0.21568833 0.2153125
 0.21518171 0.21519315 0.21515337 0.21469863 0.21399339 0.2133907
 0.21318272 0.21334255 0.21339084 0.21315916 0.21289903 0.21288186
 0.21294092 0.21286935 0.2127457  0.2127     0.21251276 0.21218644
 0.21185681 0.21199578 0.21260071 0.21314228 0.21310522 0.21302457
 0.21320604 0.21360599 0.21397299 0.21427108 0.21418568 0.21365194
 0.2129336  0.21242383 0.21221878 0.21233384 0.21241975 0.21227066
 0.21203324 0.2119758  0.21218063 0.21248303 0.21270233 0.2127561
 0.2126591  0.21260744 0.21293165 0.21345967 0.21384189 0.21368602
 0.21310134 0.21246597 0.21199022 0.2116636  0.21129352 0.21099766
 0.21103765 0.21139316 0.21136625 0.21059488 0.20960943 0.20928338
 0.20980957 0.21063267 0.2110651  0.21099178 0.21072002 0.21060236
 0.21080235 0.21111764 0.21123774 0.21087596 0.21037343 0.21032827
 0.21065232 0.21074167 0.21025454 0.20970872 0.20955066 0.20951995
 0.20939991 0.20928206 0.20936388 0.20947844 0.2091575  0.20846589
 0.2079421  0.20783527 0.20782915 0.20739675 0.20657489 0.206062
 0.20606168 0.20612223 0.20608005 0.20623194 0.20691042 0.20758091
 0.20776    0.20752683 0.20754759 0.208034   0.20825906 0.20784092
 0.20752886 0.20784341 0.20822538 0.20781574 0.20685709 0.20634906
 0.20669413 0.207104   0.20701785 0.20662737 0.20662093 0.20710963
 0.20750393 0.20731302 0.20700091 0.20698932 0.20725568 0.20761861
 0.20774457 0.20738561 0.20658468 0.20566119 0.20492135 0.20467716
 0.204875   0.20512263 0.20520063 0.20533276 0.20548931 0.20557527
 0.20558201 0.20564373 0.2057522  0.2058758  0.2058958  0.20581299
 0.20553243 0.2052632  0.20516038 0.20524685 0.20549974 0.2060127
 0.2066092  0.20709115 0.20735477 0.20750815 0.20748591 0.20737809
 0.20727523 0.2072564  0.2072821  0.20737138 0.2074344  0.20740755
 0.20732412 0.20723146 0.20732819 0.20748912 0.20756254 0.20757829
 0.20775226 0.20796001 0.20806023 0.20800593 0.20798673 0.20833294
 0.20874897 0.20865172 0.20800059 0.20732988 0.20674467 0.20610036
 0.20558399 0.20544703 0.20554036 0.20542414 0.2048982  0.20434792
 0.20426248 0.20466104 0.20505863 0.20528209 0.20513387 0.20491283
 0.20477952 0.2048231  0.20514546 0.20558566 0.2057767  0.20557097
 0.2053681  0.20536023 0.20522979 0.2045759  0.20356022 0.20292479
 0.20302905 0.20310949 0.20252115 0.20180386 0.2017676  0.2024258
 0.20296036 0.20307027 0.20303601 0.20312582 0.20310429 0.20312709
 0.20331843 0.2037109  0.20405084 0.20414627 0.20394786 0.20395583
 0.20405439 0.20366311 0.20278507 0.2020163  0.20161761 0.20124738
 0.20071994 0.20032533 0.20028397 0.20031752 0.20009634 0.19979544
 0.19977961 0.19996302 0.199768   0.19894579 0.19811878 0.19783834
 0.19791351 0.19783996 0.19794118 0.1988402  0.19990139 0.20041537
 0.20022121 0.2001719  0.20085064 0.20154709 0.20125736 0.20024459
 0.19991438 0.20026563 0.20000055 0.19881374 0.19791631 0.19850528
 0.19966945 0.19979751 0.19898134 0.1985739  0.19928524 0.20029558
 0.20045364 0.19987775 0.19967222 0.20030178 0.20122467 0.20207323
 0.2025448  0.20239654 0.20179355 0.2013554  0.20149478 0.20176767
 0.20172098 0.2011379  0.20044711 0.20007282 0.19993591 0.19974858
 0.19970171 0.20015943 0.20058566 0.20037585 0.19953893 0.19897321
 0.19943804 0.20072149 0.20174032 0.20235457 0.20227823 0.20170689
 0.20103334 0.20053048 0.20033865 0.20030642 0.20001306 0.19970214
 0.20014179 0.20126115 0.2018435  0.20152245 0.2010003  0.20118088
 0.20181467 0.20184857 0.20093489 0.19966777 0.19896148 0.1991524
 0.1997682  0.20015748 0.2002496  0.200374   0.2008137  0.20170154
 0.20237905 0.20236847 0.20191847 0.20164865 0.2015834  0.2011966
 0.20052776 0.2000238  0.19998461 0.20024687 0.20026416 0.20023574
 0.2006165  0.20131138 0.20156707 0.2010564  0.2003709  0.20035999
 0.2008407  0.20124935 0.20152394 0.20171295 0.20173694 0.20093897
 0.19965206 0.19917326 0.19979043 0.2004569  0.20015141 0.1993414
 0.19927044 0.19975342 0.19942576 0.19841065 0.19794072 0.19856954
 0.19921415 0.19874929 0.19774914 0.1972702  0.19730158 0.19701986
 0.19639418 0.19636936 0.19741939 0.19887955 0.2000003  0.20091039
 0.20176347 0.20223795 0.20188878 0.2013296  0.20126587 0.20133165
 0.20106278 0.20057802 0.20032689 0.20034793 0.20031145 0.20003091
 0.19977993 0.19986536 0.20015384 0.20011869 0.20026748 0.201059
 0.20201181 0.20231064 0.20233539 0.20272234 0.20407638 0.20474803
 0.20413122 0.20280024 0.20211548 0.20219982 0.20169637 0.20061535
 0.20001465 0.20032498 0.20044675 0.19997089 0.19956958 0.20001322
 0.20073059 0.20072359 0.20019865 0.19976848 0.19973557 0.19960813
 0.19932577 0.19932918 0.19958465 0.1994687  0.19884115 0.19832025
 0.19815582 0.19791353 0.1968845  0.19548525 0.19471386 0.19469194
 0.19473079 0.1944765  0.19414492 0.19393697 0.19368641 0.19355436
 0.19388321 0.19441114 0.19462353 0.1938648  0.19323148 0.19350332
 0.19429761 0.19426212 0.19357821 0.19302084 0.19297881 0.19290082
 0.1926927  0.19293982 0.1932574  0.19296871 0.19155939 0.19013773
 0.1903867  0.19149809 0.19162968 0.19026566 0.18920301 0.18948872
 0.19045348 0.19046088 0.18958712 0.18854403 0.18780811 0.18746
 0.18796661 0.18969107 0.19181347 0.19274479 0.19185816 0.1900838
 0.18865505 0.18775494 0.18704027 0.18642531 0.18581174 0.1847943
 0.18384607 0.18394862 0.18510889 0.18622753 0.1859372  0.18448125
 0.18325971 0.18289697 0.18289034 0.18258266 0.18230616 0.18253271
 0.18296856 0.18313171 0.18337184 0.18396175 0.184232   0.18322267
 0.18147631 0.18062806 0.18098637 0.18141124 0.18089123 0.18026584
 0.18087387 0.18203473 0.18196543 0.18060535 0.17929864 0.1787911
 0.17829248 0.17728074 0.1766072  0.17668732 0.1768841  0.17681104
 0.17697255 0.17805775 0.17943174 0.18001333 0.18012129 0.18061762
 0.18116784 0.18087776 0.17970568 0.17887464 0.178234   0.1769822
 0.17540717 0.17471859 0.1753059  0.1757163  0.17513958 0.174703
 0.17551251 0.17640144 0.17584677 0.1742949  0.1739778  0.17537114
 0.17643133 0.17581613 0.17462713 0.17451702 0.1747196  0.17354952
 0.17104094 0.16892292 0.1675385  0.16613302 0.16491659 0.16468419
 0.16592157 0.16640393 0.16492626 0.16284426 0.16271946 0.16408545
 0.1643378  0.16277729 0.16168045 0.16221021 0.16353486 0.164526
 0.16493925 0.16503601 0.16496919 0.16523921 0.16632268 0.16729695
 0.16651449 0.1648685  0.1640614  0.16465391 0.16374691 0.1606579
 0.15893005 0.16037709 0.16242857 0.16184565 0.15996395 0.15962167
 0.16057245 0.1600331  0.15919006 0.16042295 0.16251682 0.16161752
 0.16013739 0.16210371 0.16398284 0.15784322 0.15590099 0.17020044]
