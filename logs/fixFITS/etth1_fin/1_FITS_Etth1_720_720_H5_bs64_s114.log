Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  48787200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 7.899240016937256
Epoch: 1, Steps: 56 | Train Loss: 0.9265040 Vali Loss: 1.8900328 Test Loss: 0.7505700
Validation loss decreased (inf --> 1.890033).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.623578071594238
Epoch: 2, Steps: 56 | Train Loss: 0.7415842 Vali Loss: 1.7243936 Test Loss: 0.6431875
Validation loss decreased (1.890033 --> 1.724394).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.016080379486084
Epoch: 3, Steps: 56 | Train Loss: 0.6869100 Vali Loss: 1.6487675 Test Loss: 0.5915476
Validation loss decreased (1.724394 --> 1.648767).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 7.917074918746948
Epoch: 4, Steps: 56 | Train Loss: 0.6558743 Vali Loss: 1.6008215 Test Loss: 0.5551286
Validation loss decreased (1.648767 --> 1.600821).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.197715759277344
Epoch: 5, Steps: 56 | Train Loss: 0.6345485 Vali Loss: 1.5657185 Test Loss: 0.5278399
Validation loss decreased (1.600821 --> 1.565719).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.095751762390137
Epoch: 6, Steps: 56 | Train Loss: 0.6188610 Vali Loss: 1.5383283 Test Loss: 0.5066825
Validation loss decreased (1.565719 --> 1.538328).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.071842432022095
Epoch: 7, Steps: 56 | Train Loss: 0.6065331 Vali Loss: 1.5164216 Test Loss: 0.4900827
Validation loss decreased (1.538328 --> 1.516422).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 7.811012506484985
Epoch: 8, Steps: 56 | Train Loss: 0.5972277 Vali Loss: 1.4982246 Test Loss: 0.4773948
Validation loss decreased (1.516422 --> 1.498225).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 7.780905723571777
Epoch: 9, Steps: 56 | Train Loss: 0.5898400 Vali Loss: 1.4779389 Test Loss: 0.4673242
Validation loss decreased (1.498225 --> 1.477939).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.872532606124878
Epoch: 10, Steps: 56 | Train Loss: 0.5837753 Vali Loss: 1.4784098 Test Loss: 0.4594252
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.736604690551758
Epoch: 11, Steps: 56 | Train Loss: 0.5793377 Vali Loss: 1.4600226 Test Loss: 0.4535217
Validation loss decreased (1.477939 --> 1.460023).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 7.12894344329834
Epoch: 12, Steps: 56 | Train Loss: 0.5753025 Vali Loss: 1.4543676 Test Loss: 0.4485788
Validation loss decreased (1.460023 --> 1.454368).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 7.597331762313843
Epoch: 13, Steps: 56 | Train Loss: 0.5722224 Vali Loss: 1.4563439 Test Loss: 0.4448848
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 7.961891412734985
Epoch: 14, Steps: 56 | Train Loss: 0.5696205 Vali Loss: 1.4469030 Test Loss: 0.4420261
Validation loss decreased (1.454368 --> 1.446903).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.739152669906616
Epoch: 15, Steps: 56 | Train Loss: 0.5673553 Vali Loss: 1.4457124 Test Loss: 0.4398244
Validation loss decreased (1.446903 --> 1.445712).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.398926258087158
Epoch: 16, Steps: 56 | Train Loss: 0.5658190 Vali Loss: 1.4413614 Test Loss: 0.4381043
Validation loss decreased (1.445712 --> 1.441361).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 7.835302114486694
Epoch: 17, Steps: 56 | Train Loss: 0.5641115 Vali Loss: 1.4431896 Test Loss: 0.4367493
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.055382251739502
Epoch: 18, Steps: 56 | Train Loss: 0.5627510 Vali Loss: 1.4371471 Test Loss: 0.4356961
Validation loss decreased (1.441361 --> 1.437147).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 7.745650291442871
Epoch: 19, Steps: 56 | Train Loss: 0.5618618 Vali Loss: 1.4396405 Test Loss: 0.4348140
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 7.859816312789917
Epoch: 20, Steps: 56 | Train Loss: 0.5610820 Vali Loss: 1.4415458 Test Loss: 0.4342808
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.069820642471313
Epoch: 21, Steps: 56 | Train Loss: 0.5600226 Vali Loss: 1.4300637 Test Loss: 0.4338412
Validation loss decreased (1.437147 --> 1.430064).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 7.804943323135376
Epoch: 22, Steps: 56 | Train Loss: 0.5596195 Vali Loss: 1.4331182 Test Loss: 0.4334532
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 7.86943793296814
Epoch: 23, Steps: 56 | Train Loss: 0.5589932 Vali Loss: 1.4352994 Test Loss: 0.4331543
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.003840923309326
Epoch: 24, Steps: 56 | Train Loss: 0.5582174 Vali Loss: 1.4363594 Test Loss: 0.4330052
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4329095482826233, mae:0.4587344825267792, rse:0.6298694610595703, corr:[0.21474522 0.23216441 0.22961183 0.22995077 0.23327127 0.23354368
 0.23191263 0.23170651 0.23280899 0.23366015 0.23296118 0.23155513
 0.23031814 0.22964418 0.22948198 0.22939199 0.22886154 0.2278876
 0.22727527 0.22761993 0.22826351 0.22810155 0.22742443 0.22720626
 0.22789864 0.22859804 0.22874124 0.22857706 0.22894563 0.22975332
 0.23014633 0.22971627 0.22902858 0.2286713  0.22866297 0.22867496
 0.22849563 0.22794485 0.22735046 0.22713071 0.22734329 0.22762594
 0.2277223  0.22778842 0.22808649 0.22872083 0.22930898 0.22943786
 0.22930877 0.22898684 0.22859262 0.22795925 0.22698604 0.22603022
 0.22561032 0.22538316 0.22528492 0.22470644 0.2240249  0.22371091
 0.22371149 0.22358948 0.22322875 0.22294968 0.2229446  0.22320665
 0.22345714 0.22352764 0.2235294  0.22348017 0.22339371 0.22350152
 0.22341648 0.22297662 0.22236262 0.22193705 0.22188023 0.22188531
 0.22156917 0.22090918 0.2203446  0.22010285 0.219977   0.21967019
 0.2191475  0.21865676 0.21835876 0.21834503 0.21831991 0.21811624
 0.2178095  0.21781568 0.21809204 0.2185428  0.2188628  0.21956807
 0.220769   0.22185741 0.22252889 0.22293106 0.2230263  0.22288674
 0.2228088  0.22289866 0.22290492 0.22249115 0.22191308 0.22162385
 0.22157003 0.22153093 0.2212945  0.22104736 0.22104572 0.22118591
 0.22128892 0.22122815 0.22113723 0.22106165 0.22099656 0.22111742
 0.2212535  0.22108306 0.2206003  0.2201744  0.21987873 0.21970113
 0.21947591 0.21915337 0.21887495 0.21869034 0.21856867 0.21830913
 0.21796855 0.21765886 0.21753497 0.21752228 0.21732278 0.21700422
 0.21684545 0.21700718 0.21727923 0.21728952 0.21683134 0.2165338
 0.21645984 0.2163174  0.21604078 0.21570913 0.21538334 0.21511352
 0.21506467 0.21532409 0.21545984 0.21500894 0.21430323 0.21405223
 0.21417803 0.2141514  0.21373016 0.2135198  0.2137662  0.21422306
 0.21434811 0.21419403 0.21411298 0.21434468 0.21423267 0.21413471
 0.21421865 0.21458669 0.21495564 0.21521327 0.21522744 0.21516994
 0.21514413 0.21513043 0.21502359 0.21495426 0.21491343 0.21468921
 0.21422245 0.21387325 0.21406038 0.21460491 0.21507771 0.2152952
 0.21548326 0.21575637 0.21592917 0.21567369 0.21527928 0.2151739
 0.21522653 0.2149654  0.21443425 0.21402009 0.21381527 0.21363246
 0.21345554 0.21344486 0.21340905 0.21298589 0.21250337 0.21242675
 0.2128668  0.21331422 0.21335632 0.21320894 0.21333532 0.21367227
 0.2136785  0.21319276 0.21262026 0.21227162 0.21209438 0.21205461
 0.21209924 0.21221124 0.21226211 0.21215725 0.21197218 0.21196584
 0.21217166 0.2122117  0.21182545 0.2112676  0.21085817 0.21066496
 0.21045856 0.21025188 0.21018846 0.21018523 0.20993863 0.20957415
 0.20949106 0.2097059  0.20977016 0.20944841 0.20907745 0.20914692
 0.20967838 0.2100831  0.21023223 0.21033691 0.21043958 0.21046682
 0.2105243  0.21053737 0.21032637 0.20971155 0.20903045 0.20874766
 0.20894483 0.20912214 0.20898345 0.20861284 0.20859018 0.20907024
 0.20951904 0.20936292 0.20891826 0.20857112 0.20849252 0.20863183
 0.20884313 0.20895018 0.20887701 0.20874527 0.20861547 0.20857279
 0.20848657 0.20817569 0.20784637 0.20782246 0.20797724 0.20802303
 0.20788275 0.20788503 0.20791575 0.20788878 0.2076739  0.20757982
 0.20768909 0.20803475 0.20813924 0.20785147 0.20748869 0.2077356
 0.20843671 0.20896813 0.20921336 0.20940988 0.20950054 0.2093717
 0.20926788 0.20952813 0.20991336 0.21000196 0.20978928 0.20973583
 0.20993745 0.21000539 0.20976833 0.20958146 0.20974806 0.2101425
 0.21045807 0.21041714 0.21035196 0.21047132 0.21055317 0.21062578
 0.21083811 0.21109478 0.21117069 0.21095096 0.21046154 0.21002237
 0.20968522 0.20925537 0.20869963 0.20830265 0.2082448  0.2082662
 0.20818727 0.20818083 0.20838365 0.20864752 0.20855631 0.2083241
 0.20831375 0.20850737 0.20867626 0.20873782 0.2087295  0.20877336
 0.20863877 0.20814037 0.20777084 0.20783657 0.20783526 0.20735708
 0.20665587 0.20616792 0.20591627 0.20566396 0.205333   0.20524438
 0.20552808 0.20585087 0.20573227 0.2054564  0.20557922 0.20628789
 0.2067003  0.20645137 0.20603143 0.20605709 0.20618345 0.20622785
 0.20615853 0.20597997 0.20575988 0.2055     0.20512384 0.20464508
 0.20411743 0.20353216 0.20301253 0.2028189  0.2028484  0.20261206
 0.20186691 0.2012523  0.20130613 0.20159921 0.20151106 0.20108384
 0.20092092 0.20102085 0.20099455 0.20083888 0.20080093 0.20158401
 0.20278123 0.20355825 0.20379408 0.20382823 0.20357133 0.20266315
 0.20168717 0.20121534 0.20100658 0.2003429  0.19915326 0.19852215
 0.19898157 0.19974184 0.19990216 0.19950168 0.19949554 0.20018175
 0.20077658 0.20066315 0.20039622 0.20058836 0.20096119 0.20131212
 0.20173234 0.20234019 0.20289743 0.20299026 0.2026618  0.20219164
 0.20200717 0.20183013 0.20146619 0.20109022 0.20100352 0.20106257
 0.20095152 0.20088008 0.20092182 0.20098567 0.2007043  0.20031604
 0.20033963 0.20085855 0.20101629 0.2009262  0.20081659 0.20120436
 0.20189662 0.20211883 0.20191342 0.20206802 0.20260791 0.20289601
 0.202711   0.20230563 0.2017922  0.20140809 0.20113973 0.20114364
 0.2012653  0.2011325  0.20064159 0.20004445 0.19981763 0.19992954
 0.19992101 0.19966173 0.19972259 0.20039442 0.20126186 0.20190008
 0.20217474 0.20235807 0.20257953 0.2025813  0.20222704 0.20149887
 0.20072733 0.20014781 0.19998041 0.20035931 0.20064728 0.20039885
 0.19981647 0.1998179  0.20053813 0.20102604 0.20053615 0.1999114
 0.20012784 0.2008859  0.20089062 0.19972402 0.19886974 0.19920212
 0.20013858 0.20055611 0.20029104 0.20003407 0.19985352 0.1993257
 0.1987037  0.19866106 0.19890176 0.1988414  0.19820704 0.19772111
 0.1979355  0.19829538 0.19808546 0.19750535 0.19746396 0.19800507
 0.19820623 0.19761902 0.19709368 0.19748807 0.19839798 0.19906494
 0.19946136 0.20017256 0.20114116 0.2017116  0.20139633 0.2003963
 0.19966455 0.19958144 0.19954818 0.19915973 0.19875051 0.1986334
 0.19872361 0.1990357  0.19970003 0.20013566 0.20033567 0.20057634
 0.20112598 0.2017323  0.20211603 0.2016533  0.2016638  0.20191537
 0.20234598 0.20208669 0.20145535 0.20137897 0.2014158  0.20111385
 0.20041911 0.2001768  0.2004404  0.20074643 0.20043252 0.19997989
 0.20002046 0.2005261  0.2012062  0.20154718 0.20171311 0.20161514
 0.20126188 0.20099185 0.20118137 0.2016871  0.20178464 0.20107096
 0.20005792 0.19963636 0.19962612 0.1993609  0.19861023 0.19771603
 0.19702505 0.19663669 0.1963392  0.19614868 0.19592434 0.1954115
 0.19447388 0.19348903 0.1933117  0.19324811 0.193194   0.1931217
 0.19353999 0.19402762 0.1943237  0.19415267 0.19382678 0.1935407
 0.19313052 0.19251987 0.19182628 0.19179876 0.19193834 0.19148737
 0.19099857 0.19073217 0.19084026 0.19073908 0.19061163 0.19078188
 0.191407   0.19162951 0.19129105 0.19072954 0.19065355 0.19101608
 0.1911834  0.19101487 0.19103983 0.19155796 0.19194804 0.1915761
 0.19075716 0.19004048 0.18949652 0.18881448 0.18779348 0.18647136
 0.18544602 0.18509775 0.1851565  0.18537885 0.18529403 0.1848181
 0.18415076 0.18365312 0.18350168 0.18316694 0.18257777 0.18243633
 0.18324676 0.18438552 0.18480256 0.18432562 0.18394342 0.1841832
 0.18452182 0.18446872 0.18424636 0.18439269 0.18440057 0.18351379
 0.18230656 0.18147478 0.18093553 0.18040797 0.17986673 0.1799686
 0.18067946 0.18099302 0.1806014  0.1799205  0.17964001 0.17969155
 0.17934167 0.1787367  0.17871204 0.17962977 0.18078424 0.18106791
 0.18031932 0.17954053 0.17933553 0.17964189 0.17917463 0.17762487
 0.1760228  0.17541572 0.17581989 0.17606966 0.17553149 0.17471002
 0.17433459 0.17445631 0.17480014 0.17467734 0.17407534 0.17363279
 0.17389727 0.17476185 0.17512998 0.17455867 0.17341709 0.17250845
 0.17147963 0.17010996 0.16844349 0.16686398 0.16577367 0.16474125
 0.16419752 0.16354112 0.16279468 0.16184038 0.1616538  0.16240801
 0.16297078 0.16238293 0.16155443 0.16142541 0.16206354 0.16252276
 0.1619639  0.16111553 0.16137362 0.16261157 0.16311145 0.16154386
 0.15894964 0.1582032  0.15916479 0.15998568 0.1582815  0.1552643
 0.1543634  0.15546443 0.15592207 0.15458974 0.15340407 0.15335874
 0.15337725 0.15225221 0.15147822 0.15109773 0.14931773 0.14714393
 0.14806472 0.14962192 0.14304459 0.1307342  0.1390269  0.15213662]
