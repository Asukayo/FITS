Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  100803584.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.546459197998047
Epoch: 1, Steps: 29 | Train Loss: 0.8208714 Vali Loss: 1.6390599 Test Loss: 0.7498865
Validation loss decreased (inf --> 1.639060).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.397677898406982
Epoch: 2, Steps: 29 | Train Loss: 0.6589923 Vali Loss: 1.4652238 Test Loss: 0.6363562
Validation loss decreased (1.639060 --> 1.465224).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.790150165557861
Epoch: 3, Steps: 29 | Train Loss: 0.5955095 Vali Loss: 1.3862449 Test Loss: 0.5841715
Validation loss decreased (1.465224 --> 1.386245).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.80783486366272
Epoch: 4, Steps: 29 | Train Loss: 0.5613890 Vali Loss: 1.3367965 Test Loss: 0.5510504
Validation loss decreased (1.386245 --> 1.336797).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.88148307800293
Epoch: 5, Steps: 29 | Train Loss: 0.5382123 Vali Loss: 1.3025739 Test Loss: 0.5270280
Validation loss decreased (1.336797 --> 1.302574).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.3568572998046875
Epoch: 6, Steps: 29 | Train Loss: 0.5203254 Vali Loss: 1.2756587 Test Loss: 0.5086986
Validation loss decreased (1.302574 --> 1.275659).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.263326406478882
Epoch: 7, Steps: 29 | Train Loss: 0.5082310 Vali Loss: 1.2535050 Test Loss: 0.4941809
Validation loss decreased (1.275659 --> 1.253505).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.309736967086792
Epoch: 8, Steps: 29 | Train Loss: 0.4968311 Vali Loss: 1.2378032 Test Loss: 0.4828637
Validation loss decreased (1.253505 --> 1.237803).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.629147529602051
Epoch: 9, Steps: 29 | Train Loss: 0.4890101 Vali Loss: 1.2286736 Test Loss: 0.4739488
Validation loss decreased (1.237803 --> 1.228674).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.7188262939453125
Epoch: 10, Steps: 29 | Train Loss: 0.4826151 Vali Loss: 1.2201489 Test Loss: 0.4664598
Validation loss decreased (1.228674 --> 1.220149).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.7496421337127686
Epoch: 11, Steps: 29 | Train Loss: 0.4763527 Vali Loss: 1.2124166 Test Loss: 0.4606864
Validation loss decreased (1.220149 --> 1.212417).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.506570339202881
Epoch: 12, Steps: 29 | Train Loss: 0.4713664 Vali Loss: 1.2055770 Test Loss: 0.4560544
Validation loss decreased (1.212417 --> 1.205577).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.940722942352295
Epoch: 13, Steps: 29 | Train Loss: 0.4675556 Vali Loss: 1.2007154 Test Loss: 0.4522426
Validation loss decreased (1.205577 --> 1.200715).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.958158016204834
Epoch: 14, Steps: 29 | Train Loss: 0.4639906 Vali Loss: 1.1983629 Test Loss: 0.4491290
Validation loss decreased (1.200715 --> 1.198363).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.1810302734375
Epoch: 15, Steps: 29 | Train Loss: 0.4615310 Vali Loss: 1.1907022 Test Loss: 0.4466950
Validation loss decreased (1.198363 --> 1.190702).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.362295627593994
Epoch: 16, Steps: 29 | Train Loss: 0.4593504 Vali Loss: 1.1913401 Test Loss: 0.4446334
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.260258913040161
Epoch: 17, Steps: 29 | Train Loss: 0.4576282 Vali Loss: 1.1884178 Test Loss: 0.4430715
Validation loss decreased (1.190702 --> 1.188418).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.534727096557617
Epoch: 18, Steps: 29 | Train Loss: 0.4552915 Vali Loss: 1.1854315 Test Loss: 0.4416378
Validation loss decreased (1.188418 --> 1.185431).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.2807230949401855
Epoch: 19, Steps: 29 | Train Loss: 0.4539063 Vali Loss: 1.1774583 Test Loss: 0.4406570
Validation loss decreased (1.185431 --> 1.177458).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.7206199169158936
Epoch: 20, Steps: 29 | Train Loss: 0.4533128 Vali Loss: 1.1788343 Test Loss: 0.4397759
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.012948513031006
Epoch: 21, Steps: 29 | Train Loss: 0.4515393 Vali Loss: 1.1897657 Test Loss: 0.4390825
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.061743259429932
Epoch: 22, Steps: 29 | Train Loss: 0.4513347 Vali Loss: 1.1836073 Test Loss: 0.4385327
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4389816224575043, mae:0.4436511695384979, rse:0.6307761073112488, corr:[0.25006172 0.26048592 0.2532959  0.25616425 0.2563827  0.252604
 0.25234368 0.2543913  0.25442782 0.25299665 0.25255296 0.25252518
 0.25200993 0.25149807 0.2509495  0.2502361  0.24989073 0.24972245
 0.24873714 0.24766755 0.24766585 0.24769396 0.2469531  0.24608997
 0.24626498 0.24661745 0.24664274 0.24691741 0.24769793 0.24808894
 0.24780801 0.24750543 0.24765475 0.24770547 0.2474351  0.24709322
 0.24686244 0.24677823 0.2468097  0.24684647 0.246741   0.24679883
 0.24721113 0.24750575 0.24742985 0.24768865 0.24839465 0.24865325
 0.24851795 0.24819854 0.24774718 0.24701974 0.2458459  0.2447514
 0.24420562 0.24382766 0.24323075 0.24261545 0.24244049 0.24249567
 0.24220167 0.24187297 0.24189943 0.24206783 0.24211144 0.24232866
 0.24279861 0.24291138 0.24276972 0.24261633 0.24258916 0.24243914
 0.24201827 0.24151582 0.24118567 0.24095872 0.24061948 0.24026248
 0.23983873 0.23945877 0.23936947 0.23938175 0.23903453 0.2385597
 0.23830007 0.23809189 0.23746242 0.23686588 0.23700117 0.23745573
 0.23728892 0.23677152 0.236547   0.23662373 0.23639308 0.23662765
 0.23772003 0.2386334  0.23894879 0.23921198 0.23946837 0.23941928
 0.2393295  0.23933727 0.23908322 0.23862351 0.23840246 0.23839952
 0.23808782 0.2377429  0.23798613 0.23849075 0.23847799 0.2381468
 0.23808508 0.23818843 0.23812287 0.23799518 0.23805587 0.2382158
 0.23807327 0.23759319 0.23711343 0.23670383 0.23614168 0.23545322
 0.23500715 0.2348366  0.23470119 0.23442596 0.2339421  0.23356488
 0.23358564 0.23358123 0.23317072 0.23275098 0.23282282 0.23322454
 0.23321308 0.23284943 0.23271228 0.2328428  0.2325674  0.23211865
 0.23186487 0.23158936 0.23138312 0.23115507 0.23058759 0.2298395
 0.22964525 0.2297768  0.22965078 0.22944638 0.22952679 0.22954664
 0.22902551 0.22867507 0.22899768 0.22922567 0.22882581 0.22871585
 0.2292059  0.22937396 0.22881028 0.22856505 0.22868082 0.22851689
 0.22785215 0.22764236 0.22816782 0.2287531  0.22876184 0.22843608
 0.22813828 0.22806133 0.22811148 0.2281363  0.22777477 0.2273013
 0.22712293 0.22714554 0.22699039 0.2268765  0.22722647 0.22771597
 0.22774969 0.22767343 0.22803538 0.2285715  0.2286536  0.22836126
 0.22793084 0.227371   0.22672635 0.22611871 0.22548921 0.22483301
 0.22442186 0.22432846 0.22425304 0.22420332 0.2241958  0.22413523
 0.22389919 0.22428921 0.2251921  0.22562759 0.22542445 0.22528127
 0.22547433 0.2252454  0.22455876 0.22425462 0.22430184 0.22398996
 0.2232698  0.2229844  0.22327895 0.22349188 0.2231515  0.22234923
 0.22188038 0.22207657 0.22235294 0.22205247 0.22144048 0.22150485
 0.22192492 0.2218504  0.22125047 0.22100756 0.22142468 0.22170927
 0.22118147 0.22072099 0.22102213 0.2211575  0.22047786 0.2198577
 0.22039172 0.22106214 0.22103372 0.22052622 0.22014871 0.2200407
 0.21999313 0.21987447 0.21957411 0.21930696 0.21914747 0.21876255
 0.2180193  0.21766949 0.21845078 0.2191844  0.21898572 0.21871547
 0.21932633 0.21974455 0.21933843 0.21900111 0.2196306  0.21996677
 0.2194078  0.21896882 0.2190845  0.21901318 0.21816908 0.21730784
 0.2169107  0.21637584 0.21584657 0.21599929 0.21613595 0.21588817
 0.2156943  0.21573305 0.21562642 0.21579197 0.2164409  0.21677385
 0.21611613 0.21557005 0.21599609 0.21598627 0.21500534 0.2146202
 0.21529628 0.21565166 0.21514726 0.21502903 0.2150095  0.21445173
 0.21376583 0.21393995 0.21423382 0.21416555 0.21426709 0.21469516
 0.21441507 0.21387468 0.21446256 0.21492265 0.21470669 0.21479832
 0.21555202 0.21466678 0.21310149 0.21302867 0.21382523 0.21279648
 0.21105544 0.21109493 0.21146196 0.20980053 0.20782384 0.2072057
 0.20663722 0.2047968  0.20401676 0.2046658  0.20394209 0.20292728
 0.20323822 0.20227216 0.20019397 0.20116214 0.20164067 0.19673595
 0.19282854 0.1948942  0.19024007 0.17564943 0.18046856 0.18321101]
