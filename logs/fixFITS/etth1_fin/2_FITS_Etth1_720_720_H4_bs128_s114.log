Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  64354304.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.689485311508179
Epoch: 1, Steps: 28 | Train Loss: 0.8881896 Vali Loss: 2.3092022 Test Loss: 1.0126653
Validation loss decreased (inf --> 2.309202).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.391847848892212
Epoch: 2, Steps: 28 | Train Loss: 0.7602663 Vali Loss: 2.1098468 Test Loss: 0.9024584
Validation loss decreased (2.309202 --> 2.109847).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.3959503173828125
Epoch: 3, Steps: 28 | Train Loss: 0.6777730 Vali Loss: 1.9872791 Test Loss: 0.8325425
Validation loss decreased (2.109847 --> 1.987279).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.317861795425415
Epoch: 4, Steps: 28 | Train Loss: 0.6242720 Vali Loss: 1.9076313 Test Loss: 0.7873703
Validation loss decreased (1.987279 --> 1.907631).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.4612510204315186
Epoch: 5, Steps: 28 | Train Loss: 0.5878897 Vali Loss: 1.8508904 Test Loss: 0.7577523
Validation loss decreased (1.907631 --> 1.850890).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.235864162445068
Epoch: 6, Steps: 28 | Train Loss: 0.5620064 Vali Loss: 1.8188946 Test Loss: 0.7374237
Validation loss decreased (1.850890 --> 1.818895).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.567564010620117
Epoch: 7, Steps: 28 | Train Loss: 0.5429544 Vali Loss: 1.7903728 Test Loss: 0.7234042
Validation loss decreased (1.818895 --> 1.790373).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.455003261566162
Epoch: 8, Steps: 28 | Train Loss: 0.5281726 Vali Loss: 1.7686161 Test Loss: 0.7124691
Validation loss decreased (1.790373 --> 1.768616).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.417659521102905
Epoch: 9, Steps: 28 | Train Loss: 0.5163375 Vali Loss: 1.7527070 Test Loss: 0.7035497
Validation loss decreased (1.768616 --> 1.752707).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.377740859985352
Epoch: 10, Steps: 28 | Train Loss: 0.5065645 Vali Loss: 1.7483330 Test Loss: 0.6969401
Validation loss decreased (1.752707 --> 1.748333).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.8231124877929688
Epoch: 11, Steps: 28 | Train Loss: 0.4979310 Vali Loss: 1.7347791 Test Loss: 0.6904452
Validation loss decreased (1.748333 --> 1.734779).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.140652894973755
Epoch: 12, Steps: 28 | Train Loss: 0.4905895 Vali Loss: 1.7253995 Test Loss: 0.6853894
Validation loss decreased (1.734779 --> 1.725399).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.009608745574951
Epoch: 13, Steps: 28 | Train Loss: 0.4842722 Vali Loss: 1.7227328 Test Loss: 0.6811043
Validation loss decreased (1.725399 --> 1.722733).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.157258987426758
Epoch: 14, Steps: 28 | Train Loss: 0.4783777 Vali Loss: 1.7179317 Test Loss: 0.6765321
Validation loss decreased (1.722733 --> 1.717932).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.321422338485718
Epoch: 15, Steps: 28 | Train Loss: 0.4731851 Vali Loss: 1.7064221 Test Loss: 0.6728079
Validation loss decreased (1.717932 --> 1.706422).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.1071600914001465
Epoch: 16, Steps: 28 | Train Loss: 0.4682019 Vali Loss: 1.7030792 Test Loss: 0.6685719
Validation loss decreased (1.706422 --> 1.703079).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.419539928436279
Epoch: 17, Steps: 28 | Train Loss: 0.4639804 Vali Loss: 1.6967168 Test Loss: 0.6654283
Validation loss decreased (1.703079 --> 1.696717).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.710992813110352
Epoch: 18, Steps: 28 | Train Loss: 0.4599073 Vali Loss: 1.6858183 Test Loss: 0.6622201
Validation loss decreased (1.696717 --> 1.685818).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.197289705276489
Epoch: 19, Steps: 28 | Train Loss: 0.4560502 Vali Loss: 1.6890259 Test Loss: 0.6589798
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.406450510025024
Epoch: 20, Steps: 28 | Train Loss: 0.4525151 Vali Loss: 1.6840367 Test Loss: 0.6563480
Validation loss decreased (1.685818 --> 1.684037).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.2269065380096436
Epoch: 21, Steps: 28 | Train Loss: 0.4495161 Vali Loss: 1.6774831 Test Loss: 0.6535327
Validation loss decreased (1.684037 --> 1.677483).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.406461477279663
Epoch: 22, Steps: 28 | Train Loss: 0.4464902 Vali Loss: 1.6769843 Test Loss: 0.6512462
Validation loss decreased (1.677483 --> 1.676984).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.064010858535767
Epoch: 23, Steps: 28 | Train Loss: 0.4438798 Vali Loss: 1.6679134 Test Loss: 0.6487733
Validation loss decreased (1.676984 --> 1.667913).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.149187088012695
Epoch: 24, Steps: 28 | Train Loss: 0.4408919 Vali Loss: 1.6675196 Test Loss: 0.6461253
Validation loss decreased (1.667913 --> 1.667520).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.136415481567383
Epoch: 25, Steps: 28 | Train Loss: 0.4388101 Vali Loss: 1.6713796 Test Loss: 0.6443427
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.077360153198242
Epoch: 26, Steps: 28 | Train Loss: 0.4364200 Vali Loss: 1.6667901 Test Loss: 0.6420375
Validation loss decreased (1.667520 --> 1.666790).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.117074966430664
Epoch: 27, Steps: 28 | Train Loss: 0.4344406 Vali Loss: 1.6578169 Test Loss: 0.6404821
Validation loss decreased (1.666790 --> 1.657817).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.050219535827637
Epoch: 28, Steps: 28 | Train Loss: 0.4324288 Vali Loss: 1.6633289 Test Loss: 0.6386018
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.988548755645752
Epoch: 29, Steps: 28 | Train Loss: 0.4304332 Vali Loss: 1.6550457 Test Loss: 0.6368104
Validation loss decreased (1.657817 --> 1.655046).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.202775716781616
Epoch: 30, Steps: 28 | Train Loss: 0.4289951 Vali Loss: 1.6530925 Test Loss: 0.6351790
Validation loss decreased (1.655046 --> 1.653093).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.552800178527832
Epoch: 31, Steps: 28 | Train Loss: 0.4272456 Vali Loss: 1.6498014 Test Loss: 0.6337090
Validation loss decreased (1.653093 --> 1.649801).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.348016023635864
Epoch: 32, Steps: 28 | Train Loss: 0.4257736 Vali Loss: 1.6472404 Test Loss: 0.6321635
Validation loss decreased (1.649801 --> 1.647240).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.2507100105285645
Epoch: 33, Steps: 28 | Train Loss: 0.4241483 Vali Loss: 1.6444893 Test Loss: 0.6309111
Validation loss decreased (1.647240 --> 1.644489).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.286848068237305
Epoch: 34, Steps: 28 | Train Loss: 0.4227642 Vali Loss: 1.6470633 Test Loss: 0.6295947
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.177417039871216
Epoch: 35, Steps: 28 | Train Loss: 0.4214672 Vali Loss: 1.6439576 Test Loss: 0.6283138
Validation loss decreased (1.644489 --> 1.643958).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.1893463134765625
Epoch: 36, Steps: 28 | Train Loss: 0.4202464 Vali Loss: 1.6432838 Test Loss: 0.6271677
Validation loss decreased (1.643958 --> 1.643284).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.426782131195068
Epoch: 37, Steps: 28 | Train Loss: 0.4190865 Vali Loss: 1.6446273 Test Loss: 0.6260619
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.8391995429992676
Epoch: 38, Steps: 28 | Train Loss: 0.4181147 Vali Loss: 1.6401737 Test Loss: 0.6249656
Validation loss decreased (1.643284 --> 1.640174).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 4.086029529571533
Epoch: 39, Steps: 28 | Train Loss: 0.4170314 Vali Loss: 1.6445370 Test Loss: 0.6240759
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.202430486679077
Epoch: 40, Steps: 28 | Train Loss: 0.4161171 Vali Loss: 1.6385522 Test Loss: 0.6229677
Validation loss decreased (1.640174 --> 1.638552).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 4.243921995162964
Epoch: 41, Steps: 28 | Train Loss: 0.4150829 Vali Loss: 1.6361072 Test Loss: 0.6221122
Validation loss decreased (1.638552 --> 1.636107).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.151279449462891
Epoch: 42, Steps: 28 | Train Loss: 0.4143267 Vali Loss: 1.6378918 Test Loss: 0.6213506
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.118169784545898
Epoch: 43, Steps: 28 | Train Loss: 0.4136039 Vali Loss: 1.6393960 Test Loss: 0.6205619
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.807780504226685
Epoch: 44, Steps: 28 | Train Loss: 0.4127493 Vali Loss: 1.6342884 Test Loss: 0.6197130
Validation loss decreased (1.636107 --> 1.634288).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.15526819229126
Epoch: 45, Steps: 28 | Train Loss: 0.4121148 Vali Loss: 1.6352890 Test Loss: 0.6189980
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.309915065765381
Epoch: 46, Steps: 28 | Train Loss: 0.4111980 Vali Loss: 1.6312203 Test Loss: 0.6183429
Validation loss decreased (1.634288 --> 1.631220).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.158633708953857
Epoch: 47, Steps: 28 | Train Loss: 0.4106879 Vali Loss: 1.6273036 Test Loss: 0.6175894
Validation loss decreased (1.631220 --> 1.627304).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.9378273487091064
Epoch: 48, Steps: 28 | Train Loss: 0.4100838 Vali Loss: 1.6291552 Test Loss: 0.6169663
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 4.584219217300415
Epoch: 49, Steps: 28 | Train Loss: 0.4095485 Vali Loss: 1.6300759 Test Loss: 0.6164987
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.137986421585083
Epoch: 50, Steps: 28 | Train Loss: 0.4086742 Vali Loss: 1.6302848 Test Loss: 0.6159553
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  64354304.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.415534496307373
Epoch: 1, Steps: 28 | Train Loss: 0.6507105 Vali Loss: 1.5923147 Test Loss: 0.5864305
Validation loss decreased (inf --> 1.592315).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.88218879699707
Epoch: 2, Steps: 28 | Train Loss: 0.6334280 Vali Loss: 1.5659499 Test Loss: 0.5610421
Validation loss decreased (1.592315 --> 1.565950).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.492328643798828
Epoch: 3, Steps: 28 | Train Loss: 0.6207109 Vali Loss: 1.5466652 Test Loss: 0.5410793
Validation loss decreased (1.565950 --> 1.546665).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.320305347442627
Epoch: 4, Steps: 28 | Train Loss: 0.6100855 Vali Loss: 1.5212159 Test Loss: 0.5246766
Validation loss decreased (1.546665 --> 1.521216).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.529360771179199
Epoch: 5, Steps: 28 | Train Loss: 0.6018986 Vali Loss: 1.5064193 Test Loss: 0.5112446
Validation loss decreased (1.521216 --> 1.506419).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.546428442001343
Epoch: 6, Steps: 28 | Train Loss: 0.5948427 Vali Loss: 1.4936304 Test Loss: 0.5001981
Validation loss decreased (1.506419 --> 1.493630).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.188331604003906
Epoch: 7, Steps: 28 | Train Loss: 0.5892803 Vali Loss: 1.4875034 Test Loss: 0.4909427
Validation loss decreased (1.493630 --> 1.487503).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.754504680633545
Epoch: 8, Steps: 28 | Train Loss: 0.5842329 Vali Loss: 1.4784770 Test Loss: 0.4832848
Validation loss decreased (1.487503 --> 1.478477).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.422292232513428
Epoch: 9, Steps: 28 | Train Loss: 0.5806740 Vali Loss: 1.4748476 Test Loss: 0.4769474
Validation loss decreased (1.478477 --> 1.474848).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.659223794937134
Epoch: 10, Steps: 28 | Train Loss: 0.5771778 Vali Loss: 1.4650404 Test Loss: 0.4718202
Validation loss decreased (1.474848 --> 1.465040).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.2644736766815186
Epoch: 11, Steps: 28 | Train Loss: 0.5744625 Vali Loss: 1.4654455 Test Loss: 0.4671865
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.413207292556763
Epoch: 12, Steps: 28 | Train Loss: 0.5717008 Vali Loss: 1.4552718 Test Loss: 0.4634880
Validation loss decreased (1.465040 --> 1.455272).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.819746017456055
Epoch: 13, Steps: 28 | Train Loss: 0.5697815 Vali Loss: 1.4541566 Test Loss: 0.4603423
Validation loss decreased (1.455272 --> 1.454157).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.411660671234131
Epoch: 14, Steps: 28 | Train Loss: 0.5680818 Vali Loss: 1.4477799 Test Loss: 0.4575687
Validation loss decreased (1.454157 --> 1.447780).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.482035398483276
Epoch: 15, Steps: 28 | Train Loss: 0.5665082 Vali Loss: 1.4440298 Test Loss: 0.4553930
Validation loss decreased (1.447780 --> 1.444030).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.7669684886932373
Epoch: 16, Steps: 28 | Train Loss: 0.5653623 Vali Loss: 1.4469614 Test Loss: 0.4533806
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.555173635482788
Epoch: 17, Steps: 28 | Train Loss: 0.5637639 Vali Loss: 1.4472013 Test Loss: 0.4517617
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.3810877799987793
Epoch: 18, Steps: 28 | Train Loss: 0.5627378 Vali Loss: 1.4384129 Test Loss: 0.4503335
Validation loss decreased (1.444030 --> 1.438413).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.335885763168335
Epoch: 19, Steps: 28 | Train Loss: 0.5617824 Vali Loss: 1.4385138 Test Loss: 0.4491389
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.617959499359131
Epoch: 20, Steps: 28 | Train Loss: 0.5611866 Vali Loss: 1.4370122 Test Loss: 0.4481664
Validation loss decreased (1.438413 --> 1.437012).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.6123275756835938
Epoch: 21, Steps: 28 | Train Loss: 0.5606093 Vali Loss: 1.4332018 Test Loss: 0.4472361
Validation loss decreased (1.437012 --> 1.433202).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.700969934463501
Epoch: 22, Steps: 28 | Train Loss: 0.5597308 Vali Loss: 1.4401979 Test Loss: 0.4464934
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.776850700378418
Epoch: 23, Steps: 28 | Train Loss: 0.5592339 Vali Loss: 1.4380915 Test Loss: 0.4458605
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.447258472442627
Epoch: 24, Steps: 28 | Train Loss: 0.5589553 Vali Loss: 1.4355030 Test Loss: 0.4453150
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4388725459575653, mae:0.4644348621368408, rse:0.6341925859451294, corr:[0.21345885 0.23043269 0.2299757  0.22591572 0.2262527  0.22869562
 0.23032987 0.23026739 0.22917095 0.228529   0.22849405 0.22863412
 0.22871265 0.22830229 0.22733484 0.22615671 0.2252893  0.22515564
 0.22560184 0.22598784 0.22584398 0.22544086 0.2250155  0.22542858
 0.2264955  0.22764768 0.22822253 0.22793797 0.22731921 0.22692296
 0.22701032 0.2271817  0.2271325  0.22667797 0.22583221 0.22494112
 0.22445421 0.22431135 0.22448744 0.22472276 0.22491534 0.22470783
 0.22420901 0.22400129 0.22455135 0.22566573 0.22681502 0.2274271
 0.22713678 0.2266295  0.22609875 0.225643   0.22525238 0.224407
 0.22322582 0.22201662 0.22134933 0.22118421 0.22104265 0.22087814
 0.220628   0.2201877  0.21963888 0.21939702 0.21953474 0.21998107
 0.22054142 0.22082223 0.22072528 0.22043142 0.22017455 0.22037035
 0.22057445 0.22053543 0.22009031 0.21946086 0.21884486 0.21832585
 0.21803477 0.21790847 0.2176555  0.21722528 0.2166636  0.21617536
 0.2159241  0.21575062 0.21552171 0.21523084 0.21484126 0.21445566
 0.2140797  0.21399295 0.21428408 0.21493436 0.21578094 0.21671017
 0.21762602 0.2184891  0.21922289 0.2198952  0.22032693 0.22037047
 0.22008054 0.21969743 0.21945772 0.21930166 0.21905142 0.21861011
 0.21818505 0.21781957 0.21757062 0.21757872 0.21777865 0.21800038
 0.21823993 0.21839023 0.2184142  0.2183204  0.21830252 0.21856855
 0.21888264 0.21880758 0.21814404 0.21736541 0.21672985 0.21640323
 0.21638833 0.21653049 0.21633239 0.21573837 0.21501535 0.2144719
 0.21432236 0.21439525 0.21452133 0.21460001 0.21459484 0.21447958
 0.21431176 0.2141973  0.2142644  0.21441387 0.21445084 0.21436316
 0.21408361 0.21357019 0.2130314  0.21256796 0.21233082 0.21208003
 0.21167265 0.21135938 0.21128024 0.2113862  0.2114604  0.21134973
 0.21101776 0.21058486 0.21036062 0.21046188 0.21063614 0.21083139
 0.21093445 0.2110046  0.21106431 0.21111242 0.21095282 0.21091877
 0.21119766 0.21174331 0.2121911  0.21242067 0.2122352  0.21181941
 0.21152996 0.2116225  0.21174055 0.21164168 0.21125637 0.21083266
 0.21065012 0.21075392 0.21100926 0.21124715 0.21143436 0.21161395
 0.21186844 0.212214   0.21261781 0.21282731 0.21278933 0.21254836
 0.21218841 0.21173956 0.21119314 0.21069735 0.21037364 0.2101053
 0.20973927 0.2094077  0.20915054 0.2090094  0.2090544  0.20927458
 0.20947152 0.20963165 0.20970263 0.20969059 0.2097066  0.20978943
 0.20989338 0.20990533 0.20982525 0.20955937 0.20918706 0.20896457
 0.20894304 0.20907396 0.2091245  0.20901717 0.20879027 0.20840165
 0.20808683 0.20797472 0.2079094  0.207702   0.2072263  0.206731
 0.20641963 0.20630346 0.20635073 0.20645982 0.2064229  0.2063071
 0.20621446 0.20614855 0.20611286 0.20612012 0.20630442 0.20665614
 0.20712274 0.20745972 0.20763493 0.20772924 0.20775613 0.20771725
 0.20765127 0.20742314 0.20692915 0.20630781 0.20589992 0.2058851
 0.20607126 0.20606095 0.20588395 0.2056925  0.20571187 0.20596075
 0.20624141 0.20626573 0.20622802 0.20612025 0.20593661 0.20577876
 0.20574422 0.20586163 0.20602645 0.20607193 0.20571563 0.20509751
 0.20458235 0.20434529 0.20419864 0.20418102 0.20421268 0.20433071
 0.20452738 0.20480268 0.20482793 0.20464513 0.2043887  0.2043581
 0.20448352 0.20471269 0.20486404 0.20494267 0.20516397 0.20570186
 0.20634945 0.20676684 0.20678098 0.20665772 0.20662291 0.2068491
 0.20713499 0.20723861 0.20698856 0.20669335 0.20667423 0.20696715
 0.20724031 0.20714277 0.2068662  0.206732   0.2068493  0.20713045
 0.20745519 0.20750213 0.20739713 0.20733066 0.20737486 0.20764023
 0.2080263  0.20830896 0.2083339  0.20817941 0.20778584 0.20709324
 0.20628387 0.20571105 0.20548917 0.20539892 0.20521204 0.20494005
 0.20474079 0.20477904 0.20499007 0.20536606 0.2054878  0.20543888
 0.20539549 0.20544271 0.20561711 0.2058066  0.20591415 0.20594266
 0.20592366 0.20567843 0.20516445 0.20458563 0.20412217 0.20382696
 0.20362292 0.20322973 0.20253843 0.20197433 0.20202135 0.20271534
 0.2034718  0.20381464 0.20365566 0.2033653  0.2032513  0.20358022
 0.20397106 0.20405369 0.20385583 0.20365497 0.20345944 0.20336267
 0.20328414 0.20307727 0.20283765 0.20262602 0.20227277 0.2015588
 0.20068759 0.20016363 0.20017985 0.20043622 0.20044793 0.19999698
 0.19918214 0.19844268 0.19821508 0.19837488 0.19865097 0.19877423
 0.19868287 0.19843385 0.1982686  0.19844845 0.19896558 0.20000827
 0.20125611 0.20219314 0.20241821 0.20203769 0.20146868 0.2008645
 0.20039737 0.19989915 0.19916598 0.1984035  0.19796307 0.19819792
 0.19877361 0.19901316 0.19876552 0.19829822 0.19812216 0.19860427
 0.19936371 0.19984253 0.19995147 0.19988525 0.19982436 0.200013
 0.20039901 0.20081727 0.20122157 0.20165141 0.20196955 0.20177157
 0.20120113 0.20057932 0.20026292 0.20032603 0.20053448 0.20053594
 0.20018739 0.19978976 0.19948655 0.19951254 0.19970956 0.19987178
 0.19984457 0.19978976 0.19975126 0.2002186  0.20096143 0.2017494
 0.20237592 0.20248955 0.20203958 0.20153259 0.20126528 0.20120604
 0.20126183 0.20135057 0.20109817 0.20066468 0.20033176 0.2003215
 0.20044042 0.2003567  0.20007695 0.1996988  0.19946024 0.19949044
 0.19968969 0.1998067  0.19988382 0.20013067 0.2006341  0.20136584
 0.20186257 0.2018829  0.20161372 0.20145883 0.20143138 0.20106894
 0.20033218 0.19951463 0.1990415  0.19924982 0.19968165 0.19989009
 0.19969103 0.1995102  0.19983865 0.20065655 0.20133093 0.20141128
 0.2009135  0.2003785  0.2002444  0.200404   0.2007349  0.20088865
 0.20083755 0.20074996 0.20061721 0.20045942 0.20013009 0.19953795
 0.1989956  0.19887796 0.19884822 0.1987808  0.1986142  0.1985093
 0.1985458  0.19849883 0.19829422 0.19802395 0.19795635 0.19815747
 0.19847836 0.19865161 0.19871852 0.1989706  0.19961084 0.20048477
 0.2011225  0.20140485 0.20148009 0.20177865 0.20231628 0.20238769
 0.2016592  0.20050195 0.19949017 0.19913098 0.19943553 0.1998858
 0.19987726 0.19949463 0.19953607 0.20012161 0.20098689 0.20145035
 0.20133485 0.20106417 0.20140015 0.20191042 0.20265985 0.2027708
 0.20274572 0.20265076 0.2024942  0.20230968 0.20160519 0.20066473
 0.2000357  0.20012699 0.2004035  0.20034595 0.19970194 0.19905584
 0.1989549  0.1994373  0.20034024 0.2010826  0.20151713 0.20158999
 0.20147195 0.20128034 0.20103696 0.20075625 0.20041005 0.19992305
 0.19915475 0.19829771 0.19750094 0.1969993  0.19673558 0.19639672
 0.19580011 0.19509386 0.19445905 0.1941662  0.19414584 0.1941983
 0.19400175 0.1935549  0.19351536 0.193591   0.19374843 0.19361901
 0.1934327  0.19325498 0.19340082 0.19349475 0.19318771 0.19247869
 0.1919147  0.19200036 0.19221841 0.19226739 0.19165894 0.19051069
 0.18987462 0.19000904 0.19060475 0.1908341  0.19062622 0.1902041
 0.19003278 0.1898989  0.18989144 0.18997358 0.19022638 0.1905511
 0.19071884 0.19054876 0.19019365 0.18999879 0.1900161  0.18996793
 0.18949921 0.18845807 0.18722516 0.18648626 0.18630412 0.18600479
 0.18518429 0.18410963 0.18330583 0.18327734 0.18370828 0.18400697
 0.18374395 0.18314451 0.18291269 0.18321644 0.18365367 0.18375948
 0.18358622 0.18359151 0.18411268 0.18483217 0.1851772  0.18481374
 0.18406275 0.18349558 0.18314806 0.18292238 0.18246707 0.18177871
 0.18141158 0.18145023 0.18133445 0.1807723  0.1800553  0.17993307
 0.18051039 0.18099861 0.18099216 0.18049787 0.17994444 0.1797839
 0.17994541 0.18020298 0.1803924  0.18059053 0.18094493 0.1811717
 0.18064506 0.17939016 0.17813772 0.1780466  0.1786384  0.17873414
 0.1776245  0.17580564 0.17462072 0.17459543 0.17525887 0.17569919
 0.17545484 0.17485887 0.17482114 0.17536013 0.17585301 0.17570178
 0.17510328 0.17487615 0.175202   0.17560077 0.17515638 0.17388353
 0.17242008 0.17152742 0.17081174 0.16942558 0.16720833 0.16479151
 0.16392194 0.1645488  0.16542777 0.16518252 0.16443881 0.16434875
 0.16491143 0.16513187 0.16483667 0.1642992  0.16432513 0.16525279
 0.16642828 0.16701229 0.1668865  0.16642585 0.1659497  0.1653488
 0.16414379 0.16294093 0.16239439 0.1632612  0.1640429  0.16325136
 0.16146016 0.16026033 0.1608695  0.1623653  0.1630882  0.16218221
 0.16068517 0.15962948 0.15977184 0.15995854 0.15986493 0.16013288
 0.16067363 0.15900975 0.15349203 0.14962645 0.1552     0.15119708]
