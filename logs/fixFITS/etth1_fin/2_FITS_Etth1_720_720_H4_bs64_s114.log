Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32177152.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.372617959976196
Epoch: 1, Steps: 56 | Train Loss: 0.8280275 Vali Loss: 2.1137133 Test Loss: 0.8998619
Validation loss decreased (inf --> 2.113713).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.29268765449524
Epoch: 2, Steps: 56 | Train Loss: 0.6535785 Vali Loss: 1.8997207 Test Loss: 0.7786858
Validation loss decreased (2.113713 --> 1.899721).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.011998653411865
Epoch: 3, Steps: 56 | Train Loss: 0.5721780 Vali Loss: 1.8102229 Test Loss: 0.7250767
Validation loss decreased (1.899721 --> 1.810223).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.143367767333984
Epoch: 4, Steps: 56 | Train Loss: 0.5295996 Vali Loss: 1.7648144 Test Loss: 0.6980392
Validation loss decreased (1.810223 --> 1.764814).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.163656949996948
Epoch: 5, Steps: 56 | Train Loss: 0.5026474 Vali Loss: 1.7331071 Test Loss: 0.6802459
Validation loss decreased (1.764814 --> 1.733107).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.550800800323486
Epoch: 6, Steps: 56 | Train Loss: 0.4829790 Vali Loss: 1.7116276 Test Loss: 0.6669927
Validation loss decreased (1.733107 --> 1.711628).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.815807342529297
Epoch: 7, Steps: 56 | Train Loss: 0.4671975 Vali Loss: 1.6961119 Test Loss: 0.6554571
Validation loss decreased (1.711628 --> 1.696112).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.118919372558594
Epoch: 8, Steps: 56 | Train Loss: 0.4541799 Vali Loss: 1.6709167 Test Loss: 0.6443466
Validation loss decreased (1.696112 --> 1.670917).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.297738313674927
Epoch: 9, Steps: 56 | Train Loss: 0.4426746 Vali Loss: 1.6622190 Test Loss: 0.6348861
Validation loss decreased (1.670917 --> 1.662219).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.463662385940552
Epoch: 10, Steps: 56 | Train Loss: 0.4328792 Vali Loss: 1.6463583 Test Loss: 0.6258348
Validation loss decreased (1.662219 --> 1.646358).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.457628965377808
Epoch: 11, Steps: 56 | Train Loss: 0.4238068 Vali Loss: 1.6462746 Test Loss: 0.6175901
Validation loss decreased (1.646358 --> 1.646275).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.97395372390747
Epoch: 12, Steps: 56 | Train Loss: 0.4161172 Vali Loss: 1.6320362 Test Loss: 0.6098639
Validation loss decreased (1.646275 --> 1.632036).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 6.993596792221069
Epoch: 13, Steps: 56 | Train Loss: 0.4092874 Vali Loss: 1.6239792 Test Loss: 0.6025093
Validation loss decreased (1.632036 --> 1.623979).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 7.801050424575806
Epoch: 14, Steps: 56 | Train Loss: 0.4031139 Vali Loss: 1.6168802 Test Loss: 0.5960041
Validation loss decreased (1.623979 --> 1.616880).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 7.837043523788452
Epoch: 15, Steps: 56 | Train Loss: 0.3973731 Vali Loss: 1.6062171 Test Loss: 0.5908418
Validation loss decreased (1.616880 --> 1.606217).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.64674973487854
Epoch: 16, Steps: 56 | Train Loss: 0.3924236 Vali Loss: 1.6035471 Test Loss: 0.5851243
Validation loss decreased (1.606217 --> 1.603547).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.729587078094482
Epoch: 17, Steps: 56 | Train Loss: 0.3879384 Vali Loss: 1.5891223 Test Loss: 0.5797405
Validation loss decreased (1.603547 --> 1.589122).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.886478185653687
Epoch: 18, Steps: 56 | Train Loss: 0.3839020 Vali Loss: 1.5885919 Test Loss: 0.5749924
Validation loss decreased (1.589122 --> 1.588592).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.442179918289185
Epoch: 19, Steps: 56 | Train Loss: 0.3801163 Vali Loss: 1.5863316 Test Loss: 0.5703501
Validation loss decreased (1.588592 --> 1.586332).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.501187801361084
Epoch: 20, Steps: 56 | Train Loss: 0.3766125 Vali Loss: 1.5774763 Test Loss: 0.5660430
Validation loss decreased (1.586332 --> 1.577476).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.676487922668457
Epoch: 21, Steps: 56 | Train Loss: 0.3734993 Vali Loss: 1.5712876 Test Loss: 0.5622353
Validation loss decreased (1.577476 --> 1.571288).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.270375967025757
Epoch: 22, Steps: 56 | Train Loss: 0.3705111 Vali Loss: 1.5671046 Test Loss: 0.5586852
Validation loss decreased (1.571288 --> 1.567105).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.109596252441406
Epoch: 23, Steps: 56 | Train Loss: 0.3677927 Vali Loss: 1.5685265 Test Loss: 0.5553715
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.101292371749878
Epoch: 24, Steps: 56 | Train Loss: 0.3653530 Vali Loss: 1.5613661 Test Loss: 0.5520346
Validation loss decreased (1.567105 --> 1.561366).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.840470314025879
Epoch: 25, Steps: 56 | Train Loss: 0.3631593 Vali Loss: 1.5558212 Test Loss: 0.5487421
Validation loss decreased (1.561366 --> 1.555821).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 9.00887942314148
Epoch: 26, Steps: 56 | Train Loss: 0.3611209 Vali Loss: 1.5541745 Test Loss: 0.5463486
Validation loss decreased (1.555821 --> 1.554175).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.32755970954895
Epoch: 27, Steps: 56 | Train Loss: 0.3590254 Vali Loss: 1.5509350 Test Loss: 0.5437928
Validation loss decreased (1.554175 --> 1.550935).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 8.824060678482056
Epoch: 28, Steps: 56 | Train Loss: 0.3573539 Vali Loss: 1.5455608 Test Loss: 0.5412589
Validation loss decreased (1.550935 --> 1.545561).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.1589515209198
Epoch: 29, Steps: 56 | Train Loss: 0.3556374 Vali Loss: 1.5499144 Test Loss: 0.5387056
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.168244123458862
Epoch: 30, Steps: 56 | Train Loss: 0.3541382 Vali Loss: 1.5474248 Test Loss: 0.5366002
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 8.952031135559082
Epoch: 31, Steps: 56 | Train Loss: 0.3525414 Vali Loss: 1.5443883 Test Loss: 0.5347314
Validation loss decreased (1.545561 --> 1.544388).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 8.748871088027954
Epoch: 32, Steps: 56 | Train Loss: 0.3513847 Vali Loss: 1.5383776 Test Loss: 0.5326465
Validation loss decreased (1.544388 --> 1.538378).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 8.797821521759033
Epoch: 33, Steps: 56 | Train Loss: 0.3501217 Vali Loss: 1.5395555 Test Loss: 0.5308180
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 8.828893184661865
Epoch: 34, Steps: 56 | Train Loss: 0.3488259 Vali Loss: 1.5364103 Test Loss: 0.5291020
Validation loss decreased (1.538378 --> 1.536410).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 9.091001749038696
Epoch: 35, Steps: 56 | Train Loss: 0.3475422 Vali Loss: 1.5313312 Test Loss: 0.5275664
Validation loss decreased (1.536410 --> 1.531331).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.790403604507446
Epoch: 36, Steps: 56 | Train Loss: 0.3467208 Vali Loss: 1.5288492 Test Loss: 0.5262187
Validation loss decreased (1.531331 --> 1.528849).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 9.02491545677185
Epoch: 37, Steps: 56 | Train Loss: 0.3456660 Vali Loss: 1.5258864 Test Loss: 0.5247790
Validation loss decreased (1.528849 --> 1.525886).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 9.013457536697388
Epoch: 38, Steps: 56 | Train Loss: 0.3445718 Vali Loss: 1.5325261 Test Loss: 0.5233352
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 9.41378116607666
Epoch: 39, Steps: 56 | Train Loss: 0.3437117 Vali Loss: 1.5274563 Test Loss: 0.5221233
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 7.986373662948608
Epoch: 40, Steps: 56 | Train Loss: 0.3429238 Vali Loss: 1.5217844 Test Loss: 0.5208429
Validation loss decreased (1.525886 --> 1.521784).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 10.502843141555786
Epoch: 41, Steps: 56 | Train Loss: 0.3422749 Vali Loss: 1.5214760 Test Loss: 0.5197104
Validation loss decreased (1.521784 --> 1.521476).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 10.242388486862183
Epoch: 42, Steps: 56 | Train Loss: 0.3415028 Vali Loss: 1.5205454 Test Loss: 0.5185669
Validation loss decreased (1.521476 --> 1.520545).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 10.054610013961792
Epoch: 43, Steps: 56 | Train Loss: 0.3408314 Vali Loss: 1.5222907 Test Loss: 0.5175505
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 9.570762395858765
Epoch: 44, Steps: 56 | Train Loss: 0.3402344 Vali Loss: 1.5238353 Test Loss: 0.5166807
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 8.900328397750854
Epoch: 45, Steps: 56 | Train Loss: 0.3397701 Vali Loss: 1.5187798 Test Loss: 0.5157126
Validation loss decreased (1.520545 --> 1.518780).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.400685787200928
Epoch: 46, Steps: 56 | Train Loss: 0.3390755 Vali Loss: 1.5149329 Test Loss: 0.5148506
Validation loss decreased (1.518780 --> 1.514933).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 9.32608437538147
Epoch: 47, Steps: 56 | Train Loss: 0.3385200 Vali Loss: 1.5180809 Test Loss: 0.5140750
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 10.009160280227661
Epoch: 48, Steps: 56 | Train Loss: 0.3379161 Vali Loss: 1.5199864 Test Loss: 0.5133075
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 9.478653192520142
Epoch: 49, Steps: 56 | Train Loss: 0.3375331 Vali Loss: 1.5153575 Test Loss: 0.5125525
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32177152.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.366756916046143
Epoch: 1, Steps: 56 | Train Loss: 0.5910172 Vali Loss: 1.4822464 Test Loss: 0.4815297
Validation loss decreased (inf --> 1.482246).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.292389392852783
Epoch: 2, Steps: 56 | Train Loss: 0.5763666 Vali Loss: 1.4620506 Test Loss: 0.4611458
Validation loss decreased (1.482246 --> 1.462051).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.088069200515747
Epoch: 3, Steps: 56 | Train Loss: 0.5672122 Vali Loss: 1.4464447 Test Loss: 0.4491068
Validation loss decreased (1.462051 --> 1.446445).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.192933320999146
Epoch: 4, Steps: 56 | Train Loss: 0.5612134 Vali Loss: 1.4361122 Test Loss: 0.4421801
Validation loss decreased (1.446445 --> 1.436112).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.417839050292969
Epoch: 5, Steps: 56 | Train Loss: 0.5572471 Vali Loss: 1.4406934 Test Loss: 0.4383130
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.508743286132812
Epoch: 6, Steps: 56 | Train Loss: 0.5557560 Vali Loss: 1.4382347 Test Loss: 0.4364400
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.203213214874268
Epoch: 7, Steps: 56 | Train Loss: 0.5545743 Vali Loss: 1.4334614 Test Loss: 0.4356038
Validation loss decreased (1.436112 --> 1.433461).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.900957584381104
Epoch: 8, Steps: 56 | Train Loss: 0.5536589 Vali Loss: 1.4372886 Test Loss: 0.4353274
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.71917176246643
Epoch: 9, Steps: 56 | Train Loss: 0.5529981 Vali Loss: 1.4373960 Test Loss: 0.4354594
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.204808712005615
Epoch: 10, Steps: 56 | Train Loss: 0.5524225 Vali Loss: 1.4340093 Test Loss: 0.4354104
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43467387557029724, mae:0.45940330624580383, rse:0.6311516761779785, corr:[0.22175352 0.23336177 0.23421289 0.23068143 0.22925432 0.22982027
 0.23078796 0.23087148 0.22996344 0.22943254 0.22953351 0.2298644
 0.22999704 0.2296039  0.2288715  0.22815935 0.22763112 0.22730742
 0.2270899  0.2270183  0.22712131 0.22750764 0.22782846 0.22819261
 0.22865276 0.22922802 0.22964573 0.22954457 0.22907397 0.2284894
 0.22806646 0.2277017  0.2273479  0.22695896 0.22656433 0.2263615
 0.22647831 0.22661011 0.22661652 0.22652075 0.22657388 0.22662845
 0.22667465 0.22685339 0.22724536 0.22778772 0.22839521 0.22879098
 0.22859815 0.22810556 0.22736059 0.22661428 0.22601925 0.2251542
 0.22399515 0.2227379  0.22198562 0.22172605 0.22158769 0.22150674
 0.22137019 0.22108735 0.22071499 0.22057733 0.22064285 0.22080974
 0.22102289 0.22113705 0.22111113 0.2210142  0.22084254 0.22084326
 0.22079724 0.22069477 0.22046566 0.22021663 0.21997713 0.21966702
 0.21938702 0.21915643 0.21881652 0.21834415 0.21774435 0.21717119
 0.2168227  0.21663456 0.21650927 0.21641612 0.21627437 0.21609849
 0.21581875 0.21566531 0.21569417 0.21600774 0.21665448 0.21767126
 0.218869   0.21992984 0.22065812 0.22122243 0.22155125 0.22154331
 0.22118817 0.22066085 0.22019869 0.21984048 0.21952209 0.2192052
 0.21901113 0.21885192 0.21866621 0.21853395 0.21845914 0.21834429
 0.21830599 0.21838212 0.21856882 0.21877895 0.2190156  0.21934088
 0.21963747 0.21965955 0.21931222 0.21889131 0.21838114 0.21781094
 0.21726397 0.21688032 0.21649164 0.2161465  0.21587478 0.21567707
 0.21556741 0.21537979 0.21510623 0.2148345  0.21466638 0.2145785
 0.21455814 0.21459116 0.2147087  0.21484359 0.2149548  0.21508682
 0.21508867 0.21479888 0.21432354 0.21373531 0.2132495  0.21280593
 0.21239212 0.21223034 0.21227929 0.2123224  0.21216594 0.2118125
 0.21137616 0.21104686 0.21108021 0.21143514 0.21171352 0.21181048
 0.21167232 0.21158513 0.2116882  0.21190718 0.21191075 0.21193266
 0.21213984 0.21254134 0.21293789 0.21331981 0.21344502 0.21330003
 0.21307388 0.21297942 0.2128196  0.212535   0.21213439 0.2117946
 0.21165548 0.21167904 0.21174227 0.21178484 0.21188681 0.21211965
 0.21245909 0.21282259 0.21314731 0.2131946  0.21303803 0.21285585
 0.212677   0.21240002 0.21195202 0.21148181 0.21113215 0.2108062
 0.21033666 0.20986089 0.2095091  0.20943236 0.20966023 0.21003628
 0.21021205 0.21012121 0.20986189 0.20962548 0.20958486 0.2096716
 0.20973247 0.20968957 0.20971161 0.20973404 0.20969006 0.20959271
 0.209393   0.2091853  0.20901531 0.20893827 0.20883375 0.20843649
 0.20791143 0.20756961 0.2075589  0.20771894 0.2077124  0.20750497
 0.20724747 0.20705032 0.20701228 0.20703381 0.20685987 0.2065101
 0.20618626 0.20608358 0.20630267 0.20671457 0.20721377 0.20758168
 0.20780413 0.20780183 0.20764571 0.20748943 0.20737489 0.20733163
 0.20744589 0.20757188 0.20748687 0.20714778 0.20675138 0.20646879
 0.20627496 0.20590907 0.20545395 0.20510422 0.205099   0.20542692
 0.20586091 0.20608872 0.2062801  0.2063933  0.20637973 0.20635524
 0.20643581 0.20666361 0.20700613 0.20727722 0.20708787 0.20645002
 0.20573276 0.2052893  0.20511077 0.20519397 0.2052438  0.20511739
 0.20486629 0.20469415 0.20450939 0.20424287 0.20384586 0.20351052
 0.20337483 0.20369776 0.2042907  0.20471159 0.20488216 0.2050143
 0.20528291 0.20591588 0.20682186 0.20764297 0.20790653 0.20763193
 0.20713337 0.20680445 0.20667185 0.20667648 0.20672768 0.20684916
 0.2070632  0.20724107 0.20737302 0.20736729 0.20714194 0.2068776
 0.20691548 0.20718978 0.20761083 0.20798555 0.20808077 0.20800288
 0.2078489  0.20763785 0.20732138 0.20702703 0.20677133 0.20646535
 0.20605749 0.20563556 0.20525682 0.20493664 0.20466201 0.20448095
 0.20439267 0.20442861 0.20456514 0.20495836 0.2052321  0.20538072
 0.20544526 0.20551011 0.20570573 0.20592318 0.2059475  0.20560789
 0.20498323 0.20420064 0.20356134 0.20330587 0.20326152 0.20309769
 0.20271383 0.20218271 0.2017078  0.20162265 0.20200565 0.20265636
 0.2031984  0.20356017 0.20378755 0.20395553 0.20392908 0.20384897
 0.20364839 0.20337078 0.20325099 0.20341247 0.20353068 0.20352848
 0.20332398 0.20282443 0.2021716  0.20157795 0.20113817 0.20078246
 0.20050447 0.2003602  0.20025216 0.20005342 0.19971399 0.1993165
 0.19887124 0.19840167 0.19799058 0.19759174 0.1973448  0.1973158
 0.1974472  0.19749233 0.19738708 0.1973151  0.19741067 0.19807647
 0.19910367 0.19990961 0.20008864 0.19984853 0.19968824 0.1997159
 0.19988118 0.1998434  0.1993159  0.19845022 0.19760522 0.19730562
 0.19754043 0.19792986 0.19835399 0.19878526 0.19923982 0.19972059
 0.19992864 0.1997118  0.1993725  0.19924906 0.19936624 0.19969846
 0.20000972 0.20020247 0.20048189 0.20106466 0.20176598 0.2019922
 0.20167218 0.20101589 0.20037301 0.19996268 0.19981638 0.19977278
 0.19971146 0.19971967 0.19962955 0.1994584  0.1991396  0.19881602
 0.1986717  0.19886166 0.19912606 0.19963261 0.20000806 0.20020294
 0.20046985 0.2008356  0.20113683 0.20129776 0.20115693 0.20078905
 0.2005859  0.20088932 0.20126933 0.20136309 0.20098715 0.20033987
 0.19971187 0.19932371 0.19928381 0.19940233 0.19947673 0.19949505
 0.19952175 0.19952907 0.19961263 0.19984603 0.20014139 0.20052381
 0.20071658 0.20069644 0.200677   0.20091414 0.20126562 0.20123823
 0.20073867 0.20000112 0.19940932 0.19939922 0.19972238 0.19996965
 0.19981946 0.19946378 0.1993633  0.19977497 0.20043547 0.20096868
 0.20107564 0.20084    0.20049807 0.20016876 0.20017044 0.20039596
 0.2006781  0.20083204 0.2005982  0.20002729 0.19928522 0.19865055
 0.19854753 0.1990967  0.19955929 0.19942792 0.19862318 0.1977081
 0.19731937 0.19750386 0.19795114 0.19821456 0.19817762 0.19797648
 0.1978255  0.19773681 0.197715   0.19785298 0.19828916 0.19904706
 0.19986974 0.20063211 0.20118618 0.2016635  0.20210642 0.2021499
 0.20167881 0.20092763 0.20015547 0.19975711 0.19995573 0.20048825
 0.2007108  0.20034379 0.19986843 0.19967242 0.19999038 0.20047018
 0.2007184  0.20065042 0.20070703 0.20078796 0.20145921 0.2020905
 0.20278482 0.2030661  0.20275661 0.20223697 0.20150831 0.20087613
 0.2004217  0.2001558  0.1996232  0.19878912 0.19784997 0.19731592
 0.19735645 0.19773269 0.19831482 0.198824   0.19939172 0.2000056
 0.20058131 0.20092733 0.20087296 0.20044033 0.19973451 0.19883275
 0.19769989 0.19660413 0.19580434 0.19556849 0.19577877 0.19590399
 0.19543847 0.19434099 0.19299822 0.19216    0.19214529 0.19265167
 0.19284502 0.19222398 0.19142658 0.19082937 0.1909291  0.19135295
 0.19164714 0.19133823 0.19086176 0.1905002  0.19043677 0.19051145
 0.19063534 0.19092652 0.19116594 0.19150515 0.19153291 0.19087236
 0.18996893 0.1892098  0.18885814 0.18864176 0.18839343 0.18784975
 0.18721569 0.18648279 0.18607686 0.18599224 0.18609929 0.18618408
 0.1863452  0.18687333 0.18787673 0.18900356 0.18956836 0.18919332
 0.1880287  0.18658625 0.18550079 0.18515553 0.18537323 0.18560712
 0.18559225 0.18525134 0.18441285 0.18327893 0.18221278 0.18173212
 0.18199153 0.18261772 0.18307276 0.18288681 0.18224032 0.18176156
 0.18193285 0.18262121 0.18327469 0.1833484  0.18293172 0.18249772
 0.18245941 0.1827533  0.18274248 0.18219715 0.18120745 0.18024883
 0.17993282 0.18019079 0.18031569 0.17994867 0.17930013 0.17897943
 0.17908481 0.17922458 0.17936987 0.17954876 0.17974666 0.17985757
 0.17966554 0.17936128 0.17925408 0.17949827 0.17994528 0.18005618
 0.17936222 0.17825393 0.1774761  0.17774974 0.17836046 0.17832792
 0.17732102 0.17586961 0.17481221 0.17419405 0.17370394 0.17303763
 0.1722455  0.17157684 0.17150135 0.17186093 0.17235075 0.17263375
 0.17270075 0.172769   0.1727614  0.17267832 0.17245013 0.1723223
 0.17215984 0.17168978 0.17040193 0.16834061 0.16620538 0.16441815
 0.16371584 0.163608   0.1634193  0.16274363 0.16248363 0.16311224
 0.16382489 0.16357237 0.16281211 0.16234851 0.16272587 0.16344932
 0.16350228 0.16275854 0.16220975 0.16268405 0.16402966 0.16516829
 0.16489676 0.16384755 0.16296212 0.16319819 0.16344857 0.16263643
 0.16130441 0.1604977  0.1606348  0.16065031 0.15955955 0.15750003
 0.15600067 0.15575302 0.15667559 0.15711716 0.1570286  0.15795252
 0.16053519 0.16211902 0.1594739  0.15500885 0.15771669 0.16717914]
