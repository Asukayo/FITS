Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  71258880.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.555922269821167
Epoch: 1, Steps: 29 | Train Loss: 0.8298163 Vali Loss: 1.6566701 Test Loss: 0.7426724
Validation loss decreased (inf --> 1.656670).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.426260471343994
Epoch: 2, Steps: 29 | Train Loss: 0.6632565 Vali Loss: 1.4545162 Test Loss: 0.6250686
Validation loss decreased (1.656670 --> 1.454516).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.3476245403289795
Epoch: 3, Steps: 29 | Train Loss: 0.5958687 Vali Loss: 1.3772122 Test Loss: 0.5735596
Validation loss decreased (1.454516 --> 1.377212).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.8719801902771
Epoch: 4, Steps: 29 | Train Loss: 0.5609644 Vali Loss: 1.3223928 Test Loss: 0.5425351
Validation loss decreased (1.377212 --> 1.322393).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.473233461380005
Epoch: 5, Steps: 29 | Train Loss: 0.5377226 Vali Loss: 1.2934089 Test Loss: 0.5202516
Validation loss decreased (1.322393 --> 1.293409).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.645688056945801
Epoch: 6, Steps: 29 | Train Loss: 0.5213706 Vali Loss: 1.2661667 Test Loss: 0.5031801
Validation loss decreased (1.293409 --> 1.266167).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.154031753540039
Epoch: 7, Steps: 29 | Train Loss: 0.5085232 Vali Loss: 1.2497829 Test Loss: 0.4899945
Validation loss decreased (1.266167 --> 1.249783).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.944270849227905
Epoch: 8, Steps: 29 | Train Loss: 0.4984562 Vali Loss: 1.2351227 Test Loss: 0.4793529
Validation loss decreased (1.249783 --> 1.235123).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.743709564208984
Epoch: 9, Steps: 29 | Train Loss: 0.4902692 Vali Loss: 1.2222009 Test Loss: 0.4709977
Validation loss decreased (1.235123 --> 1.222201).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.0381360054016113
Epoch: 10, Steps: 29 | Train Loss: 0.4839723 Vali Loss: 1.2142007 Test Loss: 0.4642422
Validation loss decreased (1.222201 --> 1.214201).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.682349920272827
Epoch: 11, Steps: 29 | Train Loss: 0.4779338 Vali Loss: 1.2023764 Test Loss: 0.4588364
Validation loss decreased (1.214201 --> 1.202376).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.4852981567382812
Epoch: 12, Steps: 29 | Train Loss: 0.4737707 Vali Loss: 1.1984094 Test Loss: 0.4544137
Validation loss decreased (1.202376 --> 1.198409).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.633960723876953
Epoch: 13, Steps: 29 | Train Loss: 0.4697892 Vali Loss: 1.1853462 Test Loss: 0.4510126
Validation loss decreased (1.198409 --> 1.185346).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.945066452026367
Epoch: 14, Steps: 29 | Train Loss: 0.4668200 Vali Loss: 1.1853577 Test Loss: 0.4482982
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.106879949569702
Epoch: 15, Steps: 29 | Train Loss: 0.4634999 Vali Loss: 1.1874102 Test Loss: 0.4460155
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.925775527954102
Epoch: 16, Steps: 29 | Train Loss: 0.4614777 Vali Loss: 1.1885641 Test Loss: 0.4441629
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.449260413646698, mae:0.4525158107280731, rse:0.6381181478500366, corr:[0.24290538 0.25897366 0.25100666 0.24714616 0.25112757 0.25232214
 0.25102735 0.25016862 0.25098404 0.2513183  0.25081435 0.25036728
 0.25016537 0.24935688 0.24837862 0.24813251 0.2484205  0.24757062
 0.24561004 0.24439713 0.24493103 0.24525775 0.24404542 0.24198534
 0.24133793 0.24231052 0.24322967 0.24329492 0.24306324 0.24336551
 0.24404965 0.24464788 0.24505872 0.24508253 0.24479568 0.24450952
 0.24463941 0.24494472 0.24487594 0.24435824 0.24392633 0.24409297
 0.24455243 0.24467376 0.24433552 0.2443849  0.24515201 0.24566178
 0.24544969 0.24476308 0.24422292 0.24406788 0.24371038 0.24293171
 0.24207763 0.2414491  0.2411356  0.24099135 0.24097307 0.24085076
 0.24040236 0.23985408 0.23963943 0.2398746  0.24007662 0.24004306
 0.23996072 0.24003728 0.24057902 0.2408837  0.24048153 0.23962705
 0.23907879 0.23890166 0.23877107 0.23852584 0.23801436 0.23752445
 0.23733217 0.2374586  0.23743297 0.23704298 0.23660047 0.23668653
 0.23692317 0.23665594 0.23568368 0.23481664 0.23470515 0.23504491
 0.2349334  0.23411457 0.23314409 0.23272844 0.23278181 0.23313046
 0.23357272 0.23415978 0.23492467 0.23570357 0.23614994 0.2361141
 0.23586103 0.23567064 0.23569793 0.23581332 0.2356842  0.2352833
 0.2348261  0.23485382 0.23539826 0.23582384 0.23544492 0.23469362
 0.23442563 0.23473114 0.23489764 0.23423558 0.2331424  0.23276424
 0.23329228 0.23356135 0.23300228 0.23218472 0.23154344 0.23114453
 0.23110795 0.23135303 0.23138417 0.23081033 0.22991882 0.22954378
 0.22995768 0.23033214 0.23003374 0.22948404 0.22938746 0.22982359
 0.2297754  0.22889037 0.22796024 0.22766495 0.2276988  0.22759317
 0.22720154 0.22682571 0.22652552 0.22605485 0.22540258 0.22480088
 0.22458307 0.22463916 0.22477385 0.22478873 0.22454679 0.22412445
 0.22345302 0.22311483 0.22342075 0.22406942 0.22415286 0.22369088
 0.22342983 0.22381178 0.22435072 0.22421929 0.22318216 0.22241008
 0.22259647 0.22320342 0.22325343 0.22291498 0.22265649 0.22262432
 0.22273462 0.22297527 0.22295853 0.22260176 0.22211492 0.22201541
 0.22218937 0.22213377 0.22163892 0.2214095  0.2222132  0.22359851
 0.22418505 0.2235103  0.22264569 0.22273263 0.2234798  0.22365478
 0.22282413 0.22186656 0.22150496 0.2214082  0.22109209 0.22045156
 0.21986853 0.21971853 0.22000913 0.22060384 0.2210449  0.22111124
 0.22072527 0.22069646 0.2213783  0.22226787 0.22240488 0.22152978
 0.22072406 0.22078246 0.22106297 0.22043768 0.2190046  0.21832836
 0.21899313 0.21980165 0.21957548 0.21896718 0.21903558 0.2194156
 0.2195566  0.21954936 0.21961452 0.21959788 0.2190724  0.21874414
 0.21896423 0.21942277 0.21939397 0.21879588 0.21831207 0.21877432
 0.21941097 0.219048   0.21792571 0.21747118 0.21827883 0.21898665
 0.21879634 0.2182889  0.21860881 0.21939914 0.21965028 0.2193425
 0.21918519 0.21942835 0.21951291 0.21928872 0.21919765 0.21943828
 0.21942061 0.21886016 0.21872541 0.21930225 0.21988095 0.21961425
 0.21910909 0.21927159 0.22004183 0.2200219  0.2189879  0.21804518
 0.21836162 0.2192901  0.21938856 0.2186996  0.21791555 0.21747126
 0.21741712 0.21744339 0.21754014 0.21775745 0.21749982 0.21709552
 0.2173817  0.21826527 0.21828578 0.21724123 0.21656159 0.21745767
 0.21855347 0.21794818 0.21628615 0.21552972 0.21643575 0.21719894
 0.21654132 0.21570453 0.21591944 0.216804   0.21676609 0.21597701
 0.21557185 0.2159313  0.21620625 0.21624759 0.21646473 0.21692386
 0.21685144 0.21597345 0.21578048 0.21675375 0.21775894 0.2174205
 0.2168184  0.21705678 0.2180784  0.21759792 0.21551897 0.2142464
 0.21515004 0.21585806 0.21422817 0.21173552 0.21092346 0.21113713
 0.21051693 0.20880802 0.20793383 0.20836014 0.20783868 0.20594373
 0.20508157 0.2055241  0.20460409 0.20167059 0.19958112 0.20110795
 0.20159267 0.19453923 0.18436272 0.18397692 0.1895874  0.16472718]
