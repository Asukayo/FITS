Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  40581632.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.238986253738403
Epoch: 1, Steps: 30 | Train Loss: 0.7523162 Vali Loss: 1.4618320 Test Loss: 0.7423153
Validation loss decreased (inf --> 1.461832).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.882188558578491
Epoch: 2, Steps: 30 | Train Loss: 0.5816732 Vali Loss: 1.2521868 Test Loss: 0.6127411
Validation loss decreased (1.461832 --> 1.252187).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.108856439590454
Epoch: 3, Steps: 30 | Train Loss: 0.5117916 Vali Loss: 1.1647629 Test Loss: 0.5511965
Validation loss decreased (1.252187 --> 1.164763).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.3066112995147705
Epoch: 4, Steps: 30 | Train Loss: 0.4759805 Vali Loss: 1.1082268 Test Loss: 0.5128270
Validation loss decreased (1.164763 --> 1.108227).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.648277997970581
Epoch: 5, Steps: 30 | Train Loss: 0.4533067 Vali Loss: 1.0748663 Test Loss: 0.4866405
Validation loss decreased (1.108227 --> 1.074866).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.557956218719482
Epoch: 6, Steps: 30 | Train Loss: 0.4375733 Vali Loss: 1.0474169 Test Loss: 0.4680465
Validation loss decreased (1.074866 --> 1.047417).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.580867052078247
Epoch: 7, Steps: 30 | Train Loss: 0.4264761 Vali Loss: 1.0317208 Test Loss: 0.4548275
Validation loss decreased (1.047417 --> 1.031721).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.725128173828125
Epoch: 8, Steps: 30 | Train Loss: 0.4181926 Vali Loss: 1.0144212 Test Loss: 0.4453760
Validation loss decreased (1.031721 --> 1.014421).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.915964603424072
Epoch: 9, Steps: 30 | Train Loss: 0.4116894 Vali Loss: 1.0034627 Test Loss: 0.4388454
Validation loss decreased (1.014421 --> 1.003463).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.3006370067596436
Epoch: 10, Steps: 30 | Train Loss: 0.4076173 Vali Loss: 0.9978848 Test Loss: 0.4341375
Validation loss decreased (1.003463 --> 0.997885).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.773624897003174
Epoch: 11, Steps: 30 | Train Loss: 0.4041690 Vali Loss: 0.9889100 Test Loss: 0.4306877
Validation loss decreased (0.997885 --> 0.988910).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.395614862442017
Epoch: 12, Steps: 30 | Train Loss: 0.4017882 Vali Loss: 0.9847723 Test Loss: 0.4284011
Validation loss decreased (0.988910 --> 0.984772).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.380275011062622
Epoch: 13, Steps: 30 | Train Loss: 0.3993882 Vali Loss: 0.9851492 Test Loss: 0.4265859
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.691969394683838
Epoch: 14, Steps: 30 | Train Loss: 0.3980743 Vali Loss: 0.9790192 Test Loss: 0.4253707
Validation loss decreased (0.984772 --> 0.979019).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.993470668792725
Epoch: 15, Steps: 30 | Train Loss: 0.3965084 Vali Loss: 0.9732996 Test Loss: 0.4245425
Validation loss decreased (0.979019 --> 0.973300).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.988648414611816
Epoch: 16, Steps: 30 | Train Loss: 0.3951134 Vali Loss: 0.9784559 Test Loss: 0.4237391
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 6.220951080322266
Epoch: 17, Steps: 30 | Train Loss: 0.3947344 Vali Loss: 0.9768783 Test Loss: 0.4233079
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 6.110302209854126
Epoch: 18, Steps: 30 | Train Loss: 0.3933577 Vali Loss: 0.9776682 Test Loss: 0.4229252
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4183678925037384, mae:0.4277559816837311, rse:0.6142377257347107, corr:[0.25523785 0.27146798 0.27035406 0.2644342  0.2627131  0.26373473
 0.2645911  0.2644972  0.26356906 0.26255855 0.2615488  0.26071218
 0.26036224 0.26044536 0.26077685 0.2606885  0.25987673 0.25885698
 0.2582768  0.25842318 0.2587746  0.25851914 0.2577125  0.25686932
 0.25665903 0.257187   0.25788164 0.25821847 0.2580477  0.25753474
 0.2570685  0.2568708  0.25709468 0.25743294 0.25750566 0.25721917
 0.25672632 0.2563383  0.2563321  0.25681382 0.2575101  0.25791717
 0.25783956 0.25763997 0.2576102  0.2579358  0.25844592 0.2586977
 0.25846115 0.25781247 0.2568075  0.25572908 0.25480157 0.25404525
 0.25350735 0.253071   0.25261343 0.25218105 0.25192934 0.2519936
 0.25224927 0.2523478  0.25215432 0.25192046 0.25185713 0.2521507
 0.2526669  0.25299862 0.25311148 0.2529788  0.25265017 0.25222108
 0.25178647 0.25132656 0.2509062  0.25062558 0.25028706 0.2497473
 0.24914001 0.24875654 0.24870893 0.24874799 0.24857217 0.2481332
 0.24766248 0.24738246 0.24737564 0.24760202 0.24778073 0.24770404
 0.2473257  0.24689835 0.24659586 0.24659786 0.2468524  0.24722211
 0.24764635 0.24798284 0.24809392 0.24801517 0.24789539 0.24784978
 0.24789204 0.24783254 0.24753757 0.24710795 0.24674709 0.2466303
 0.24679828 0.24705921 0.24726033 0.24740127 0.24738714 0.24738388
 0.2475244  0.24767968 0.24772395 0.24763186 0.24745221 0.24726638
 0.2469761  0.24629134 0.24527621 0.2442882  0.24344896 0.24269567
 0.2421615  0.24188754 0.24175182 0.24170971 0.2415489  0.2411838
 0.24067663 0.2402652  0.24016449 0.24042852 0.24095394 0.24132437
 0.24124467 0.24076237 0.2402927  0.24009603 0.24006927 0.23981968
 0.23915707 0.2380615  0.23676072 0.23563175 0.23510477 0.23490968
 0.23467286 0.23416689 0.23357    0.23329401 0.23338899 0.23373675
 0.23398988 0.23396929 0.23371255 0.23330455 0.23282993 0.23274226
 0.23298706 0.23334554 0.23353563 0.23306501 0.23187605 0.23042722
 0.22955865 0.22960378 0.22978865 0.22931026 0.22789834 0.22631608
 0.22554529 0.22590107 0.22632968 0.22637187 0.22594167 0.22558933
 0.22538966 0.22472188 0.22356874 0.22290374 0.22333455 0.22368132
 0.22135237 0.21589406 0.21168607 0.21299985 0.21437456 0.19470474]
