Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  81163264.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.5639514923095703
Epoch: 1, Steps: 15 | Train Loss: 0.8136233 Vali Loss: 1.6628749 Test Loss: 0.8861110
Validation loss decreased (inf --> 1.662875).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.1150848865509033
Epoch: 2, Steps: 15 | Train Loss: 0.6818260 Vali Loss: 1.4448000 Test Loss: 0.7609801
Validation loss decreased (1.662875 --> 1.444800).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.0260486602783203
Epoch: 3, Steps: 15 | Train Loss: 0.6024696 Vali Loss: 1.3268683 Test Loss: 0.6851582
Validation loss decreased (1.444800 --> 1.326868).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.9693548679351807
Epoch: 4, Steps: 15 | Train Loss: 0.5549599 Vali Loss: 1.2553885 Test Loss: 0.6372984
Validation loss decreased (1.326868 --> 1.255388).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.948528528213501
Epoch: 5, Steps: 15 | Train Loss: 0.5251330 Vali Loss: 1.2055677 Test Loss: 0.6042270
Validation loss decreased (1.255388 --> 1.205568).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.1084539890289307
Epoch: 6, Steps: 15 | Train Loss: 0.5038628 Vali Loss: 1.1731086 Test Loss: 0.5792108
Validation loss decreased (1.205568 --> 1.173109).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1585590839385986
Epoch: 7, Steps: 15 | Train Loss: 0.4876849 Vali Loss: 1.1457825 Test Loss: 0.5593020
Validation loss decreased (1.173109 --> 1.145782).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.270235538482666
Epoch: 8, Steps: 15 | Train Loss: 0.4749165 Vali Loss: 1.1242421 Test Loss: 0.5429609
Validation loss decreased (1.145782 --> 1.124242).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.782060146331787
Epoch: 9, Steps: 15 | Train Loss: 0.4645999 Vali Loss: 1.1042039 Test Loss: 0.5292466
Validation loss decreased (1.124242 --> 1.104204).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.643545150756836
Epoch: 10, Steps: 15 | Train Loss: 0.4563370 Vali Loss: 1.0949562 Test Loss: 0.5176367
Validation loss decreased (1.104204 --> 1.094956).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.6903345584869385
Epoch: 11, Steps: 15 | Train Loss: 0.4491828 Vali Loss: 1.0785223 Test Loss: 0.5077326
Validation loss decreased (1.094956 --> 1.078522).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.733804941177368
Epoch: 12, Steps: 15 | Train Loss: 0.4429704 Vali Loss: 1.0666342 Test Loss: 0.4991566
Validation loss decreased (1.078522 --> 1.066634).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.220085620880127
Epoch: 13, Steps: 15 | Train Loss: 0.4377984 Vali Loss: 1.0579933 Test Loss: 0.4917576
Validation loss decreased (1.066634 --> 1.057993).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.4452950954437256
Epoch: 14, Steps: 15 | Train Loss: 0.4334099 Vali Loss: 1.0413516 Test Loss: 0.4854962
Validation loss decreased (1.057993 --> 1.041352).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.241147041320801
Epoch: 15, Steps: 15 | Train Loss: 0.4292427 Vali Loss: 1.0335070 Test Loss: 0.4800725
Validation loss decreased (1.041352 --> 1.033507).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.1734023094177246
Epoch: 16, Steps: 15 | Train Loss: 0.4261262 Vali Loss: 1.0293654 Test Loss: 0.4752478
Validation loss decreased (1.033507 --> 1.029365).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.2857284545898438
Epoch: 17, Steps: 15 | Train Loss: 0.4230981 Vali Loss: 1.0250776 Test Loss: 0.4711231
Validation loss decreased (1.029365 --> 1.025078).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.984811305999756
Epoch: 18, Steps: 15 | Train Loss: 0.4206334 Vali Loss: 1.0238947 Test Loss: 0.4675071
Validation loss decreased (1.025078 --> 1.023895).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.109113931655884
Epoch: 19, Steps: 15 | Train Loss: 0.4180441 Vali Loss: 1.0215356 Test Loss: 0.4642380
Validation loss decreased (1.023895 --> 1.021536).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.2428154945373535
Epoch: 20, Steps: 15 | Train Loss: 0.4163312 Vali Loss: 1.0136821 Test Loss: 0.4614284
Validation loss decreased (1.021536 --> 1.013682).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.1629133224487305
Epoch: 21, Steps: 15 | Train Loss: 0.4142895 Vali Loss: 1.0126243 Test Loss: 0.4588903
Validation loss decreased (1.013682 --> 1.012624).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.4735944271087646
Epoch: 22, Steps: 15 | Train Loss: 0.4125386 Vali Loss: 1.0092943 Test Loss: 0.4567551
Validation loss decreased (1.012624 --> 1.009294).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.2913031578063965
Epoch: 23, Steps: 15 | Train Loss: 0.4110945 Vali Loss: 1.0072846 Test Loss: 0.4547673
Validation loss decreased (1.009294 --> 1.007285).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.361982583999634
Epoch: 24, Steps: 15 | Train Loss: 0.4098346 Vali Loss: 1.0049615 Test Loss: 0.4530374
Validation loss decreased (1.007285 --> 1.004961).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.1520721912384033
Epoch: 25, Steps: 15 | Train Loss: 0.4086048 Vali Loss: 1.0003421 Test Loss: 0.4514843
Validation loss decreased (1.004961 --> 1.000342).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.2399909496307373
Epoch: 26, Steps: 15 | Train Loss: 0.4078084 Vali Loss: 0.9990845 Test Loss: 0.4501503
Validation loss decreased (1.000342 --> 0.999084).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.5101206302642822
Epoch: 27, Steps: 15 | Train Loss: 0.4068188 Vali Loss: 0.9979266 Test Loss: 0.4488779
Validation loss decreased (0.999084 --> 0.997927).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.445368766784668
Epoch: 28, Steps: 15 | Train Loss: 0.4056718 Vali Loss: 0.9964584 Test Loss: 0.4478229
Validation loss decreased (0.997927 --> 0.996458).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.5905025005340576
Epoch: 29, Steps: 15 | Train Loss: 0.4051618 Vali Loss: 0.9948810 Test Loss: 0.4467549
Validation loss decreased (0.996458 --> 0.994881).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.63835072517395
Epoch: 30, Steps: 15 | Train Loss: 0.4043902 Vali Loss: 0.9924154 Test Loss: 0.4458380
Validation loss decreased (0.994881 --> 0.992415).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.488436222076416
Epoch: 31, Steps: 15 | Train Loss: 0.4036602 Vali Loss: 0.9897950 Test Loss: 0.4450271
Validation loss decreased (0.992415 --> 0.989795).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.4837646484375
Epoch: 32, Steps: 15 | Train Loss: 0.4031458 Vali Loss: 0.9913828 Test Loss: 0.4442924
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.6553826332092285
Epoch: 33, Steps: 15 | Train Loss: 0.4025775 Vali Loss: 0.9936296 Test Loss: 0.4436150
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.6475939750671387
Epoch: 34, Steps: 15 | Train Loss: 0.4022153 Vali Loss: 0.9877680 Test Loss: 0.4430019
Validation loss decreased (0.989795 --> 0.987768).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.7034764289855957
Epoch: 35, Steps: 15 | Train Loss: 0.4016878 Vali Loss: 0.9857947 Test Loss: 0.4424173
Validation loss decreased (0.987768 --> 0.985795).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.6235058307647705
Epoch: 36, Steps: 15 | Train Loss: 0.4012531 Vali Loss: 0.9882323 Test Loss: 0.4419076
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.5364866256713867
Epoch: 37, Steps: 15 | Train Loss: 0.4009340 Vali Loss: 0.9850097 Test Loss: 0.4414571
Validation loss decreased (0.985795 --> 0.985010).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.49819016456604
Epoch: 38, Steps: 15 | Train Loss: 0.4003797 Vali Loss: 0.9879831 Test Loss: 0.4410085
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.215545415878296
Epoch: 39, Steps: 15 | Train Loss: 0.4002948 Vali Loss: 0.9854596 Test Loss: 0.4406042
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.336359977722168
Epoch: 40, Steps: 15 | Train Loss: 0.3993201 Vali Loss: 0.9835121 Test Loss: 0.4402618
Validation loss decreased (0.985010 --> 0.983512).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.315500259399414
Epoch: 41, Steps: 15 | Train Loss: 0.3996098 Vali Loss: 0.9856447 Test Loss: 0.4399220
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.240133762359619
Epoch: 42, Steps: 15 | Train Loss: 0.3990931 Vali Loss: 0.9825815 Test Loss: 0.4396113
Validation loss decreased (0.983512 --> 0.982581).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.2826619148254395
Epoch: 43, Steps: 15 | Train Loss: 0.3991266 Vali Loss: 0.9822067 Test Loss: 0.4393159
Validation loss decreased (0.982581 --> 0.982207).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.2278361320495605
Epoch: 44, Steps: 15 | Train Loss: 0.3986832 Vali Loss: 0.9815171 Test Loss: 0.4390439
Validation loss decreased (0.982207 --> 0.981517).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.2510769367218018
Epoch: 45, Steps: 15 | Train Loss: 0.3984312 Vali Loss: 0.9840482 Test Loss: 0.4387922
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.272174119949341
Epoch: 46, Steps: 15 | Train Loss: 0.3982362 Vali Loss: 0.9772971 Test Loss: 0.4385926
Validation loss decreased (0.981517 --> 0.977297).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.1620371341705322
Epoch: 47, Steps: 15 | Train Loss: 0.3980353 Vali Loss: 0.9777647 Test Loss: 0.4383686
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.090481996536255
Epoch: 48, Steps: 15 | Train Loss: 0.3977668 Vali Loss: 0.9783715 Test Loss: 0.4381631
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.268498182296753
Epoch: 49, Steps: 15 | Train Loss: 0.3976165 Vali Loss: 0.9823529 Test Loss: 0.4379684
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4208117127418518, mae:0.4296713173389435, rse:0.6160291433334351, corr:[0.25398615 0.27135846 0.26990166 0.26370725 0.2619995  0.26320723
 0.26420492 0.2641926  0.26329884 0.26229993 0.26128048 0.2603885
 0.25998175 0.26006663 0.26041856 0.2602757  0.25931227 0.25809452
 0.25740355 0.2575898  0.25801784 0.25773928 0.25682572 0.25591084
 0.2557736  0.25642896 0.2571464  0.2574069  0.2571379  0.25658232
 0.25617647 0.25606883 0.25633505 0.25664446 0.25663838 0.25629643
 0.2557954  0.25539953 0.25537685 0.25584772 0.25656888 0.25702304
 0.2569616  0.25674662 0.2566751  0.25699225 0.25756854 0.25789484
 0.25766474 0.25698137 0.25593928 0.2548557  0.25397077 0.25327894
 0.25279036 0.25233546 0.2517756  0.25120234 0.250843   0.2508881
 0.25118035 0.251294   0.25105816 0.2507615  0.25069106 0.25106546
 0.25170392 0.25213015 0.2522548  0.25204656 0.251607   0.25108337
 0.25063837 0.25027424 0.2499744  0.24975704 0.24936956 0.24870498
 0.24798608 0.24757165 0.2475855  0.24774225 0.24763933 0.24717535
 0.24660434 0.24622509 0.2461931  0.24644984 0.24665147 0.24655548
 0.2461426  0.24571562 0.24545787 0.24551763 0.24582122 0.24622892
 0.2466597  0.2469642  0.24702093 0.24690364 0.24679126 0.2468082
 0.24695113 0.24696808 0.24666047 0.24615926 0.24574663 0.24565731
 0.24592172 0.24626409 0.24645276 0.24649984 0.24641351 0.24644327
 0.24669704 0.24694408 0.24696848 0.24674617 0.24642777 0.2461909
 0.24595718 0.24539042 0.24451171 0.24366128 0.2429249  0.24218659
 0.24158533 0.24122573 0.24106316 0.24107096 0.24096544 0.24058963
 0.24000719 0.23955171 0.23952554 0.2399583  0.24062417 0.24098557
 0.24075699 0.24013342 0.23965676 0.23958433 0.2397088  0.23951875
 0.23882264 0.23768769 0.23638727 0.23525207 0.23463021 0.23426771
 0.23393005 0.23346543 0.23301268 0.23284619 0.23288831 0.23308744
 0.23326448 0.23332971 0.23326524 0.23302038 0.23259464 0.2324638
 0.23262314 0.23288468 0.23299813 0.23249541 0.2313429  0.22998723
 0.22924773 0.22942059 0.22970594 0.22931041 0.22795638 0.22638324
 0.22557606 0.2259433  0.22652757 0.22685187 0.22662465 0.22619012
 0.22564438 0.22469278 0.22359087 0.22324952 0.22391452 0.22404331
 0.22114083 0.21526155 0.21120745 0.21296085 0.21399929 0.19163916]
