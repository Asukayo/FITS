Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20290816.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.658575296401978
Epoch: 1, Steps: 60 | Train Loss: 0.6734298 Vali Loss: 1.2617162 Test Loss: 0.6033155
Validation loss decreased (inf --> 1.261716).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.254494905471802
Epoch: 2, Steps: 60 | Train Loss: 0.4966861 Vali Loss: 1.1084697 Test Loss: 0.4966565
Validation loss decreased (1.261716 --> 1.108470).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.275036573410034
Epoch: 3, Steps: 60 | Train Loss: 0.4437891 Vali Loss: 1.0418119 Test Loss: 0.4527515
Validation loss decreased (1.108470 --> 1.041812).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.540854215621948
Epoch: 4, Steps: 60 | Train Loss: 0.4196114 Vali Loss: 1.0090718 Test Loss: 0.4332653
Validation loss decreased (1.041812 --> 1.009072).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.038948059082031
Epoch: 5, Steps: 60 | Train Loss: 0.4073554 Vali Loss: 0.9922374 Test Loss: 0.4249711
Validation loss decreased (1.009072 --> 0.992237).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.008588552474976
Epoch: 6, Steps: 60 | Train Loss: 0.4010972 Vali Loss: 0.9825665 Test Loss: 0.4215073
Validation loss decreased (0.992237 --> 0.982567).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.34081768989563
Epoch: 7, Steps: 60 | Train Loss: 0.3969151 Vali Loss: 0.9788165 Test Loss: 0.4204703
Validation loss decreased (0.982567 --> 0.978817).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.865862131118774
Epoch: 8, Steps: 60 | Train Loss: 0.3942146 Vali Loss: 0.9752460 Test Loss: 0.4202857
Validation loss decreased (0.978817 --> 0.975246).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.2384033203125
Epoch: 9, Steps: 60 | Train Loss: 0.3925142 Vali Loss: 0.9733477 Test Loss: 0.4203228
Validation loss decreased (0.975246 --> 0.973348).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.477370023727417
Epoch: 10, Steps: 60 | Train Loss: 0.3916086 Vali Loss: 0.9721976 Test Loss: 0.4202271
Validation loss decreased (0.973348 --> 0.972198).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.6111421585083
Epoch: 11, Steps: 60 | Train Loss: 0.3906205 Vali Loss: 0.9713603 Test Loss: 0.4202922
Validation loss decreased (0.972198 --> 0.971360).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.109464883804321
Epoch: 12, Steps: 60 | Train Loss: 0.3898615 Vali Loss: 0.9699330 Test Loss: 0.4206758
Validation loss decreased (0.971360 --> 0.969933).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.729376554489136
Epoch: 13, Steps: 60 | Train Loss: 0.3890036 Vali Loss: 0.9694897 Test Loss: 0.4203894
Validation loss decreased (0.969933 --> 0.969490).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.862827062606812
Epoch: 14, Steps: 60 | Train Loss: 0.3885249 Vali Loss: 0.9697640 Test Loss: 0.4206954
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.34746241569519
Epoch: 15, Steps: 60 | Train Loss: 0.3877465 Vali Loss: 0.9698510 Test Loss: 0.4205955
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.439430713653564
Epoch: 16, Steps: 60 | Train Loss: 0.3870440 Vali Loss: 0.9683974 Test Loss: 0.4208271
Validation loss decreased (0.969490 --> 0.968397).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.638396501541138
Epoch: 17, Steps: 60 | Train Loss: 0.3866998 Vali Loss: 0.9683545 Test Loss: 0.4208603
Validation loss decreased (0.968397 --> 0.968355).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.898289680480957
Epoch: 18, Steps: 60 | Train Loss: 0.3863004 Vali Loss: 0.9677371 Test Loss: 0.4209073
Validation loss decreased (0.968355 --> 0.967737).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.904320001602173
Epoch: 19, Steps: 60 | Train Loss: 0.3861211 Vali Loss: 0.9678200 Test Loss: 0.4208314
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 8.917822122573853
Epoch: 20, Steps: 60 | Train Loss: 0.3857464 Vali Loss: 0.9680540 Test Loss: 0.4209970
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.538685083389282
Epoch: 21, Steps: 60 | Train Loss: 0.3854976 Vali Loss: 0.9679763 Test Loss: 0.4210305
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.41715753078460693, mae:0.4256545901298523, rse:0.6133486032485962, corr:[0.25757027 0.2699396  0.2714464  0.26766056 0.26531488 0.26442266
 0.2641006  0.26341942 0.2625983  0.26212645 0.26187342 0.26160738
 0.2611682  0.26087657 0.2609857  0.26124755 0.26129103 0.26100007
 0.26055655 0.26021573 0.26004094 0.25999308 0.25982267 0.25972828
 0.25981492 0.25991848 0.2598729  0.25957745 0.25914067 0.25880125
 0.25861758 0.25843245 0.2582834  0.2581164  0.2579335  0.25785643
 0.25789395 0.25804    0.25815418 0.2582902  0.2585373  0.25872624
 0.2587694  0.25879973 0.25895146 0.2592216  0.2594522  0.25938243
 0.25876892 0.25792646 0.25699672 0.25603983 0.25505665 0.2539662
 0.25301144 0.25245148 0.2522466  0.25231135 0.25241423 0.25243077
 0.2522922  0.2520405  0.25177285 0.25173664 0.25190875 0.25222048
 0.25261828 0.2528422  0.2529372  0.25291905 0.25274152 0.2523894
 0.25199395 0.25156635 0.25112075 0.2506955  0.25019383 0.249593
 0.24899499 0.24852757 0.24821045 0.24796407 0.2477885  0.24771208
 0.24773987 0.24766666 0.24737972 0.24701084 0.24674276 0.24673496
 0.24684201 0.24688552 0.24685861 0.24695973 0.24727593 0.24771123
 0.2480422  0.24807994 0.24792144 0.24788512 0.24809797 0.24826734
 0.24810106 0.24766871 0.2472842  0.24721426 0.2472728  0.24702342
 0.24648696 0.2460121  0.2460039  0.24657106 0.2470818  0.24708673
 0.24675325 0.24644838 0.24645257 0.24667832 0.24670489 0.24631016
 0.24558848 0.24467218 0.2437817  0.24298595 0.24221647 0.24152088
 0.2411735  0.24121791 0.24121517 0.24087352 0.24023297 0.23965652
 0.23943248 0.23954864 0.23962419 0.23950137 0.23955362 0.23992322
 0.24037912 0.24037875 0.23984589 0.23911569 0.23863234 0.2384667
 0.2382517  0.23743041 0.23599443 0.23445398 0.23363607 0.23345259
 0.23325396 0.23272167 0.23223674 0.23235637 0.23287371 0.23307618
 0.23245145 0.23143955 0.23092546 0.23119082 0.23152012 0.23138718
 0.23067042 0.23007624 0.2303238  0.23092669 0.23080556 0.22954372
 0.2281567  0.22786103 0.22842751 0.22853899 0.22729366 0.22541597
 0.22435161 0.22483522 0.22532703 0.22466522 0.22302045 0.22194625
 0.2225902  0.22362125 0.22280237 0.22056226 0.21924157 0.22040847
 0.22191758 0.21984483 0.21520534 0.21358636 0.21911417 0.22005554]
