Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  25200896.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5426626
	speed: 0.0132s/iter; left time: 76.6310s
Epoch: 1 cost time: 1.5014467239379883
Epoch: 1, Steps: 118 | Train Loss: 0.6270008 Vali Loss: 1.5978440 Test Loss: 0.7546908
Validation loss decreased (inf --> 1.597844).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4258092
	speed: 0.0326s/iter; left time: 185.2397s
Epoch: 2 cost time: 1.7337851524353027
Epoch: 2, Steps: 118 | Train Loss: 0.4608216 Vali Loss: 1.4987202 Test Loss: 0.7045991
Validation loss decreased (1.597844 --> 1.498720).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3773717
	speed: 0.0383s/iter; left time: 213.3378s
Epoch: 3 cost time: 1.8980638980865479
Epoch: 3, Steps: 118 | Train Loss: 0.3969724 Vali Loss: 1.4455707 Test Loss: 0.6780174
Validation loss decreased (1.498720 --> 1.445571).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3418240
	speed: 0.0369s/iter; left time: 200.8980s
Epoch: 4 cost time: 1.6462185382843018
Epoch: 4, Steps: 118 | Train Loss: 0.3547045 Vali Loss: 1.4166912 Test Loss: 0.6558942
Validation loss decreased (1.445571 --> 1.416691).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3310727
	speed: 0.0390s/iter; left time: 208.0030s
Epoch: 5 cost time: 2.0542986392974854
Epoch: 5, Steps: 118 | Train Loss: 0.3226697 Vali Loss: 1.3893265 Test Loss: 0.6350273
Validation loss decreased (1.416691 --> 1.389326).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3120087
	speed: 0.0390s/iter; left time: 203.0089s
Epoch: 6 cost time: 1.883042573928833
Epoch: 6, Steps: 118 | Train Loss: 0.2970233 Vali Loss: 1.3675317 Test Loss: 0.6168096
Validation loss decreased (1.389326 --> 1.367532).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2790658
	speed: 0.0384s/iter; left time: 195.5156s
Epoch: 7 cost time: 1.802908182144165
Epoch: 7, Steps: 118 | Train Loss: 0.2765262 Vali Loss: 1.3455796 Test Loss: 0.5989736
Validation loss decreased (1.367532 --> 1.345580).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2309245
	speed: 0.0401s/iter; left time: 199.3477s
Epoch: 8 cost time: 2.15004825592041
Epoch: 8, Steps: 118 | Train Loss: 0.2597251 Vali Loss: 1.3258621 Test Loss: 0.5814063
Validation loss decreased (1.345580 --> 1.325862).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2627560
	speed: 0.0376s/iter; left time: 182.7097s
Epoch: 9 cost time: 2.1561615467071533
Epoch: 9, Steps: 118 | Train Loss: 0.2456807 Vali Loss: 1.3141580 Test Loss: 0.5682378
Validation loss decreased (1.325862 --> 1.314158).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2257172
	speed: 0.0401s/iter; left time: 189.8979s
Epoch: 10 cost time: 1.802983283996582
Epoch: 10, Steps: 118 | Train Loss: 0.2340644 Vali Loss: 1.3067945 Test Loss: 0.5563674
Validation loss decreased (1.314158 --> 1.306795).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2365278
	speed: 0.0370s/iter; left time: 171.0558s
Epoch: 11 cost time: 2.0531082153320312
Epoch: 11, Steps: 118 | Train Loss: 0.2243051 Vali Loss: 1.2932391 Test Loss: 0.5444613
Validation loss decreased (1.306795 --> 1.293239).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2137195
	speed: 0.0422s/iter; left time: 189.8237s
Epoch: 12 cost time: 2.3303544521331787
Epoch: 12, Steps: 118 | Train Loss: 0.2160944 Vali Loss: 1.2875718 Test Loss: 0.5357705
Validation loss decreased (1.293239 --> 1.287572).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2193689
	speed: 0.0940s/iter; left time: 411.9794s
Epoch: 13 cost time: 6.239663600921631
Epoch: 13, Steps: 118 | Train Loss: 0.2090915 Vali Loss: 1.2801361 Test Loss: 0.5271104
Validation loss decreased (1.287572 --> 1.280136).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1897345
	speed: 0.1232s/iter; left time: 525.5123s
Epoch: 14 cost time: 6.3751959800720215
Epoch: 14, Steps: 118 | Train Loss: 0.2030350 Vali Loss: 1.2704800 Test Loss: 0.5176824
Validation loss decreased (1.280136 --> 1.270480).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1924971
	speed: 0.1205s/iter; left time: 500.1012s
Epoch: 15 cost time: 5.566336631774902
Epoch: 15, Steps: 118 | Train Loss: 0.1977687 Vali Loss: 1.2657663 Test Loss: 0.5107194
Validation loss decreased (1.270480 --> 1.265766).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2102378
	speed: 0.1127s/iter; left time: 454.1455s
Epoch: 16 cost time: 5.483351230621338
Epoch: 16, Steps: 118 | Train Loss: 0.1932815 Vali Loss: 1.2641052 Test Loss: 0.5043658
Validation loss decreased (1.265766 --> 1.264105).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2026821
	speed: 0.1140s/iter; left time: 446.1803s
Epoch: 17 cost time: 6.157338619232178
Epoch: 17, Steps: 118 | Train Loss: 0.1892087 Vali Loss: 1.2567319 Test Loss: 0.5003124
Validation loss decreased (1.264105 --> 1.256732).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1835084
	speed: 0.1255s/iter; left time: 476.2842s
Epoch: 18 cost time: 5.924228668212891
Epoch: 18, Steps: 118 | Train Loss: 0.1859596 Vali Loss: 1.2534088 Test Loss: 0.4944318
Validation loss decreased (1.256732 --> 1.253409).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1805406
	speed: 0.1204s/iter; left time: 442.8747s
Epoch: 19 cost time: 5.631936550140381
Epoch: 19, Steps: 118 | Train Loss: 0.1827570 Vali Loss: 1.2539324 Test Loss: 0.4906188
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1923158
	speed: 0.1398s/iter; left time: 497.5282s
Epoch: 20 cost time: 6.960391044616699
Epoch: 20, Steps: 118 | Train Loss: 0.1803587 Vali Loss: 1.2501804 Test Loss: 0.4860903
Validation loss decreased (1.253409 --> 1.250180).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1753324
	speed: 0.2287s/iter; left time: 786.8798s
Epoch: 21 cost time: 12.435663938522339
Epoch: 21, Steps: 118 | Train Loss: 0.1778633 Vali Loss: 1.2438750 Test Loss: 0.4820850
Validation loss decreased (1.250180 --> 1.243875).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1800190
	speed: 0.2584s/iter; left time: 858.5368s
Epoch: 22 cost time: 12.294159412384033
Epoch: 22, Steps: 118 | Train Loss: 0.1758031 Vali Loss: 1.2450655 Test Loss: 0.4791285
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1689936
	speed: 0.2518s/iter; left time: 807.0680s
Epoch: 23 cost time: 12.23465347290039
Epoch: 23, Steps: 118 | Train Loss: 0.1739344 Vali Loss: 1.2438896 Test Loss: 0.4761221
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1737672
	speed: 0.2518s/iter; left time: 777.1844s
Epoch: 24 cost time: 12.433797121047974
Epoch: 24, Steps: 118 | Train Loss: 0.1722475 Vali Loss: 1.2415538 Test Loss: 0.4738376
Validation loss decreased (1.243875 --> 1.241554).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1761472
	speed: 0.2703s/iter; left time: 802.5561s
Epoch: 25 cost time: 12.887693643569946
Epoch: 25, Steps: 118 | Train Loss: 0.1707413 Vali Loss: 1.2423395 Test Loss: 0.4711514
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1536471
	speed: 0.2738s/iter; left time: 780.5303s
Epoch: 26 cost time: 13.414834260940552
Epoch: 26, Steps: 118 | Train Loss: 0.1693180 Vali Loss: 1.2411731 Test Loss: 0.4689124
Validation loss decreased (1.241554 --> 1.241173).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1745019
	speed: 0.2822s/iter; left time: 771.2024s
Epoch: 27 cost time: 13.433778047561646
Epoch: 27, Steps: 118 | Train Loss: 0.1682346 Vali Loss: 1.2337302 Test Loss: 0.4665725
Validation loss decreased (1.241173 --> 1.233730).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1573869
	speed: 0.2715s/iter; left time: 709.8485s
Epoch: 28 cost time: 12.941705703735352
Epoch: 28, Steps: 118 | Train Loss: 0.1670501 Vali Loss: 1.2388438 Test Loss: 0.4649082
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1702438
	speed: 0.2739s/iter; left time: 683.8974s
Epoch: 29 cost time: 13.198152303695679
Epoch: 29, Steps: 118 | Train Loss: 0.1660357 Vali Loss: 1.2336245 Test Loss: 0.4634758
Validation loss decreased (1.233730 --> 1.233624).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1627600
	speed: 0.2678s/iter; left time: 637.0283s
Epoch: 30 cost time: 12.96869969367981
Epoch: 30, Steps: 118 | Train Loss: 0.1651062 Vali Loss: 1.2346097 Test Loss: 0.4616459
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1647957
	speed: 0.2699s/iter; left time: 610.1872s
Epoch: 31 cost time: 13.078263759613037
Epoch: 31, Steps: 118 | Train Loss: 0.1642736 Vali Loss: 1.2354004 Test Loss: 0.4607839
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1518296
	speed: 0.3365s/iter; left time: 721.1464s
Epoch: 32 cost time: 16.307183504104614
Epoch: 32, Steps: 118 | Train Loss: 0.1635708 Vali Loss: 1.2303692 Test Loss: 0.4591610
Validation loss decreased (1.233624 --> 1.230369).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1627023
	speed: 0.3356s/iter; left time: 679.6272s
Epoch: 33 cost time: 15.514302015304565
Epoch: 33, Steps: 118 | Train Loss: 0.1628617 Vali Loss: 1.2291962 Test Loss: 0.4581553
Validation loss decreased (1.230369 --> 1.229196).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1620924
	speed: 0.3266s/iter; left time: 622.8411s
Epoch: 34 cost time: 15.374647617340088
Epoch: 34, Steps: 118 | Train Loss: 0.1622058 Vali Loss: 1.2292751 Test Loss: 0.4571552
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1592711
	speed: 0.3248s/iter; left time: 581.1098s
Epoch: 35 cost time: 15.6598961353302
Epoch: 35, Steps: 118 | Train Loss: 0.1617049 Vali Loss: 1.2316788 Test Loss: 0.4559519
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1647476
	speed: 0.3267s/iter; left time: 545.9018s
Epoch: 36 cost time: 15.140169620513916
Epoch: 36, Steps: 118 | Train Loss: 0.1611773 Vali Loss: 1.2288840 Test Loss: 0.4549164
Validation loss decreased (1.229196 --> 1.228884).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1538121
	speed: 0.3237s/iter; left time: 502.6365s
Epoch: 37 cost time: 15.406657457351685
Epoch: 37, Steps: 118 | Train Loss: 0.1605536 Vali Loss: 1.2266349 Test Loss: 0.4541802
Validation loss decreased (1.228884 --> 1.226635).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1478036
	speed: 0.3272s/iter; left time: 469.4786s
Epoch: 38 cost time: 15.477185010910034
Epoch: 38, Steps: 118 | Train Loss: 0.1602297 Vali Loss: 1.2292023 Test Loss: 0.4534633
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1579979
	speed: 0.3503s/iter; left time: 461.3149s
Epoch: 39 cost time: 17.71234393119812
Epoch: 39, Steps: 118 | Train Loss: 0.1597043 Vali Loss: 1.2265402 Test Loss: 0.4528867
Validation loss decreased (1.226635 --> 1.226540).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1543295
	speed: 0.3834s/iter; left time: 459.7308s
Epoch: 40 cost time: 18.22707986831665
Epoch: 40, Steps: 118 | Train Loss: 0.1593347 Vali Loss: 1.2275474 Test Loss: 0.4520882
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1562837
	speed: 0.3836s/iter; left time: 414.6208s
Epoch: 41 cost time: 18.211794137954712
Epoch: 41, Steps: 118 | Train Loss: 0.1590712 Vali Loss: 1.2255834 Test Loss: 0.4514845
Validation loss decreased (1.226540 --> 1.225583).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1663497
	speed: 0.3769s/iter; left time: 362.9557s
Epoch: 42 cost time: 17.44998526573181
Epoch: 42, Steps: 118 | Train Loss: 0.1586444 Vali Loss: 1.2271671 Test Loss: 0.4509814
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1578659
	speed: 0.3759s/iter; left time: 317.6634s
Epoch: 43 cost time: 17.79087495803833
Epoch: 43, Steps: 118 | Train Loss: 0.1583728 Vali Loss: 1.2249430 Test Loss: 0.4504270
Validation loss decreased (1.225583 --> 1.224943).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.1611441
	speed: 0.3758s/iter; left time: 273.1783s
Epoch: 44 cost time: 17.931047677993774
Epoch: 44, Steps: 118 | Train Loss: 0.1580205 Vali Loss: 1.2263396 Test Loss: 0.4499523
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.1565688
	speed: 0.3724s/iter; left time: 226.7671s
Epoch: 45 cost time: 17.279534578323364
Epoch: 45, Steps: 118 | Train Loss: 0.1578889 Vali Loss: 1.2249743 Test Loss: 0.4494590
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.1515339
	speed: 0.3666s/iter; left time: 179.9765s
Epoch: 46 cost time: 17.486961126327515
Epoch: 46, Steps: 118 | Train Loss: 0.1575725 Vali Loss: 1.2232591 Test Loss: 0.4490279
Validation loss decreased (1.224943 --> 1.223259).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.1561933
	speed: 0.3778s/iter; left time: 140.9281s
Epoch: 47 cost time: 18.43767547607422
Epoch: 47, Steps: 118 | Train Loss: 0.1574059 Vali Loss: 1.2239685 Test Loss: 0.4486584
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.1527403
	speed: 0.3817s/iter; left time: 97.3211s
Epoch: 48 cost time: 18.10938835144043
Epoch: 48, Steps: 118 | Train Loss: 0.1572200 Vali Loss: 1.2251985 Test Loss: 0.4483208
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.1529208
	speed: 0.3773s/iter; left time: 51.6871s
Epoch: 49 cost time: 17.483678340911865
Epoch: 49, Steps: 118 | Train Loss: 0.1569173 Vali Loss: 1.2210997 Test Loss: 0.4478756
Validation loss decreased (1.223259 --> 1.221100).  Saving model ...
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.1605420
	speed: 0.3659s/iter; left time: 6.9512s
Epoch: 50 cost time: 18.010563850402832
Epoch: 50, Steps: 118 | Train Loss: 0.1567994 Vali Loss: 1.2231767 Test Loss: 0.4476189
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.0497355408796396e-05
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  25200896.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4061906
	speed: 0.1543s/iter; left time: 895.2963s
Epoch: 1 cost time: 18.162731885910034
Epoch: 1, Steps: 118 | Train Loss: 0.4367045 Vali Loss: 1.2111568 Test Loss: 0.4416552
Validation loss decreased (inf --> 1.211157).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4102388
	speed: 0.3886s/iter; left time: 2208.6173s
Epoch: 2 cost time: 18.161311626434326
Epoch: 2, Steps: 118 | Train Loss: 0.4347035 Vali Loss: 1.2137069 Test Loss: 0.4428001
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4538130
	speed: 0.3795s/iter; left time: 2111.8843s
Epoch: 3 cost time: 17.670807600021362
Epoch: 3, Steps: 118 | Train Loss: 0.4336118 Vali Loss: 1.2177132 Test Loss: 0.4435599
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4228918
	speed: 0.3824s/iter; left time: 2083.1229s
Epoch: 4 cost time: 18.216935634613037
Epoch: 4, Steps: 118 | Train Loss: 0.4335995 Vali Loss: 1.2144374 Test Loss: 0.4433007
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.44088953733444214, mae:0.43970027565956116, rse:0.632145345211029, corr:[0.25883055 0.26358667 0.26187807 0.26037523 0.2590052  0.2565089
 0.25407934 0.25328684 0.25335947 0.25312978 0.2524583  0.25195268
 0.2518351  0.2520403  0.25258335 0.25297225 0.2524654  0.25157824
 0.25106266 0.25083125 0.25051543 0.25019106 0.2501595  0.25002143
 0.24964772 0.24948858 0.24970531 0.24968326 0.24915022 0.24852857
 0.24843292 0.24869375 0.24901578 0.2490039  0.24857426 0.24810594
 0.24806291 0.24854416 0.24898931 0.24882373 0.24830048 0.24794872
 0.24809413 0.24854797 0.24886073 0.24893041 0.24888472 0.24870397
 0.24866642 0.24867187 0.24811749 0.2469365  0.24530031 0.24386518
 0.24348743 0.24373402 0.2433809  0.24219604 0.24114865 0.24125944
 0.24186063 0.24226287 0.242267   0.24216299 0.24212313 0.24245945
 0.24310327 0.24321079 0.24284771 0.24248269 0.24246633 0.24249624
 0.24241334 0.24219072 0.24195272 0.24173556 0.24125353 0.24071386
 0.24036954 0.24007916 0.2396526  0.23923415 0.23865278 0.23782256
 0.23732355 0.23769757 0.23826937 0.23805846 0.23712671 0.23641913
 0.23604445 0.23565656 0.23525882 0.23541659 0.23611201 0.23707122
 0.23790635 0.23848844 0.23880939 0.23888353 0.2385864  0.23803051
 0.23778665 0.23821549 0.2388421  0.23898378 0.23835681 0.23741436
 0.23680098 0.23668392 0.23720287 0.23812556 0.23874967 0.23882048
 0.23855007 0.23811491 0.237559   0.23703516 0.2367607  0.23663087
 0.23610507 0.23505338 0.2346473  0.23518436 0.23510222 0.23370092
 0.23279338 0.23334791 0.23368469 0.23303354 0.23246236 0.23264922
 0.23272403 0.23239382 0.23274076 0.233715   0.23374303 0.23266086
 0.2321636  0.23263139 0.23262271 0.23180063 0.23132311 0.23166238
 0.23177533 0.23134555 0.23132876 0.2312021  0.2299824  0.22855316
 0.22856903 0.22948818 0.22970885 0.2296205  0.23005979 0.23034123
 0.22965643 0.22902253 0.22949934 0.23001726 0.22945619 0.22872041
 0.22839157 0.22801971 0.22722006 0.22659005 0.22631618 0.22638854
 0.22698423 0.22814304 0.22930847 0.22981918 0.2293435  0.22830053
 0.2275136  0.22761448 0.2279955  0.2279826  0.22753443 0.22713764
 0.22672312 0.22598073 0.2254202  0.22566575 0.22621049 0.22627926
 0.2261379  0.22653024 0.2272578  0.22766383 0.22777823 0.22780691
 0.22742918 0.22702579 0.22719188 0.22724134 0.22575839 0.22368734
 0.22366354 0.22537336 0.22563227 0.22385937 0.22237377 0.22272746
 0.22335726 0.22343744 0.22345525 0.22360651 0.22370741 0.22392777
 0.22442926 0.22422822 0.22310907 0.22212936 0.22202174 0.22222137
 0.22193155 0.22161347 0.22155648 0.22134076 0.22091872 0.22047244
 0.22008571 0.2197284  0.2199345  0.2204209  0.2198253  0.2182094
 0.21737593 0.21842188 0.21970537 0.21961313 0.21888018 0.21892118
 0.2193387  0.21936505 0.21941595 0.21977323 0.21981338 0.21912517
 0.21876952 0.21915324 0.2197945  0.22015132 0.22021073 0.21968418
 0.21868819 0.21810538 0.21829912 0.21837316 0.21757795 0.21653073
 0.21628505 0.21660833 0.21682909 0.21701895 0.21757558 0.21795495
 0.21742323 0.21602763 0.21532318 0.21595658 0.21701787 0.217208
 0.21657613 0.21583784 0.21517889 0.21445023 0.21372241 0.21369454
 0.21469797 0.21586782 0.21603677 0.2149168  0.21315965 0.21247557
 0.21333401 0.21416202 0.21394195 0.21327016 0.21298024 0.21291502
 0.21254174 0.2119125  0.21139649 0.21098593 0.211319   0.21263553
 0.21358354 0.21286386 0.21131273 0.21113881 0.21154179 0.21112914
 0.21060763 0.21171756 0.21336924 0.2132988  0.21185143 0.2110957
 0.21132562 0.21126619 0.21094905 0.21077448 0.21143603 0.21198498
 0.21145616 0.20971178 0.2090257  0.21000412 0.21085237 0.2099249
 0.20847219 0.20811908 0.20788482 0.20673636 0.20591824 0.20566538
 0.2053866  0.20519131 0.20598766 0.20635518 0.20467022 0.20326345
 0.20363605 0.20278199 0.19978197 0.1991161  0.20070893 0.200448
 0.19893432 0.20052525 0.2019246  0.19778524 0.19702983 0.20423044]
