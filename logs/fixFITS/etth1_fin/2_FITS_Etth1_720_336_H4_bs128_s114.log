Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  47065088.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.13396692276001
Epoch: 1, Steps: 29 | Train Loss: 0.7415761 Vali Loss: 1.9216090 Test Loss: 0.9195768
Validation loss decreased (inf --> 1.921609).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.32521915435791
Epoch: 2, Steps: 29 | Train Loss: 0.6372983 Vali Loss: 1.7515457 Test Loss: 0.8325545
Validation loss decreased (1.921609 --> 1.751546).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.226289987564087
Epoch: 3, Steps: 29 | Train Loss: 0.5691333 Vali Loss: 1.6645052 Test Loss: 0.7795691
Validation loss decreased (1.751546 --> 1.664505).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.342833995819092
Epoch: 4, Steps: 29 | Train Loss: 0.5231301 Vali Loss: 1.5995414 Test Loss: 0.7465233
Validation loss decreased (1.664505 --> 1.599541).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.1086156368255615
Epoch: 5, Steps: 29 | Train Loss: 0.4909287 Vali Loss: 1.5554776 Test Loss: 0.7258130
Validation loss decreased (1.599541 --> 1.555478).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.031051874160767
Epoch: 6, Steps: 29 | Train Loss: 0.4666786 Vali Loss: 1.5200002 Test Loss: 0.7110280
Validation loss decreased (1.555478 --> 1.520000).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.042074918746948
Epoch: 7, Steps: 29 | Train Loss: 0.4484040 Vali Loss: 1.4991177 Test Loss: 0.7014201
Validation loss decreased (1.520000 --> 1.499118).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.114680528640747
Epoch: 8, Steps: 29 | Train Loss: 0.4328678 Vali Loss: 1.4861642 Test Loss: 0.6940127
Validation loss decreased (1.499118 --> 1.486164).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.3837034702301025
Epoch: 9, Steps: 29 | Train Loss: 0.4206756 Vali Loss: 1.4717464 Test Loss: 0.6883754
Validation loss decreased (1.486164 --> 1.471746).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.025613307952881
Epoch: 10, Steps: 29 | Train Loss: 0.4105210 Vali Loss: 1.4643233 Test Loss: 0.6835470
Validation loss decreased (1.471746 --> 1.464323).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.552109956741333
Epoch: 11, Steps: 29 | Train Loss: 0.4013308 Vali Loss: 1.4580675 Test Loss: 0.6796188
Validation loss decreased (1.464323 --> 1.458068).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.019997596740723
Epoch: 12, Steps: 29 | Train Loss: 0.3926819 Vali Loss: 1.4490808 Test Loss: 0.6757113
Validation loss decreased (1.458068 --> 1.449081).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.050505638122559
Epoch: 13, Steps: 29 | Train Loss: 0.3850865 Vali Loss: 1.4409339 Test Loss: 0.6721618
Validation loss decreased (1.449081 --> 1.440934).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.973024606704712
Epoch: 14, Steps: 29 | Train Loss: 0.3789534 Vali Loss: 1.4364130 Test Loss: 0.6695091
Validation loss decreased (1.440934 --> 1.436413).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.635481595993042
Epoch: 15, Steps: 29 | Train Loss: 0.3728371 Vali Loss: 1.4273202 Test Loss: 0.6661888
Validation loss decreased (1.436413 --> 1.427320).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.728752374649048
Epoch: 16, Steps: 29 | Train Loss: 0.3670576 Vali Loss: 1.4220414 Test Loss: 0.6635342
Validation loss decreased (1.427320 --> 1.422041).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.393737554550171
Epoch: 17, Steps: 29 | Train Loss: 0.3619347 Vali Loss: 1.4198427 Test Loss: 0.6610345
Validation loss decreased (1.422041 --> 1.419843).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.134929895401001
Epoch: 18, Steps: 29 | Train Loss: 0.3572217 Vali Loss: 1.4205285 Test Loss: 0.6580337
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.301717519760132
Epoch: 19, Steps: 29 | Train Loss: 0.3530035 Vali Loss: 1.4122776 Test Loss: 0.6564532
Validation loss decreased (1.419843 --> 1.412278).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.034379005432129
Epoch: 20, Steps: 29 | Train Loss: 0.3489661 Vali Loss: 1.4129176 Test Loss: 0.6537321
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.177402973175049
Epoch: 21, Steps: 29 | Train Loss: 0.3451006 Vali Loss: 1.4078647 Test Loss: 0.6519166
Validation loss decreased (1.412278 --> 1.407865).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.043910503387451
Epoch: 22, Steps: 29 | Train Loss: 0.3419121 Vali Loss: 1.4127481 Test Loss: 0.6493781
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.117318153381348
Epoch: 23, Steps: 29 | Train Loss: 0.3386380 Vali Loss: 1.4014502 Test Loss: 0.6477754
Validation loss decreased (1.407865 --> 1.401450).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.216959238052368
Epoch: 24, Steps: 29 | Train Loss: 0.3353786 Vali Loss: 1.4003503 Test Loss: 0.6457677
Validation loss decreased (1.401450 --> 1.400350).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.884974479675293
Epoch: 25, Steps: 29 | Train Loss: 0.3322347 Vali Loss: 1.3939109 Test Loss: 0.6441626
Validation loss decreased (1.400350 --> 1.393911).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.854686975479126
Epoch: 26, Steps: 29 | Train Loss: 0.3299734 Vali Loss: 1.3915687 Test Loss: 0.6426587
Validation loss decreased (1.393911 --> 1.391569).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.0288941860198975
Epoch: 27, Steps: 29 | Train Loss: 0.3272404 Vali Loss: 1.3842475 Test Loss: 0.6406412
Validation loss decreased (1.391569 --> 1.384248).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.789586067199707
Epoch: 28, Steps: 29 | Train Loss: 0.3251222 Vali Loss: 1.3855008 Test Loss: 0.6394652
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.690141916275024
Epoch: 29, Steps: 29 | Train Loss: 0.3231933 Vali Loss: 1.3867360 Test Loss: 0.6380445
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.880904912948608
Epoch: 30, Steps: 29 | Train Loss: 0.3210908 Vali Loss: 1.3787868 Test Loss: 0.6368638
Validation loss decreased (1.384248 --> 1.378787).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.922001838684082
Epoch: 31, Steps: 29 | Train Loss: 0.3187742 Vali Loss: 1.3821721 Test Loss: 0.6352154
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.6874096393585205
Epoch: 32, Steps: 29 | Train Loss: 0.3172822 Vali Loss: 1.3843780 Test Loss: 0.6339968
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.511415481567383
Epoch: 33, Steps: 29 | Train Loss: 0.3156151 Vali Loss: 1.3710006 Test Loss: 0.6329554
Validation loss decreased (1.378787 --> 1.371001).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.711629390716553
Epoch: 34, Steps: 29 | Train Loss: 0.3140655 Vali Loss: 1.3749304 Test Loss: 0.6318434
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.424178123474121
Epoch: 35, Steps: 29 | Train Loss: 0.3121740 Vali Loss: 1.3828077 Test Loss: 0.6308974
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.7593913078308105
Epoch: 36, Steps: 29 | Train Loss: 0.3108510 Vali Loss: 1.3734730 Test Loss: 0.6298620
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  47065088.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.865375757217407
Epoch: 1, Steps: 29 | Train Loss: 0.5472925 Vali Loss: 1.3147987 Test Loss: 0.5742927
Validation loss decreased (inf --> 1.314799).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.954129219055176
Epoch: 2, Steps: 29 | Train Loss: 0.5157381 Vali Loss: 1.2729187 Test Loss: 0.5352072
Validation loss decreased (1.314799 --> 1.272919).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.948057413101196
Epoch: 3, Steps: 29 | Train Loss: 0.4952649 Vali Loss: 1.2495805 Test Loss: 0.5085871
Validation loss decreased (1.272919 --> 1.249581).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.996988296508789
Epoch: 4, Steps: 29 | Train Loss: 0.4805888 Vali Loss: 1.2263680 Test Loss: 0.4898902
Validation loss decreased (1.249581 --> 1.226368).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.674647808074951
Epoch: 5, Steps: 29 | Train Loss: 0.4693301 Vali Loss: 1.2113272 Test Loss: 0.4766428
Validation loss decreased (1.226368 --> 1.211327).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.620326280593872
Epoch: 6, Steps: 29 | Train Loss: 0.4630131 Vali Loss: 1.2003107 Test Loss: 0.4667139
Validation loss decreased (1.211327 --> 1.200311).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.594738006591797
Epoch: 7, Steps: 29 | Train Loss: 0.4567525 Vali Loss: 1.1998237 Test Loss: 0.4598691
Validation loss decreased (1.200311 --> 1.199824).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.9560675621032715
Epoch: 8, Steps: 29 | Train Loss: 0.4522403 Vali Loss: 1.1992625 Test Loss: 0.4545766
Validation loss decreased (1.199824 --> 1.199262).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.720946311950684
Epoch: 9, Steps: 29 | Train Loss: 0.4488730 Vali Loss: 1.1926212 Test Loss: 0.4508603
Validation loss decreased (1.199262 --> 1.192621).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.642408132553101
Epoch: 10, Steps: 29 | Train Loss: 0.4474958 Vali Loss: 1.1894010 Test Loss: 0.4481893
Validation loss decreased (1.192621 --> 1.189401).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.056258201599121
Epoch: 11, Steps: 29 | Train Loss: 0.4449201 Vali Loss: 1.1887748 Test Loss: 0.4463191
Validation loss decreased (1.189401 --> 1.188775).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.670736312866211
Epoch: 12, Steps: 29 | Train Loss: 0.4435717 Vali Loss: 1.1927946 Test Loss: 0.4450668
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.6043596267700195
Epoch: 13, Steps: 29 | Train Loss: 0.4425525 Vali Loss: 1.1958517 Test Loss: 0.4439574
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.744480133056641
Epoch: 14, Steps: 29 | Train Loss: 0.4414592 Vali Loss: 1.1835536 Test Loss: 0.4434252
Validation loss decreased (1.188775 --> 1.183554).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.673596382141113
Epoch: 15, Steps: 29 | Train Loss: 0.4400819 Vali Loss: 1.1849191 Test Loss: 0.4428287
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.819937229156494
Epoch: 16, Steps: 29 | Train Loss: 0.4402512 Vali Loss: 1.1868196 Test Loss: 0.4426630
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.678107500076294
Epoch: 17, Steps: 29 | Train Loss: 0.4395692 Vali Loss: 1.1922593 Test Loss: 0.4424461
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.44173863530158997, mae:0.44272083044052124, rse:0.6327537894248962, corr:[0.24914394 0.25963953 0.25936186 0.25516665 0.25332552 0.25294277
 0.2526775  0.25195432 0.25105742 0.2505765  0.25036317 0.2501848
 0.25000834 0.24988495 0.2497904  0.24946092 0.24886051 0.24823287
 0.24787839 0.24788572 0.24799126 0.2479754  0.24756612 0.24732389
 0.24760753 0.24816187 0.248417   0.24804613 0.24722004 0.2464323
 0.24608831 0.24605985 0.24622259 0.24627714 0.24601763 0.2457176
 0.24555175 0.24552305 0.24566759 0.24592789 0.24636579 0.24668609
 0.24661438 0.24632442 0.24627444 0.24677941 0.2476538  0.24830529
 0.24809901 0.24738751 0.2463434  0.24535497 0.24453858 0.24360332
 0.24264385 0.24189898 0.24136701 0.24111804 0.24092391 0.24082784
 0.24067406 0.24053343 0.24037316 0.2403927  0.24053614 0.24077453
 0.24115129 0.24141356 0.2416673  0.24178974 0.24161586 0.24124223
 0.2408577  0.24051102 0.24016826 0.23986165 0.23933697 0.23860675
 0.23794773 0.23760991 0.23751344 0.23740438 0.23708871 0.23673528
 0.2365616  0.23658232 0.23664215 0.2366124  0.23641711 0.2361449
 0.2358127  0.23556522 0.2355036  0.23577112 0.23629849 0.2369464
 0.23758867 0.23806553 0.2382958  0.23851438 0.23878297 0.23890142
 0.23879184 0.23847196 0.23810308 0.23785055 0.23768361 0.23756759
 0.23758468 0.23766252 0.23774618 0.23785968 0.23786198 0.2377567
 0.2376838  0.23770039 0.23786022 0.23808244 0.23820294 0.23817982
 0.23796569 0.23735383 0.23644307 0.23556118 0.23489499 0.23440531
 0.23417911 0.23421751 0.23404452 0.2336321  0.23306812 0.23258285
 0.23239705 0.23253702 0.23285028 0.23315671 0.23339824 0.23351085
 0.23348892 0.23338327 0.23333557 0.23332728 0.23325753 0.2330037
 0.23246807 0.23165303 0.23077771 0.23002319 0.22960414 0.22921047
 0.22869886 0.22824006 0.22804135 0.22824696 0.2286414  0.22888246
 0.22872669 0.22828643 0.22795658 0.22798555 0.22815858 0.22842166
 0.2284877  0.22841382 0.22835962 0.22839798 0.22833985 0.22809146
 0.22787741 0.22792806 0.22812922 0.22824441 0.22793338 0.2272604
 0.22659588 0.22641158 0.22659299 0.22693934 0.22701313 0.2267575
 0.22630456 0.22593127 0.22583267 0.22605596 0.22646517 0.22686629
 0.2270914  0.22719099 0.22741765 0.22786371 0.22835474 0.2286128
 0.22837831 0.22759832 0.22634734 0.22503145 0.22416708 0.22377059
 0.22356145 0.2233551  0.22300093 0.22255617 0.22214778 0.2220551
 0.22214319 0.2224492  0.22288412 0.22329752 0.2236004  0.22362989
 0.22345549 0.22319011 0.22311349 0.22316553 0.22315054 0.22292419
 0.2224645  0.22203286 0.22175114 0.22161867 0.22154231 0.2210857
 0.22042903 0.2199862  0.21993835 0.22002889 0.21990463 0.21967965
 0.21945977 0.2192679  0.21926507 0.21936756 0.21934772 0.21935044
 0.21940948 0.219402   0.21930821 0.2191496  0.21917795 0.21927759
 0.21944156 0.21937516 0.21903756 0.21855709 0.21818227 0.21811894
 0.21832357 0.21857707 0.21855955 0.21821901 0.21778001 0.21750838
 0.21734752 0.21709272 0.21692966 0.21695207 0.21722086 0.21747226
 0.21757466 0.21742386 0.21743163 0.2176101  0.21770427 0.21736175
 0.21675478 0.2162496  0.21599579 0.21586259 0.21529402 0.21431167
 0.21351069 0.21321957 0.21329202 0.21349767 0.21333776 0.21294528
 0.21278268 0.21302226 0.21333483 0.21338275 0.21317455 0.21303256
 0.21304275 0.21297564 0.21289903 0.21277326 0.21287373 0.21317212
 0.21344039 0.2134273  0.21268539 0.21173218 0.21106699 0.21120405
 0.21161672 0.21156946 0.21089411 0.21027845 0.21027552 0.2107871
 0.2111466  0.21092941 0.21073161 0.21075194 0.21109737 0.21130426
 0.21133192 0.21103582 0.21114317 0.21131036 0.21093656 0.2097768
 0.20848794 0.20785522 0.2078247  0.20763806 0.20661514 0.20460433
 0.2028404  0.20232333 0.20297039 0.20337383 0.20257495 0.2013866
 0.20107593 0.20121448 0.2007078  0.19990444 0.19928548 0.19978918
 0.20019083 0.19796824 0.19401838 0.1932036  0.1980647  0.19602877]
