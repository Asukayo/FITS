Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  94130176.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.9975228309631348
Epoch: 1, Steps: 14 | Train Loss: 0.7738120 Vali Loss: 2.0358634 Test Loss: 0.9775508
Validation loss decreased (inf --> 2.035863).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.7724690437316895
Epoch: 2, Steps: 14 | Train Loss: 0.7110670 Vali Loss: 1.9266758 Test Loss: 0.9214877
Validation loss decreased (2.035863 --> 1.926676).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.8968374729156494
Epoch: 3, Steps: 14 | Train Loss: 0.6604175 Vali Loss: 1.8233416 Test Loss: 0.8757092
Validation loss decreased (1.926676 --> 1.823342).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.8927459716796875
Epoch: 4, Steps: 14 | Train Loss: 0.6212661 Vali Loss: 1.7781947 Test Loss: 0.8395441
Validation loss decreased (1.823342 --> 1.778195).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.89280104637146
Epoch: 5, Steps: 14 | Train Loss: 0.5914862 Vali Loss: 1.7170283 Test Loss: 0.8119010
Validation loss decreased (1.778195 --> 1.717028).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.823582172393799
Epoch: 6, Steps: 14 | Train Loss: 0.5654289 Vali Loss: 1.6822550 Test Loss: 0.7896618
Validation loss decreased (1.717028 --> 1.682255).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.9316015243530273
Epoch: 7, Steps: 14 | Train Loss: 0.5439264 Vali Loss: 1.6438451 Test Loss: 0.7723562
Validation loss decreased (1.682255 --> 1.643845).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.6915371417999268
Epoch: 8, Steps: 14 | Train Loss: 0.5281573 Vali Loss: 1.6213105 Test Loss: 0.7587775
Validation loss decreased (1.643845 --> 1.621310).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.884580612182617
Epoch: 9, Steps: 14 | Train Loss: 0.5131192 Vali Loss: 1.6044195 Test Loss: 0.7477332
Validation loss decreased (1.621310 --> 1.604419).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.93681001663208
Epoch: 10, Steps: 14 | Train Loss: 0.5010351 Vali Loss: 1.5805345 Test Loss: 0.7384354
Validation loss decreased (1.604419 --> 1.580534).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.7713189125061035
Epoch: 11, Steps: 14 | Train Loss: 0.4901063 Vali Loss: 1.5696781 Test Loss: 0.7305377
Validation loss decreased (1.580534 --> 1.569678).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.9807653427124023
Epoch: 12, Steps: 14 | Train Loss: 0.4810519 Vali Loss: 1.5517786 Test Loss: 0.7244843
Validation loss decreased (1.569678 --> 1.551779).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.6726269721984863
Epoch: 13, Steps: 14 | Train Loss: 0.4735598 Vali Loss: 1.5433317 Test Loss: 0.7190493
Validation loss decreased (1.551779 --> 1.543332).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.9278409481048584
Epoch: 14, Steps: 14 | Train Loss: 0.4655295 Vali Loss: 1.5326600 Test Loss: 0.7143549
Validation loss decreased (1.543332 --> 1.532660).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.8219735622406006
Epoch: 15, Steps: 14 | Train Loss: 0.4588673 Vali Loss: 1.5172859 Test Loss: 0.7106942
Validation loss decreased (1.532660 --> 1.517286).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.905029058456421
Epoch: 16, Steps: 14 | Train Loss: 0.4535412 Vali Loss: 1.5039562 Test Loss: 0.7075927
Validation loss decreased (1.517286 --> 1.503956).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.9354658126831055
Epoch: 17, Steps: 14 | Train Loss: 0.4489085 Vali Loss: 1.5101501 Test Loss: 0.7047716
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.833833694458008
Epoch: 18, Steps: 14 | Train Loss: 0.4447355 Vali Loss: 1.5054580 Test Loss: 0.7019295
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.1343014240264893
Epoch: 19, Steps: 14 | Train Loss: 0.4401618 Vali Loss: 1.5017060 Test Loss: 0.6995915
Validation loss decreased (1.503956 --> 1.501706).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.1660141944885254
Epoch: 20, Steps: 14 | Train Loss: 0.4364346 Vali Loss: 1.4853172 Test Loss: 0.6974874
Validation loss decreased (1.501706 --> 1.485317).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.064805746078491
Epoch: 21, Steps: 14 | Train Loss: 0.4328652 Vali Loss: 1.4897698 Test Loss: 0.6956165
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.940692901611328
Epoch: 22, Steps: 14 | Train Loss: 0.4292408 Vali Loss: 1.4831824 Test Loss: 0.6942623
Validation loss decreased (1.485317 --> 1.483182).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.0764617919921875
Epoch: 23, Steps: 14 | Train Loss: 0.4264568 Vali Loss: 1.4758841 Test Loss: 0.6929387
Validation loss decreased (1.483182 --> 1.475884).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.783231019973755
Epoch: 24, Steps: 14 | Train Loss: 0.4231089 Vali Loss: 1.4770353 Test Loss: 0.6914815
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.917337656021118
Epoch: 25, Steps: 14 | Train Loss: 0.4214472 Vali Loss: 1.4768664 Test Loss: 0.6902254
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.6921546459198
Epoch: 26, Steps: 14 | Train Loss: 0.4183063 Vali Loss: 1.4709549 Test Loss: 0.6892300
Validation loss decreased (1.475884 --> 1.470955).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.902745008468628
Epoch: 27, Steps: 14 | Train Loss: 0.4164389 Vali Loss: 1.4798169 Test Loss: 0.6880810
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.017691135406494
Epoch: 28, Steps: 14 | Train Loss: 0.4138508 Vali Loss: 1.4718473 Test Loss: 0.6871895
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.003999948501587
Epoch: 29, Steps: 14 | Train Loss: 0.4114036 Vali Loss: 1.4654194 Test Loss: 0.6865418
Validation loss decreased (1.470955 --> 1.465419).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.771416664123535
Epoch: 30, Steps: 14 | Train Loss: 0.4105501 Vali Loss: 1.4617848 Test Loss: 0.6857330
Validation loss decreased (1.465419 --> 1.461785).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.68125581741333
Epoch: 31, Steps: 14 | Train Loss: 0.4087185 Vali Loss: 1.4722769 Test Loss: 0.6848940
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.7247862815856934
Epoch: 32, Steps: 14 | Train Loss: 0.4074900 Vali Loss: 1.4652127 Test Loss: 0.6841203
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.5946688652038574
Epoch: 33, Steps: 14 | Train Loss: 0.4052355 Vali Loss: 1.4704139 Test Loss: 0.6836510
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  94130176.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.016395330429077
Epoch: 1, Steps: 14 | Train Loss: 0.6068243 Vali Loss: 1.4087917 Test Loss: 0.6469332
Validation loss decreased (inf --> 1.408792).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.4761011600494385
Epoch: 2, Steps: 14 | Train Loss: 0.5808988 Vali Loss: 1.3757380 Test Loss: 0.6172324
Validation loss decreased (1.408792 --> 1.375738).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.3940677642822266
Epoch: 3, Steps: 14 | Train Loss: 0.5614682 Vali Loss: 1.3495384 Test Loss: 0.5940906
Validation loss decreased (1.375738 --> 1.349538).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.484703540802002
Epoch: 4, Steps: 14 | Train Loss: 0.5474835 Vali Loss: 1.3083414 Test Loss: 0.5753973
Validation loss decreased (1.349538 --> 1.308341).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.4445202350616455
Epoch: 5, Steps: 14 | Train Loss: 0.5340478 Vali Loss: 1.3069620 Test Loss: 0.5598722
Validation loss decreased (1.308341 --> 1.306962).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.363077163696289
Epoch: 6, Steps: 14 | Train Loss: 0.5235304 Vali Loss: 1.2919952 Test Loss: 0.5469030
Validation loss decreased (1.306962 --> 1.291995).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1295220851898193
Epoch: 7, Steps: 14 | Train Loss: 0.5160598 Vali Loss: 1.2779397 Test Loss: 0.5357965
Validation loss decreased (1.291995 --> 1.277940).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.20896053314209
Epoch: 8, Steps: 14 | Train Loss: 0.5088196 Vali Loss: 1.2665867 Test Loss: 0.5262408
Validation loss decreased (1.277940 --> 1.266587).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.993626594543457
Epoch: 9, Steps: 14 | Train Loss: 0.5031854 Vali Loss: 1.2654793 Test Loss: 0.5179829
Validation loss decreased (1.266587 --> 1.265479).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.095991849899292
Epoch: 10, Steps: 14 | Train Loss: 0.4978866 Vali Loss: 1.2537875 Test Loss: 0.5108640
Validation loss decreased (1.265479 --> 1.253788).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.870521068572998
Epoch: 11, Steps: 14 | Train Loss: 0.4920282 Vali Loss: 1.2405533 Test Loss: 0.5046586
Validation loss decreased (1.253788 --> 1.240553).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.563352584838867
Epoch: 12, Steps: 14 | Train Loss: 0.4873942 Vali Loss: 1.2386370 Test Loss: 0.4990262
Validation loss decreased (1.240553 --> 1.238637).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.7635347843170166
Epoch: 13, Steps: 14 | Train Loss: 0.4841532 Vali Loss: 1.2305238 Test Loss: 0.4941302
Validation loss decreased (1.238637 --> 1.230524).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.928083658218384
Epoch: 14, Steps: 14 | Train Loss: 0.4812680 Vali Loss: 1.2237256 Test Loss: 0.4898041
Validation loss decreased (1.230524 --> 1.223726).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.182255983352661
Epoch: 15, Steps: 14 | Train Loss: 0.4777032 Vali Loss: 1.2283101 Test Loss: 0.4859964
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.8348464965820312
Epoch: 16, Steps: 14 | Train Loss: 0.4751418 Vali Loss: 1.2162167 Test Loss: 0.4825709
Validation loss decreased (1.223726 --> 1.216217).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.064481735229492
Epoch: 17, Steps: 14 | Train Loss: 0.4743042 Vali Loss: 1.2157433 Test Loss: 0.4794495
Validation loss decreased (1.216217 --> 1.215743).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.948756217956543
Epoch: 18, Steps: 14 | Train Loss: 0.4711221 Vali Loss: 1.2107234 Test Loss: 0.4766325
Validation loss decreased (1.215743 --> 1.210723).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.0261991024017334
Epoch: 19, Steps: 14 | Train Loss: 0.4683970 Vali Loss: 1.2011337 Test Loss: 0.4741614
Validation loss decreased (1.210723 --> 1.201134).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.930616855621338
Epoch: 20, Steps: 14 | Train Loss: 0.4682589 Vali Loss: 1.2155279 Test Loss: 0.4719537
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.14683198928833
Epoch: 21, Steps: 14 | Train Loss: 0.4663145 Vali Loss: 1.2058327 Test Loss: 0.4699532
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.052415609359741
Epoch: 22, Steps: 14 | Train Loss: 0.4638222 Vali Loss: 1.2006571 Test Loss: 0.4681690
Validation loss decreased (1.201134 --> 1.200657).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.3279757499694824
Epoch: 23, Steps: 14 | Train Loss: 0.4636656 Vali Loss: 1.2008973 Test Loss: 0.4664640
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.287954092025757
Epoch: 24, Steps: 14 | Train Loss: 0.4621611 Vali Loss: 1.2072163 Test Loss: 0.4649589
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.3706955909729004
Epoch: 25, Steps: 14 | Train Loss: 0.4618015 Vali Loss: 1.1976097 Test Loss: 0.4636370
Validation loss decreased (1.200657 --> 1.197610).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.368234634399414
Epoch: 26, Steps: 14 | Train Loss: 0.4613713 Vali Loss: 1.1955385 Test Loss: 0.4623652
Validation loss decreased (1.197610 --> 1.195539).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.5554656982421875
Epoch: 27, Steps: 14 | Train Loss: 0.4587738 Vali Loss: 1.1998088 Test Loss: 0.4612362
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.5111656188964844
Epoch: 28, Steps: 14 | Train Loss: 0.4595335 Vali Loss: 1.1936209 Test Loss: 0.4601606
Validation loss decreased (1.195539 --> 1.193621).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.4547903537750244
Epoch: 29, Steps: 14 | Train Loss: 0.4578179 Vali Loss: 1.2035172 Test Loss: 0.4592105
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.606365919113159
Epoch: 30, Steps: 14 | Train Loss: 0.4571939 Vali Loss: 1.1938984 Test Loss: 0.4583323
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.6337618827819824
Epoch: 31, Steps: 14 | Train Loss: 0.4564342 Vali Loss: 1.1908337 Test Loss: 0.4575558
Validation loss decreased (1.193621 --> 1.190834).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.6565258502960205
Epoch: 32, Steps: 14 | Train Loss: 0.4564445 Vali Loss: 1.1839592 Test Loss: 0.4568125
Validation loss decreased (1.190834 --> 1.183959).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.4392807483673096
Epoch: 33, Steps: 14 | Train Loss: 0.4560457 Vali Loss: 1.1917243 Test Loss: 0.4561339
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.4991872310638428
Epoch: 34, Steps: 14 | Train Loss: 0.4552112 Vali Loss: 1.1952977 Test Loss: 0.4555140
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.6510887145996094
Epoch: 35, Steps: 14 | Train Loss: 0.4563063 Vali Loss: 1.1950605 Test Loss: 0.4548955
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.45584845542907715, mae:0.45385342836380005, rse:0.6427799463272095, corr:[0.24564767 0.25975212 0.25781676 0.2522119  0.25146845 0.25246373
 0.2526828  0.25187677 0.25080338 0.25027826 0.2499235  0.24957906
 0.24937066 0.24930431 0.24927427 0.24869989 0.24752977 0.24639794
 0.24596475 0.24620304 0.24641177 0.24607523 0.24518351 0.2445474
 0.24479015 0.24555582 0.24600665 0.24573085 0.24495302 0.24424621
 0.24400271 0.24403667 0.24422565 0.24424376 0.24392939 0.24352601
 0.24316686 0.24290304 0.24290647 0.24319597 0.2437552  0.24410242
 0.24390024 0.24348606 0.24343418 0.24412186 0.24522245 0.24597488
 0.24585493 0.24519555 0.24421975 0.24331526 0.24255726 0.24166107
 0.24073985 0.2399844  0.23943664 0.23911722 0.23887332 0.23877431
 0.23863918 0.23845305 0.23818678 0.2381187  0.23824508 0.23857684
 0.23907104 0.23936397 0.23954415 0.23949157 0.23914917 0.23872632
 0.2384287  0.23820822 0.23791167 0.23755407 0.23690198 0.2360844
 0.23544857 0.23522443 0.2352129  0.2350641  0.23459499 0.23403504
 0.23358154 0.23331608 0.233195   0.23317842 0.23310359 0.23287305
 0.23243397 0.23195247 0.231592   0.2316627  0.23210174 0.23279124
 0.23354901 0.23412797 0.23439565 0.23456876 0.23481278 0.23503368
 0.23512176 0.23494296 0.23459873 0.23424175 0.23397766 0.23387143
 0.23392592 0.23403433 0.23408094 0.23418069 0.23427767 0.23438571
 0.23448618 0.23449156 0.23440814 0.23429728 0.23424424 0.23432279
 0.23437865 0.23399656 0.23315765 0.2322033  0.23134486 0.23056899
 0.23005761 0.22995302 0.22991718 0.22975904 0.2293406  0.22877887
 0.22832519 0.22818783 0.22843474 0.22885701 0.2291613  0.22912881
 0.22874461 0.22830644 0.22813314 0.22819641 0.22823238 0.22807315
 0.22762115 0.22683673 0.22580072 0.22473264 0.22405979 0.22370929
 0.22357051 0.22349788 0.22340375 0.22323172 0.22304586 0.22304922
 0.22312365 0.22309229 0.22291759 0.22268254 0.222403   0.22247928
 0.22277711 0.22313161 0.22334354 0.22326435 0.22272098 0.2219663
 0.22148031 0.2215537  0.2219101  0.2221757  0.22194658 0.22132465
 0.22067016 0.22051345 0.22075513 0.2211786  0.2212913  0.22100267
 0.2204255  0.2199658  0.21993299 0.22036693 0.22103174 0.22155198
 0.22174338 0.22183965 0.22214136 0.22264613 0.22307603 0.22316152
 0.22278176 0.2219882  0.22085682 0.21964955 0.21881168 0.21835344
 0.21810746 0.21797128 0.21776369 0.21744762 0.21716557 0.21729305
 0.21769044 0.21832545 0.21889667 0.21917129 0.21927181 0.21929902
 0.21939558 0.21945095 0.21939561 0.21909659 0.21858475 0.21807054
 0.21766758 0.21747401 0.21733244 0.21720324 0.21713409 0.2167365
 0.2162022  0.2159422  0.21608184 0.21629113 0.2161386  0.21581344
 0.21541995 0.21513149 0.21524304 0.21567382 0.21594752 0.21600603
 0.21583463 0.21552423 0.2152776  0.21524832 0.21559152 0.21596268
 0.2162608  0.21629047 0.21613795 0.21583621 0.21548867 0.21533816
 0.215498   0.21584675 0.21599253 0.21567361 0.21503186 0.21458478
 0.21453278 0.21463625 0.21484807 0.21495207 0.21501729 0.21506827
 0.21527334 0.21552372 0.21597932 0.2163496  0.21635777 0.21583818
 0.21521564 0.21491265 0.21485035 0.21475302 0.21407816 0.21298368
 0.21216054 0.2118517  0.21189679 0.21214107 0.21210189 0.21182737
 0.2116326  0.2116832  0.211802   0.21195476 0.21220067 0.21259639
 0.2128271  0.21254098 0.21200582 0.21151383 0.2115117  0.21183589
 0.21211518 0.21209237 0.21135181 0.21043055 0.20977452 0.20992501
 0.21048975 0.21080774 0.21060102 0.21033727 0.21033283 0.21061951
 0.21091273 0.21098281 0.21125148 0.21149173 0.2115953  0.21135876
 0.21124826 0.21118654 0.2115723  0.21165358 0.21087535 0.20941852
 0.20828344 0.20802377 0.20809427 0.20766771 0.20642273 0.20452775
 0.20313253 0.20271921 0.2029738  0.20288278 0.20206083 0.20132586
 0.20134749 0.20105743 0.19976561 0.19877481 0.19869676 0.19939624
 0.19822218 0.19287778 0.18676014 0.18669048 0.19217579 0.18075705]
