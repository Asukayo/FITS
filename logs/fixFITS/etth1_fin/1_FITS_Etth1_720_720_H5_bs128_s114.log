Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  97574400.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.8753228187561035
Epoch: 1, Steps: 28 | Train Loss: 1.0081640 Vali Loss: 2.1213045 Test Loss: 0.8908594
Validation loss decreased (inf --> 2.121305).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.760390281677246
Epoch: 2, Steps: 28 | Train Loss: 0.8304162 Vali Loss: 1.8725905 Test Loss: 0.7474183
Validation loss decreased (2.121305 --> 1.872591).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.87532114982605
Epoch: 3, Steps: 28 | Train Loss: 0.7550380 Vali Loss: 1.7747532 Test Loss: 0.6860392
Validation loss decreased (1.872591 --> 1.774753).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.786718368530273
Epoch: 4, Steps: 28 | Train Loss: 0.7186618 Vali Loss: 1.7294258 Test Loss: 0.6516789
Validation loss decreased (1.774753 --> 1.729426).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.122764587402344
Epoch: 5, Steps: 28 | Train Loss: 0.6959800 Vali Loss: 1.6910956 Test Loss: 0.6267079
Validation loss decreased (1.729426 --> 1.691096).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.8710949420928955
Epoch: 6, Steps: 28 | Train Loss: 0.6790687 Vali Loss: 1.6605802 Test Loss: 0.6067178
Validation loss decreased (1.691096 --> 1.660580).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.887002229690552
Epoch: 7, Steps: 28 | Train Loss: 0.6655961 Vali Loss: 1.6351712 Test Loss: 0.5898331
Validation loss decreased (1.660580 --> 1.635171).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.96831488609314
Epoch: 8, Steps: 28 | Train Loss: 0.6543780 Vali Loss: 1.6100869 Test Loss: 0.5750641
Validation loss decreased (1.635171 --> 1.610087).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.0387396812438965
Epoch: 9, Steps: 28 | Train Loss: 0.6449089 Vali Loss: 1.5971644 Test Loss: 0.5623751
Validation loss decreased (1.610087 --> 1.597164).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.760381460189819
Epoch: 10, Steps: 28 | Train Loss: 0.6369395 Vali Loss: 1.5793905 Test Loss: 0.5512282
Validation loss decreased (1.597164 --> 1.579391).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.481142520904541
Epoch: 11, Steps: 28 | Train Loss: 0.6297546 Vali Loss: 1.5683321 Test Loss: 0.5412641
Validation loss decreased (1.579391 --> 1.568332).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.76837944984436
Epoch: 12, Steps: 28 | Train Loss: 0.6241968 Vali Loss: 1.5572970 Test Loss: 0.5325143
Validation loss decreased (1.568332 --> 1.557297).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.536381006240845
Epoch: 13, Steps: 28 | Train Loss: 0.6184899 Vali Loss: 1.5472791 Test Loss: 0.5248192
Validation loss decreased (1.557297 --> 1.547279).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.048053026199341
Epoch: 14, Steps: 28 | Train Loss: 0.6139467 Vali Loss: 1.5388944 Test Loss: 0.5178260
Validation loss decreased (1.547279 --> 1.538894).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.726315259933472
Epoch: 15, Steps: 28 | Train Loss: 0.6096772 Vali Loss: 1.5305688 Test Loss: 0.5116726
Validation loss decreased (1.538894 --> 1.530569).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.238083600997925
Epoch: 16, Steps: 28 | Train Loss: 0.6059055 Vali Loss: 1.5256947 Test Loss: 0.5061272
Validation loss decreased (1.530569 --> 1.525695).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.627094745635986
Epoch: 17, Steps: 28 | Train Loss: 0.6028431 Vali Loss: 1.5163434 Test Loss: 0.5011927
Validation loss decreased (1.525695 --> 1.516343).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.5315492153167725
Epoch: 18, Steps: 28 | Train Loss: 0.5996403 Vali Loss: 1.5107076 Test Loss: 0.4966388
Validation loss decreased (1.516343 --> 1.510708).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.479238271713257
Epoch: 19, Steps: 28 | Train Loss: 0.5971945 Vali Loss: 1.5036464 Test Loss: 0.4925424
Validation loss decreased (1.510708 --> 1.503646).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.2287514209747314
Epoch: 20, Steps: 28 | Train Loss: 0.5945600 Vali Loss: 1.5021853 Test Loss: 0.4889682
Validation loss decreased (1.503646 --> 1.502185).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.4918365478515625
Epoch: 21, Steps: 28 | Train Loss: 0.5926887 Vali Loss: 1.4982046 Test Loss: 0.4856785
Validation loss decreased (1.502185 --> 1.498205).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.172882795333862
Epoch: 22, Steps: 28 | Train Loss: 0.5905849 Vali Loss: 1.4931097 Test Loss: 0.4826411
Validation loss decreased (1.498205 --> 1.493110).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 5.505520343780518
Epoch: 23, Steps: 28 | Train Loss: 0.5884330 Vali Loss: 1.4913312 Test Loss: 0.4799485
Validation loss decreased (1.493110 --> 1.491331).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.880176782608032
Epoch: 24, Steps: 28 | Train Loss: 0.5870930 Vali Loss: 1.4831599 Test Loss: 0.4774944
Validation loss decreased (1.491331 --> 1.483160).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.785506010055542
Epoch: 25, Steps: 28 | Train Loss: 0.5854254 Vali Loss: 1.4773942 Test Loss: 0.4752161
Validation loss decreased (1.483160 --> 1.477394).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.884618043899536
Epoch: 26, Steps: 28 | Train Loss: 0.5838691 Vali Loss: 1.4774665 Test Loss: 0.4731790
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.9220993518829346
Epoch: 27, Steps: 28 | Train Loss: 0.5826065 Vali Loss: 1.4772021 Test Loss: 0.4712540
Validation loss decreased (1.477394 --> 1.477202).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.7328221797943115
Epoch: 28, Steps: 28 | Train Loss: 0.5816297 Vali Loss: 1.4743195 Test Loss: 0.4696360
Validation loss decreased (1.477202 --> 1.474319).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 5.0765697956085205
Epoch: 29, Steps: 28 | Train Loss: 0.5804768 Vali Loss: 1.4693114 Test Loss: 0.4680248
Validation loss decreased (1.474319 --> 1.469311).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.939509391784668
Epoch: 30, Steps: 28 | Train Loss: 0.5792132 Vali Loss: 1.4688685 Test Loss: 0.4665677
Validation loss decreased (1.469311 --> 1.468868).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 5.179957151412964
Epoch: 31, Steps: 28 | Train Loss: 0.5784865 Vali Loss: 1.4701750 Test Loss: 0.4652544
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.833385229110718
Epoch: 32, Steps: 28 | Train Loss: 0.5777268 Vali Loss: 1.4642100 Test Loss: 0.4640302
Validation loss decreased (1.468868 --> 1.464210).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.905087232589722
Epoch: 33, Steps: 28 | Train Loss: 0.5770310 Vali Loss: 1.4626944 Test Loss: 0.4628983
Validation loss decreased (1.464210 --> 1.462694).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.865915536880493
Epoch: 34, Steps: 28 | Train Loss: 0.5766121 Vali Loss: 1.4686483 Test Loss: 0.4618464
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.72021484375
Epoch: 35, Steps: 28 | Train Loss: 0.5755331 Vali Loss: 1.4605651 Test Loss: 0.4608844
Validation loss decreased (1.462694 --> 1.460565).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.92731785774231
Epoch: 36, Steps: 28 | Train Loss: 0.5751826 Vali Loss: 1.4625449 Test Loss: 0.4600259
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.531362295150757
Epoch: 37, Steps: 28 | Train Loss: 0.5742344 Vali Loss: 1.4625450 Test Loss: 0.4591622
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.901384115219116
Epoch: 38, Steps: 28 | Train Loss: 0.5734979 Vali Loss: 1.4618027 Test Loss: 0.4584548
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4519226551055908, mae:0.47500744462013245, rse:0.6435526013374329, corr:[0.20769967 0.2320514  0.22683427 0.22492868 0.23047175 0.23259155
 0.23142435 0.23077416 0.23175815 0.23240598 0.23166837 0.23046659
 0.2293771  0.22814138 0.22703359 0.22639684 0.22607905 0.22493732
 0.22331226 0.22262172 0.22337997 0.22376567 0.22296357 0.2217474
 0.22206879 0.22362345 0.22477849 0.22487436 0.22487453 0.22558859
 0.22634485 0.22645453 0.2260486  0.22563297 0.2253607  0.22502415
 0.22464697 0.22426708 0.22395974 0.22370648 0.22343734 0.22328049
 0.22335257 0.2235725  0.22375475 0.2241336  0.22472225 0.22514874
 0.22541456 0.22517963 0.22482644 0.22446328 0.22382289 0.22301684
 0.22245899 0.2220128  0.22194184 0.2215223  0.22098872 0.22055787
 0.22019231 0.21973893 0.21935719 0.21923666 0.21929057 0.21934472
 0.2192357  0.21916473 0.21942213 0.21954504 0.21914193 0.21859592
 0.21818669 0.21795975 0.21782252 0.21762225 0.21747266 0.217397
 0.21722269 0.21687791 0.21650675 0.2161701  0.21579055 0.21531707
 0.21467008 0.21406421 0.21349277 0.21313295 0.2128706  0.21265285
 0.21250094 0.21261828 0.2127533  0.21295366 0.21320377 0.21412097
 0.21554609 0.21670678 0.21751155 0.2182236  0.21872494 0.21886227
 0.21879217 0.21877053 0.2187677  0.21840984 0.21774262 0.21719956
 0.2167929  0.21661122 0.2163866  0.21612443 0.21593969 0.21580862
 0.21567178 0.2155261  0.21556094 0.21566433 0.21559988 0.21559234
 0.21570249 0.21569544 0.2154805  0.2152022  0.21483149 0.2145448
 0.21437696 0.21420835 0.21402593 0.21373077 0.2134698  0.21323344
 0.21297155 0.2125762  0.21217352 0.2119093  0.211601   0.21135938
 0.21119379 0.21115178 0.21109597 0.21083897 0.21034405 0.21029325
 0.21044834 0.2103615  0.21009883 0.20993307 0.20986988 0.20974727
 0.20955841 0.20951666 0.20954517 0.2092607  0.20874289 0.20848224
 0.20832467 0.20802547 0.20751299 0.20746851 0.20791    0.20838957
 0.20837222 0.20803894 0.20785615 0.20807767 0.20772202 0.20727266
 0.20718235 0.20772998 0.20851596 0.20919316 0.20942657 0.20943479
 0.20947835 0.20952287 0.20932978 0.20901987 0.2088344  0.20873506
 0.20852831 0.20835714 0.20854959 0.20897013 0.20924234 0.2092291
 0.20928004 0.2095966  0.20981048 0.20942123 0.20887157 0.2088014
 0.20903437 0.20883432 0.20816661 0.20764871 0.20766607 0.20788316
 0.20794523 0.20793043 0.20794675 0.20776966 0.20756231 0.20754476
 0.20795284 0.20848814 0.20867486 0.20857105 0.20853353 0.20858662
 0.2083307  0.20768268 0.2070397  0.20662896 0.2062649  0.20602916
 0.20602706 0.20629862 0.20671172 0.20698856 0.20694797 0.2068521
 0.20698194 0.20708731 0.20673874 0.20609742 0.20556778 0.20540754
 0.20522587 0.20495446 0.20474812 0.20467946 0.20448248 0.20413677
 0.20388101 0.20374577 0.20347193 0.20304765 0.20278871 0.20301895
 0.20370561 0.20427407 0.20461725 0.2048698  0.20505007 0.20505412
 0.20500477 0.20494764 0.20481955 0.20436645 0.20371297 0.20327154
 0.20319642 0.2032877  0.20335658 0.2033424  0.20356481 0.2039569
 0.20404872 0.20364505 0.20333326 0.20334631 0.20343181 0.20333013
 0.20329775 0.2035394  0.20390023 0.20405115 0.20379491 0.20351736
 0.20349765 0.20339124 0.2030762  0.20281474 0.2028075  0.20293424
 0.20283762 0.2027351  0.20251805 0.20237115 0.20208314 0.20184238
 0.20168655 0.201852   0.20172027 0.20114566 0.20058823 0.20092599
 0.20193149 0.20277649 0.20323455 0.2036846  0.20427683 0.20463856
 0.20453037 0.20429426 0.20428087 0.20447847 0.20455478 0.20453495
 0.20449795 0.20443296 0.20432882 0.20442365 0.20474252 0.20506045
 0.2051701  0.2048766  0.20467438 0.2048118  0.2048892  0.20486166
 0.20506968 0.20557626 0.20612359 0.20622003 0.20561853 0.20490848
 0.20457637 0.20434545 0.20380239 0.2031599  0.20299654 0.20317249
 0.20322561 0.20303114 0.20299074 0.20329745 0.20345394 0.20341167
 0.20349702 0.20378079 0.20392632 0.20372087 0.20334908 0.20338777
 0.20368958 0.20362073 0.20321563 0.20290758 0.20271815 0.20236142
 0.20173237 0.20106734 0.20070112 0.20070773 0.20062304 0.20042698
 0.20038372 0.20055059 0.20051104 0.20029485 0.20030063 0.20077759
 0.20098846 0.20076631 0.20058197 0.20078708 0.20079726 0.20056456
 0.20046738 0.20063123 0.2008492  0.20067596 0.20002645 0.19935423
 0.19907498 0.19882095 0.1981495  0.1974557  0.19730821 0.1973878
 0.19683309 0.19590794 0.19547482 0.19556898 0.19546919 0.19491352
 0.19451654 0.19445218 0.194326   0.19398697 0.19371197 0.19446117
 0.19596834 0.19719893 0.19786257 0.19833736 0.19858913 0.19797012
 0.1968298  0.19593988 0.19556345 0.19508377 0.1940335  0.19324291
 0.19333924 0.19380283 0.19387007 0.19353916 0.19370899 0.19441482
 0.19472681 0.19420175 0.1938788  0.1944399  0.19502698 0.19499554
 0.19482076 0.195423   0.19672734 0.19762912 0.1976163  0.19707254
 0.1969315  0.19686352 0.19639699 0.19573231 0.1954865  0.19558324
 0.1953638  0.1950034  0.19489092 0.19517675 0.19512701 0.19457997
 0.19421184 0.19458131 0.1948051  0.19469234 0.19445361 0.19490999
 0.19604915 0.1968257  0.19693841 0.1972117  0.19774601 0.19780056
 0.19727321 0.19672552 0.19651926 0.1966485  0.19652349 0.1963657
 0.19643672 0.1964768  0.19605869 0.19525494 0.19494513 0.19521658
 0.19524397 0.19464381 0.19428362 0.19479679 0.19561742 0.1960021
 0.19608158 0.19659242 0.1974589  0.1976664  0.19694744 0.1960396
 0.19590344 0.19603188 0.19570631 0.19547826 0.1957616  0.19635698
 0.19640428 0.19609289 0.19620904 0.1967756  0.19689113 0.19647487
 0.19620197 0.19640292 0.19627178 0.19526975 0.19455291 0.19497913
 0.1960005  0.19627793 0.19567728 0.19546914 0.19598246 0.19609976
 0.19533385 0.19451986 0.19428669 0.19453596 0.19445051 0.19412327
 0.1941998  0.19442685 0.19404733 0.19314486 0.19286695 0.19344896
 0.19366983 0.19279586 0.19197543 0.19246332 0.19359165 0.1941274
 0.19417378 0.19487993 0.19629773 0.19732876 0.1971472  0.19618055
 0.19569243 0.1957535  0.19529295 0.19429609 0.19397989 0.19454665
 0.19489445 0.19466959 0.1948335  0.19541523 0.19595563 0.19588105
 0.19567595 0.19604547 0.19694385 0.19693337 0.19687259 0.1968382
 0.19745713 0.19760774 0.19706832 0.19700311 0.19736576 0.19756146
 0.19691814 0.196153   0.19603491 0.19646528 0.19626358 0.19562727
 0.19546433 0.19598031 0.19668576 0.19688535 0.19720297 0.1976944
 0.19773975 0.19704945 0.19641955 0.19663608 0.19701034 0.1963623
 0.19499521 0.19447108 0.19486402 0.19487137 0.19385685 0.19280033
 0.192694   0.19299452 0.19242977 0.19137478 0.19101654 0.19142549
 0.19129135 0.1901631  0.18962091 0.18963565 0.18989213 0.18961275
 0.18953936 0.18979914 0.18993086 0.18913953 0.18802673 0.18762979
 0.18785907 0.18776104 0.18693095 0.18695791 0.18780412 0.18792787
 0.18707876 0.18593079 0.18596697 0.18655124 0.18667679 0.1862217
 0.18630923 0.18672368 0.18678503 0.18628362 0.1862758  0.18703884
 0.18740447 0.18669097 0.1859949  0.1864413  0.1869849  0.18611437
 0.1845528  0.18401876 0.18461378 0.1846914  0.18342611 0.181877
 0.18158095 0.18190536 0.18113875 0.17989644 0.17954376 0.18017717
 0.180239   0.17939733 0.1791295  0.17975175 0.18005385 0.17942102
 0.17912036 0.1799914  0.1809068  0.18051553 0.17968227 0.179892
 0.18075624 0.18063852 0.17926958 0.17857957 0.1791998  0.17955858
 0.17875658 0.17753896 0.17713812 0.17748769 0.17739098 0.17715544
 0.17754868 0.17787908 0.17742342 0.17646335 0.1763306  0.17713128
 0.17728236 0.17625171 0.17556538 0.17640236 0.17760019 0.17726934
 0.17580874 0.1753995  0.1763724  0.17727113 0.17647134 0.17505084
 0.1746951  0.17483073 0.17410891 0.1728361  0.1723406  0.17281672
 0.17295799 0.1723266  0.17212392 0.17249918 0.17249742 0.17202625
 0.172166   0.1730739  0.17292115 0.17125247 0.16990587 0.17038542
 0.17065687 0.16875675 0.16584992 0.16459931 0.16509886 0.16418989
 0.16193332 0.16036959 0.16106382 0.16179147 0.16134524 0.16093716
 0.1613795  0.16146402 0.16067123 0.16007437 0.16090924 0.16211565
 0.16150583 0.15982862 0.15994407 0.16163675 0.16157293 0.15827332
 0.15516806 0.15606135 0.15818909 0.15771896 0.15448897 0.15253693
 0.1541927  0.15554562 0.15416577 0.1521878  0.15213218 0.15242688
 0.15126704 0.14966062 0.14950477 0.14883974 0.14579983 0.14433861
 0.14596291 0.14344105 0.12932412 0.11940663 0.13230108 0.118908  ]
