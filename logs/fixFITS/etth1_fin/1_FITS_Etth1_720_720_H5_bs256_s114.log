Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  195148800.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0760085582733154
Epoch: 1, Steps: 14 | Train Loss: 1.0705613 Vali Loss: 2.3523676 Test Loss: 1.0240450
Validation loss decreased (inf --> 2.352368).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.311530590057373
Epoch: 2, Steps: 14 | Train Loss: 0.9377026 Vali Loss: 2.1134377 Test Loss: 0.8912830
Validation loss decreased (2.352368 --> 2.113438).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.248227596282959
Epoch: 3, Steps: 14 | Train Loss: 0.8533898 Vali Loss: 1.9680753 Test Loss: 0.8088140
Validation loss decreased (2.113438 --> 1.968075).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.5804123878479004
Epoch: 4, Steps: 14 | Train Loss: 0.8008460 Vali Loss: 1.8771873 Test Loss: 0.7573029
Validation loss decreased (1.968075 --> 1.877187).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.1201138496398926
Epoch: 5, Steps: 14 | Train Loss: 0.7673725 Vali Loss: 1.8256242 Test Loss: 0.7246383
Validation loss decreased (1.877187 --> 1.825624).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.9645118713378906
Epoch: 6, Steps: 14 | Train Loss: 0.7451480 Vali Loss: 1.7892487 Test Loss: 0.7021462
Validation loss decreased (1.825624 --> 1.789249).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.9780189990997314
Epoch: 7, Steps: 14 | Train Loss: 0.7287398 Vali Loss: 1.7578330 Test Loss: 0.6856322
Validation loss decreased (1.789249 --> 1.757833).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.254153251647949
Epoch: 8, Steps: 14 | Train Loss: 0.7164432 Vali Loss: 1.7405753 Test Loss: 0.6726006
Validation loss decreased (1.757833 --> 1.740575).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.3217554092407227
Epoch: 9, Steps: 14 | Train Loss: 0.7063473 Vali Loss: 1.7200003 Test Loss: 0.6616890
Validation loss decreased (1.740575 --> 1.720000).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.3176445960998535
Epoch: 10, Steps: 14 | Train Loss: 0.6985388 Vali Loss: 1.7013159 Test Loss: 0.6522029
Validation loss decreased (1.720000 --> 1.701316).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.4400107860565186
Epoch: 11, Steps: 14 | Train Loss: 0.6908583 Vali Loss: 1.6904767 Test Loss: 0.6438742
Validation loss decreased (1.701316 --> 1.690477).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.248307228088379
Epoch: 12, Steps: 14 | Train Loss: 0.6848045 Vali Loss: 1.6804481 Test Loss: 0.6364251
Validation loss decreased (1.690477 --> 1.680448).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.1020684242248535
Epoch: 13, Steps: 14 | Train Loss: 0.6790727 Vali Loss: 1.6722693 Test Loss: 0.6295635
Validation loss decreased (1.680448 --> 1.672269).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.3610894680023193
Epoch: 14, Steps: 14 | Train Loss: 0.6740324 Vali Loss: 1.6596398 Test Loss: 0.6233248
Validation loss decreased (1.672269 --> 1.659640).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.3307318687438965
Epoch: 15, Steps: 14 | Train Loss: 0.6693695 Vali Loss: 1.6527894 Test Loss: 0.6176013
Validation loss decreased (1.659640 --> 1.652789).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.0051028728485107
Epoch: 16, Steps: 14 | Train Loss: 0.6656822 Vali Loss: 1.6435511 Test Loss: 0.6122870
Validation loss decreased (1.652789 --> 1.643551).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.342132091522217
Epoch: 17, Steps: 14 | Train Loss: 0.6615684 Vali Loss: 1.6365120 Test Loss: 0.6075140
Validation loss decreased (1.643551 --> 1.636512).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.485442876815796
Epoch: 18, Steps: 14 | Train Loss: 0.6578313 Vali Loss: 1.6329306 Test Loss: 0.6028842
Validation loss decreased (1.636512 --> 1.632931).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.866482734680176
Epoch: 19, Steps: 14 | Train Loss: 0.6547151 Vali Loss: 1.6259377 Test Loss: 0.5987135
Validation loss decreased (1.632931 --> 1.625938).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.459089517593384
Epoch: 20, Steps: 14 | Train Loss: 0.6520191 Vali Loss: 1.6198746 Test Loss: 0.5948019
Validation loss decreased (1.625938 --> 1.619875).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.512927293777466
Epoch: 21, Steps: 14 | Train Loss: 0.6492515 Vali Loss: 1.6116316 Test Loss: 0.5911506
Validation loss decreased (1.619875 --> 1.611632).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.5236263275146484
Epoch: 22, Steps: 14 | Train Loss: 0.6468936 Vali Loss: 1.6101463 Test Loss: 0.5877393
Validation loss decreased (1.611632 --> 1.610146).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.489807367324829
Epoch: 23, Steps: 14 | Train Loss: 0.6444741 Vali Loss: 1.6049066 Test Loss: 0.5845232
Validation loss decreased (1.610146 --> 1.604907).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.6453394889831543
Epoch: 24, Steps: 14 | Train Loss: 0.6426506 Vali Loss: 1.6012609 Test Loss: 0.5815508
Validation loss decreased (1.604907 --> 1.601261).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2025818824768066
Epoch: 25, Steps: 14 | Train Loss: 0.6405689 Vali Loss: 1.5970650 Test Loss: 0.5787795
Validation loss decreased (1.601261 --> 1.597065).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.7804644107818604
Epoch: 26, Steps: 14 | Train Loss: 0.6391473 Vali Loss: 1.5958036 Test Loss: 0.5761808
Validation loss decreased (1.597065 --> 1.595804).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.602996349334717
Epoch: 27, Steps: 14 | Train Loss: 0.6370912 Vali Loss: 1.5882993 Test Loss: 0.5737516
Validation loss decreased (1.595804 --> 1.588299).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.6205320358276367
Epoch: 28, Steps: 14 | Train Loss: 0.6356589 Vali Loss: 1.5813544 Test Loss: 0.5714574
Validation loss decreased (1.588299 --> 1.581354).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.683814287185669
Epoch: 29, Steps: 14 | Train Loss: 0.6339787 Vali Loss: 1.5854570 Test Loss: 0.5692973
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.1776483058929443
Epoch: 30, Steps: 14 | Train Loss: 0.6326023 Vali Loss: 1.5844250 Test Loss: 0.5672714
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.334916591644287
Epoch: 31, Steps: 14 | Train Loss: 0.6317897 Vali Loss: 1.5780199 Test Loss: 0.5653451
Validation loss decreased (1.581354 --> 1.578020).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.2970144748687744
Epoch: 32, Steps: 14 | Train Loss: 0.6296619 Vali Loss: 1.5739009 Test Loss: 0.5635265
Validation loss decreased (1.578020 --> 1.573901).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.066986560821533
Epoch: 33, Steps: 14 | Train Loss: 0.6291838 Vali Loss: 1.5742004 Test Loss: 0.5618465
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.2424895763397217
Epoch: 34, Steps: 14 | Train Loss: 0.6278063 Vali Loss: 1.5721734 Test Loss: 0.5602515
Validation loss decreased (1.573901 --> 1.572173).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.376563549041748
Epoch: 35, Steps: 14 | Train Loss: 0.6270846 Vali Loss: 1.5696799 Test Loss: 0.5587187
Validation loss decreased (1.572173 --> 1.569680).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.0985255241394043
Epoch: 36, Steps: 14 | Train Loss: 0.6260614 Vali Loss: 1.5701175 Test Loss: 0.5573224
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.1677982807159424
Epoch: 37, Steps: 14 | Train Loss: 0.6251675 Vali Loss: 1.5649953 Test Loss: 0.5559451
Validation loss decreased (1.569680 --> 1.564995).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.130390167236328
Epoch: 38, Steps: 14 | Train Loss: 0.6243671 Vali Loss: 1.5629197 Test Loss: 0.5546746
Validation loss decreased (1.564995 --> 1.562920).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.1648857593536377
Epoch: 39, Steps: 14 | Train Loss: 0.6234676 Vali Loss: 1.5611826 Test Loss: 0.5534898
Validation loss decreased (1.562920 --> 1.561183).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.1546192169189453
Epoch: 40, Steps: 14 | Train Loss: 0.6223700 Vali Loss: 1.5627661 Test Loss: 0.5523778
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.0439658164978027
Epoch: 41, Steps: 14 | Train Loss: 0.6221833 Vali Loss: 1.5518305 Test Loss: 0.5512923
Validation loss decreased (1.561183 --> 1.551831).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.9743785858154297
Epoch: 42, Steps: 14 | Train Loss: 0.6212648 Vali Loss: 1.5575802 Test Loss: 0.5502721
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.071791648864746
Epoch: 43, Steps: 14 | Train Loss: 0.6206940 Vali Loss: 1.5570130 Test Loss: 0.5493271
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.0438506603240967
Epoch: 44, Steps: 14 | Train Loss: 0.6199958 Vali Loss: 1.5553827 Test Loss: 0.5484349
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.5262640714645386, mae:0.5194217562675476, rse:0.6944705843925476, corr:[0.1887814  0.22391146 0.21746224 0.21224506 0.22018442 0.22657312
 0.22727095 0.22613916 0.22674428 0.22742966 0.22700667 0.22624339
 0.22532526 0.2235256  0.22088405 0.21922551 0.21922536 0.21839459
 0.2154446  0.2125871  0.21259332 0.21374004 0.21305215 0.21006727
 0.20862858 0.21077435 0.214139   0.21571872 0.2157315  0.21615198
 0.2173356  0.21838365 0.21873055 0.21833605 0.21770856 0.21693876
 0.21638669 0.216243   0.21608831 0.21522944 0.21353364 0.21226898
 0.21232516 0.21294641 0.21259765 0.21181156 0.21172567 0.21278505
 0.2143114  0.21453159 0.21388616 0.21367101 0.21385114 0.21396859
 0.21366164 0.21271348 0.21209869 0.21159051 0.21158035 0.21148679
 0.21066305 0.20919389 0.20824951 0.20824334 0.20852038 0.20806585
 0.20669681 0.20562044 0.20598468 0.20648374 0.2055401  0.20339929
 0.20196837 0.2023672  0.20375675 0.20445278 0.20413183 0.20377569
 0.20387395 0.20427215 0.2042419  0.2033796  0.20205954 0.20120773
 0.20074764 0.2003571  0.1993681  0.1980594  0.19712572 0.19689295
 0.19704418 0.19687411 0.19579408 0.1946564  0.19450541 0.19627847
 0.19870882 0.2003104  0.20133197 0.20279057 0.20449384 0.20562194
 0.20571049 0.20528413 0.20490916 0.2046016  0.20401904 0.20299132
 0.20125811 0.1998325  0.19913809 0.19892102 0.19839361 0.19722894
 0.19595596 0.19535278 0.19580583 0.19633733 0.19567217 0.19441484
 0.19387303 0.1945057  0.19546217 0.19582517 0.19523622 0.19471486
 0.19503072 0.19593751 0.19651257 0.19592288 0.19490415 0.19453175
 0.1948923  0.19487384 0.19371597 0.19187757 0.19027522 0.18984735
 0.18999334 0.18978225 0.18895742 0.18797319 0.18739654 0.18787602
 0.18823443 0.18778583 0.18729816 0.18789525 0.18932234 0.19058292
 0.19095826 0.19057876 0.19011234 0.18994276 0.19000928 0.19012924
 0.18940288 0.18826847 0.18765575 0.18842244 0.18963668 0.18988046
 0.1889037  0.18773034 0.18738961 0.18775655 0.18661022 0.18488525
 0.18415914 0.18535095 0.18730111 0.18878402 0.18917638 0.18914053
 0.18948373 0.1902091  0.19049282 0.19010171 0.18962465 0.18960047
 0.18982379 0.18989918 0.18983129 0.1897578  0.18987887 0.19020765
 0.1906711  0.19085842 0.19027112 0.188996   0.18821283 0.18849231
 0.1889506  0.18864724 0.18802692 0.18815057 0.1893712  0.19080499
 0.19152276 0.19159174 0.19169551 0.19207968 0.19259752 0.19284366
 0.19307764 0.19360447 0.19421907 0.19468492 0.19484869 0.19447272
 0.19359179 0.1926669  0.19213705 0.19169284 0.19073373 0.1898173
 0.18981479 0.19079407 0.19200628 0.1927698  0.19282988 0.19277404
 0.19317453 0.19373737 0.1934799  0.19223765 0.19081329 0.1904143
 0.19064559 0.1908929  0.19059789 0.18983766 0.18908477 0.18871175
 0.18854788 0.18790784 0.18655801 0.18530358 0.18492784 0.18534592
 0.1861548  0.18684763 0.18778467 0.18901859 0.1902211  0.19091785
 0.1910372  0.19071476 0.19023053 0.18972819 0.18925375 0.1887872
 0.1880731  0.18762186 0.18778497 0.18836178 0.18878147 0.1885635
 0.1878302  0.18722211 0.18728638 0.18740281 0.1868291  0.18561773
 0.18520685 0.18619081 0.18774179 0.1887407  0.18872616 0.1884286
 0.1884457  0.1883441  0.18787527 0.18718174 0.18666962 0.18669735
 0.18686418 0.18694136 0.18613897 0.18497127 0.18395726 0.18372026
 0.18368225 0.18354335 0.18241255 0.18090956 0.17999144 0.18040298
 0.18138996 0.18221493 0.18308973 0.18448047 0.18624528 0.18756174
 0.1877163  0.18705212 0.18655436 0.1868859  0.18745518 0.18751161
 0.18667194 0.18563722 0.18535501 0.1860844  0.18679243 0.186742
 0.18636519 0.18608189 0.186275   0.18643    0.18570209 0.18462263
 0.1845965  0.18589395 0.18777479 0.18890339 0.18858984 0.1878781
 0.18767495 0.1876603  0.18707502 0.18585576 0.18496549 0.18494524
 0.18554166 0.1856546  0.1850654  0.18440706 0.1841325  0.18466079
 0.18543935 0.18552017 0.18471156 0.18379334 0.18346255 0.1839914
 0.18442522 0.18401325 0.1833044  0.18325321 0.1837623  0.1841005
 0.18361895 0.18229824 0.18092349 0.1806193  0.18074186 0.18088147
 0.18066151 0.18020746 0.1797094  0.17953329 0.1796722  0.17973024
 0.17900631 0.17797124 0.17733301 0.17694445 0.17598964 0.17510565
 0.1755056  0.17681874 0.17791232 0.17800222 0.17752545 0.17744175
 0.17819196 0.17878468 0.17823309 0.17711584 0.17670499 0.1771334
 0.17707181 0.17615762 0.1750385  0.17439048 0.17425959 0.17442419
 0.17454532 0.17372973 0.17178829 0.16982517 0.16899714 0.16995862
 0.17170851 0.17314006 0.17466623 0.17673427 0.17868395 0.17897505
 0.17766552 0.17596321 0.17491117 0.17426525 0.1732791  0.17242827
 0.17205812 0.17203952 0.1723312  0.1728725  0.17364828 0.1739301
 0.173346   0.17256556 0.17284864 0.17359845 0.17299394 0.17112821
 0.1704269  0.17246433 0.17576233 0.17767096 0.17771968 0.17696095
 0.17695329 0.17710619 0.17662027 0.17548953 0.17472932 0.17489378
 0.17535628 0.17567974 0.17539664 0.17485048 0.17429574 0.1741292
 0.17429604 0.17425911 0.17303023 0.17159748 0.17123787 0.17245519
 0.17429443 0.17547949 0.17617923 0.17769618 0.17960821 0.18056577
 0.18047412 0.18002313 0.17986764 0.18037674 0.18062826 0.18041743
 0.17978206 0.17887767 0.17820454 0.17792076 0.17814925 0.17818588
 0.17746915 0.17658179 0.17667527 0.17735837 0.17733738 0.1764149
 0.17628871 0.1782251  0.18091048 0.18222722 0.18189922 0.18110842
 0.18133676 0.18184172 0.1816412  0.18142162 0.18171142 0.18254283
 0.18303034 0.18308131 0.18317808 0.18336883 0.18322584 0.18301637
 0.18294933 0.18272975 0.18158573 0.17980286 0.17905416 0.17991993
 0.18113968 0.18112409 0.18033153 0.18067724 0.1822587  0.18304418
 0.1824907  0.18161859 0.18135534 0.18186373 0.18208681 0.18171108
 0.18136366 0.18110463 0.18060496 0.1799473  0.1797991  0.18001263
 0.17943996 0.17786844 0.17684762 0.17721057 0.17762914 0.17709234
 0.17673342 0.17821683 0.18088354 0.18284865 0.18308996 0.18231745
 0.18205534 0.18253724 0.18227518 0.18087156 0.18007872 0.18064378
 0.18134227 0.1812497  0.18097718 0.18092315 0.18127075 0.18134937
 0.18084553 0.1800921  0.17963941 0.1789784  0.17902589 0.17892049
 0.1789528  0.17817104 0.17716758 0.17782156 0.17938092 0.1802572
 0.17944956 0.17802888 0.1777957  0.17880958 0.17908397 0.17830797
 0.17758584 0.1777894  0.17879516 0.17926525 0.17925715 0.17893209
 0.17846578 0.17810434 0.17811015 0.17828746 0.17761192 0.1755946
 0.17384158 0.17436227 0.17596626 0.176138   0.17452383 0.17324767
 0.17402901 0.17553255 0.17533435 0.17366561 0.17275159 0.17364551
 0.17441192 0.17329478 0.17171663 0.17046371 0.17046686 0.17076519
 0.17081425 0.16984019 0.1678692  0.16562878 0.16438648 0.16444431
 0.16446988 0.16329822 0.16151513 0.16213976 0.16469488 0.16607027
 0.16526537 0.16332494 0.16331041 0.16513354 0.16674559 0.16668342
 0.16615625 0.16611299 0.16660604 0.1669597  0.16716108 0.16716324
 0.1665459  0.16550466 0.16508634 0.16549753 0.1650469  0.16289699
 0.16121909 0.16193901 0.16393323 0.16432172 0.16254283 0.16057287
 0.16096926 0.16255715 0.16225728 0.16062109 0.1599432  0.16122341
 0.16246654 0.16234888 0.16191792 0.16203818 0.1622728  0.16229092
 0.1626795  0.16330227 0.16295804 0.16144435 0.16072826 0.16169162
 0.1625128  0.1613286  0.15897924 0.15878429 0.16109642 0.16268675
 0.16194788 0.16036178 0.1602908  0.16164799 0.16226618 0.16204895
 0.16219115 0.16238236 0.16223827 0.16186075 0.16186155 0.16205525
 0.16129634 0.15997735 0.15978438 0.16073826 0.16063441 0.15830377
 0.15621263 0.15714264 0.1596991  0.16087624 0.15931468 0.15778327
 0.15904477 0.16118526 0.16125938 0.15992387 0.15935439 0.16028665
 0.16114467 0.16095601 0.16064514 0.1605964  0.16035953 0.1604462
 0.16129337 0.16175972 0.15986739 0.15708716 0.15672599 0.1587757
 0.15889622 0.15541449 0.15201141 0.15266892 0.15573715 0.15542868
 0.15257423 0.1512617  0.15374213 0.1558035  0.15551122 0.15472394
 0.15484636 0.15449011 0.15363593 0.15374403 0.15516652 0.15586491
 0.15406376 0.15262575 0.15413931 0.15585794 0.15340832 0.14813222
 0.14643988 0.15039045 0.15300949 0.15066381 0.14709494 0.14780065
 0.15205969 0.15305674 0.15095651 0.14977813 0.15064566 0.1499784
 0.14824638 0.14740813 0.14723063 0.1439897  0.14042442 0.14180398
 0.14352037 0.13311628 0.1146595  0.11436183 0.12718298 0.07038681]
