Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13823040.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4039771
	speed: 0.1353s/iter; left time: 812.0276s
Epoch: 1 cost time: 16.409951210021973
Epoch: 1, Steps: 122 | Train Loss: 0.4618777 Vali Loss: 0.8082722 Test Loss: 0.3949888
Validation loss decreased (inf --> 0.808272).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3497860
	speed: 0.3628s/iter; left time: 2132.9333s
Epoch: 2 cost time: 17.75360417366028
Epoch: 2, Steps: 122 | Train Loss: 0.3570412 Vali Loss: 0.7584181 Test Loss: 0.3843449
Validation loss decreased (0.808272 --> 0.758418).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3193838
	speed: 0.3670s/iter; left time: 2112.8262s
Epoch: 3 cost time: 16.615922927856445
Epoch: 3, Steps: 122 | Train Loss: 0.3468457 Vali Loss: 0.7354593 Test Loss: 0.3822616
Validation loss decreased (0.758418 --> 0.735459).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3820716
	speed: 0.3611s/iter; left time: 2034.8790s
Epoch: 4 cost time: 17.039344787597656
Epoch: 4, Steps: 122 | Train Loss: 0.3425273 Vali Loss: 0.7272184 Test Loss: 0.3820680
Validation loss decreased (0.735459 --> 0.727218).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3633704
	speed: 0.3737s/iter; left time: 2060.2034s
Epoch: 5 cost time: 15.227723598480225
Epoch: 5, Steps: 122 | Train Loss: 0.3396502 Vali Loss: 0.7192986 Test Loss: 0.3816669
Validation loss decreased (0.727218 --> 0.719299).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3808508
	speed: 0.3027s/iter; left time: 1631.6500s
Epoch: 6 cost time: 16.12848949432373
Epoch: 6, Steps: 122 | Train Loss: 0.3381267 Vali Loss: 0.7141440 Test Loss: 0.3819023
Validation loss decreased (0.719299 --> 0.714144).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3562023
	speed: 0.3699s/iter; left time: 1948.9910s
Epoch: 7 cost time: 17.69801902770996
Epoch: 7, Steps: 122 | Train Loss: 0.3369657 Vali Loss: 0.7111462 Test Loss: 0.3819352
Validation loss decreased (0.714144 --> 0.711146).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3575024
	speed: 0.3639s/iter; left time: 1873.1750s
Epoch: 8 cost time: 15.819002389907837
Epoch: 8, Steps: 122 | Train Loss: 0.3362520 Vali Loss: 0.7089180 Test Loss: 0.3809462
Validation loss decreased (0.711146 --> 0.708918).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3634327
	speed: 0.3383s/iter; left time: 1700.0625s
Epoch: 9 cost time: 15.731800556182861
Epoch: 9, Steps: 122 | Train Loss: 0.3355020 Vali Loss: 0.7061806 Test Loss: 0.3818632
Validation loss decreased (0.708918 --> 0.706181).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3786379
	speed: 0.3556s/iter; left time: 1743.2705s
Epoch: 10 cost time: 16.52112126350403
Epoch: 10, Steps: 122 | Train Loss: 0.3348007 Vali Loss: 0.7047986 Test Loss: 0.3814951
Validation loss decreased (0.706181 --> 0.704799).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3150988
	speed: 0.3467s/iter; left time: 1657.6208s
Epoch: 11 cost time: 15.933063983917236
Epoch: 11, Steps: 122 | Train Loss: 0.3343871 Vali Loss: 0.7047916 Test Loss: 0.3814740
Validation loss decreased (0.704799 --> 0.704792).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3460202
	speed: 0.3420s/iter; left time: 1593.4092s
Epoch: 12 cost time: 16.328855514526367
Epoch: 12, Steps: 122 | Train Loss: 0.3341669 Vali Loss: 0.7030150 Test Loss: 0.3815804
Validation loss decreased (0.704792 --> 0.703015).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3235640
	speed: 0.3580s/iter; left time: 1624.2098s
Epoch: 13 cost time: 16.20683526992798
Epoch: 13, Steps: 122 | Train Loss: 0.3336677 Vali Loss: 0.7010174 Test Loss: 0.3808357
Validation loss decreased (0.703015 --> 0.701017).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3208117
	speed: 0.3066s/iter; left time: 1353.6911s
Epoch: 14 cost time: 12.176275968551636
Epoch: 14, Steps: 122 | Train Loss: 0.3336780 Vali Loss: 0.7034929 Test Loss: 0.3819762
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3151430
	speed: 0.2845s/iter; left time: 1221.2309s
Epoch: 15 cost time: 13.772611141204834
Epoch: 15, Steps: 122 | Train Loss: 0.3334576 Vali Loss: 0.6990080 Test Loss: 0.3812934
Validation loss decreased (0.701017 --> 0.699008).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3472568
	speed: 0.3024s/iter; left time: 1261.4931s
Epoch: 16 cost time: 14.138139247894287
Epoch: 16, Steps: 122 | Train Loss: 0.3332003 Vali Loss: 0.7001985 Test Loss: 0.3814096
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3341301
	speed: 0.3400s/iter; left time: 1376.5851s
Epoch: 17 cost time: 16.227422952651978
Epoch: 17, Steps: 122 | Train Loss: 0.3328464 Vali Loss: 0.7020777 Test Loss: 0.3815066
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3464153
	speed: 0.3555s/iter; left time: 1396.2252s
Epoch: 18 cost time: 16.461039543151855
Epoch: 18, Steps: 122 | Train Loss: 0.3326431 Vali Loss: 0.7001896 Test Loss: 0.3812679
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.380771666765213, mae:0.40384387969970703, rse:0.5861248970031738, corr:[0.26657876 0.2779733  0.2784575  0.27725467 0.2758805  0.27353933
 0.27147266 0.2709987  0.27159545 0.27208272 0.27176517 0.27092528
 0.27047604 0.27053466 0.27081147 0.27105853 0.27107728 0.27094102
 0.27058303 0.27001134 0.2696168  0.26944718 0.26910344 0.26899776
 0.26913464 0.2693381  0.26930693 0.26902846 0.26866207 0.26811922
 0.26745266 0.26686218 0.26666042 0.26682574 0.26681134 0.26661733
 0.26641482 0.26638043 0.26652867 0.2667862  0.26720136 0.26744968
 0.26730213 0.2670113  0.2669408  0.26702854 0.26716068 0.26726294
 0.26683065 0.26607537 0.26511773 0.26400107 0.26284358 0.2617996
 0.26132932 0.2610575  0.2605017  0.25991535 0.25932726 0.25923386
 0.25930765 0.25936127 0.25919145 0.2587998  0.25835967 0.2583892
 0.2590146  0.2593602  0.258972   0.25853434 0.2585576  0.2583517
 0.25735533 0.25595585 0.25513858 0.25558352 0.25573164 0.2544529
 0.252739   0.25198138 0.2519354  0.25147173 0.25051162 0.24993886
 0.2498561  0.24942236 0.24896406 0.24902993 0.24897447 0.24788608
 0.24698573 0.24722733 0.2464474  0.24360944 0.24496157 0.253439  ]
