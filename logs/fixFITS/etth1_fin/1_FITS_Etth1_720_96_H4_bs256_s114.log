Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=134, out_features=151, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  72518656.0
params:  20385.0
Trainable parameters:  20385
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.7662105560302734
Epoch: 1, Steps: 15 | Train Loss: 0.7327092 Vali Loss: 1.3719460 Test Loss: 0.7790367
Validation loss decreased (inf --> 1.371946).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.8238437175750732
Epoch: 2, Steps: 15 | Train Loss: 0.5766764 Vali Loss: 1.1346071 Test Loss: 0.6382940
Validation loss decreased (1.371946 --> 1.134607).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.713109254837036
Epoch: 3, Steps: 15 | Train Loss: 0.4939125 Vali Loss: 1.0147748 Test Loss: 0.5606598
Validation loss decreased (1.134607 --> 1.014775).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.711909294128418
Epoch: 4, Steps: 15 | Train Loss: 0.4486794 Vali Loss: 0.9443793 Test Loss: 0.5136284
Validation loss decreased (1.014775 --> 0.944379).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2782609462738037
Epoch: 5, Steps: 15 | Train Loss: 0.4213207 Vali Loss: 0.8964332 Test Loss: 0.4816396
Validation loss decreased (0.944379 --> 0.896433).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.1466472148895264
Epoch: 6, Steps: 15 | Train Loss: 0.4027857 Vali Loss: 0.8654073 Test Loss: 0.4587694
Validation loss decreased (0.896433 --> 0.865407).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.0788145065307617
Epoch: 7, Steps: 15 | Train Loss: 0.3899782 Vali Loss: 0.8342990 Test Loss: 0.4420930
Validation loss decreased (0.865407 --> 0.834299).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.24835205078125
Epoch: 8, Steps: 15 | Train Loss: 0.3802578 Vali Loss: 0.8206266 Test Loss: 0.4297177
Validation loss decreased (0.834299 --> 0.820627).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.806295394897461
Epoch: 9, Steps: 15 | Train Loss: 0.3735376 Vali Loss: 0.8038632 Test Loss: 0.4204705
Validation loss decreased (0.820627 --> 0.803863).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.7512471675872803
Epoch: 10, Steps: 15 | Train Loss: 0.3678394 Vali Loss: 0.7976328 Test Loss: 0.4136800
Validation loss decreased (0.803863 --> 0.797633).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.9985852241516113
Epoch: 11, Steps: 15 | Train Loss: 0.3632339 Vali Loss: 0.7837181 Test Loss: 0.4085395
Validation loss decreased (0.797633 --> 0.783718).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.836223602294922
Epoch: 12, Steps: 15 | Train Loss: 0.3600067 Vali Loss: 0.7722676 Test Loss: 0.4047805
Validation loss decreased (0.783718 --> 0.772268).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.7462241649627686
Epoch: 13, Steps: 15 | Train Loss: 0.3578272 Vali Loss: 0.7675881 Test Loss: 0.4019263
Validation loss decreased (0.772268 --> 0.767588).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.768298387527466
Epoch: 14, Steps: 15 | Train Loss: 0.3548907 Vali Loss: 0.7657556 Test Loss: 0.3998601
Validation loss decreased (0.767588 --> 0.765756).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.880652666091919
Epoch: 15, Steps: 15 | Train Loss: 0.3529386 Vali Loss: 0.7606447 Test Loss: 0.3982049
Validation loss decreased (0.765756 --> 0.760645).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.9335851669311523
Epoch: 16, Steps: 15 | Train Loss: 0.3511478 Vali Loss: 0.7570528 Test Loss: 0.3967284
Validation loss decreased (0.760645 --> 0.757053).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.887111186981201
Epoch: 17, Steps: 15 | Train Loss: 0.3509281 Vali Loss: 0.7545413 Test Loss: 0.3958190
Validation loss decreased (0.757053 --> 0.754541).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.8541576862335205
Epoch: 18, Steps: 15 | Train Loss: 0.3502172 Vali Loss: 0.7525559 Test Loss: 0.3949628
Validation loss decreased (0.754541 --> 0.752556).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.9795353412628174
Epoch: 19, Steps: 15 | Train Loss: 0.3487538 Vali Loss: 0.7503835 Test Loss: 0.3942615
Validation loss decreased (0.752556 --> 0.750383).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.151132345199585
Epoch: 20, Steps: 15 | Train Loss: 0.3483882 Vali Loss: 0.7484928 Test Loss: 0.3938281
Validation loss decreased (0.750383 --> 0.748493).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.196702003479004
Epoch: 21, Steps: 15 | Train Loss: 0.3476459 Vali Loss: 0.7458346 Test Loss: 0.3933662
Validation loss decreased (0.748493 --> 0.745835).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.085333347320557
Epoch: 22, Steps: 15 | Train Loss: 0.3472040 Vali Loss: 0.7476683 Test Loss: 0.3930690
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.9123048782348633
Epoch: 23, Steps: 15 | Train Loss: 0.3470795 Vali Loss: 0.7442210 Test Loss: 0.3927567
Validation loss decreased (0.745835 --> 0.744221).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.908381700515747
Epoch: 24, Steps: 15 | Train Loss: 0.3462344 Vali Loss: 0.7434501 Test Loss: 0.3924176
Validation loss decreased (0.744221 --> 0.743450).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.262294054031372
Epoch: 25, Steps: 15 | Train Loss: 0.3455285 Vali Loss: 0.7406237 Test Loss: 0.3921829
Validation loss decreased (0.743450 --> 0.740624).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.7692744731903076
Epoch: 26, Steps: 15 | Train Loss: 0.3454936 Vali Loss: 0.7402276 Test Loss: 0.3920361
Validation loss decreased (0.740624 --> 0.740228).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.165336847305298
Epoch: 27, Steps: 15 | Train Loss: 0.3449802 Vali Loss: 0.7366291 Test Loss: 0.3919497
Validation loss decreased (0.740228 --> 0.736629).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.473550319671631
Epoch: 28, Steps: 15 | Train Loss: 0.3450294 Vali Loss: 0.7379445 Test Loss: 0.3917134
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.2392995357513428
Epoch: 29, Steps: 15 | Train Loss: 0.3449769 Vali Loss: 0.7407214 Test Loss: 0.3915658
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.183506727218628
Epoch: 30, Steps: 15 | Train Loss: 0.3444437 Vali Loss: 0.7351351 Test Loss: 0.3914432
Validation loss decreased (0.736629 --> 0.735135).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.6586546897888184
Epoch: 31, Steps: 15 | Train Loss: 0.3449891 Vali Loss: 0.7357388 Test Loss: 0.3913561
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.942006826400757
Epoch: 32, Steps: 15 | Train Loss: 0.3439166 Vali Loss: 0.7357861 Test Loss: 0.3912422
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.9864773750305176
Epoch: 33, Steps: 15 | Train Loss: 0.3436479 Vali Loss: 0.7383744 Test Loss: 0.3911512
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.3839240074157715, mae:0.4071640372276306, rse:0.588546097278595, corr:[0.2708044  0.28063855 0.2794854  0.27543905 0.27361116 0.27314374
 0.27282146 0.27239227 0.27172455 0.2710821  0.27034256 0.269659
 0.26935562 0.26949275 0.26994112 0.27010152 0.2696884  0.26902783
 0.26852682 0.26842454 0.2684388  0.26817408 0.26757115 0.26699632
 0.26680407 0.26692295 0.26701668 0.26693857 0.26665267 0.26620257
 0.2658099  0.26554757 0.26561832 0.26582476 0.26584625 0.26568556
 0.26543882 0.26529133 0.26540756 0.26582444 0.26649576 0.2669905
 0.26705262 0.26690012 0.26673687 0.266771   0.26701736 0.26714727
 0.26684093 0.2662781  0.26528046 0.26396313 0.2626907  0.26164824
 0.2609778  0.26045334 0.25984716 0.2593565  0.25913534 0.2594144
 0.25985625 0.26011324 0.2600523  0.2599875  0.26002672 0.26017657
 0.26038644 0.26035517 0.26023927 0.26008317 0.25967    0.25881693
 0.25784695 0.2570583  0.25657657 0.25636485 0.25580913 0.2546899
 0.25348687 0.2528474  0.25268823 0.25255185 0.25202465 0.25140616
 0.25123194 0.25109664 0.2506235  0.2498402  0.24926956 0.24876249
 0.24676071 0.2424666  0.23817076 0.23749517 0.23831417 0.22385976]
