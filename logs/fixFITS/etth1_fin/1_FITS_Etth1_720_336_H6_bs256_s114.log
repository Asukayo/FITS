Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  201607168.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.9560799598693848
Epoch: 1, Steps: 14 | Train Loss: 0.8812770 Vali Loss: 1.8348805 Test Loss: 0.8636864
Validation loss decreased (inf --> 1.834880).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.238083600997925
Epoch: 2, Steps: 14 | Train Loss: 0.7566248 Vali Loss: 1.6454406 Test Loss: 0.7484843
Validation loss decreased (1.834880 --> 1.645441).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.930375814437866
Epoch: 3, Steps: 14 | Train Loss: 0.6822781 Vali Loss: 1.5369247 Test Loss: 0.6820899
Validation loss decreased (1.645441 --> 1.536925).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.18691086769104
Epoch: 4, Steps: 14 | Train Loss: 0.6383022 Vali Loss: 1.4633704 Test Loss: 0.6417429
Validation loss decreased (1.536925 --> 1.463370).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.1009020805358887
Epoch: 5, Steps: 14 | Train Loss: 0.6099173 Vali Loss: 1.4297057 Test Loss: 0.6145364
Validation loss decreased (1.463370 --> 1.429706).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.1305384635925293
Epoch: 6, Steps: 14 | Train Loss: 0.5909456 Vali Loss: 1.3979373 Test Loss: 0.5942990
Validation loss decreased (1.429706 --> 1.397937).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.046475648880005
Epoch: 7, Steps: 14 | Train Loss: 0.5750986 Vali Loss: 1.3687353 Test Loss: 0.5781437
Validation loss decreased (1.397937 --> 1.368735).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.0246589183807373
Epoch: 8, Steps: 14 | Train Loss: 0.5630359 Vali Loss: 1.3507310 Test Loss: 0.5648077
Validation loss decreased (1.368735 --> 1.350731).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.135267972946167
Epoch: 9, Steps: 14 | Train Loss: 0.5516561 Vali Loss: 1.3475928 Test Loss: 0.5535287
Validation loss decreased (1.350731 --> 1.347593).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2205896377563477
Epoch: 10, Steps: 14 | Train Loss: 0.5440577 Vali Loss: 1.3241032 Test Loss: 0.5437391
Validation loss decreased (1.347593 --> 1.324103).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.989215612411499
Epoch: 11, Steps: 14 | Train Loss: 0.5373587 Vali Loss: 1.3083436 Test Loss: 0.5351720
Validation loss decreased (1.324103 --> 1.308344).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3748979568481445
Epoch: 12, Steps: 14 | Train Loss: 0.5296078 Vali Loss: 1.3014352 Test Loss: 0.5275948
Validation loss decreased (1.308344 --> 1.301435).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.1131818294525146
Epoch: 13, Steps: 14 | Train Loss: 0.5242079 Vali Loss: 1.2988110 Test Loss: 0.5209366
Validation loss decreased (1.301435 --> 1.298811).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.045220375061035
Epoch: 14, Steps: 14 | Train Loss: 0.5196905 Vali Loss: 1.2782236 Test Loss: 0.5149106
Validation loss decreased (1.298811 --> 1.278224).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.209972620010376
Epoch: 15, Steps: 14 | Train Loss: 0.5154326 Vali Loss: 1.2759840 Test Loss: 0.5095869
Validation loss decreased (1.278224 --> 1.275984).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.0582454204559326
Epoch: 16, Steps: 14 | Train Loss: 0.5120707 Vali Loss: 1.2602646 Test Loss: 0.5048360
Validation loss decreased (1.275984 --> 1.260265).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.082929849624634
Epoch: 17, Steps: 14 | Train Loss: 0.5068987 Vali Loss: 1.2618179 Test Loss: 0.5005619
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.047055244445801
Epoch: 18, Steps: 14 | Train Loss: 0.5026804 Vali Loss: 1.2554016 Test Loss: 0.4966862
Validation loss decreased (1.260265 --> 1.255402).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.135707139968872
Epoch: 19, Steps: 14 | Train Loss: 0.5001364 Vali Loss: 1.2470224 Test Loss: 0.4931566
Validation loss decreased (1.255402 --> 1.247022).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.130913734436035
Epoch: 20, Steps: 14 | Train Loss: 0.4970043 Vali Loss: 1.2430601 Test Loss: 0.4899503
Validation loss decreased (1.247022 --> 1.243060).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.15618634223938
Epoch: 21, Steps: 14 | Train Loss: 0.4949625 Vali Loss: 1.2373421 Test Loss: 0.4870109
Validation loss decreased (1.243060 --> 1.237342).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.0296926498413086
Epoch: 22, Steps: 14 | Train Loss: 0.4934526 Vali Loss: 1.2362204 Test Loss: 0.4843808
Validation loss decreased (1.237342 --> 1.236220).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.3360178470611572
Epoch: 23, Steps: 14 | Train Loss: 0.4915979 Vali Loss: 1.2248994 Test Loss: 0.4819658
Validation loss decreased (1.236220 --> 1.224899).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.136054039001465
Epoch: 24, Steps: 14 | Train Loss: 0.4895796 Vali Loss: 1.2330630 Test Loss: 0.4797095
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2176737785339355
Epoch: 25, Steps: 14 | Train Loss: 0.4873232 Vali Loss: 1.2316806 Test Loss: 0.4776812
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.2459325790405273
Epoch: 26, Steps: 14 | Train Loss: 0.4867313 Vali Loss: 1.2244570 Test Loss: 0.4758114
Validation loss decreased (1.224899 --> 1.224457).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.1877596378326416
Epoch: 27, Steps: 14 | Train Loss: 0.4838583 Vali Loss: 1.2193140 Test Loss: 0.4740686
Validation loss decreased (1.224457 --> 1.219314).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.9986467361450195
Epoch: 28, Steps: 14 | Train Loss: 0.4837920 Vali Loss: 1.2289976 Test Loss: 0.4724932
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.023651123046875
Epoch: 29, Steps: 14 | Train Loss: 0.4821754 Vali Loss: 1.2279040 Test Loss: 0.4710172
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.0207602977752686
Epoch: 30, Steps: 14 | Train Loss: 0.4795855 Vali Loss: 1.2061307 Test Loss: 0.4696588
Validation loss decreased (1.219314 --> 1.206131).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.9214863777160645
Epoch: 31, Steps: 14 | Train Loss: 0.4785707 Vali Loss: 1.2153239 Test Loss: 0.4683917
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.0741682052612305
Epoch: 32, Steps: 14 | Train Loss: 0.4791526 Vali Loss: 1.2166216 Test Loss: 0.4672477
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.956279754638672
Epoch: 33, Steps: 14 | Train Loss: 0.4771438 Vali Loss: 1.2192062 Test Loss: 0.4661748
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.46878424286842346, mae:0.46511927247047424, rse:0.6518362760543823, corr:[0.24559116 0.25638533 0.2469749  0.25048172 0.2528647  0.24991998
 0.24947801 0.25207305 0.25274974 0.25169274 0.25131768 0.25119263
 0.25033242 0.24959348 0.24923727 0.24824534 0.24716027 0.24652767
 0.24560145 0.2442386  0.24345864 0.2430952  0.24246372 0.24131799
 0.24116975 0.24181034 0.24219745 0.24258807 0.24360177 0.24436657
 0.24422638 0.24396072 0.24449903 0.24487293 0.24449678 0.24386117
 0.24359159 0.24340251 0.24315386 0.24313997 0.24311148 0.24288125
 0.2427852  0.24295953 0.24288772 0.24296041 0.24341625 0.24366169
 0.2436726  0.24357988 0.24365216 0.243401   0.2422818  0.24096589
 0.24031368 0.23990579 0.23921159 0.23829576 0.23783064 0.23770124
 0.23727934 0.23678057 0.23661527 0.23655969 0.23640646 0.23663937
 0.23705581 0.23687991 0.23649506 0.23636949 0.23646617 0.23593807
 0.23504251 0.23433165 0.23406595 0.23394854 0.23368286 0.2334372
 0.23311138 0.23277223 0.23256347 0.23247372 0.2320107  0.23138021
 0.23087665 0.23050943 0.22968286 0.22882907 0.22867303 0.2287707
 0.22821866 0.22752827 0.22729176 0.22716348 0.22642589 0.2263187
 0.22734742 0.22842102 0.22899003 0.22957388 0.23035598 0.23069698
 0.23051748 0.23025864 0.22993755 0.22945504 0.22902791 0.22889008
 0.22848818 0.22813405 0.22833009 0.22858642 0.22821018 0.22770612
 0.22763647 0.22751428 0.22701564 0.22659296 0.22656369 0.22654663
 0.22625569 0.22572626 0.22515707 0.22454172 0.22380601 0.22322555
 0.22298834 0.22267072 0.2221442  0.22178204 0.22147197 0.22133334
 0.22149457 0.22137812 0.22061464 0.2200412  0.22035675 0.2209666
 0.22067317 0.21995544 0.21982332 0.21999414 0.21945307 0.21880282
 0.21867079 0.21860655 0.21815205 0.21764846 0.2172723  0.21674623
 0.21643035 0.21643843 0.21663631 0.216787   0.21680032 0.21672757
 0.21629915 0.21629018 0.21688798 0.21695255 0.216164   0.21601503
 0.21690899 0.21722075 0.21640366 0.2160339  0.21603113 0.21532324
 0.2141739  0.21425658 0.2154484  0.21629655 0.21609765 0.21586624
 0.21615732 0.21643354 0.2161125  0.21561453 0.21503034 0.21455257
 0.21423098 0.2138944  0.21335706 0.21317226 0.21374582 0.21423616
 0.21404073 0.21404494 0.21473175 0.21519302 0.21482113 0.21435669
 0.2141625  0.21359758 0.21249713 0.21172383 0.21166164 0.21153961
 0.2109924  0.21057928 0.21060713 0.21098596 0.2111122  0.21081279
 0.21032847 0.21101858 0.21234995 0.21279322 0.21260944 0.21297607
 0.21370773 0.21340823 0.21244033 0.21237653 0.21277852 0.21234502
 0.21145822 0.21160923 0.21253344 0.21285214 0.21232465 0.21174398
 0.21183008 0.21223028 0.21225509 0.21186876 0.21151583 0.21200922
 0.2124536  0.2121075  0.21147594 0.2116575  0.21235475 0.21247287
 0.2117473  0.21143602 0.21175861 0.21152745 0.2106463  0.21028143
 0.21103753 0.21164642 0.21178085 0.21196644 0.21228921 0.21232407
 0.21205245 0.2119005  0.21182185 0.21174574 0.21155705 0.21110381
 0.21057476 0.21074793 0.21176372 0.21211226 0.21187253 0.21238168
 0.21351811 0.21350066 0.21266216 0.21255077 0.21334304 0.2133876
 0.21290113 0.21315378 0.2136638  0.21346426 0.21260776 0.2121705
 0.21220317 0.21184462 0.21155106 0.21199001 0.21226536 0.21232854
 0.21252592 0.21265304 0.21266027 0.21338348 0.21421902 0.21410006
 0.21344104 0.21344233 0.21356298 0.21241222 0.21131451 0.21184522
 0.2127711  0.212524   0.21196733 0.21273376 0.21327162 0.21274577
 0.2123525  0.21282315 0.21295133 0.21293966 0.21345918 0.21399608
 0.21397127 0.21423705 0.21497737 0.21454519 0.214279   0.21528687
 0.21590954 0.2138771  0.21249856 0.21357028 0.21399519 0.21191876
 0.2108047  0.21217558 0.21241128 0.21046549 0.20972794 0.20996094
 0.20884828 0.20696743 0.20697764 0.20717447 0.205934   0.2057298
 0.20621392 0.20422132 0.20282796 0.20475973 0.20306532 0.19677992
 0.19604222 0.1980766  0.18691929 0.1723076  0.18212433 0.16645864]
