Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17814720.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5276452
	speed: 0.1506s/iter; left time: 873.5837s
Epoch: 1 cost time: 17.602972984313965
Epoch: 1, Steps: 118 | Train Loss: 0.6248241 Vali Loss: 1.5958509 Test Loss: 0.7375760
Validation loss decreased (inf --> 1.595851).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4430924
	speed: 0.3639s/iter; left time: 2068.0636s
Epoch: 2 cost time: 16.946122407913208
Epoch: 2, Steps: 118 | Train Loss: 0.4527838 Vali Loss: 1.4675459 Test Loss: 0.6774357
Validation loss decreased (1.595851 --> 1.467546).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3987609
	speed: 0.3466s/iter; left time: 1928.7259s
Epoch: 3 cost time: 16.826329469680786
Epoch: 3, Steps: 118 | Train Loss: 0.3872338 Vali Loss: 1.4144119 Test Loss: 0.6508962
Validation loss decreased (1.467546 --> 1.414412).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3284591
	speed: 0.3648s/iter; left time: 1986.9563s
Epoch: 4 cost time: 17.326740980148315
Epoch: 4, Steps: 118 | Train Loss: 0.3451896 Vali Loss: 1.3813187 Test Loss: 0.6292112
Validation loss decreased (1.414412 --> 1.381319).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2961219
	speed: 0.3539s/iter; left time: 1885.7246s
Epoch: 5 cost time: 16.416215419769287
Epoch: 5, Steps: 118 | Train Loss: 0.3137224 Vali Loss: 1.3550262 Test Loss: 0.6097130
Validation loss decreased (1.381319 --> 1.355026).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2830962
	speed: 0.3542s/iter; left time: 1845.4773s
Epoch: 6 cost time: 16.36584496498108
Epoch: 6, Steps: 118 | Train Loss: 0.2888680 Vali Loss: 1.3314224 Test Loss: 0.5927044
Validation loss decreased (1.355026 --> 1.331422).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2789494
	speed: 0.3020s/iter; left time: 1538.1058s
Epoch: 7 cost time: 16.586612939834595
Epoch: 7, Steps: 118 | Train Loss: 0.2691625 Vali Loss: 1.3130175 Test Loss: 0.5767115
Validation loss decreased (1.331422 --> 1.313017).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2644513
	speed: 0.3485s/iter; left time: 1733.9647s
Epoch: 8 cost time: 16.491873502731323
Epoch: 8, Steps: 118 | Train Loss: 0.2529789 Vali Loss: 1.2951659 Test Loss: 0.5621899
Validation loss decreased (1.313017 --> 1.295166).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2526004
	speed: 0.3542s/iter; left time: 1720.4741s
Epoch: 9 cost time: 17.312872409820557
Epoch: 9, Steps: 118 | Train Loss: 0.2395974 Vali Loss: 1.2836698 Test Loss: 0.5485877
Validation loss decreased (1.295166 --> 1.283670).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2310483
	speed: 0.3511s/iter; left time: 1663.8925s
Epoch: 10 cost time: 16.103722095489502
Epoch: 10, Steps: 118 | Train Loss: 0.2285191 Vali Loss: 1.2715347 Test Loss: 0.5371870
Validation loss decreased (1.283670 --> 1.271535).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2139415
	speed: 0.3101s/iter; left time: 1432.8438s
Epoch: 11 cost time: 14.80388879776001
Epoch: 11, Steps: 118 | Train Loss: 0.2193885 Vali Loss: 1.2619632 Test Loss: 0.5274876
Validation loss decreased (1.271535 --> 1.261963).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2219141
	speed: 0.3125s/iter; left time: 1407.0668s
Epoch: 12 cost time: 16.406193017959595
Epoch: 12, Steps: 118 | Train Loss: 0.2115146 Vali Loss: 1.2554700 Test Loss: 0.5181624
Validation loss decreased (1.261963 --> 1.255470).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2114293
	speed: 0.3732s/iter; left time: 1636.6102s
Epoch: 13 cost time: 18.061522722244263
Epoch: 13, Steps: 118 | Train Loss: 0.2048999 Vali Loss: 1.2491932 Test Loss: 0.5106093
Validation loss decreased (1.255470 --> 1.249193).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2141731
	speed: 0.3886s/iter; left time: 1657.9444s
Epoch: 14 cost time: 18.187409162521362
Epoch: 14, Steps: 118 | Train Loss: 0.1993459 Vali Loss: 1.2445904 Test Loss: 0.5037659
Validation loss decreased (1.249193 --> 1.244590).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2107469
	speed: 0.3666s/iter; left time: 1520.8704s
Epoch: 15 cost time: 17.79275631904602
Epoch: 15, Steps: 118 | Train Loss: 0.1944535 Vali Loss: 1.2411563 Test Loss: 0.4975019
Validation loss decreased (1.244590 --> 1.241156).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1960032
	speed: 0.3807s/iter; left time: 1534.5841s
Epoch: 16 cost time: 16.5950710773468
Epoch: 16, Steps: 118 | Train Loss: 0.1902867 Vali Loss: 1.2366006 Test Loss: 0.4919765
Validation loss decreased (1.241156 --> 1.236601).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1862975
	speed: 0.2891s/iter; left time: 1131.3063s
Epoch: 17 cost time: 13.918738842010498
Epoch: 17, Steps: 118 | Train Loss: 0.1867785 Vali Loss: 1.2346755 Test Loss: 0.4878663
Validation loss decreased (1.236601 --> 1.234676).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1899842
	speed: 0.2833s/iter; left time: 1075.0712s
Epoch: 18 cost time: 14.27903699874878
Epoch: 18, Steps: 118 | Train Loss: 0.1836475 Vali Loss: 1.2325076 Test Loss: 0.4832256
Validation loss decreased (1.234676 --> 1.232508).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1721199
	speed: 0.3516s/iter; left time: 1292.8570s
Epoch: 19 cost time: 17.50745940208435
Epoch: 19, Steps: 118 | Train Loss: 0.1809339 Vali Loss: 1.2273747 Test Loss: 0.4790574
Validation loss decreased (1.232508 --> 1.227375).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1828810
	speed: 0.3721s/iter; left time: 1324.2731s
Epoch: 20 cost time: 18.70996642112732
Epoch: 20, Steps: 118 | Train Loss: 0.1785112 Vali Loss: 1.2262173 Test Loss: 0.4762285
Validation loss decreased (1.227375 --> 1.226217).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1900067
	speed: 0.3746s/iter; left time: 1289.0749s
Epoch: 21 cost time: 17.24200129508972
Epoch: 21, Steps: 118 | Train Loss: 0.1763419 Vali Loss: 1.2245098 Test Loss: 0.4728095
Validation loss decreased (1.226217 --> 1.224510).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1807780
	speed: 0.3619s/iter; left time: 1202.6688s
Epoch: 22 cost time: 17.865644693374634
Epoch: 22, Steps: 118 | Train Loss: 0.1745999 Vali Loss: 1.2226752 Test Loss: 0.4705545
Validation loss decreased (1.224510 --> 1.222675).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1680842
	speed: 0.3897s/iter; left time: 1248.8692s
Epoch: 23 cost time: 18.284510612487793
Epoch: 23, Steps: 118 | Train Loss: 0.1729274 Vali Loss: 1.2224827 Test Loss: 0.4675336
Validation loss decreased (1.222675 --> 1.222483).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1734824
	speed: 0.3678s/iter; left time: 1135.2741s
Epoch: 24 cost time: 17.184893369674683
Epoch: 24, Steps: 118 | Train Loss: 0.1713959 Vali Loss: 1.2247958 Test Loss: 0.4657770
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1797016
	speed: 0.2878s/iter; left time: 854.5022s
Epoch: 25 cost time: 12.964891195297241
Epoch: 25, Steps: 118 | Train Loss: 0.1700794 Vali Loss: 1.2216434 Test Loss: 0.4635556
Validation loss decreased (1.222483 --> 1.221643).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1670876
	speed: 0.3415s/iter; left time: 973.7557s
Epoch: 26 cost time: 17.39413094520569
Epoch: 26, Steps: 118 | Train Loss: 0.1689004 Vali Loss: 1.2236054 Test Loss: 0.4619299
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1677720
	speed: 0.3692s/iter; left time: 1008.9923s
Epoch: 27 cost time: 16.918486833572388
Epoch: 27, Steps: 118 | Train Loss: 0.1678217 Vali Loss: 1.2213912 Test Loss: 0.4602790
Validation loss decreased (1.221643 --> 1.221391).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1633695
	speed: 0.3637s/iter; left time: 951.1400s
Epoch: 28 cost time: 16.551978826522827
Epoch: 28, Steps: 118 | Train Loss: 0.1670186 Vali Loss: 1.2196699 Test Loss: 0.4590736
Validation loss decreased (1.221391 --> 1.219670).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1716405
	speed: 0.3548s/iter; left time: 885.8911s
Epoch: 29 cost time: 17.568039417266846
Epoch: 29, Steps: 118 | Train Loss: 0.1661392 Vali Loss: 1.2193906 Test Loss: 0.4575826
Validation loss decreased (1.219670 --> 1.219391).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1675465
	speed: 0.3625s/iter; left time: 862.5056s
Epoch: 30 cost time: 17.38502812385559
Epoch: 30, Steps: 118 | Train Loss: 0.1651987 Vali Loss: 1.2196072 Test Loss: 0.4564729
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1704033
	speed: 0.3510s/iter; left time: 793.5435s
Epoch: 31 cost time: 16.762420654296875
Epoch: 31, Steps: 118 | Train Loss: 0.1645407 Vali Loss: 1.2215412 Test Loss: 0.4551508
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1777198
	speed: 0.3584s/iter; left time: 768.0080s
Epoch: 32 cost time: 17.400084495544434
Epoch: 32, Steps: 118 | Train Loss: 0.1639338 Vali Loss: 1.2185172 Test Loss: 0.4541685
Validation loss decreased (1.219391 --> 1.218517).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1567991
	speed: 0.3460s/iter; left time: 700.7002s
Epoch: 33 cost time: 16.135778188705444
Epoch: 33, Steps: 118 | Train Loss: 0.1633252 Vali Loss: 1.2200022 Test Loss: 0.4534029
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1538786
	speed: 0.3395s/iter; left time: 647.4633s
Epoch: 34 cost time: 15.853697299957275
Epoch: 34, Steps: 118 | Train Loss: 0.1629454 Vali Loss: 1.2182126 Test Loss: 0.4525301
Validation loss decreased (1.218517 --> 1.218213).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1680251
	speed: 0.2873s/iter; left time: 513.9868s
Epoch: 35 cost time: 13.547459125518799
Epoch: 35, Steps: 118 | Train Loss: 0.1625381 Vali Loss: 1.2208793 Test Loss: 0.4518039
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1677144
	speed: 0.2911s/iter; left time: 486.4745s
Epoch: 36 cost time: 12.890518426895142
Epoch: 36, Steps: 118 | Train Loss: 0.1619322 Vali Loss: 1.2198513 Test Loss: 0.4511993
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1627349
	speed: 0.2544s/iter; left time: 395.0217s
Epoch: 37 cost time: 12.067901134490967
Epoch: 37, Steps: 118 | Train Loss: 0.1616224 Vali Loss: 1.2207397 Test Loss: 0.4503840
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17814720.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4326271
	speed: 0.0844s/iter; left time: 489.6362s
Epoch: 1 cost time: 9.972556829452515
Epoch: 1, Steps: 118 | Train Loss: 0.4378721 Vali Loss: 1.2099965 Test Loss: 0.4424296
Validation loss decreased (inf --> 1.209996).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4860258
	speed: 0.2424s/iter; left time: 1377.5216s
Epoch: 2 cost time: 14.545485496520996
Epoch: 2, Steps: 118 | Train Loss: 0.4352093 Vali Loss: 1.2114805 Test Loss: 0.4437887
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4854532
	speed: 0.3941s/iter; left time: 2193.3454s
Epoch: 3 cost time: 19.020970582962036
Epoch: 3, Steps: 118 | Train Loss: 0.4345523 Vali Loss: 1.2143974 Test Loss: 0.4438957
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4554384
	speed: 0.4003s/iter; left time: 2180.3890s
Epoch: 4 cost time: 17.411311388015747
Epoch: 4, Steps: 118 | Train Loss: 0.4342245 Vali Loss: 1.2185107 Test Loss: 0.4441433
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4416522681713104, mae:0.4406636655330658, rse:0.6326918601989746, corr:[0.2556332  0.26156163 0.2610062  0.2581046  0.2562626  0.25516275
 0.2543964  0.2535596  0.25287047 0.25271925 0.25273058 0.25252837
 0.25235316 0.25235623 0.25251886 0.25270072 0.25261784 0.25214416
 0.2514322  0.25085494 0.25077096 0.2508003  0.2504286  0.24963018
 0.2490046  0.24900454 0.24944204 0.24968486 0.2495354  0.24928202
 0.24927886 0.2495198  0.24979353 0.24978667 0.2495763  0.24950042
 0.24957317 0.24956326 0.24944253 0.24935807 0.24943887 0.2494949
 0.24941462 0.2492748  0.24921535 0.24935988 0.24986057 0.25043452
 0.2503994  0.24962564 0.2482825  0.24687223 0.24561566 0.24461551
 0.24418657 0.24411206 0.24383147 0.24319853 0.24228989 0.24181186
 0.24187306 0.2422284  0.24226648 0.24197787 0.24180114 0.24210528
 0.24259676 0.24246946 0.24198017 0.2417568  0.24200903 0.24232332
 0.24225055 0.24155569 0.24057208 0.24006866 0.24015804 0.24047835
 0.24057876 0.24015611 0.23922303 0.2382321  0.23771715 0.23789011
 0.23834202 0.23855375 0.2384141  0.23810522 0.23766004 0.2370503
 0.23636274 0.236127   0.2365144  0.23706655 0.23721258 0.23719543
 0.23743673 0.23786315 0.23811594 0.2382055  0.23820493 0.23820394
 0.23832096 0.23838636 0.23810744 0.2376739  0.23747922 0.23772706
 0.23811069 0.23816371 0.23786376 0.23761934 0.2377389  0.23809521
 0.23821886 0.23787487 0.23748568 0.23757198 0.23804219 0.23845124
 0.23838201 0.23770471 0.23696212 0.23647635 0.23596881 0.23515184
 0.23436522 0.23392895 0.23361276 0.23351254 0.233679   0.23410642
 0.23456189 0.23466289 0.23432271 0.23385958 0.23356141 0.23334661
 0.23302999 0.2327203  0.23288056 0.2334593  0.23382005 0.23347858
 0.23243359 0.23116161 0.23032784 0.23013139 0.23022471 0.22974913
 0.22880039 0.22810699 0.22801727 0.22827885 0.2281714  0.22778563
 0.22768    0.22808419 0.22855039 0.22870986 0.22856233 0.2286973
 0.2291164  0.22947143 0.22940211 0.2288565  0.22795871 0.22734056
 0.22762649 0.22864957 0.22934555 0.22888449 0.22746928 0.22639439
 0.22664899 0.22783865 0.22842915 0.22794284 0.22706938 0.22679676
 0.2271574  0.22739986 0.22704098 0.22641347 0.22608826 0.22631809
 0.22691274 0.22757089 0.2280875  0.22829898 0.22823608 0.22810817
 0.22787979 0.2273032  0.22618555 0.22474217 0.22335261 0.22207956
 0.2210892  0.22103433 0.22214586 0.22363229 0.22412372 0.2234398
 0.22240724 0.22223446 0.22297865 0.2237929  0.2240057  0.22372186
 0.22348882 0.22339463 0.22334217 0.22316028 0.22301082 0.22297613
 0.22279017 0.22228576 0.22144689 0.2206673  0.2203742  0.22037087
 0.22060153 0.22086054 0.2208547  0.22038706 0.21971948 0.2194093
 0.21939977 0.21902063 0.21805896 0.216801   0.2158467  0.21578863
 0.21635528 0.21669185 0.21648395 0.21619156 0.2165713  0.21723376
 0.21748532 0.21690713 0.2162537  0.21625309 0.21676953 0.21695103
 0.21673219 0.2168614  0.2176222  0.21817325 0.21765476 0.21648547
 0.21574771 0.21585724 0.21635324 0.21646267 0.2162025  0.21604043
 0.21628511 0.21655852 0.21679394 0.21685842 0.21673936 0.21636552
 0.21598232 0.21575342 0.21548645 0.21497968 0.2142641  0.2138779
 0.21416576 0.21445256 0.21394128 0.21284302 0.21159747 0.21089534
 0.2109464  0.21118212 0.2109792  0.21013574 0.2091906  0.20906931
 0.21000972 0.21121368 0.21192846 0.21174699 0.21135618 0.21130916
 0.21123399 0.2107483  0.21011476 0.21052812 0.21148515 0.21171671
 0.21063654 0.20917214 0.2085419  0.20886949 0.20922656 0.20954025
 0.21028672 0.21105528 0.21124554 0.2105248  0.20996334 0.21021213
 0.2108855  0.21075092 0.21042567 0.21030362 0.21026626 0.20971021
 0.20882934 0.20819119 0.20774311 0.20725708 0.20694871 0.20666964
 0.20640527 0.20573743 0.2053261  0.20610437 0.20716307 0.20671901
 0.20482036 0.20335317 0.20360507 0.20453948 0.20367992 0.20242523
 0.2029136  0.20353605 0.2016373  0.199349   0.2017888  0.2033278 ]
