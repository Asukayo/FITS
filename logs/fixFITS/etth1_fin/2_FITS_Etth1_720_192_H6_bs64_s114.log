Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.688506603240967
Epoch: 1, Steps: 60 | Train Loss: 0.6342490 Vali Loss: 1.5169913 Test Loss: 0.7889140
Validation loss decreased (inf --> 1.516991).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.315634965896606
Epoch: 2, Steps: 60 | Train Loss: 0.4939151 Vali Loss: 1.3757616 Test Loss: 0.7187268
Validation loss decreased (1.516991 --> 1.375762).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.003669023513794
Epoch: 3, Steps: 60 | Train Loss: 0.4300952 Vali Loss: 1.3238534 Test Loss: 0.6970771
Validation loss decreased (1.375762 --> 1.323853).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.529185056686401
Epoch: 4, Steps: 60 | Train Loss: 0.3923500 Vali Loss: 1.2960604 Test Loss: 0.6845494
Validation loss decreased (1.323853 --> 1.296060).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.456312417984009
Epoch: 5, Steps: 60 | Train Loss: 0.3649291 Vali Loss: 1.2788004 Test Loss: 0.6769071
Validation loss decreased (1.296060 --> 1.278800).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.013172149658203
Epoch: 6, Steps: 60 | Train Loss: 0.3425029 Vali Loss: 1.2621199 Test Loss: 0.6669786
Validation loss decreased (1.278800 --> 1.262120).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.444413185119629
Epoch: 7, Steps: 60 | Train Loss: 0.3236919 Vali Loss: 1.2500410 Test Loss: 0.6591768
Validation loss decreased (1.262120 --> 1.250041).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.6745126247406
Epoch: 8, Steps: 60 | Train Loss: 0.3073657 Vali Loss: 1.2356665 Test Loss: 0.6500521
Validation loss decreased (1.250041 --> 1.235667).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.149384021759033
Epoch: 9, Steps: 60 | Train Loss: 0.2927960 Vali Loss: 1.2219743 Test Loss: 0.6403986
Validation loss decreased (1.235667 --> 1.221974).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.124825477600098
Epoch: 10, Steps: 60 | Train Loss: 0.2801399 Vali Loss: 1.2118477 Test Loss: 0.6336150
Validation loss decreased (1.221974 --> 1.211848).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 7.117633104324341
Epoch: 11, Steps: 60 | Train Loss: 0.2687744 Vali Loss: 1.2004507 Test Loss: 0.6253151
Validation loss decreased (1.211848 --> 1.200451).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 7.800153493881226
Epoch: 12, Steps: 60 | Train Loss: 0.2586351 Vali Loss: 1.1911318 Test Loss: 0.6183091
Validation loss decreased (1.200451 --> 1.191132).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.074715852737427
Epoch: 13, Steps: 60 | Train Loss: 0.2495426 Vali Loss: 1.1790359 Test Loss: 0.6090131
Validation loss decreased (1.191132 --> 1.179036).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 7.541685342788696
Epoch: 14, Steps: 60 | Train Loss: 0.2413184 Vali Loss: 1.1695349 Test Loss: 0.6022046
Validation loss decreased (1.179036 --> 1.169535).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.085753917694092
Epoch: 15, Steps: 60 | Train Loss: 0.2337908 Vali Loss: 1.1626740 Test Loss: 0.5967424
Validation loss decreased (1.169535 --> 1.162674).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.653710842132568
Epoch: 16, Steps: 60 | Train Loss: 0.2270627 Vali Loss: 1.1522688 Test Loss: 0.5886586
Validation loss decreased (1.162674 --> 1.152269).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 7.8489696979522705
Epoch: 17, Steps: 60 | Train Loss: 0.2208535 Vali Loss: 1.1454352 Test Loss: 0.5837966
Validation loss decreased (1.152269 --> 1.145435).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.629748582839966
Epoch: 18, Steps: 60 | Train Loss: 0.2152613 Vali Loss: 1.1377512 Test Loss: 0.5771306
Validation loss decreased (1.145435 --> 1.137751).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 7.655216932296753
Epoch: 19, Steps: 60 | Train Loss: 0.2101381 Vali Loss: 1.1314603 Test Loss: 0.5724184
Validation loss decreased (1.137751 --> 1.131460).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 7.6262969970703125
Epoch: 20, Steps: 60 | Train Loss: 0.2051243 Vali Loss: 1.1250646 Test Loss: 0.5678493
Validation loss decreased (1.131460 --> 1.125065).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 7.157120227813721
Epoch: 21, Steps: 60 | Train Loss: 0.2009613 Vali Loss: 1.1192757 Test Loss: 0.5633654
Validation loss decreased (1.125065 --> 1.119276).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.155336618423462
Epoch: 22, Steps: 60 | Train Loss: 0.1968971 Vali Loss: 1.1142949 Test Loss: 0.5588880
Validation loss decreased (1.119276 --> 1.114295).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.18705129623413
Epoch: 23, Steps: 60 | Train Loss: 0.1932484 Vali Loss: 1.1076654 Test Loss: 0.5537985
Validation loss decreased (1.114295 --> 1.107665).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.230285882949829
Epoch: 24, Steps: 60 | Train Loss: 0.1897916 Vali Loss: 1.1033150 Test Loss: 0.5500947
Validation loss decreased (1.107665 --> 1.103315).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 7.83076024055481
Epoch: 25, Steps: 60 | Train Loss: 0.1866274 Vali Loss: 1.0984824 Test Loss: 0.5462306
Validation loss decreased (1.103315 --> 1.098482).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 7.945464134216309
Epoch: 26, Steps: 60 | Train Loss: 0.1837053 Vali Loss: 1.0944124 Test Loss: 0.5431884
Validation loss decreased (1.098482 --> 1.094412).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.338988304138184
Epoch: 27, Steps: 60 | Train Loss: 0.1809789 Vali Loss: 1.0901277 Test Loss: 0.5395590
Validation loss decreased (1.094412 --> 1.090128).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 8.218090534210205
Epoch: 28, Steps: 60 | Train Loss: 0.1784647 Vali Loss: 1.0863134 Test Loss: 0.5366172
Validation loss decreased (1.090128 --> 1.086313).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.458041191101074
Epoch: 29, Steps: 60 | Train Loss: 0.1761301 Vali Loss: 1.0822245 Test Loss: 0.5336269
Validation loss decreased (1.086313 --> 1.082224).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 8.603340864181519
Epoch: 30, Steps: 60 | Train Loss: 0.1739047 Vali Loss: 1.0795738 Test Loss: 0.5310605
Validation loss decreased (1.082224 --> 1.079574).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 8.61981201171875
Epoch: 31, Steps: 60 | Train Loss: 0.1718055 Vali Loss: 1.0756840 Test Loss: 0.5283179
Validation loss decreased (1.079574 --> 1.075684).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.029087543487549
Epoch: 32, Steps: 60 | Train Loss: 0.1699640 Vali Loss: 1.0734066 Test Loss: 0.5260071
Validation loss decreased (1.075684 --> 1.073407).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.668615579605103
Epoch: 33, Steps: 60 | Train Loss: 0.1681833 Vali Loss: 1.0702162 Test Loss: 0.5234556
Validation loss decreased (1.073407 --> 1.070216).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 9.37103796005249
Epoch: 34, Steps: 60 | Train Loss: 0.1664309 Vali Loss: 1.0677770 Test Loss: 0.5216870
Validation loss decreased (1.070216 --> 1.067777).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 9.164783716201782
Epoch: 35, Steps: 60 | Train Loss: 0.1648483 Vali Loss: 1.0654161 Test Loss: 0.5196642
Validation loss decreased (1.067777 --> 1.065416).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 9.31664514541626
Epoch: 36, Steps: 60 | Train Loss: 0.1633939 Vali Loss: 1.0629530 Test Loss: 0.5175620
Validation loss decreased (1.065416 --> 1.062953).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 9.445385456085205
Epoch: 37, Steps: 60 | Train Loss: 0.1620444 Vali Loss: 1.0599638 Test Loss: 0.5157697
Validation loss decreased (1.062953 --> 1.059964).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 9.255714178085327
Epoch: 38, Steps: 60 | Train Loss: 0.1607277 Vali Loss: 1.0585951 Test Loss: 0.5139752
Validation loss decreased (1.059964 --> 1.058595).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 9.170423030853271
Epoch: 39, Steps: 60 | Train Loss: 0.1594815 Vali Loss: 1.0565006 Test Loss: 0.5125808
Validation loss decreased (1.058595 --> 1.056501).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 9.174099683761597
Epoch: 40, Steps: 60 | Train Loss: 0.1582983 Vali Loss: 1.0547308 Test Loss: 0.5110557
Validation loss decreased (1.056501 --> 1.054731).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 9.19432258605957
Epoch: 41, Steps: 60 | Train Loss: 0.1573703 Vali Loss: 1.0532278 Test Loss: 0.5096344
Validation loss decreased (1.054731 --> 1.053228).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 9.095752000808716
Epoch: 42, Steps: 60 | Train Loss: 0.1562072 Vali Loss: 1.0514234 Test Loss: 0.5082523
Validation loss decreased (1.053228 --> 1.051423).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 9.019087791442871
Epoch: 43, Steps: 60 | Train Loss: 0.1553247 Vali Loss: 1.0495481 Test Loss: 0.5070819
Validation loss decreased (1.051423 --> 1.049548).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 8.855651617050171
Epoch: 44, Steps: 60 | Train Loss: 0.1544255 Vali Loss: 1.0480094 Test Loss: 0.5056087
Validation loss decreased (1.049548 --> 1.048009).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 9.523736238479614
Epoch: 45, Steps: 60 | Train Loss: 0.1535902 Vali Loss: 1.0467842 Test Loss: 0.5044069
Validation loss decreased (1.048009 --> 1.046784).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.588675498962402
Epoch: 46, Steps: 60 | Train Loss: 0.1527030 Vali Loss: 1.0455669 Test Loss: 0.5034238
Validation loss decreased (1.046784 --> 1.045567).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 9.37586498260498
Epoch: 47, Steps: 60 | Train Loss: 0.1519541 Vali Loss: 1.0445036 Test Loss: 0.5024542
Validation loss decreased (1.045567 --> 1.044504).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 7.8542187213897705
Epoch: 48, Steps: 60 | Train Loss: 0.1510900 Vali Loss: 1.0430838 Test Loss: 0.5013899
Validation loss decreased (1.044504 --> 1.043084).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 7.157298564910889
Epoch: 49, Steps: 60 | Train Loss: 0.1505654 Vali Loss: 1.0420763 Test Loss: 0.5003421
Validation loss decreased (1.043084 --> 1.042076).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 6.072057723999023
Epoch: 50, Steps: 60 | Train Loss: 0.1498524 Vali Loss: 1.0409654 Test Loss: 0.4996451
Validation loss decreased (1.042076 --> 1.040965).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 6.721432209014893
Epoch: 1, Steps: 60 | Train Loss: 0.4114148 Vali Loss: 0.9623035 Test Loss: 0.4323236
Validation loss decreased (inf --> 0.962304).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 6.877979278564453
Epoch: 2, Steps: 60 | Train Loss: 0.3875753 Vali Loss: 0.9487911 Test Loss: 0.4179102
Validation loss decreased (0.962304 --> 0.948791).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 7.214200973510742
Epoch: 3, Steps: 60 | Train Loss: 0.3824902 Vali Loss: 0.9519460 Test Loss: 0.4173858
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
Epoch: 4 cost time: 7.137823820114136
Epoch: 4, Steps: 60 | Train Loss: 0.3809477 Vali Loss: 0.9547195 Test Loss: 0.4183109
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 7.342142820358276
Epoch: 5, Steps: 60 | Train Loss: 0.3803230 Vali Loss: 0.9549817 Test Loss: 0.4184901
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.41553476452827454, mae:0.4246237277984619, rse:0.6121544241905212, corr:[0.26713035 0.27291462 0.2701496  0.27065977 0.269814   0.2668608
 0.26498476 0.26548967 0.26587388 0.2654047  0.26483092 0.2648279
 0.26491296 0.2645149  0.2640097  0.26372918 0.26359957 0.2635212
 0.26297697 0.26205638 0.26122618 0.26077056 0.26045457 0.26026896
 0.26020646 0.26028377 0.26041508 0.26049075 0.26039943 0.26002657
 0.25966862 0.2595833  0.25986078 0.259948   0.25948107 0.259459
 0.2602396  0.2608142  0.2601872  0.2589815  0.25877437 0.2594529
 0.2598212  0.25926706 0.2584009  0.25843012 0.25960234 0.2606368
 0.2604056  0.25953108 0.25879636 0.25814328 0.2569464  0.25535107
 0.25462016 0.25458196 0.2541824  0.2536145  0.25364825 0.25429606
 0.25412175 0.25320682 0.25287494 0.25336155 0.25358328 0.25331122
 0.25313994 0.25289392 0.25251034 0.25221148 0.25215378 0.25202614
 0.2517172  0.25130722 0.25096604 0.25104138 0.251179   0.2509341
 0.2500459  0.24888101 0.24837299 0.24875894 0.24895324 0.24831536
 0.24753766 0.2473417  0.24756517 0.24740997 0.24694337 0.24681392
 0.24690862 0.24667728 0.24606964 0.24572247 0.24602123 0.24686795
 0.24783963 0.24848652 0.24853928 0.24814568 0.2475468  0.24715532
 0.24718718 0.24704513 0.24652876 0.24641064 0.24662654 0.24627788
 0.2453037  0.24474682 0.24533962 0.24600662 0.24585018 0.2458121
 0.24657491 0.24712022 0.24672876 0.24607272 0.24607478 0.24669178
 0.24709325 0.24641305 0.24485935 0.24358875 0.24278049 0.24185449
 0.24101098 0.24061589 0.24034195 0.2399273  0.2391351  0.23867096
 0.23870394 0.23882174 0.23896997 0.23949938 0.24018519 0.24008392
 0.23967616 0.23966333 0.2396322  0.23914021 0.23858036 0.2383579
 0.23814437 0.23712741 0.23607196 0.23560043 0.2348289  0.23337306
 0.23279002 0.23337147 0.23349346 0.23297258 0.23289469 0.23362638
 0.23383257 0.23354906 0.23383848 0.23394158 0.23288862 0.2315926
 0.23131673 0.23147199 0.23104547 0.23072591 0.23138873 0.2315505
 0.23056182 0.22991575 0.23021545 0.23033497 0.22948179 0.2287376
 0.22877881 0.22913729 0.2287324  0.22847731 0.22907573 0.22994548
 0.22953047 0.22766843 0.22609112 0.22588144 0.22538143 0.22355546
 0.22271456 0.22357138 0.22202398 0.21836455 0.22094843 0.22395226]
