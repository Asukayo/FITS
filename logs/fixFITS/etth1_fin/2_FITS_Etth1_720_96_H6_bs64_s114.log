Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  38986752.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.90175199508667
Epoch: 1, Steps: 61 | Train Loss: 0.6233854 Vali Loss: 1.3890805 Test Loss: 0.7978275
Validation loss decreased (inf --> 1.389081).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.716901540756226
Epoch: 2, Steps: 61 | Train Loss: 0.4912066 Vali Loss: 1.2672398 Test Loss: 0.7510614
Validation loss decreased (1.389081 --> 1.267240).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.809155941009521
Epoch: 3, Steps: 61 | Train Loss: 0.4292768 Vali Loss: 1.2312167 Test Loss: 0.7390025
Validation loss decreased (1.267240 --> 1.231217).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.044923543930054
Epoch: 4, Steps: 61 | Train Loss: 0.3916352 Vali Loss: 1.2134554 Test Loss: 0.7306850
Validation loss decreased (1.231217 --> 1.213455).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.122878789901733
Epoch: 5, Steps: 61 | Train Loss: 0.3632662 Vali Loss: 1.2016896 Test Loss: 0.7257953
Validation loss decreased (1.213455 --> 1.201690).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.669135093688965
Epoch: 6, Steps: 61 | Train Loss: 0.3401394 Vali Loss: 1.1885763 Test Loss: 0.7176841
Validation loss decreased (1.201690 --> 1.188576).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.602428674697876
Epoch: 7, Steps: 61 | Train Loss: 0.3202373 Vali Loss: 1.1711450 Test Loss: 0.7092597
Validation loss decreased (1.188576 --> 1.171145).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.910511493682861
Epoch: 8, Steps: 61 | Train Loss: 0.3031781 Vali Loss: 1.1491053 Test Loss: 0.6960252
Validation loss decreased (1.171145 --> 1.149105).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.794187545776367
Epoch: 9, Steps: 61 | Train Loss: 0.2878328 Vali Loss: 1.1318370 Test Loss: 0.6821997
Validation loss decreased (1.149105 --> 1.131837).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.919146537780762
Epoch: 10, Steps: 61 | Train Loss: 0.2742034 Vali Loss: 1.1141272 Test Loss: 0.6719512
Validation loss decreased (1.131837 --> 1.114127).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.518604040145874
Epoch: 11, Steps: 61 | Train Loss: 0.2619975 Vali Loss: 1.0981562 Test Loss: 0.6607122
Validation loss decreased (1.114127 --> 1.098156).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.228639841079712
Epoch: 12, Steps: 61 | Train Loss: 0.2510620 Vali Loss: 1.0834134 Test Loss: 0.6503733
Validation loss decreased (1.098156 --> 1.083413).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.98705506324768
Epoch: 13, Steps: 61 | Train Loss: 0.2411077 Vali Loss: 1.0642684 Test Loss: 0.6404400
Validation loss decreased (1.083413 --> 1.064268).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.751844644546509
Epoch: 14, Steps: 61 | Train Loss: 0.2319605 Vali Loss: 1.0507581 Test Loss: 0.6310738
Validation loss decreased (1.064268 --> 1.050758).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.6261146068573
Epoch: 15, Steps: 61 | Train Loss: 0.2237152 Vali Loss: 1.0401909 Test Loss: 0.6221815
Validation loss decreased (1.050758 --> 1.040191).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 10.067958116531372
Epoch: 16, Steps: 61 | Train Loss: 0.2162159 Vali Loss: 1.0277586 Test Loss: 0.6139938
Validation loss decreased (1.040191 --> 1.027759).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.154829740524292
Epoch: 17, Steps: 61 | Train Loss: 0.2092863 Vali Loss: 1.0093886 Test Loss: 0.6043801
Validation loss decreased (1.027759 --> 1.009389).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.98034954071045
Epoch: 18, Steps: 61 | Train Loss: 0.2029246 Vali Loss: 1.0007586 Test Loss: 0.5977893
Validation loss decreased (1.009389 --> 1.000759).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.524150848388672
Epoch: 19, Steps: 61 | Train Loss: 0.1971130 Vali Loss: 0.9958268 Test Loss: 0.5910049
Validation loss decreased (1.000759 --> 0.995827).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.81349778175354
Epoch: 20, Steps: 61 | Train Loss: 0.1917886 Vali Loss: 0.9830017 Test Loss: 0.5838013
Validation loss decreased (0.995827 --> 0.983002).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 10.072091579437256
Epoch: 21, Steps: 61 | Train Loss: 0.1868257 Vali Loss: 0.9765716 Test Loss: 0.5775981
Validation loss decreased (0.983002 --> 0.976572).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.089534759521484
Epoch: 22, Steps: 61 | Train Loss: 0.1822228 Vali Loss: 0.9676170 Test Loss: 0.5720653
Validation loss decreased (0.976572 --> 0.967617).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.679288387298584
Epoch: 23, Steps: 61 | Train Loss: 0.1779574 Vali Loss: 0.9576716 Test Loss: 0.5668474
Validation loss decreased (0.967617 --> 0.957672).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.592941999435425
Epoch: 24, Steps: 61 | Train Loss: 0.1740785 Vali Loss: 0.9455263 Test Loss: 0.5601095
Validation loss decreased (0.957672 --> 0.945526).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.011604070663452
Epoch: 25, Steps: 61 | Train Loss: 0.1703983 Vali Loss: 0.9349962 Test Loss: 0.5557191
Validation loss decreased (0.945526 --> 0.934996).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.138892412185669
Epoch: 26, Steps: 61 | Train Loss: 0.1669959 Vali Loss: 0.9307119 Test Loss: 0.5513610
Validation loss decreased (0.934996 --> 0.930712).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 9.655382871627808
Epoch: 27, Steps: 61 | Train Loss: 0.1637879 Vali Loss: 0.9262512 Test Loss: 0.5464654
Validation loss decreased (0.930712 --> 0.926251).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.349829912185669
Epoch: 28, Steps: 61 | Train Loss: 0.1608679 Vali Loss: 0.9212522 Test Loss: 0.5416143
Validation loss decreased (0.926251 --> 0.921252).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.792198181152344
Epoch: 29, Steps: 61 | Train Loss: 0.1580804 Vali Loss: 0.9205173 Test Loss: 0.5386745
Validation loss decreased (0.921252 --> 0.920517).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.482141494750977
Epoch: 30, Steps: 61 | Train Loss: 0.1554824 Vali Loss: 0.9127830 Test Loss: 0.5361080
Validation loss decreased (0.920517 --> 0.912783).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 9.48248839378357
Epoch: 31, Steps: 61 | Train Loss: 0.1531072 Vali Loss: 0.9102067 Test Loss: 0.5325434
Validation loss decreased (0.912783 --> 0.910207).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.31721568107605
Epoch: 32, Steps: 61 | Train Loss: 0.1508565 Vali Loss: 0.9067712 Test Loss: 0.5289079
Validation loss decreased (0.910207 --> 0.906771).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 10.031918048858643
Epoch: 33, Steps: 61 | Train Loss: 0.1487288 Vali Loss: 0.9006807 Test Loss: 0.5256563
Validation loss decreased (0.906771 --> 0.900681).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 10.058420658111572
Epoch: 34, Steps: 61 | Train Loss: 0.1467771 Vali Loss: 0.8978133 Test Loss: 0.5229065
Validation loss decreased (0.900681 --> 0.897813).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 9.636637210845947
Epoch: 35, Steps: 61 | Train Loss: 0.1448447 Vali Loss: 0.8912239 Test Loss: 0.5203742
Validation loss decreased (0.897813 --> 0.891224).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.8257155418396
Epoch: 36, Steps: 61 | Train Loss: 0.1430964 Vali Loss: 0.8904909 Test Loss: 0.5181161
Validation loss decreased (0.891224 --> 0.890491).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 10.075273752212524
Epoch: 37, Steps: 61 | Train Loss: 0.1414592 Vali Loss: 0.8867500 Test Loss: 0.5158578
Validation loss decreased (0.890491 --> 0.886750).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 8.764930486679077
Epoch: 38, Steps: 61 | Train Loss: 0.1399038 Vali Loss: 0.8799817 Test Loss: 0.5127047
Validation loss decreased (0.886750 --> 0.879982).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 10.559422969818115
Epoch: 39, Steps: 61 | Train Loss: 0.1384994 Vali Loss: 0.8802351 Test Loss: 0.5112105
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 9.46785831451416
Epoch: 40, Steps: 61 | Train Loss: 0.1370634 Vali Loss: 0.8776214 Test Loss: 0.5092497
Validation loss decreased (0.879982 --> 0.877621).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 9.59424352645874
Epoch: 41, Steps: 61 | Train Loss: 0.1357760 Vali Loss: 0.8747721 Test Loss: 0.5076869
Validation loss decreased (0.877621 --> 0.874772).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 9.078310251235962
Epoch: 42, Steps: 61 | Train Loss: 0.1345469 Vali Loss: 0.8681155 Test Loss: 0.5053068
Validation loss decreased (0.874772 --> 0.868115).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 10.068679809570312
Epoch: 43, Steps: 61 | Train Loss: 0.1334142 Vali Loss: 0.8707684 Test Loss: 0.5036048
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 8.41571831703186
Epoch: 44, Steps: 61 | Train Loss: 0.1323181 Vali Loss: 0.8657396 Test Loss: 0.5020240
Validation loss decreased (0.868115 --> 0.865740).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 9.777524709701538
Epoch: 45, Steps: 61 | Train Loss: 0.1313107 Vali Loss: 0.8609034 Test Loss: 0.5006432
Validation loss decreased (0.865740 --> 0.860903).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.999682903289795
Epoch: 46, Steps: 61 | Train Loss: 0.1303040 Vali Loss: 0.8593010 Test Loss: 0.4990495
Validation loss decreased (0.860903 --> 0.859301).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 9.871726751327515
Epoch: 47, Steps: 61 | Train Loss: 0.1293375 Vali Loss: 0.8596728 Test Loss: 0.4980365
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 9.595139980316162
Epoch: 48, Steps: 61 | Train Loss: 0.1285391 Vali Loss: 0.8567502 Test Loss: 0.4963850
Validation loss decreased (0.859301 --> 0.856750).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 9.648395776748657
Epoch: 49, Steps: 61 | Train Loss: 0.1277212 Vali Loss: 0.8578240 Test Loss: 0.4956630
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 9.774792909622192
Epoch: 50, Steps: 61 | Train Loss: 0.1269216 Vali Loss: 0.8561174 Test Loss: 0.4943328
Validation loss decreased (0.856750 --> 0.856117).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  38986752.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.718322038650513
Epoch: 1, Steps: 61 | Train Loss: 0.3650324 Vali Loss: 0.7086714 Test Loss: 0.3872979
Validation loss decreased (inf --> 0.708671).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.151219367980957
Epoch: 2, Steps: 61 | Train Loss: 0.3380527 Vali Loss: 0.6997135 Test Loss: 0.3808887
Validation loss decreased (0.708671 --> 0.699714).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.730841398239136
Epoch: 3, Steps: 61 | Train Loss: 0.3350771 Vali Loss: 0.6974149 Test Loss: 0.3809397
Validation loss decreased (0.699714 --> 0.697415).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.291070222854614
Epoch: 4, Steps: 61 | Train Loss: 0.3337455 Vali Loss: 0.6954144 Test Loss: 0.3807000
Validation loss decreased (0.697415 --> 0.695414).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.64970588684082
Epoch: 5, Steps: 61 | Train Loss: 0.3329310 Vali Loss: 0.6985247 Test Loss: 0.3810181
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.587564706802368
Epoch: 6, Steps: 61 | Train Loss: 0.3326967 Vali Loss: 0.6951294 Test Loss: 0.3805467
Validation loss decreased (0.695414 --> 0.695129).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.919766664505005
Epoch: 7, Steps: 61 | Train Loss: 0.3323455 Vali Loss: 0.7008100 Test Loss: 0.3808859
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.38937759399414
Epoch: 8, Steps: 61 | Train Loss: 0.3318909 Vali Loss: 0.6993601 Test Loss: 0.3804259
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.756940364837646
Epoch: 9, Steps: 61 | Train Loss: 0.3313253 Vali Loss: 0.6955104 Test Loss: 0.3801560
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.3801780343055725, mae:0.4030163288116455, rse:0.5856677889823914, corr:[0.27326077 0.2795433  0.2793635  0.27915856 0.27745104 0.2747533
 0.2731151  0.2729881  0.27245268 0.27162012 0.27157733 0.27198666
 0.27176252 0.270958   0.27060214 0.27082464 0.27068853 0.27024585
 0.2696564  0.2690117  0.2685014  0.268382   0.26831543 0.26852
 0.26874945 0.26890257 0.26884314 0.26858956 0.2682641  0.26781917
 0.26728407 0.26672092 0.266576   0.26690033 0.266987   0.26667204
 0.2661323  0.26592532 0.26620114 0.2665797  0.26702726 0.26734257
 0.26746327 0.26733896 0.2670799  0.2669277  0.267213   0.26779512
 0.26796997 0.26755208 0.2663722  0.26484624 0.2633186  0.26192105
 0.26126766 0.26112077 0.26100388 0.2609193  0.26040256 0.26035988
 0.2605699  0.26037467 0.25942275 0.25864637 0.25874028 0.25924838
 0.2598039  0.25944936 0.25806338 0.2567916  0.25699177 0.2581159
 0.2582657  0.25670928 0.25498492 0.25478506 0.2546005  0.25348976
 0.2525599  0.2522357  0.25152633 0.25069624 0.25031206 0.25000334
 0.24958645 0.24944152 0.24953093 0.24920122 0.24958907 0.25052056
 0.24990082 0.24845846 0.24781245 0.24635303 0.24604863 0.25084317]
