Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  275365888.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0441784858703613
Epoch: 1, Steps: 14 | Train Loss: 0.9275804 Vali Loss: 2.4191759 Test Loss: 1.0756313
Validation loss decreased (inf --> 2.419176).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.119964838027954
Epoch: 2, Steps: 14 | Train Loss: 0.8353898 Vali Loss: 2.2612290 Test Loss: 0.9902989
Validation loss decreased (2.419176 --> 2.261229).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.1912193298339844
Epoch: 3, Steps: 14 | Train Loss: 0.7665242 Vali Loss: 2.1518900 Test Loss: 0.9268094
Validation loss decreased (2.261229 --> 2.151890).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.5389795303344727
Epoch: 4, Steps: 14 | Train Loss: 0.7158235 Vali Loss: 2.0689719 Test Loss: 0.8795093
Validation loss decreased (2.151890 --> 2.068972).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.4707071781158447
Epoch: 5, Steps: 14 | Train Loss: 0.6773430 Vali Loss: 2.0084202 Test Loss: 0.8441768
Validation loss decreased (2.068972 --> 2.008420).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.3069992065429688
Epoch: 6, Steps: 14 | Train Loss: 0.6481387 Vali Loss: 1.9627258 Test Loss: 0.8173981
Validation loss decreased (2.008420 --> 1.962726).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.4565019607543945
Epoch: 7, Steps: 14 | Train Loss: 0.6253714 Vali Loss: 1.9243598 Test Loss: 0.7971681
Validation loss decreased (1.962726 --> 1.924360).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.486082077026367
Epoch: 8, Steps: 14 | Train Loss: 0.6071961 Vali Loss: 1.9031719 Test Loss: 0.7818013
Validation loss decreased (1.924360 --> 1.903172).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.131859064102173
Epoch: 9, Steps: 14 | Train Loss: 0.5925860 Vali Loss: 1.8796158 Test Loss: 0.7694460
Validation loss decreased (1.903172 --> 1.879616).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.16036057472229
Epoch: 10, Steps: 14 | Train Loss: 0.5805754 Vali Loss: 1.8573058 Test Loss: 0.7597474
Validation loss decreased (1.879616 --> 1.857306).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.271721839904785
Epoch: 11, Steps: 14 | Train Loss: 0.5704023 Vali Loss: 1.8467853 Test Loss: 0.7519392
Validation loss decreased (1.857306 --> 1.846785).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.1273019313812256
Epoch: 12, Steps: 14 | Train Loss: 0.5621150 Vali Loss: 1.8354630 Test Loss: 0.7455950
Validation loss decreased (1.846785 --> 1.835463).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.1293745040893555
Epoch: 13, Steps: 14 | Train Loss: 0.5549790 Vali Loss: 1.8213710 Test Loss: 0.7402412
Validation loss decreased (1.835463 --> 1.821371).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.2317698001861572
Epoch: 14, Steps: 14 | Train Loss: 0.5487901 Vali Loss: 1.8147190 Test Loss: 0.7357658
Validation loss decreased (1.821371 --> 1.814719).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.050330400466919
Epoch: 15, Steps: 14 | Train Loss: 0.5432289 Vali Loss: 1.8107295 Test Loss: 0.7320217
Validation loss decreased (1.814719 --> 1.810730).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.3778669834136963
Epoch: 16, Steps: 14 | Train Loss: 0.5380520 Vali Loss: 1.8031057 Test Loss: 0.7288945
Validation loss decreased (1.810730 --> 1.803106).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.083453893661499
Epoch: 17, Steps: 14 | Train Loss: 0.5338259 Vali Loss: 1.7978879 Test Loss: 0.7258815
Validation loss decreased (1.803106 --> 1.797888).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.2177038192749023
Epoch: 18, Steps: 14 | Train Loss: 0.5300622 Vali Loss: 1.7974631 Test Loss: 0.7232383
Validation loss decreased (1.797888 --> 1.797463).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.096930980682373
Epoch: 19, Steps: 14 | Train Loss: 0.5265208 Vali Loss: 1.7895536 Test Loss: 0.7209561
Validation loss decreased (1.797463 --> 1.789554).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.057023286819458
Epoch: 20, Steps: 14 | Train Loss: 0.5231966 Vali Loss: 1.7795010 Test Loss: 0.7189984
Validation loss decreased (1.789554 --> 1.779501).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.611985921859741
Epoch: 21, Steps: 14 | Train Loss: 0.5200801 Vali Loss: 1.7810985 Test Loss: 0.7169791
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.1521377563476562
Epoch: 22, Steps: 14 | Train Loss: 0.5175839 Vali Loss: 1.7810403 Test Loss: 0.7153276
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.0125465393066406
Epoch: 23, Steps: 14 | Train Loss: 0.5148769 Vali Loss: 1.7754170 Test Loss: 0.7137203
Validation loss decreased (1.779501 --> 1.775417).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.841123342514038
Epoch: 24, Steps: 14 | Train Loss: 0.5128823 Vali Loss: 1.7752285 Test Loss: 0.7123577
Validation loss decreased (1.775417 --> 1.775229).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.0843355655670166
Epoch: 25, Steps: 14 | Train Loss: 0.5104744 Vali Loss: 1.7700680 Test Loss: 0.7109247
Validation loss decreased (1.775229 --> 1.770068).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.0794827938079834
Epoch: 26, Steps: 14 | Train Loss: 0.5084136 Vali Loss: 1.7706578 Test Loss: 0.7096858
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.334796190261841
Epoch: 27, Steps: 14 | Train Loss: 0.5065506 Vali Loss: 1.7680050 Test Loss: 0.7084886
Validation loss decreased (1.770068 --> 1.768005).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.221489429473877
Epoch: 28, Steps: 14 | Train Loss: 0.5048657 Vali Loss: 1.7639101 Test Loss: 0.7073409
Validation loss decreased (1.768005 --> 1.763910).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.2887156009674072
Epoch: 29, Steps: 14 | Train Loss: 0.5031674 Vali Loss: 1.7620289 Test Loss: 0.7062580
Validation loss decreased (1.763910 --> 1.762029).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.9980900287628174
Epoch: 30, Steps: 14 | Train Loss: 0.5017313 Vali Loss: 1.7636526 Test Loss: 0.7053047
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.119906425476074
Epoch: 31, Steps: 14 | Train Loss: 0.5002176 Vali Loss: 1.7611468 Test Loss: 0.7044038
Validation loss decreased (1.762029 --> 1.761147).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.122039318084717
Epoch: 32, Steps: 14 | Train Loss: 0.4987993 Vali Loss: 1.7609699 Test Loss: 0.7035761
Validation loss decreased (1.761147 --> 1.760970).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.8765971660614014
Epoch: 33, Steps: 14 | Train Loss: 0.4975164 Vali Loss: 1.7550890 Test Loss: 0.7028257
Validation loss decreased (1.760970 --> 1.755089).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.871690511703491
Epoch: 34, Steps: 14 | Train Loss: 0.4967169 Vali Loss: 1.7615995 Test Loss: 0.7019540
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.969672203063965
Epoch: 35, Steps: 14 | Train Loss: 0.4954758 Vali Loss: 1.7618072 Test Loss: 0.7012910
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.060187816619873
Epoch: 36, Steps: 14 | Train Loss: 0.4942201 Vali Loss: 1.7537353 Test Loss: 0.7007118
Validation loss decreased (1.755089 --> 1.753735).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.822767734527588
Epoch: 37, Steps: 14 | Train Loss: 0.4934519 Vali Loss: 1.7575538 Test Loss: 0.6999957
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.1706109046936035
Epoch: 38, Steps: 14 | Train Loss: 0.4924285 Vali Loss: 1.7562442 Test Loss: 0.6993521
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.048175811767578
Epoch: 39, Steps: 14 | Train Loss: 0.4914080 Vali Loss: 1.7587572 Test Loss: 0.6987516
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  275365888.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0350518226623535
Epoch: 1, Steps: 14 | Train Loss: 0.7014819 Vali Loss: 1.7276672 Test Loss: 0.6804344
Validation loss decreased (inf --> 1.727667).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.2768640518188477
Epoch: 2, Steps: 14 | Train Loss: 0.6877578 Vali Loss: 1.6994729 Test Loss: 0.6626087
Validation loss decreased (1.727667 --> 1.699473).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.2233407497406006
Epoch: 3, Steps: 14 | Train Loss: 0.6761772 Vali Loss: 1.6843541 Test Loss: 0.6474327
Validation loss decreased (1.699473 --> 1.684354).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.445474863052368
Epoch: 4, Steps: 14 | Train Loss: 0.6666844 Vali Loss: 1.6652924 Test Loss: 0.6340095
Validation loss decreased (1.684354 --> 1.665292).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2736148834228516
Epoch: 5, Steps: 14 | Train Loss: 0.6589571 Vali Loss: 1.6488241 Test Loss: 0.6220068
Validation loss decreased (1.665292 --> 1.648824).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.1813113689422607
Epoch: 6, Steps: 14 | Train Loss: 0.6519042 Vali Loss: 1.6367493 Test Loss: 0.6113091
Validation loss decreased (1.648824 --> 1.636749).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1711008548736572
Epoch: 7, Steps: 14 | Train Loss: 0.6458378 Vali Loss: 1.6249833 Test Loss: 0.6017933
Validation loss decreased (1.636749 --> 1.624983).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.3906707763671875
Epoch: 8, Steps: 14 | Train Loss: 0.6407261 Vali Loss: 1.6111195 Test Loss: 0.5928714
Validation loss decreased (1.624983 --> 1.611120).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.195375919342041
Epoch: 9, Steps: 14 | Train Loss: 0.6359126 Vali Loss: 1.6049109 Test Loss: 0.5849967
Validation loss decreased (1.611120 --> 1.604911).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.4290404319763184
Epoch: 10, Steps: 14 | Train Loss: 0.6315298 Vali Loss: 1.5949609 Test Loss: 0.5777387
Validation loss decreased (1.604911 --> 1.594961).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.3449296951293945
Epoch: 11, Steps: 14 | Train Loss: 0.6271137 Vali Loss: 1.5823917 Test Loss: 0.5711268
Validation loss decreased (1.594961 --> 1.582392).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.7166357040405273
Epoch: 12, Steps: 14 | Train Loss: 0.6238914 Vali Loss: 1.5757822 Test Loss: 0.5650878
Validation loss decreased (1.582392 --> 1.575782).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.55503511428833
Epoch: 13, Steps: 14 | Train Loss: 0.6206514 Vali Loss: 1.5667160 Test Loss: 0.5596436
Validation loss decreased (1.575782 --> 1.566716).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.750112771987915
Epoch: 14, Steps: 14 | Train Loss: 0.6179013 Vali Loss: 1.5624204 Test Loss: 0.5545191
Validation loss decreased (1.566716 --> 1.562420).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.888791084289551
Epoch: 15, Steps: 14 | Train Loss: 0.6149623 Vali Loss: 1.5584904 Test Loss: 0.5498832
Validation loss decreased (1.562420 --> 1.558490).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.550135374069214
Epoch: 16, Steps: 14 | Train Loss: 0.6123691 Vali Loss: 1.5535810 Test Loss: 0.5457112
Validation loss decreased (1.558490 --> 1.553581).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.414064645767212
Epoch: 17, Steps: 14 | Train Loss: 0.6100880 Vali Loss: 1.5503657 Test Loss: 0.5417216
Validation loss decreased (1.553581 --> 1.550366).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8112516403198242
Epoch: 18, Steps: 14 | Train Loss: 0.6081779 Vali Loss: 1.5467318 Test Loss: 0.5381050
Validation loss decreased (1.550366 --> 1.546732).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.8694865703582764
Epoch: 19, Steps: 14 | Train Loss: 0.6062540 Vali Loss: 1.5388775 Test Loss: 0.5347915
Validation loss decreased (1.546732 --> 1.538877).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.575334072113037
Epoch: 20, Steps: 14 | Train Loss: 0.6045887 Vali Loss: 1.5351592 Test Loss: 0.5317075
Validation loss decreased (1.538877 --> 1.535159).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.5804898738861084
Epoch: 21, Steps: 14 | Train Loss: 0.6029073 Vali Loss: 1.5372711 Test Loss: 0.5288266
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.2570929527282715
Epoch: 22, Steps: 14 | Train Loss: 0.6010648 Vali Loss: 1.5330071 Test Loss: 0.5261791
Validation loss decreased (1.535159 --> 1.533007).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.111588716506958
Epoch: 23, Steps: 14 | Train Loss: 0.5999918 Vali Loss: 1.5257001 Test Loss: 0.5237926
Validation loss decreased (1.533007 --> 1.525700).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.060450553894043
Epoch: 24, Steps: 14 | Train Loss: 0.5986737 Vali Loss: 1.5208091 Test Loss: 0.5215046
Validation loss decreased (1.525700 --> 1.520809).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.997114896774292
Epoch: 25, Steps: 14 | Train Loss: 0.5976082 Vali Loss: 1.5203900 Test Loss: 0.5193539
Validation loss decreased (1.520809 --> 1.520390).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.896777629852295
Epoch: 26, Steps: 14 | Train Loss: 0.5962434 Vali Loss: 1.5217443 Test Loss: 0.5174206
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.7543742656707764
Epoch: 27, Steps: 14 | Train Loss: 0.5951273 Vali Loss: 1.5251160 Test Loss: 0.5155663
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.762890100479126
Epoch: 28, Steps: 14 | Train Loss: 0.5942757 Vali Loss: 1.5157218 Test Loss: 0.5138689
Validation loss decreased (1.520390 --> 1.515722).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.976404905319214
Epoch: 29, Steps: 14 | Train Loss: 0.5936786 Vali Loss: 1.5113596 Test Loss: 0.5122710
Validation loss decreased (1.515722 --> 1.511360).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.6678059101104736
Epoch: 30, Steps: 14 | Train Loss: 0.5926773 Vali Loss: 1.5111682 Test Loss: 0.5107685
Validation loss decreased (1.511360 --> 1.511168).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.015312194824219
Epoch: 31, Steps: 14 | Train Loss: 0.5919927 Vali Loss: 1.5119866 Test Loss: 0.5093480
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.813581705093384
Epoch: 32, Steps: 14 | Train Loss: 0.5908792 Vali Loss: 1.5115465 Test Loss: 0.5080301
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.040593385696411
Epoch: 33, Steps: 14 | Train Loss: 0.5900700 Vali Loss: 1.5093678 Test Loss: 0.5068194
Validation loss decreased (1.511168 --> 1.509368).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.0254065990448
Epoch: 34, Steps: 14 | Train Loss: 0.5896358 Vali Loss: 1.5090746 Test Loss: 0.5056835
Validation loss decreased (1.509368 --> 1.509075).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.656848907470703
Epoch: 35, Steps: 14 | Train Loss: 0.5887229 Vali Loss: 1.5030812 Test Loss: 0.5046076
Validation loss decreased (1.509075 --> 1.503081).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.579669237136841
Epoch: 36, Steps: 14 | Train Loss: 0.5884989 Vali Loss: 1.5114090 Test Loss: 0.5035976
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.668976068496704
Epoch: 37, Steps: 14 | Train Loss: 0.5878672 Vali Loss: 1.5037481 Test Loss: 0.5026410
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.6304314136505127
Epoch: 38, Steps: 14 | Train Loss: 0.5871256 Vali Loss: 1.4992387 Test Loss: 0.5017471
Validation loss decreased (1.503081 --> 1.499239).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.6629421710968018
Epoch: 39, Steps: 14 | Train Loss: 0.5872414 Vali Loss: 1.4955848 Test Loss: 0.5009179
Validation loss decreased (1.499239 --> 1.495585).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.960096597671509
Epoch: 40, Steps: 14 | Train Loss: 0.5863681 Vali Loss: 1.4972594 Test Loss: 0.5001098
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.630330801010132
Epoch: 41, Steps: 14 | Train Loss: 0.5862779 Vali Loss: 1.4985530 Test Loss: 0.4993657
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.672572135925293
Epoch: 42, Steps: 14 | Train Loss: 0.5855125 Vali Loss: 1.5003943 Test Loss: 0.4986800
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4787750840187073, mae:0.4918869435787201, rse:0.6623960137367249, corr:[0.21547085 0.22319476 0.21727178 0.22896795 0.22985353 0.22436513
 0.22578175 0.23027529 0.23054856 0.22908337 0.22858278 0.22894138
 0.2288649  0.2270485  0.22452222 0.223915   0.2245934  0.22334473
 0.22066362 0.2203644  0.22168295 0.22090676 0.21905057 0.22009462
 0.22226171 0.22224182 0.22139627 0.22248659 0.2242381  0.22422804
 0.22316061 0.22283597 0.22331142 0.22330387 0.22229785 0.22116338
 0.22099979 0.22131091 0.22089414 0.21947297 0.21887156 0.2196114
 0.21992482 0.21901098 0.21869823 0.22011648 0.22131453 0.22121
 0.22136682 0.22260809 0.22298208 0.22190048 0.22078927 0.22030124
 0.21992151 0.21875703 0.21764438 0.21729644 0.21722545 0.21696599
 0.21611673 0.2151881  0.21505038 0.21534656 0.21493404 0.21428806
 0.21483791 0.21586284 0.21574758 0.21488336 0.21492967 0.21594188
 0.21565372 0.21428178 0.21373239 0.21454675 0.21471375 0.21369582
 0.21286784 0.21286803 0.21281894 0.21212228 0.21113595 0.21066877
 0.21060304 0.21016438 0.20909044 0.20820266 0.20817624 0.20840421
 0.20782104 0.2070793  0.20730884 0.20803943 0.20780885 0.20773958
 0.20929746 0.21139064 0.21234591 0.21263961 0.21325521 0.21389882
 0.21377836 0.21307354 0.21265651 0.21269162 0.21248144 0.21170413
 0.21089223 0.21065113 0.21086627 0.21082814 0.21033229 0.21015774
 0.21078375 0.21111217 0.21049534 0.20987868 0.21029967 0.2111281
 0.21114726 0.21052851 0.21023437 0.21040371 0.20983505 0.2087388
 0.20850372 0.20905156 0.20893513 0.20799936 0.20736241 0.20743673
 0.20744203 0.20669279 0.20590623 0.20594086 0.20638765 0.20630947
 0.20575425 0.20561445 0.20610102 0.20614237 0.2050122  0.20426576
 0.20480315 0.2052637  0.204764   0.20390016 0.20358825 0.20354022
 0.2031464  0.20255771 0.20249599 0.20283586 0.20258272 0.20174024
 0.20114793 0.20126924 0.2013631  0.20087829 0.20044276 0.20095171
 0.20182925 0.20171115 0.20066623 0.20024233 0.20020643 0.20001394
 0.19944519 0.19966213 0.20097986 0.2022426  0.2021121  0.2013616
 0.20135434 0.20192043 0.20167495 0.20083892 0.2004561  0.20069125
 0.20060495 0.19997157 0.19973524 0.20023629 0.2007836  0.20079458
 0.20080017 0.20134765 0.20199995 0.20186788 0.2012452  0.20112745
 0.20143369 0.20121329 0.20032708 0.19964433 0.19957243 0.19941074
 0.19873022 0.19820993 0.19836746 0.19858226 0.19813974 0.19760962
 0.19788517 0.1988588  0.19926159 0.19883987 0.19868852 0.1991717
 0.19943894 0.1987878  0.19791561 0.19770715 0.1977225  0.19739684
 0.19700418 0.19726278 0.19791467 0.19819929 0.19778991 0.19726352
 0.19746569 0.19769007 0.19685984 0.1957144  0.19543138 0.19570923
 0.19524248 0.19442552 0.19439249 0.19483185 0.19472253 0.1941788
 0.19397473 0.19416687 0.19409482 0.1935087  0.19298884 0.1932628
 0.19415462 0.19465946 0.19485442 0.19531219 0.19601797 0.1962146
 0.1956425  0.19506688 0.19516355 0.1951713  0.1942903  0.19332676
 0.19327296 0.193725   0.1937264  0.1932851  0.19334787 0.19412255
 0.19462653 0.19422838 0.19379018 0.19405332 0.19456694 0.19455443
 0.1941388  0.19421811 0.19489776 0.19530468 0.19468576 0.19408107
 0.19433583 0.19438876 0.19348489 0.19277483 0.19295442 0.19338542
 0.19328798 0.19330306 0.19359945 0.19379552 0.19335435 0.1929565
 0.19303349 0.19354343 0.19345342 0.1926244  0.19178617 0.19203734
 0.19294241 0.1936667  0.19416142 0.19490863 0.19566914 0.19576512
 0.19526075 0.19518068 0.19564332 0.19575484 0.19522844 0.19515604
 0.19578177 0.19608429 0.19573008 0.19547963 0.19588372 0.19641961
 0.19665213 0.19650576 0.19643873 0.19653322 0.19644822 0.19629887
 0.1964238  0.19694977 0.19776468 0.1980504  0.19729897 0.19658758
 0.19658868 0.19614074 0.19493657 0.19425373 0.19463703 0.19486941
 0.19434187 0.194073   0.19468957 0.19528916 0.19507268 0.19492279
 0.1954692  0.19601764 0.19593896 0.19558676 0.19543393 0.19559617
 0.19558866 0.19516627 0.1949764  0.19544882 0.19556233 0.19450235
 0.19313747 0.19261166 0.19265641 0.19244032 0.19190781 0.19193833
 0.19245504 0.19266184 0.19235235 0.19237156 0.19301899 0.19376926
 0.19381244 0.19346984 0.1934961  0.19390443 0.19356258 0.19275905
 0.19280073 0.19363146 0.19420208 0.1937264  0.19270013 0.19225961
 0.19237642 0.19177599 0.19054076 0.18997632 0.19002563 0.1895419
 0.18845911 0.18786852 0.18784763 0.18757565 0.18713306 0.18704511
 0.18724683 0.18713318 0.18663435 0.18631256 0.18626226 0.18685533
 0.18791759 0.1889917  0.19019087 0.19172409 0.19250353 0.1913795
 0.18974379 0.18905091 0.18863918 0.18734662 0.18578878 0.18567425
 0.1862909  0.18630758 0.18617238 0.18652363 0.18708073 0.1873648
 0.18753004 0.18777262 0.18802541 0.18813051 0.18786494 0.18759885
 0.18804277 0.18916671 0.19027689 0.19071294 0.19077623 0.19088727
 0.19090429 0.18992461 0.18891416 0.18905777 0.18938729 0.18879284
 0.18801257 0.18823086 0.18850891 0.18813534 0.18764974 0.18770221
 0.18795322 0.18785812 0.18710321 0.18673013 0.18705408 0.18791887
 0.18875715 0.1891462  0.18978389 0.19109507 0.19177534 0.19100069
 0.19035456 0.19068329 0.19071744 0.19001818 0.18960027 0.1901691
 0.19050083 0.18980509 0.18931466 0.18939027 0.18938012 0.18895303
 0.18863508 0.18851806 0.18847744 0.18838762 0.1882802  0.18859647
 0.18958548 0.19093496 0.19174221 0.19148901 0.19095117 0.1907791
 0.19063017 0.1896792  0.18911193 0.1898831  0.19018133 0.1894789
 0.18926534 0.190319   0.19114788 0.19083387 0.19043337 0.19081807
 0.19104066 0.19052416 0.18985204 0.18944661 0.18940425 0.1894553
 0.18951873 0.18955784 0.18980716 0.19038816 0.19041677 0.18966922
 0.18949978 0.18996143 0.18946664 0.18838674 0.18816715 0.18891466
 0.18907367 0.18827169 0.18801172 0.1885013  0.18870464 0.18842617
 0.18805826 0.1876225  0.18725193 0.18719591 0.18708725 0.18722232
 0.18821982 0.1897708  0.19067484 0.19113277 0.19190028 0.19213152
 0.19108388 0.18972717 0.18942934 0.18963856 0.189147   0.18849625
 0.1889195  0.190015   0.19081348 0.19095676 0.19129664 0.1917388
 0.1919731  0.19187027 0.19178697 0.19153696 0.1921293  0.19248684
 0.19246657 0.19217592 0.19251648 0.19351636 0.19316122 0.19198576
 0.1918059  0.19230238 0.19181445 0.19107711 0.19133894 0.1920242
 0.19167125 0.19115633 0.19213976 0.19336465 0.19377553 0.19373219
 0.1938644  0.19374265 0.1932795  0.19286172 0.19222157 0.19125387
 0.19075164 0.19082151 0.19031675 0.1896812  0.18998235 0.19014502
 0.18898122 0.18772794 0.18795474 0.18833756 0.18735163 0.18646616
 0.18689448 0.18720503 0.18676466 0.18598342 0.18613613 0.18629886
 0.1862279  0.1859436  0.18567075 0.18535672 0.18524753 0.18475549
 0.18364559 0.18327804 0.18417753 0.18544321 0.18491781 0.18366085
 0.18417355 0.18450747 0.1834612  0.18268068 0.18381482 0.1845184
 0.18370588 0.18297254 0.18365352 0.1841479  0.18382324 0.18375823
 0.1842268  0.18451913 0.18456699 0.1844557  0.18372941 0.18287367
 0.18291728 0.18285993 0.18186827 0.18146154 0.18204446 0.18125148
 0.17907138 0.17830034 0.17916511 0.17925687 0.17799892 0.17795825
 0.1789502  0.17864728 0.17767648 0.17802858 0.17912783 0.17925031
 0.17884131 0.17868568 0.17873007 0.17908937 0.17975783 0.17943119
 0.17822567 0.17832667 0.17955202 0.17933223 0.17740794 0.176849
 0.17829281 0.17813124 0.17623955 0.17620747 0.17753044 0.17754841
 0.17671087 0.17710905 0.17821284 0.17788845 0.1768869  0.17685537
 0.17721507 0.17715223 0.17691286 0.17661752 0.17639719 0.17688067
 0.17747146 0.17668825 0.17525041 0.17596315 0.17691451 0.17548726
 0.17351016 0.17376275 0.1747224  0.17369361 0.1722751  0.17290112
 0.1739894  0.1734572  0.17284322 0.17320552 0.17358999 0.17360012
 0.17366283 0.17352656 0.1730834  0.17336495 0.17328891 0.17158832
 0.16955481 0.16951181 0.16928887 0.16684513 0.16536736 0.16591258
 0.16570696 0.16347758 0.16321681 0.16488303 0.1651468  0.16403997
 0.16428217 0.16525012 0.16529691 0.16475795 0.16505817 0.16548574
 0.1654271  0.16535588 0.16462666 0.16308706 0.16312963 0.16391134
 0.16166258 0.15885642 0.1602105  0.16345125 0.16130456 0.15765356
 0.16001342 0.1627899  0.16079044 0.1587849  0.16056669 0.16123472
 0.15964478 0.15825364 0.15762693 0.15673594 0.15651283 0.15385701
 0.14804316 0.14938545 0.14953032 0.12479848 0.12593417 0.1442106 ]
