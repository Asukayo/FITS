Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.327872514724731
Epoch: 1, Steps: 56 | Train Loss: 0.8184630 Vali Loss: 2.0696192 Test Loss: 0.8643990
Validation loss decreased (inf --> 2.069619).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.165192127227783
Epoch: 2, Steps: 56 | Train Loss: 0.6372918 Vali Loss: 1.8764153 Test Loss: 0.7523523
Validation loss decreased (2.069619 --> 1.876415).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.665428876876831
Epoch: 3, Steps: 56 | Train Loss: 0.5638902 Vali Loss: 1.8121651 Test Loss: 0.7125830
Validation loss decreased (1.876415 --> 1.812165).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.86413860321045
Epoch: 4, Steps: 56 | Train Loss: 0.5265481 Vali Loss: 1.7760214 Test Loss: 0.6904148
Validation loss decreased (1.812165 --> 1.776021).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.318729877471924
Epoch: 5, Steps: 56 | Train Loss: 0.5020056 Vali Loss: 1.7501311 Test Loss: 0.6752229
Validation loss decreased (1.776021 --> 1.750131).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.12209177017212
Epoch: 6, Steps: 56 | Train Loss: 0.4827942 Vali Loss: 1.7325850 Test Loss: 0.6619548
Validation loss decreased (1.750131 --> 1.732585).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.272250175476074
Epoch: 7, Steps: 56 | Train Loss: 0.4669540 Vali Loss: 1.7175539 Test Loss: 0.6511230
Validation loss decreased (1.732585 --> 1.717554).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.474466800689697
Epoch: 8, Steps: 56 | Train Loss: 0.4535033 Vali Loss: 1.7055610 Test Loss: 0.6402756
Validation loss decreased (1.717554 --> 1.705561).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.757103443145752
Epoch: 9, Steps: 56 | Train Loss: 0.4416094 Vali Loss: 1.6877644 Test Loss: 0.6305857
Validation loss decreased (1.705561 --> 1.687764).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.50147819519043
Epoch: 10, Steps: 56 | Train Loss: 0.4314231 Vali Loss: 1.6804368 Test Loss: 0.6222580
Validation loss decreased (1.687764 --> 1.680437).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.665971040725708
Epoch: 11, Steps: 56 | Train Loss: 0.4221990 Vali Loss: 1.6658936 Test Loss: 0.6135620
Validation loss decreased (1.680437 --> 1.665894).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.760821342468262
Epoch: 12, Steps: 56 | Train Loss: 0.4140853 Vali Loss: 1.6573943 Test Loss: 0.6060133
Validation loss decreased (1.665894 --> 1.657394).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 7.963731527328491
Epoch: 13, Steps: 56 | Train Loss: 0.4068034 Vali Loss: 1.6486226 Test Loss: 0.5987916
Validation loss decreased (1.657394 --> 1.648623).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.280991315841675
Epoch: 14, Steps: 56 | Train Loss: 0.4001589 Vali Loss: 1.6448588 Test Loss: 0.5925990
Validation loss decreased (1.648623 --> 1.644859).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.105664491653442
Epoch: 15, Steps: 56 | Train Loss: 0.3944876 Vali Loss: 1.6332824 Test Loss: 0.5859108
Validation loss decreased (1.644859 --> 1.633282).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.043567895889282
Epoch: 16, Steps: 56 | Train Loss: 0.3890884 Vali Loss: 1.6262410 Test Loss: 0.5805560
Validation loss decreased (1.633282 --> 1.626241).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.633235692977905
Epoch: 17, Steps: 56 | Train Loss: 0.3844310 Vali Loss: 1.6107944 Test Loss: 0.5756236
Validation loss decreased (1.626241 --> 1.610794).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.854132175445557
Epoch: 18, Steps: 56 | Train Loss: 0.3800061 Vali Loss: 1.6058924 Test Loss: 0.5705850
Validation loss decreased (1.610794 --> 1.605892).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.17006516456604
Epoch: 19, Steps: 56 | Train Loss: 0.3759637 Vali Loss: 1.6041858 Test Loss: 0.5661918
Validation loss decreased (1.605892 --> 1.604186).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.43465805053711
Epoch: 20, Steps: 56 | Train Loss: 0.3723719 Vali Loss: 1.5965211 Test Loss: 0.5617803
Validation loss decreased (1.604186 --> 1.596521).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.427566528320312
Epoch: 21, Steps: 56 | Train Loss: 0.3688836 Vali Loss: 1.5945144 Test Loss: 0.5579640
Validation loss decreased (1.596521 --> 1.594514).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.545016765594482
Epoch: 22, Steps: 56 | Train Loss: 0.3660276 Vali Loss: 1.5857265 Test Loss: 0.5541051
Validation loss decreased (1.594514 --> 1.585726).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.28469443321228
Epoch: 23, Steps: 56 | Train Loss: 0.3632503 Vali Loss: 1.5793499 Test Loss: 0.5505905
Validation loss decreased (1.585726 --> 1.579350).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 10.408814430236816
Epoch: 24, Steps: 56 | Train Loss: 0.3605286 Vali Loss: 1.5795965 Test Loss: 0.5475148
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.954949140548706
Epoch: 25, Steps: 56 | Train Loss: 0.3579152 Vali Loss: 1.5747890 Test Loss: 0.5445672
Validation loss decreased (1.579350 --> 1.574789).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.889143943786621
Epoch: 26, Steps: 56 | Train Loss: 0.3558257 Vali Loss: 1.5753232 Test Loss: 0.5416381
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.14447808265686
Epoch: 27, Steps: 56 | Train Loss: 0.3540797 Vali Loss: 1.5733875 Test Loss: 0.5390906
Validation loss decreased (1.574789 --> 1.573388).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.479829788208008
Epoch: 28, Steps: 56 | Train Loss: 0.3518501 Vali Loss: 1.5662479 Test Loss: 0.5365958
Validation loss decreased (1.573388 --> 1.566248).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 10.328382968902588
Epoch: 29, Steps: 56 | Train Loss: 0.3500566 Vali Loss: 1.5658035 Test Loss: 0.5342110
Validation loss decreased (1.566248 --> 1.565804).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 10.884207248687744
Epoch: 30, Steps: 56 | Train Loss: 0.3484350 Vali Loss: 1.5559396 Test Loss: 0.5319804
Validation loss decreased (1.565804 --> 1.555940).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 10.717530012130737
Epoch: 31, Steps: 56 | Train Loss: 0.3470297 Vali Loss: 1.5573062 Test Loss: 0.5300654
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 10.770354509353638
Epoch: 32, Steps: 56 | Train Loss: 0.3455166 Vali Loss: 1.5585676 Test Loss: 0.5282019
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 10.555750131607056
Epoch: 33, Steps: 56 | Train Loss: 0.3440953 Vali Loss: 1.5520895 Test Loss: 0.5263165
Validation loss decreased (1.555940 --> 1.552089).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 10.184694528579712
Epoch: 34, Steps: 56 | Train Loss: 0.3430581 Vali Loss: 1.5463097 Test Loss: 0.5244096
Validation loss decreased (1.552089 --> 1.546310).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 10.616745471954346
Epoch: 35, Steps: 56 | Train Loss: 0.3416733 Vali Loss: 1.5464288 Test Loss: 0.5229236
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 10.266201734542847
Epoch: 36, Steps: 56 | Train Loss: 0.3405694 Vali Loss: 1.5420871 Test Loss: 0.5214616
Validation loss decreased (1.546310 --> 1.542087).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 9.610124349594116
Epoch: 37, Steps: 56 | Train Loss: 0.3395812 Vali Loss: 1.5471294 Test Loss: 0.5198900
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 10.09181261062622
Epoch: 38, Steps: 56 | Train Loss: 0.3386586 Vali Loss: 1.5430830 Test Loss: 0.5186669
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 10.196912050247192
Epoch: 39, Steps: 56 | Train Loss: 0.3378511 Vali Loss: 1.5416504 Test Loss: 0.5172791
Validation loss decreased (1.542087 --> 1.541650).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 10.884464979171753
Epoch: 40, Steps: 56 | Train Loss: 0.3369626 Vali Loss: 1.5370498 Test Loss: 0.5161434
Validation loss decreased (1.541650 --> 1.537050).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 10.248692989349365
Epoch: 41, Steps: 56 | Train Loss: 0.3360945 Vali Loss: 1.5384724 Test Loss: 0.5149786
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 9.3827645778656
Epoch: 42, Steps: 56 | Train Loss: 0.3354047 Vali Loss: 1.5408274 Test Loss: 0.5139202
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 10.01682710647583
Epoch: 43, Steps: 56 | Train Loss: 0.3348475 Vali Loss: 1.5330907 Test Loss: 0.5128685
Validation loss decreased (1.537050 --> 1.533091).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 10.054942607879639
Epoch: 44, Steps: 56 | Train Loss: 0.3338163 Vali Loss: 1.5365639 Test Loss: 0.5119092
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 10.514764547348022
Epoch: 45, Steps: 56 | Train Loss: 0.3333411 Vali Loss: 1.5289975 Test Loss: 0.5109681
Validation loss decreased (1.533091 --> 1.528998).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 10.521703958511353
Epoch: 46, Steps: 56 | Train Loss: 0.3326232 Vali Loss: 1.5331608 Test Loss: 0.5101401
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 9.678685903549194
Epoch: 47, Steps: 56 | Train Loss: 0.3320600 Vali Loss: 1.5297773 Test Loss: 0.5093560
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 10.279810905456543
Epoch: 48, Steps: 56 | Train Loss: 0.3316422 Vali Loss: 1.5298839 Test Loss: 0.5085170
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.195182800292969
Epoch: 1, Steps: 56 | Train Loss: 0.5896452 Vali Loss: 1.4928408 Test Loss: 0.4772651
Validation loss decreased (inf --> 1.492841).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.65624713897705
Epoch: 2, Steps: 56 | Train Loss: 0.5741296 Vali Loss: 1.4678648 Test Loss: 0.4565143
Validation loss decreased (1.492841 --> 1.467865).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.335119724273682
Epoch: 3, Steps: 56 | Train Loss: 0.5647958 Vali Loss: 1.4482470 Test Loss: 0.4445477
Validation loss decreased (1.467865 --> 1.448247).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.22514796257019
Epoch: 4, Steps: 56 | Train Loss: 0.5589941 Vali Loss: 1.4430745 Test Loss: 0.4378557
Validation loss decreased (1.448247 --> 1.443074).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.476944923400879
Epoch: 5, Steps: 56 | Train Loss: 0.5552974 Vali Loss: 1.4400198 Test Loss: 0.4343582
Validation loss decreased (1.443074 --> 1.440020).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.726238250732422
Epoch: 6, Steps: 56 | Train Loss: 0.5530928 Vali Loss: 1.4318607 Test Loss: 0.4326360
Validation loss decreased (1.440020 --> 1.431861).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.98923397064209
Epoch: 7, Steps: 56 | Train Loss: 0.5520159 Vali Loss: 1.4289852 Test Loss: 0.4317097
Validation loss decreased (1.431861 --> 1.428985).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.477968692779541
Epoch: 8, Steps: 56 | Train Loss: 0.5508517 Vali Loss: 1.4341855 Test Loss: 0.4317410
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.17431640625
Epoch: 9, Steps: 56 | Train Loss: 0.5504486 Vali Loss: 1.4335155 Test Loss: 0.4316058
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.434990167617798
Epoch: 10, Steps: 56 | Train Loss: 0.5495186 Vali Loss: 1.4403036 Test Loss: 0.4318349
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.430771142244339, mae:0.45641472935676575, rse:0.6283118724822998, corr:[0.22500224 0.23264697 0.23123716 0.23356308 0.23346364 0.23015136
 0.22939514 0.23161745 0.23204957 0.23086658 0.23029935 0.23081379
 0.23089746 0.23002629 0.22934377 0.22920687 0.22896545 0.22838333
 0.22784747 0.22777925 0.22807246 0.22843486 0.22865483 0.22911993
 0.22972913 0.22982354 0.22960608 0.22982539 0.23028621 0.22988936
 0.22883594 0.22827512 0.22867416 0.2290074  0.22850119 0.22788705
 0.2279563  0.2280785  0.22765824 0.22696257 0.22692895 0.22743084
 0.22785033 0.22775654 0.22764623 0.22817534 0.22917682 0.22982693
 0.22984774 0.22959577 0.22908892 0.22824968 0.22726668 0.22636692
 0.22573024 0.2249258  0.22427261 0.22409236 0.22396961 0.22379039
 0.22331119 0.22279857 0.22252908 0.2225769  0.22258922 0.22251956
 0.22257757 0.22264269 0.22266254 0.22281694 0.22297405 0.22305363
 0.22251755 0.22174852 0.2213837  0.22155172 0.22143488 0.22072984
 0.22001745 0.21959206 0.21929002 0.21897659 0.21854661 0.21819824
 0.21802041 0.21776547 0.21733822 0.21695119 0.21688949 0.21707182
 0.21696728 0.21671474 0.21668185 0.21700642 0.21741284 0.21839948
 0.22002564 0.2214101  0.22186883 0.2219247  0.2222047  0.2226919
 0.22276431 0.22241476 0.22208896 0.22189571 0.22151773 0.22102916
 0.22085261 0.2207977  0.2206276  0.22052598 0.22068189 0.2209327
 0.22117996 0.22118089 0.22083746 0.22038873 0.22037855 0.22090895
 0.22125947 0.22075988 0.21980943 0.21926138 0.2186791  0.21775763
 0.21716227 0.21733198 0.21758924 0.21752194 0.2172188  0.2169498
 0.21677816 0.21652679 0.21634063 0.21633257 0.21641453 0.21633257
 0.2160565  0.21578933 0.21591465 0.21627133 0.21623209 0.21604715
 0.21597292 0.21575516 0.21541153 0.2149789  0.21460551 0.2141726
 0.21356457 0.21325788 0.21348567 0.21377298 0.21354996 0.21319248
 0.2131441  0.21321319 0.21309519 0.21283492 0.21260442 0.21267332
 0.21303661 0.21342856 0.21346441 0.21341103 0.21344346 0.21377844
 0.21396436 0.21413326 0.21482486 0.21573777 0.21570799 0.21489745
 0.21444635 0.2144813  0.21415612 0.21366633 0.21350957 0.21339895
 0.21295092 0.21264043 0.21293302 0.21321142 0.2131352  0.21325244
 0.21395342 0.21458128 0.21476465 0.21459576 0.21459045 0.21472941
 0.2145241  0.21373104 0.21260232 0.21177886 0.21161488 0.21169148
 0.2113713  0.21099976 0.21096979 0.21104534 0.21078876 0.21061492
 0.21079549 0.21092315 0.21063852 0.21041921 0.21079084 0.21123171
 0.21111372 0.21053141 0.21014646 0.21026027 0.21074179 0.21119905
 0.21119419 0.21093194 0.21089078 0.21106607 0.21092594 0.21063161
 0.21087709 0.21108693 0.21030934 0.20910557 0.2085824  0.20876491
 0.20863521 0.20791796 0.2072951  0.2070673  0.20711748 0.20731765
 0.20745118 0.20745206 0.20751968 0.20752202 0.20729913 0.20719865
 0.20780545 0.20858514 0.208806   0.20871963 0.20897764 0.2092777
 0.2090214  0.20847486 0.20812    0.20761013 0.20669198 0.20619619
 0.20683323 0.20767839 0.20773456 0.20712972 0.20675863 0.20700115
 0.20751165 0.20764388 0.20740703 0.20715791 0.20713761 0.20726705
 0.20709306 0.20669058 0.20652352 0.20639566 0.2057491  0.20521651
 0.20544712 0.2057741  0.20554678 0.20525591 0.2050786  0.20506832
 0.20558736 0.2066784  0.2069449  0.20591955 0.20500043 0.20561756
 0.20664339 0.20666507 0.20586656 0.20539388 0.2055835  0.20625326
 0.20694001 0.207419   0.20762259 0.20777635 0.20806482 0.20840962
 0.20863706 0.20895465 0.2090282  0.2083611  0.20748216 0.20750952
 0.20819844 0.20842056 0.20843308 0.20864019 0.20851907 0.20767164
 0.20695795 0.20713231 0.20727304 0.2068453  0.2068162  0.20800436
 0.20924203 0.20955287 0.20947869 0.20916967 0.20808458 0.20705736
 0.20723002 0.20748973 0.20675121 0.20602173 0.20628946 0.20664188
 0.20620687 0.20595132 0.2064184  0.20675996 0.20634794 0.20604
 0.20613638 0.20601161 0.20575625 0.20592855 0.20629197 0.2064942
 0.20661746 0.20630641 0.20534585 0.204407   0.2043036  0.20471711
 0.20479554 0.20445342 0.20410846 0.20373742 0.20319554 0.20298822
 0.20331357 0.20363896 0.20353824 0.20354152 0.2038365  0.20397829
 0.20364965 0.20364515 0.20449145 0.20538054 0.20523164 0.20465347
 0.2044642  0.20436704 0.20387925 0.20295218 0.20189214 0.20120804
 0.20097682 0.20075838 0.20040616 0.20013833 0.1997839  0.19931985
 0.19924517 0.19981684 0.20040338 0.20032346 0.19985296 0.19925289
 0.19880548 0.19888799 0.19959082 0.20033151 0.20030308 0.20024472
 0.20063874 0.20108713 0.20153108 0.20209463 0.2019141  0.20058745
 0.19971392 0.20020036 0.20063803 0.19961023 0.19781834 0.19730161
 0.19812836 0.19900739 0.19942757 0.19952086 0.19958128 0.19983245
 0.20006719 0.20000766 0.19982396 0.19978082 0.19962803 0.1994473
 0.19986422 0.20094192 0.2018189  0.20189436 0.20187011 0.20222133
 0.2022713  0.20138758 0.20079145 0.20125897 0.20155104 0.20103833
 0.20075944 0.2011409  0.20084682 0.19980697 0.19923492 0.19948682
 0.19994222 0.2004109  0.20058995 0.20064121 0.20068878 0.20160246
 0.20294836 0.20303613 0.20200334 0.20145015 0.20146999 0.20121898
 0.20126003 0.20210566 0.20238002 0.20150425 0.2005592  0.2002601
 0.19977544 0.19907938 0.19953519 0.20079458 0.2011787  0.20063493
 0.20038936 0.20053037 0.2006399  0.20099765 0.20160937 0.20198348
 0.20187831 0.20201644 0.20236169 0.20193365 0.2010729  0.2011029
 0.20161924 0.20107985 0.20015243 0.2004288  0.20115001 0.20123804
 0.20115277 0.201265   0.20077623 0.19989678 0.20010942 0.20149487
 0.20210499 0.2013975  0.20078607 0.20095874 0.20150177 0.20171385
 0.20146951 0.20079762 0.2001572  0.20011488 0.20023102 0.20000213
 0.1997762  0.19981189 0.19966197 0.19957593 0.19977197 0.19989772
 0.19962321 0.19956699 0.20019865 0.20032784 0.19941537 0.19866255
 0.19883803 0.19906497 0.19902864 0.19942203 0.19995709 0.20011038
 0.20045419 0.20163517 0.20262177 0.20257014 0.20215529 0.20176178
 0.2008912  0.19961193 0.19938268 0.2004587  0.201039   0.20048948
 0.20014116 0.20057552 0.2011542  0.20134674 0.20152943 0.201443
 0.20101148 0.20084102 0.2014281  0.20183595 0.202464   0.2031052
 0.2034797  0.20293538 0.20227085 0.20232318 0.20188594 0.20071214
 0.20010698 0.200517   0.20062695 0.20027405 0.20022567 0.2003281
 0.19974548 0.19930798 0.20022988 0.20103619 0.20050836 0.19984497
 0.20036808 0.20101176 0.20066407 0.20013852 0.20004807 0.19978563
 0.19925751 0.19902408 0.19839998 0.19678423 0.19540454 0.19531743
 0.19572645 0.19573365 0.19569996 0.19563551 0.19467773 0.19334123
 0.19283153 0.19275802 0.1923857  0.19200449 0.19273269 0.19341892
 0.19296482 0.19216366 0.19282733 0.19419448 0.19448039 0.1933543
 0.19223344 0.19239822 0.19340563 0.19441462 0.19432926 0.19328451
 0.19254811 0.19217548 0.19196399 0.19163665 0.19104803 0.18991116
 0.18927522 0.18983112 0.19102138 0.19103333 0.19006497 0.18967544
 0.19033845 0.19104661 0.19108683 0.19057089 0.18959104 0.18861434
 0.18854234 0.18871588 0.18798847 0.18665025 0.18572934 0.1852234
 0.18497746 0.18513475 0.1854297  0.18521746 0.18413892 0.18286237
 0.18189575 0.18119125 0.18099089 0.1813151  0.18186243 0.18239513
 0.18299472 0.18319198 0.1829746  0.18318768 0.18403956 0.18425195
 0.18355682 0.18322599 0.18343471 0.18324527 0.18258078 0.18246804
 0.18289495 0.18248975 0.18125711 0.18018316 0.17905591 0.17833117
 0.17911226 0.18082237 0.18135977 0.18075012 0.1811222  0.18207455
 0.18136689 0.17991872 0.18000166 0.18077429 0.18038853 0.18005782
 0.18110448 0.18174788 0.18012178 0.1785976  0.17887026 0.17924435
 0.17794032 0.17605993 0.17552379 0.1758399  0.17578207 0.17500246
 0.1741265  0.17391437 0.17451671 0.17457567 0.17394947 0.17420568
 0.17540449 0.17557423 0.17467485 0.17512567 0.17620917 0.17517397
 0.17264286 0.17165041 0.17123891 0.16932318 0.16769293 0.16756558
 0.16739166 0.16561094 0.16479172 0.16551596 0.1656783  0.16488594
 0.16479145 0.16485302 0.16410005 0.16347453 0.16431019 0.16525142
 0.16535091 0.16598783 0.16727702 0.16758832 0.16722299 0.16679364
 0.16510822 0.16364537 0.16425562 0.16549213 0.16328283 0.16060823
 0.16216573 0.1642473  0.16238508 0.15963382 0.15945788 0.1592469
 0.1585789  0.15953885 0.16072512 0.15915559 0.15895547 0.161512
 0.16096523 0.15946835 0.16237779 0.1587921  0.15351544 0.17616613]
