Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32177152.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.238263368606567
Epoch: 1, Steps: 56 | Train Loss: 0.9131721 Vali Loss: 1.8568995 Test Loss: 0.7365046
Validation loss decreased (inf --> 1.856899).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.23928689956665
Epoch: 2, Steps: 56 | Train Loss: 0.7329053 Vali Loss: 1.6890469 Test Loss: 0.6246353
Validation loss decreased (1.856899 --> 1.689047).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.875744342803955
Epoch: 3, Steps: 56 | Train Loss: 0.6772893 Vali Loss: 1.6230271 Test Loss: 0.5742725
Validation loss decreased (1.689047 --> 1.623027).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.50629186630249
Epoch: 4, Steps: 56 | Train Loss: 0.6481621 Vali Loss: 1.5799032 Test Loss: 0.5406951
Validation loss decreased (1.623027 --> 1.579903).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.581053018569946
Epoch: 5, Steps: 56 | Train Loss: 0.6280072 Vali Loss: 1.5440468 Test Loss: 0.5155368
Validation loss decreased (1.579903 --> 1.544047).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.00170612335205
Epoch: 6, Steps: 56 | Train Loss: 0.6135324 Vali Loss: 1.5199640 Test Loss: 0.4965321
Validation loss decreased (1.544047 --> 1.519964).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.882339715957642
Epoch: 7, Steps: 56 | Train Loss: 0.6025513 Vali Loss: 1.5019636 Test Loss: 0.4819816
Validation loss decreased (1.519964 --> 1.501964).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.20184874534607
Epoch: 8, Steps: 56 | Train Loss: 0.5942984 Vali Loss: 1.4781860 Test Loss: 0.4707182
Validation loss decreased (1.501964 --> 1.478186).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.223577737808228
Epoch: 9, Steps: 56 | Train Loss: 0.5874686 Vali Loss: 1.4711113 Test Loss: 0.4621887
Validation loss decreased (1.478186 --> 1.471111).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 12.226528406143188
Epoch: 10, Steps: 56 | Train Loss: 0.5825089 Vali Loss: 1.4591640 Test Loss: 0.4555941
Validation loss decreased (1.471111 --> 1.459164).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.498119831085205
Epoch: 11, Steps: 56 | Train Loss: 0.5779159 Vali Loss: 1.4626698 Test Loss: 0.4504441
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 12.959612131118774
Epoch: 12, Steps: 56 | Train Loss: 0.5747124 Vali Loss: 1.4534192 Test Loss: 0.4466127
Validation loss decreased (1.459164 --> 1.453419).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 13.471227884292603
Epoch: 13, Steps: 56 | Train Loss: 0.5717748 Vali Loss: 1.4502180 Test Loss: 0.4434662
Validation loss decreased (1.453419 --> 1.450218).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.775743961334229
Epoch: 14, Steps: 56 | Train Loss: 0.5696133 Vali Loss: 1.4485945 Test Loss: 0.4412154
Validation loss decreased (1.450218 --> 1.448594).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 12.031817197799683
Epoch: 15, Steps: 56 | Train Loss: 0.5676208 Vali Loss: 1.4417540 Test Loss: 0.4394591
Validation loss decreased (1.448594 --> 1.441754).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.63187575340271
Epoch: 16, Steps: 56 | Train Loss: 0.5663551 Vali Loss: 1.4433885 Test Loss: 0.4380037
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 12.279733180999756
Epoch: 17, Steps: 56 | Train Loss: 0.5651533 Vali Loss: 1.4341360 Test Loss: 0.4370582
Validation loss decreased (1.441754 --> 1.434136).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 12.082728385925293
Epoch: 18, Steps: 56 | Train Loss: 0.5640685 Vali Loss: 1.4385172 Test Loss: 0.4362388
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.873364925384521
Epoch: 19, Steps: 56 | Train Loss: 0.5630585 Vali Loss: 1.4400851 Test Loss: 0.4357056
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.262376546859741
Epoch: 20, Steps: 56 | Train Loss: 0.5622879 Vali Loss: 1.4368570 Test Loss: 0.4352001
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43610963225364685, mae:0.46175312995910645, rse:0.6321931481361389, corr:[0.21070862 0.23231111 0.23332065 0.22727239 0.22757077 0.2307072
 0.2326561  0.23240663 0.23123273 0.23062791 0.23031513 0.2298589
 0.2291806  0.22867131 0.22851655 0.22805943 0.22703247 0.22580022
 0.22521673 0.22564419 0.2263627  0.22639726 0.2257245  0.22507888
 0.22542718 0.22654122 0.22747593 0.22771087 0.22744158 0.22728406
 0.22756457 0.22791107 0.2279674  0.22755091 0.22696422 0.22667296
 0.22672956 0.22666118 0.22636837 0.22604673 0.22605512 0.22630218
 0.2265389  0.22676359 0.22696103 0.22731702 0.22782639 0.22818692
 0.22815005 0.22760946 0.2268558  0.22627634 0.22583733 0.22522403
 0.22441046 0.22346607 0.22310776 0.22302549 0.22302072 0.22287357
 0.22253627 0.22207993 0.22169802 0.22154476 0.22158937 0.22177526
 0.22199446 0.22219107 0.22234294 0.22223175 0.22178508 0.2214334
 0.22117738 0.22106272 0.22099844 0.22074945 0.22025909 0.21973957
 0.2195165  0.21960211 0.21969981 0.21952046 0.21905099 0.21851541
 0.21804027 0.21759719 0.21711755 0.216733   0.2164208  0.2162187
 0.21614009 0.21639362 0.2167607  0.2171965  0.21761875 0.2183562
 0.21938157 0.22031848 0.22096431 0.22139123 0.22165497 0.2218144
 0.22185531 0.22177045 0.2216428  0.22135022 0.22094713 0.22054607
 0.22020082 0.22004455 0.21992435 0.21981734 0.21974005 0.21962704
 0.21961711 0.21975453 0.22001709 0.2202286  0.22029978 0.22031648
 0.2201888  0.21984048 0.21939915 0.21911037 0.21880876 0.21850848
 0.21822543 0.21803379 0.21786943 0.21753886 0.21709089 0.21669981
 0.21655712 0.21651813 0.21644017 0.21624011 0.21590792 0.21563482
 0.215561   0.21575357 0.21593703 0.21578929 0.21525969 0.21488135
 0.21475823 0.21473873 0.2146457  0.214385   0.214082   0.21385024
 0.21375492 0.21380366 0.21382688 0.21360305 0.21325892 0.21305113
 0.21289195 0.21271695 0.21255071 0.21256284 0.21262456 0.21277414
 0.21295467 0.21316898 0.21332508 0.21345282 0.21328057 0.21324341
 0.21342772 0.21384901 0.21429229 0.21473353 0.21492252 0.21484648
 0.21464573 0.21454777 0.21448162 0.21435091 0.2140823  0.21374972
 0.21348006 0.21339041 0.2135119  0.21371536 0.21391872 0.21406189
 0.21419922 0.21443298 0.2146861  0.21459879 0.21425624 0.2138984
 0.2136679  0.21345152 0.2131136  0.21265285 0.21220739 0.21191177
 0.2117943  0.21185952 0.21194172 0.2118595  0.21180129 0.21188402
 0.21210125 0.21236062 0.2125546  0.21265088 0.21269342 0.21265681
 0.21238817 0.21188089 0.21132809 0.2108855  0.2106692  0.21077953
 0.2109831  0.21105488 0.21101277 0.21105686 0.21118487 0.21120366
 0.21105938 0.2108584  0.21063082 0.21039507 0.21003672 0.20969851
 0.20946825 0.20942007 0.20946176 0.20937617 0.2089423  0.2083574
 0.20798005 0.20805039 0.20839384 0.20863202 0.20864715 0.20861891
 0.20882328 0.2092131  0.20963486 0.20983084 0.20967235 0.20938158
 0.20931427 0.20935896 0.20913509 0.20848247 0.20773666 0.20734392
 0.20743948 0.20766176 0.20780118 0.20778193 0.20785668 0.20813592
 0.20844387 0.2084453  0.20832811 0.20820217 0.20824936 0.20843866
 0.2085718  0.20847194 0.20818026 0.20796697 0.20789106 0.20791717
 0.2079246  0.20782974 0.20764032 0.2075678  0.20761712 0.20766778
 0.20759748 0.20761715 0.2075718  0.20751521 0.20726812 0.20693898
 0.20657833 0.20660064 0.2069177  0.20719653 0.20723829 0.20730743
 0.2075953  0.20815319 0.20883778 0.20935169 0.20951122 0.20941427
 0.20929697 0.20935237 0.20933774 0.2091508  0.20893544 0.20897716
 0.20926186 0.209512   0.20952961 0.20941097 0.20929563 0.20939884
 0.20976335 0.20987394 0.20965284 0.20941682 0.20947008 0.21000148
 0.21065609 0.21083765 0.21046202 0.21004306 0.20988916 0.20987085
 0.20960264 0.20904256 0.2084176  0.20803493 0.20800649 0.20807412
 0.20800076 0.20782827 0.20777403 0.20805033 0.20826268 0.2082025
 0.2079591  0.20784374 0.20814428 0.20862705 0.2087558  0.20853628
 0.2082551  0.20816071 0.20829153 0.20827758 0.20773101 0.20679657
 0.2060387  0.20578535 0.20581105 0.2057823  0.20558336 0.20539871
 0.20544298 0.20573623 0.20591992 0.20582488 0.20558363 0.20571057
 0.20611583 0.20642732 0.20639175 0.20613098 0.2057693  0.20574312
 0.20601125 0.20608722 0.20577957 0.20532471 0.20496567 0.2046516
 0.20420592 0.20360662 0.20300429 0.20265508 0.20259537 0.20258181
 0.20216158 0.20142202 0.200964   0.20108938 0.20155747 0.20174341
 0.20131499 0.20049657 0.20011064 0.2006654  0.20170079 0.20282835
 0.2036392  0.2041832  0.2047182  0.20518963 0.20523244 0.204442
 0.20324636 0.20228145 0.20180002 0.2015161  0.20100209 0.20037304
 0.19999225 0.20005146 0.20042469 0.20062381 0.20053037 0.20047297
 0.20070465 0.20102413 0.20113908 0.20091254 0.20052776 0.20072839
 0.20168625 0.20280054 0.20336385 0.20321158 0.20279424 0.20247391
 0.20247237 0.20244968 0.20218036 0.20180005 0.20169698 0.2019487
 0.20207775 0.20177332 0.2010372  0.20060632 0.2007903  0.20129114
 0.20133485 0.2007677  0.19996926 0.200074   0.20094678 0.20190045
 0.20230253 0.20201272 0.201615   0.2018514  0.20241496 0.20251285
 0.20199387 0.2014703  0.20136674 0.20173873 0.20200247 0.20181958
 0.20127177 0.20088491 0.2009308  0.20093624 0.20049861 0.19977212
 0.19937755 0.19959821 0.20019428 0.20064117 0.20080622 0.20113435
 0.20189779 0.20282128 0.20325902 0.2027817  0.20173913 0.2007884
 0.20051564 0.20070781 0.20067129 0.20029703 0.19984126 0.19986378
 0.20030755 0.20075761 0.20087872 0.20084621 0.20096113 0.2013201
 0.2014603  0.20108676 0.20035951 0.1997871  0.1999531  0.20049813
 0.20081735 0.20065053 0.20025457 0.20020495 0.20050016 0.20057233
 0.200062   0.19937076 0.19879627 0.19867285 0.19876419 0.19869404
 0.19837637 0.19806589 0.19811209 0.19843969 0.19871362 0.19864236
 0.19836149 0.19812572 0.19809934 0.19827427 0.19864848 0.1993965
 0.20051081 0.20168474 0.20229402 0.20206605 0.20131917 0.20047349
 0.19995771 0.1997484  0.19925323 0.19845751 0.19806446 0.19855733
 0.1994214  0.19982223 0.19958277 0.19898187 0.199052   0.20001858
 0.20119661 0.20177838 0.20184752 0.20128168 0.2013276  0.20149322
 0.20182745 0.20162423 0.20092034 0.20057684 0.20065051 0.20099653
 0.20099302 0.20061824 0.19999285 0.1996718  0.19967614 0.19994327
 0.20016971 0.20029142 0.20065485 0.20104888 0.201284   0.20111685
 0.20084323 0.20085073 0.20108896 0.20122772 0.20097777 0.20050076
 0.20014755 0.20015778 0.20004037 0.19932516 0.19807066 0.19692805
 0.1964935  0.19671556 0.19664398 0.19594796 0.19499134 0.19452746
 0.19456375 0.19448242 0.19427064 0.19369884 0.1936965  0.19433877
 0.19523092 0.195269   0.19448093 0.19344422 0.19295068 0.19306977
 0.19328444 0.19311534 0.1923792  0.19207287 0.19246791 0.19287139
 0.19275238 0.19156843 0.19001512 0.1888536  0.18873382 0.18930165
 0.19004789 0.19042546 0.19076882 0.1910919  0.19114837 0.19074832
 0.1902066  0.19004567 0.1903349  0.19051729 0.18996221 0.18887617
 0.18817554 0.1883517  0.18886842 0.18867916 0.1872391  0.18510044
 0.18363452 0.18352686 0.18405434 0.18432496 0.18402049 0.18385923
 0.1842185  0.18464682 0.18453288 0.18376128 0.18319225 0.18360655
 0.1847868  0.18573686 0.18574755 0.18512858 0.18481964 0.1850592
 0.18517704 0.1844359  0.1829138  0.18185931 0.18188599 0.18239328
 0.18245144 0.1815506  0.1802179  0.17956002 0.17988269 0.18059637
 0.18078907 0.18018392 0.1798358  0.18028806 0.18096286 0.1809818
 0.1802157  0.17965692 0.18005064 0.18098654 0.1814002  0.18071781
 0.17941774 0.1786264  0.17858188 0.17884924 0.17820586 0.17666422
 0.17538808 0.17524183 0.17582913 0.17572498 0.17453358 0.17331454
 0.17304535 0.17350203 0.17392918 0.17370959 0.17350252 0.17406663
 0.17505199 0.17550035 0.17468227 0.17342131 0.17285787 0.17323612
 0.17304197 0.17124926 0.16836135 0.1662852  0.16618791 0.16679484
 0.16672444 0.16509444 0.16311565 0.16209921 0.16278318 0.16405775
 0.16406427 0.16273493 0.16198754 0.16251735 0.16342811 0.1633681
 0.16247445 0.16238607 0.16366014 0.16462724 0.16339207 0.16033684
 0.15804967 0.15885168 0.1611812  0.16211459 0.15952758 0.155453
 0.15390286 0.15578704 0.15836501 0.15801804 0.15536897 0.1532655
 0.15330559 0.1535898  0.15195683 0.14910986 0.14842096 0.15054245
 0.15190727 0.14698946 0.138919   0.13776442 0.14535135 0.13070081]
