Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.047793865203857
Epoch: 1, Steps: 56 | Train Loss: 0.8909898 Vali Loss: 1.8278899 Test Loss: 0.7069760
Validation loss decreased (inf --> 1.827890).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.24852180480957
Epoch: 2, Steps: 56 | Train Loss: 0.7193062 Vali Loss: 1.6869624 Test Loss: 0.6141303
Validation loss decreased (1.827890 --> 1.686962).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.972288131713867
Epoch: 3, Steps: 56 | Train Loss: 0.6714658 Vali Loss: 1.6258272 Test Loss: 0.5679553
Validation loss decreased (1.686962 --> 1.625827).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.517711162567139
Epoch: 4, Steps: 56 | Train Loss: 0.6436845 Vali Loss: 1.5820875 Test Loss: 0.5357041
Validation loss decreased (1.625827 --> 1.582088).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.010454177856445
Epoch: 5, Steps: 56 | Train Loss: 0.6246133 Vali Loss: 1.5477353 Test Loss: 0.5114982
Validation loss decreased (1.582088 --> 1.547735).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.508411884307861
Epoch: 6, Steps: 56 | Train Loss: 0.6103256 Vali Loss: 1.5263253 Test Loss: 0.4931144
Validation loss decreased (1.547735 --> 1.526325).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.626733779907227
Epoch: 7, Steps: 56 | Train Loss: 0.5995674 Vali Loss: 1.5069821 Test Loss: 0.4789073
Validation loss decreased (1.526325 --> 1.506982).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.602398157119751
Epoch: 8, Steps: 56 | Train Loss: 0.5914768 Vali Loss: 1.4946736 Test Loss: 0.4678512
Validation loss decreased (1.506982 --> 1.494674).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.301931619644165
Epoch: 9, Steps: 56 | Train Loss: 0.5846882 Vali Loss: 1.4783266 Test Loss: 0.4592518
Validation loss decreased (1.494674 --> 1.478327).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.547053337097168
Epoch: 10, Steps: 56 | Train Loss: 0.5796286 Vali Loss: 1.4719582 Test Loss: 0.4526028
Validation loss decreased (1.478327 --> 1.471958).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.76238465309143
Epoch: 11, Steps: 56 | Train Loss: 0.5753875 Vali Loss: 1.4622681 Test Loss: 0.4475619
Validation loss decreased (1.471958 --> 1.462268).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.43490219116211
Epoch: 12, Steps: 56 | Train Loss: 0.5719831 Vali Loss: 1.4574308 Test Loss: 0.4435622
Validation loss decreased (1.462268 --> 1.457431).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 7.167859792709351
Epoch: 13, Steps: 56 | Train Loss: 0.5691879 Vali Loss: 1.4532160 Test Loss: 0.4405725
Validation loss decreased (1.457431 --> 1.453216).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 7.6804046630859375
Epoch: 14, Steps: 56 | Train Loss: 0.5669144 Vali Loss: 1.4537477 Test Loss: 0.4382860
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 7.360137939453125
Epoch: 15, Steps: 56 | Train Loss: 0.5651621 Vali Loss: 1.4492157 Test Loss: 0.4364380
Validation loss decreased (1.453216 --> 1.449216).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 7.3601624965667725
Epoch: 16, Steps: 56 | Train Loss: 0.5634131 Vali Loss: 1.4457371 Test Loss: 0.4350529
Validation loss decreased (1.449216 --> 1.445737).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 7.345496416091919
Epoch: 17, Steps: 56 | Train Loss: 0.5623190 Vali Loss: 1.4357440 Test Loss: 0.4339714
Validation loss decreased (1.445737 --> 1.435744).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 7.833266019821167
Epoch: 18, Steps: 56 | Train Loss: 0.5611860 Vali Loss: 1.4350274 Test Loss: 0.4331225
Validation loss decreased (1.435744 --> 1.435027).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.225683689117432
Epoch: 19, Steps: 56 | Train Loss: 0.5600819 Vali Loss: 1.4375346 Test Loss: 0.4325237
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.674610376358032
Epoch: 20, Steps: 56 | Train Loss: 0.5593334 Vali Loss: 1.4352129 Test Loss: 0.4320751
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.03171968460083
Epoch: 21, Steps: 56 | Train Loss: 0.5583849 Vali Loss: 1.4383817 Test Loss: 0.4317590
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43216297030448914, mae:0.4588250517845154, rse:0.6293261051177979, corr:[0.21651281 0.23197421 0.2284006  0.2337941  0.23489888 0.23097263
 0.23097055 0.23358631 0.23394601 0.23271373 0.2320743  0.2322265
 0.23206286 0.2310979  0.22967799 0.22895466 0.22928192 0.22910616
 0.22798413 0.22752187 0.22838868 0.22858898 0.22800861 0.22795115
 0.22884806 0.22926524 0.22919515 0.22979036 0.23084839 0.23096827
 0.23003507 0.2293846  0.22957498 0.22970577 0.22919968 0.22857422
 0.22862117 0.2289262  0.22862002 0.22778356 0.2274046  0.22781828
 0.22815855 0.22784916 0.22778238 0.22875513 0.2297255  0.22962038
 0.22935255 0.2293485  0.22921339 0.2283646  0.22722714 0.22668117
 0.22654161 0.22581193 0.22509816 0.2246545  0.2246466  0.22461659
 0.22420947 0.22371368 0.22357774 0.2236769  0.22365713 0.2235934
 0.2237228  0.22395314 0.22394398 0.22362493 0.22336446 0.22349793
 0.22326943 0.22251765 0.22200638 0.22209041 0.2221562  0.22179914
 0.22143549 0.22143292 0.22145769 0.22096175 0.22026867 0.22000368
 0.2200701  0.21974142 0.21873678 0.2179695  0.21811746 0.2186264
 0.21855158 0.21819173 0.21817182 0.21863231 0.21881455 0.2193836
 0.22065629 0.22168748 0.22210492 0.22240941 0.22278316 0.22303584
 0.22297287 0.22268932 0.22253208 0.22250398 0.22234482 0.22191812
 0.22146794 0.2214728  0.22173078 0.22169606 0.22138888 0.22124837
 0.22154328 0.22178039 0.22165273 0.22137493 0.22132406 0.22145203
 0.22132984 0.22093041 0.22077672 0.2208864  0.22043201 0.21961178
 0.21923906 0.219337   0.21927159 0.21878646 0.21840245 0.21825576
 0.21812572 0.2177597  0.21746555 0.2174811  0.21748903 0.21731952
 0.21710794 0.21712127 0.21734974 0.2173641  0.21673797 0.21639636
 0.216597   0.21671234 0.21635164 0.21565908 0.21513021 0.21494749
 0.2148608  0.21476467 0.214863   0.21493714 0.21455623 0.21393521
 0.21362326 0.21379216 0.2138894  0.21378689 0.21377108 0.21405503
 0.21429048 0.21426456 0.2141916  0.21443143 0.21414706 0.21376249
 0.21373934 0.2143996  0.21528089 0.21579675 0.21553789 0.21511745
 0.215159   0.21536748 0.21505117 0.21450411 0.21438085 0.21453084
 0.21430227 0.21389256 0.21410277 0.21471475 0.2150397  0.21497104
 0.21508235 0.21549055 0.21582277 0.21573956 0.21555616 0.21546377
 0.21517986 0.21464586 0.21426696 0.21402161 0.21360493 0.21304525
 0.21268709 0.21265668 0.21276611 0.21265356 0.21243697 0.21246007
 0.21294867 0.21350323 0.21363857 0.21341875 0.21331196 0.21326168
 0.21303648 0.21273702 0.2126474  0.21255004 0.21204594 0.21162961
 0.21169889 0.21194941 0.21213403 0.21238017 0.2124363  0.21228807
 0.21232845 0.2123971  0.21188323 0.21109396 0.21089537 0.21117766
 0.21095888 0.21032627 0.21001986 0.21005505 0.20987669 0.20952699
 0.20947295 0.2096763  0.2096442  0.20924419 0.20888276 0.209032
 0.20959926 0.20993286 0.20998636 0.20996213 0.21004154 0.21013492
 0.2100374  0.20982751 0.20979309 0.20964666 0.208995   0.20829596
 0.2084175  0.20907526 0.20936126 0.2089835  0.20879576 0.2092154
 0.20969129 0.20962873 0.20939562 0.20931491 0.20923154 0.20909713
 0.20893976 0.20883721 0.2087857  0.20864786 0.20829126 0.2082259
 0.20852642 0.20843984 0.20787294 0.20761281 0.20774919 0.20769311
 0.20735087 0.2076091  0.20819367 0.2082586  0.20761827 0.207212
 0.20730682 0.2075799  0.20747921 0.20720574 0.20705393 0.20739812
 0.20778476 0.20791063 0.20822467 0.20896204 0.20963839 0.20955446
 0.20898859 0.2089573  0.20937063 0.20931445 0.20877916 0.20884918
 0.2095917  0.2099918  0.20971796 0.20955905 0.20993002 0.21038473
 0.21051463 0.21017557 0.20966096 0.20943944 0.2096977  0.21024735
 0.21046565 0.21033594 0.21061823 0.21105978 0.21062438 0.20974149
 0.2094042  0.20926084 0.20869511 0.20818922 0.20819286 0.2081478
 0.20772198 0.20750721 0.20771144 0.20781392 0.2076984  0.20794041
 0.2083224  0.20822984 0.20797853 0.20817779 0.20848434 0.20862317
 0.20855172 0.20815592 0.20765567 0.20744883 0.20749146 0.20731845
 0.20674832 0.20617755 0.20595782 0.2059852  0.20606889 0.2063096
 0.20650752 0.2063594  0.20600027 0.20612237 0.20662378 0.2069694
 0.20677832 0.20642696 0.20623702 0.20630908 0.20629503 0.20644699
 0.20673946 0.20680273 0.20663747 0.20616104 0.20537876 0.20488954
 0.2046864  0.20395142 0.20283619 0.20256083 0.20300613 0.20292576
 0.20204546 0.20145091 0.20147508 0.20135447 0.201048   0.201021
 0.20122732 0.20108904 0.20074521 0.20081942 0.20105335 0.20161976
 0.20249826 0.20326117 0.20354524 0.20367962 0.20377892 0.20325683
 0.20231253 0.20146976 0.20085745 0.20027994 0.20000759 0.20064718
 0.20144677 0.20140527 0.20073749 0.20028122 0.20040281 0.20065506
 0.20070116 0.20056619 0.20047122 0.20056868 0.20082502 0.20143737
 0.20229389 0.20300063 0.20347549 0.20355117 0.20335184 0.20309204
 0.2028242  0.20206828 0.20139588 0.20147763 0.2017313  0.20148817
 0.20116407 0.20137405 0.20143276 0.20102698 0.20070566 0.20091522
 0.20113587 0.20094873 0.20036657 0.20048068 0.20111501 0.20186108
 0.20231779 0.20217167 0.20184612 0.20194545 0.20200056 0.20162053
 0.2015478  0.2020088  0.2019642  0.20127557 0.20091587 0.20138434
 0.20151018 0.20075725 0.20016347 0.2001427  0.2001125  0.19979313
 0.19972716 0.19996399 0.20010751 0.20005597 0.20027845 0.20116168
 0.20212576 0.20258038 0.20258543 0.2022922  0.20199555 0.2017923
 0.20140608 0.20067698 0.20033845 0.20075166 0.2007863  0.20026356
 0.20003955 0.2004202  0.20059478 0.20041706 0.20064321 0.20142263
 0.20177211 0.20138879 0.20083247 0.20044784 0.20056507 0.20087938
 0.2009734  0.20059446 0.2002998  0.20064493 0.20098406 0.20079492
 0.2004696  0.200348   0.20000619 0.19969955 0.19964065 0.19952199
 0.19906707 0.1988535  0.1991905  0.19911025 0.19841139 0.19826813
 0.19891469 0.19911087 0.19848806 0.1981131  0.19847685 0.19923611
 0.19999905 0.20068035 0.20098056 0.20086026 0.20071651 0.20041235
 0.19969648 0.19906323 0.19927414 0.19980289 0.19949415 0.19874758
 0.19884044 0.19965594 0.2001417  0.19985385 0.19996755 0.20084125
 0.20173928 0.20183508 0.20145035 0.20088786 0.20148951 0.2020907
 0.20209561 0.20144635 0.20143048 0.2022421  0.20196275 0.20091577
 0.20080657 0.20160922 0.20151599 0.20094691 0.20109485 0.20168374
 0.20158792 0.20136262 0.20195043 0.20224163 0.20188482 0.20190288
 0.2025971  0.20275131 0.20207494 0.20159532 0.2013415  0.20063122
 0.19981176 0.19951086 0.19907668 0.1983112  0.19817682 0.19866207
 0.19855943 0.19778231 0.1971112  0.19652721 0.19553621 0.19505201
 0.19554465 0.19573566 0.19509768 0.19406004 0.19392163 0.19407071
 0.19421239 0.1941453  0.1940335  0.19380558 0.19382635 0.19381799
 0.19333018 0.19283278 0.19287954 0.19324237 0.19253692 0.19148742
 0.19192807 0.19225805 0.19143045 0.19086541 0.19178319 0.19218242
 0.19133444 0.1907814  0.19147848 0.19161746 0.19105701 0.19165652
 0.19341001 0.19418001 0.19342926 0.19254749 0.19188367 0.19082911
 0.18978472 0.18903324 0.18849246 0.18832602 0.18851337 0.18803461
 0.18670495 0.18570913 0.18572174 0.1861139  0.18592685 0.18552682
 0.18509851 0.1843075  0.18344344 0.1829008  0.18294139 0.18354991
 0.18446597 0.18476556 0.18446258 0.18480952 0.18597168 0.18615702
 0.1850065  0.18440929 0.18471561 0.18406667 0.1820146  0.18091993
 0.18170881 0.18208228 0.18137754 0.18111913 0.18119754 0.18114744
 0.18140753 0.18193735 0.18162152 0.18020411 0.1794794  0.18031065
 0.18131442 0.18163913 0.18146318 0.1807852  0.18005602 0.18026263
 0.18106748 0.18115337 0.18054569 0.18076886 0.1806434  0.17895494
 0.17734393 0.1773038  0.17744647 0.17651469 0.17606981 0.17678674
 0.1767466  0.175623   0.17537151 0.17559187 0.1749161  0.17446813
 0.17554654 0.17661409 0.17596582 0.17516561 0.17502703 0.17432158
 0.17241555 0.17104122 0.17039013 0.16954589 0.16906404 0.16842747
 0.16705033 0.16522802 0.16503751 0.16528898 0.16445474 0.16379237
 0.16429234 0.16435954 0.1636778  0.16333719 0.16370125 0.16408
 0.16487607 0.16594547 0.16504902 0.16251674 0.16213092 0.16310528
 0.16132437 0.15887992 0.15942255 0.16120708 0.15955368 0.15764521
 0.15932184 0.15913323 0.15534298 0.15403894 0.15626518 0.15567258
 0.15338208 0.15294068 0.15245403 0.15024376 0.15040258 0.1507103
 0.14661616 0.14669997 0.14960691 0.13559805 0.13096324 0.16217047]
