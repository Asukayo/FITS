Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  34420736.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6830205
	speed: 0.1499s/iter; left time: 824.3264s
Epoch: 1 cost time: 16.68839168548584
Epoch: 1, Steps: 112 | Train Loss: 0.8158586 Vali Loss: 1.7019790 Test Loss: 0.6203633
Validation loss decreased (inf --> 1.701979).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6379513
	speed: 0.3297s/iter; left time: 1776.9751s
Epoch: 2 cost time: 15.688158750534058
Epoch: 2, Steps: 112 | Train Loss: 0.6625643 Vali Loss: 1.5813901 Test Loss: 0.5335808
Validation loss decreased (1.701979 --> 1.581390).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5897224
	speed: 0.3197s/iter; left time: 1686.8705s
Epoch: 3 cost time: 16.18814492225647
Epoch: 3, Steps: 112 | Train Loss: 0.6178469 Vali Loss: 1.5187236 Test Loss: 0.4875099
Validation loss decreased (1.581390 --> 1.518724).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5308332
	speed: 0.2973s/iter; left time: 1535.6471s
Epoch: 4 cost time: 14.56880235671997
Epoch: 4, Steps: 112 | Train Loss: 0.5935980 Vali Loss: 1.4798807 Test Loss: 0.4613984
Validation loss decreased (1.518724 --> 1.479881).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6094618
	speed: 0.3136s/iter; left time: 1584.6509s
Epoch: 5 cost time: 15.283443689346313
Epoch: 5, Steps: 112 | Train Loss: 0.5796007 Vali Loss: 1.4625326 Test Loss: 0.4464021
Validation loss decreased (1.479881 --> 1.462533).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5942917
	speed: 0.3299s/iter; left time: 1630.2622s
Epoch: 6 cost time: 16.790509700775146
Epoch: 6, Steps: 112 | Train Loss: 0.5710305 Vali Loss: 1.4504446 Test Loss: 0.4385257
Validation loss decreased (1.462533 --> 1.450445).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5721900
	speed: 0.2637s/iter; left time: 1273.2339s
Epoch: 7 cost time: 7.7484047412872314
Epoch: 7, Steps: 112 | Train Loss: 0.5658003 Vali Loss: 1.4415290 Test Loss: 0.4342641
Validation loss decreased (1.450445 --> 1.441529).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5710343
	speed: 0.3178s/iter; left time: 1499.0127s
Epoch: 8 cost time: 15.979912042617798
Epoch: 8, Steps: 112 | Train Loss: 0.5625060 Vali Loss: 1.4388263 Test Loss: 0.4326189
Validation loss decreased (1.441529 --> 1.438826).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5652360
	speed: 0.3302s/iter; left time: 1520.7524s
Epoch: 9 cost time: 16.18553876876831
Epoch: 9, Steps: 112 | Train Loss: 0.5600060 Vali Loss: 1.4368639 Test Loss: 0.4312638
Validation loss decreased (1.438826 --> 1.436864).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5559145
	speed: 0.3118s/iter; left time: 1401.1362s
Epoch: 10 cost time: 15.220778226852417
Epoch: 10, Steps: 112 | Train Loss: 0.5582993 Vali Loss: 1.4322972 Test Loss: 0.4315657
Validation loss decreased (1.436864 --> 1.432297).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5391092
	speed: 0.3104s/iter; left time: 1359.9536s
Epoch: 11 cost time: 15.677961111068726
Epoch: 11, Steps: 112 | Train Loss: 0.5571070 Vali Loss: 1.4343166 Test Loss: 0.4314996
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5400148
	speed: 0.2927s/iter; left time: 1249.4796s
Epoch: 12 cost time: 12.708911895751953
Epoch: 12, Steps: 112 | Train Loss: 0.5558883 Vali Loss: 1.4367988 Test Loss: 0.4316606
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6100905
	speed: 0.2542s/iter; left time: 1056.5442s
Epoch: 13 cost time: 12.920416355133057
Epoch: 13, Steps: 112 | Train Loss: 0.5553671 Vali Loss: 1.4338936 Test Loss: 0.4320065
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4303549826145172, mae:0.45522555708885193, rse:0.6280083060264587, corr:[0.21800977 0.23241903 0.23008466 0.23465726 0.23563695 0.23182788
 0.23131639 0.23308401 0.23270845 0.23131107 0.23092496 0.23129348
 0.2311217  0.23029515 0.22950605 0.22944055 0.2297973  0.22932027
 0.22832946 0.22833465 0.22923268 0.22927028 0.22889452 0.22922362
 0.23011544 0.23052736 0.230613   0.23115738 0.2318164  0.23164865
 0.23093943 0.23064438 0.230852   0.23074849 0.22996898 0.22897643
 0.22871049 0.22901864 0.22900964 0.22843525 0.22822377 0.22882156
 0.22929071 0.22903977 0.22895706 0.22985081 0.23070814 0.23077096
 0.23088174 0.2309255  0.23029688 0.22893335 0.22780252 0.22755365
 0.22735606 0.22629057 0.22542942 0.22518972 0.2253902  0.2254288
 0.2249944  0.22446245 0.22430019 0.2243376  0.22431888 0.22432902
 0.2244925  0.22466558 0.22457348 0.22430927 0.2242883  0.22463174
 0.2244307  0.2236507  0.2231075  0.22311527 0.2230212  0.22241941
 0.22186619 0.22189163 0.22228506 0.22230728 0.22186778 0.22136872
 0.22105387 0.22075205 0.22017093 0.21963637 0.21950306 0.21963367
 0.21952283 0.21941507 0.21955286 0.2199747  0.22009127 0.2205426
 0.22164012 0.22263671 0.22319858 0.22365232 0.22400849 0.2240839
 0.22389802 0.22365315 0.223476   0.22324339 0.22294524 0.22262102
 0.22235751 0.22238602 0.22261022 0.22267182 0.22250946 0.22245161
 0.22281371 0.22313106 0.22299074 0.22267377 0.22260298 0.22264804
 0.22240292 0.22192597 0.2215443  0.22136799 0.22089274 0.22030117
 0.22012256 0.22017385 0.21994571 0.21953006 0.21943432 0.21942781
 0.21917175 0.21870287 0.21831311 0.21807957 0.21795675 0.2181228
 0.21846923 0.21867862 0.2186558  0.21836744 0.21776989 0.2176714
 0.2180714  0.21820524 0.21766645 0.21679828 0.21637447 0.2164808
 0.21645421 0.21615113 0.21592934 0.21581751 0.2155792  0.2152132
 0.2148657  0.21466196 0.21460412 0.21481372 0.2151366  0.21535486
 0.21544214 0.21545745 0.21539643 0.21546102 0.21524066 0.21521422
 0.21537046 0.21571136 0.2162652  0.21683635 0.2168449  0.21659097
 0.21668328 0.21682641 0.21626768 0.21559076 0.21564029 0.2162229
 0.21633527 0.21598059 0.21583758 0.21595581 0.2159593  0.21582468
 0.21587536 0.21616027 0.21652025 0.21678771 0.21698377 0.21707869
 0.21681905 0.21626501 0.21568754 0.21525997 0.21502748 0.21489656
 0.21477488 0.21482667 0.21510288 0.21508831 0.21459883 0.21414441
 0.21435466 0.21488833 0.21508157 0.21484601 0.21468766 0.21469015
 0.2146281  0.21433544 0.21392655 0.2134645  0.21304134 0.21303137
 0.21321264 0.21310002 0.212823   0.21274234 0.21270244 0.21268997
 0.21292996 0.21310626 0.21270145 0.212182   0.21212159 0.21227846
 0.21213214 0.21193151 0.21187207 0.21165277 0.21129598 0.21131301
 0.21161774 0.21166062 0.21127804 0.21095473 0.21103847 0.21123184
 0.21146694 0.2116423  0.21191372 0.21214455 0.21211386 0.21183163
 0.2114989  0.21124044 0.21111001 0.21084397 0.21042146 0.21001443
 0.20985667 0.20992656 0.21016447 0.21025664 0.21015857 0.21027417
 0.21081737 0.21113499 0.21093099 0.21060298 0.2106052  0.21080647
 0.21065988 0.21030435 0.21012452 0.20994735 0.20941462 0.20918289
 0.20948072 0.20933826 0.20858338 0.20854253 0.20940301 0.20974462
 0.20909037 0.20865543 0.20865735 0.20835768 0.20794003 0.2084303
 0.20910539 0.20903018 0.20827208 0.20813003 0.20870626 0.20933048
 0.20960492 0.20975357 0.21005231 0.21046978 0.2109952  0.21141034
 0.21151392 0.2114508  0.21128833 0.21105194 0.21088916 0.21103327
 0.21110646 0.2106775  0.21000782 0.20984724 0.21003416 0.21021093
 0.21068569 0.21136501 0.2116864  0.2115485  0.21141604 0.21180367
 0.21219562 0.21217543 0.21226618 0.21235302 0.21155745 0.21038623
 0.21005528 0.21023427 0.20981564 0.20904122 0.20901239 0.20960379
 0.20972425 0.20923753 0.20894562 0.20920762 0.20956898 0.20993158
 0.21032286 0.21033956 0.20969865 0.20898668 0.2089867  0.20969598
 0.21021198 0.21010537 0.20992523 0.20969644 0.20900705 0.20822167
 0.20809147 0.20819366 0.20753215 0.20662363 0.20673618 0.20749585
 0.2074353  0.20659944 0.2060725  0.2061877  0.2063609  0.2068923
 0.20763284 0.2078356  0.20745142 0.20731752 0.207162   0.20676184
 0.20651692 0.2069382  0.20759021 0.20754024 0.20686676 0.20654562
 0.2065159  0.20616211 0.20570327 0.2055359  0.2052406  0.20481603
 0.20467468 0.20434086 0.20318222 0.20213266 0.2024515  0.20309567
 0.20280704 0.20222841 0.20243838 0.20297237 0.20283365 0.2029361
 0.20364872 0.20413862 0.20440266 0.20517378 0.20558427 0.20462476
 0.20373623 0.20384689 0.20358753 0.20231874 0.20181528 0.20277232
 0.20285976 0.20160969 0.20119868 0.2022195  0.20281759 0.20252532
 0.20249505 0.20295866 0.20314111 0.20303231 0.20292257 0.20284837
 0.20306283 0.20401934 0.2051147  0.20499279 0.20405102 0.2037908
 0.20423977 0.20388187 0.20292687 0.20255294 0.20271745 0.20288502
 0.20304516 0.20284763 0.20155832 0.20037523 0.20078966 0.20191331
 0.20217794 0.20179033 0.20137164 0.20162705 0.20236664 0.20364273
 0.20453149 0.20385101 0.20274429 0.20293917 0.2033233  0.20257449
 0.20198871 0.20253342 0.20256191 0.20155737 0.20124176 0.20220682
 0.2025183  0.20191622 0.20218444 0.20312509 0.20316222 0.20252538
 0.2023936  0.20237555 0.20207337 0.20219183 0.20281948 0.20310436
 0.20300002 0.20351686 0.20431507 0.20395839 0.20288503 0.20247152
 0.20239177 0.20160888 0.20091903 0.2012359  0.2015046  0.2014565
 0.20183203 0.202173   0.20143136 0.2007863  0.20171635 0.2031099
 0.20320064 0.20266683 0.20250805 0.20185228 0.20092605 0.20094308
 0.20178401 0.20190702 0.20161977 0.20217112 0.20243604 0.20133655
 0.20049936 0.20126775 0.20156169 0.2003866  0.19972059 0.20082866
 0.20175089 0.20137346 0.20111044 0.20137285 0.2010507  0.20019968
 0.1997922  0.19967757 0.19933213 0.19931255 0.19997804 0.20098679
 0.20189184 0.20253679 0.20252503 0.20205942 0.20212491 0.20258914
 0.20239809 0.20143795 0.20071645 0.20027797 0.19951142 0.1993573
 0.20051953 0.20145254 0.20090915 0.19998513 0.20060945 0.20198734
 0.20254587 0.20223516 0.20213579 0.20211162 0.20297779 0.20377034
 0.20371069 0.20262875 0.20242892 0.20348386 0.20329022 0.20210867
 0.2024592  0.20410122 0.20376147 0.20179118 0.20120813 0.20220315
 0.20209503 0.20094165 0.20109546 0.2022035  0.20300344 0.20361646
 0.20426843 0.20396566 0.20279755 0.2024213  0.2027879  0.20226094
 0.20073125 0.19962566 0.19910522 0.19882892 0.19926074 0.20008418
 0.19991413 0.19878559 0.19788305 0.1971988  0.19606933 0.19551055
 0.19593976 0.19582415 0.19501108 0.19478372 0.19585806 0.19606347
 0.1952473  0.19516529 0.196225   0.1963789  0.1953427  0.19477174
 0.1952908  0.19585612 0.19564004 0.19525883 0.19460386 0.19393554
 0.19441196 0.19494523 0.19428574 0.1929607  0.19253433 0.1923805
 0.19180048 0.1913487  0.19209231 0.19286382 0.1927122  0.19234985
 0.1927106  0.1933811  0.19332767 0.1924089  0.19129044 0.19062757
 0.19022168 0.18892999 0.18732528 0.18731263 0.18845668 0.18832509
 0.18719448 0.1872208  0.18763225 0.18614376 0.183664   0.1834251
 0.1850496  0.18561955 0.18504664 0.18499914 0.18550992 0.18563564
 0.18597157 0.18691677 0.18756802 0.18748532 0.18746963 0.18749782
 0.18668042 0.18518764 0.18425444 0.18422185 0.18419828 0.18369949
 0.18296647 0.18200237 0.1812956  0.18118587 0.18094951 0.18085368
 0.18142529 0.18206544 0.18211606 0.18202789 0.18207109 0.18108225
 0.17892948 0.17814462 0.17954022 0.1809067  0.18104114 0.18142232
 0.1822181  0.18183619 0.18054086 0.18053587 0.18075247 0.17960493
 0.17864469 0.17923655 0.17941287 0.17775586 0.17663004 0.17730457
 0.17753424 0.17595835 0.17434835 0.1738413  0.17455973 0.17630407
 0.17753167 0.1763237  0.17395504 0.17350575 0.17395927 0.17296274
 0.1714656  0.17165431 0.17174633 0.17002183 0.16881433 0.16848183
 0.16745664 0.16561918 0.16529167 0.16501068 0.16349669 0.16300099
 0.16394955 0.16308382 0.16096221 0.16122633 0.16339567 0.16351385
 0.16229343 0.16283908 0.16353239 0.16208239 0.16188629 0.16402341
 0.16409633 0.16213205 0.16178867 0.16247058 0.1598635  0.15747993
 0.15952654 0.16009885 0.15604292 0.15344319 0.15460174 0.15466319
 0.15416478 0.15505098 0.15342864 0.1493273  0.15003967 0.15202615
 0.14658134 0.14483602 0.15028135 0.13830808 0.12914322 0.16867515]
