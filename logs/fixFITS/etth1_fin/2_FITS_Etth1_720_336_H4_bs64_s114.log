Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  23532544.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.126696825027466
Epoch: 1, Steps: 59 | Train Loss: 0.6917426 Vali Loss: 1.7620996 Test Loss: 0.8338407
Validation loss decreased (inf --> 1.762100).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.828426837921143
Epoch: 2, Steps: 59 | Train Loss: 0.5457130 Vali Loss: 1.5846359 Test Loss: 0.7407942
Validation loss decreased (1.762100 --> 1.584636).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.509338140487671
Epoch: 3, Steps: 59 | Train Loss: 0.4734915 Vali Loss: 1.5150059 Test Loss: 0.7053838
Validation loss decreased (1.584636 --> 1.515006).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.016988277435303
Epoch: 4, Steps: 59 | Train Loss: 0.4320471 Vali Loss: 1.4725050 Test Loss: 0.6867484
Validation loss decreased (1.515006 --> 1.472505).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.75233244895935
Epoch: 5, Steps: 59 | Train Loss: 0.4033887 Vali Loss: 1.4467709 Test Loss: 0.6737820
Validation loss decreased (1.472505 --> 1.446771).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.951035022735596
Epoch: 6, Steps: 59 | Train Loss: 0.3813961 Vali Loss: 1.4303712 Test Loss: 0.6652430
Validation loss decreased (1.446771 --> 1.430371).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.755749702453613
Epoch: 7, Steps: 59 | Train Loss: 0.3634531 Vali Loss: 1.4154959 Test Loss: 0.6555089
Validation loss decreased (1.430371 --> 1.415496).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.608689785003662
Epoch: 8, Steps: 59 | Train Loss: 0.3481071 Vali Loss: 1.3981612 Test Loss: 0.6468910
Validation loss decreased (1.415496 --> 1.398161).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.599384546279907
Epoch: 9, Steps: 59 | Train Loss: 0.3347482 Vali Loss: 1.3929893 Test Loss: 0.6396767
Validation loss decreased (1.398161 --> 1.392989).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.640303611755371
Epoch: 10, Steps: 59 | Train Loss: 0.3230584 Vali Loss: 1.3820724 Test Loss: 0.6319048
Validation loss decreased (1.392989 --> 1.382072).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.527307748794556
Epoch: 11, Steps: 59 | Train Loss: 0.3126132 Vali Loss: 1.3729306 Test Loss: 0.6252371
Validation loss decreased (1.382072 --> 1.372931).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.492992162704468
Epoch: 12, Steps: 59 | Train Loss: 0.3035130 Vali Loss: 1.3656249 Test Loss: 0.6175789
Validation loss decreased (1.372931 --> 1.365625).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.540634870529175
Epoch: 13, Steps: 59 | Train Loss: 0.2954306 Vali Loss: 1.3561885 Test Loss: 0.6123704
Validation loss decreased (1.365625 --> 1.356189).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.520447254180908
Epoch: 14, Steps: 59 | Train Loss: 0.2879845 Vali Loss: 1.3517305 Test Loss: 0.6061324
Validation loss decreased (1.356189 --> 1.351730).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.463426113128662
Epoch: 15, Steps: 59 | Train Loss: 0.2812100 Vali Loss: 1.3477813 Test Loss: 0.6000584
Validation loss decreased (1.351730 --> 1.347781).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.60031509399414
Epoch: 16, Steps: 59 | Train Loss: 0.2751465 Vali Loss: 1.3357648 Test Loss: 0.5950450
Validation loss decreased (1.347781 --> 1.335765).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.98514199256897
Epoch: 17, Steps: 59 | Train Loss: 0.2696601 Vali Loss: 1.3279638 Test Loss: 0.5894147
Validation loss decreased (1.335765 --> 1.327964).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.684393644332886
Epoch: 18, Steps: 59 | Train Loss: 0.2648525 Vali Loss: 1.3253800 Test Loss: 0.5852192
Validation loss decreased (1.327964 --> 1.325380).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.563283920288086
Epoch: 19, Steps: 59 | Train Loss: 0.2601907 Vali Loss: 1.3251683 Test Loss: 0.5807153
Validation loss decreased (1.325380 --> 1.325168).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.652153491973877
Epoch: 20, Steps: 59 | Train Loss: 0.2560582 Vali Loss: 1.3162279 Test Loss: 0.5766956
Validation loss decreased (1.325168 --> 1.316228).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.265282392501831
Epoch: 21, Steps: 59 | Train Loss: 0.2522052 Vali Loss: 1.3165411 Test Loss: 0.5728275
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.578794479370117
Epoch: 22, Steps: 59 | Train Loss: 0.2486764 Vali Loss: 1.3094498 Test Loss: 0.5692011
Validation loss decreased (1.316228 --> 1.309450).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.46216082572937
Epoch: 23, Steps: 59 | Train Loss: 0.2455804 Vali Loss: 1.3026577 Test Loss: 0.5660175
Validation loss decreased (1.309450 --> 1.302658).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.629302978515625
Epoch: 24, Steps: 59 | Train Loss: 0.2425549 Vali Loss: 1.2986350 Test Loss: 0.5627936
Validation loss decreased (1.302658 --> 1.298635).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 9.371338844299316
Epoch: 25, Steps: 59 | Train Loss: 0.2398401 Vali Loss: 1.3033901 Test Loss: 0.5594364
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 9.003707885742188
Epoch: 26, Steps: 59 | Train Loss: 0.2372877 Vali Loss: 1.2932382 Test Loss: 0.5570835
Validation loss decreased (1.298635 --> 1.293238).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.949823379516602
Epoch: 27, Steps: 59 | Train Loss: 0.2350797 Vali Loss: 1.2940769 Test Loss: 0.5544211
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.213745355606079
Epoch: 28, Steps: 59 | Train Loss: 0.2327320 Vali Loss: 1.2934421 Test Loss: 0.5516191
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.074476480484009
Epoch: 29, Steps: 59 | Train Loss: 0.2307063 Vali Loss: 1.2963177 Test Loss: 0.5495762
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  23532544.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.219675302505493
Epoch: 1, Steps: 59 | Train Loss: 0.4871473 Vali Loss: 1.2252789 Test Loss: 0.4880089
Validation loss decreased (inf --> 1.225279).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.980909585952759
Epoch: 2, Steps: 59 | Train Loss: 0.4587225 Vali Loss: 1.2024131 Test Loss: 0.4588940
Validation loss decreased (1.225279 --> 1.202413).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.502712726593018
Epoch: 3, Steps: 59 | Train Loss: 0.4468418 Vali Loss: 1.1929686 Test Loss: 0.4479334
Validation loss decreased (1.202413 --> 1.192969).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.876096963882446
Epoch: 4, Steps: 59 | Train Loss: 0.4416737 Vali Loss: 1.1899371 Test Loss: 0.4434912
Validation loss decreased (1.192969 --> 1.189937).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.57185173034668
Epoch: 5, Steps: 59 | Train Loss: 0.4392537 Vali Loss: 1.1956522 Test Loss: 0.4426617
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.822776794433594
Epoch: 6, Steps: 59 | Train Loss: 0.4379424 Vali Loss: 1.2020922 Test Loss: 0.4427080
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.79796838760376
Epoch: 7, Steps: 59 | Train Loss: 0.4373064 Vali Loss: 1.2026888 Test Loss: 0.4432177
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4423285126686096, mae:0.44308197498321533, rse:0.6331760883331299, corr:[0.25162587 0.26066402 0.26052213 0.2564638  0.25410533 0.25342226
 0.25315052 0.25275666 0.251942   0.25128588 0.25094667 0.25086898
 0.25101587 0.25112966 0.25112143 0.2508431  0.2502121  0.24952257
 0.24899267 0.24874695 0.24870014 0.24857992 0.24809742 0.24760513
 0.24752653 0.24788415 0.24820913 0.2480452  0.24738887 0.24659848
 0.24618037 0.24620669 0.24659474 0.24690609 0.24672803 0.24625751
 0.24577503 0.24546713 0.2454465  0.24566483 0.24616198 0.24658701
 0.24659425 0.2463189  0.24617529 0.24657144 0.24752386 0.24840556
 0.24839874 0.24762912 0.24627489 0.24496327 0.24401842 0.24325801
 0.24266295 0.24221429 0.24174349 0.2412953  0.24077852 0.24040844
 0.24011959 0.23998138 0.23997003 0.24019624 0.24049361 0.24070892
 0.2408136  0.24073017 0.2407857  0.24092457 0.24085405 0.24040647
 0.23978122 0.23919429 0.238757   0.23859619 0.23839283 0.23798802
 0.2376056  0.23751791 0.23761034 0.23757173 0.23712106 0.23640102
 0.23575602 0.23541287 0.23535573 0.23539858 0.23534417 0.23522726
 0.23506625 0.2349847  0.23492461 0.23494737 0.23517719 0.23580249
 0.2367395  0.23757474 0.23790477 0.23785824 0.23765607 0.23741853
 0.23727158 0.2372144  0.2371798  0.23711614 0.2369357  0.23673844
 0.23669906 0.23674056 0.23679046 0.2368925  0.2369823  0.23707764
 0.23721994 0.2373351  0.23738359 0.23734435 0.23720443 0.23697507
 0.2366182  0.23598313 0.23524673 0.23471673 0.23441012 0.234068
 0.23366529 0.23332448 0.23282735 0.2322994  0.23172246 0.23118225
 0.23081394 0.2307042  0.23083192 0.23100215 0.23108867 0.23110263
 0.23113266 0.23125212 0.23149472 0.23165943 0.23161508 0.23137063
 0.23093076 0.23027933 0.22949986 0.22869527 0.22820668 0.22796342
 0.22787021 0.227854   0.22776288 0.22762625 0.2275619  0.22771533
 0.22792095 0.22788766 0.22756347 0.22714753 0.22683053 0.22698095
 0.22730194 0.22749233 0.22735952 0.22692977 0.22633165 0.22582996
 0.22562389 0.22563368 0.22562058 0.22556838 0.22538598 0.22511417
 0.22487429 0.22489375 0.2250061  0.22514933 0.22502649 0.2246512
 0.2242235  0.22408769 0.22446643 0.22522308 0.22594418 0.22632535
 0.22631435 0.22621274 0.2264239  0.22689517 0.22727022 0.22726785
 0.22669762 0.22564043 0.22428037 0.22295748 0.2220523  0.22165895
 0.22178634 0.22239192 0.22301523 0.22315499 0.22265756 0.22206573
 0.22180034 0.2221454  0.22277874 0.223159   0.2231084  0.22273228
 0.22235838 0.22208159 0.22196926 0.22179304 0.22148393 0.22112566
 0.22078066 0.22053623 0.22022647 0.2198031  0.21947373 0.21912196
 0.21890262 0.21892148 0.2190569  0.21898326 0.21851993 0.21794891
 0.21746796 0.21711549 0.21706624 0.21719816 0.21722993 0.21725097
 0.21727674 0.21724899 0.21725233 0.21731561 0.21765059 0.21809404
 0.21857488 0.21873792 0.21851873 0.21805914 0.21759565 0.21733843
 0.21737562 0.21763408 0.21782824 0.21775357 0.21742848 0.217084
 0.21681732 0.2165038  0.21629235 0.21621141 0.21632661 0.21648172
 0.21657449 0.21639103 0.21624157 0.21616617 0.21604998 0.21562926
 0.21502095 0.2144055  0.21382043 0.21330993 0.21267839 0.21208984
 0.21189275 0.21204126 0.21223165 0.21242931 0.21233964 0.2120845
 0.21194667 0.21198572 0.21210912 0.21220122 0.2122586  0.21238428
 0.2124599  0.21226685 0.21210916 0.21205904 0.21230255 0.21268927
 0.21290836 0.21269788 0.21172027 0.21065366 0.20993425 0.21000461
 0.21054918 0.21106564 0.21121716 0.21118948 0.211117   0.21107757
 0.21102566 0.21080062 0.21080585 0.21089418 0.21105859 0.21110325
 0.2112705  0.21127124 0.21150798 0.21150714 0.21089159 0.20969146
 0.2086143  0.20817629 0.20793535 0.20722702 0.20584896 0.20385006
 0.2022951  0.2018638  0.20244907 0.20294356 0.20238586 0.20126958
 0.20070882 0.20076603 0.20104213 0.20162494 0.20177361 0.20158742
 0.200536   0.19822122 0.19635332 0.19745    0.20115583 0.19756548]
