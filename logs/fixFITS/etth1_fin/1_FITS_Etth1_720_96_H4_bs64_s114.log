Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=134, out_features=151, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  18129664.0
params:  20385.0
Trainable parameters:  20385
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.586931467056274
Epoch: 1, Steps: 61 | Train Loss: 0.5726724 Vali Loss: 0.9445329 Test Loss: 0.4960516
Validation loss decreased (inf --> 0.944533).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.192354202270508
Epoch: 2, Steps: 61 | Train Loss: 0.3969544 Vali Loss: 0.8112354 Test Loss: 0.4086438
Validation loss decreased (0.944533 --> 0.811235).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.611295461654663
Epoch: 3, Steps: 61 | Train Loss: 0.3623565 Vali Loss: 0.7657559 Test Loss: 0.3893509
Validation loss decreased (0.811235 --> 0.765756).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.059010982513428
Epoch: 4, Steps: 61 | Train Loss: 0.3521815 Vali Loss: 0.7534004 Test Loss: 0.3851471
Validation loss decreased (0.765756 --> 0.753400).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.24834156036377
Epoch: 5, Steps: 61 | Train Loss: 0.3477320 Vali Loss: 0.7383146 Test Loss: 0.3841058
Validation loss decreased (0.753400 --> 0.738315).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.439048528671265
Epoch: 6, Steps: 61 | Train Loss: 0.3452663 Vali Loss: 0.7324632 Test Loss: 0.3839906
Validation loss decreased (0.738315 --> 0.732463).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.444334268569946
Epoch: 7, Steps: 61 | Train Loss: 0.3432317 Vali Loss: 0.7292576 Test Loss: 0.3838882
Validation loss decreased (0.732463 --> 0.729258).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.422308444976807
Epoch: 8, Steps: 61 | Train Loss: 0.3421255 Vali Loss: 0.7237006 Test Loss: 0.3832594
Validation loss decreased (0.729258 --> 0.723701).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.83331561088562
Epoch: 9, Steps: 61 | Train Loss: 0.3410461 Vali Loss: 0.7194583 Test Loss: 0.3835515
Validation loss decreased (0.723701 --> 0.719458).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.5435950756073
Epoch: 10, Steps: 61 | Train Loss: 0.3403774 Vali Loss: 0.7234833 Test Loss: 0.3834646
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.43017292022705
Epoch: 11, Steps: 61 | Train Loss: 0.3394797 Vali Loss: 0.7150596 Test Loss: 0.3836168
Validation loss decreased (0.719458 --> 0.715060).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.445975065231323
Epoch: 12, Steps: 61 | Train Loss: 0.3386894 Vali Loss: 0.7158991 Test Loss: 0.3833253
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.511109590530396
Epoch: 13, Steps: 61 | Train Loss: 0.3386277 Vali Loss: 0.7142155 Test Loss: 0.3835396
Validation loss decreased (0.715060 --> 0.714216).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.468525648117065
Epoch: 14, Steps: 61 | Train Loss: 0.3381549 Vali Loss: 0.7119251 Test Loss: 0.3831373
Validation loss decreased (0.714216 --> 0.711925).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.506749629974365
Epoch: 15, Steps: 61 | Train Loss: 0.3376692 Vali Loss: 0.7108117 Test Loss: 0.3833817
Validation loss decreased (0.711925 --> 0.710812).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.514997720718384
Epoch: 16, Steps: 61 | Train Loss: 0.3374927 Vali Loss: 0.7151328 Test Loss: 0.3832248
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.672247886657715
Epoch: 17, Steps: 61 | Train Loss: 0.3371649 Vali Loss: 0.7069810 Test Loss: 0.3831382
Validation loss decreased (0.710812 --> 0.706981).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.720660209655762
Epoch: 18, Steps: 61 | Train Loss: 0.3370392 Vali Loss: 0.7123569 Test Loss: 0.3834525
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.48981523513794
Epoch: 19, Steps: 61 | Train Loss: 0.3368240 Vali Loss: 0.7101355 Test Loss: 0.3835109
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.57267451286316
Epoch: 20, Steps: 61 | Train Loss: 0.3366152 Vali Loss: 0.7090963 Test Loss: 0.3834195
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.3828255832195282, mae:0.40566861629486084, rse:0.5877035856246948, corr:[0.26820108 0.27824417 0.2793654  0.2761451  0.27376738 0.2724441
 0.27169272 0.27104443 0.2703003  0.26988274 0.26982665 0.26973525
 0.269435   0.26910505 0.2690294  0.26920834 0.26926795 0.26904193
 0.26854393 0.26799077 0.26771626 0.26787093 0.26791686 0.26783383
 0.26767755 0.2676112  0.26760685 0.26744753 0.26709148 0.26662806
 0.26623026 0.2659058  0.26576716 0.26568875 0.26549214 0.2652902
 0.26517287 0.26517177 0.26524985 0.26539993 0.26578793 0.2662324
 0.26646182 0.2664831  0.2664599  0.26644883 0.2665059  0.2664986
 0.26601344 0.2653367  0.26436707 0.26318452 0.2619996  0.26075435
 0.2596968  0.25902206 0.2586548  0.25859228 0.2584801  0.25836182
 0.25817665 0.2581205  0.25827712 0.2586146  0.25874403 0.25854906
 0.25834292 0.25819445 0.258324   0.25852966 0.25836453 0.25759193
 0.25659397 0.25567484 0.2547979  0.25400835 0.25315794 0.25236675
 0.2519264  0.25179586 0.25130126 0.25028288 0.24912938 0.24858093
 0.2490605  0.24946223 0.24875955 0.24725693 0.24628034 0.24659628
 0.24654895 0.24383923 0.23946969 0.23724131 0.24075809 0.24180666]
