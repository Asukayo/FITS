Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  61797120.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.297143459320068
Epoch: 1, Steps: 30 | Train Loss: 0.7439804 Vali Loss: 1.4063797 Test Loss: 0.7013643
Validation loss decreased (inf --> 1.406380).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.614358425140381
Epoch: 2, Steps: 30 | Train Loss: 0.5732784 Vali Loss: 1.2164707 Test Loss: 0.5878401
Validation loss decreased (1.406380 --> 1.216471).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.427241086959839
Epoch: 3, Steps: 30 | Train Loss: 0.5066845 Vali Loss: 1.1318847 Test Loss: 0.5323536
Validation loss decreased (1.216471 --> 1.131885).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.562440872192383
Epoch: 4, Steps: 30 | Train Loss: 0.4716590 Vali Loss: 1.0855081 Test Loss: 0.4976778
Validation loss decreased (1.131885 --> 1.085508).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.6596720218658447
Epoch: 5, Steps: 30 | Train Loss: 0.4491159 Vali Loss: 1.0458897 Test Loss: 0.4738567
Validation loss decreased (1.085508 --> 1.045890).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.16285252571106
Epoch: 6, Steps: 30 | Train Loss: 0.4342247 Vali Loss: 1.0194972 Test Loss: 0.4574004
Validation loss decreased (1.045890 --> 1.019497).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.562469244003296
Epoch: 7, Steps: 30 | Train Loss: 0.4230854 Vali Loss: 1.0043849 Test Loss: 0.4459941
Validation loss decreased (1.019497 --> 1.004385).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.703105449676514
Epoch: 8, Steps: 30 | Train Loss: 0.4158595 Vali Loss: 0.9940094 Test Loss: 0.4379970
Validation loss decreased (1.004385 --> 0.994009).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.978425979614258
Epoch: 9, Steps: 30 | Train Loss: 0.4101758 Vali Loss: 0.9914582 Test Loss: 0.4324413
Validation loss decreased (0.994009 --> 0.991458).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.839443206787109
Epoch: 10, Steps: 30 | Train Loss: 0.4060676 Vali Loss: 0.9801596 Test Loss: 0.4285919
Validation loss decreased (0.991458 --> 0.980160).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.165462255477905
Epoch: 11, Steps: 30 | Train Loss: 0.4026429 Vali Loss: 0.9767187 Test Loss: 0.4259134
Validation loss decreased (0.980160 --> 0.976719).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.046695232391357
Epoch: 12, Steps: 30 | Train Loss: 0.3999432 Vali Loss: 0.9782146 Test Loss: 0.4241492
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.014446020126343
Epoch: 13, Steps: 30 | Train Loss: 0.3982338 Vali Loss: 0.9701399 Test Loss: 0.4227482
Validation loss decreased (0.976719 --> 0.970140).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.108355760574341
Epoch: 14, Steps: 30 | Train Loss: 0.3969652 Vali Loss: 0.9713047 Test Loss: 0.4218774
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.6148624420166016
Epoch: 15, Steps: 30 | Train Loss: 0.3954442 Vali Loss: 0.9664172 Test Loss: 0.4211906
Validation loss decreased (0.970140 --> 0.966417).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.025719881057739
Epoch: 16, Steps: 30 | Train Loss: 0.3940427 Vali Loss: 0.9675911 Test Loss: 0.4207665
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.713268280029297
Epoch: 17, Steps: 30 | Train Loss: 0.3934687 Vali Loss: 0.9649992 Test Loss: 0.4204888
Validation loss decreased (0.966417 --> 0.964999).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.561467170715332
Epoch: 18, Steps: 30 | Train Loss: 0.3929598 Vali Loss: 0.9696161 Test Loss: 0.4201277
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.038711309432983
Epoch: 19, Steps: 30 | Train Loss: 0.3920790 Vali Loss: 0.9695088 Test Loss: 0.4199713
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.5777549743652344
Epoch: 20, Steps: 30 | Train Loss: 0.3912674 Vali Loss: 0.9693543 Test Loss: 0.4198343
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.41437700390815735, mae:0.4247879385948181, rse:0.6113011240959167, corr:[0.2588859  0.27353474 0.26982188 0.26744473 0.26833242 0.2673117
 0.26544407 0.26474473 0.26519945 0.2654457  0.26492593 0.26405218
 0.26336488 0.26274884 0.26242    0.26244196 0.26248956 0.2618633
 0.26094556 0.26050493 0.26074168 0.26066655 0.26009205 0.25952002
 0.25957716 0.259828   0.25986272 0.25983107 0.2599863  0.2601045
 0.25982836 0.25935718 0.25920084 0.2593279  0.25922257 0.258937
 0.2588778  0.25913125 0.25927597 0.2592803  0.25945017 0.25985894
 0.26018706 0.26019564 0.26009044 0.2602713  0.2607681  0.2609708
 0.2606108  0.26001796 0.25946844 0.25872105 0.25758055 0.256283
 0.25552037 0.25537682 0.25539514 0.2552269  0.25484398 0.25459483
 0.25457838 0.25460997 0.25445682 0.25431135 0.25435132 0.25469783
 0.25509575 0.25512707 0.25502923 0.2550058  0.25502115 0.2548471
 0.25446102 0.25383282 0.2532499  0.25307006 0.2530421  0.25287765
 0.25254962 0.25216222 0.25184664 0.25161603 0.2513644  0.25108698
 0.25083616 0.2506474  0.25053972 0.25052172 0.25045946 0.25038007
 0.25025046 0.25013626 0.24999613 0.24986751 0.24983647 0.2501083
 0.2506755  0.25119406 0.25149724 0.25167662 0.25174472 0.25146222
 0.25106567 0.25088745 0.25100443 0.25104472 0.25062826 0.25008637
 0.25002402 0.25033635 0.2505754  0.25069696 0.2507709  0.25099045
 0.25115228 0.25097352 0.25073126 0.25073534 0.25079143 0.25062862
 0.25020313 0.24953836 0.24876101 0.24795263 0.24718402 0.24653143
 0.24625547 0.24600005 0.24541926 0.24492142 0.24472885 0.2446329
 0.24431467 0.24405183 0.24418701 0.24450143 0.24455942 0.24435022
 0.24430408 0.24448399 0.2444332  0.24389206 0.24324284 0.24298055
 0.24291608 0.24224035 0.24091665 0.23971267 0.23915863 0.23851907
 0.23761971 0.23709668 0.23737982 0.2376979  0.23721561 0.23664655
 0.23675561 0.2369545  0.2363291  0.23544553 0.23541296 0.23629251
 0.23653345 0.23572421 0.23514721 0.2353972  0.23537202 0.2341981
 0.2329522  0.23294272 0.23314065 0.23217332 0.23044874 0.22965336
 0.22980638 0.22928677 0.22761229 0.22684953 0.2274965  0.22740659
 0.22573847 0.22473706 0.22548716 0.22523244 0.22265472 0.22114319
 0.22272012 0.22183856 0.21415012 0.20782709 0.21303079 0.20748425]
