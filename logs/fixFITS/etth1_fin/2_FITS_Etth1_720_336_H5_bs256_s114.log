Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  142517760.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.883497476577759
Epoch: 1, Steps: 14 | Train Loss: 0.7854628 Vali Loss: 2.0581136 Test Loss: 0.9636095
Validation loss decreased (inf --> 2.058114).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.8477485179901123
Epoch: 2, Steps: 14 | Train Loss: 0.7159172 Vali Loss: 1.9361248 Test Loss: 0.9045669
Validation loss decreased (2.058114 --> 1.936125).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.0953118801116943
Epoch: 3, Steps: 14 | Train Loss: 0.6615526 Vali Loss: 1.8412106 Test Loss: 0.8593575
Validation loss decreased (1.936125 --> 1.841211).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.1340746879577637
Epoch: 4, Steps: 14 | Train Loss: 0.6197429 Vali Loss: 1.7667198 Test Loss: 0.8234583
Validation loss decreased (1.841211 --> 1.766720).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2880139350891113
Epoch: 5, Steps: 14 | Train Loss: 0.5872520 Vali Loss: 1.7127416 Test Loss: 0.7960748
Validation loss decreased (1.766720 --> 1.712742).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.0803797245025635
Epoch: 6, Steps: 14 | Train Loss: 0.5614093 Vali Loss: 1.6828879 Test Loss: 0.7747792
Validation loss decreased (1.712742 --> 1.682888).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.0801355838775635
Epoch: 7, Steps: 14 | Train Loss: 0.5401418 Vali Loss: 1.6376140 Test Loss: 0.7579969
Validation loss decreased (1.682888 --> 1.637614).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.9347240924835205
Epoch: 8, Steps: 14 | Train Loss: 0.5233300 Vali Loss: 1.6201890 Test Loss: 0.7447860
Validation loss decreased (1.637614 --> 1.620189).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.0402445793151855
Epoch: 9, Steps: 14 | Train Loss: 0.5085596 Vali Loss: 1.5901674 Test Loss: 0.7353794
Validation loss decreased (1.620189 --> 1.590167).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.9050381183624268
Epoch: 10, Steps: 14 | Train Loss: 0.4963897 Vali Loss: 1.5828668 Test Loss: 0.7268697
Validation loss decreased (1.590167 --> 1.582867).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.7200515270233154
Epoch: 11, Steps: 14 | Train Loss: 0.4867047 Vali Loss: 1.5707756 Test Loss: 0.7199729
Validation loss decreased (1.582867 --> 1.570776).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.291245698928833
Epoch: 12, Steps: 14 | Train Loss: 0.4768825 Vali Loss: 1.5386724 Test Loss: 0.7139710
Validation loss decreased (1.570776 --> 1.538672).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.251530408859253
Epoch: 13, Steps: 14 | Train Loss: 0.4692944 Vali Loss: 1.5405095 Test Loss: 0.7091994
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.5652170181274414
Epoch: 14, Steps: 14 | Train Loss: 0.4622531 Vali Loss: 1.5268756 Test Loss: 0.7057637
Validation loss decreased (1.538672 --> 1.526876).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.304966926574707
Epoch: 15, Steps: 14 | Train Loss: 0.4564310 Vali Loss: 1.5148218 Test Loss: 0.7024738
Validation loss decreased (1.526876 --> 1.514822).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.7256405353546143
Epoch: 16, Steps: 14 | Train Loss: 0.4511147 Vali Loss: 1.5070312 Test Loss: 0.6994348
Validation loss decreased (1.514822 --> 1.507031).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.6233434677124023
Epoch: 17, Steps: 14 | Train Loss: 0.4456998 Vali Loss: 1.5085477 Test Loss: 0.6969293
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.9935197830200195
Epoch: 18, Steps: 14 | Train Loss: 0.4417047 Vali Loss: 1.5039437 Test Loss: 0.6941580
Validation loss decreased (1.507031 --> 1.503944).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.2302119731903076
Epoch: 19, Steps: 14 | Train Loss: 0.4373536 Vali Loss: 1.4995372 Test Loss: 0.6922539
Validation loss decreased (1.503944 --> 1.499537).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.0094499588012695
Epoch: 20, Steps: 14 | Train Loss: 0.4334486 Vali Loss: 1.4891702 Test Loss: 0.6904415
Validation loss decreased (1.499537 --> 1.489170).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.512725591659546
Epoch: 21, Steps: 14 | Train Loss: 0.4308999 Vali Loss: 1.5032368 Test Loss: 0.6889702
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.512946367263794
Epoch: 22, Steps: 14 | Train Loss: 0.4267039 Vali Loss: 1.4809606 Test Loss: 0.6872725
Validation loss decreased (1.489170 --> 1.480961).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.475141763687134
Epoch: 23, Steps: 14 | Train Loss: 0.4238877 Vali Loss: 1.4758954 Test Loss: 0.6860045
Validation loss decreased (1.480961 --> 1.475895).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.8794403076171875
Epoch: 24, Steps: 14 | Train Loss: 0.4221151 Vali Loss: 1.4694815 Test Loss: 0.6848673
Validation loss decreased (1.475895 --> 1.469481).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.7817132472991943
Epoch: 25, Steps: 14 | Train Loss: 0.4185593 Vali Loss: 1.4720426 Test Loss: 0.6837422
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.633882522583008
Epoch: 26, Steps: 14 | Train Loss: 0.4169277 Vali Loss: 1.4744077 Test Loss: 0.6825408
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.503901481628418
Epoch: 27, Steps: 14 | Train Loss: 0.4143477 Vali Loss: 1.4699308 Test Loss: 0.6818070
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  142517760.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.925520658493042
Epoch: 1, Steps: 14 | Train Loss: 0.6157321 Vali Loss: 1.4101934 Test Loss: 0.6456423
Validation loss decreased (inf --> 1.410193).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.164395332336426
Epoch: 2, Steps: 14 | Train Loss: 0.5891615 Vali Loss: 1.3776242 Test Loss: 0.6160166
Validation loss decreased (1.410193 --> 1.377624).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.2871499061584473
Epoch: 3, Steps: 14 | Train Loss: 0.5702898 Vali Loss: 1.3380586 Test Loss: 0.5929304
Validation loss decreased (1.377624 --> 1.338059).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.2836053371429443
Epoch: 4, Steps: 14 | Train Loss: 0.5535704 Vali Loss: 1.3162079 Test Loss: 0.5742449
Validation loss decreased (1.338059 --> 1.316208).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2176051139831543
Epoch: 5, Steps: 14 | Train Loss: 0.5413931 Vali Loss: 1.3031194 Test Loss: 0.5589172
Validation loss decreased (1.316208 --> 1.303119).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.1484487056732178
Epoch: 6, Steps: 14 | Train Loss: 0.5303979 Vali Loss: 1.2736686 Test Loss: 0.5459266
Validation loss decreased (1.303119 --> 1.273669).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.9590563774108887
Epoch: 7, Steps: 14 | Train Loss: 0.5223618 Vali Loss: 1.2716031 Test Loss: 0.5347704
Validation loss decreased (1.273669 --> 1.271603).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.0375149250030518
Epoch: 8, Steps: 14 | Train Loss: 0.5126169 Vali Loss: 1.2554021 Test Loss: 0.5250198
Validation loss decreased (1.271603 --> 1.255402).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.899980068206787
Epoch: 9, Steps: 14 | Train Loss: 0.5079987 Vali Loss: 1.2572125 Test Loss: 0.5167028
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.191417694091797
Epoch: 10, Steps: 14 | Train Loss: 0.5010474 Vali Loss: 1.2448568 Test Loss: 0.5095118
Validation loss decreased (1.255402 --> 1.244857).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.1018214225769043
Epoch: 11, Steps: 14 | Train Loss: 0.4966590 Vali Loss: 1.2309504 Test Loss: 0.5030606
Validation loss decreased (1.244857 --> 1.230950).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.9084312915802
Epoch: 12, Steps: 14 | Train Loss: 0.4926519 Vali Loss: 1.2264926 Test Loss: 0.4973385
Validation loss decreased (1.230950 --> 1.226493).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.1784000396728516
Epoch: 13, Steps: 14 | Train Loss: 0.4880203 Vali Loss: 1.2212434 Test Loss: 0.4923878
Validation loss decreased (1.226493 --> 1.221243).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.336543083190918
Epoch: 14, Steps: 14 | Train Loss: 0.4852610 Vali Loss: 1.2204449 Test Loss: 0.4878402
Validation loss decreased (1.221243 --> 1.220445).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.2967092990875244
Epoch: 15, Steps: 14 | Train Loss: 0.4824591 Vali Loss: 1.2138281 Test Loss: 0.4839603
Validation loss decreased (1.220445 --> 1.213828).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.3201587200164795
Epoch: 16, Steps: 14 | Train Loss: 0.4780012 Vali Loss: 1.2152100 Test Loss: 0.4804465
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.0677239894866943
Epoch: 17, Steps: 14 | Train Loss: 0.4752629 Vali Loss: 1.2049999 Test Loss: 0.4772708
Validation loss decreased (1.213828 --> 1.205000).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.3374521732330322
Epoch: 18, Steps: 14 | Train Loss: 0.4740015 Vali Loss: 1.1980143 Test Loss: 0.4745083
Validation loss decreased (1.205000 --> 1.198014).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.0311381816864014
Epoch: 19, Steps: 14 | Train Loss: 0.4714582 Vali Loss: 1.2070436 Test Loss: 0.4720226
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.106459617614746
Epoch: 20, Steps: 14 | Train Loss: 0.4709306 Vali Loss: 1.2033696 Test Loss: 0.4697801
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.1123173236846924
Epoch: 21, Steps: 14 | Train Loss: 0.4687050 Vali Loss: 1.1952306 Test Loss: 0.4676594
Validation loss decreased (1.198014 --> 1.195231).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.1064412593841553
Epoch: 22, Steps: 14 | Train Loss: 0.4671474 Vali Loss: 1.1925232 Test Loss: 0.4657935
Validation loss decreased (1.195231 --> 1.192523).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.311476469039917
Epoch: 23, Steps: 14 | Train Loss: 0.4661769 Vali Loss: 1.1926284 Test Loss: 0.4640830
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.1754722595214844
Epoch: 24, Steps: 14 | Train Loss: 0.4652084 Vali Loss: 1.1908979 Test Loss: 0.4625527
Validation loss decreased (1.192523 --> 1.190898).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.1236162185668945
Epoch: 25, Steps: 14 | Train Loss: 0.4635320 Vali Loss: 1.2009381 Test Loss: 0.4611352
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.2655820846557617
Epoch: 26, Steps: 14 | Train Loss: 0.4619439 Vali Loss: 1.1846914 Test Loss: 0.4598140
Validation loss decreased (1.190898 --> 1.184691).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.332949638366699
Epoch: 27, Steps: 14 | Train Loss: 0.4615865 Vali Loss: 1.1878889 Test Loss: 0.4586820
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.322056531906128
Epoch: 28, Steps: 14 | Train Loss: 0.4604500 Vali Loss: 1.1880107 Test Loss: 0.4576151
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.7325189113616943
Epoch: 29, Steps: 14 | Train Loss: 0.4605266 Vali Loss: 1.1942480 Test Loss: 0.4565962
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4588756561279297, mae:0.4568803310394287, rse:0.6449106335639954, corr:[0.24788497 0.25978085 0.2545288  0.25355977 0.25562677 0.25447184
 0.25189623 0.2511662  0.2519342  0.2526073  0.25209072 0.250938
 0.2500036  0.24935286 0.24905452 0.24891979 0.24849729 0.24729495
 0.24596974 0.24547012 0.24576849 0.24559969 0.24464099 0.2438536
 0.24421154 0.24482442 0.24473797 0.24423692 0.2442138  0.24468412
 0.24489811 0.2443953  0.24379179 0.24359189 0.24354386 0.24344921
 0.2433     0.2431394  0.24295194 0.2427966  0.24290195 0.24325961
 0.2433891  0.24315551 0.24286442 0.2431035  0.24391894 0.24455081
 0.24464092 0.24454188 0.24425556 0.24351954 0.24217446 0.24047785
 0.23940095 0.23915786 0.23919672 0.23889436 0.23809594 0.23749143
 0.23722515 0.23711452 0.23684788 0.23675519 0.236915   0.23734744
 0.2377857  0.23784526 0.23789702 0.23797391 0.23790945 0.2376634
 0.23736542 0.23683248 0.23613006 0.23575711 0.23557615 0.23531093
 0.23484738 0.23430465 0.23381479 0.23349446 0.2331459  0.23276646
 0.23221256 0.23164193 0.23117474 0.23092632 0.23083028 0.23077103
 0.23042949 0.2298224  0.22917877 0.22893411 0.22910684 0.22966625
 0.23049958 0.23143725 0.23222621 0.23269992 0.23274541 0.23235486
 0.23202637 0.23204526 0.23225673 0.23220935 0.23171191 0.2312495
 0.23117505 0.23134397 0.23147306 0.23168069 0.23192823 0.2321773
 0.23222569 0.23205702 0.23190795 0.23181091 0.23161061 0.23150066
 0.23167393 0.23155639 0.23075482 0.22961004 0.22868513 0.22816074
 0.2280752  0.22798711 0.22746226 0.22680156 0.22625141 0.22589993
 0.22568941 0.22561465 0.22569385 0.22585006 0.22592176 0.22596464
 0.22586763 0.22563772 0.22528417 0.22485262 0.22442251 0.22412825
 0.2239478  0.22362138 0.22302078 0.2222743  0.22160342 0.22084093
 0.22030142 0.22034754 0.22085477 0.22100492 0.22039445 0.21983966
 0.21975367 0.21982679 0.21951106 0.21922626 0.21931799 0.2196858
 0.21965578 0.21922149 0.21901144 0.21914795 0.21878368 0.21784128
 0.21724376 0.21767762 0.21848077 0.21886678 0.21857661 0.21813753
 0.21804416 0.21818158 0.2179558  0.21768872 0.21759255 0.21759145
 0.21732028 0.21703976 0.21717927 0.21779591 0.21838197 0.218647
 0.21874218 0.21904196 0.21949199 0.21975589 0.21976636 0.21972354
 0.21961746 0.21912459 0.21812958 0.21695364 0.21607488 0.21525632
 0.21447714 0.21421884 0.21453251 0.21483034 0.2144991  0.2142632
 0.21461925 0.21554048 0.21608861 0.21619919 0.21651658 0.21702902
 0.21722272 0.21688287 0.21651915 0.21640956 0.21613878 0.21558607
 0.2151659  0.21527375 0.21544015 0.21523042 0.21479382 0.21424225
 0.2140382  0.2139693  0.21366464 0.2132756  0.21308003 0.21328129
 0.21316546 0.21281171 0.21299985 0.21373819 0.21406755 0.21399704
 0.21396299 0.21413778 0.21411164 0.21368513 0.2134432  0.21356924
 0.21393566 0.21401376 0.21402006 0.21419634 0.21438125 0.21416517
 0.21377946 0.21378915 0.2142242  0.21433823 0.21379672 0.21344818
 0.21371944 0.2139819  0.21385126 0.21362415 0.21419924 0.21522278
 0.21566589 0.21514119 0.21486041 0.21524335 0.21562885 0.21512231
 0.21445188 0.214646   0.21521236 0.21509883 0.21397974 0.21291551
 0.21287338 0.21296456 0.21254022 0.21237718 0.2126416  0.21293837
 0.21279253 0.21265227 0.2129548  0.21340391 0.21324319 0.21287715
 0.21301508 0.21347181 0.21350767 0.21256539 0.2118108  0.21206988
 0.21287243 0.2132884  0.21284668 0.21262585 0.21272305 0.21258004
 0.21196674 0.2116144  0.21201427 0.21262461 0.2125534  0.21245725
 0.21309662 0.2135467  0.21323466 0.21269703 0.21330468 0.21434239
 0.21443097 0.21313238 0.21276973 0.21353343 0.21363641 0.21207383
 0.21057433 0.21063463 0.21104842 0.20996687 0.20794557 0.20670721
 0.20696028 0.20653652 0.20513272 0.2045189  0.20488961 0.2043424
 0.20288469 0.20249864 0.2035403  0.20322801 0.19960661 0.19744661
 0.19921121 0.19804288 0.1883069  0.18053716 0.18823028 0.18271409]
