Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  34420736.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6072593
	speed: 0.1530s/iter; left time: 841.4719s
Epoch: 1 cost time: 17.146509647369385
Epoch: 1, Steps: 112 | Train Loss: 0.7360747 Vali Loss: 1.8886365 Test Loss: 0.7584297
Validation loss decreased (inf --> 1.888636).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5129977
	speed: 0.3041s/iter; left time: 1638.5616s
Epoch: 2 cost time: 15.144855976104736
Epoch: 2, Steps: 112 | Train Loss: 0.5494112 Vali Loss: 1.7737036 Test Loss: 0.6883128
Validation loss decreased (1.888636 --> 1.773704).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4627125
	speed: 0.3014s/iter; left time: 1590.4379s
Epoch: 3 cost time: 15.51017427444458
Epoch: 3, Steps: 112 | Train Loss: 0.4905504 Vali Loss: 1.7252827 Test Loss: 0.6561554
Validation loss decreased (1.773704 --> 1.725283).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4252459
	speed: 0.2868s/iter; left time: 1481.2561s
Epoch: 4 cost time: 13.992335319519043
Epoch: 4, Steps: 112 | Train Loss: 0.4539264 Vali Loss: 1.6850451 Test Loss: 0.6297421
Validation loss decreased (1.725283 --> 1.685045).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4428660
	speed: 0.3018s/iter; left time: 1524.8976s
Epoch: 5 cost time: 14.888629913330078
Epoch: 5, Steps: 112 | Train Loss: 0.4267583 Vali Loss: 1.6600933 Test Loss: 0.6077780
Validation loss decreased (1.685045 --> 1.660093).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4145712
	speed: 0.2931s/iter; left time: 1448.0421s
Epoch: 6 cost time: 14.850035667419434
Epoch: 6, Steps: 112 | Train Loss: 0.4054195 Vali Loss: 1.6375037 Test Loss: 0.5902373
Validation loss decreased (1.660093 --> 1.637504).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3814580
	speed: 0.3083s/iter; left time: 1488.8625s
Epoch: 7 cost time: 15.991239786148071
Epoch: 7, Steps: 112 | Train Loss: 0.3884412 Vali Loss: 1.6107574 Test Loss: 0.5712962
Validation loss decreased (1.637504 --> 1.610757).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3620434
	speed: 0.2992s/iter; left time: 1411.3750s
Epoch: 8 cost time: 14.871667623519897
Epoch: 8, Steps: 112 | Train Loss: 0.3746677 Vali Loss: 1.5929017 Test Loss: 0.5566720
Validation loss decreased (1.610757 --> 1.592902).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3624366
	speed: 0.3036s/iter; left time: 1398.0197s
Epoch: 9 cost time: 15.300415277481079
Epoch: 9, Steps: 112 | Train Loss: 0.3631690 Vali Loss: 1.5764118 Test Loss: 0.5433249
Validation loss decreased (1.592902 --> 1.576412).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3539274
	speed: 0.3011s/iter; left time: 1352.7758s
Epoch: 10 cost time: 14.736997842788696
Epoch: 10, Steps: 112 | Train Loss: 0.3537459 Vali Loss: 1.5575466 Test Loss: 0.5316010
Validation loss decreased (1.576412 --> 1.557547).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3331451
	speed: 0.2917s/iter; left time: 1277.8103s
Epoch: 11 cost time: 14.655394077301025
Epoch: 11, Steps: 112 | Train Loss: 0.3457164 Vali Loss: 1.5474235 Test Loss: 0.5218612
Validation loss decreased (1.557547 --> 1.547423).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3180008
	speed: 0.2924s/iter; left time: 1248.2303s
Epoch: 12 cost time: 15.077290773391724
Epoch: 12, Steps: 112 | Train Loss: 0.3388994 Vali Loss: 1.5376732 Test Loss: 0.5127417
Validation loss decreased (1.547423 --> 1.537673).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3454938
	speed: 0.2903s/iter; left time: 1206.7498s
Epoch: 13 cost time: 14.364604473114014
Epoch: 13, Steps: 112 | Train Loss: 0.3332663 Vali Loss: 1.5236988 Test Loss: 0.5045834
Validation loss decreased (1.537673 --> 1.523699).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3082772
	speed: 0.2952s/iter; left time: 1194.2103s
Epoch: 14 cost time: 14.859227895736694
Epoch: 14, Steps: 112 | Train Loss: 0.3280711 Vali Loss: 1.5173954 Test Loss: 0.4977103
Validation loss decreased (1.523699 --> 1.517395).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3056605
	speed: 0.2935s/iter; left time: 1154.1683s
Epoch: 15 cost time: 14.845291376113892
Epoch: 15, Steps: 112 | Train Loss: 0.3238402 Vali Loss: 1.5082701 Test Loss: 0.4914013
Validation loss decreased (1.517395 --> 1.508270).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2834532
	speed: 0.2986s/iter; left time: 1140.8414s
Epoch: 16 cost time: 15.025206327438354
Epoch: 16, Steps: 112 | Train Loss: 0.3201361 Vali Loss: 1.5034488 Test Loss: 0.4857004
Validation loss decreased (1.508270 --> 1.503449).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3094837
	speed: 0.3023s/iter; left time: 1121.4156s
Epoch: 17 cost time: 15.248130798339844
Epoch: 17, Steps: 112 | Train Loss: 0.3168776 Vali Loss: 1.4973149 Test Loss: 0.4810979
Validation loss decreased (1.503449 --> 1.497315).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3043183
	speed: 0.2957s/iter; left time: 1063.5926s
Epoch: 18 cost time: 14.568896055221558
Epoch: 18, Steps: 112 | Train Loss: 0.3142311 Vali Loss: 1.4914947 Test Loss: 0.4762372
Validation loss decreased (1.497315 --> 1.491495).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3005981
	speed: 0.2896s/iter; left time: 1009.4283s
Epoch: 19 cost time: 14.601380109786987
Epoch: 19, Steps: 112 | Train Loss: 0.3113728 Vali Loss: 1.4844787 Test Loss: 0.4726335
Validation loss decreased (1.491495 --> 1.484479).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2991123
	speed: 0.2986s/iter; left time: 1007.1183s
Epoch: 20 cost time: 15.541374444961548
Epoch: 20, Steps: 112 | Train Loss: 0.3092828 Vali Loss: 1.4806489 Test Loss: 0.4690864
Validation loss decreased (1.484479 --> 1.480649).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3164839
	speed: 0.3000s/iter; left time: 978.1487s
Epoch: 21 cost time: 15.432065725326538
Epoch: 21, Steps: 112 | Train Loss: 0.3073912 Vali Loss: 1.4795218 Test Loss: 0.4658266
Validation loss decreased (1.480649 --> 1.479522).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2975472
	speed: 0.2953s/iter; left time: 930.0440s
Epoch: 22 cost time: 14.61864948272705
Epoch: 22, Steps: 112 | Train Loss: 0.3054909 Vali Loss: 1.4747319 Test Loss: 0.4630266
Validation loss decreased (1.479522 --> 1.474732).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3120500
	speed: 0.2879s/iter; left time: 874.2647s
Epoch: 23 cost time: 14.378846645355225
Epoch: 23, Steps: 112 | Train Loss: 0.3039940 Vali Loss: 1.4746445 Test Loss: 0.4603752
Validation loss decreased (1.474732 --> 1.474645).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3047283
	speed: 0.2985s/iter; left time: 873.0044s
Epoch: 24 cost time: 15.27544903755188
Epoch: 24, Steps: 112 | Train Loss: 0.3023779 Vali Loss: 1.4680740 Test Loss: 0.4582593
Validation loss decreased (1.474645 --> 1.468074).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3282635
	speed: 0.3201s/iter; left time: 900.4147s
Epoch: 25 cost time: 16.499955654144287
Epoch: 25, Steps: 112 | Train Loss: 0.3012755 Vali Loss: 1.4694955 Test Loss: 0.4561362
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2676548
	speed: 0.3029s/iter; left time: 818.1752s
Epoch: 26 cost time: 15.185641050338745
Epoch: 26, Steps: 112 | Train Loss: 0.3000850 Vali Loss: 1.4671613 Test Loss: 0.4545079
Validation loss decreased (1.468074 --> 1.467161).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2924376
	speed: 0.2900s/iter; left time: 750.7631s
Epoch: 27 cost time: 14.287614107131958
Epoch: 27, Steps: 112 | Train Loss: 0.2989325 Vali Loss: 1.4635116 Test Loss: 0.4528833
Validation loss decreased (1.467161 --> 1.463512).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2835012
	speed: 0.2826s/iter; left time: 700.0989s
Epoch: 28 cost time: 14.173336744308472
Epoch: 28, Steps: 112 | Train Loss: 0.2981293 Vali Loss: 1.4583440 Test Loss: 0.4513129
Validation loss decreased (1.463512 --> 1.458344).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2860588
	speed: 0.2527s/iter; left time: 597.6901s
Epoch: 29 cost time: 12.26162052154541
Epoch: 29, Steps: 112 | Train Loss: 0.2972367 Vali Loss: 1.4638693 Test Loss: 0.4499564
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3184350
	speed: 0.2405s/iter; left time: 541.8205s
Epoch: 30 cost time: 12.766504526138306
Epoch: 30, Steps: 112 | Train Loss: 0.2964024 Vali Loss: 1.4568321 Test Loss: 0.4485792
Validation loss decreased (1.458344 --> 1.456832).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2650687
	speed: 0.2864s/iter; left time: 613.2471s
Epoch: 31 cost time: 14.918953657150269
Epoch: 31, Steps: 112 | Train Loss: 0.2956297 Vali Loss: 1.4568601 Test Loss: 0.4476636
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3183844
	speed: 0.2898s/iter; left time: 588.0818s
Epoch: 32 cost time: 14.965200662612915
Epoch: 32, Steps: 112 | Train Loss: 0.2950627 Vali Loss: 1.4545578 Test Loss: 0.4466580
Validation loss decreased (1.456832 --> 1.454558).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2893608
	speed: 0.2942s/iter; left time: 563.9801s
Epoch: 33 cost time: 14.663039445877075
Epoch: 33, Steps: 112 | Train Loss: 0.2944644 Vali Loss: 1.4541667 Test Loss: 0.4456856
Validation loss decreased (1.454558 --> 1.454167).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2982543
	speed: 0.3028s/iter; left time: 546.5783s
Epoch: 34 cost time: 15.403591871261597
Epoch: 34, Steps: 112 | Train Loss: 0.2940134 Vali Loss: 1.4563457 Test Loss: 0.4448791
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2748137
	speed: 0.3110s/iter; left time: 526.5214s
Epoch: 35 cost time: 15.791826248168945
Epoch: 35, Steps: 112 | Train Loss: 0.2934006 Vali Loss: 1.4509256 Test Loss: 0.4441669
Validation loss decreased (1.454167 --> 1.450926).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2992865
	speed: 0.2888s/iter; left time: 456.5392s
Epoch: 36 cost time: 14.426481008529663
Epoch: 36, Steps: 112 | Train Loss: 0.2929125 Vali Loss: 1.4547672 Test Loss: 0.4433872
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3011385
	speed: 0.2881s/iter; left time: 423.1484s
Epoch: 37 cost time: 14.269356966018677
Epoch: 37, Steps: 112 | Train Loss: 0.2926415 Vali Loss: 1.4567965 Test Loss: 0.4427871
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2915432
	speed: 0.3025s/iter; left time: 410.4893s
Epoch: 38 cost time: 15.487676858901978
Epoch: 38, Steps: 112 | Train Loss: 0.2921199 Vali Loss: 1.4571894 Test Loss: 0.4422239
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  34420736.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5119268
	speed: 0.1321s/iter; left time: 726.9533s
Epoch: 1 cost time: 15.013617277145386
Epoch: 1, Steps: 112 | Train Loss: 0.5551424 Vali Loss: 1.4383463 Test Loss: 0.4342119
Validation loss decreased (inf --> 1.438346).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5335692
	speed: 0.3172s/iter; left time: 1709.6159s
Epoch: 2 cost time: 16.281567573547363
Epoch: 2, Steps: 112 | Train Loss: 0.5511449 Vali Loss: 1.4395877 Test Loss: 0.4345767
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5711666
	speed: 0.3033s/iter; left time: 1600.6387s
Epoch: 3 cost time: 14.703916788101196
Epoch: 3, Steps: 112 | Train Loss: 0.5500620 Vali Loss: 1.4412551 Test Loss: 0.4354554
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5151684
	speed: 0.2949s/iter; left time: 1523.2847s
Epoch: 4 cost time: 15.004356384277344
Epoch: 4, Steps: 112 | Train Loss: 0.5496788 Vali Loss: 1.4444703 Test Loss: 0.4354534
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43301621079444885, mae:0.4574662744998932, rse:0.6299470663070679, corr:[0.23117395 0.23755066 0.23569551 0.23462698 0.23415649 0.23238368
 0.23132339 0.2323646  0.23311202 0.2324939  0.23138441 0.23147409
 0.23241787 0.23268782 0.23221476 0.23163714 0.23111942 0.23046583
 0.22972482 0.22914079 0.22886346 0.22886948 0.22901307 0.22889614
 0.22875515 0.22897135 0.22921202 0.22906221 0.22894758 0.22906154
 0.22909468 0.2286833  0.22833797 0.22853526 0.22901076 0.22916606
 0.2289584  0.2287166  0.2288754  0.22917785 0.22906725 0.22838777
 0.22792353 0.22791372 0.2280807  0.22834639 0.22900634 0.22980636
 0.23005544 0.22972232 0.22920322 0.22832897 0.22695154 0.22559474
 0.22498058 0.22444719 0.22383992 0.2233729  0.22342336 0.22400278
 0.22425611 0.22398894 0.22365762 0.22358978 0.22356021 0.2234868
 0.22340041 0.22322233 0.22325625 0.2235224  0.2234991  0.22318543
 0.22269304 0.22228879 0.22180861 0.22135545 0.22111425 0.22097611
 0.22038831 0.21926945 0.21845089 0.21863508 0.21886979 0.2184966
 0.21783353 0.2173801  0.21710189 0.21692419 0.2169552  0.21720478
 0.2173752  0.2174857  0.21739519 0.21712781 0.21688244 0.21765047
 0.21931192 0.2206987  0.22114807 0.2209831  0.2206955  0.2207976
 0.22114041 0.22121331 0.22089893 0.22056982 0.22031689 0.21998197
 0.21967803 0.21970779 0.2199342  0.21986505 0.21959198 0.21954046
 0.21973903 0.21986568 0.21999225 0.22013701 0.22016314 0.22009648
 0.2201727  0.22007135 0.21944681 0.21872708 0.2182922  0.21820347
 0.21802653 0.21776798 0.2174237  0.21727346 0.21703564 0.21678157
 0.21679126 0.21668512 0.21624804 0.2160262  0.21628422 0.21634187
 0.21568725 0.2147083  0.21451017 0.21493594 0.21492375 0.2145219
 0.2143712  0.21466058 0.21483095 0.2142204  0.21321371 0.21251878
 0.21248372 0.2129516  0.21360473 0.21394525 0.21361688 0.21298313
 0.21252568 0.21218567 0.21173818 0.21151185 0.21207412 0.21296085
 0.21273717 0.21157207 0.21119846 0.2123277  0.21304369 0.21277282
 0.21240251 0.21290167 0.21384853 0.21425588 0.21361078 0.21280317
 0.21270904 0.21301498 0.21308376 0.21308912 0.21308605 0.21282336
 0.21209127 0.21136716 0.21152309 0.21213989 0.21206167 0.21119332
 0.21062048 0.21113904 0.21198466 0.21257116 0.21311611 0.21324384
 0.21237244 0.21134596 0.2112599  0.21154407 0.2113036  0.21078216
 0.21059543 0.21051645 0.21016054 0.21032804 0.21114385 0.21125276
 0.210169   0.20947906 0.21021396 0.2111888  0.21140875 0.21112832
 0.21072313 0.21000597 0.209386   0.2092596  0.2091237  0.20800604
 0.20631576 0.20621881 0.20813112 0.20969129 0.2093835  0.20816953
 0.2078057  0.20824009 0.20802648 0.20710585 0.20662022 0.2068802
 0.207076   0.20694868 0.20685972 0.206504   0.20573309 0.20548068
 0.20627777 0.2070682  0.20692895 0.20655026 0.20685911 0.2072212
 0.20700578 0.20634127 0.20610683 0.20635156 0.20672075 0.20745653
 0.20832765 0.20842752 0.208056   0.2083829  0.20913707 0.20886913
 0.20756237 0.2061152  0.20544305 0.2054387  0.2057221  0.20584686
 0.20539811 0.20437527 0.20395084 0.20466629 0.20552084 0.20589878
 0.20556936 0.20495266 0.2048306  0.20544213 0.2058417  0.20539503
 0.20475602 0.20480207 0.20521647 0.20508637 0.20448913 0.20426507
 0.20452835 0.20486334 0.20510076 0.20513918 0.20448464 0.20365018
 0.20382555 0.20483509 0.205018   0.20448537 0.20452617 0.20527533
 0.20528278 0.20438455 0.20381942 0.20421235 0.20473632 0.20507126
 0.2054414  0.2057897  0.20607632 0.20679583 0.20760784 0.20752539
 0.20673175 0.20649287 0.2074248  0.20825006 0.20803584 0.20727369
 0.20699869 0.20748594 0.2081698  0.20819722 0.20761712 0.20750949
 0.20812494 0.20844422 0.20786929 0.20699905 0.20627834 0.20554182
 0.20480356 0.2043778  0.20439573 0.2045911  0.20469591 0.20458509
 0.20410779 0.20371976 0.20398054 0.20470977 0.20523408 0.2056689
 0.20571573 0.20463654 0.20309752 0.20299895 0.20430627 0.2046821
 0.20392564 0.20393148 0.20513225 0.20518014 0.20327824 0.20160013
 0.20124738 0.20065404 0.19949701 0.19968893 0.20151488 0.20290762
 0.20257358 0.20113876 0.19987613 0.19990619 0.20111953 0.20218448
 0.20177037 0.2007458  0.20047644 0.20043582 0.1993323  0.1985412
 0.1994279  0.20065549 0.20077379 0.20019574 0.19963884 0.19912583
 0.19882686 0.19893661 0.19911824 0.19933102 0.20004399 0.2009003
 0.20072488 0.19973266 0.19917803 0.19881147 0.19791773 0.19764398
 0.19923557 0.20062265 0.19961481 0.19786954 0.1977     0.1988742
 0.19950785 0.19983415 0.20078214 0.20125411 0.20059042 0.20024851
 0.20091578 0.20075925 0.19958077 0.19945951 0.20020731 0.19995746
 0.19872162 0.19841741 0.19918832 0.1991603  0.1983262  0.19844148
 0.19974335 0.20083818 0.20069154 0.19951673 0.19842117 0.19856827
 0.19945589 0.19952911 0.19888219 0.19883274 0.19948545 0.1997543
 0.19982107 0.20009431 0.20022188 0.1999211  0.19974276 0.19960435
 0.198788   0.19798183 0.19835798 0.19918248 0.19850774 0.19711903
 0.19753361 0.19938885 0.20007816 0.19975485 0.19957721 0.19973913
 0.1998238  0.19985992 0.19943686 0.19813085 0.19737352 0.19883248
 0.20097631 0.20125845 0.20022647 0.20035766 0.20102197 0.20044956
 0.1991373  0.19858456 0.19884226 0.1995795  0.20074956 0.20113483
 0.19994055 0.19882476 0.19948772 0.20012008 0.19876981 0.19759777
 0.19878818 0.20048448 0.2002969  0.19971071 0.20037001 0.20091714
 0.20039445 0.1999648  0.20040719 0.20093401 0.20109437 0.20100637
 0.19990341 0.19806364 0.19764097 0.19895071 0.19956072 0.19898817
 0.19899197 0.1996225  0.19900325 0.19752127 0.19768268 0.19897199
 0.19931613 0.19873397 0.19834915 0.19791538 0.19720776 0.19739261
 0.19830371 0.19765182 0.19544704 0.19492006 0.1965482  0.19756648
 0.19688177 0.19615066 0.19642404 0.19703014 0.19761355 0.19790591
 0.19726926 0.19608097 0.19567922 0.19631809 0.19734791 0.19914003
 0.20148517 0.20251615 0.20087446 0.19860056 0.19842151 0.19976993
 0.20044293 0.1996075  0.1983658  0.19796051 0.19865248 0.19954076
 0.19908014 0.19769779 0.19788662 0.1998008  0.20099136 0.20016068
 0.19920042 0.19976717 0.20106657 0.2013191  0.2013048  0.20125055
 0.20105022 0.20012826 0.19941148 0.20023447 0.20158571 0.20168488
 0.200291   0.19942428 0.20006809 0.20102108 0.20095617 0.20058781
 0.20066752 0.20056558 0.19989628 0.19945341 0.19982053 0.19984224
 0.1990491  0.19855312 0.19901247 0.19969018 0.19983093 0.19932996
 0.19790462 0.1958095  0.1942677  0.19434302 0.19568904 0.19726358
 0.19826183 0.19820268 0.19713394 0.19612977 0.19589254 0.19594373
 0.19537029 0.19423981 0.19330189 0.19217467 0.1909448  0.18969959
 0.18830015 0.1869687  0.18756574 0.19012819 0.19196981 0.1910478
 0.1892732  0.1891625  0.18970053 0.18996912 0.19060573 0.19162276
 0.1916757  0.19034696 0.18980102 0.19025742 0.19003919 0.18881565
 0.18832193 0.18806227 0.18683027 0.18522885 0.18528959 0.18691534
 0.18825467 0.18842012 0.18841016 0.18879543 0.1887346  0.18733498
 0.18556497 0.18465114 0.18476489 0.18515272 0.18535213 0.18531427
 0.18486908 0.18381289 0.1829802  0.183461   0.18389665 0.18279238
 0.18128847 0.18115892 0.1819911  0.18226786 0.18264395 0.18369304
 0.18400039 0.18280931 0.18199046 0.18201068 0.1811492  0.17955305
 0.17958577 0.1807572  0.18015207 0.17847416 0.1783064  0.17869398
 0.1775781  0.17653714 0.17779468 0.17996708 0.18006074 0.17941572
 0.18020017 0.18133256 0.18110302 0.17975809 0.17824174 0.17691551
 0.17631288 0.17674634 0.17731789 0.17768385 0.17868501 0.1796931
 0.17866626 0.17637846 0.17548183 0.17622064 0.17655756 0.17653322
 0.17706831 0.17709067 0.1758425  0.1748621  0.1751299  0.17516282
 0.17423241 0.17347138 0.17331457 0.1725075  0.17128736 0.17074966
 0.17089687 0.17114308 0.17124036 0.17114654 0.17047949 0.16976115
 0.16882373 0.16644987 0.16325445 0.16208607 0.16353151 0.16434146
 0.1639812  0.16429466 0.16573612 0.16596307 0.16507687 0.16496605
 0.16566218 0.1660751  0.1663337  0.16574432 0.16392717 0.16240187
 0.162409   0.16249874 0.16161299 0.16149448 0.16260979 0.1625581
 0.16089442 0.1606838  0.16081238 0.15964574 0.1583088  0.1586825
 0.15884988 0.15699995 0.15666386 0.1587787  0.1591567  0.15700293
 0.15649697 0.1563363  0.1540653  0.15296437 0.15525527 0.15529951
 0.15234213 0.15448257 0.1588739  0.15443139 0.15448038 0.16808641]
