Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  27646080.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.354861974716187
Epoch: 1, Steps: 61 | Train Loss: 0.5279685 Vali Loss: 0.8944467 Test Loss: 0.4474041
Validation loss decreased (inf --> 0.894447).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.172049760818481
Epoch: 2, Steps: 61 | Train Loss: 0.3825500 Vali Loss: 0.7981052 Test Loss: 0.3937744
Validation loss decreased (0.894447 --> 0.798105).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.474847316741943
Epoch: 3, Steps: 61 | Train Loss: 0.3580852 Vali Loss: 0.7669081 Test Loss: 0.3842171
Validation loss decreased (0.798105 --> 0.766908).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.078319787979126
Epoch: 4, Steps: 61 | Train Loss: 0.3503165 Vali Loss: 0.7516958 Test Loss: 0.3824815
Validation loss decreased (0.766908 --> 0.751696).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.9493727684021
Epoch: 5, Steps: 61 | Train Loss: 0.3459958 Vali Loss: 0.7395145 Test Loss: 0.3817376
Validation loss decreased (0.751696 --> 0.739514).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.875921726226807
Epoch: 6, Steps: 61 | Train Loss: 0.3432389 Vali Loss: 0.7333050 Test Loss: 0.3814349
Validation loss decreased (0.739514 --> 0.733305).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.961659908294678
Epoch: 7, Steps: 61 | Train Loss: 0.3414109 Vali Loss: 0.7237174 Test Loss: 0.3813922
Validation loss decreased (0.733305 --> 0.723717).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.82710886001587
Epoch: 8, Steps: 61 | Train Loss: 0.3399255 Vali Loss: 0.7212289 Test Loss: 0.3815543
Validation loss decreased (0.723717 --> 0.721229).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.914284467697144
Epoch: 9, Steps: 61 | Train Loss: 0.3388325 Vali Loss: 0.7203727 Test Loss: 0.3811154
Validation loss decreased (0.721229 --> 0.720373).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.642211675643921
Epoch: 10, Steps: 61 | Train Loss: 0.3378670 Vali Loss: 0.7135895 Test Loss: 0.3810294
Validation loss decreased (0.720373 --> 0.713590).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.891187191009521
Epoch: 11, Steps: 61 | Train Loss: 0.3371926 Vali Loss: 0.7160199 Test Loss: 0.3812160
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.795687913894653
Epoch: 12, Steps: 61 | Train Loss: 0.3364594 Vali Loss: 0.7121286 Test Loss: 0.3808325
Validation loss decreased (0.713590 --> 0.712129).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.78819727897644
Epoch: 13, Steps: 61 | Train Loss: 0.3360228 Vali Loss: 0.7120593 Test Loss: 0.3810662
Validation loss decreased (0.712129 --> 0.712059).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.462311267852783
Epoch: 14, Steps: 61 | Train Loss: 0.3352983 Vali Loss: 0.7062146 Test Loss: 0.3807381
Validation loss decreased (0.712059 --> 0.706215).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 7.061497688293457
Epoch: 15, Steps: 61 | Train Loss: 0.3352867 Vali Loss: 0.7081096 Test Loss: 0.3808317
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 6.256587266921997
Epoch: 16, Steps: 61 | Train Loss: 0.3347415 Vali Loss: 0.7068534 Test Loss: 0.3807073
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.001460075378418
Epoch: 17, Steps: 61 | Train Loss: 0.3344874 Vali Loss: 0.7087178 Test Loss: 0.3808749
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.38042911887168884, mae:0.4036933481693268, rse:0.5858611464500427, corr:[0.26655298 0.27911454 0.27933344 0.27780366 0.27669448 0.2747567
 0.2726822  0.27191174 0.27212784 0.2726237  0.2724391  0.27168608
 0.27118528 0.2712325  0.27153197 0.2717025  0.27143952 0.27105877
 0.27072886 0.27050364 0.2702497  0.27006224 0.26988783 0.26986364
 0.26980376 0.26961917 0.26926875 0.26886114 0.2686746  0.26864964
 0.26845014 0.26780817 0.2672574  0.26712182 0.2671688  0.26717162
 0.2670943  0.26708108 0.2673151  0.26768893 0.26803717 0.26804325
 0.26779753 0.26766804 0.26785004 0.2681476  0.26838085 0.26841938
 0.26781404 0.26675352 0.26562938 0.26480755 0.26397488 0.2628043
 0.26185536 0.2614166  0.26121303 0.2611003  0.2608761  0.2609688
 0.26110095 0.26090714 0.26035735 0.25999632 0.26009306 0.26044342
 0.260726   0.26049232 0.2600541  0.2599167  0.26008832 0.25990337
 0.25924975 0.25819382 0.25690916 0.2562862  0.25612757 0.2556057
 0.25444642 0.25331372 0.2528838  0.25290838 0.25237995 0.2512302
 0.25038397 0.25019532 0.25020537 0.24926288 0.24767654 0.24691518
 0.24717762 0.24611804 0.24189167 0.2376515  0.24027292 0.24366878]
