Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  94130176.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0819170475006104
Epoch: 1, Steps: 14 | Train Loss: 0.8855205 Vali Loss: 1.8684199 Test Loss: 0.8804539
Validation loss decreased (inf --> 1.868420).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.9810187816619873
Epoch: 2, Steps: 14 | Train Loss: 0.7704370 Vali Loss: 1.6740770 Test Loss: 0.7677569
Validation loss decreased (1.868420 --> 1.674077).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.180694580078125
Epoch: 3, Steps: 14 | Train Loss: 0.6944039 Vali Loss: 1.5388544 Test Loss: 0.6949533
Validation loss decreased (1.674077 --> 1.538854).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.155604124069214
Epoch: 4, Steps: 14 | Train Loss: 0.6463388 Vali Loss: 1.4845744 Test Loss: 0.6473142
Validation loss decreased (1.538854 --> 1.484574).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.837536573410034
Epoch: 5, Steps: 14 | Train Loss: 0.6152135 Vali Loss: 1.4229249 Test Loss: 0.6146578
Validation loss decreased (1.484574 --> 1.422925).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.980499505996704
Epoch: 6, Steps: 14 | Train Loss: 0.5920725 Vali Loss: 1.3946286 Test Loss: 0.5907343
Validation loss decreased (1.422925 --> 1.394629).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.2127511501312256
Epoch: 7, Steps: 14 | Train Loss: 0.5738671 Vali Loss: 1.3619707 Test Loss: 0.5724839
Validation loss decreased (1.394629 --> 1.361971).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.001289129257202
Epoch: 8, Steps: 14 | Train Loss: 0.5622452 Vali Loss: 1.3437972 Test Loss: 0.5577828
Validation loss decreased (1.361971 --> 1.343797).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.317110776901245
Epoch: 9, Steps: 14 | Train Loss: 0.5503915 Vali Loss: 1.3303827 Test Loss: 0.5456262
Validation loss decreased (1.343797 --> 1.330383).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.901524782180786
Epoch: 10, Steps: 14 | Train Loss: 0.5413752 Vali Loss: 1.3104577 Test Loss: 0.5353978
Validation loss decreased (1.330383 --> 1.310458).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.9249913692474365
Epoch: 11, Steps: 14 | Train Loss: 0.5336990 Vali Loss: 1.3035418 Test Loss: 0.5265030
Validation loss decreased (1.310458 --> 1.303542).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.998324394226074
Epoch: 12, Steps: 14 | Train Loss: 0.5272274 Vali Loss: 1.2860636 Test Loss: 0.5188812
Validation loss decreased (1.303542 --> 1.286064).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.9791572093963623
Epoch: 13, Steps: 14 | Train Loss: 0.5212751 Vali Loss: 1.2796676 Test Loss: 0.5121687
Validation loss decreased (1.286064 --> 1.279668).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.6106221675872803
Epoch: 14, Steps: 14 | Train Loss: 0.5151464 Vali Loss: 1.2703598 Test Loss: 0.5063554
Validation loss decreased (1.279668 --> 1.270360).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.5527150630950928
Epoch: 15, Steps: 14 | Train Loss: 0.5107732 Vali Loss: 1.2561311 Test Loss: 0.5012037
Validation loss decreased (1.270360 --> 1.256131).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.545830249786377
Epoch: 16, Steps: 14 | Train Loss: 0.5077331 Vali Loss: 1.2458414 Test Loss: 0.4966772
Validation loss decreased (1.256131 --> 1.245841).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.568063259124756
Epoch: 17, Steps: 14 | Train Loss: 0.5040003 Vali Loss: 1.2521414 Test Loss: 0.4925459
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.5667479038238525
Epoch: 18, Steps: 14 | Train Loss: 0.5008087 Vali Loss: 1.2470460 Test Loss: 0.4888820
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.628427267074585
Epoch: 19, Steps: 14 | Train Loss: 0.4983199 Vali Loss: 1.2455792 Test Loss: 0.4855699
Validation loss decreased (1.245841 --> 1.245579).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.672883987426758
Epoch: 20, Steps: 14 | Train Loss: 0.4953334 Vali Loss: 1.2324450 Test Loss: 0.4826008
Validation loss decreased (1.245579 --> 1.232445).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.470087766647339
Epoch: 21, Steps: 14 | Train Loss: 0.4930125 Vali Loss: 1.2333446 Test Loss: 0.4798937
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.5972626209259033
Epoch: 22, Steps: 14 | Train Loss: 0.4900971 Vali Loss: 1.2280962 Test Loss: 0.4774410
Validation loss decreased (1.232445 --> 1.228096).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.51721453666687
Epoch: 23, Steps: 14 | Train Loss: 0.4882226 Vali Loss: 1.2204061 Test Loss: 0.4752289
Validation loss decreased (1.228096 --> 1.220406).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.884270429611206
Epoch: 24, Steps: 14 | Train Loss: 0.4872299 Vali Loss: 1.2213089 Test Loss: 0.4732520
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.865483522415161
Epoch: 25, Steps: 14 | Train Loss: 0.4850920 Vali Loss: 1.2236611 Test Loss: 0.4714277
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.919943332672119
Epoch: 26, Steps: 14 | Train Loss: 0.4831877 Vali Loss: 1.2167907 Test Loss: 0.4697708
Validation loss decreased (1.220406 --> 1.216791).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.1183111667633057
Epoch: 27, Steps: 14 | Train Loss: 0.4821328 Vali Loss: 1.2249992 Test Loss: 0.4682385
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.196124315261841
Epoch: 28, Steps: 14 | Train Loss: 0.4800651 Vali Loss: 1.2141941 Test Loss: 0.4668289
Validation loss decreased (1.216791 --> 1.214194).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.232614040374756
Epoch: 29, Steps: 14 | Train Loss: 0.4792023 Vali Loss: 1.2126958 Test Loss: 0.4655581
Validation loss decreased (1.214194 --> 1.212696).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.157449960708618
Epoch: 30, Steps: 14 | Train Loss: 0.4796109 Vali Loss: 1.2084740 Test Loss: 0.4643796
Validation loss decreased (1.212696 --> 1.208474).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.1406126022338867
Epoch: 31, Steps: 14 | Train Loss: 0.4781395 Vali Loss: 1.2160379 Test Loss: 0.4632889
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.227165699005127
Epoch: 32, Steps: 14 | Train Loss: 0.4766275 Vali Loss: 1.2111540 Test Loss: 0.4622556
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.363095998764038
Epoch: 33, Steps: 14 | Train Loss: 0.4753360 Vali Loss: 1.2194123 Test Loss: 0.4613389
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4634832739830017, mae:0.4616850018501282, rse:0.6481403708457947, corr:[0.23901117 0.25613856 0.25370023 0.24789126 0.2466989  0.24893968
 0.25012994 0.24986582 0.24928491 0.24931514 0.2497969  0.24967436
 0.2489308  0.24832231 0.24839695 0.24888417 0.24861108 0.2470866
 0.24528137 0.24428432 0.24451014 0.24464978 0.24399637 0.24249904
 0.24124633 0.24106555 0.24168362 0.24209794 0.24202852 0.2417877
 0.24194242 0.24264815 0.24345113 0.24376713 0.2434831  0.24302496
 0.24289037 0.24281266 0.24253993 0.24216522 0.24189721 0.24208252
 0.2425513  0.24300548 0.24315494 0.24315245 0.24326028 0.24353662
 0.24381928 0.24364594 0.24293947 0.24216074 0.24154589 0.2410246
 0.24059738 0.24029298 0.24001464 0.23965403 0.23934317 0.2391136
 0.23896286 0.23883268 0.23876481 0.23889865 0.23897977 0.23895863
 0.23879164 0.23847203 0.2384696  0.23855028 0.23848955 0.23807809
 0.23742037 0.23671445 0.23622721 0.23596019 0.23567823 0.23535077
 0.23509914 0.23514803 0.23530105 0.2354609  0.2355489  0.23558061
 0.23536181 0.23496537 0.23449722 0.23416576 0.23387896 0.23357385
 0.23319323 0.23275983 0.23213607 0.23140839 0.23067518 0.23062198
 0.23127279 0.23213884 0.23281305 0.23325622 0.23362884 0.23397186
 0.2343892  0.2348943  0.23529232 0.23534574 0.23512664 0.2349189
 0.2348041  0.23480871 0.23477106 0.23478122 0.23470432 0.23451199
 0.23414578 0.23352325 0.23292637 0.23259664 0.23264779 0.2328732
 0.23273042 0.23186788 0.23066318 0.22979312 0.22957407 0.22967792
 0.22980101 0.22987135 0.22974825 0.2295426  0.22928676 0.22888361
 0.22838363 0.2278986  0.22770323 0.22780652 0.22779676 0.22749637
 0.2269246  0.22650069 0.22643499 0.2263574  0.2258604  0.22535004
 0.22499372 0.2247406  0.2244679  0.22402649 0.2235261  0.22304265
 0.22287872 0.22319253 0.22376159 0.22397043 0.22367446 0.22327627
 0.22296321 0.22284542 0.22276528 0.2227804  0.22277847 0.22294782
 0.22308058 0.22287676 0.22228794 0.22162014 0.22086954 0.22026052
 0.21980275 0.21947315 0.21919525 0.21913208 0.21929662 0.21969469
 0.22005361 0.22037718 0.22057724 0.22084849 0.22116753 0.22140063
 0.2212562  0.22093368 0.22084783 0.22120677 0.22171044 0.22187848
 0.22164518 0.22148685 0.22176847 0.22225095 0.22232437 0.22177841
 0.22080702 0.21981975 0.21916342 0.21878116 0.21852939 0.21823314
 0.21798994 0.21810277 0.21843335 0.21850337 0.21822725 0.21815921
 0.21856833 0.21949509 0.22027005 0.22052017 0.22049543 0.22041814
 0.2203314  0.21988897 0.21896301 0.21791787 0.2173244  0.21752916
 0.21810095 0.21836746 0.21798784 0.21737    0.217231   0.2174904
 0.21775623 0.21783382 0.21773298 0.21767259 0.21771519 0.21798526
 0.2178684  0.21742034 0.21729566 0.21783945 0.21841556 0.2185432
 0.21806735 0.21738553 0.21705328 0.21720384 0.21744764 0.21719085
 0.21670474 0.2163988  0.2167888  0.21755248 0.21791895 0.21773621
 0.21742862 0.21753034 0.21792358 0.21792135 0.21721734 0.2164488
 0.21620168 0.21646169 0.2169132  0.21692422 0.2166496  0.216536
 0.21690145 0.21726221 0.21722965 0.21670574 0.21623735 0.2161069
 0.2163857  0.21657822 0.21610235 0.21516627 0.21406202 0.21341106
 0.21341594 0.21342283 0.21309724 0.2128912  0.21293478 0.21307912
 0.21280138 0.21208525 0.21146257 0.21172468 0.21270898 0.21347436
 0.21308431 0.21185122 0.21105482 0.21123548 0.21179897 0.21164364
 0.21063437 0.2097293  0.20971128 0.21081403 0.21169387 0.21162145
 0.21088181 0.21041486 0.21073624 0.21137445 0.21132816 0.21081387
 0.21081436 0.21168315 0.21293172 0.21318804 0.21248025 0.21182565
 0.21241803 0.21325016 0.21310413 0.21117029 0.2086953  0.20763774
 0.20879126 0.21028234 0.20965174 0.20701432 0.2047382  0.20447323
 0.2055797  0.20565476 0.20433024 0.20307022 0.2032637  0.20436795
 0.20419765 0.20161945 0.19912946 0.19990186 0.20225552 0.2018573
 0.19572566 0.18734634 0.1848488  0.189311   0.1846127  0.13637185]
