Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  47065088.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.094583988189697
Epoch: 1, Steps: 29 | Train Loss: 0.8270420 Vali Loss: 1.6727228 Test Loss: 0.7674250
Validation loss decreased (inf --> 1.672723).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.464709043502808
Epoch: 2, Steps: 29 | Train Loss: 0.6683055 Vali Loss: 1.4608053 Test Loss: 0.6399108
Validation loss decreased (1.672723 --> 1.460805).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.253040552139282
Epoch: 3, Steps: 29 | Train Loss: 0.5966793 Vali Loss: 1.3825413 Test Loss: 0.5798756
Validation loss decreased (1.460805 --> 1.382541).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.4473230838775635
Epoch: 4, Steps: 29 | Train Loss: 0.5589544 Vali Loss: 1.3284473 Test Loss: 0.5436574
Validation loss decreased (1.382541 --> 1.328447).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.844542026519775
Epoch: 5, Steps: 29 | Train Loss: 0.5347422 Vali Loss: 1.2910669 Test Loss: 0.5187771
Validation loss decreased (1.328447 --> 1.291067).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.116680145263672
Epoch: 6, Steps: 29 | Train Loss: 0.5172076 Vali Loss: 1.2634516 Test Loss: 0.5004752
Validation loss decreased (1.291067 --> 1.263452).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.885554552078247
Epoch: 7, Steps: 29 | Train Loss: 0.5039779 Vali Loss: 1.2426876 Test Loss: 0.4867115
Validation loss decreased (1.263452 --> 1.242688).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.155436277389526
Epoch: 8, Steps: 29 | Train Loss: 0.4937520 Vali Loss: 1.2325841 Test Loss: 0.4761844
Validation loss decreased (1.242688 --> 1.232584).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.038552522659302
Epoch: 9, Steps: 29 | Train Loss: 0.4859106 Vali Loss: 1.2184786 Test Loss: 0.4680231
Validation loss decreased (1.232584 --> 1.218479).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.229628801345825
Epoch: 10, Steps: 29 | Train Loss: 0.4804672 Vali Loss: 1.2130946 Test Loss: 0.4616540
Validation loss decreased (1.218479 --> 1.213095).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.769437313079834
Epoch: 11, Steps: 29 | Train Loss: 0.4750139 Vali Loss: 1.2085081 Test Loss: 0.4567835
Validation loss decreased (1.213095 --> 1.208508).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.9436256885528564
Epoch: 12, Steps: 29 | Train Loss: 0.4704835 Vali Loss: 1.2021190 Test Loss: 0.4527667
Validation loss decreased (1.208508 --> 1.202119).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.948967456817627
Epoch: 13, Steps: 29 | Train Loss: 0.4667921 Vali Loss: 1.1969234 Test Loss: 0.4497892
Validation loss decreased (1.202119 --> 1.196923).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.195354700088501
Epoch: 14, Steps: 29 | Train Loss: 0.4650421 Vali Loss: 1.1936189 Test Loss: 0.4472501
Validation loss decreased (1.196923 --> 1.193619).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.134317636489868
Epoch: 15, Steps: 29 | Train Loss: 0.4620854 Vali Loss: 1.1885177 Test Loss: 0.4454747
Validation loss decreased (1.193619 --> 1.188518).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.799934387207031
Epoch: 16, Steps: 29 | Train Loss: 0.4596972 Vali Loss: 1.1847829 Test Loss: 0.4440621
Validation loss decreased (1.188518 --> 1.184783).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.227907657623291
Epoch: 17, Steps: 29 | Train Loss: 0.4579204 Vali Loss: 1.1864628 Test Loss: 0.4428762
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.149352550506592
Epoch: 18, Steps: 29 | Train Loss: 0.4564638 Vali Loss: 1.1908056 Test Loss: 0.4419678
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.871784925460815
Epoch: 19, Steps: 29 | Train Loss: 0.4559226 Vali Loss: 1.1862872 Test Loss: 0.4412624
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.44237855076789856, mae:0.44648560881614685, rse:0.6332119107246399, corr:[0.24682254 0.2600568  0.25723872 0.25093693 0.24975444 0.2514383
 0.25200623 0.25146413 0.25062683 0.25048494 0.25065884 0.25037932
 0.24979177 0.24937133 0.24959777 0.24995272 0.24947515 0.2481397
 0.24677108 0.24621089 0.24655178 0.24658674 0.24593689 0.24477096
 0.24407485 0.24435708 0.24502261 0.24527018 0.24500653 0.24460183
 0.24465789 0.2452356  0.24588634 0.24612631 0.2458302  0.24543446
 0.24532962 0.24529652 0.24520813 0.24508554 0.24506073 0.24533719
 0.245748   0.24614242 0.24640203 0.24666606 0.24701536 0.2472638
 0.24721411 0.24666467 0.24567784 0.24478452 0.2441638  0.24365747
 0.24322534 0.2428653  0.24254374 0.24224085 0.24206297 0.242026
 0.24200693 0.24192034 0.24178924 0.24179256 0.24187085 0.24203186
 0.24215502 0.24211709 0.24222635 0.2422755  0.242119   0.24170716
 0.24116683 0.24064432 0.24027398 0.24007307 0.23982419 0.2394385
 0.23910642 0.23903835 0.23910613 0.23914516 0.2390228  0.2388053
 0.23850873 0.23822223 0.23798446 0.23782589 0.23755574 0.2372257
 0.23689355 0.23670824 0.23655896 0.23641995 0.23624018 0.23640482
 0.23692575 0.23748104 0.23785482 0.23812151 0.23844992 0.23882657
 0.23922268 0.23949522 0.23952118 0.23929346 0.2390179  0.23893651
 0.23901668 0.2391261  0.23908061 0.2390012  0.23895329 0.23900585
 0.23902208 0.2388101  0.2385098  0.23830135 0.23833102 0.23851545
 0.23845857 0.23787677 0.237058   0.23647106 0.2362163  0.23597296
 0.23563755 0.2353737  0.23516515 0.23505014 0.23485704 0.2344347
 0.23391907 0.23354681 0.2336203  0.23403537 0.23426175 0.23407024
 0.23354138 0.23313417 0.23311915 0.23317055 0.23285723 0.23234798
 0.23180431 0.23131366 0.2308749  0.23036203 0.22988953 0.22953118
 0.22952579 0.22987482 0.23022075 0.23015481 0.22975402 0.22949323
 0.22948878 0.22954138 0.22940294 0.22919556 0.2290004  0.22919187
 0.229538   0.22960983 0.22924165 0.22866994 0.22808017 0.22780792
 0.22777435 0.22777925 0.22767666 0.22763036 0.22771999 0.22788647
 0.2278967  0.22781408 0.22768655 0.22771685 0.22776687 0.22760068
 0.2270397  0.2264248  0.22631958 0.22688767 0.22760542 0.22784424
 0.2274905  0.22711611 0.22733869 0.22801512 0.22846742 0.22830565
 0.22756164 0.22665447 0.22597139 0.22547626 0.22504106 0.22463521
 0.22443499 0.22464569 0.22494583 0.22478764 0.22419061 0.22383617
 0.22414026 0.22504885 0.22580308 0.22590338 0.22558153 0.22530654
 0.22530502 0.22520755 0.22472171 0.22392416 0.2232618  0.22321
 0.22354639 0.22366354 0.2232208  0.22257853 0.22240461 0.2226417
 0.22281885 0.22264749 0.22221465 0.22190015 0.22188753 0.22214068
 0.22201782 0.22144209 0.22108427 0.22150043 0.2222304  0.22263414
 0.22230856 0.2215355  0.22104715 0.22126119 0.22187543 0.22204222
 0.22168255 0.22109008 0.22087833 0.221074   0.22114116 0.22088504
 0.22061054 0.22072636 0.22103119 0.22094539 0.22022781 0.21945299
 0.2192345  0.2195624  0.2200299  0.21997432 0.21951175 0.21922319
 0.21960245 0.22020528 0.22051539 0.22021937 0.21973722 0.21952756
 0.21981248 0.22008269 0.21968414 0.21871771 0.21755716 0.21680368
 0.21657503 0.21625225 0.21555841 0.21508129 0.2151155  0.21546115
 0.21539225 0.21466236 0.21378116 0.21374205 0.21466815 0.21567325
 0.21559139 0.2144254  0.21345799 0.2135495  0.21442266 0.2149097
 0.21441375 0.21354023 0.21311492 0.21366933 0.21420199 0.21402952
 0.21331884 0.21287698 0.21309587 0.21353485 0.21325922 0.21240978
 0.21198669 0.21251981 0.21363983 0.21392421 0.21310562 0.21209542
 0.21233042 0.21325625 0.21380754 0.21278064 0.21078949 0.20949939
 0.20995975 0.21095121 0.21035564 0.20805246 0.20589863 0.205183
 0.20550694 0.20500763 0.20340501 0.2020373  0.20210269 0.20314275
 0.20307949 0.20050052 0.1976035  0.19785099 0.20033513 0.20107192
 0.19617632 0.18756343 0.18314853 0.18709888 0.18829876 0.15653655]
