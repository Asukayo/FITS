Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=134, out_features=151, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  36259328.0
params:  20385.0
Trainable parameters:  20385
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.997066259384155
Epoch: 1, Steps: 30 | Train Loss: 0.6592469 Vali Loss: 1.5550078 Test Loss: 0.8896502
Validation loss decreased (inf --> 1.555008).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 6.1108057498931885
Epoch: 2, Steps: 30 | Train Loss: 0.5695835 Vali Loss: 1.4112866 Test Loss: 0.8220489
Validation loss decreased (1.555008 --> 1.411287).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.652044773101807
Epoch: 3, Steps: 30 | Train Loss: 0.5102742 Vali Loss: 1.3307483 Test Loss: 0.7792129
Validation loss decreased (1.411287 --> 1.330748).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.770401477813721
Epoch: 4, Steps: 30 | Train Loss: 0.4692304 Vali Loss: 1.2875471 Test Loss: 0.7549154
Validation loss decreased (1.330748 --> 1.287547).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.731122732162476
Epoch: 5, Steps: 30 | Train Loss: 0.4395892 Vali Loss: 1.2496259 Test Loss: 0.7396925
Validation loss decreased (1.287547 --> 1.249626).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.591792583465576
Epoch: 6, Steps: 30 | Train Loss: 0.4170983 Vali Loss: 1.2394944 Test Loss: 0.7298362
Validation loss decreased (1.249626 --> 1.239494).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.704826593399048
Epoch: 7, Steps: 30 | Train Loss: 0.3995990 Vali Loss: 1.2226162 Test Loss: 0.7241137
Validation loss decreased (1.239494 --> 1.222616).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.70474648475647
Epoch: 8, Steps: 30 | Train Loss: 0.3848675 Vali Loss: 1.2115467 Test Loss: 0.7187186
Validation loss decreased (1.222616 --> 1.211547).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.7481849193573
Epoch: 9, Steps: 30 | Train Loss: 0.3722478 Vali Loss: 1.2069778 Test Loss: 0.7166212
Validation loss decreased (1.211547 --> 1.206978).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.797935724258423
Epoch: 10, Steps: 30 | Train Loss: 0.3614905 Vali Loss: 1.1925532 Test Loss: 0.7105256
Validation loss decreased (1.206978 --> 1.192553).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.941270351409912
Epoch: 11, Steps: 30 | Train Loss: 0.3518571 Vali Loss: 1.1833149 Test Loss: 0.7092096
Validation loss decreased (1.192553 --> 1.183315).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.899099111557007
Epoch: 12, Steps: 30 | Train Loss: 0.3434167 Vali Loss: 1.1762631 Test Loss: 0.7062315
Validation loss decreased (1.183315 --> 1.176263).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.734835386276245
Epoch: 13, Steps: 30 | Train Loss: 0.3356758 Vali Loss: 1.1672957 Test Loss: 0.7023772
Validation loss decreased (1.176263 --> 1.167296).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.477774381637573
Epoch: 14, Steps: 30 | Train Loss: 0.3286234 Vali Loss: 1.1622089 Test Loss: 0.6993239
Validation loss decreased (1.167296 --> 1.162209).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.71384334564209
Epoch: 15, Steps: 30 | Train Loss: 0.3223467 Vali Loss: 1.1537135 Test Loss: 0.6970043
Validation loss decreased (1.162209 --> 1.153713).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.8035056591033936
Epoch: 16, Steps: 30 | Train Loss: 0.3162700 Vali Loss: 1.1427895 Test Loss: 0.6937228
Validation loss decreased (1.153713 --> 1.142789).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.755382299423218
Epoch: 17, Steps: 30 | Train Loss: 0.3107961 Vali Loss: 1.1442668 Test Loss: 0.6897253
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.704486846923828
Epoch: 18, Steps: 30 | Train Loss: 0.3059766 Vali Loss: 1.1352177 Test Loss: 0.6875882
Validation loss decreased (1.142789 --> 1.135218).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.802515745162964
Epoch: 19, Steps: 30 | Train Loss: 0.3013020 Vali Loss: 1.1341021 Test Loss: 0.6845427
Validation loss decreased (1.135218 --> 1.134102).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.6654229164123535
Epoch: 20, Steps: 30 | Train Loss: 0.2966548 Vali Loss: 1.1345669 Test Loss: 0.6823356
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.815485000610352
Epoch: 21, Steps: 30 | Train Loss: 0.2926488 Vali Loss: 1.1204182 Test Loss: 0.6793227
Validation loss decreased (1.134102 --> 1.120418).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 6.0165019035339355
Epoch: 22, Steps: 30 | Train Loss: 0.2887632 Vali Loss: 1.1211748 Test Loss: 0.6766933
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 5.797224760055542
Epoch: 23, Steps: 30 | Train Loss: 0.2850484 Vali Loss: 1.1154711 Test Loss: 0.6737304
Validation loss decreased (1.120418 --> 1.115471).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 6.1618897914886475
Epoch: 24, Steps: 30 | Train Loss: 0.2818251 Vali Loss: 1.1175846 Test Loss: 0.6719747
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 6.242910623550415
Epoch: 25, Steps: 30 | Train Loss: 0.2785535 Vali Loss: 1.1062819 Test Loss: 0.6696072
Validation loss decreased (1.115471 --> 1.106282).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 6.193097352981567
Epoch: 26, Steps: 30 | Train Loss: 0.2758239 Vali Loss: 1.1057254 Test Loss: 0.6669257
Validation loss decreased (1.106282 --> 1.105725).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 6.017072439193726
Epoch: 27, Steps: 30 | Train Loss: 0.2729798 Vali Loss: 1.1000164 Test Loss: 0.6650147
Validation loss decreased (1.105725 --> 1.100016).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 6.146022081375122
Epoch: 28, Steps: 30 | Train Loss: 0.2703486 Vali Loss: 1.0960795 Test Loss: 0.6632103
Validation loss decreased (1.100016 --> 1.096079).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 6.380653142929077
Epoch: 29, Steps: 30 | Train Loss: 0.2679733 Vali Loss: 1.0891585 Test Loss: 0.6608496
Validation loss decreased (1.096079 --> 1.089159).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 6.049529075622559
Epoch: 30, Steps: 30 | Train Loss: 0.2654573 Vali Loss: 1.0843586 Test Loss: 0.6589644
Validation loss decreased (1.089159 --> 1.084359).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 5.653674125671387
Epoch: 31, Steps: 30 | Train Loss: 0.2634124 Vali Loss: 1.0785511 Test Loss: 0.6569709
Validation loss decreased (1.084359 --> 1.078551).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 5.68531060218811
Epoch: 32, Steps: 30 | Train Loss: 0.2612749 Vali Loss: 1.0856197 Test Loss: 0.6559906
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 5.9803972244262695
Epoch: 33, Steps: 30 | Train Loss: 0.2597553 Vali Loss: 1.0785075 Test Loss: 0.6534636
Validation loss decreased (1.078551 --> 1.078508).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 5.704256057739258
Epoch: 34, Steps: 30 | Train Loss: 0.2576630 Vali Loss: 1.0745938 Test Loss: 0.6519910
Validation loss decreased (1.078508 --> 1.074594).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 6.351266622543335
Epoch: 35, Steps: 30 | Train Loss: 0.2560755 Vali Loss: 1.0771222 Test Loss: 0.6503806
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 6.66091513633728
Epoch: 36, Steps: 30 | Train Loss: 0.2545152 Vali Loss: 1.0729773 Test Loss: 0.6488083
Validation loss decreased (1.074594 --> 1.072977).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 6.576514720916748
Epoch: 37, Steps: 30 | Train Loss: 0.2528298 Vali Loss: 1.0746361 Test Loss: 0.6475854
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 6.58599328994751
Epoch: 38, Steps: 30 | Train Loss: 0.2514928 Vali Loss: 1.0604782 Test Loss: 0.6461434
Validation loss decreased (1.072977 --> 1.060478).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 6.187297105789185
Epoch: 39, Steps: 30 | Train Loss: 0.2501854 Vali Loss: 1.0664859 Test Loss: 0.6448482
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 6.132858037948608
Epoch: 40, Steps: 30 | Train Loss: 0.2489619 Vali Loss: 1.0629323 Test Loss: 0.6438298
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 6.618801832199097
Epoch: 41, Steps: 30 | Train Loss: 0.2475482 Vali Loss: 1.0645682 Test Loss: 0.6428510
EarlyStopping counter: 3 out of 3
Early stopping
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=134, out_features=151, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  36259328.0
params:  20385.0
Trainable parameters:  20385
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 6.455777406692505
Epoch: 1, Steps: 30 | Train Loss: 0.4527666 Vali Loss: 0.8728392 Test Loss: 0.4986683
Validation loss decreased (inf --> 0.872839).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 6.816906690597534
Epoch: 2, Steps: 30 | Train Loss: 0.3900524 Vali Loss: 0.7821395 Test Loss: 0.4279863
Validation loss decreased (0.872839 --> 0.782140).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 7.1839680671691895
Epoch: 3, Steps: 30 | Train Loss: 0.3609429 Vali Loss: 0.7368519 Test Loss: 0.3996002
Validation loss decreased (0.782140 --> 0.736852).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 6.986667156219482
Epoch: 4, Steps: 30 | Train Loss: 0.3491753 Vali Loss: 0.7248564 Test Loss: 0.3892730
Validation loss decreased (0.736852 --> 0.724856).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 7.1506805419921875
Epoch: 5, Steps: 30 | Train Loss: 0.3442454 Vali Loss: 0.7119602 Test Loss: 0.3859552
Validation loss decreased (0.724856 --> 0.711960).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 7.173591613769531
Epoch: 6, Steps: 30 | Train Loss: 0.3416438 Vali Loss: 0.7125325 Test Loss: 0.3849469
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 7.221806764602661
Epoch: 7, Steps: 30 | Train Loss: 0.3409158 Vali Loss: 0.7126846 Test Loss: 0.3847168
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 7.404378414154053
Epoch: 8, Steps: 30 | Train Loss: 0.3390275 Vali Loss: 0.7075004 Test Loss: 0.3844842
Validation loss decreased (0.711960 --> 0.707500).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.035928726196289
Epoch: 9, Steps: 30 | Train Loss: 0.3383663 Vali Loss: 0.7014946 Test Loss: 0.3845334
Validation loss decreased (0.707500 --> 0.701495).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.0066659450531
Epoch: 10, Steps: 30 | Train Loss: 0.3383423 Vali Loss: 0.7022911 Test Loss: 0.3844019
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 7.957695245742798
Epoch: 11, Steps: 30 | Train Loss: 0.3381573 Vali Loss: 0.7046446 Test Loss: 0.3844786
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 7.704709529876709
Epoch: 12, Steps: 30 | Train Loss: 0.3376447 Vali Loss: 0.7036352 Test Loss: 0.3847253
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.38369041681289673, mae:0.40691789984703064, rse:0.5883670449256897, corr:[0.27344614 0.27977964 0.2796431  0.27657393 0.27435416 0.2729186
 0.27200902 0.27133042 0.2706575  0.27037114 0.270217   0.27003688
 0.26986986 0.26976836 0.26990208 0.27002075 0.26982686 0.26940754
 0.26879385 0.26814973 0.2676064  0.26741925 0.2672691  0.26729962
 0.2672827  0.26720795 0.26704004 0.26674172 0.26636064 0.26595357
 0.26565272 0.2653596  0.26523617 0.2651908  0.2649972  0.26481694
 0.2647182  0.26470852 0.26479626 0.26502842 0.2656109  0.26615497
 0.26625833 0.26604566 0.26589176 0.2660577  0.26652125 0.26685518
 0.2663823  0.2655839  0.26447126 0.2632684  0.26220644 0.26104784
 0.25994885 0.25903216 0.2583371  0.258183   0.25824574 0.2584815
 0.25851515 0.2583896  0.25824326 0.25834444 0.25847682 0.25840756
 0.25835037 0.258177   0.25825    0.2586075  0.25863078 0.2578915
 0.25684568 0.25591558 0.25522822 0.25490588 0.2544044  0.25345862
 0.2523962  0.2516522  0.25097248 0.25015172 0.24922739 0.24868223
 0.24912354 0.24987563 0.24990049 0.2490956  0.24837081 0.24833141
 0.24803554 0.24600098 0.24300766 0.24108556 0.24198726 0.24081445]
