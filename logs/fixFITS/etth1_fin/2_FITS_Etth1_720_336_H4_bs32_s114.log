Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11766272.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5111161
	speed: 0.1695s/iter; left time: 983.5090s
Epoch: 1 cost time: 20.046067714691162
Epoch: 1, Steps: 118 | Train Loss: 0.6247921 Vali Loss: 1.5979086 Test Loss: 0.7470190
Validation loss decreased (inf --> 1.597909).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4522477
	speed: 0.4122s/iter; left time: 2342.6894s
Epoch: 2 cost time: 19.51058340072632
Epoch: 2, Steps: 118 | Train Loss: 0.4557067 Vali Loss: 1.4735959 Test Loss: 0.6856713
Validation loss decreased (1.597909 --> 1.473596).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3631351
	speed: 0.3306s/iter; left time: 1839.7149s
Epoch: 3 cost time: 15.86675214767456
Epoch: 3, Steps: 118 | Train Loss: 0.3892238 Vali Loss: 1.4253684 Test Loss: 0.6586967
Validation loss decreased (1.473596 --> 1.425368).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3286674
	speed: 0.4095s/iter; left time: 2230.7389s
Epoch: 4 cost time: 19.4464430809021
Epoch: 4, Steps: 118 | Train Loss: 0.3479563 Vali Loss: 1.3907291 Test Loss: 0.6387962
Validation loss decreased (1.425368 --> 1.390729).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3085544
	speed: 0.4062s/iter; left time: 2164.4153s
Epoch: 5 cost time: 19.345035791397095
Epoch: 5, Steps: 118 | Train Loss: 0.3172357 Vali Loss: 1.3682268 Test Loss: 0.6188418
Validation loss decreased (1.390729 --> 1.368227).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2990152
	speed: 0.4031s/iter; left time: 2100.4305s
Epoch: 6 cost time: 18.627967357635498
Epoch: 6, Steps: 118 | Train Loss: 0.2931982 Vali Loss: 1.3427533 Test Loss: 0.5998408
Validation loss decreased (1.368227 --> 1.342753).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2712047
	speed: 0.4544s/iter; left time: 2314.4400s
Epoch: 7 cost time: 22.663661003112793
Epoch: 7, Steps: 118 | Train Loss: 0.2738543 Vali Loss: 1.3277091 Test Loss: 0.5849048
Validation loss decreased (1.342753 --> 1.327709).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2673319
	speed: 0.4723s/iter; left time: 2349.4803s
Epoch: 8 cost time: 22.946755409240723
Epoch: 8, Steps: 118 | Train Loss: 0.2579605 Vali Loss: 1.3110415 Test Loss: 0.5705360
Validation loss decreased (1.327709 --> 1.311041).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2551757
	speed: 0.3991s/iter; left time: 1938.3542s
Epoch: 9 cost time: 15.991491794586182
Epoch: 9, Steps: 118 | Train Loss: 0.2448707 Vali Loss: 1.2970573 Test Loss: 0.5582255
Validation loss decreased (1.311041 --> 1.297057).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2240039
	speed: 0.3934s/iter; left time: 1864.3322s
Epoch: 10 cost time: 19.98197913169861
Epoch: 10, Steps: 118 | Train Loss: 0.2341176 Vali Loss: 1.2884796 Test Loss: 0.5452026
Validation loss decreased (1.297057 --> 1.288480).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2312943
	speed: 0.4082s/iter; left time: 1886.4844s
Epoch: 11 cost time: 19.142674207687378
Epoch: 11, Steps: 118 | Train Loss: 0.2250377 Vali Loss: 1.2777315 Test Loss: 0.5354553
Validation loss decreased (1.288480 --> 1.277732).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2199065
	speed: 0.4083s/iter; left time: 1838.6935s
Epoch: 12 cost time: 19.396072149276733
Epoch: 12, Steps: 118 | Train Loss: 0.2174505 Vali Loss: 1.2688946 Test Loss: 0.5262347
Validation loss decreased (1.277732 --> 1.268895).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2089012
	speed: 0.3949s/iter; left time: 1731.5457s
Epoch: 13 cost time: 19.184405088424683
Epoch: 13, Steps: 118 | Train Loss: 0.2109227 Vali Loss: 1.2664974 Test Loss: 0.5197151
Validation loss decreased (1.268895 --> 1.266497).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2156602
	speed: 0.3503s/iter; left time: 1494.6342s
Epoch: 14 cost time: 17.043746948242188
Epoch: 14, Steps: 118 | Train Loss: 0.2054068 Vali Loss: 1.2575495 Test Loss: 0.5105941
Validation loss decreased (1.266497 --> 1.257550).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2100656
	speed: 0.4029s/iter; left time: 1671.7012s
Epoch: 15 cost time: 18.870195150375366
Epoch: 15, Steps: 118 | Train Loss: 0.2006649 Vali Loss: 1.2534418 Test Loss: 0.5056235
Validation loss decreased (1.257550 --> 1.253442).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2128586
	speed: 0.4180s/iter; left time: 1684.7951s
Epoch: 16 cost time: 20.688920259475708
Epoch: 16, Steps: 118 | Train Loss: 0.1966855 Vali Loss: 1.2483922 Test Loss: 0.4992667
Validation loss decreased (1.253442 --> 1.248392).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1886522
	speed: 0.4347s/iter; left time: 1701.0497s
Epoch: 17 cost time: 20.666617155075073
Epoch: 17, Steps: 118 | Train Loss: 0.1931110 Vali Loss: 1.2479969 Test Loss: 0.4945543
Validation loss decreased (1.248392 --> 1.247997).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2074596
	speed: 0.4356s/iter; left time: 1653.0633s
Epoch: 18 cost time: 20.32776975631714
Epoch: 18, Steps: 118 | Train Loss: 0.1901122 Vali Loss: 1.2416123 Test Loss: 0.4898412
Validation loss decreased (1.247997 --> 1.241612).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1863196
	speed: 0.4122s/iter; left time: 1515.5770s
Epoch: 19 cost time: 19.559373140335083
Epoch: 19, Steps: 118 | Train Loss: 0.1873935 Vali Loss: 1.2390984 Test Loss: 0.4870102
Validation loss decreased (1.241612 --> 1.239098).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1800400
	speed: 0.4090s/iter; left time: 1455.7490s
Epoch: 20 cost time: 19.337923765182495
Epoch: 20, Steps: 118 | Train Loss: 0.1849279 Vali Loss: 1.2385387 Test Loss: 0.4823960
Validation loss decreased (1.239098 --> 1.238539).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1878908
	speed: 0.3392s/iter; left time: 1167.2104s
Epoch: 21 cost time: 15.063466787338257
Epoch: 21, Steps: 118 | Train Loss: 0.1827999 Vali Loss: 1.2337068 Test Loss: 0.4790463
Validation loss decreased (1.238539 --> 1.233707).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1831821
	speed: 0.3470s/iter; left time: 1153.1292s
Epoch: 22 cost time: 15.203993558883667
Epoch: 22, Steps: 118 | Train Loss: 0.1810238 Vali Loss: 1.2363552 Test Loss: 0.4761671
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1905138
	speed: 0.3013s/iter; left time: 965.6427s
Epoch: 23 cost time: 19.291760206222534
Epoch: 23, Steps: 118 | Train Loss: 0.1793333 Vali Loss: 1.2339274 Test Loss: 0.4736206
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1798320
	speed: 0.4140s/iter; left time: 1278.1658s
Epoch: 24 cost time: 19.82644534111023
Epoch: 24, Steps: 118 | Train Loss: 0.1779169 Vali Loss: 1.2331339 Test Loss: 0.4714971
Validation loss decreased (1.233707 --> 1.233134).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1802820
	speed: 0.4124s/iter; left time: 1224.5296s
Epoch: 25 cost time: 19.478057861328125
Epoch: 25, Steps: 118 | Train Loss: 0.1766646 Vali Loss: 1.2310412 Test Loss: 0.4692826
Validation loss decreased (1.233134 --> 1.231041).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1663203
	speed: 0.4130s/iter; left time: 1177.3825s
Epoch: 26 cost time: 18.58430814743042
Epoch: 26, Steps: 118 | Train Loss: 0.1754890 Vali Loss: 1.2273551 Test Loss: 0.4672825
Validation loss decreased (1.231041 --> 1.227355).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1731717
	speed: 0.3286s/iter; left time: 898.0905s
Epoch: 27 cost time: 15.880197286605835
Epoch: 27, Steps: 118 | Train Loss: 0.1743202 Vali Loss: 1.2301614 Test Loss: 0.4658362
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1648753
	speed: 0.4033s/iter; left time: 1054.7444s
Epoch: 28 cost time: 19.179706811904907
Epoch: 28, Steps: 118 | Train Loss: 0.1734715 Vali Loss: 1.2304351 Test Loss: 0.4641806
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1610240
	speed: 0.3970s/iter; left time: 991.2228s
Epoch: 29 cost time: 18.864288091659546
Epoch: 29, Steps: 118 | Train Loss: 0.1726250 Vali Loss: 1.2282041 Test Loss: 0.4627488
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11766272.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4820535
	speed: 0.1629s/iter; left time: 944.7594s
Epoch: 1 cost time: 19.239588975906372
Epoch: 1, Steps: 118 | Train Loss: 0.4427104 Vali Loss: 1.2092750 Test Loss: 0.4447222
Validation loss decreased (inf --> 1.209275).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4082859
	speed: 0.4015s/iter; left time: 2281.8567s
Epoch: 2 cost time: 18.79887294769287
Epoch: 2, Steps: 118 | Train Loss: 0.4379433 Vali Loss: 1.2181269 Test Loss: 0.4445969
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4240373
	speed: 0.3653s/iter; left time: 2032.7956s
Epoch: 3 cost time: 15.175068378448486
Epoch: 3, Steps: 118 | Train Loss: 0.4370404 Vali Loss: 1.2191325 Test Loss: 0.4455668
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3973474
	speed: 0.3082s/iter; left time: 1679.0048s
Epoch: 4 cost time: 14.297595262527466
Epoch: 4, Steps: 118 | Train Loss: 0.4364115 Vali Loss: 1.2207595 Test Loss: 0.4458075
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4438803791999817, mae:0.44304361939430237, rse:0.6342858672142029, corr:[0.25507918 0.2605801  0.26024872 0.25681955 0.25385767 0.25244567
 0.2519029  0.25180086 0.25159657 0.25144154 0.25146443 0.25158057
 0.25183195 0.252027   0.25208232 0.25206748 0.2518032  0.2512662
 0.25039783 0.24941412 0.24881727 0.24865358 0.24861236 0.24853328
 0.24841487 0.24846524 0.24872698 0.24885303 0.24873967 0.24840175
 0.2480158  0.24775738 0.24770856 0.24765696 0.24728788 0.24671657
 0.24630816 0.24628769 0.24662608 0.24703668 0.24728052 0.2471643
 0.24677913 0.24640502 0.24631351 0.24661495 0.24726169 0.24795438
 0.24822879 0.24804117 0.24728791 0.24628387 0.24521926 0.24409351
 0.2431003  0.2423918  0.24181597 0.24139875 0.24114694 0.24114256
 0.24110644 0.24090123 0.24046877 0.24020311 0.2403912  0.2410672
 0.24186192 0.24210466 0.24186035 0.24140532 0.24104181 0.24085183
 0.2407422  0.2404685  0.23998962 0.23953326 0.23911105 0.23860355
 0.23794577 0.23721096 0.23657873 0.23631519 0.23645437 0.23679937
 0.2369945  0.23684673 0.23649313 0.23623852 0.23615384 0.23608896
 0.23571512 0.23509884 0.23450042 0.2343099  0.23465562 0.23554212
 0.2366207  0.23725744 0.23714261 0.23673849 0.2364267  0.23626678
 0.23625864 0.23634335 0.23641446 0.23642871 0.23632863 0.23621227
 0.23620848 0.23618756 0.2360338  0.2357913  0.23553488 0.2354379
 0.23560186 0.23586833 0.2360312  0.23594384 0.23562571 0.23524913
 0.23496386 0.23459026 0.23411594 0.23372953 0.2335075  0.23326711
 0.23290811 0.23245361 0.23171999 0.23104917 0.23064576 0.23054934
 0.23057218 0.23042785 0.23005977 0.2296268  0.22937313 0.22941433
 0.22963308 0.22978476 0.22981516 0.2296875  0.22944681 0.22914958
 0.22862668 0.2276179  0.22622061 0.22486845 0.22422689 0.22428495
 0.22474943 0.22523478 0.22540905 0.22529404 0.22515109 0.22534473
 0.22586526 0.22644077 0.2268519  0.22707444 0.22712347 0.2273668
 0.22766802 0.22787353 0.22785497 0.22761652 0.22718716 0.22676234
 0.226575   0.2266113  0.22666456 0.22657514 0.22617471 0.22559908
 0.22513148 0.22501254 0.22503477 0.22509646 0.22494781 0.22457874
 0.2240887  0.2237228  0.22376278 0.22424279 0.22486831 0.22530961
 0.22545782 0.22552255 0.22577806 0.22612262 0.2261875  0.22573835
 0.22484575 0.22395237 0.22333807 0.22294164 0.222502   0.22176513
 0.22081658 0.2201119  0.21991287 0.220041   0.22003275 0.22000507
 0.22020054 0.22086976 0.22183011 0.22262658 0.22296596 0.2228508
 0.22258663 0.22226273 0.22190551 0.22127157 0.2203586  0.21948406
 0.21898362 0.2190669  0.21943477 0.21977413 0.21999486 0.21981338
 0.2193062  0.21866414 0.21804838 0.21752225 0.21720164 0.21725756
 0.21746323 0.21743993 0.21728435 0.21717922 0.21718822 0.21746843
 0.21782838 0.21796036 0.2178419  0.21755417 0.21742864 0.21735805
 0.21734247 0.21720983 0.21698731 0.21675462 0.21659158 0.21646728
 0.21632183 0.2160988  0.21567632 0.21508855 0.21460305 0.21458197
 0.21502809 0.21555577 0.2159324  0.2159788  0.21589902 0.21594201
 0.21623687 0.2163807  0.21622151 0.21565439 0.21489443 0.21421115
 0.21389493 0.21379614 0.21350397 0.21292788 0.21216795 0.21164882
 0.21148786 0.21114407 0.2101613  0.20899814 0.20823172 0.20849623
 0.20964979 0.21071689 0.21098031 0.21042933 0.2095778  0.20901483
 0.20879583 0.20868535 0.20893718 0.20957644 0.21052241 0.21121901
 0.21126765 0.21094786 0.21058561 0.21081308 0.21130759 0.21168515
 0.21148798 0.21076898 0.20988582 0.20923352 0.20872594 0.20832396
 0.20815185 0.2082523  0.20879018 0.2091626  0.2092349  0.20914057
 0.20950565 0.21005079 0.2105392  0.21007062 0.20858908 0.2068857
 0.20620264 0.20682526 0.20750162 0.20722745 0.20640226 0.20563127
 0.20531744 0.2046872  0.203111   0.2010195  0.1994775  0.19952852
 0.20062786 0.20077437 0.1994636  0.19846912 0.19875677 0.20029041
 0.20067564 0.1983112  0.19547078 0.19562332 0.19916245 0.1974595 ]
