Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  137682944.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.757544755935669
Epoch: 1, Steps: 28 | Train Loss: 0.9696210 Vali Loss: 2.0275583 Test Loss: 0.8383882
Validation loss decreased (inf --> 2.027558).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.286355018615723
Epoch: 2, Steps: 28 | Train Loss: 0.7966136 Vali Loss: 1.8143675 Test Loss: 0.7055322
Validation loss decreased (2.027558 --> 1.814368).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.022139549255371
Epoch: 3, Steps: 28 | Train Loss: 0.7298174 Vali Loss: 1.7333174 Test Loss: 0.6529925
Validation loss decreased (1.814368 --> 1.733317).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.09602689743042
Epoch: 4, Steps: 28 | Train Loss: 0.6986457 Vali Loss: 1.6876973 Test Loss: 0.6230981
Validation loss decreased (1.733317 --> 1.687697).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.324552536010742
Epoch: 5, Steps: 28 | Train Loss: 0.6786857 Vali Loss: 1.6564209 Test Loss: 0.6010135
Validation loss decreased (1.687697 --> 1.656421).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.050052642822266
Epoch: 6, Steps: 28 | Train Loss: 0.6634210 Vali Loss: 1.6310933 Test Loss: 0.5828010
Validation loss decreased (1.656421 --> 1.631093).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.416112899780273
Epoch: 7, Steps: 28 | Train Loss: 0.6512606 Vali Loss: 1.6064394 Test Loss: 0.5678316
Validation loss decreased (1.631093 --> 1.606439).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.370847702026367
Epoch: 8, Steps: 28 | Train Loss: 0.6413987 Vali Loss: 1.5926003 Test Loss: 0.5547293
Validation loss decreased (1.606439 --> 1.592600).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.065512657165527
Epoch: 9, Steps: 28 | Train Loss: 0.6329869 Vali Loss: 1.5710539 Test Loss: 0.5433685
Validation loss decreased (1.592600 --> 1.571054).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.256825923919678
Epoch: 10, Steps: 28 | Train Loss: 0.6260033 Vali Loss: 1.5630295 Test Loss: 0.5335531
Validation loss decreased (1.571054 --> 1.563030).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.057345151901245
Epoch: 11, Steps: 28 | Train Loss: 0.6195425 Vali Loss: 1.5509197 Test Loss: 0.5249566
Validation loss decreased (1.563030 --> 1.550920).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.3550026416778564
Epoch: 12, Steps: 28 | Train Loss: 0.6144953 Vali Loss: 1.5404295 Test Loss: 0.5172287
Validation loss decreased (1.550920 --> 1.540429).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.280291557312012
Epoch: 13, Steps: 28 | Train Loss: 0.6093754 Vali Loss: 1.5343127 Test Loss: 0.5104902
Validation loss decreased (1.540429 --> 1.534313).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.457244157791138
Epoch: 14, Steps: 28 | Train Loss: 0.6052967 Vali Loss: 1.5256839 Test Loss: 0.5044165
Validation loss decreased (1.534313 --> 1.525684).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.438710689544678
Epoch: 15, Steps: 28 | Train Loss: 0.6020666 Vali Loss: 1.5203240 Test Loss: 0.4990140
Validation loss decreased (1.525684 --> 1.520324).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.0344038009643555
Epoch: 16, Steps: 28 | Train Loss: 0.5983512 Vali Loss: 1.5114346 Test Loss: 0.4942566
Validation loss decreased (1.520324 --> 1.511435).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.243276119232178
Epoch: 17, Steps: 28 | Train Loss: 0.5958978 Vali Loss: 1.5001428 Test Loss: 0.4899123
Validation loss decreased (1.511435 --> 1.500143).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.139211416244507
Epoch: 18, Steps: 28 | Train Loss: 0.5931605 Vali Loss: 1.5013385 Test Loss: 0.4860845
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.3659467697143555
Epoch: 19, Steps: 28 | Train Loss: 0.5907523 Vali Loss: 1.4906912 Test Loss: 0.4826275
Validation loss decreased (1.500143 --> 1.490691).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.349622011184692
Epoch: 20, Steps: 28 | Train Loss: 0.5883379 Vali Loss: 1.4932302 Test Loss: 0.4794156
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.159861326217651
Epoch: 21, Steps: 28 | Train Loss: 0.5865585 Vali Loss: 1.4856033 Test Loss: 0.4766474
Validation loss decreased (1.490691 --> 1.485603).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.4506676197052
Epoch: 22, Steps: 28 | Train Loss: 0.5849616 Vali Loss: 1.4844871 Test Loss: 0.4740053
Validation loss decreased (1.485603 --> 1.484487).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 5.2086944580078125
Epoch: 23, Steps: 28 | Train Loss: 0.5828284 Vali Loss: 1.4821968 Test Loss: 0.4718114
Validation loss decreased (1.484487 --> 1.482197).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 5.335356712341309
Epoch: 24, Steps: 28 | Train Loss: 0.5815516 Vali Loss: 1.4749620 Test Loss: 0.4696293
Validation loss decreased (1.482197 --> 1.474962).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 5.33352255821228
Epoch: 25, Steps: 28 | Train Loss: 0.5803982 Vali Loss: 1.4735415 Test Loss: 0.4677513
Validation loss decreased (1.474962 --> 1.473541).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 5.520031929016113
Epoch: 26, Steps: 28 | Train Loss: 0.5793190 Vali Loss: 1.4775825 Test Loss: 0.4659649
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 5.377553462982178
Epoch: 27, Steps: 28 | Train Loss: 0.5778015 Vali Loss: 1.4674881 Test Loss: 0.4643885
Validation loss decreased (1.473541 --> 1.467488).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 5.45121431350708
Epoch: 28, Steps: 28 | Train Loss: 0.5773957 Vali Loss: 1.4710519 Test Loss: 0.4629928
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 5.575406789779663
Epoch: 29, Steps: 28 | Train Loss: 0.5761165 Vali Loss: 1.4684274 Test Loss: 0.4616446
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 5.422987461090088
Epoch: 30, Steps: 28 | Train Loss: 0.5754327 Vali Loss: 1.4629118 Test Loss: 0.4603625
Validation loss decreased (1.467488 --> 1.462912).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 5.250190496444702
Epoch: 31, Steps: 28 | Train Loss: 0.5744854 Vali Loss: 1.4646236 Test Loss: 0.4592654
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 5.221951007843018
Epoch: 32, Steps: 28 | Train Loss: 0.5735421 Vali Loss: 1.4657389 Test Loss: 0.4582740
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 5.4238197803497314
Epoch: 33, Steps: 28 | Train Loss: 0.5727139 Vali Loss: 1.4627295 Test Loss: 0.4573018
Validation loss decreased (1.462912 --> 1.462729).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 5.414562463760376
Epoch: 34, Steps: 28 | Train Loss: 0.5728367 Vali Loss: 1.4592004 Test Loss: 0.4564293
Validation loss decreased (1.462729 --> 1.459200).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 5.389801502227783
Epoch: 35, Steps: 28 | Train Loss: 0.5719561 Vali Loss: 1.4562819 Test Loss: 0.4556024
Validation loss decreased (1.459200 --> 1.456282).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 5.291297674179077
Epoch: 36, Steps: 28 | Train Loss: 0.5714104 Vali Loss: 1.4569060 Test Loss: 0.4548618
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 5.374807357788086
Epoch: 37, Steps: 28 | Train Loss: 0.5709227 Vali Loss: 1.4551401 Test Loss: 0.4541924
Validation loss decreased (1.456282 --> 1.455140).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 5.255613327026367
Epoch: 38, Steps: 28 | Train Loss: 0.5700633 Vali Loss: 1.4594018 Test Loss: 0.4535272
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 5.282679557800293
Epoch: 39, Steps: 28 | Train Loss: 0.5696550 Vali Loss: 1.4537315 Test Loss: 0.4529403
Validation loss decreased (1.455140 --> 1.453732).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 5.453413963317871
Epoch: 40, Steps: 28 | Train Loss: 0.5691495 Vali Loss: 1.4593446 Test Loss: 0.4524263
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 5.324512004852295
Epoch: 41, Steps: 28 | Train Loss: 0.5691207 Vali Loss: 1.4581156 Test Loss: 0.4518700
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 5.752898693084717
Epoch: 42, Steps: 28 | Train Loss: 0.5681715 Vali Loss: 1.4492121 Test Loss: 0.4513830
Validation loss decreased (1.453732 --> 1.449212).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 5.292820453643799
Epoch: 43, Steps: 28 | Train Loss: 0.5684696 Vali Loss: 1.4523193 Test Loss: 0.4509741
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.991472959518433
Epoch: 44, Steps: 28 | Train Loss: 0.5675473 Vali Loss: 1.4526442 Test Loss: 0.4505509
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 5.137193918228149
Epoch: 45, Steps: 28 | Train Loss: 0.5675160 Vali Loss: 1.4497559 Test Loss: 0.4501972
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4422076344490051, mae:0.46816349029541016, rse:0.636597752571106, corr:[0.2153961  0.23239854 0.2264852  0.232455   0.2344761  0.23084222
 0.23082696 0.23344266 0.2339348  0.23265855 0.2318572  0.23177245
 0.23156627 0.23079684 0.2292282  0.22791962 0.22778133 0.22751626
 0.22625528 0.2255248  0.22630723 0.22649533 0.22572401 0.22548534
 0.22664526 0.22746195 0.22744119 0.22786757 0.22894323 0.22928669
 0.22860086 0.22816907 0.22852553 0.2286822  0.22804396 0.22716899
 0.22690067 0.22705524 0.2268461  0.22612405 0.22556478 0.22579269
 0.22628582 0.22620519 0.22602847 0.22683166 0.22787353 0.22787036
 0.22768174 0.22786032 0.22800054 0.22728288 0.22598097 0.22527578
 0.22515102 0.22438848 0.22355853 0.22306894 0.22320563 0.22319473
 0.2226034  0.22194535 0.22182874 0.22188257 0.221763   0.2217197
 0.22196057 0.22225046 0.2223617  0.22216892 0.22190626 0.22182554
 0.22138125 0.22062276 0.22031361 0.220555   0.22052602 0.21992445
 0.21942256 0.2195122  0.21969198 0.219192   0.21837777 0.21802686
 0.21798508 0.21752916 0.21650036 0.21579997 0.21581224 0.21595418
 0.2157133  0.21564484 0.21595125 0.21638325 0.2162494  0.2166463
 0.21799102 0.21930423 0.22007038 0.2205704  0.22096159 0.22122537
 0.22119197 0.22086608 0.22060166 0.22054678 0.22042917 0.21991125
 0.21915507 0.21896303 0.21923523 0.21919046 0.21879506 0.21867177
 0.21901721 0.21915181 0.21888815 0.21873218 0.21895532 0.21926838
 0.21913783 0.21859609 0.2182942  0.21834546 0.21788345 0.21709628
 0.21684067 0.21710032 0.21712476 0.21655792 0.21608798 0.21605685
 0.21604472 0.21549866 0.21490966 0.21488185 0.21501909 0.21491864
 0.21463335 0.21460555 0.2148245  0.21479936 0.2140631  0.21365267
 0.21381313 0.21392022 0.21369235 0.21323328 0.2128377  0.212668
 0.21252163 0.21225183 0.21218331 0.2123411  0.21221982 0.21165311
 0.21110398 0.21117125 0.21130508 0.2110942  0.21084596 0.21115477
 0.21158344 0.21153532 0.21120834 0.21129978 0.210991   0.2106816
 0.21062428 0.21114108 0.21201849 0.2127926  0.21287924 0.21263172
 0.21263647 0.21280488 0.21253957 0.2119732  0.21173157 0.21189098
 0.21181142 0.21141137 0.2113732  0.21179426 0.21212164 0.2121126
 0.21222734 0.21261586 0.21285869 0.212621   0.21240312 0.21249345
 0.21242541 0.21196298 0.21152376 0.21125005 0.21097355 0.21071728
 0.21059683 0.21055572 0.21053402 0.21040054 0.21017578 0.20996381
 0.21021204 0.21083638 0.21112934 0.21093874 0.21092762 0.21109378
 0.21084735 0.21012089 0.20962724 0.20946842 0.20900446 0.20853338
 0.2086009  0.20900099 0.20925991 0.20937562 0.20939218 0.20941684
 0.20960379 0.20962158 0.20897841 0.20812607 0.20787723 0.20811684
 0.20785491 0.20728284 0.20716126 0.20729564 0.2070293  0.2065536
 0.20648114 0.20657748 0.20624578 0.20567018 0.20542438 0.20565477
 0.2061276  0.20645282 0.20679368 0.20705232 0.20714298 0.20717172
 0.2071072  0.20682155 0.20656644 0.20635563 0.20591418 0.20538113
 0.20534115 0.20570043 0.20579235 0.20543194 0.20544635 0.20599307
 0.20629673 0.20598239 0.20575221 0.20580982 0.20577878 0.20566456
 0.20563917 0.20563617 0.20562251 0.20566034 0.20549074 0.20526698
 0.20520261 0.2050081  0.20452791 0.20419723 0.2042818  0.2045392
 0.20442945 0.20438552 0.20455363 0.20470117 0.20438175 0.20406663
 0.20398386 0.20408532 0.20372912 0.2031625  0.20282109 0.20312719
 0.20369524 0.20431158 0.20498998 0.20551202 0.2058024  0.20586984
 0.20574267 0.20575224 0.20598668 0.20611688 0.20589197 0.20581731
 0.20609893 0.2062335  0.20593333 0.20579624 0.20619996 0.20657623
 0.2065385  0.20627594 0.20624796 0.20638305 0.20643067 0.20665826
 0.20695025 0.20696855 0.20714633 0.20745675 0.20708235 0.2062915
 0.2059113  0.20561862 0.20485362 0.20420575 0.20441999 0.20476995
 0.20444077 0.20412159 0.20461364 0.20513779 0.20489244 0.20467398
 0.205072   0.20530301 0.20496881 0.2048112  0.20497924 0.20509212
 0.2048757  0.20461461 0.20466277 0.2047011  0.20432387 0.20372467
 0.20313308 0.20260572 0.20227557 0.20223379 0.20219634 0.2021678
 0.20226605 0.20234704 0.2021579  0.20216827 0.20262563 0.20316608
 0.20313385 0.20286377 0.20285171 0.20300281 0.20262142 0.20225528
 0.20242794 0.20272312 0.20287289 0.20269434 0.20199527 0.20130822
 0.20108993 0.20076051 0.19997226 0.19943483 0.19939813 0.19913232
 0.19826202 0.19765447 0.19760366 0.19732618 0.19679631 0.19670074
 0.19698071 0.19671482 0.19608381 0.19614471 0.19654855 0.1971005
 0.19799747 0.19925548 0.20032758 0.20085713 0.20085658 0.20020536
 0.19921748 0.1982611  0.19758435 0.19687887 0.19601183 0.1957784
 0.19611771 0.19625261 0.19606552 0.19608808 0.1965512  0.19677334
 0.19659902 0.19658186 0.19692598 0.19709066 0.19685087 0.19703074
 0.19786665 0.1986625  0.19927272 0.19963929 0.19961175 0.19926877
 0.19916834 0.19888593 0.19842441 0.198267   0.19835623 0.1981168
 0.19752209 0.19738571 0.19753361 0.19741276 0.196949   0.19676328
 0.19694595 0.19700779 0.19651102 0.19653638 0.19700588 0.1975679
 0.19813442 0.19858618 0.19898306 0.19948055 0.19973262 0.19946043
 0.19914125 0.19903387 0.19880998 0.19850439 0.19832651 0.19857481
 0.19864652 0.19816248 0.19775036 0.19769147 0.19762537 0.1972271
 0.19697765 0.1970741  0.19716112 0.19695571 0.19694716 0.19769613
 0.19862367 0.19923633 0.19966787 0.19964144 0.19905576 0.19847025
 0.19828613 0.19784825 0.19724242 0.19734079 0.19764376 0.1976039
 0.19752504 0.19805628 0.1985739  0.1983161  0.19788207 0.19817919
 0.19845064 0.19799228 0.19736877 0.19713227 0.19721265 0.19720851
 0.19742635 0.19772905 0.19769345 0.19759637 0.19759822 0.19745609
 0.197226   0.19713704 0.19687234 0.19646204 0.1962145  0.19641653
 0.19645625 0.19590183 0.19553922 0.19574325 0.19582449 0.19537492
 0.19494888 0.19488649 0.19481872 0.1946521  0.1947327  0.19544494
 0.19635554 0.19706644 0.197637   0.19819547 0.19847238 0.19815949
 0.19758205 0.19702394 0.19658111 0.19637363 0.19630122 0.19623052
 0.19629794 0.19671918 0.19737826 0.19763194 0.19792001 0.19837162
 0.19855759 0.19824018 0.19829197 0.19851945 0.19916184 0.19919369
 0.19940491 0.1996574  0.19973482 0.19989106 0.19962712 0.19921066
 0.19894575 0.19891119 0.19860637 0.19834901 0.19838317 0.19865322
 0.19850902 0.19823438 0.19889171 0.19982621 0.20012246 0.19979602
 0.19979568 0.20000787 0.19968738 0.19908543 0.19873899 0.19844061
 0.19779713 0.19716766 0.19682328 0.19665737 0.19641681 0.19596548
 0.19535877 0.19490798 0.19465576 0.19428055 0.19347705 0.19290163
 0.19291432 0.19285646 0.19257732 0.19201137 0.1921943  0.19243404
 0.19223064 0.19156729 0.1913468  0.19144906 0.19131076 0.19071269
 0.1901849  0.19018652 0.19040579 0.19093359 0.19093814 0.19042201
 0.19054155 0.19046293 0.18985121 0.18930918 0.18974532 0.19001688
 0.18950476 0.18889096 0.18933031 0.1897897  0.18950729 0.18930654
 0.18979253 0.1900456  0.1895178  0.18910287 0.18910486 0.18873984
 0.18798487 0.18729885 0.18704294 0.18700044 0.18669659 0.1857101
 0.18457781 0.18397532 0.18375383 0.18351611 0.182918   0.18264028
 0.18276373 0.18262047 0.18225683 0.18223132 0.18273225 0.18298285
 0.18277265 0.18257035 0.18282951 0.18325841 0.18340419 0.183113
 0.18283494 0.18278742 0.18258071 0.18207607 0.18142308 0.18115745
 0.18143424 0.1811125  0.18020229 0.17988367 0.1800642  0.18025343
 0.18017022 0.18010429 0.18041737 0.18039179 0.17981946 0.17942832
 0.17944948 0.17942248 0.1789877  0.17867205 0.17912579 0.17981751
 0.17970537 0.1789645  0.1785921  0.17919269 0.1790831  0.1777732
 0.17670362 0.17652287 0.17630982 0.17543277 0.17479776 0.17501569
 0.17522393 0.17479272 0.17452928 0.17468329 0.1749799  0.17515124
 0.17504175 0.1747591  0.1744515  0.17435458 0.17390274 0.17312221
 0.17208491 0.17096819 0.16945001 0.16794677 0.16746405 0.1666343
 0.16517694 0.16390108 0.16430555 0.16462384 0.16396113 0.16368994
 0.16423424 0.16428667 0.16400449 0.16386892 0.16386494 0.16377804
 0.16385965 0.16378495 0.16294017 0.16228049 0.1628269  0.16258849
 0.16020454 0.15924583 0.16049303 0.16095442 0.15836534 0.15671709
 0.15826187 0.15802307 0.15555099 0.1550232  0.15602188 0.15470468
 0.1531961  0.15269753 0.15127642 0.14954725 0.14997396 0.1483783
 0.1435019  0.14410894 0.14430642 0.1271301  0.1265176  0.14968285]
