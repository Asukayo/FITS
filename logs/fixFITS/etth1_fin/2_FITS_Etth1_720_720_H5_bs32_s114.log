Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  24393600.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6604012
	speed: 0.1444s/iter; left time: 794.5507s
Epoch: 1 cost time: 16.179049968719482
Epoch: 1, Steps: 112 | Train Loss: 0.7672445 Vali Loss: 1.9554074 Test Loss: 0.8059995
Validation loss decreased (inf --> 1.955407).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5398008
	speed: 0.3391s/iter; left time: 1827.4493s
Epoch: 2 cost time: 17.520840644836426
Epoch: 2, Steps: 112 | Train Loss: 0.5703225 Vali Loss: 1.8106823 Test Loss: 0.7250090
Validation loss decreased (1.955407 --> 1.810682).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4788010
	speed: 0.3278s/iter; left time: 1729.7069s
Epoch: 3 cost time: 15.517539501190186
Epoch: 3, Steps: 112 | Train Loss: 0.5071268 Vali Loss: 1.7573570 Test Loss: 0.6898555
Validation loss decreased (1.810682 --> 1.757357).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4454673
	speed: 0.3348s/iter; left time: 1729.4461s
Epoch: 4 cost time: 16.240723848342896
Epoch: 4, Steps: 112 | Train Loss: 0.4692443 Vali Loss: 1.7164849 Test Loss: 0.6608467
Validation loss decreased (1.757357 --> 1.716485).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4155982
	speed: 0.3474s/iter; left time: 1755.4948s
Epoch: 5 cost time: 17.137230157852173
Epoch: 5, Steps: 112 | Train Loss: 0.4412512 Vali Loss: 1.6853970 Test Loss: 0.6372470
Validation loss decreased (1.716485 --> 1.685397).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4143376
	speed: 0.3461s/iter; left time: 1710.2260s
Epoch: 6 cost time: 16.375006437301636
Epoch: 6, Steps: 112 | Train Loss: 0.4191968 Vali Loss: 1.6586604 Test Loss: 0.6152536
Validation loss decreased (1.685397 --> 1.658660).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4108978
	speed: 0.3305s/iter; left time: 1595.9530s
Epoch: 7 cost time: 17.172022342681885
Epoch: 7, Steps: 112 | Train Loss: 0.4015180 Vali Loss: 1.6293764 Test Loss: 0.5977808
Validation loss decreased (1.658660 --> 1.629376).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3627089
	speed: 0.3446s/iter; left time: 1625.3951s
Epoch: 8 cost time: 16.223571062088013
Epoch: 8, Steps: 112 | Train Loss: 0.3870890 Vali Loss: 1.6093280 Test Loss: 0.5796826
Validation loss decreased (1.629376 --> 1.609328).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3707461
	speed: 0.3298s/iter; left time: 1518.6310s
Epoch: 9 cost time: 16.70036506652832
Epoch: 9, Steps: 112 | Train Loss: 0.3750089 Vali Loss: 1.5911930 Test Loss: 0.5651922
Validation loss decreased (1.609328 --> 1.591193).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3685640
	speed: 0.3552s/iter; left time: 1595.9029s
Epoch: 10 cost time: 16.922756671905518
Epoch: 10, Steps: 112 | Train Loss: 0.3648967 Vali Loss: 1.5771140 Test Loss: 0.5521474
Validation loss decreased (1.591193 --> 1.577114).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3539751
	speed: 0.2850s/iter; left time: 1248.7403s
Epoch: 11 cost time: 13.772785425186157
Epoch: 11, Steps: 112 | Train Loss: 0.3566574 Vali Loss: 1.5634350 Test Loss: 0.5403591
Validation loss decreased (1.577114 --> 1.563435).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3166771
	speed: 0.3128s/iter; left time: 1335.5443s
Epoch: 12 cost time: 17.215333700180054
Epoch: 12, Steps: 112 | Train Loss: 0.3495838 Vali Loss: 1.5484589 Test Loss: 0.5309719
Validation loss decreased (1.563435 --> 1.548459).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3322992
	speed: 0.3441s/iter; left time: 1430.3520s
Epoch: 13 cost time: 16.84897756576538
Epoch: 13, Steps: 112 | Train Loss: 0.3433613 Vali Loss: 1.5416878 Test Loss: 0.5223678
Validation loss decreased (1.548459 --> 1.541688).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3292553
	speed: 0.3298s/iter; left time: 1333.9258s
Epoch: 14 cost time: 16.56032633781433
Epoch: 14, Steps: 112 | Train Loss: 0.3378854 Vali Loss: 1.5325058 Test Loss: 0.5139709
Validation loss decreased (1.541688 --> 1.532506).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3095664
	speed: 0.3362s/iter; left time: 1322.1920s
Epoch: 15 cost time: 16.839019060134888
Epoch: 15, Steps: 112 | Train Loss: 0.3333260 Vali Loss: 1.5234265 Test Loss: 0.5068805
Validation loss decreased (1.532506 --> 1.523427).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3217793
	speed: 0.3346s/iter; left time: 1278.5747s
Epoch: 16 cost time: 16.524263381958008
Epoch: 16, Steps: 112 | Train Loss: 0.3291642 Vali Loss: 1.5128448 Test Loss: 0.5000523
Validation loss decreased (1.523427 --> 1.512845).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3359621
	speed: 0.3352s/iter; left time: 1243.1710s
Epoch: 17 cost time: 17.346801280975342
Epoch: 17, Steps: 112 | Train Loss: 0.3256172 Vali Loss: 1.5095495 Test Loss: 0.4942547
Validation loss decreased (1.512845 --> 1.509549).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3207717
	speed: 0.3401s/iter; left time: 1223.4601s
Epoch: 18 cost time: 17.013715028762817
Epoch: 18, Steps: 112 | Train Loss: 0.3224291 Vali Loss: 1.5030380 Test Loss: 0.4893534
Validation loss decreased (1.509549 --> 1.503038).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3365795
	speed: 0.3419s/iter; left time: 1191.6242s
Epoch: 19 cost time: 16.822274923324585
Epoch: 19, Steps: 112 | Train Loss: 0.3194518 Vali Loss: 1.4989787 Test Loss: 0.4848665
Validation loss decreased (1.503038 --> 1.498979).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3076236
	speed: 0.3425s/iter; left time: 1155.1138s
Epoch: 20 cost time: 17.043534755706787
Epoch: 20, Steps: 112 | Train Loss: 0.3171970 Vali Loss: 1.4915155 Test Loss: 0.4805889
Validation loss decreased (1.498979 --> 1.491516).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2800446
	speed: 0.3366s/iter; left time: 1097.6330s
Epoch: 21 cost time: 16.35212016105652
Epoch: 21, Steps: 112 | Train Loss: 0.3146172 Vali Loss: 1.4859264 Test Loss: 0.4769327
Validation loss decreased (1.491516 --> 1.485926).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3192311
	speed: 0.3313s/iter; left time: 1043.1441s
Epoch: 22 cost time: 17.036141872406006
Epoch: 22, Steps: 112 | Train Loss: 0.3129276 Vali Loss: 1.4861289 Test Loss: 0.4736436
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3143673
	speed: 0.3416s/iter; left time: 1037.4559s
Epoch: 23 cost time: 16.527406930923462
Epoch: 23, Steps: 112 | Train Loss: 0.3109442 Vali Loss: 1.4785126 Test Loss: 0.4705489
Validation loss decreased (1.485926 --> 1.478513).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3018720
	speed: 0.3181s/iter; left time: 930.3871s
Epoch: 24 cost time: 16.33467173576355
Epoch: 24, Steps: 112 | Train Loss: 0.3094154 Vali Loss: 1.4786794 Test Loss: 0.4679587
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3071859
	speed: 0.3338s/iter; left time: 939.0762s
Epoch: 25 cost time: 16.64491319656372
Epoch: 25, Steps: 112 | Train Loss: 0.3077772 Vali Loss: 1.4772300 Test Loss: 0.4653561
Validation loss decreased (1.478513 --> 1.477230).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3170152
	speed: 0.3306s/iter; left time: 893.0398s
Epoch: 26 cost time: 16.165003538131714
Epoch: 26, Steps: 112 | Train Loss: 0.3064065 Vali Loss: 1.4716841 Test Loss: 0.4633271
Validation loss decreased (1.477230 --> 1.471684).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2852654
	speed: 0.3280s/iter; left time: 849.3206s
Epoch: 27 cost time: 16.699095964431763
Epoch: 27, Steps: 112 | Train Loss: 0.3053139 Vali Loss: 1.4716914 Test Loss: 0.4612296
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3091983
	speed: 0.3275s/iter; left time: 811.2303s
Epoch: 28 cost time: 16.44131851196289
Epoch: 28, Steps: 112 | Train Loss: 0.3042169 Vali Loss: 1.4715958 Test Loss: 0.4593596
Validation loss decreased (1.471684 --> 1.471596).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3117599
	speed: 0.3256s/iter; left time: 770.0791s
Epoch: 29 cost time: 16.15052318572998
Epoch: 29, Steps: 112 | Train Loss: 0.3032102 Vali Loss: 1.4694729 Test Loss: 0.4576802
Validation loss decreased (1.471596 --> 1.469473).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3078207
	speed: 0.3334s/iter; left time: 751.2486s
Epoch: 30 cost time: 16.871376752853394
Epoch: 30, Steps: 112 | Train Loss: 0.3022224 Vali Loss: 1.4646218 Test Loss: 0.4562385
Validation loss decreased (1.469473 --> 1.464622).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2911720
	speed: 0.3341s/iter; left time: 715.3380s
Epoch: 31 cost time: 16.531868934631348
Epoch: 31, Steps: 112 | Train Loss: 0.3013137 Vali Loss: 1.4669042 Test Loss: 0.4548280
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2959212
	speed: 0.3224s/iter; left time: 654.0968s
Epoch: 32 cost time: 16.279144048690796
Epoch: 32, Steps: 112 | Train Loss: 0.3005299 Vali Loss: 1.4638860 Test Loss: 0.4533731
Validation loss decreased (1.464622 --> 1.463886).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2808208
	speed: 0.3375s/iter; left time: 646.9642s
Epoch: 33 cost time: 16.80032753944397
Epoch: 33, Steps: 112 | Train Loss: 0.2997708 Vali Loss: 1.4632816 Test Loss: 0.4522778
Validation loss decreased (1.463886 --> 1.463282).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3000521
	speed: 0.3265s/iter; left time: 589.3969s
Epoch: 34 cost time: 15.865570545196533
Epoch: 34, Steps: 112 | Train Loss: 0.2990739 Vali Loss: 1.4619048 Test Loss: 0.4512384
Validation loss decreased (1.463282 --> 1.461905).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3110782
	speed: 0.3276s/iter; left time: 554.5789s
Epoch: 35 cost time: 16.7927565574646
Epoch: 35, Steps: 112 | Train Loss: 0.2985936 Vali Loss: 1.4626890 Test Loss: 0.4502517
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3001403
	speed: 0.3373s/iter; left time: 533.1961s
Epoch: 36 cost time: 17.169424057006836
Epoch: 36, Steps: 112 | Train Loss: 0.2976221 Vali Loss: 1.4597481 Test Loss: 0.4492913
Validation loss decreased (1.461905 --> 1.459748).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2960494
	speed: 0.3279s/iter; left time: 481.7278s
Epoch: 37 cost time: 16.397135019302368
Epoch: 37, Steps: 112 | Train Loss: 0.2974971 Vali Loss: 1.4608078 Test Loss: 0.4484576
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3295766
	speed: 0.3323s/iter; left time: 450.8853s
Epoch: 38 cost time: 17.053424835205078
Epoch: 38, Steps: 112 | Train Loss: 0.2969394 Vali Loss: 1.4614971 Test Loss: 0.4477017
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2776508
	speed: 0.3292s/iter; left time: 409.8905s
Epoch: 39 cost time: 16.61997151374817
Epoch: 39, Steps: 112 | Train Loss: 0.2964895 Vali Loss: 1.4572226 Test Loss: 0.4470049
Validation loss decreased (1.459748 --> 1.457223).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3080430
	speed: 0.3196s/iter; left time: 362.0547s
Epoch: 40 cost time: 15.957303524017334
Epoch: 40, Steps: 112 | Train Loss: 0.2959307 Vali Loss: 1.4585787 Test Loss: 0.4463855
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2983304
	speed: 0.3362s/iter; left time: 343.2807s
Epoch: 41 cost time: 16.834097862243652
Epoch: 41, Steps: 112 | Train Loss: 0.2956463 Vali Loss: 1.4573480 Test Loss: 0.4457696
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2980843
	speed: 0.3316s/iter; left time: 301.4121s
Epoch: 42 cost time: 16.55426049232483
Epoch: 42, Steps: 112 | Train Loss: 0.2952748 Vali Loss: 1.4560362 Test Loss: 0.4451660
Validation loss decreased (1.457223 --> 1.456036).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2916494
	speed: 0.3356s/iter; left time: 267.4610s
Epoch: 43 cost time: 16.860467433929443
Epoch: 43, Steps: 112 | Train Loss: 0.2948351 Vali Loss: 1.4559013 Test Loss: 0.4447115
Validation loss decreased (1.456036 --> 1.455901).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2920640
	speed: 0.3195s/iter; left time: 218.8849s
Epoch: 44 cost time: 17.02786636352539
Epoch: 44, Steps: 112 | Train Loss: 0.2946224 Vali Loss: 1.4540486 Test Loss: 0.4442238
Validation loss decreased (1.455901 --> 1.454049).  Saving model ...
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2949969
	speed: 0.3211s/iter; left time: 183.9678s
Epoch: 45 cost time: 15.858958959579468
Epoch: 45, Steps: 112 | Train Loss: 0.2944188 Vali Loss: 1.4546107 Test Loss: 0.4437878
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2936983
	speed: 0.3080s/iter; left time: 141.9981s
Epoch: 46 cost time: 15.259932041168213
Epoch: 46, Steps: 112 | Train Loss: 0.2940572 Vali Loss: 1.4575524 Test Loss: 0.4433460
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2952341
	speed: 0.3259s/iter; left time: 113.7334s
Epoch: 47 cost time: 16.30580425262451
Epoch: 47, Steps: 112 | Train Loss: 0.2940756 Vali Loss: 1.4517128 Test Loss: 0.4429917
Validation loss decreased (1.454049 --> 1.451713).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2855377
	speed: 0.3121s/iter; left time: 73.9628s
Epoch: 48 cost time: 15.203975915908813
Epoch: 48, Steps: 112 | Train Loss: 0.2936411 Vali Loss: 1.4522991 Test Loss: 0.4425672
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2998664
	speed: 0.3315s/iter; left time: 41.4428s
Epoch: 49 cost time: 16.51949167251587
Epoch: 49, Steps: 112 | Train Loss: 0.2933919 Vali Loss: 1.4528180 Test Loss: 0.4422547
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2859622
	speed: 0.3251s/iter; left time: 4.2267s
Epoch: 50 cost time: 16.200716495513916
Epoch: 50, Steps: 112 | Train Loss: 0.2931474 Vali Loss: 1.4495888 Test Loss: 0.4419681
Validation loss decreased (1.451713 --> 1.449589).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  24393600.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5554630
	speed: 0.1513s/iter; left time: 832.1299s
Epoch: 1 cost time: 16.94055414199829
Epoch: 1, Steps: 112 | Train Loss: 0.5536157 Vali Loss: 1.4387910 Test Loss: 0.4357480
Validation loss decreased (inf --> 1.438791).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5549597
	speed: 0.3388s/iter; left time: 1825.7789s
Epoch: 2 cost time: 16.728034019470215
Epoch: 2, Steps: 112 | Train Loss: 0.5506112 Vali Loss: 1.4394318 Test Loss: 0.4365008
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5428296
	speed: 0.3168s/iter; left time: 1671.8309s
Epoch: 3 cost time: 15.906719446182251
Epoch: 3, Steps: 112 | Train Loss: 0.5503371 Vali Loss: 1.4416038 Test Loss: 0.4365035
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5190288
	speed: 0.3326s/iter; left time: 1717.7831s
Epoch: 4 cost time: 16.95736050605774
Epoch: 4, Steps: 112 | Train Loss: 0.5496117 Vali Loss: 1.4470546 Test Loss: 0.4367500
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43454617261886597, mae:0.45795267820358276, rse:0.631058931350708, corr:[0.23134886 0.23786426 0.23788685 0.23529653 0.23389666 0.23319332
 0.23289394 0.23269834 0.23272225 0.23322575 0.23350002 0.23321258
 0.23281038 0.23256949 0.23246172 0.23216243 0.23169574 0.2310753
 0.23021577 0.22937791 0.22897404 0.22901297 0.22899744 0.22868717
 0.22849421 0.22883801 0.2293899  0.22958712 0.22958848 0.22964752
 0.22978841 0.22973682 0.22947389 0.22917873 0.22921978 0.22969697
 0.23027027 0.23032832 0.22981201 0.22908245 0.22861987 0.22841729
 0.22845061 0.22860898 0.2287663  0.22896326 0.22948699 0.2301067
 0.2301695  0.22948323 0.22848858 0.22770093 0.22706635 0.22616653
 0.22527917 0.22476657 0.22500822 0.22535719 0.2250359  0.2242058
 0.22335675 0.22290048 0.22294539 0.22346528 0.22395837 0.22404072
 0.22360265 0.22294739 0.22268125 0.22295406 0.22331882 0.22345066
 0.22319634 0.22282277 0.22235097 0.22182493 0.2213112  0.22098291
 0.22088365 0.22063509 0.21994491 0.21927108 0.21912272 0.21952868
 0.21994458 0.21969853 0.21881407 0.21781889 0.21705744 0.21669295
 0.21663706 0.21699384 0.21745792 0.21786132 0.21825443 0.21900003
 0.22015071 0.22131139 0.22206363 0.22252563 0.22261122 0.22226627
 0.22163564 0.22115007 0.22107641 0.22133571 0.22158125 0.2215483
 0.22129701 0.22094005 0.2206157  0.22048885 0.22043443 0.2201972
 0.21993251 0.21977031 0.21987575 0.22010095 0.22016263 0.2200987
 0.2200226  0.2197892  0.21925151 0.21858719 0.21788333 0.21748905
 0.2178216  0.21853916 0.21869135 0.21820498 0.217618   0.21728565
 0.21710478 0.21675552 0.21630399 0.21595064 0.21568945 0.21545644
 0.21541736 0.21567684 0.21596697 0.21576704 0.21506809 0.21465762
 0.21483308 0.21492879 0.21431379 0.21323001 0.2127337  0.21311733
 0.21367659 0.21369821 0.21302266 0.21230166 0.21205463 0.21229121
 0.21261464 0.21267012 0.2125112  0.2124682  0.21251735 0.21255085
 0.21235424 0.21213613 0.2121235  0.21234195 0.21223886 0.2119835
 0.21192661 0.21229862 0.21291019 0.2134858  0.2134971  0.21295679
 0.21249038 0.21255474 0.21285166 0.21300039 0.21278474 0.21230319
 0.21178497 0.21159622 0.2119816  0.21263081 0.21295518 0.2127762
 0.2125734  0.21292306 0.21370836 0.21404684 0.21363321 0.2128055
 0.21217772 0.21201183 0.21188459 0.21151215 0.21089411 0.21009944
 0.2094168  0.20916906 0.2092078  0.20922302 0.20916575 0.20933604
 0.20984165 0.21042871 0.21070382 0.21067987 0.21071996 0.21095441
 0.21104679 0.21081212 0.21049549 0.2102615  0.21019168 0.21009824
 0.2098772  0.20979095 0.20988901 0.20999654 0.20998147 0.20983255
 0.20976612 0.20969991 0.20937347 0.20870471 0.20784271 0.20710526
 0.20660418 0.20624739 0.2060612  0.20595975 0.20569842 0.2053296
 0.2051293  0.20525877 0.20554549 0.20567973 0.20582777 0.2062486
 0.2070459  0.20773996 0.20793179 0.20766158 0.20719604 0.20675734
 0.2065991  0.20668146 0.2068637  0.2069985  0.20707291 0.20712799
 0.20712173 0.20693943 0.20675267 0.20666315 0.20674282 0.20673095
 0.20632747 0.20566207 0.20553991 0.20605142 0.20650686 0.20642924
 0.20609553 0.20599033 0.20613742 0.20613164 0.20567037 0.2051885
 0.20508239 0.2050554  0.20471145 0.20436181 0.20432705 0.2045305
 0.20446813 0.20406191 0.20379227 0.20402227 0.2042608  0.20393342
 0.20314118 0.20297556 0.20394093 0.20526868 0.2059558  0.2059403
 0.20556827 0.20544843 0.20592657 0.20678905 0.20742586 0.20762295
 0.20745935 0.2071397  0.20674755 0.20644543 0.20640196 0.20668313
 0.20703469 0.20702727 0.20679216 0.20670144 0.20689085 0.20715195
 0.20733371 0.20750803 0.2079836  0.2085136  0.20839721 0.2076434
 0.2069884  0.20709798 0.20754175 0.20742996 0.20649885 0.20551403
 0.20513289 0.20490871 0.20420231 0.2033044  0.20293199 0.20314741
 0.20325546 0.20302005 0.20280303 0.2029336  0.20279649 0.20242141
 0.20249912 0.20328216 0.2039607  0.20387073 0.20334478 0.20326678
 0.20382126 0.2040249  0.20347248 0.20276794 0.2023686  0.20187677
 0.20103455 0.20039047 0.20066231 0.20161815 0.20216022 0.20191126
 0.2016632  0.20224033 0.20291242 0.20264517 0.2015488  0.20087907
 0.20077617 0.20036599 0.1995744  0.19948055 0.20023565 0.20113266
 0.20108552 0.20003939 0.19923894 0.19923909 0.19918704 0.19848318
 0.19764635 0.19727762 0.19719473 0.19703767 0.19701876 0.19741109
 0.19762817 0.19707122 0.19618352 0.19594939 0.19674402 0.19760151
 0.1975745  0.19684508 0.19649525 0.19712952 0.19792968 0.19850188
 0.19895755 0.19960713 0.20039256 0.20075063 0.20011383 0.1986835
 0.1977026  0.19784398 0.1984839  0.19864793 0.19817142 0.197995
 0.19853027 0.19903429 0.19879518 0.19798505 0.19764154 0.19840203
 0.1993492  0.19942716 0.1989541  0.19895186 0.19951823 0.20001616
 0.19991577 0.19953462 0.19965978 0.20028885 0.20064484 0.20035852
 0.2000616  0.19994558 0.19963108 0.19896612 0.19859676 0.19893281
 0.19947092 0.19955234 0.1991041  0.19880407 0.19880176 0.1987929
 0.19866239 0.19879076 0.1991569  0.19994844 0.20057428 0.20090193
 0.20095493 0.20047304 0.19959955 0.19907747 0.19920269 0.19965702
 0.20009844 0.20053063 0.20085153 0.20097303 0.20051672 0.19967899
 0.19917981 0.19931722 0.19945055 0.19875377 0.19768342 0.19745986
 0.19819798 0.1987332  0.19854718 0.19841313 0.1991132  0.20042711
 0.20099077 0.20037283 0.19953382 0.19947572 0.20002808 0.20037645
 0.20042263 0.2003871  0.20022276 0.19985189 0.1991635  0.1984956
 0.19817424 0.19829984 0.1985586  0.19856372 0.19824778 0.19818857
 0.19877027 0.1997146  0.20019656 0.19978292 0.19943681 0.19974558
 0.20033711 0.20050603 0.20018956 0.2000343  0.20006146 0.19952233
 0.19830942 0.19740309 0.19755188 0.19843493 0.19869532 0.19808444
 0.19769548 0.19810113 0.19846466 0.1977128  0.19638388 0.19589248
 0.19662449 0.19760619 0.19813068 0.1984633  0.19879289 0.19874349
 0.19812994 0.19791837 0.19882771 0.2004493  0.20145829 0.20108414
 0.20015818 0.19939695 0.1983778  0.19685459 0.19574754 0.19592577
 0.19691975 0.1978223  0.19850698 0.19899988 0.19942011 0.19974703
 0.200305   0.20140499 0.20263886 0.20258805 0.20142621 0.19989362
 0.19938435 0.19953135 0.19944662 0.19914089 0.19894293 0.19926786
 0.19956039 0.19932859 0.19862835 0.19819482 0.1982208  0.19847484
 0.19845489 0.19783014 0.196985   0.19649339 0.1969689  0.19787069
 0.19816987 0.1977477  0.19764498 0.19839975 0.19876742 0.19726135
 0.19464473 0.1936722  0.1951329  0.19664907 0.19576053 0.19343191
 0.19260404 0.19419795 0.19591928 0.19557391 0.19383487 0.19262548
 0.19226511 0.19182684 0.19135019 0.1911372  0.19140264 0.19127683
 0.19049671 0.18957898 0.18958953 0.19015904 0.19040355 0.18990512
 0.18909225 0.18863164 0.18867283 0.18966511 0.19094515 0.19114558
 0.18980464 0.1875804  0.18596765 0.18554129 0.18607105 0.1868906
 0.18800445 0.18869458 0.18822399 0.18644221 0.18480542 0.18448938
 0.18492734 0.18474314 0.18391694 0.18372913 0.18449095 0.1850105
 0.18445009 0.18332866 0.18290965 0.18335767 0.18382569 0.1837074
 0.18358882 0.18376851 0.18356672 0.18286209 0.18202867 0.18127291
 0.18045394 0.17978258 0.18003643 0.18085863 0.18067002 0.17866422
 0.17630106 0.17558344 0.1767322  0.1780878  0.17868143 0.17906143
 0.17974035 0.18016872 0.17975193 0.17947513 0.18012126 0.18105344
 0.1812697  0.18053846 0.17959169 0.17918377 0.17917573 0.17965604
 0.1806258  0.18113382 0.1799941  0.17758684 0.17612486 0.1768722
 0.17823453 0.17790681 0.17622767 0.17569815 0.17734203 0.17912927
 0.17873113 0.17701769 0.17628363 0.1772455  0.17787792 0.1768493
 0.17482385 0.1728972  0.17164369 0.17112619 0.17161795 0.17291154
 0.17396647 0.17383468 0.17300779 0.17204897 0.17115848 0.17047438
 0.1706237  0.17169237 0.17216001 0.17091689 0.16873877 0.16715461
 0.16591823 0.1642735  0.16282007 0.16294622 0.16418745 0.16401668
 0.16260287 0.16195872 0.1635077  0.16502656 0.16442989 0.16264904
 0.16179243 0.16179766 0.16131121 0.15986994 0.1593566  0.1607071
 0.1621573  0.16210689 0.16177642 0.16255909 0.16311106 0.1614412
 0.15864752 0.15814002 0.15955009 0.16075638 0.16011418 0.15912203
 0.15915217 0.15876323 0.15734902 0.156587   0.15723047 0.1568112
 0.15447563 0.15315653 0.15570606 0.1583601  0.15608817 0.15263914
 0.1551151  0.16086957 0.1590993  0.15090056 0.15311392 0.16590159]
