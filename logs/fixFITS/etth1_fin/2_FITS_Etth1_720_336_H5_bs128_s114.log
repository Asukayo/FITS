Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  71258880.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.979600191116333
Epoch: 1, Steps: 29 | Train Loss: 0.7505833 Vali Loss: 1.9294147 Test Loss: 0.9034236
Validation loss decreased (inf --> 1.929415).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.212619781494141
Epoch: 2, Steps: 29 | Train Loss: 0.6368927 Vali Loss: 1.7559782 Test Loss: 0.8166866
Validation loss decreased (1.929415 --> 1.755978).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.937366247177124
Epoch: 3, Steps: 29 | Train Loss: 0.5650912 Vali Loss: 1.6606899 Test Loss: 0.7649166
Validation loss decreased (1.755978 --> 1.660690).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.855623722076416
Epoch: 4, Steps: 29 | Train Loss: 0.5188632 Vali Loss: 1.5912628 Test Loss: 0.7350208
Validation loss decreased (1.660690 --> 1.591263).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.875932216644287
Epoch: 5, Steps: 29 | Train Loss: 0.4864559 Vali Loss: 1.5502659 Test Loss: 0.7151154
Validation loss decreased (1.591263 --> 1.550266).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.991237163543701
Epoch: 6, Steps: 29 | Train Loss: 0.4636345 Vali Loss: 1.5201902 Test Loss: 0.7028850
Validation loss decreased (1.550266 --> 1.520190).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.085644006729126
Epoch: 7, Steps: 29 | Train Loss: 0.4459092 Vali Loss: 1.5015028 Test Loss: 0.6931009
Validation loss decreased (1.520190 --> 1.501503).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.24590277671814
Epoch: 8, Steps: 29 | Train Loss: 0.4314168 Vali Loss: 1.4864901 Test Loss: 0.6876017
Validation loss decreased (1.501503 --> 1.486490).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.344561576843262
Epoch: 9, Steps: 29 | Train Loss: 0.4189631 Vali Loss: 1.4707527 Test Loss: 0.6814198
Validation loss decreased (1.486490 --> 1.470753).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.514234781265259
Epoch: 10, Steps: 29 | Train Loss: 0.4086532 Vali Loss: 1.4637711 Test Loss: 0.6773750
Validation loss decreased (1.470753 --> 1.463771).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.954850912094116
Epoch: 11, Steps: 29 | Train Loss: 0.3989107 Vali Loss: 1.4461582 Test Loss: 0.6727352
Validation loss decreased (1.463771 --> 1.446158).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.05622124671936
Epoch: 12, Steps: 29 | Train Loss: 0.3907840 Vali Loss: 1.4432582 Test Loss: 0.6694754
Validation loss decreased (1.446158 --> 1.443258).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.034125804901123
Epoch: 13, Steps: 29 | Train Loss: 0.3836597 Vali Loss: 1.4292172 Test Loss: 0.6656090
Validation loss decreased (1.443258 --> 1.429217).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.12713098526001
Epoch: 14, Steps: 29 | Train Loss: 0.3768148 Vali Loss: 1.4256564 Test Loss: 0.6628229
Validation loss decreased (1.429217 --> 1.425656).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.774148464202881
Epoch: 15, Steps: 29 | Train Loss: 0.3701640 Vali Loss: 1.4237529 Test Loss: 0.6598563
Validation loss decreased (1.425656 --> 1.423753).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.205514669418335
Epoch: 16, Steps: 29 | Train Loss: 0.3648526 Vali Loss: 1.4183562 Test Loss: 0.6563957
Validation loss decreased (1.423753 --> 1.418356).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.072253942489624
Epoch: 17, Steps: 29 | Train Loss: 0.3597654 Vali Loss: 1.4168029 Test Loss: 0.6535379
Validation loss decreased (1.418356 --> 1.416803).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.423884868621826
Epoch: 18, Steps: 29 | Train Loss: 0.3548451 Vali Loss: 1.4081416 Test Loss: 0.6513811
Validation loss decreased (1.416803 --> 1.408142).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.114830017089844
Epoch: 19, Steps: 29 | Train Loss: 0.3500828 Vali Loss: 1.3997713 Test Loss: 0.6495361
Validation loss decreased (1.408142 --> 1.399771).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.107259750366211
Epoch: 20, Steps: 29 | Train Loss: 0.3463191 Vali Loss: 1.4017954 Test Loss: 0.6460081
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.049791574478149
Epoch: 21, Steps: 29 | Train Loss: 0.3426621 Vali Loss: 1.3908485 Test Loss: 0.6441420
Validation loss decreased (1.399771 --> 1.390849).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.7560341358184814
Epoch: 22, Steps: 29 | Train Loss: 0.3387374 Vali Loss: 1.3917172 Test Loss: 0.6425213
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.931964874267578
Epoch: 23, Steps: 29 | Train Loss: 0.3356921 Vali Loss: 1.3920846 Test Loss: 0.6406503
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.886526107788086
Epoch: 24, Steps: 29 | Train Loss: 0.3325146 Vali Loss: 1.3970761 Test Loss: 0.6381119
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  71258880.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.276531219482422
Epoch: 1, Steps: 29 | Train Loss: 0.5640311 Vali Loss: 1.3161963 Test Loss: 0.5820774
Validation loss decreased (inf --> 1.316196).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.458468914031982
Epoch: 2, Steps: 29 | Train Loss: 0.5291466 Vali Loss: 1.2733337 Test Loss: 0.5410255
Validation loss decreased (1.316196 --> 1.273334).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.52686882019043
Epoch: 3, Steps: 29 | Train Loss: 0.5052507 Vali Loss: 1.2456399 Test Loss: 0.5130235
Validation loss decreased (1.273334 --> 1.245640).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.332431316375732
Epoch: 4, Steps: 29 | Train Loss: 0.4886122 Vali Loss: 1.2182832 Test Loss: 0.4929268
Validation loss decreased (1.245640 --> 1.218283).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.0730249881744385
Epoch: 5, Steps: 29 | Train Loss: 0.4763535 Vali Loss: 1.2074252 Test Loss: 0.4782655
Validation loss decreased (1.218283 --> 1.207425).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.126858949661255
Epoch: 6, Steps: 29 | Train Loss: 0.4681907 Vali Loss: 1.2007143 Test Loss: 0.4673557
Validation loss decreased (1.207425 --> 1.200714).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.048068284988403
Epoch: 7, Steps: 29 | Train Loss: 0.4607471 Vali Loss: 1.1902030 Test Loss: 0.4594321
Validation loss decreased (1.200714 --> 1.190203).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.229454278945923
Epoch: 8, Steps: 29 | Train Loss: 0.4558652 Vali Loss: 1.1795188 Test Loss: 0.4536718
Validation loss decreased (1.190203 --> 1.179519).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.4487433433532715
Epoch: 9, Steps: 29 | Train Loss: 0.4526313 Vali Loss: 1.1800348 Test Loss: 0.4494830
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.443468809127808
Epoch: 10, Steps: 29 | Train Loss: 0.4493559 Vali Loss: 1.1754911 Test Loss: 0.4462675
Validation loss decreased (1.179519 --> 1.175491).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.083064556121826
Epoch: 11, Steps: 29 | Train Loss: 0.4466981 Vali Loss: 1.1774317 Test Loss: 0.4441053
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.896460056304932
Epoch: 12, Steps: 29 | Train Loss: 0.4451459 Vali Loss: 1.1752653 Test Loss: 0.4425450
Validation loss decreased (1.175491 --> 1.175265).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.956984043121338
Epoch: 13, Steps: 29 | Train Loss: 0.4430314 Vali Loss: 1.1768320 Test Loss: 0.4413757
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.902361631393433
Epoch: 14, Steps: 29 | Train Loss: 0.4419025 Vali Loss: 1.1704484 Test Loss: 0.4405565
Validation loss decreased (1.175265 --> 1.170448).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.017727375030518
Epoch: 15, Steps: 29 | Train Loss: 0.4404818 Vali Loss: 1.1765552 Test Loss: 0.4401637
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.291311502456665
Epoch: 16, Steps: 29 | Train Loss: 0.4404632 Vali Loss: 1.1829019 Test Loss: 0.4396996
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.308571815490723
Epoch: 17, Steps: 29 | Train Loss: 0.4398639 Vali Loss: 1.1863430 Test Loss: 0.4394973
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.43887776136398315, mae:0.44137635827064514, rse:0.63070148229599, corr:[0.25087687 0.25934255 0.25715458 0.25662947 0.25707263 0.25552264
 0.25316998 0.252283   0.25252333 0.2530847  0.2528154  0.2518915
 0.2511536  0.25088042 0.25080654 0.25079456 0.25051805 0.2498969
 0.24918216 0.2487016  0.24858966 0.2484732  0.248183   0.24824977
 0.24873959 0.24894425 0.2484856  0.2478407  0.24769862 0.24789208
 0.24785212 0.24725372 0.24671878 0.24663211 0.24663465 0.24646513
 0.24618554 0.24599862 0.24605496 0.24628355 0.246726   0.2472104
 0.24739818 0.2473341  0.2473287  0.24770191 0.2483322  0.24871002
 0.24849355 0.2481954  0.24775635 0.24696824 0.24575454 0.24434237
 0.24356367 0.24348855 0.24339382 0.24287847 0.24207899 0.24177293
 0.24189296 0.2419959  0.24171108 0.24150272 0.24162701 0.2420216
 0.2423799  0.24232605 0.24227132 0.24243225 0.24261478 0.24266095
 0.24248335 0.2419092  0.24103941 0.24054602 0.24044141 0.24033625
 0.23994811 0.23930171 0.23871636 0.23846048 0.23829788 0.23800282
 0.23763147 0.23746191 0.23750177 0.23750885 0.2372763  0.23697072
 0.23661426 0.23630987 0.236134   0.23624279 0.23666577 0.23740605
 0.2382605  0.23893677 0.23932342 0.23960286 0.23970655 0.23954327
 0.23945662 0.23960713 0.23961724 0.23916233 0.2384695  0.23812506
 0.2383648  0.2386284  0.23861109 0.23862644 0.23888429 0.2392748
 0.23941305 0.23928075 0.23920777 0.23932628 0.23946232 0.23960492
 0.23973267 0.23933241 0.2382588  0.23712401 0.23656385 0.23646668
 0.23644376 0.23610055 0.2353385  0.23480481 0.23461607 0.23444408
 0.23415713 0.23407082 0.2343644  0.2347204  0.23481001 0.23470883
 0.23464292 0.23457707 0.23431166 0.23385297 0.23345186 0.23340702
 0.23349269 0.23313001 0.23222317 0.2311336  0.2303478  0.22965483
 0.22920486 0.229288   0.22970438 0.22976907 0.22927654 0.22904666
 0.22944607 0.22985113 0.22956555 0.2289913  0.22879414 0.22916278
 0.22935154 0.22912845 0.22901317 0.22929727 0.22937822 0.22898218
 0.22865373 0.22885588 0.22926661 0.22933178 0.22879605 0.22808136
 0.22767766 0.22772643 0.22780794 0.22796682 0.22801204 0.22772765
 0.2270856  0.2266504  0.22688848 0.2275433  0.22791019 0.22783297
 0.22773267 0.22807817 0.22874151 0.22918004 0.22909777 0.22865574
 0.22807111 0.22737443 0.22651464 0.22562163 0.22490278 0.22426327
 0.2238976  0.22411929 0.2244295  0.22412322 0.2232644  0.22306019
 0.22380204 0.2247015  0.22468413 0.22415422 0.22424544 0.22495992
 0.22532968 0.22484788 0.22418438 0.2239213  0.22382791 0.22362012
 0.22352467 0.22378223 0.22376512 0.22291866 0.22177623 0.221094
 0.2214233  0.22191685 0.22165588 0.22088988 0.2203377  0.22026958
 0.21998617 0.21939294 0.21937276 0.22007507 0.22054982 0.22048184
 0.22029409 0.22035511 0.22039072 0.22005598 0.2198439  0.22009264
 0.22072093 0.22092693 0.22069484 0.22046451 0.22032435 0.21991575
 0.21939334 0.21933462 0.21977067 0.21989763 0.21915637 0.21824564
 0.21790513 0.21783222 0.21757556 0.21731624 0.21793026 0.21914697
 0.21974182 0.21902461 0.21825221 0.21841247 0.21907972 0.21895076
 0.21811616 0.21761844 0.2176482  0.21735822 0.21614437 0.21491817
 0.21480638 0.21521986 0.21512716 0.21477026 0.21443698 0.21436605
 0.21447958 0.2147431  0.21508907 0.21512142 0.21454781 0.21403512
 0.21434355 0.21517587 0.21559732 0.21488138 0.21397267 0.21387592
 0.21434179 0.21441728 0.21356569 0.21303803 0.21327348 0.21356852
 0.2131824  0.21257028 0.21235704 0.21255761 0.21258213 0.21283685
 0.21373743 0.21421398 0.21363561 0.21249051 0.2124344  0.21322878
 0.21351361 0.21249308 0.21220775 0.2130782  0.21348916 0.21210971
 0.21024649 0.2096275  0.20967273 0.2087707  0.207115   0.2060023
 0.20603883 0.20531262 0.2034679  0.2024509  0.20301625 0.20344327
 0.20269689 0.20215473 0.20287202 0.20305453 0.20019901 0.19781682
 0.19929315 0.20009801 0.19357418 0.18557556 0.1909194  0.19553499]
