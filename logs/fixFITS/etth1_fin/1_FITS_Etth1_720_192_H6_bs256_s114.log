Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  174211072.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.049145221710205
Epoch: 1, Steps: 15 | Train Loss: 0.7928817 Vali Loss: 1.5546157 Test Loss: 0.8274519
Validation loss decreased (inf --> 1.554616).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.024028778076172
Epoch: 2, Steps: 15 | Train Loss: 0.6549459 Vali Loss: 1.3597459 Test Loss: 0.7086685
Validation loss decreased (1.554616 --> 1.359746).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.281240940093994
Epoch: 3, Steps: 15 | Train Loss: 0.5805994 Vali Loss: 1.2515072 Test Loss: 0.6430222
Validation loss decreased (1.359746 --> 1.251507).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.0192089080810547
Epoch: 4, Steps: 15 | Train Loss: 0.5377595 Vali Loss: 1.1919467 Test Loss: 0.6020959
Validation loss decreased (1.251507 --> 1.191947).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.325577735900879
Epoch: 5, Steps: 15 | Train Loss: 0.5108023 Vali Loss: 1.1450273 Test Loss: 0.5731699
Validation loss decreased (1.191947 --> 1.145027).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.894490957260132
Epoch: 6, Steps: 15 | Train Loss: 0.4908755 Vali Loss: 1.1150401 Test Loss: 0.5510020
Validation loss decreased (1.145027 --> 1.115040).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.0224502086639404
Epoch: 7, Steps: 15 | Train Loss: 0.4754880 Vali Loss: 1.0911319 Test Loss: 0.5332063
Validation loss decreased (1.115040 --> 1.091132).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.5119991302490234
Epoch: 8, Steps: 15 | Train Loss: 0.4640237 Vali Loss: 1.0732859 Test Loss: 0.5185356
Validation loss decreased (1.091132 --> 1.073286).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9039628505706787
Epoch: 9, Steps: 15 | Train Loss: 0.4539698 Vali Loss: 1.0552639 Test Loss: 0.5063813
Validation loss decreased (1.073286 --> 1.055264).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.462761163711548
Epoch: 10, Steps: 15 | Train Loss: 0.4461187 Vali Loss: 1.0444083 Test Loss: 0.4961042
Validation loss decreased (1.055264 --> 1.044408).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.6042685508728027
Epoch: 11, Steps: 15 | Train Loss: 0.4394342 Vali Loss: 1.0330950 Test Loss: 0.4875264
Validation loss decreased (1.044408 --> 1.033095).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.3783416748046875
Epoch: 12, Steps: 15 | Train Loss: 0.4340201 Vali Loss: 1.0289847 Test Loss: 0.4801354
Validation loss decreased (1.033095 --> 1.028985).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.6310131549835205
Epoch: 13, Steps: 15 | Train Loss: 0.4288127 Vali Loss: 1.0150921 Test Loss: 0.4739783
Validation loss decreased (1.028985 --> 1.015092).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.4174914360046387
Epoch: 14, Steps: 15 | Train Loss: 0.4250438 Vali Loss: 1.0072310 Test Loss: 0.4687083
Validation loss decreased (1.015092 --> 1.007231).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.273127555847168
Epoch: 15, Steps: 15 | Train Loss: 0.4215779 Vali Loss: 1.0018564 Test Loss: 0.4641257
Validation loss decreased (1.007231 --> 1.001856).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.065432548522949
Epoch: 16, Steps: 15 | Train Loss: 0.4185026 Vali Loss: 0.9992399 Test Loss: 0.4601989
Validation loss decreased (1.001856 --> 0.999240).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.1764049530029297
Epoch: 17, Steps: 15 | Train Loss: 0.4158417 Vali Loss: 0.9937161 Test Loss: 0.4568384
Validation loss decreased (0.999240 --> 0.993716).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.951775312423706
Epoch: 18, Steps: 15 | Train Loss: 0.4133574 Vali Loss: 0.9932534 Test Loss: 0.4539254
Validation loss decreased (0.993716 --> 0.993253).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.9869332313537598
Epoch: 19, Steps: 15 | Train Loss: 0.4112159 Vali Loss: 0.9853376 Test Loss: 0.4513874
Validation loss decreased (0.993253 --> 0.985338).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.9747936725616455
Epoch: 20, Steps: 15 | Train Loss: 0.4097603 Vali Loss: 0.9833739 Test Loss: 0.4491988
Validation loss decreased (0.985338 --> 0.983374).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.9533207416534424
Epoch: 21, Steps: 15 | Train Loss: 0.4080393 Vali Loss: 0.9846528 Test Loss: 0.4472996
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.13055157661438
Epoch: 22, Steps: 15 | Train Loss: 0.4068633 Vali Loss: 0.9822758 Test Loss: 0.4455648
Validation loss decreased (0.983374 --> 0.982276).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.8136603832244873
Epoch: 23, Steps: 15 | Train Loss: 0.4055105 Vali Loss: 0.9770767 Test Loss: 0.4441341
Validation loss decreased (0.982276 --> 0.977077).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.1808838844299316
Epoch: 24, Steps: 15 | Train Loss: 0.4043893 Vali Loss: 0.9775097 Test Loss: 0.4427934
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2273194789886475
Epoch: 25, Steps: 15 | Train Loss: 0.4032728 Vali Loss: 0.9789596 Test Loss: 0.4415987
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.111518383026123
Epoch: 26, Steps: 15 | Train Loss: 0.4025345 Vali Loss: 0.9751609 Test Loss: 0.4406109
Validation loss decreased (0.977077 --> 0.975161).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.2632908821105957
Epoch: 27, Steps: 15 | Train Loss: 0.4013452 Vali Loss: 0.9721502 Test Loss: 0.4396594
Validation loss decreased (0.975161 --> 0.972150).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.9631476402282715
Epoch: 28, Steps: 15 | Train Loss: 0.4008679 Vali Loss: 0.9739418 Test Loss: 0.4388556
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.2013437747955322
Epoch: 29, Steps: 15 | Train Loss: 0.4000952 Vali Loss: 0.9692143 Test Loss: 0.4381258
Validation loss decreased (0.972150 --> 0.969214).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.0054986476898193
Epoch: 30, Steps: 15 | Train Loss: 0.3996667 Vali Loss: 0.9729564 Test Loss: 0.4374839
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.0000345706939697
Epoch: 31, Steps: 15 | Train Loss: 0.3988706 Vali Loss: 0.9749163 Test Loss: 0.4368778
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.153761148452759
Epoch: 32, Steps: 15 | Train Loss: 0.3983924 Vali Loss: 0.9723548 Test Loss: 0.4363456
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4194198250770569, mae:0.4288293719291687, rse:0.6150094866752625, corr:[0.25789776 0.27149507 0.26325455 0.26722118 0.26815242 0.26432535
 0.26323333 0.26510423 0.26542157 0.26398152 0.2630109  0.26282778
 0.26283246 0.26246214 0.2616055  0.26077273 0.26070526 0.26066774
 0.25962752 0.2586058  0.25865138 0.25862888 0.2576656  0.25681764
 0.25734192 0.25789183 0.25755355 0.25751665 0.25858164 0.25928527
 0.2587593  0.25800484 0.2582982  0.25876787 0.25844273 0.25780228
 0.25770232 0.2580721  0.25814682 0.25779375 0.2575367  0.25785297
 0.25851938 0.2587617  0.25844628 0.2585454  0.25931507 0.2595524
 0.25906533 0.25865465 0.2586634  0.25813746 0.2566061  0.2552081
 0.25497887 0.25480428 0.2537374  0.2527322  0.25284368 0.2534079
 0.25308603 0.2523701  0.25240698 0.25291798 0.25296596 0.25309035
 0.25391397 0.254483   0.25431332 0.25388783 0.25381142 0.25379363
 0.25344664 0.2525971  0.2518361  0.251792   0.2519516  0.25159973
 0.25069842 0.2501608  0.2503833  0.2504242  0.24960446 0.2489963
 0.24937722 0.24966721 0.24887148 0.24807866 0.24845462 0.24909657
 0.24879408 0.24812435 0.24801996 0.24813421 0.24776864 0.247699
 0.24851541 0.24945077 0.2498708  0.2499528  0.24983005 0.24971256
 0.2499411  0.2500352  0.24944882 0.24898012 0.24925081 0.24935642
 0.24855314 0.2480317  0.24884084 0.24956909 0.24907237 0.2488326
 0.24972372 0.2501466  0.24943148 0.24891078 0.24919416 0.24932972
 0.24889204 0.24832852 0.24787335 0.24744685 0.24674846 0.24573378
 0.2449582  0.24478552 0.24477315 0.24417575 0.24309951 0.24294023
 0.24368885 0.24359256 0.24243863 0.2422369  0.24335675 0.24353093
 0.24254255 0.24233876 0.24303012 0.2428273  0.24153337 0.24089807
 0.24114637 0.24065495 0.23941934 0.2385256  0.23779705 0.23671111
 0.23609135 0.23603544 0.23599267 0.2362504  0.23673137 0.23657762
 0.23572741 0.23591976 0.23702653 0.23659253 0.23522018 0.23559897
 0.23677918 0.23584458 0.23396787 0.2341551  0.23508662 0.23366742
 0.2316778  0.23217712 0.23306945 0.23234378 0.23111387 0.23070724
 0.23032223 0.23008074 0.2300468  0.22963901 0.22873715 0.22898756
 0.22935638 0.22729582 0.22530197 0.22667657 0.22661653 0.22152059
 0.21916597 0.22175336 0.21554126 0.20187208 0.20690615 0.20175706]
