Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  55292160.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.9460813999176025
Epoch: 1, Steps: 30 | Train Loss: 0.6093077 Vali Loss: 1.0512004 Test Loss: 0.5441324
Validation loss decreased (inf --> 1.051200).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.829762697219849
Epoch: 2, Steps: 30 | Train Loss: 0.4400020 Vali Loss: 0.8922984 Test Loss: 0.4494414
Validation loss decreased (1.051200 --> 0.892298).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.6577160358428955
Epoch: 3, Steps: 30 | Train Loss: 0.3912806 Vali Loss: 0.8311689 Test Loss: 0.4126119
Validation loss decreased (0.892298 --> 0.831169).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.579869747161865
Epoch: 4, Steps: 30 | Train Loss: 0.3711846 Vali Loss: 0.8008574 Test Loss: 0.3963638
Validation loss decreased (0.831169 --> 0.800857).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.401374816894531
Epoch: 5, Steps: 30 | Train Loss: 0.3604349 Vali Loss: 0.7799901 Test Loss: 0.3889351
Validation loss decreased (0.800857 --> 0.779990).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.653023719787598
Epoch: 6, Steps: 30 | Train Loss: 0.3541734 Vali Loss: 0.7679245 Test Loss: 0.3858218
Validation loss decreased (0.779990 --> 0.767925).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.4282026290893555
Epoch: 7, Steps: 30 | Train Loss: 0.3507433 Vali Loss: 0.7596694 Test Loss: 0.3840630
Validation loss decreased (0.767925 --> 0.759669).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.513476371765137
Epoch: 8, Steps: 30 | Train Loss: 0.3483741 Vali Loss: 0.7440959 Test Loss: 0.3834082
Validation loss decreased (0.759669 --> 0.744096).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.599590063095093
Epoch: 9, Steps: 30 | Train Loss: 0.3468183 Vali Loss: 0.7412504 Test Loss: 0.3827735
Validation loss decreased (0.744096 --> 0.741250).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.435563325881958
Epoch: 10, Steps: 30 | Train Loss: 0.3449000 Vali Loss: 0.7326236 Test Loss: 0.3824215
Validation loss decreased (0.741250 --> 0.732624).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.714213848114014
Epoch: 11, Steps: 30 | Train Loss: 0.3437608 Vali Loss: 0.7434799 Test Loss: 0.3820323
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.09842586517334
Epoch: 12, Steps: 30 | Train Loss: 0.3432089 Vali Loss: 0.7360015 Test Loss: 0.3818644
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.552033185958862
Epoch: 13, Steps: 30 | Train Loss: 0.3414343 Vali Loss: 0.7294018 Test Loss: 0.3817184
Validation loss decreased (0.732624 --> 0.729402).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.535912036895752
Epoch: 14, Steps: 30 | Train Loss: 0.3410825 Vali Loss: 0.7321334 Test Loss: 0.3817465
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.812833547592163
Epoch: 15, Steps: 30 | Train Loss: 0.3397497 Vali Loss: 0.7243500 Test Loss: 0.3817693
Validation loss decreased (0.729402 --> 0.724350).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.85194993019104
Epoch: 16, Steps: 30 | Train Loss: 0.3395266 Vali Loss: 0.7279657 Test Loss: 0.3813530
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.570678472518921
Epoch: 17, Steps: 30 | Train Loss: 0.3396160 Vali Loss: 0.7180224 Test Loss: 0.3815756
Validation loss decreased (0.724350 --> 0.718022).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.621170520782471
Epoch: 18, Steps: 30 | Train Loss: 0.3391282 Vali Loss: 0.7195111 Test Loss: 0.3815622
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.804172992706299
Epoch: 19, Steps: 30 | Train Loss: 0.3387192 Vali Loss: 0.7218285 Test Loss: 0.3815313
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 6.059917211532593
Epoch: 20, Steps: 30 | Train Loss: 0.3372511 Vali Loss: 0.7192845 Test Loss: 0.3815991
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.3807161748409271, mae:0.40425771474838257, rse:0.5860821604728699, corr:[0.26944903 0.28115436 0.2794891  0.2778827  0.2772724  0.27545166
 0.27327535 0.2725127  0.27267638 0.27294314 0.27254984 0.27180967
 0.27129257 0.27117497 0.27132306 0.2714837  0.27137038 0.27107543
 0.27079928 0.27059507 0.2703546  0.2698503  0.26930028 0.26907492
 0.2693164  0.2694339  0.26919508 0.26891264 0.26893374 0.2690097
 0.2687314  0.2680156  0.26749474 0.26747814 0.26760006 0.26755416
 0.26731747 0.26714033 0.26719594 0.26745793 0.26786044 0.26819533
 0.26833647 0.26845732 0.2685983  0.26875368 0.2689456  0.26909375
 0.2688344  0.26820934 0.267282   0.26622036 0.26504534 0.26365516
 0.2626243  0.26207316 0.26170948 0.26131532 0.26081467 0.26082522
 0.2613059  0.26158458 0.26119137 0.26076534 0.26094344 0.26163548
 0.26207045 0.26167262 0.26111308 0.26107624 0.2611821  0.26072878
 0.2598705  0.25902867 0.2581492  0.25759652 0.25730583 0.2570338
 0.2563323  0.25502646 0.25382397 0.2534638  0.25343308 0.2526117
 0.2515446  0.25140044 0.2521523  0.2513983  0.2490289  0.24767217
 0.24820857 0.24676946 0.24125823 0.23661819 0.23923671 0.23532395]
