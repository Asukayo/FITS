Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.853599071502686
Epoch: 1, Steps: 59 | Train Loss: 0.7460661 Vali Loss: 1.4660884 Test Loss: 0.6399646
Validation loss decreased (inf --> 1.466088).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.348738670349121
Epoch: 2, Steps: 59 | Train Loss: 0.5799197 Vali Loss: 1.3280987 Test Loss: 0.5459132
Validation loss decreased (1.466088 --> 1.328099).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.197797060012817
Epoch: 3, Steps: 59 | Train Loss: 0.5266714 Vali Loss: 1.2706199 Test Loss: 0.4994324
Validation loss decreased (1.328099 --> 1.270620).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.08262014389038
Epoch: 4, Steps: 59 | Train Loss: 0.4973493 Vali Loss: 1.2314873 Test Loss: 0.4723166
Validation loss decreased (1.270620 --> 1.231487).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.093593120574951
Epoch: 5, Steps: 59 | Train Loss: 0.4795559 Vali Loss: 1.2109989 Test Loss: 0.4559413
Validation loss decreased (1.231487 --> 1.210999).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 7.654746055603027
Epoch: 6, Steps: 59 | Train Loss: 0.4679473 Vali Loss: 1.1958604 Test Loss: 0.4463244
Validation loss decreased (1.210999 --> 1.195860).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 7.405125617980957
Epoch: 7, Steps: 59 | Train Loss: 0.4601214 Vali Loss: 1.1835699 Test Loss: 0.4410014
Validation loss decreased (1.195860 --> 1.183570).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 7.431586742401123
Epoch: 8, Steps: 59 | Train Loss: 0.4549546 Vali Loss: 1.1817029 Test Loss: 0.4378861
Validation loss decreased (1.183570 --> 1.181703).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 7.137911558151245
Epoch: 9, Steps: 59 | Train Loss: 0.4506532 Vali Loss: 1.1837502 Test Loss: 0.4362238
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.430584192276001
Epoch: 10, Steps: 59 | Train Loss: 0.4483190 Vali Loss: 1.1818228 Test Loss: 0.4357681
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.830392599105835
Epoch: 11, Steps: 59 | Train Loss: 0.4466168 Vali Loss: 1.1811122 Test Loss: 0.4356518
Validation loss decreased (1.181703 --> 1.181112).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.323371410369873
Epoch: 12, Steps: 59 | Train Loss: 0.4451970 Vali Loss: 1.1880554 Test Loss: 0.4357254
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.479390621185303
Epoch: 13, Steps: 59 | Train Loss: 0.4437039 Vali Loss: 1.1863438 Test Loss: 0.4358904
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.966788530349731
Epoch: 14, Steps: 59 | Train Loss: 0.4429679 Vali Loss: 1.1868228 Test Loss: 0.4361485
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.43451520800590515, mae:0.43806079030036926, rse:0.6275589466094971, corr:[0.251044   0.26165813 0.2565178  0.25838822 0.25785884 0.2539699
 0.25343555 0.25502408 0.25491402 0.25375652 0.2534441  0.2533852
 0.25276184 0.2521301  0.25169834 0.25128156 0.25118932 0.25110233
 0.25039908 0.24957366 0.24945931 0.24913757 0.24841064 0.24795637
 0.24832006 0.24846278 0.24837849 0.24865527 0.24911287 0.24915798
 0.24896953 0.24907374 0.24920507 0.2488952  0.24842882 0.2483261
 0.24847646 0.24864978 0.24885541 0.24885818 0.24855931 0.24834874
 0.24860089 0.24894178 0.24902768 0.24939299 0.2500302  0.25025144
 0.25014415 0.24969135 0.24887085 0.2479758  0.24701235 0.24610141
 0.24552646 0.24527587 0.2451349  0.24485558 0.24451497 0.24425107
 0.2441112  0.24410579 0.24411966 0.24413328 0.24433796 0.24478272
 0.24519838 0.24499774 0.24464093 0.2446147  0.244867   0.2448537
 0.24416278 0.2431985  0.24273708 0.24285905 0.24282704 0.24236329
 0.24179456 0.24161646 0.24187091 0.24191421 0.24136217 0.24070205
 0.24041827 0.24039686 0.24018493 0.23981592 0.2396708  0.23969835
 0.23946589 0.23922567 0.2392319  0.23938999 0.23913053 0.23915686
 0.24013215 0.24128728 0.24178447 0.24182902 0.24174364 0.24157013
 0.24151799 0.2416785  0.24173166 0.2415358  0.24126653 0.24107364
 0.24088275 0.24082759 0.2410951  0.2414902  0.24167384 0.24157023
 0.24136858 0.24121866 0.2412     0.24129878 0.2413419  0.24132772
 0.24118221 0.24063389 0.23980707 0.23903985 0.23833789 0.23758559
 0.2370752  0.23691355 0.23686811 0.23678331 0.23662008 0.23644894
 0.23641613 0.2364783  0.23639958 0.2360838  0.23580895 0.23606914
 0.23659007 0.23678838 0.23659338 0.23640712 0.23604745 0.2356331
 0.23538987 0.2351823  0.23482877 0.23401208 0.23295091 0.23241699
 0.2328718  0.23333612 0.23310146 0.23264578 0.23253946 0.23257487
 0.23235568 0.23229171 0.23259072 0.2326948  0.23228647 0.23213275
 0.23244962 0.23265375 0.23233928 0.23202646 0.23185852 0.2317399
 0.23159437 0.23181623 0.23246612 0.23301706 0.2328851  0.23222597
 0.23158547 0.23133153 0.23129751 0.23135366 0.23133482 0.23116925
 0.23080118 0.23039488 0.2300667  0.22995706 0.2301832  0.23070706
 0.23114458 0.23137695 0.2316954  0.23218626 0.23241597 0.23219426
 0.23172082 0.23119873 0.23044913 0.22945632 0.22872955 0.22850066
 0.22832927 0.22786777 0.22731595 0.22716853 0.22726932 0.22736073
 0.22725786 0.22736692 0.22769426 0.22792782 0.22798228 0.22784694
 0.22764659 0.22728531 0.2268996  0.22670838 0.22656186 0.22626545
 0.22586715 0.22578526 0.22614059 0.2265166  0.22622204 0.22513086
 0.22433417 0.22450182 0.22484562 0.22469418 0.22439978 0.22456531
 0.2247228  0.22434415 0.22364101 0.22320026 0.22330509 0.22357321
 0.22328822 0.22289342 0.2232999  0.22402276 0.2239985  0.22345701
 0.22359455 0.22403002 0.22391179 0.22321548 0.22280148 0.22289735
 0.22286092 0.22259548 0.22242895 0.22244844 0.22233663 0.2218246
 0.22090389 0.22016479 0.22041446 0.22097051 0.22080076 0.220339
 0.22083741 0.22150859 0.22116607 0.22037597 0.22076212 0.22148801
 0.22118132 0.22045523 0.22027332 0.2199821  0.2187482  0.21766737
 0.21757181 0.21733291 0.21656738 0.21618663 0.21597753 0.21552597
 0.21544193 0.21596925 0.21594226 0.2153731  0.21574108 0.21691847
 0.21693294 0.21610488 0.21643025 0.21708553 0.21639852 0.21550325
 0.21588579 0.2162621  0.21512756 0.21435031 0.21498422 0.21525142
 0.21408533 0.21324018 0.21327281 0.21325143 0.21346638 0.21448591
 0.21460146 0.21337335 0.21339059 0.21460176 0.21489629 0.21428983
 0.21462247 0.21432714 0.21271993 0.21177074 0.21259673 0.21223636
 0.21001069 0.20905553 0.2097611  0.20876363 0.20646177 0.20559889
 0.2052716  0.20286655 0.20115201 0.20212673 0.20189187 0.2004539
 0.20141204 0.20219988 0.1996982  0.19883348 0.20001984 0.19707336
 0.19227722 0.19403383 0.19306162 0.17966457 0.1813881  0.19500409]
