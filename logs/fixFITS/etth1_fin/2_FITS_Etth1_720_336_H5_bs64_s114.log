Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  35629440.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.858689308166504
Epoch: 1, Steps: 59 | Train Loss: 0.6962596 Vali Loss: 1.7527851 Test Loss: 0.8182055
Validation loss decreased (inf --> 1.752785).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.780221462249756
Epoch: 2, Steps: 59 | Train Loss: 0.5423968 Vali Loss: 1.5808246 Test Loss: 0.7303904
Validation loss decreased (1.752785 --> 1.580825).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 7.961729526519775
Epoch: 3, Steps: 59 | Train Loss: 0.4706721 Vali Loss: 1.5129902 Test Loss: 0.6970912
Validation loss decreased (1.580825 --> 1.512990).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.06130599975586
Epoch: 4, Steps: 59 | Train Loss: 0.4299541 Vali Loss: 1.4757470 Test Loss: 0.6812934
Validation loss decreased (1.512990 --> 1.475747).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.918494939804077
Epoch: 5, Steps: 59 | Train Loss: 0.4018007 Vali Loss: 1.4478732 Test Loss: 0.6679875
Validation loss decreased (1.475747 --> 1.447873).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.148611545562744
Epoch: 6, Steps: 59 | Train Loss: 0.3795445 Vali Loss: 1.4241968 Test Loss: 0.6575477
Validation loss decreased (1.447873 --> 1.424197).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.979476928710938
Epoch: 7, Steps: 59 | Train Loss: 0.3611442 Vali Loss: 1.4092607 Test Loss: 0.6492633
Validation loss decreased (1.424197 --> 1.409261).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.758331298828125
Epoch: 8, Steps: 59 | Train Loss: 0.3455014 Vali Loss: 1.3934596 Test Loss: 0.6395286
Validation loss decreased (1.409261 --> 1.393460).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.491152286529541
Epoch: 9, Steps: 59 | Train Loss: 0.3318667 Vali Loss: 1.3788916 Test Loss: 0.6316644
Validation loss decreased (1.393460 --> 1.378892).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.798848628997803
Epoch: 10, Steps: 59 | Train Loss: 0.3197550 Vali Loss: 1.3705527 Test Loss: 0.6239483
Validation loss decreased (1.378892 --> 1.370553).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.871697902679443
Epoch: 11, Steps: 59 | Train Loss: 0.3091826 Vali Loss: 1.3550975 Test Loss: 0.6164389
Validation loss decreased (1.370553 --> 1.355098).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.709973812103271
Epoch: 12, Steps: 59 | Train Loss: 0.2998052 Vali Loss: 1.3477557 Test Loss: 0.6100980
Validation loss decreased (1.355098 --> 1.347756).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.74677848815918
Epoch: 13, Steps: 59 | Train Loss: 0.2913450 Vali Loss: 1.3375652 Test Loss: 0.6032535
Validation loss decreased (1.347756 --> 1.337565).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.932908535003662
Epoch: 14, Steps: 59 | Train Loss: 0.2836585 Vali Loss: 1.3334696 Test Loss: 0.5972108
Validation loss decreased (1.337565 --> 1.333470).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.316597938537598
Epoch: 15, Steps: 59 | Train Loss: 0.2769612 Vali Loss: 1.3277863 Test Loss: 0.5920216
Validation loss decreased (1.333470 --> 1.327786).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.793359994888306
Epoch: 16, Steps: 59 | Train Loss: 0.2706441 Vali Loss: 1.3194646 Test Loss: 0.5861349
Validation loss decreased (1.327786 --> 1.319465).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.748120069503784
Epoch: 17, Steps: 59 | Train Loss: 0.2650002 Vali Loss: 1.3150533 Test Loss: 0.5804268
Validation loss decreased (1.319465 --> 1.315053).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.389192581176758
Epoch: 18, Steps: 59 | Train Loss: 0.2600361 Vali Loss: 1.3056092 Test Loss: 0.5766737
Validation loss decreased (1.315053 --> 1.305609).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.126554489135742
Epoch: 19, Steps: 59 | Train Loss: 0.2552904 Vali Loss: 1.3034431 Test Loss: 0.5721794
Validation loss decreased (1.305609 --> 1.303443).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.007037162780762
Epoch: 20, Steps: 59 | Train Loss: 0.2508777 Vali Loss: 1.3024539 Test Loss: 0.5680583
Validation loss decreased (1.303443 --> 1.302454).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.6473388671875
Epoch: 21, Steps: 59 | Train Loss: 0.2471227 Vali Loss: 1.2949814 Test Loss: 0.5644069
Validation loss decreased (1.302454 --> 1.294981).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.706151247024536
Epoch: 22, Steps: 59 | Train Loss: 0.2434989 Vali Loss: 1.2915004 Test Loss: 0.5608063
Validation loss decreased (1.294981 --> 1.291500).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.447155714035034
Epoch: 23, Steps: 59 | Train Loss: 0.2401775 Vali Loss: 1.2945272 Test Loss: 0.5572638
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.609723091125488
Epoch: 24, Steps: 59 | Train Loss: 0.2372727 Vali Loss: 1.2862158 Test Loss: 0.5543125
Validation loss decreased (1.291500 --> 1.286216).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.63167428970337
Epoch: 25, Steps: 59 | Train Loss: 0.2344221 Vali Loss: 1.2887379 Test Loss: 0.5513920
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 8.553404569625854
Epoch: 26, Steps: 59 | Train Loss: 0.2318494 Vali Loss: 1.2801002 Test Loss: 0.5485946
Validation loss decreased (1.286216 --> 1.280100).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.070972442626953
Epoch: 27, Steps: 59 | Train Loss: 0.2293868 Vali Loss: 1.2798028 Test Loss: 0.5461030
Validation loss decreased (1.280100 --> 1.279803).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 7.953261613845825
Epoch: 28, Steps: 59 | Train Loss: 0.2272124 Vali Loss: 1.2749166 Test Loss: 0.5432562
Validation loss decreased (1.279803 --> 1.274917).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.72639012336731
Epoch: 29, Steps: 59 | Train Loss: 0.2251473 Vali Loss: 1.2784256 Test Loss: 0.5413684
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 8.565388679504395
Epoch: 30, Steps: 59 | Train Loss: 0.2231184 Vali Loss: 1.2712587 Test Loss: 0.5391580
Validation loss decreased (1.274917 --> 1.271259).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 8.28425645828247
Epoch: 31, Steps: 59 | Train Loss: 0.2213686 Vali Loss: 1.2756710 Test Loss: 0.5372520
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.155034303665161
Epoch: 32, Steps: 59 | Train Loss: 0.2197126 Vali Loss: 1.2713217 Test Loss: 0.5352873
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.050277948379517
Epoch: 33, Steps: 59 | Train Loss: 0.2180237 Vali Loss: 1.2657292 Test Loss: 0.5335335
Validation loss decreased (1.271259 --> 1.265729).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 8.395841121673584
Epoch: 34, Steps: 59 | Train Loss: 0.2167107 Vali Loss: 1.2683580 Test Loss: 0.5316692
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 8.284569263458252
Epoch: 35, Steps: 59 | Train Loss: 0.2153070 Vali Loss: 1.2626134 Test Loss: 0.5301419
Validation loss decreased (1.265729 --> 1.262613).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.48051905632019
Epoch: 36, Steps: 59 | Train Loss: 0.2141035 Vali Loss: 1.2609129 Test Loss: 0.5286900
Validation loss decreased (1.262613 --> 1.260913).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 8.90579891204834
Epoch: 37, Steps: 59 | Train Loss: 0.2129788 Vali Loss: 1.2671509 Test Loss: 0.5272449
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 8.898386478424072
Epoch: 38, Steps: 59 | Train Loss: 0.2116221 Vali Loss: 1.2609562 Test Loss: 0.5258931
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 8.301949739456177
Epoch: 39, Steps: 59 | Train Loss: 0.2106097 Vali Loss: 1.2566818 Test Loss: 0.5245240
Validation loss decreased (1.260913 --> 1.256682).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 8.468528270721436
Epoch: 40, Steps: 59 | Train Loss: 0.2096276 Vali Loss: 1.2618376 Test Loss: 0.5233310
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 8.174566745758057
Epoch: 41, Steps: 59 | Train Loss: 0.2087295 Vali Loss: 1.2607819 Test Loss: 0.5223905
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 8.620044708251953
Epoch: 42, Steps: 59 | Train Loss: 0.2078662 Vali Loss: 1.2593077 Test Loss: 0.5212240
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  35629440.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.116365194320679
Epoch: 1, Steps: 59 | Train Loss: 0.4719786 Vali Loss: 1.2063959 Test Loss: 0.4684896
Validation loss decreased (inf --> 1.206396).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.52492880821228
Epoch: 2, Steps: 59 | Train Loss: 0.4491526 Vali Loss: 1.1904805 Test Loss: 0.4479436
Validation loss decreased (1.206396 --> 1.190480).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.439411640167236
Epoch: 3, Steps: 59 | Train Loss: 0.4406065 Vali Loss: 1.1898957 Test Loss: 0.4416755
Validation loss decreased (1.190480 --> 1.189896).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.470924139022827
Epoch: 4, Steps: 59 | Train Loss: 0.4369528 Vali Loss: 1.1952538 Test Loss: 0.4404629
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.027493953704834
Epoch: 5, Steps: 59 | Train Loss: 0.4356143 Vali Loss: 1.1967584 Test Loss: 0.4407512
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.786964178085327
Epoch: 6, Steps: 59 | Train Loss: 0.4346087 Vali Loss: 1.2025400 Test Loss: 0.4412700
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4405314326286316, mae:0.4418191611766815, rse:0.6318885684013367, corr:[0.25253063 0.26101628 0.2595389  0.25676596 0.25605068 0.25490004
 0.25327483 0.25237206 0.25249937 0.25314733 0.25329268 0.2526436
 0.25191075 0.25131512 0.25089073 0.2506203  0.25051206 0.2503125
 0.24984922 0.24923761 0.24898763 0.2489308  0.24852014 0.24777141
 0.24738267 0.24770686 0.24802521 0.24783431 0.24751163 0.24745561
 0.24762952 0.24764909 0.24757954 0.247493   0.24735904 0.24729376
 0.24725786 0.24724722 0.24730843 0.24736847 0.2474259  0.24746883
 0.24754198 0.24762067 0.24773496 0.24800353 0.24855132 0.24899152
 0.24863367 0.24766201 0.24640287 0.24520776 0.24397492 0.24264118
 0.24183945 0.24181673 0.24202783 0.24194255 0.2413515  0.24096878
 0.24093923 0.24097395 0.24057671 0.24010028 0.23991124 0.24014065
 0.2405483  0.24073185 0.24109732 0.24169567 0.24198242 0.24152212
 0.24079815 0.24038278 0.2403282  0.24044232 0.24021335 0.23970443
 0.23932362 0.23924054 0.2391953  0.2389591  0.2384609  0.23786116
 0.237269   0.23676547 0.23636335 0.23600225 0.23559096 0.23525791
 0.23500612 0.23493871 0.23484157 0.23472826 0.23482992 0.2354611
 0.23658259 0.23741262 0.2376618  0.23771895 0.23771901 0.23767412
 0.23758616 0.23756362 0.23764035 0.23766248 0.23743531 0.2372394
 0.23745298 0.23786254 0.23802295 0.23777357 0.23727739 0.2368569
 0.23662935 0.23657319 0.23677875 0.23718517 0.23734435 0.23705477
 0.23657489 0.2358311  0.23500057 0.23439565 0.23401706 0.23379977
 0.23383921 0.23384258 0.23329647 0.23264629 0.23236738 0.2324969
 0.23261972 0.2325213  0.2324304  0.23252809 0.23252015 0.23215154
 0.23170196 0.23166159 0.23208645 0.23230241 0.23195833 0.23152347
 0.23138489 0.23122059 0.23063882 0.22972901 0.22905762 0.22851352
 0.2279747  0.22767074 0.2277878  0.22803776 0.22786151 0.22747469
 0.22738491 0.22766006 0.22760412 0.22690031 0.22604829 0.22601454
 0.22664617 0.22716512 0.2271871  0.22690102 0.22648652 0.22617203
 0.22621158 0.22658916 0.22682    0.22659072 0.22595137 0.2255404
 0.22585803 0.22655039 0.22649078 0.22565761 0.22490199 0.22513139
 0.22591884 0.22629726 0.22594595 0.22548528 0.22557437 0.226151
 0.22664282 0.22666192 0.22634748 0.22608042 0.22613528 0.22655971
 0.22689378 0.2265797  0.22563513 0.2246835  0.22411163 0.22344777
 0.22247712 0.22186892 0.2220512  0.22255926 0.2223829  0.22174068
 0.22138427 0.22185193 0.22250749 0.22268412 0.22240144 0.22189318
 0.22127938 0.2206334  0.2205289  0.22117032 0.22191975 0.2218709
 0.22091538 0.21999328 0.2197432  0.21970592 0.21909314 0.21780565
 0.21726823 0.21807507 0.21909729 0.21910934 0.21838042 0.21822613
 0.21877994 0.21920821 0.21934557 0.2193324  0.21904626 0.21840936
 0.21751577 0.2169018  0.21705474 0.21761656 0.21814647 0.21827085
 0.21826705 0.21808156 0.21768987 0.2171426  0.21662292 0.21619508
 0.215986   0.21616867 0.2165811  0.21655534 0.2156892  0.21461238
 0.21442133 0.2152843  0.2162857  0.21653157 0.21634683 0.21635832
 0.21666667 0.21657893 0.21633135 0.21647747 0.21720722 0.21765117
 0.217253   0.21644498 0.21601923 0.21608327 0.21576752 0.21483949
 0.21398874 0.21344484 0.21295214 0.21262817 0.21231313 0.21220684
 0.21223195 0.21211211 0.21201801 0.21221776 0.21251999 0.2126942
 0.21272306 0.21275997 0.21308453 0.21300556 0.2124982  0.21210817
 0.2123414  0.21295583 0.21276514 0.21194676 0.21117197 0.21122743
 0.21165521 0.21184099 0.2116371  0.21122259 0.2104857  0.20979007
 0.20991333 0.21066898 0.21150239 0.21164581 0.2116865  0.21214157
 0.21287532 0.2125651  0.21171156 0.21085796 0.21057601 0.21065794
 0.2107281  0.21029918 0.20910186 0.2075743  0.20664018 0.20640537
 0.20660919 0.20599702 0.20438217 0.202814   0.20194615 0.20159885
 0.20157121 0.20176557 0.20216541 0.2023062  0.20121332 0.20088318
 0.20218334 0.2021422  0.19818756 0.19463791 0.19886166 0.20107427]
