Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21776384.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4469655
	speed: 0.1486s/iter; left time: 876.6195s
Epoch: 1 cost time: 17.908722400665283
Epoch: 1, Steps: 120 | Train Loss: 0.5762539 Vali Loss: 1.0689435 Test Loss: 0.4770181
Validation loss decreased (inf --> 1.068944).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3613533
	speed: 0.3998s/iter; left time: 2311.5205s
Epoch: 2 cost time: 18.325825452804565
Epoch: 2, Steps: 120 | Train Loss: 0.4272120 Vali Loss: 0.9886662 Test Loss: 0.4222717
Validation loss decreased (1.068944 --> 0.988666).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4026872
	speed: 0.3739s/iter; left time: 2116.3893s
Epoch: 3 cost time: 18.09867286682129
Epoch: 3, Steps: 120 | Train Loss: 0.4019937 Vali Loss: 0.9726021 Test Loss: 0.4149645
Validation loss decreased (0.988666 --> 0.972602).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3994904
	speed: 0.3495s/iter; left time: 1936.5641s
Epoch: 4 cost time: 15.066925287246704
Epoch: 4, Steps: 120 | Train Loss: 0.3944002 Vali Loss: 0.9695050 Test Loss: 0.4149984
Validation loss decreased (0.972602 --> 0.969505).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3521481
	speed: 0.3159s/iter; left time: 1712.5501s
Epoch: 5 cost time: 14.499863624572754
Epoch: 5, Steps: 120 | Train Loss: 0.3906964 Vali Loss: 0.9671580 Test Loss: 0.4150817
Validation loss decreased (0.969505 --> 0.967158).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3809727
	speed: 0.3444s/iter; left time: 1825.6937s
Epoch: 6 cost time: 16.144837856292725
Epoch: 6, Steps: 120 | Train Loss: 0.3882716 Vali Loss: 0.9688264 Test Loss: 0.4152649
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3759898
	speed: 0.3347s/iter; left time: 1734.1322s
Epoch: 7 cost time: 16.054239988327026
Epoch: 7, Steps: 120 | Train Loss: 0.3863933 Vali Loss: 0.9662367 Test Loss: 0.4155478
Validation loss decreased (0.967158 --> 0.966237).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4001726
	speed: 0.3421s/iter; left time: 1731.2800s
Epoch: 8 cost time: 15.83285641670227
Epoch: 8, Steps: 120 | Train Loss: 0.3848468 Vali Loss: 0.9653367 Test Loss: 0.4156538
Validation loss decreased (0.966237 --> 0.965337).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4211725
	speed: 0.3186s/iter; left time: 1574.3392s
Epoch: 9 cost time: 14.3415687084198
Epoch: 9, Steps: 120 | Train Loss: 0.3841035 Vali Loss: 0.9655226 Test Loss: 0.4158278
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4006193
	speed: 0.3295s/iter; left time: 1588.5759s
Epoch: 10 cost time: 16.670120000839233
Epoch: 10, Steps: 120 | Train Loss: 0.3829983 Vali Loss: 0.9644021 Test Loss: 0.4160325
Validation loss decreased (0.965337 --> 0.964402).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3562872
	speed: 0.3465s/iter; left time: 1628.8028s
Epoch: 11 cost time: 16.171929121017456
Epoch: 11, Steps: 120 | Train Loss: 0.3823567 Vali Loss: 0.9652269 Test Loss: 0.4165716
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3544240
	speed: 0.3414s/iter; left time: 1564.1312s
Epoch: 12 cost time: 16.651427030563354
Epoch: 12, Steps: 120 | Train Loss: 0.3818943 Vali Loss: 0.9632190 Test Loss: 0.4161595
Validation loss decreased (0.964402 --> 0.963219).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3637859
	speed: 0.3676s/iter; left time: 1639.6428s
Epoch: 13 cost time: 16.90678095817566
Epoch: 13, Steps: 120 | Train Loss: 0.3817415 Vali Loss: 0.9646117 Test Loss: 0.4165214
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3889385
	speed: 0.3657s/iter; left time: 1587.5921s
Epoch: 14 cost time: 16.776833057403564
Epoch: 14, Steps: 120 | Train Loss: 0.3808377 Vali Loss: 0.9632356 Test Loss: 0.4162250
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3334172
	speed: 0.3768s/iter; left time: 1590.2890s
Epoch: 15 cost time: 17.218661069869995
Epoch: 15, Steps: 120 | Train Loss: 0.3807858 Vali Loss: 0.9625915 Test Loss: 0.4165032
Validation loss decreased (0.963219 --> 0.962591).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3996139
	speed: 0.3295s/iter; left time: 1351.4725s
Epoch: 16 cost time: 14.925827264785767
Epoch: 16, Steps: 120 | Train Loss: 0.3804355 Vali Loss: 0.9635351 Test Loss: 0.4165033
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4235342
	speed: 0.2725s/iter; left time: 1084.9697s
Epoch: 17 cost time: 12.560415267944336
Epoch: 17, Steps: 120 | Train Loss: 0.3802383 Vali Loss: 0.9632292 Test Loss: 0.4168836
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3839872
	speed: 0.2719s/iter; left time: 1049.9545s
Epoch: 18 cost time: 11.054402351379395
Epoch: 18, Steps: 120 | Train Loss: 0.3798128 Vali Loss: 0.9631708 Test Loss: 0.4166598
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.414715439081192, mae:0.4228082299232483, rse:0.6115506887435913, corr:[0.25747326 0.26875985 0.2683914  0.26936662 0.26794907 0.26508254
 0.26406497 0.26435867 0.26398888 0.26378217 0.26394537 0.26350927
 0.26285136 0.2628632  0.2633737  0.2634276  0.2630468  0.26278222
 0.26259932 0.26222697 0.2620109  0.26214528 0.26216188 0.2621296
 0.2619588  0.26151356 0.2612093  0.26133436 0.26124537 0.26051474
 0.2600518  0.2599845  0.2597935  0.2593233  0.25915027 0.2595856
 0.25979167 0.25935435 0.25904188 0.2593556  0.25995818 0.26017806
 0.26011702 0.26009545 0.26008382 0.26013348 0.26048875 0.2609296
 0.26092327 0.26024228 0.25880486 0.2571063  0.2557101  0.2547314
 0.2542942  0.2541895  0.25410992 0.25396487 0.25354028 0.2530581
 0.2527039  0.25279495 0.25309637 0.2531875  0.25306475 0.2532989
 0.2541158  0.25465092 0.25451258 0.2541647  0.25396684 0.2534978
 0.2525734  0.25171965 0.25135776 0.25118786 0.25065285 0.25014192
 0.24996214 0.24957739 0.24894044 0.24880692 0.24901432 0.24875896
 0.24830328 0.24846405 0.24885024 0.2483382  0.24729562 0.24719445
 0.2479131  0.24809746 0.24787748 0.24786524 0.24786885 0.24793318
 0.24856535 0.24933292 0.249245   0.24844804 0.24807982 0.2483616
 0.24843629 0.24823122 0.2482009  0.24806425 0.24719477 0.24646734
 0.24676742 0.24716878 0.24710937 0.24733102 0.24781574 0.24773309
 0.24713108 0.24675322 0.24676217 0.24675587 0.2467602  0.24668291
 0.24608609 0.24497615 0.24433844 0.2441272  0.24312575 0.24205545
 0.24229704 0.24275877 0.24187364 0.24121444 0.24170668 0.24154186
 0.23981473 0.23868544 0.23955898 0.24039434 0.24012451 0.23990463
 0.24048686 0.24042188 0.24002172 0.24039967 0.24062587 0.23982498
 0.23911408 0.23887461 0.23793276 0.23611288 0.2348215  0.23435552
 0.23365507 0.23313083 0.2334352  0.23342992 0.23203322 0.23105215
 0.23137403 0.2314644  0.23139384 0.2323039  0.23301996 0.23217174
 0.23137176 0.23189914 0.23174438 0.230236   0.22999862 0.23114215
 0.2308357  0.22958347 0.23004267 0.23093142 0.22947404 0.2276644
 0.22761168 0.22724938 0.22575103 0.22616918 0.22735025 0.22582708
 0.2240824  0.2250127  0.22466329 0.22226636 0.22304112 0.2251692
 0.22221124 0.22040494 0.22525042 0.22367832 0.21975493 0.23583889]
