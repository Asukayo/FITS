Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  123594240.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.6771090030670166
Epoch: 1, Steps: 15 | Train Loss: 0.8079966 Vali Loss: 1.6136307 Test Loss: 0.8406097
Validation loss decreased (inf --> 1.613631).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.697539806365967
Epoch: 2, Steps: 15 | Train Loss: 0.6700010 Vali Loss: 1.4024937 Test Loss: 0.7219661
Validation loss decreased (1.613631 --> 1.402494).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.7466604709625244
Epoch: 3, Steps: 15 | Train Loss: 0.5926101 Vali Loss: 1.2816817 Test Loss: 0.6554041
Validation loss decreased (1.402494 --> 1.281682).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.6227877140045166
Epoch: 4, Steps: 15 | Train Loss: 0.5478367 Vali Loss: 1.2190254 Test Loss: 0.6139265
Validation loss decreased (1.281682 --> 1.219025).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.8782918453216553
Epoch: 5, Steps: 15 | Train Loss: 0.5189303 Vali Loss: 1.1750475 Test Loss: 0.5845017
Validation loss decreased (1.219025 --> 1.175048).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.80745267868042
Epoch: 6, Steps: 15 | Train Loss: 0.4987847 Vali Loss: 1.1409199 Test Loss: 0.5616882
Validation loss decreased (1.175048 --> 1.140920).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.756747007369995
Epoch: 7, Steps: 15 | Train Loss: 0.4829821 Vali Loss: 1.1189272 Test Loss: 0.5434104
Validation loss decreased (1.140920 --> 1.118927).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.7816433906555176
Epoch: 8, Steps: 15 | Train Loss: 0.4704627 Vali Loss: 1.0940222 Test Loss: 0.5282440
Validation loss decreased (1.118927 --> 1.094022).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.5915701389312744
Epoch: 9, Steps: 15 | Train Loss: 0.4605744 Vali Loss: 1.0796083 Test Loss: 0.5154487
Validation loss decreased (1.094022 --> 1.079608).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.8045215606689453
Epoch: 10, Steps: 15 | Train Loss: 0.4524984 Vali Loss: 1.0613635 Test Loss: 0.5047747
Validation loss decreased (1.079608 --> 1.061363).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.0324342250823975
Epoch: 11, Steps: 15 | Train Loss: 0.4454940 Vali Loss: 1.0521284 Test Loss: 0.4956771
Validation loss decreased (1.061363 --> 1.052128).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.740471601486206
Epoch: 12, Steps: 15 | Train Loss: 0.4394916 Vali Loss: 1.0385129 Test Loss: 0.4878757
Validation loss decreased (1.052128 --> 1.038513).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.7120652198791504
Epoch: 13, Steps: 15 | Train Loss: 0.4345962 Vali Loss: 1.0281756 Test Loss: 0.4812640
Validation loss decreased (1.038513 --> 1.028176).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.7330498695373535
Epoch: 14, Steps: 15 | Train Loss: 0.4304767 Vali Loss: 1.0231997 Test Loss: 0.4756273
Validation loss decreased (1.028176 --> 1.023200).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.812581777572632
Epoch: 15, Steps: 15 | Train Loss: 0.4258698 Vali Loss: 1.0178840 Test Loss: 0.4706302
Validation loss decreased (1.023200 --> 1.017884).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.7569479942321777
Epoch: 16, Steps: 15 | Train Loss: 0.4231151 Vali Loss: 1.0115101 Test Loss: 0.4662821
Validation loss decreased (1.017884 --> 1.011510).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.728635549545288
Epoch: 17, Steps: 15 | Train Loss: 0.4201704 Vali Loss: 1.0053384 Test Loss: 0.4626459
Validation loss decreased (1.011510 --> 1.005338).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.686224937438965
Epoch: 18, Steps: 15 | Train Loss: 0.4183349 Vali Loss: 1.0021855 Test Loss: 0.4594193
Validation loss decreased (1.005338 --> 1.002185).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.6463351249694824
Epoch: 19, Steps: 15 | Train Loss: 0.4156063 Vali Loss: 0.9987496 Test Loss: 0.4566629
Validation loss decreased (1.002185 --> 0.998750).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.5719478130340576
Epoch: 20, Steps: 15 | Train Loss: 0.4136085 Vali Loss: 0.9971544 Test Loss: 0.4541758
Validation loss decreased (0.998750 --> 0.997154).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.706106662750244
Epoch: 21, Steps: 15 | Train Loss: 0.4121643 Vali Loss: 0.9942160 Test Loss: 0.4520048
Validation loss decreased (0.997154 --> 0.994216).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.827293872833252
Epoch: 22, Steps: 15 | Train Loss: 0.4104260 Vali Loss: 0.9894931 Test Loss: 0.4500860
Validation loss decreased (0.994216 --> 0.989493).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.7271974086761475
Epoch: 23, Steps: 15 | Train Loss: 0.4092478 Vali Loss: 0.9871942 Test Loss: 0.4484033
Validation loss decreased (0.989493 --> 0.987194).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.8642477989196777
Epoch: 24, Steps: 15 | Train Loss: 0.4078869 Vali Loss: 0.9828006 Test Loss: 0.4469227
Validation loss decreased (0.987194 --> 0.982801).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.756819248199463
Epoch: 25, Steps: 15 | Train Loss: 0.4071834 Vali Loss: 0.9819206 Test Loss: 0.4456439
Validation loss decreased (0.982801 --> 0.981921).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.8502533435821533
Epoch: 26, Steps: 15 | Train Loss: 0.4059255 Vali Loss: 0.9826674 Test Loss: 0.4444214
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.945234775543213
Epoch: 27, Steps: 15 | Train Loss: 0.4052261 Vali Loss: 0.9816271 Test Loss: 0.4433393
Validation loss decreased (0.981921 --> 0.981627).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.9646177291870117
Epoch: 28, Steps: 15 | Train Loss: 0.4038698 Vali Loss: 0.9792188 Test Loss: 0.4423908
Validation loss decreased (0.981627 --> 0.979219).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.1315269470214844
Epoch: 29, Steps: 15 | Train Loss: 0.4035827 Vali Loss: 0.9824568 Test Loss: 0.4415793
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.6286656856536865
Epoch: 30, Steps: 15 | Train Loss: 0.4025408 Vali Loss: 0.9797912 Test Loss: 0.4408404
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.4195895195007324
Epoch: 31, Steps: 15 | Train Loss: 0.4020268 Vali Loss: 0.9801332 Test Loss: 0.4401825
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4223002791404724, mae:0.4310878813266754, rse:0.617117702960968, corr:[0.25273377 0.27168566 0.26605967 0.26300085 0.26568308 0.26582706
 0.2644487  0.26364583 0.264113   0.26416844 0.2636184  0.2629523
 0.26245198 0.26142088 0.26040545 0.26020083 0.26063824 0.26009905
 0.25854158 0.25746813 0.25773796 0.25806168 0.2574176  0.25626945
 0.25601575 0.25661144 0.25697652 0.25696024 0.25698155 0.2572585
 0.2573528  0.25719294 0.25712618 0.2570357  0.2566829  0.25628722
 0.25635874 0.25671276 0.25670883 0.2562938  0.25615218 0.25667977
 0.25738952 0.25753313 0.25715086 0.25703517 0.25762734 0.2581377
 0.25793537 0.25734735 0.256821   0.25623852 0.25527486 0.2540917
 0.25324494 0.2528325  0.25259677 0.2524001  0.25219908 0.25202817
 0.2518086  0.25157183 0.25143668 0.2515735  0.2517641  0.25197124
 0.25224602 0.25240123 0.25266188 0.25283074 0.25271714 0.25225753
 0.2518358  0.2514482  0.2510737  0.2509535  0.25081083 0.25053775
 0.2502811  0.2501776  0.24993238 0.24944596 0.24895084 0.24892269
 0.24917698 0.24918    0.24870615 0.24817714 0.24803238 0.24819423
 0.24818553 0.24785173 0.24741317 0.24715956 0.24723724 0.24760357
 0.24821231 0.24892513 0.2494737  0.2497569  0.24991907 0.24987009
 0.24963601 0.249346   0.249252   0.24929708 0.24900438 0.24837406
 0.24798173 0.2481638  0.24866237 0.24910201 0.24917221 0.24922602
 0.24940583 0.24937393 0.24913393 0.24891204 0.2488148  0.24879317
 0.24870473 0.24819654 0.24748378 0.24680324 0.24600177 0.24509569
 0.24479848 0.24502122 0.24487565 0.24423444 0.24356161 0.24335475
 0.24336503 0.24328336 0.2431309  0.24320738 0.24355122 0.24386679
 0.24389248 0.24367326 0.24335346 0.24285522 0.24227469 0.24180526
 0.24166001 0.24131945 0.24032255 0.2390774  0.23845428 0.23799783
 0.23727003 0.23657459 0.23679632 0.23761182 0.2377131  0.23709832
 0.23662551 0.23679894 0.23697966 0.23683475 0.2366742  0.23696579
 0.23711069 0.23668078 0.23618938 0.23600921 0.23566243 0.23472005
 0.23380594 0.23375186 0.23397343 0.23339462 0.23186544 0.23071592
 0.2306982  0.23074003 0.22969377 0.22884512 0.22909847 0.22922339
 0.22811694 0.2268213  0.2264488  0.2258792  0.22442773 0.22367804
 0.2236979  0.2204961  0.21307527 0.2096841  0.21376997 0.19658579]
