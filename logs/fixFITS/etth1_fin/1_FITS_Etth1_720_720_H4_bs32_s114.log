Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16088576.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6541318
	speed: 0.1631s/iter; left time: 897.0548s
Epoch: 1 cost time: 18.29019331932068
Epoch: 1, Steps: 112 | Train Loss: 0.8338565 Vali Loss: 1.6982182 Test Loss: 0.6327417
Validation loss decreased (inf --> 1.698218).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6715356
	speed: 0.3747s/iter; left time: 2019.4868s
Epoch: 2 cost time: 18.91806435585022
Epoch: 2, Steps: 112 | Train Loss: 0.6680552 Vali Loss: 1.5766424 Test Loss: 0.5395236
Validation loss decreased (1.698218 --> 1.576642).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5623732
	speed: 0.3656s/iter; left time: 1929.4986s
Epoch: 3 cost time: 17.990602493286133
Epoch: 3, Steps: 112 | Train Loss: 0.6214363 Vali Loss: 1.5146781 Test Loss: 0.4916578
Validation loss decreased (1.576642 --> 1.514678).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5811524
	speed: 0.3842s/iter; left time: 1984.2300s
Epoch: 4 cost time: 19.734924793243408
Epoch: 4, Steps: 112 | Train Loss: 0.5972150 Vali Loss: 1.4785293 Test Loss: 0.4648194
Validation loss decreased (1.514678 --> 1.478529).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5400120
	speed: 0.3908s/iter; left time: 1974.5259s
Epoch: 5 cost time: 19.458887100219727
Epoch: 5, Steps: 112 | Train Loss: 0.5825857 Vali Loss: 1.4636365 Test Loss: 0.4497151
Validation loss decreased (1.478529 --> 1.463637).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5746675
	speed: 0.3942s/iter; left time: 1947.7522s
Epoch: 6 cost time: 19.494883060455322
Epoch: 6, Steps: 112 | Train Loss: 0.5738814 Vali Loss: 1.4465885 Test Loss: 0.4416413
Validation loss decreased (1.463637 --> 1.446589).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5963206
	speed: 0.3727s/iter; left time: 1799.7813s
Epoch: 7 cost time: 18.37879729270935
Epoch: 7, Steps: 112 | Train Loss: 0.5689378 Vali Loss: 1.4437635 Test Loss: 0.4374466
Validation loss decreased (1.446589 --> 1.443763).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5618619
	speed: 0.3673s/iter; left time: 1732.7712s
Epoch: 8 cost time: 18.127211570739746
Epoch: 8, Steps: 112 | Train Loss: 0.5647839 Vali Loss: 1.4388443 Test Loss: 0.4354942
Validation loss decreased (1.443763 --> 1.438844).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5608357
	speed: 0.3693s/iter; left time: 1700.5211s
Epoch: 9 cost time: 18.43800187110901
Epoch: 9, Steps: 112 | Train Loss: 0.5628520 Vali Loss: 1.4393429 Test Loss: 0.4348105
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5424659
	speed: 0.3673s/iter; left time: 1650.1341s
Epoch: 10 cost time: 18.243202924728394
Epoch: 10, Steps: 112 | Train Loss: 0.5610695 Vali Loss: 1.4388642 Test Loss: 0.4343854
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5133902
	speed: 0.3400s/iter; left time: 1489.5005s
Epoch: 11 cost time: 17.123353481292725
Epoch: 11, Steps: 112 | Train Loss: 0.5595702 Vali Loss: 1.4343526 Test Loss: 0.4342678
Validation loss decreased (1.438844 --> 1.434353).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5579515
	speed: 0.2992s/iter; left time: 1277.0977s
Epoch: 12 cost time: 14.76153826713562
Epoch: 12, Steps: 112 | Train Loss: 0.5588655 Vali Loss: 1.4401315 Test Loss: 0.4344428
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5417606
	speed: 0.3347s/iter; left time: 1391.4550s
Epoch: 13 cost time: 17.68230366706848
Epoch: 13, Steps: 112 | Train Loss: 0.5577254 Vali Loss: 1.4350097 Test Loss: 0.4346025
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5258076
	speed: 0.3683s/iter; left time: 1489.8027s
Epoch: 14 cost time: 19.09203600883484
Epoch: 14, Steps: 112 | Train Loss: 0.5575205 Vali Loss: 1.4357119 Test Loss: 0.4347169
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43307459354400635, mae:0.45705828070640564, rse:0.629989504814148, corr:[0.21645175 0.23281392 0.23406331 0.22971654 0.22973387 0.23165704
 0.23280486 0.23217565 0.2309288  0.23039995 0.23044138 0.23048943
 0.23017824 0.22964005 0.22906065 0.22835891 0.22766265 0.22723743
 0.22741832 0.22801158 0.22846219 0.22842632 0.2280025  0.22780514
 0.22827946 0.22906809 0.229548   0.22944015 0.22907561 0.2289621
 0.22923186 0.22952259 0.22953342 0.22912143 0.22852725 0.22809206
 0.22800508 0.22798106 0.22788167 0.22774963 0.22771758 0.22769274
 0.22771133 0.22792505 0.22837675 0.22895315 0.22940521 0.22948705
 0.22912027 0.22849943 0.22794376 0.22761673 0.22731353 0.22662368
 0.22558334 0.22449268 0.22413678 0.22429913 0.22468494 0.22493002
 0.22483867 0.22441453 0.22384347 0.22345334 0.22336778 0.22358942
 0.22395886 0.22423263 0.22426884 0.22395608 0.22329593 0.22275682
 0.22233468 0.22207536 0.22188258 0.22158901 0.22117369 0.22069621
 0.22035524 0.22020389 0.22005436 0.2197851  0.21947691 0.21925113
 0.21912448 0.2189729  0.21871127 0.21847256 0.21823256 0.21802217
 0.21784389 0.21797082 0.21834512 0.21895212 0.21961659 0.2204697
 0.22143221 0.22220294 0.22268367 0.22302307 0.22323795 0.22329102
 0.2231419  0.22290897 0.22279318 0.22268645 0.22247903 0.22211109
 0.22165973 0.22132039 0.22112513 0.22115403 0.22135253 0.22146419
 0.22150591 0.22152004 0.22154763 0.22153987 0.22150339 0.22151753
 0.22144344 0.22110957 0.22057031 0.22011678 0.2197002  0.21940419
 0.2192387  0.2191597  0.21893238 0.21840447 0.21770313 0.21711412
 0.21693118 0.21701723 0.21716745 0.21722078 0.21707302 0.21683823
 0.21669437 0.2167892  0.21704745 0.21718122 0.21698669 0.2167601
 0.21648668 0.2160766  0.21567407 0.21543287 0.21545503 0.21555239
 0.2154887  0.21530664 0.21508288 0.21485533 0.21471967 0.21467012
 0.21451052 0.21419536 0.21388641 0.21383634 0.21390168 0.21404111
 0.21415097 0.21430403 0.21445298 0.21463163 0.21460475 0.21471003
 0.21498804 0.21544279 0.21587628 0.21619853 0.21612638 0.21565928
 0.21510346 0.2149176  0.21500397 0.21502224 0.21477203 0.21436626
 0.21405599 0.21406047 0.21437246 0.2147777  0.2151152  0.2152973
 0.21540521 0.21558423 0.21583144 0.21585275 0.21567103 0.2154035
 0.215148   0.21486378 0.2145058  0.21411978 0.21377604 0.21348804
 0.21322216 0.21306914 0.21300769 0.21294977 0.21296597 0.21298125
 0.2129284  0.21283916 0.2127931  0.21287364 0.21311559 0.2133886
 0.21348266 0.21335721 0.21315649 0.21293473 0.21276198 0.21279772
 0.21292298 0.21295652 0.2128525  0.21271473 0.21254134 0.21222204
 0.21185304 0.21157533 0.21139425 0.21125971 0.2110589  0.2108629
 0.2107417  0.21071023 0.21069956 0.21061887 0.21033528 0.2099652
 0.20970966 0.2096465  0.20966294 0.20965067 0.20968752 0.20992975
 0.21045658 0.21098916 0.21128187 0.21123552 0.21090886 0.21057394
 0.21048705 0.21051234 0.21038763 0.21004568 0.20974638 0.20965007
 0.2096507  0.20942745 0.20907831 0.20881099 0.20889425 0.20925096
 0.20950839 0.20930243 0.20896061 0.20873849 0.20883721 0.20919415
 0.20950243 0.20945734 0.2090534  0.20856416 0.20812649 0.20785475
 0.2077895  0.20791723 0.20811841 0.20836309 0.20853658 0.20859592
 0.20852935 0.20853718 0.20847324 0.20835201 0.2080919  0.20779999
 0.20744482 0.20729274 0.20724835 0.2072055  0.2072751  0.2076749
 0.20834516 0.20910934 0.20975551 0.21013848 0.2102438  0.2102232
 0.2102171  0.21027674 0.21016811 0.20987222 0.20962122 0.20971675
 0.21009721 0.21040878 0.21042405 0.2101701  0.20980312 0.20961694
 0.20977572 0.20995726 0.21007437 0.21024325 0.21055257 0.21110351
 0.21162353 0.21172987 0.21144713 0.21119568 0.21106932 0.21090025
 0.21045522 0.20980194 0.2091254  0.20859283 0.2082463  0.20799536
 0.20774966 0.20761682 0.20772883 0.20817661 0.20848022 0.20840915
 0.2080203  0.20764677 0.20769684 0.20814043 0.20859528 0.20881853
 0.20867956 0.2082732  0.20797144 0.20788343 0.20779072 0.20750475
 0.20699796 0.20637462 0.20581424 0.20558736 0.20575696 0.20614149
 0.2064207  0.20647812 0.20632032 0.20616424 0.20613009 0.20642416
 0.2067534  0.20686868 0.20682494 0.20684339 0.20682107 0.20684187
 0.20675619 0.2064476  0.20616792 0.20619535 0.2063473  0.20618993
 0.20553772 0.20460445 0.20370473 0.20310159 0.2028404  0.20280458
 0.20265594 0.20233212 0.20206527 0.2019438  0.20197636 0.20193103
 0.20158488 0.2009572  0.20053907 0.20076916 0.20148544 0.20256016
 0.20347291 0.20388713 0.20382333 0.20353162 0.20326294 0.2029898
 0.20281543 0.20268005 0.20245072 0.2020862  0.2016334  0.20132893
 0.2012176  0.20116946 0.20114602 0.20104791 0.20094343 0.20100337
 0.2011937  0.20132893 0.20144278 0.20159532 0.20168895 0.20192783
 0.20227054 0.20257714 0.20281358 0.20300563 0.20316257 0.2030837
 0.2028648  0.20252427 0.20213176 0.2017084  0.20136425 0.20115916
 0.20105508 0.20113033 0.20123504 0.2014246  0.20153573 0.20144607
 0.20100956 0.20048627 0.20014921 0.2007356  0.20191316 0.2031241
 0.203803   0.20360307 0.20276946 0.20204848 0.2017721  0.2018387
 0.20206568 0.20230424 0.20223331 0.2019163  0.20151404 0.20134176
 0.2014265  0.20161131 0.20163637 0.20121394 0.2004003  0.19963257
 0.19938387 0.19971313 0.20044316 0.2012982  0.20210527 0.20290391
 0.20354727 0.20381753 0.20364083 0.20311035 0.20248948 0.20190145
 0.20150137 0.20128185 0.20106155 0.20092207 0.20087025 0.20103824
 0.2013171  0.20155495 0.20160887 0.20149429 0.20126994 0.20113523
 0.20108622 0.20112991 0.20118333 0.20114477 0.2012882  0.20150106
 0.20163217 0.20156142 0.20120548 0.20073469 0.20028707 0.19993404
 0.19968478 0.19963239 0.1994229  0.19912992 0.1989653  0.19917324
 0.19968233 0.20008045 0.20005181 0.19952658 0.19881316 0.198243
 0.19805114 0.19815125 0.19840036 0.19875799 0.19927791 0.19999799
 0.20068109 0.20115519 0.20137918 0.20159398 0.20191269 0.20196624
 0.20156924 0.20086506 0.20001514 0.19946364 0.19959308 0.20029606
 0.20094669 0.20115234 0.20117913 0.20121728 0.20160247 0.20215826
 0.2025479  0.20261666 0.2027076  0.20247558 0.20261656 0.20263782
 0.2030244  0.20353192 0.20398729 0.20440547 0.2043465  0.2039357
 0.20329586 0.2028033  0.20225675 0.20164806 0.20097488 0.20058396
 0.2005644  0.2007574  0.20113012 0.2013768  0.20154493 0.20159206
 0.2015372  0.20133318 0.20093988 0.20058224 0.20054165 0.2009466
 0.20147593 0.20169368 0.20119989 0.2002875  0.1995386  0.19918466
 0.19889478 0.1982475  0.19697404 0.195691   0.19506298 0.19525248
 0.1955482  0.1951658  0.1944469  0.1936771  0.19374402 0.19446218
 0.19532755 0.19549228 0.19518624 0.19478066 0.19458126 0.1944153
 0.19396944 0.19323686 0.1923641  0.19215147 0.1925685  0.19295776
 0.19313478 0.19282335 0.19232665 0.1917181  0.19128413 0.1910124
 0.19105914 0.19125225 0.19172756 0.19208685 0.1919002  0.19103672
 0.19000523 0.18959697 0.19015354 0.19119795 0.19182736 0.19170515
 0.19113904 0.19049425 0.18988581 0.18921143 0.18826151 0.18703489
 0.18602814 0.18553592 0.1850867  0.18421383 0.18281421 0.18156713
 0.18107891 0.18144321 0.18235922 0.18316545 0.18364842 0.18396352
 0.18432315 0.18481088 0.18543124 0.18612568 0.18682139 0.18710732
 0.18650068 0.18502714 0.18330137 0.18235047 0.18235622 0.1826689
 0.18259975 0.18179147 0.18055952 0.17973745 0.17974393 0.18035616
 0.18075177 0.18034738 0.17982394 0.17985141 0.18042627 0.18106501
 0.18127504 0.18126161 0.18137582 0.18159944 0.18164732 0.18126006
 0.18045406 0.17971495 0.1792269  0.17906086 0.17850518 0.1774474
 0.17646547 0.17605802 0.1759931  0.17522588 0.17355768 0.17196801
 0.17149626 0.17221437 0.17350382 0.17420228 0.17421398 0.17426868
 0.17482641 0.17571841 0.17606859 0.17553717 0.17440057 0.1732969
 0.17196205 0.17013052 0.16779082 0.16572866 0.16485013 0.16482893
 0.16531265 0.16518497 0.164358   0.1634286  0.16357988 0.16466975
 0.16508621 0.16390538 0.16237222 0.16164346 0.16191168 0.1624065
 0.16250242 0.1627126  0.16372423 0.16502942 0.16535397 0.16393766
 0.16140771 0.1598422  0.15973325 0.16055903 0.16026811 0.15820357
 0.15609938 0.15530129 0.15550542 0.15503545 0.15355614 0.15213433
 0.15203987 0.1526428  0.15264401 0.15128031 0.15049581 0.1516464
 0.15315741 0.15079212 0.14434677 0.14158458 0.14969128 0.15295145]
