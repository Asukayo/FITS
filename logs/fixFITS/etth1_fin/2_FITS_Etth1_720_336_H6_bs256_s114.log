Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  201607168.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.8937108516693115
Epoch: 1, Steps: 14 | Train Loss: 0.7881843 Vali Loss: 2.0237093 Test Loss: 0.9765151
Validation loss decreased (inf --> 2.023709).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.9672694206237793
Epoch: 2, Steps: 14 | Train Loss: 0.7149919 Vali Loss: 1.9126215 Test Loss: 0.9173902
Validation loss decreased (2.023709 --> 1.912621).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.044123888015747
Epoch: 3, Steps: 14 | Train Loss: 0.6605009 Vali Loss: 1.8252157 Test Loss: 0.8723683
Validation loss decreased (1.912621 --> 1.825216).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.1021156311035156
Epoch: 4, Steps: 14 | Train Loss: 0.6185387 Vali Loss: 1.7519964 Test Loss: 0.8385946
Validation loss decreased (1.825216 --> 1.751996).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.0066640377044678
Epoch: 5, Steps: 14 | Train Loss: 0.5862915 Vali Loss: 1.7126025 Test Loss: 0.8124356
Validation loss decreased (1.751996 --> 1.712602).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.989107847213745
Epoch: 6, Steps: 14 | Train Loss: 0.5620100 Vali Loss: 1.6709124 Test Loss: 0.7922319
Validation loss decreased (1.712602 --> 1.670912).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.191427230834961
Epoch: 7, Steps: 14 | Train Loss: 0.5415495 Vali Loss: 1.6392920 Test Loss: 0.7771131
Validation loss decreased (1.670912 --> 1.639292).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.096501111984253
Epoch: 8, Steps: 14 | Train Loss: 0.5256652 Vali Loss: 1.6174279 Test Loss: 0.7649647
Validation loss decreased (1.639292 --> 1.617428).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.02911639213562
Epoch: 9, Steps: 14 | Train Loss: 0.5120872 Vali Loss: 1.6128674 Test Loss: 0.7558330
Validation loss decreased (1.617428 --> 1.612867).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.0077626705169678
Epoch: 10, Steps: 14 | Train Loss: 0.5012147 Vali Loss: 1.5894042 Test Loss: 0.7493836
Validation loss decreased (1.612867 --> 1.589404).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.395718812942505
Epoch: 11, Steps: 14 | Train Loss: 0.4918066 Vali Loss: 1.5741485 Test Loss: 0.7434337
Validation loss decreased (1.589404 --> 1.574149).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.1622862815856934
Epoch: 12, Steps: 14 | Train Loss: 0.4828953 Vali Loss: 1.5676786 Test Loss: 0.7383351
Validation loss decreased (1.574149 --> 1.567679).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.0135910511016846
Epoch: 13, Steps: 14 | Train Loss: 0.4756208 Vali Loss: 1.5660279 Test Loss: 0.7347586
Validation loss decreased (1.567679 --> 1.566028).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.0655667781829834
Epoch: 14, Steps: 14 | Train Loss: 0.4703949 Vali Loss: 1.5446675 Test Loss: 0.7309088
Validation loss decreased (1.566028 --> 1.544667).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.9952704906463623
Epoch: 15, Steps: 14 | Train Loss: 0.4647766 Vali Loss: 1.5440449 Test Loss: 0.7280443
Validation loss decreased (1.544667 --> 1.544045).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.9576263427734375
Epoch: 16, Steps: 14 | Train Loss: 0.4595491 Vali Loss: 1.5283960 Test Loss: 0.7256155
Validation loss decreased (1.544045 --> 1.528396).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.9448671340942383
Epoch: 17, Steps: 14 | Train Loss: 0.4540377 Vali Loss: 1.5290380 Test Loss: 0.7236707
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.0996737480163574
Epoch: 18, Steps: 14 | Train Loss: 0.4496690 Vali Loss: 1.5262889 Test Loss: 0.7218952
Validation loss decreased (1.528396 --> 1.526289).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.0935919284820557
Epoch: 19, Steps: 14 | Train Loss: 0.4459780 Vali Loss: 1.5155905 Test Loss: 0.7200801
Validation loss decreased (1.526289 --> 1.515591).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.32772159576416
Epoch: 20, Steps: 14 | Train Loss: 0.4428859 Vali Loss: 1.5132966 Test Loss: 0.7179924
Validation loss decreased (1.515591 --> 1.513297).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.987647294998169
Epoch: 21, Steps: 14 | Train Loss: 0.4389329 Vali Loss: 1.5056336 Test Loss: 0.7169858
Validation loss decreased (1.513297 --> 1.505634).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.5767264366149902
Epoch: 22, Steps: 14 | Train Loss: 0.4365481 Vali Loss: 1.5072725 Test Loss: 0.7154665
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.7184324264526367
Epoch: 23, Steps: 14 | Train Loss: 0.4341044 Vali Loss: 1.4972223 Test Loss: 0.7142491
Validation loss decreased (1.505634 --> 1.497222).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.6391873359680176
Epoch: 24, Steps: 14 | Train Loss: 0.4310301 Vali Loss: 1.5086517 Test Loss: 0.7132549
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.682157039642334
Epoch: 25, Steps: 14 | Train Loss: 0.4284406 Vali Loss: 1.5066110 Test Loss: 0.7120711
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.6702659130096436
Epoch: 26, Steps: 14 | Train Loss: 0.4264514 Vali Loss: 1.4970071 Test Loss: 0.7112172
Validation loss decreased (1.497222 --> 1.497007).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.9003894329071045
Epoch: 27, Steps: 14 | Train Loss: 0.4241396 Vali Loss: 1.4955980 Test Loss: 0.7103330
Validation loss decreased (1.497007 --> 1.495598).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.1055026054382324
Epoch: 28, Steps: 14 | Train Loss: 0.4226895 Vali Loss: 1.5045582 Test Loss: 0.7096262
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.1633288860321045
Epoch: 29, Steps: 14 | Train Loss: 0.4202624 Vali Loss: 1.5072916 Test Loss: 0.7086920
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.3809523582458496
Epoch: 30, Steps: 14 | Train Loss: 0.4180016 Vali Loss: 1.4813805 Test Loss: 0.7078780
Validation loss decreased (1.495598 --> 1.481380).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.5534610748291016
Epoch: 31, Steps: 14 | Train Loss: 0.4169505 Vali Loss: 1.4896076 Test Loss: 0.7072928
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.436494827270508
Epoch: 32, Steps: 14 | Train Loss: 0.4154236 Vali Loss: 1.4927742 Test Loss: 0.7066860
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.0361990928649902
Epoch: 33, Steps: 14 | Train Loss: 0.4137781 Vali Loss: 1.4955266 Test Loss: 0.7060773
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  201607168.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.3405773639678955
Epoch: 1, Steps: 14 | Train Loss: 0.6194337 Vali Loss: 1.4416567 Test Loss: 0.6677473
Validation loss decreased (inf --> 1.441657).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.3148345947265625
Epoch: 2, Steps: 14 | Train Loss: 0.5926481 Vali Loss: 1.3905418 Test Loss: 0.6363755
Validation loss decreased (1.441657 --> 1.390542).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.3712093830108643
Epoch: 3, Steps: 14 | Train Loss: 0.5727468 Vali Loss: 1.3794382 Test Loss: 0.6116555
Validation loss decreased (1.390542 --> 1.379438).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.2284493446350098
Epoch: 4, Steps: 14 | Train Loss: 0.5570123 Vali Loss: 1.3463405 Test Loss: 0.5913734
Validation loss decreased (1.379438 --> 1.346341).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.4216482639312744
Epoch: 5, Steps: 14 | Train Loss: 0.5430621 Vali Loss: 1.3232534 Test Loss: 0.5746807
Validation loss decreased (1.346341 --> 1.323253).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.3210527896881104
Epoch: 6, Steps: 14 | Train Loss: 0.5322809 Vali Loss: 1.3083942 Test Loss: 0.5604523
Validation loss decreased (1.323253 --> 1.308394).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.402804374694824
Epoch: 7, Steps: 14 | Train Loss: 0.5237194 Vali Loss: 1.2905246 Test Loss: 0.5481852
Validation loss decreased (1.308394 --> 1.290525).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.322882652282715
Epoch: 8, Steps: 14 | Train Loss: 0.5153828 Vali Loss: 1.2796477 Test Loss: 0.5377057
Validation loss decreased (1.290525 --> 1.279648).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.5421695709228516
Epoch: 9, Steps: 14 | Train Loss: 0.5083906 Vali Loss: 1.2673826 Test Loss: 0.5285343
Validation loss decreased (1.279648 --> 1.267383).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.434542655944824
Epoch: 10, Steps: 14 | Train Loss: 0.5028613 Vali Loss: 1.2647153 Test Loss: 0.5205275
Validation loss decreased (1.267383 --> 1.264715).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.587404727935791
Epoch: 11, Steps: 14 | Train Loss: 0.4959296 Vali Loss: 1.2487129 Test Loss: 0.5135427
Validation loss decreased (1.264715 --> 1.248713).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.514338970184326
Epoch: 12, Steps: 14 | Train Loss: 0.4927371 Vali Loss: 1.2446830 Test Loss: 0.5072913
Validation loss decreased (1.248713 --> 1.244683).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.5171711444854736
Epoch: 13, Steps: 14 | Train Loss: 0.4889304 Vali Loss: 1.2410107 Test Loss: 0.5017856
Validation loss decreased (1.244683 --> 1.241011).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.2567033767700195
Epoch: 14, Steps: 14 | Train Loss: 0.4861702 Vali Loss: 1.2418879 Test Loss: 0.4968816
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.5872514247894287
Epoch: 15, Steps: 14 | Train Loss: 0.4803329 Vali Loss: 1.2276962 Test Loss: 0.4924929
Validation loss decreased (1.241011 --> 1.227696).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.5986168384552
Epoch: 16, Steps: 14 | Train Loss: 0.4790637 Vali Loss: 1.2247154 Test Loss: 0.4884773
Validation loss decreased (1.227696 --> 1.224715).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.6635053157806396
Epoch: 17, Steps: 14 | Train Loss: 0.4759292 Vali Loss: 1.2269374 Test Loss: 0.4849015
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.548316717147827
Epoch: 18, Steps: 14 | Train Loss: 0.4739090 Vali Loss: 1.2182678 Test Loss: 0.4816751
Validation loss decreased (1.224715 --> 1.218268).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.895606756210327
Epoch: 19, Steps: 14 | Train Loss: 0.4712993 Vali Loss: 1.2130899 Test Loss: 0.4788273
Validation loss decreased (1.218268 --> 1.213090).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.5504698753356934
Epoch: 20, Steps: 14 | Train Loss: 0.4700167 Vali Loss: 1.2044724 Test Loss: 0.4762713
Validation loss decreased (1.213090 --> 1.204472).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.5100369453430176
Epoch: 21, Steps: 14 | Train Loss: 0.4691493 Vali Loss: 1.2069519 Test Loss: 0.4738745
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.529888391494751
Epoch: 22, Steps: 14 | Train Loss: 0.4660592 Vali Loss: 1.1973431 Test Loss: 0.4717307
Validation loss decreased (1.204472 --> 1.197343).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.551718235015869
Epoch: 23, Steps: 14 | Train Loss: 0.4655523 Vali Loss: 1.1962545 Test Loss: 0.4697381
Validation loss decreased (1.197343 --> 1.196254).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.598538398742676
Epoch: 24, Steps: 14 | Train Loss: 0.4635912 Vali Loss: 1.1910315 Test Loss: 0.4679475
Validation loss decreased (1.196254 --> 1.191031).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2512526512145996
Epoch: 25, Steps: 14 | Train Loss: 0.4619120 Vali Loss: 1.2001544 Test Loss: 0.4663104
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.070369243621826
Epoch: 26, Steps: 14 | Train Loss: 0.4615630 Vali Loss: 1.2037004 Test Loss: 0.4648117
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.057251214981079
Epoch: 27, Steps: 14 | Train Loss: 0.4593731 Vali Loss: 1.1890038 Test Loss: 0.4634697
Validation loss decreased (1.191031 --> 1.189004).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.11336350440979
Epoch: 28, Steps: 14 | Train Loss: 0.4598123 Vali Loss: 1.2013135 Test Loss: 0.4622512
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.024303674697876
Epoch: 29, Steps: 14 | Train Loss: 0.4587386 Vali Loss: 1.1892760 Test Loss: 0.4610975
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.1254355907440186
Epoch: 30, Steps: 14 | Train Loss: 0.4579580 Vali Loss: 1.2004439 Test Loss: 0.4600441
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.46252429485321045, mae:0.4584615230560303, rse:0.6474695205688477, corr:[0.24892727 0.25715348 0.25262612 0.25749958 0.2569689  0.25234628
 0.2515933  0.2534823  0.253459   0.2519852  0.25129378 0.25134003
 0.25104272 0.25000164 0.24878086 0.24834117 0.24849361 0.24803512
 0.2465631  0.24557437 0.24570851 0.24540202 0.2443866  0.24424696
 0.24502435 0.2449674  0.24437112 0.2446803  0.24556878 0.24560542
 0.2448525  0.2443425  0.24441044 0.24428579 0.24363662 0.24310113
 0.24295227 0.2429565  0.24291213 0.2425678  0.242348   0.24263133
 0.24314009 0.24306941 0.24255167 0.24275401 0.24370198 0.24414541
 0.24400623 0.24412662 0.24391964 0.24294549 0.24147281 0.24020578
 0.23960757 0.23903707 0.23815832 0.23753352 0.23728958 0.23715878
 0.23651792 0.23585948 0.23580617 0.23615259 0.2362622  0.2363241
 0.23679397 0.23713316 0.23706467 0.23673505 0.23663801 0.23683462
 0.23661281 0.23577023 0.23500265 0.23496638 0.23479532 0.2341238
 0.23330295 0.23285012 0.23268783 0.23232326 0.2315912  0.23105869
 0.23088534 0.23070629 0.23011637 0.22935179 0.22907138 0.22931737
 0.22923724 0.2286208  0.22807217 0.2281395  0.22816135 0.22832784
 0.22929685 0.23065877 0.2314916  0.23173884 0.23168087 0.23147461
 0.23132189 0.23106556 0.23065165 0.2303368  0.23002703 0.22961065
 0.2291625  0.22898538 0.22928682 0.22971787 0.22982015 0.22984058
 0.23007123 0.23028506 0.23009829 0.22964528 0.22947012 0.22977544
 0.22994964 0.22943464 0.22856449 0.22799698 0.22742666 0.22642596
 0.2257776  0.22578526 0.22563516 0.22493227 0.22411534 0.22392781
 0.22417615 0.22409518 0.2236079  0.223338   0.22356796 0.22389387
 0.22372623 0.22336811 0.2234628  0.22385776 0.22350805 0.2225929
 0.22208908 0.22189161 0.22135267 0.2203005  0.21922098 0.218534
 0.21847531 0.21846147 0.21829697 0.21828371 0.2183612  0.21834607
 0.21798116 0.21763192 0.21758762 0.2175532  0.21726802 0.21732086
 0.21785821 0.21820444 0.2177955  0.2173351  0.2171921  0.21695052
 0.21629564 0.21598084 0.21644762 0.21707667 0.21678829 0.2159061
 0.21550854 0.21577416 0.21562375 0.21517421 0.21494433 0.21505523
 0.21501939 0.21489671 0.21491402 0.21506262 0.21537831 0.21588641
 0.21626128 0.21652661 0.21708326 0.21786678 0.21808258 0.217612
 0.21721868 0.21708708 0.21650466 0.21522564 0.21411344 0.21359786
 0.2131719  0.21256179 0.21212682 0.2123203  0.2124465  0.21240224
 0.21236803 0.2131265  0.21411636 0.21454214 0.21441266 0.21422137
 0.21463852 0.21520877 0.21505813 0.21429478 0.21381313 0.21369098
 0.21327974 0.21266374 0.21237066 0.21254618 0.21256892 0.21181001
 0.21139438 0.21177478 0.21200779 0.21161449 0.21128954 0.21162456
 0.21182159 0.21183316 0.21201096 0.21208283 0.21213144 0.21259765
 0.21269152 0.21203777 0.21162878 0.21196523 0.212195   0.21171872
 0.21176219 0.21240038 0.2127982  0.21249755 0.21224597 0.21234258
 0.21228498 0.21212097 0.21234487 0.21268602 0.21260817 0.21231402
 0.21200924 0.2116683  0.21193746 0.2126409  0.21304615 0.21294554
 0.21338943 0.21407121 0.21398237 0.21321294 0.21348898 0.21427754
 0.21410923 0.21334097 0.2130895  0.21308656 0.21203466 0.21083148
 0.21094488 0.2112358  0.21076554 0.2105795  0.21080643 0.21082753
 0.21065481 0.21078233 0.21097219 0.21102998 0.21145575 0.21219346
 0.2120811  0.21135946 0.21154894 0.21183719 0.21099721 0.21013692
 0.21075284 0.21180633 0.21132843 0.21047118 0.2104511  0.21071817
 0.21032812 0.2102933  0.21082765 0.21096002 0.21071894 0.21117796
 0.21154016 0.2111142  0.2115629  0.2122919  0.21217425 0.21170571
 0.21248944 0.21251719 0.21118768 0.2104303  0.21148913 0.21143772
 0.20940153 0.20839569 0.209258   0.20865238 0.20631075 0.20527342
 0.20581797 0.20463468 0.20325015 0.20397726 0.20394601 0.20270799
 0.20308435 0.20332229 0.20090438 0.20031583 0.20165879 0.1988734
 0.19389623 0.19528879 0.19438644 0.18002333 0.18105125 0.19260955]
