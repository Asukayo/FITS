Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  137682944.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.7124924659729
Epoch: 1, Steps: 28 | Train Loss: 0.8843349 Vali Loss: 2.2658143 Test Loss: 0.9829924
Validation loss decreased (inf --> 2.265814).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.586099147796631
Epoch: 2, Steps: 28 | Train Loss: 0.7424152 Vali Loss: 2.0605195 Test Loss: 0.8656372
Validation loss decreased (2.265814 --> 2.060519).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.084142446517944
Epoch: 3, Steps: 28 | Train Loss: 0.6588006 Vali Loss: 1.9467872 Test Loss: 0.7986857
Validation loss decreased (2.060519 --> 1.946787).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.485211133956909
Epoch: 4, Steps: 28 | Train Loss: 0.6084410 Vali Loss: 1.8798118 Test Loss: 0.7602837
Validation loss decreased (1.946787 --> 1.879812).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.072524070739746
Epoch: 5, Steps: 28 | Train Loss: 0.5763800 Vali Loss: 1.8411591 Test Loss: 0.7372928
Validation loss decreased (1.879812 --> 1.841159).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.4749462604522705
Epoch: 6, Steps: 28 | Train Loss: 0.5540412 Vali Loss: 1.8154504 Test Loss: 0.7224418
Validation loss decreased (1.841159 --> 1.815450).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.786821365356445
Epoch: 7, Steps: 28 | Train Loss: 0.5377155 Vali Loss: 1.7926998 Test Loss: 0.7114258
Validation loss decreased (1.815450 --> 1.792700).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.773416519165039
Epoch: 8, Steps: 28 | Train Loss: 0.5248682 Vali Loss: 1.7832747 Test Loss: 0.7035528
Validation loss decreased (1.792700 --> 1.783275).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.605649471282959
Epoch: 9, Steps: 28 | Train Loss: 0.5142596 Vali Loss: 1.7657518 Test Loss: 0.6966894
Validation loss decreased (1.783275 --> 1.765752).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.193670034408569
Epoch: 10, Steps: 28 | Train Loss: 0.5054397 Vali Loss: 1.7617381 Test Loss: 0.6908260
Validation loss decreased (1.765752 --> 1.761738).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.451594591140747
Epoch: 11, Steps: 28 | Train Loss: 0.4972448 Vali Loss: 1.7530448 Test Loss: 0.6856900
Validation loss decreased (1.761738 --> 1.753045).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.007459878921509
Epoch: 12, Steps: 28 | Train Loss: 0.4902424 Vali Loss: 1.7457378 Test Loss: 0.6812900
Validation loss decreased (1.753045 --> 1.745738).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.170476675033569
Epoch: 13, Steps: 28 | Train Loss: 0.4837084 Vali Loss: 1.7423718 Test Loss: 0.6768374
Validation loss decreased (1.745738 --> 1.742372).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.21730637550354
Epoch: 14, Steps: 28 | Train Loss: 0.4780888 Vali Loss: 1.7335982 Test Loss: 0.6720683
Validation loss decreased (1.742372 --> 1.733598).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.099860429763794
Epoch: 15, Steps: 28 | Train Loss: 0.4728219 Vali Loss: 1.7305902 Test Loss: 0.6683575
Validation loss decreased (1.733598 --> 1.730590).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.1601722240448
Epoch: 16, Steps: 28 | Train Loss: 0.4678247 Vali Loss: 1.7232518 Test Loss: 0.6653399
Validation loss decreased (1.730590 --> 1.723252).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.9491188526153564
Epoch: 17, Steps: 28 | Train Loss: 0.4636435 Vali Loss: 1.7134584 Test Loss: 0.6614851
Validation loss decreased (1.723252 --> 1.713458).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.4026899337768555
Epoch: 18, Steps: 28 | Train Loss: 0.4594380 Vali Loss: 1.7143720 Test Loss: 0.6585730
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.230703830718994
Epoch: 19, Steps: 28 | Train Loss: 0.4555915 Vali Loss: 1.7050372 Test Loss: 0.6554669
Validation loss decreased (1.713458 --> 1.705037).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.350828170776367
Epoch: 20, Steps: 28 | Train Loss: 0.4518646 Vali Loss: 1.7076278 Test Loss: 0.6525320
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.296199083328247
Epoch: 21, Steps: 28 | Train Loss: 0.4486309 Vali Loss: 1.7000487 Test Loss: 0.6500909
Validation loss decreased (1.705037 --> 1.700049).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.333275556564331
Epoch: 22, Steps: 28 | Train Loss: 0.4456306 Vali Loss: 1.6990776 Test Loss: 0.6470839
Validation loss decreased (1.700049 --> 1.699078).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 5.156932353973389
Epoch: 23, Steps: 28 | Train Loss: 0.4424272 Vali Loss: 1.6971622 Test Loss: 0.6451627
Validation loss decreased (1.699078 --> 1.697162).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 5.303811311721802
Epoch: 24, Steps: 28 | Train Loss: 0.4398890 Vali Loss: 1.6898445 Test Loss: 0.6426618
Validation loss decreased (1.697162 --> 1.689844).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 5.217641830444336
Epoch: 25, Steps: 28 | Train Loss: 0.4374478 Vali Loss: 1.6875489 Test Loss: 0.6404610
Validation loss decreased (1.689844 --> 1.687549).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 5.3332366943359375
Epoch: 26, Steps: 28 | Train Loss: 0.4351822 Vali Loss: 1.6912215 Test Loss: 0.6384938
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 5.256836414337158
Epoch: 27, Steps: 28 | Train Loss: 0.4327432 Vali Loss: 1.6809731 Test Loss: 0.6364862
Validation loss decreased (1.687549 --> 1.680973).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 5.086730480194092
Epoch: 28, Steps: 28 | Train Loss: 0.4310810 Vali Loss: 1.6839335 Test Loss: 0.6346690
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 5.012612581253052
Epoch: 29, Steps: 28 | Train Loss: 0.4289378 Vali Loss: 1.6810228 Test Loss: 0.6329186
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.897950172424316
Epoch: 30, Steps: 28 | Train Loss: 0.4270886 Vali Loss: 1.6747248 Test Loss: 0.6312859
Validation loss decreased (1.680973 --> 1.674725).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 5.345482587814331
Epoch: 31, Steps: 28 | Train Loss: 0.4253349 Vali Loss: 1.6750150 Test Loss: 0.6297516
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 5.185342788696289
Epoch: 32, Steps: 28 | Train Loss: 0.4237191 Vali Loss: 1.6768781 Test Loss: 0.6282651
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 5.2080793380737305
Epoch: 33, Steps: 28 | Train Loss: 0.4221671 Vali Loss: 1.6726708 Test Loss: 0.6269487
Validation loss decreased (1.674725 --> 1.672671).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 5.561013460159302
Epoch: 34, Steps: 28 | Train Loss: 0.4209659 Vali Loss: 1.6690114 Test Loss: 0.6257145
Validation loss decreased (1.672671 --> 1.669011).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 5.249369382858276
Epoch: 35, Steps: 28 | Train Loss: 0.4195532 Vali Loss: 1.6650772 Test Loss: 0.6243532
Validation loss decreased (1.669011 --> 1.665077).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 5.388973712921143
Epoch: 36, Steps: 28 | Train Loss: 0.4182731 Vali Loss: 1.6658119 Test Loss: 0.6231387
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 5.0153114795684814
Epoch: 37, Steps: 28 | Train Loss: 0.4170626 Vali Loss: 1.6638212 Test Loss: 0.6222207
Validation loss decreased (1.665077 --> 1.663821).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 5.33011531829834
Epoch: 38, Steps: 28 | Train Loss: 0.4157834 Vali Loss: 1.6669308 Test Loss: 0.6210554
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 5.406520128250122
Epoch: 39, Steps: 28 | Train Loss: 0.4147335 Vali Loss: 1.6606851 Test Loss: 0.6199625
Validation loss decreased (1.663821 --> 1.660685).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 5.325391530990601
Epoch: 40, Steps: 28 | Train Loss: 0.4136275 Vali Loss: 1.6650496 Test Loss: 0.6189882
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 5.154076814651489
Epoch: 41, Steps: 28 | Train Loss: 0.4128795 Vali Loss: 1.6643555 Test Loss: 0.6180570
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 5.316554069519043
Epoch: 42, Steps: 28 | Train Loss: 0.4118251 Vali Loss: 1.6550586 Test Loss: 0.6173217
Validation loss decreased (1.660685 --> 1.655059).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.7315967082977295
Epoch: 43, Steps: 28 | Train Loss: 0.4110792 Vali Loss: 1.6587780 Test Loss: 0.6165290
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.833359479904175
Epoch: 44, Steps: 28 | Train Loss: 0.4101498 Vali Loss: 1.6576002 Test Loss: 0.6157250
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.445852279663086
Epoch: 45, Steps: 28 | Train Loss: 0.4094278 Vali Loss: 1.6543347 Test Loss: 0.6149932
Validation loss decreased (1.655059 --> 1.654335).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.329172372817993
Epoch: 46, Steps: 28 | Train Loss: 0.4087184 Vali Loss: 1.6577425 Test Loss: 0.6142517
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.4577836990356445
Epoch: 47, Steps: 28 | Train Loss: 0.4081427 Vali Loss: 1.6537950 Test Loss: 0.6136153
Validation loss decreased (1.654335 --> 1.653795).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.6970999240875244
Epoch: 48, Steps: 28 | Train Loss: 0.4073752 Vali Loss: 1.6552951 Test Loss: 0.6130239
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.654729127883911
Epoch: 49, Steps: 28 | Train Loss: 0.4067522 Vali Loss: 1.6543809 Test Loss: 0.6123343
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.136048316955566
Epoch: 50, Steps: 28 | Train Loss: 0.4062394 Vali Loss: 1.6554763 Test Loss: 0.6118143
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  137682944.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.23100471496582
Epoch: 1, Steps: 28 | Train Loss: 0.6486464 Vali Loss: 1.6133108 Test Loss: 0.5823603
Validation loss decreased (inf --> 1.613311).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.773269414901733
Epoch: 2, Steps: 28 | Train Loss: 0.6313093 Vali Loss: 1.5825257 Test Loss: 0.5569931
Validation loss decreased (1.613311 --> 1.582526).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.444595813751221
Epoch: 3, Steps: 28 | Train Loss: 0.6177885 Vali Loss: 1.5613817 Test Loss: 0.5365233
Validation loss decreased (1.582526 --> 1.561382).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.923964500427246
Epoch: 4, Steps: 28 | Train Loss: 0.6071710 Vali Loss: 1.5383501 Test Loss: 0.5200074
Validation loss decreased (1.561382 --> 1.538350).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.71150279045105
Epoch: 5, Steps: 28 | Train Loss: 0.5990289 Vali Loss: 1.5198171 Test Loss: 0.5065843
Validation loss decreased (1.538350 --> 1.519817).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.984756231307983
Epoch: 6, Steps: 28 | Train Loss: 0.5921046 Vali Loss: 1.5132426 Test Loss: 0.4953990
Validation loss decreased (1.519817 --> 1.513243).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.455944776535034
Epoch: 7, Steps: 28 | Train Loss: 0.5861755 Vali Loss: 1.4980534 Test Loss: 0.4862013
Validation loss decreased (1.513243 --> 1.498053).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.81616735458374
Epoch: 8, Steps: 28 | Train Loss: 0.5815067 Vali Loss: 1.4870903 Test Loss: 0.4785579
Validation loss decreased (1.498053 --> 1.487090).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.872503042221069
Epoch: 9, Steps: 28 | Train Loss: 0.5776790 Vali Loss: 1.4773625 Test Loss: 0.4722901
Validation loss decreased (1.487090 --> 1.477363).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.943022966384888
Epoch: 10, Steps: 28 | Train Loss: 0.5740879 Vali Loss: 1.4717181 Test Loss: 0.4670958
Validation loss decreased (1.477363 --> 1.471718).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.606415748596191
Epoch: 11, Steps: 28 | Train Loss: 0.5711804 Vali Loss: 1.4685084 Test Loss: 0.4625376
Validation loss decreased (1.471718 --> 1.468508).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 6.14268946647644
Epoch: 12, Steps: 28 | Train Loss: 0.5689664 Vali Loss: 1.4608982 Test Loss: 0.4588953
Validation loss decreased (1.468508 --> 1.460898).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.760982990264893
Epoch: 13, Steps: 28 | Train Loss: 0.5667310 Vali Loss: 1.4579060 Test Loss: 0.4556830
Validation loss decreased (1.460898 --> 1.457906).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.875011920928955
Epoch: 14, Steps: 28 | Train Loss: 0.5650582 Vali Loss: 1.4569185 Test Loss: 0.4531534
Validation loss decreased (1.457906 --> 1.456918).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.709824323654175
Epoch: 15, Steps: 28 | Train Loss: 0.5636800 Vali Loss: 1.4494928 Test Loss: 0.4508117
Validation loss decreased (1.456918 --> 1.449493).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 6.057540416717529
Epoch: 16, Steps: 28 | Train Loss: 0.5619918 Vali Loss: 1.4483529 Test Loss: 0.4489955
Validation loss decreased (1.449493 --> 1.448353).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.84752893447876
Epoch: 17, Steps: 28 | Train Loss: 0.5610811 Vali Loss: 1.4497192 Test Loss: 0.4473584
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.65209436416626
Epoch: 18, Steps: 28 | Train Loss: 0.5601927 Vali Loss: 1.4383585 Test Loss: 0.4459791
Validation loss decreased (1.448353 --> 1.438359).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.517218828201294
Epoch: 19, Steps: 28 | Train Loss: 0.5592407 Vali Loss: 1.4440320 Test Loss: 0.4449728
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.713833808898926
Epoch: 20, Steps: 28 | Train Loss: 0.5581720 Vali Loss: 1.4426367 Test Loss: 0.4437906
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.733309507369995
Epoch: 21, Steps: 28 | Train Loss: 0.5570700 Vali Loss: 1.4411945 Test Loss: 0.4430422
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.437575101852417, mae:0.46427202224731445, rse:0.6332544684410095, corr:[0.2195056  0.22602764 0.22394893 0.23119079 0.23178972 0.22785161
 0.22833474 0.23170835 0.23201685 0.23079194 0.23038046 0.23079893
 0.23065145 0.22906686 0.22748727 0.22739558 0.22785772 0.22703688
 0.2256506  0.22567408 0.22648591 0.2262733  0.22539712 0.22617947
 0.22762002 0.22784962 0.22745988 0.22794008 0.22869632 0.22833675
 0.22746134 0.22717756 0.22739542 0.22719522 0.22630283 0.22545773
 0.22534235 0.22545986 0.22523914 0.22459503 0.22448365 0.22496006
 0.225254   0.22503291 0.22512232 0.2259427  0.2266472  0.22681558
 0.22707225 0.2277174  0.22759181 0.22654058 0.22547151 0.22464095
 0.22400312 0.22313155 0.2224743  0.22220577 0.22174144 0.22131008
 0.22092624 0.22065538 0.2205718  0.22056875 0.22021109 0.21979147
 0.22018902 0.22105795 0.22137508 0.22103208 0.22076964 0.2211624
 0.2211569  0.22061929 0.22018117 0.22032697 0.2201491  0.21938954
 0.21884812 0.21870333 0.21832998 0.21763235 0.2169541  0.21665536
 0.21658145 0.21624492 0.21560629 0.21507894 0.21491414 0.21488866
 0.21450487 0.21420945 0.21457157 0.21523888 0.21542455 0.21591404
 0.21737218 0.2188616  0.21945281 0.2196972  0.22013468 0.22047716
 0.22026622 0.21979763 0.21951486 0.21933618 0.21889827 0.21836385
 0.2181808  0.2181925  0.21817487 0.21796611 0.21761698 0.21744451
 0.2178984  0.21850985 0.21864328 0.2183723  0.2183415  0.21870105
 0.21888071 0.21857958 0.21812972 0.21792188 0.21735509 0.21659002
 0.21645659 0.21673538 0.21639387 0.21568507 0.21529296 0.2151477
 0.21484774 0.21433534 0.21408811 0.21423022 0.21453547 0.21469773
 0.21460147 0.21433519 0.21431483 0.2145014  0.2142226  0.21373186
 0.21363185 0.21370356 0.21350114 0.21289736 0.21246044 0.21222955
 0.21181293 0.21133733 0.21133989 0.211719   0.21172613 0.21130258
 0.2109074  0.21070825 0.21053164 0.21028972 0.21019785 0.21043237
 0.21077117 0.2108664  0.21066406 0.21061209 0.2105277  0.21055205
 0.21062219 0.21104024 0.2118805  0.21275373 0.21281652 0.21231952
 0.21206516 0.21202612 0.2114972  0.21083601 0.2106008  0.21062967
 0.21043412 0.21022898 0.21046713 0.21083254 0.21094419 0.2109877
 0.21130311 0.21173535 0.21202755 0.21204615 0.21197754 0.21210496
 0.21225263 0.2119317  0.21099836 0.21016651 0.21003467 0.21008289
 0.20956837 0.20885976 0.20865785 0.20883459 0.20867492 0.20836766
 0.20842691 0.20887902 0.20913072 0.20915388 0.20941654 0.20970914
 0.20951313 0.20877552 0.20832133 0.20850576 0.20877214 0.20873006
 0.20850341 0.20856135 0.2088949  0.20909397 0.20887166 0.20854801
 0.20863727 0.20863625 0.20801789 0.2073356  0.20713608 0.20704919
 0.20653167 0.20596956 0.20595226 0.20613216 0.20603728 0.20577782
 0.20553416 0.2053676  0.20540936 0.20555422 0.2055238  0.20539236
 0.2056216  0.20601894 0.20626491 0.20630117 0.20633602 0.20637101
 0.20638153 0.20643823 0.2064854  0.20610341 0.20529443 0.20469481
 0.20469919 0.20495343 0.20525369 0.20541066 0.2053598  0.20535578
 0.20557936 0.20566669 0.20550199 0.20534799 0.20549968 0.2058305
 0.20583469 0.20575269 0.20600711 0.20612362 0.20536144 0.2045379
 0.20469087 0.20501992 0.2045517  0.20403355 0.2040231  0.2041045
 0.2038969  0.20398824 0.20431854 0.20426929 0.2037816  0.20372099
 0.20417601 0.20463681 0.20453364 0.2040362  0.20367156 0.20405628
 0.20484845 0.20535524 0.20550647 0.20583731 0.20632583 0.20651037
 0.20635204 0.20633142 0.20625836 0.20581317 0.20544846 0.20588481
 0.20652542 0.20631957 0.2057564  0.20581277 0.20639974 0.20668025
 0.20655993 0.20641443 0.20665598 0.2072075  0.20763296 0.20770475
 0.20742433 0.2073354  0.20778322 0.20785771 0.20689222 0.20605104
 0.20615083 0.20598173 0.2050419  0.20452717 0.20489918 0.20505016
 0.20446926 0.2041278  0.20444134 0.20484927 0.20486508 0.20489025
 0.20496255 0.2048998  0.20499851 0.20541157 0.20565839 0.20552643
 0.20522003 0.20467615 0.20404348 0.20379068 0.20378686 0.20337087
 0.20262599 0.20218645 0.20216171 0.20213148 0.20203188 0.20224111
 0.20249738 0.20240757 0.20222011 0.20251983 0.20298183 0.20324005
 0.20330301 0.20351581 0.20377508 0.2038808  0.2034505  0.20288926
 0.2025375  0.20248783 0.2027988  0.20286375 0.20208646 0.20106095
 0.20066956 0.20054227 0.20000884 0.19939883 0.19919474 0.19911756
 0.19864336 0.19791673 0.19738916 0.19709839 0.19712123 0.19747768
 0.19798326 0.19813696 0.19788076 0.19788723 0.19831803 0.19913504
 0.19977315 0.20008756 0.20074357 0.20188396 0.20213856 0.20086715
 0.19961727 0.19917978 0.19862024 0.19753642 0.19686675 0.19745363
 0.19820277 0.1982172  0.19796447 0.1978501  0.19791034 0.19830064
 0.19888319 0.19916177 0.19911256 0.19917475 0.19927852 0.19932893
 0.19963156 0.20038992 0.20113708 0.2011817  0.2008195  0.20066983
 0.20061535 0.19982158 0.19898807 0.19924717 0.2000335  0.20006298
 0.19920132 0.19857067 0.19863153 0.1990334  0.19892761 0.19836652
 0.19848114 0.19960724 0.20007592 0.19948898 0.19883083 0.19941509
 0.20055507 0.20083264 0.20080607 0.20144735 0.20177548 0.20099142
 0.20033918 0.20054492 0.2005376  0.20015112 0.20010649 0.20039044
 0.2001569  0.19951925 0.19935007 0.19928487 0.19893008 0.19878237
 0.19907114 0.19912176 0.19894451 0.19904225 0.19924994 0.19951813
 0.2001795  0.20130289 0.20179553 0.20107898 0.20026638 0.20013914
 0.19996414 0.19914927 0.19879596 0.19940242 0.1996631  0.19942488
 0.19941112 0.1996088  0.19966014 0.20003039 0.20070495 0.20080763
 0.20014791 0.19983885 0.1998717  0.19911392 0.19836149 0.19891292
 0.20013268 0.20027986 0.19944808 0.19899362 0.19879663 0.19855615
 0.19898683 0.19965573 0.1989285  0.19749506 0.19726454 0.19818929
 0.19856276 0.19828738 0.19856857 0.19890691 0.19833587 0.19751021
 0.19733022 0.19745117 0.19755179 0.19780199 0.19794492 0.19814867
 0.19907796 0.2006189  0.20136318 0.20111637 0.20107333 0.20126142
 0.20076394 0.19956324 0.19878449 0.19867101 0.19847979 0.19831395
 0.19856378 0.19887869 0.19928089 0.19990997 0.20061319 0.20060526
 0.20030949 0.2005238  0.20113865 0.20109707 0.20137636 0.20167512
 0.20166445 0.20126677 0.20151868 0.20227009 0.20158026 0.20013614
 0.19993377 0.20052385 0.19997737 0.19901241 0.19911373 0.1998389
 0.19998857 0.20016786 0.20123379 0.20188995 0.201492   0.20075321
 0.20039555 0.20030643 0.20053318 0.20087825 0.20048442 0.19948648
 0.19910705 0.19930555 0.19858839 0.19729556 0.19682328 0.19661647
 0.19558254 0.1947427  0.19504632 0.19512346 0.19416995 0.19392405
 0.19478188 0.19470316 0.19356473 0.192859   0.19340257 0.19359528
 0.19324014 0.19293417 0.19295394 0.19297104 0.19335099 0.1935893
 0.19275928 0.19158112 0.19131973 0.19194229 0.19179103 0.1912686
 0.19171587 0.1915346  0.19016758 0.18954359 0.19067362 0.19113173
 0.19014178 0.18930872 0.18973356 0.1901678  0.19030502 0.19055383
 0.19045888 0.18998215 0.19033629 0.19142093 0.19132924 0.18998122
 0.18931112 0.18925525 0.18856676 0.18768173 0.18733129 0.18654802
 0.185324   0.18495041 0.18506455 0.18440737 0.1835909  0.18429081
 0.18518564 0.18424295 0.1830555  0.18367264 0.18495718 0.18499176
 0.18447876 0.18423095 0.18412603 0.18459088 0.18582363 0.18596883
 0.18435717 0.1835313  0.18457885 0.18498884 0.18328232 0.18195076
 0.18245421 0.18233095 0.1808523  0.18037775 0.18092681 0.18095878
 0.18081038 0.18162288 0.1824425  0.18185896 0.18101932 0.1811134
 0.18134981 0.18138002 0.18160579 0.18159437 0.18117067 0.18147838
 0.18237749 0.18191488 0.17997935 0.17939311 0.1799548  0.17972063
 0.17892088 0.17873293 0.17851537 0.17733915 0.17672218 0.17735708
 0.17743021 0.17649509 0.17657869 0.1773942  0.17723544 0.17678598
 0.17739293 0.17773427 0.17652398 0.17566696 0.17582712 0.1753857
 0.17377901 0.17273268 0.17184088 0.17012449 0.16924477 0.16913955
 0.16814975 0.16630326 0.16665673 0.16803804 0.16737905 0.16586114
 0.16625342 0.16711946 0.16674045 0.16651276 0.16768214 0.16811787
 0.16725482 0.16720462 0.16751692 0.16649663 0.16620114 0.16754119
 0.1669253  0.1646032  0.16500475 0.16779023 0.1661955  0.16203241
 0.16263303 0.16500752 0.16362903 0.16169606 0.16332293 0.16430734
 0.16268477 0.16143404 0.16154878 0.16063678 0.16028558 0.15983853
 0.15620224 0.15621988 0.16001005 0.14894077 0.1423876  0.16280182]
