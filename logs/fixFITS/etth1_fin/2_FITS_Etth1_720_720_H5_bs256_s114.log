Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  195148800.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0634467601776123
Epoch: 1, Steps: 14 | Train Loss: 0.9568248 Vali Loss: 2.5232060 Test Loss: 1.1190305
Validation loss decreased (inf --> 2.523206).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.388340473175049
Epoch: 2, Steps: 14 | Train Loss: 0.8681155 Vali Loss: 2.3647325 Test Loss: 1.0382366
Validation loss decreased (2.523206 --> 2.364733).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.2185311317443848
Epoch: 3, Steps: 14 | Train Loss: 0.8005487 Vali Loss: 2.2477145 Test Loss: 0.9770821
Validation loss decreased (2.364733 --> 2.247715).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.9230356216430664
Epoch: 4, Steps: 14 | Train Loss: 0.7496642 Vali Loss: 2.1568048 Test Loss: 0.9295068
Validation loss decreased (2.247715 --> 2.156805).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.905754327774048
Epoch: 5, Steps: 14 | Train Loss: 0.7102119 Vali Loss: 2.0949395 Test Loss: 0.8933799
Validation loss decreased (2.156805 --> 2.094939).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.364671468734741
Epoch: 6, Steps: 14 | Train Loss: 0.6796746 Vali Loss: 2.0443673 Test Loss: 0.8655084
Validation loss decreased (2.094939 --> 2.044367).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1572213172912598
Epoch: 7, Steps: 14 | Train Loss: 0.6549913 Vali Loss: 2.0002558 Test Loss: 0.8439724
Validation loss decreased (2.044367 --> 2.000256).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.0192604064941406
Epoch: 8, Steps: 14 | Train Loss: 0.6355806 Vali Loss: 1.9716448 Test Loss: 0.8268088
Validation loss decreased (2.000256 --> 1.971645).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.194366216659546
Epoch: 9, Steps: 14 | Train Loss: 0.6194382 Vali Loss: 1.9437854 Test Loss: 0.8131738
Validation loss decreased (1.971645 --> 1.943785).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.4461801052093506
Epoch: 10, Steps: 14 | Train Loss: 0.6063823 Vali Loss: 1.9194189 Test Loss: 0.8023543
Validation loss decreased (1.943785 --> 1.919419).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.896955966949463
Epoch: 11, Steps: 14 | Train Loss: 0.5949316 Vali Loss: 1.9036741 Test Loss: 0.7936417
Validation loss decreased (1.919419 --> 1.903674).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.85860013961792
Epoch: 12, Steps: 14 | Train Loss: 0.5855860 Vali Loss: 1.8904016 Test Loss: 0.7860069
Validation loss decreased (1.903674 --> 1.890402).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.043196439743042
Epoch: 13, Steps: 14 | Train Loss: 0.5774604 Vali Loss: 1.8804498 Test Loss: 0.7796862
Validation loss decreased (1.890402 --> 1.880450).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.4064342975616455
Epoch: 14, Steps: 14 | Train Loss: 0.5704026 Vali Loss: 1.8669795 Test Loss: 0.7745990
Validation loss decreased (1.880450 --> 1.866979).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.349868059158325
Epoch: 15, Steps: 14 | Train Loss: 0.5640832 Vali Loss: 1.8599749 Test Loss: 0.7700065
Validation loss decreased (1.866979 --> 1.859975).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.126098394393921
Epoch: 16, Steps: 14 | Train Loss: 0.5589760 Vali Loss: 1.8504938 Test Loss: 0.7662080
Validation loss decreased (1.859975 --> 1.850494).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.2092394828796387
Epoch: 17, Steps: 14 | Train Loss: 0.5538555 Vali Loss: 1.8437178 Test Loss: 0.7629507
Validation loss decreased (1.850494 --> 1.843718).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.628791093826294
Epoch: 18, Steps: 14 | Train Loss: 0.5493491 Vali Loss: 1.8407711 Test Loss: 0.7597461
Validation loss decreased (1.843718 --> 1.840771).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.104116201400757
Epoch: 19, Steps: 14 | Train Loss: 0.5454719 Vali Loss: 1.8344352 Test Loss: 0.7571119
Validation loss decreased (1.840771 --> 1.834435).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.449953317642212
Epoch: 20, Steps: 14 | Train Loss: 0.5420186 Vali Loss: 1.8295822 Test Loss: 0.7547507
Validation loss decreased (1.834435 --> 1.829582).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.526637554168701
Epoch: 21, Steps: 14 | Train Loss: 0.5387509 Vali Loss: 1.8218839 Test Loss: 0.7524108
Validation loss decreased (1.829582 --> 1.821884).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.272254228591919
Epoch: 22, Steps: 14 | Train Loss: 0.5357139 Vali Loss: 1.8211801 Test Loss: 0.7503031
Validation loss decreased (1.821884 --> 1.821180).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.254384756088257
Epoch: 23, Steps: 14 | Train Loss: 0.5329881 Vali Loss: 1.8172907 Test Loss: 0.7485434
Validation loss decreased (1.821180 --> 1.817291).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.0573911666870117
Epoch: 24, Steps: 14 | Train Loss: 0.5305875 Vali Loss: 1.8137910 Test Loss: 0.7469478
Validation loss decreased (1.817291 --> 1.813791).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.251373052597046
Epoch: 25, Steps: 14 | Train Loss: 0.5281328 Vali Loss: 1.8110753 Test Loss: 0.7454427
Validation loss decreased (1.813791 --> 1.811075).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.2107367515563965
Epoch: 26, Steps: 14 | Train Loss: 0.5261316 Vali Loss: 1.8103495 Test Loss: 0.7439585
Validation loss decreased (1.811075 --> 1.810349).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.1548452377319336
Epoch: 27, Steps: 14 | Train Loss: 0.5239757 Vali Loss: 1.8036919 Test Loss: 0.7427013
Validation loss decreased (1.810349 --> 1.803692).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.1663265228271484
Epoch: 28, Steps: 14 | Train Loss: 0.5221536 Vali Loss: 1.7973056 Test Loss: 0.7414764
Validation loss decreased (1.803692 --> 1.797306).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.219182252883911
Epoch: 29, Steps: 14 | Train Loss: 0.5204843 Vali Loss: 1.8020592 Test Loss: 0.7403015
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.6001062393188477
Epoch: 30, Steps: 14 | Train Loss: 0.5187268 Vali Loss: 1.8019171 Test Loss: 0.7391695
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.3288466930389404
Epoch: 31, Steps: 14 | Train Loss: 0.5174071 Vali Loss: 1.7964005 Test Loss: 0.7381036
Validation loss decreased (1.797306 --> 1.796401).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.6899631023406982
Epoch: 32, Steps: 14 | Train Loss: 0.5155462 Vali Loss: 1.7938746 Test Loss: 0.7372906
Validation loss decreased (1.796401 --> 1.793875).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.3088018894195557
Epoch: 33, Steps: 14 | Train Loss: 0.5143127 Vali Loss: 1.7942398 Test Loss: 0.7363425
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.111154079437256
Epoch: 34, Steps: 14 | Train Loss: 0.5130273 Vali Loss: 1.7928568 Test Loss: 0.7354948
Validation loss decreased (1.793875 --> 1.792857).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.018493890762329
Epoch: 35, Steps: 14 | Train Loss: 0.5119144 Vali Loss: 1.7907697 Test Loss: 0.7346688
Validation loss decreased (1.792857 --> 1.790770).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.096388578414917
Epoch: 36, Steps: 14 | Train Loss: 0.5108637 Vali Loss: 1.7915199 Test Loss: 0.7339435
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.1866564750671387
Epoch: 37, Steps: 14 | Train Loss: 0.5097764 Vali Loss: 1.7869792 Test Loss: 0.7332224
Validation loss decreased (1.790770 --> 1.786979).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.282288074493408
Epoch: 38, Steps: 14 | Train Loss: 0.5087541 Vali Loss: 1.7859657 Test Loss: 0.7325917
Validation loss decreased (1.786979 --> 1.785966).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.1928844451904297
Epoch: 39, Steps: 14 | Train Loss: 0.5078489 Vali Loss: 1.7843604 Test Loss: 0.7319557
Validation loss decreased (1.785966 --> 1.784360).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.0543763637542725
Epoch: 40, Steps: 14 | Train Loss: 0.5067368 Vali Loss: 1.7869350 Test Loss: 0.7313377
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.267303466796875
Epoch: 41, Steps: 14 | Train Loss: 0.5061357 Vali Loss: 1.7759018 Test Loss: 0.7307208
Validation loss decreased (1.784360 --> 1.775902).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.3826937675476074
Epoch: 42, Steps: 14 | Train Loss: 0.5053124 Vali Loss: 1.7818606 Test Loss: 0.7302383
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.468924045562744
Epoch: 43, Steps: 14 | Train Loss: 0.5044568 Vali Loss: 1.7815779 Test Loss: 0.7296762
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.509544610977173
Epoch: 44, Steps: 14 | Train Loss: 0.5038213 Vali Loss: 1.7798401 Test Loss: 0.7292732
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  195148800.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.6004629135131836
Epoch: 1, Steps: 14 | Train Loss: 0.7171652 Vali Loss: 1.7496884 Test Loss: 0.7089585
Validation loss decreased (inf --> 1.749688).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.274489402770996
Epoch: 2, Steps: 14 | Train Loss: 0.7025040 Vali Loss: 1.7241864 Test Loss: 0.6899722
Validation loss decreased (1.749688 --> 1.724186).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.23124623298645
Epoch: 3, Steps: 14 | Train Loss: 0.6906369 Vali Loss: 1.7011751 Test Loss: 0.6736964
Validation loss decreased (1.724186 --> 1.701175).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.285465717315674
Epoch: 4, Steps: 14 | Train Loss: 0.6807539 Vali Loss: 1.6814563 Test Loss: 0.6592869
Validation loss decreased (1.701175 --> 1.681456).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.433375120162964
Epoch: 5, Steps: 14 | Train Loss: 0.6719998 Vali Loss: 1.6709176 Test Loss: 0.6464378
Validation loss decreased (1.681456 --> 1.670918).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.5613784790039062
Epoch: 6, Steps: 14 | Train Loss: 0.6647380 Vali Loss: 1.6517823 Test Loss: 0.6349524
Validation loss decreased (1.670918 --> 1.651782).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.2974331378936768
Epoch: 7, Steps: 14 | Train Loss: 0.6581019 Vali Loss: 1.6397018 Test Loss: 0.6245178
Validation loss decreased (1.651782 --> 1.639702).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.394757032394409
Epoch: 8, Steps: 14 | Train Loss: 0.6521453 Vali Loss: 1.6292233 Test Loss: 0.6150647
Validation loss decreased (1.639702 --> 1.629223).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.4225761890411377
Epoch: 9, Steps: 14 | Train Loss: 0.6468376 Vali Loss: 1.6227601 Test Loss: 0.6063287
Validation loss decreased (1.629223 --> 1.622760).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.624375104904175
Epoch: 10, Steps: 14 | Train Loss: 0.6421017 Vali Loss: 1.6095726 Test Loss: 0.5984543
Validation loss decreased (1.622760 --> 1.609573).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.9780328273773193
Epoch: 11, Steps: 14 | Train Loss: 0.6379560 Vali Loss: 1.6044269 Test Loss: 0.5912941
Validation loss decreased (1.609573 --> 1.604427).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.6556878089904785
Epoch: 12, Steps: 14 | Train Loss: 0.6343361 Vali Loss: 1.5896552 Test Loss: 0.5846840
Validation loss decreased (1.604427 --> 1.589655).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.520815372467041
Epoch: 13, Steps: 14 | Train Loss: 0.6305983 Vali Loss: 1.5843658 Test Loss: 0.5786158
Validation loss decreased (1.589655 --> 1.584366).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.3230526447296143
Epoch: 14, Steps: 14 | Train Loss: 0.6274454 Vali Loss: 1.5779396 Test Loss: 0.5732190
Validation loss decreased (1.584366 --> 1.577940).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.4478559494018555
Epoch: 15, Steps: 14 | Train Loss: 0.6244738 Vali Loss: 1.5763009 Test Loss: 0.5680968
Validation loss decreased (1.577940 --> 1.576301).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.312410354614258
Epoch: 16, Steps: 14 | Train Loss: 0.6216917 Vali Loss: 1.5657232 Test Loss: 0.5632982
Validation loss decreased (1.576301 --> 1.565723).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.3339903354644775
Epoch: 17, Steps: 14 | Train Loss: 0.6191109 Vali Loss: 1.5617120 Test Loss: 0.5590108
Validation loss decreased (1.565723 --> 1.561712).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.3771650791168213
Epoch: 18, Steps: 14 | Train Loss: 0.6172335 Vali Loss: 1.5635166 Test Loss: 0.5549878
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.645595073699951
Epoch: 19, Steps: 14 | Train Loss: 0.6152147 Vali Loss: 1.5547954 Test Loss: 0.5512729
Validation loss decreased (1.561712 --> 1.554795).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.655714511871338
Epoch: 20, Steps: 14 | Train Loss: 0.6130201 Vali Loss: 1.5487132 Test Loss: 0.5478967
Validation loss decreased (1.554795 --> 1.548713).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.639275074005127
Epoch: 21, Steps: 14 | Train Loss: 0.6113436 Vali Loss: 1.5468148 Test Loss: 0.5447346
Validation loss decreased (1.548713 --> 1.546815).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.52134370803833
Epoch: 22, Steps: 14 | Train Loss: 0.6095014 Vali Loss: 1.5444350 Test Loss: 0.5418202
Validation loss decreased (1.546815 --> 1.544435).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.548982620239258
Epoch: 23, Steps: 14 | Train Loss: 0.6080292 Vali Loss: 1.5378293 Test Loss: 0.5390640
Validation loss decreased (1.544435 --> 1.537829).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.574040651321411
Epoch: 24, Steps: 14 | Train Loss: 0.6068545 Vali Loss: 1.5372455 Test Loss: 0.5364879
Validation loss decreased (1.537829 --> 1.537246).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.292393445968628
Epoch: 25, Steps: 14 | Train Loss: 0.6050441 Vali Loss: 1.5388416 Test Loss: 0.5341681
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.301098346710205
Epoch: 26, Steps: 14 | Train Loss: 0.6041237 Vali Loss: 1.5283509 Test Loss: 0.5319563
Validation loss decreased (1.537246 --> 1.528351).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.024653434753418
Epoch: 27, Steps: 14 | Train Loss: 0.6028956 Vali Loss: 1.5320724 Test Loss: 0.5299153
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.0574162006378174
Epoch: 28, Steps: 14 | Train Loss: 0.6017655 Vali Loss: 1.5255131 Test Loss: 0.5279659
Validation loss decreased (1.528351 --> 1.525513).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.8612706661224365
Epoch: 29, Steps: 14 | Train Loss: 0.6007811 Vali Loss: 1.5251799 Test Loss: 0.5262290
Validation loss decreased (1.525513 --> 1.525180).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.89882755279541
Epoch: 30, Steps: 14 | Train Loss: 0.6000666 Vali Loss: 1.5201374 Test Loss: 0.5245477
Validation loss decreased (1.525180 --> 1.520137).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.1526620388031006
Epoch: 31, Steps: 14 | Train Loss: 0.5987460 Vali Loss: 1.5187063 Test Loss: 0.5229626
Validation loss decreased (1.520137 --> 1.518706).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.520355701446533
Epoch: 32, Steps: 14 | Train Loss: 0.5979833 Vali Loss: 1.5199350 Test Loss: 0.5215196
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.692999839782715
Epoch: 33, Steps: 14 | Train Loss: 0.5972719 Vali Loss: 1.5100206 Test Loss: 0.5201187
Validation loss decreased (1.518706 --> 1.510021).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.0120158195495605
Epoch: 34, Steps: 14 | Train Loss: 0.5965083 Vali Loss: 1.5152581 Test Loss: 0.5188439
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.610104560852051
Epoch: 35, Steps: 14 | Train Loss: 0.5960186 Vali Loss: 1.5127263 Test Loss: 0.5176083
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.385730266571045
Epoch: 36, Steps: 14 | Train Loss: 0.5949485 Vali Loss: 1.5165431 Test Loss: 0.5164872
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4984360337257385, mae:0.5029126405715942, rse:0.6758599281311035, corr:[0.20761254 0.22637759 0.2168228  0.21975504 0.2277299  0.22798634
 0.22508563 0.22531354 0.22761697 0.22939152 0.22888778 0.22684935
 0.22465    0.22343214 0.22355558 0.22355437 0.22170268 0.2186191
 0.21717    0.21831295 0.21924943 0.21790141 0.21560392 0.21568364
 0.21807726 0.21980439 0.21949945 0.21866664 0.21927984 0.22091356
 0.22192028 0.22137189 0.22024515 0.21958508 0.21949266 0.21952006
 0.21921192 0.21800698 0.21658735 0.21606135 0.21669626 0.21709207
 0.2161151  0.21472654 0.21471818 0.21639603 0.21799226 0.21830723
 0.21797755 0.2185204  0.2195428  0.21974665 0.2187063  0.21698618
 0.21596776 0.21571809 0.21578845 0.21514894 0.21369995 0.21259867
 0.21251708 0.2128016  0.21255672 0.21165521 0.21069935 0.21070616
 0.21172956 0.2124408  0.2120375  0.21106286 0.21067743 0.21147257
 0.21201989 0.21144359 0.21020335 0.20982772 0.2103514  0.21070851
 0.21014881 0.20890437 0.20781763 0.20751007 0.20752867 0.20720251
 0.2062258  0.20495974 0.2040982  0.2040336  0.20422179 0.20385844
 0.2027986  0.20203106 0.2022734  0.20319192 0.20359868 0.20370996
 0.20453474 0.2064555  0.20842992 0.20945835 0.20929983 0.20874837
 0.20876361 0.20933093 0.20958021 0.20882262 0.20757097 0.20687318
 0.20703414 0.20733343 0.20697886 0.206115   0.2054956  0.20565683
 0.20627642 0.20636624 0.20567206 0.20486403 0.20481014 0.20575288
 0.20658761 0.20628932 0.2051622  0.20464486 0.20495182 0.20519274
 0.20472637 0.20373803 0.20291    0.2027686  0.20297773 0.20276628
 0.2019852  0.20111306 0.20087564 0.20125015 0.2014423  0.2009514
 0.20013422 0.19982524 0.2002636  0.2006161  0.19996296 0.19895732
 0.19857804 0.19898717 0.19942196 0.19912726 0.19815953 0.19719797
 0.19703974 0.19770305 0.19818564 0.19755262 0.19625202 0.1956785
 0.19595774 0.19612223 0.19547729 0.19471836 0.19473858 0.19560395
 0.19632089 0.19608717 0.19509684 0.1943198  0.19377704 0.19390555
 0.19419584 0.19434616 0.19434465 0.19488904 0.19573519 0.1962297
 0.19604458 0.19550289 0.19502649 0.1951903  0.19555648 0.1953796
 0.19457944 0.19390702 0.19417123 0.19504301 0.19561115 0.19544937
 0.19518076 0.19552408 0.19630772 0.19657741 0.196032   0.19534637
 0.19518794 0.19537479 0.19526592 0.19456014 0.19362512 0.19297548
 0.19293916 0.193359   0.19338289 0.19250624 0.19149557 0.19147329
 0.19245496 0.1935317  0.19365667 0.1931241  0.19303936 0.19364408
 0.19407643 0.19357467 0.19246793 0.19166952 0.19162808 0.19207627
 0.19228488 0.19207036 0.19173403 0.1918645  0.19243099 0.19266315
 0.19218576 0.1911886  0.1902791  0.18998475 0.1898823  0.18955109
 0.18877822 0.18817063 0.18829304 0.18886328 0.18898468 0.1884848
 0.18792947 0.18782043 0.1880216  0.18801178 0.18756886 0.18710385
 0.18744318 0.18864124 0.19005214 0.19074948 0.19046924 0.18990357
 0.18997242 0.19054502 0.19064319 0.18962476 0.18821941 0.18768859
 0.18796124 0.18817364 0.18790646 0.18757105 0.18784504 0.18862298
 0.18904863 0.18855596 0.18780139 0.18745402 0.187736   0.18825069
 0.18861398 0.18876308 0.18886153 0.18922165 0.18961826 0.18976766
 0.18945153 0.18865138 0.18804367 0.18825947 0.18865551 0.18844768
 0.1876597  0.18729046 0.18738897 0.18772767 0.18764871 0.18731952
 0.18700628 0.18722713 0.18747193 0.18723376 0.18641393 0.18591928
 0.18646349 0.18789764 0.18932053 0.18995294 0.18959843 0.18913555
 0.18942235 0.19019389 0.19042929 0.18995668 0.18943472 0.18968548
 0.19036312 0.19053227 0.19014461 0.18997513 0.19054134 0.19143084
 0.19199283 0.19164746 0.19095948 0.19062118 0.1907884  0.19127338
 0.19172418 0.19180116 0.19177812 0.19209625 0.19236554 0.19210619
 0.19113708 0.18976553 0.18887202 0.1888139  0.18913956 0.1890791
 0.1886818  0.18850769 0.18899015 0.18981917 0.19013    0.18993773
 0.18978861 0.1899887  0.19043538 0.19071417 0.19038107 0.18970138
 0.18931076 0.18949619 0.18986149 0.1897676  0.18883893 0.18775472
 0.18746257 0.1876628  0.18732256 0.18636256 0.1854869  0.18558878
 0.18635947 0.18685871 0.1866222  0.18635938 0.18681228 0.18787308
 0.18839033 0.18798542 0.18724993 0.18691236 0.1869298  0.18719271
 0.18747436 0.18741162 0.18715383 0.18707421 0.18717027 0.18684857
 0.1858851  0.18456408 0.18359661 0.18343702 0.18371244 0.18347979
 0.18233082 0.18118276 0.18093565 0.1812287  0.18121669 0.18076737
 0.18049747 0.18059617 0.18069717 0.18050726 0.17988622 0.17965929
 0.18054397 0.18244977 0.18442312 0.18547872 0.18522897 0.18394338
 0.18290997 0.18246236 0.18182203 0.18051034 0.17910478 0.17897208
 0.17987758 0.18037285 0.18003885 0.17960407 0.1800907  0.18129635
 0.18198912 0.18160444 0.18092676 0.18074875 0.18099625 0.18136716
 0.18177533 0.18233898 0.18317242 0.18427798 0.18534963 0.18554273
 0.18482889 0.18357034 0.1828052  0.1829402  0.18327597 0.1829457
 0.18202059 0.18160523 0.18186563 0.18230495 0.18214877 0.18154481
 0.18123995 0.18166055 0.18195194 0.18182592 0.1812376  0.18103011
 0.1820069  0.18372996 0.18512271 0.18574674 0.18558604 0.1851383
 0.18535349 0.18585798 0.18548831 0.18444021 0.18375508 0.18440615
 0.18558104 0.18561864 0.18448484 0.18326129 0.1831966  0.18394086
 0.18429975 0.18376364 0.18303922 0.18282226 0.1832741  0.18418832
 0.18508682 0.1857466  0.18611792 0.18633844 0.18649414 0.18612562
 0.18546645 0.18466449 0.18433182 0.18499738 0.1856357  0.18559544
 0.18508837 0.18515821 0.18604048 0.18678351 0.18657838 0.18604644
 0.18595922 0.18620548 0.18609078 0.18540287 0.18479629 0.18465872
 0.18508288 0.18572055 0.18606694 0.18611085 0.18584748 0.18527313
 0.18518846 0.18553941 0.18522695 0.18436739 0.18373816 0.1840362
 0.18476133 0.18467495 0.18384889 0.18331064 0.18363047 0.18419515
 0.18403621 0.183127   0.18240713 0.18247163 0.18294077 0.18343334
 0.1839661  0.18477456 0.18568185 0.18681413 0.1878667  0.18781112
 0.18659367 0.18533315 0.18492566 0.18517843 0.18536495 0.18490997
 0.18428989 0.18458681 0.18606469 0.18724377 0.18764499 0.18752262
 0.18751805 0.1877806  0.18834189 0.1883992  0.18841125 0.18774852
 0.18765014 0.18817234 0.18877737 0.18907169 0.18831463 0.1874177
 0.18737699 0.18793443 0.18790287 0.18708761 0.18631013 0.18671802
 0.1876272  0.18781312 0.1877483  0.18821268 0.18950582 0.19036579
 0.1901315  0.18928583 0.18868831 0.18845555 0.18827015 0.18777438
 0.18696816 0.18630473 0.18576124 0.1856228  0.1857956  0.18549988
 0.18441561 0.18324675 0.18300846 0.1835494  0.1836238  0.18271296
 0.18154629 0.1811038  0.18183814 0.18200101 0.18170057 0.18144077
 0.181769   0.18184198 0.18152377 0.18097116 0.18034609 0.17943384
 0.17863943 0.17878082 0.17929302 0.17988902 0.17953396 0.17858607
 0.17878468 0.17935283 0.17931408 0.17829861 0.17792349 0.17874426
 0.17976803 0.17951816 0.17879136 0.1788479  0.17976865 0.18041402
 0.1801683  0.17961109 0.17938276 0.1794409  0.17931242 0.17873338
 0.17798495 0.17726822 0.17672497 0.17684734 0.17731068 0.17668629
 0.17486109 0.17335717 0.1732878  0.17432697 0.17451395 0.17355464
 0.17272308 0.1731658  0.17435981 0.17477447 0.17409374 0.17345655
 0.17373347 0.17447607 0.17491989 0.17483516 0.17456341 0.17428637
 0.17431642 0.17486143 0.1751476  0.17467111 0.17348006 0.1726438
 0.17332655 0.17415161 0.17340663 0.17187059 0.17153563 0.17307256
 0.17460893 0.17426287 0.17318389 0.17291811 0.17345749 0.17365663
 0.17305017 0.17240933 0.17232427 0.1726468  0.17312291 0.17329417
 0.17264721 0.171402   0.17068726 0.17186713 0.17302473 0.17211437
 0.16975543 0.16848457 0.1695654  0.17094241 0.1703628  0.16864368
 0.16810827 0.16917257 0.17039552 0.17039394 0.1697498  0.16960248
 0.16988955 0.17015024 0.16982631 0.16918264 0.16823421 0.1672974
 0.16639726 0.16568813 0.16436975 0.16208187 0.16017368 0.15965573
 0.16064496 0.16083911 0.15978214 0.15855397 0.15947218 0.16167681
 0.16248393 0.16108201 0.160034   0.16074651 0.16214329 0.16257975
 0.16162328 0.1602914  0.15984738 0.16055128 0.16099787 0.15933657
 0.15579548 0.15419179 0.15615083 0.15923792 0.15829892 0.15417017
 0.15324534 0.15663445 0.15947367 0.15834618 0.15567109 0.15476812
 0.15576072 0.1559756  0.15523659 0.1535026  0.15008952 0.14766486
 0.14907768 0.14775825 0.13251755 0.1157871  0.13412367 0.12295537]
