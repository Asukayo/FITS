Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  64354304.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.318378686904907
Epoch: 1, Steps: 28 | Train Loss: 0.9900808 Vali Loss: 2.0811567 Test Loss: 0.8779867
Validation loss decreased (inf --> 2.081157).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.9460155963897705
Epoch: 2, Steps: 28 | Train Loss: 0.8237788 Vali Loss: 1.8465244 Test Loss: 0.7347069
Validation loss decreased (2.081157 --> 1.846524).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.764557600021362
Epoch: 3, Steps: 28 | Train Loss: 0.7471145 Vali Loss: 1.7470237 Test Loss: 0.6694775
Validation loss decreased (1.846524 --> 1.747024).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.197195053100586
Epoch: 4, Steps: 28 | Train Loss: 0.7092415 Vali Loss: 1.6928837 Test Loss: 0.6331645
Validation loss decreased (1.747024 --> 1.692884).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.818928480148315
Epoch: 5, Steps: 28 | Train Loss: 0.6863504 Vali Loss: 1.6532152 Test Loss: 0.6082980
Validation loss decreased (1.692884 --> 1.653215).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.26961088180542
Epoch: 6, Steps: 28 | Train Loss: 0.6695134 Vali Loss: 1.6300802 Test Loss: 0.5889857
Validation loss decreased (1.653215 --> 1.630080).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.3241941928863525
Epoch: 7, Steps: 28 | Train Loss: 0.6568978 Vali Loss: 1.6050105 Test Loss: 0.5729903
Validation loss decreased (1.630080 --> 1.605011).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.539212703704834
Epoch: 8, Steps: 28 | Train Loss: 0.6462543 Vali Loss: 1.5848424 Test Loss: 0.5595406
Validation loss decreased (1.605011 --> 1.584842).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.9490556716918945
Epoch: 9, Steps: 28 | Train Loss: 0.6378340 Vali Loss: 1.5679203 Test Loss: 0.5476004
Validation loss decreased (1.584842 --> 1.567920).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.135088920593262
Epoch: 10, Steps: 28 | Train Loss: 0.6303854 Vali Loss: 1.5618567 Test Loss: 0.5375091
Validation loss decreased (1.567920 --> 1.561857).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.633053302764893
Epoch: 11, Steps: 28 | Train Loss: 0.6237147 Vali Loss: 1.5464405 Test Loss: 0.5284094
Validation loss decreased (1.561857 --> 1.546440).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.604611158370972
Epoch: 12, Steps: 28 | Train Loss: 0.6179379 Vali Loss: 1.5355518 Test Loss: 0.5205330
Validation loss decreased (1.546440 --> 1.535552).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.901067733764648
Epoch: 13, Steps: 28 | Train Loss: 0.6136269 Vali Loss: 1.5311004 Test Loss: 0.5136204
Validation loss decreased (1.535552 --> 1.531100).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.889572381973267
Epoch: 14, Steps: 28 | Train Loss: 0.6092135 Vali Loss: 1.5249943 Test Loss: 0.5074126
Validation loss decreased (1.531100 --> 1.524994).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.901179075241089
Epoch: 15, Steps: 28 | Train Loss: 0.6053045 Vali Loss: 1.5124271 Test Loss: 0.5018631
Validation loss decreased (1.524994 --> 1.512427).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.94045090675354
Epoch: 16, Steps: 28 | Train Loss: 0.6017871 Vali Loss: 1.5086493 Test Loss: 0.4969290
Validation loss decreased (1.512427 --> 1.508649).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.39782190322876
Epoch: 17, Steps: 28 | Train Loss: 0.5991646 Vali Loss: 1.5018191 Test Loss: 0.4925424
Validation loss decreased (1.508649 --> 1.501819).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.309448003768921
Epoch: 18, Steps: 28 | Train Loss: 0.5963737 Vali Loss: 1.4903119 Test Loss: 0.4886063
Validation loss decreased (1.501819 --> 1.490312).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.3852622509002686
Epoch: 19, Steps: 28 | Train Loss: 0.5936924 Vali Loss: 1.4933521 Test Loss: 0.4850360
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.61029314994812
Epoch: 20, Steps: 28 | Train Loss: 0.5913315 Vali Loss: 1.4889417 Test Loss: 0.4819236
Validation loss decreased (1.490312 --> 1.488942).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.088454484939575
Epoch: 21, Steps: 28 | Train Loss: 0.5899874 Vali Loss: 1.4820483 Test Loss: 0.4790317
Validation loss decreased (1.488942 --> 1.482048).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.1805737018585205
Epoch: 22, Steps: 28 | Train Loss: 0.5878170 Vali Loss: 1.4822961 Test Loss: 0.4765052
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.925863027572632
Epoch: 23, Steps: 28 | Train Loss: 0.5865648 Vali Loss: 1.4736266 Test Loss: 0.4741200
Validation loss decreased (1.482048 --> 1.473627).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.612461090087891
Epoch: 24, Steps: 28 | Train Loss: 0.5844478 Vali Loss: 1.4732636 Test Loss: 0.4720197
Validation loss decreased (1.473627 --> 1.473264).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 5.109621286392212
Epoch: 25, Steps: 28 | Train Loss: 0.5837083 Vali Loss: 1.4769359 Test Loss: 0.4700902
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.522969484329224
Epoch: 26, Steps: 28 | Train Loss: 0.5820384 Vali Loss: 1.4729979 Test Loss: 0.4682889
Validation loss decreased (1.473264 --> 1.472998).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.019466161727905
Epoch: 27, Steps: 28 | Train Loss: 0.5813607 Vali Loss: 1.4650290 Test Loss: 0.4667311
Validation loss decreased (1.472998 --> 1.465029).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.103568077087402
Epoch: 28, Steps: 28 | Train Loss: 0.5803705 Vali Loss: 1.4707000 Test Loss: 0.4652929
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.260335683822632
Epoch: 29, Steps: 28 | Train Loss: 0.5792189 Vali Loss: 1.4627607 Test Loss: 0.4639892
Validation loss decreased (1.465029 --> 1.462761).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.656846761703491
Epoch: 30, Steps: 28 | Train Loss: 0.5786925 Vali Loss: 1.4609091 Test Loss: 0.4627641
Validation loss decreased (1.462761 --> 1.460909).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.8532819747924805
Epoch: 31, Steps: 28 | Train Loss: 0.5775747 Vali Loss: 1.4577217 Test Loss: 0.4616264
Validation loss decreased (1.460909 --> 1.457722).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.141407489776611
Epoch: 32, Steps: 28 | Train Loss: 0.5771620 Vali Loss: 1.4574354 Test Loss: 0.4605993
Validation loss decreased (1.457722 --> 1.457435).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.10631799697876
Epoch: 33, Steps: 28 | Train Loss: 0.5761504 Vali Loss: 1.4553186 Test Loss: 0.4596592
Validation loss decreased (1.457435 --> 1.455319).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.2040934562683105
Epoch: 34, Steps: 28 | Train Loss: 0.5755480 Vali Loss: 1.4574957 Test Loss: 0.4587810
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 5.3932905197143555
Epoch: 35, Steps: 28 | Train Loss: 0.5746298 Vali Loss: 1.4545062 Test Loss: 0.4579665
Validation loss decreased (1.455319 --> 1.454506).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 5.148974895477295
Epoch: 36, Steps: 28 | Train Loss: 0.5743577 Vali Loss: 1.4554169 Test Loss: 0.4572332
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 5.212106227874756
Epoch: 37, Steps: 28 | Train Loss: 0.5736880 Vali Loss: 1.4568095 Test Loss: 0.4565668
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 5.0791521072387695
Epoch: 38, Steps: 28 | Train Loss: 0.5734256 Vali Loss: 1.4527917 Test Loss: 0.4559465
Validation loss decreased (1.454506 --> 1.452792).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 5.343637228012085
Epoch: 39, Steps: 28 | Train Loss: 0.5727896 Vali Loss: 1.4577391 Test Loss: 0.4553259
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 5.572370767593384
Epoch: 40, Steps: 28 | Train Loss: 0.5725632 Vali Loss: 1.4527278 Test Loss: 0.4547845
Validation loss decreased (1.452792 --> 1.452728).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 5.040949583053589
Epoch: 41, Steps: 28 | Train Loss: 0.5718349 Vali Loss: 1.4506093 Test Loss: 0.4542744
Validation loss decreased (1.452728 --> 1.450609).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.953779458999634
Epoch: 42, Steps: 28 | Train Loss: 0.5717314 Vali Loss: 1.4524837 Test Loss: 0.4538201
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.956509351730347
Epoch: 43, Steps: 28 | Train Loss: 0.5716255 Vali Loss: 1.4549217 Test Loss: 0.4533887
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.717703104019165
Epoch: 44, Steps: 28 | Train Loss: 0.5709825 Vali Loss: 1.4494355 Test Loss: 0.4529546
Validation loss decreased (1.450609 --> 1.449435).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.863665580749512
Epoch: 45, Steps: 28 | Train Loss: 0.5707874 Vali Loss: 1.4513384 Test Loss: 0.4525895
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.896316289901733
Epoch: 46, Steps: 28 | Train Loss: 0.5701517 Vali Loss: 1.4482719 Test Loss: 0.4522374
Validation loss decreased (1.449435 --> 1.448272).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.543334722518921
Epoch: 47, Steps: 28 | Train Loss: 0.5702048 Vali Loss: 1.4442158 Test Loss: 0.4518951
Validation loss decreased (1.448272 --> 1.444216).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.516764879226685
Epoch: 48, Steps: 28 | Train Loss: 0.5699333 Vali Loss: 1.4471686 Test Loss: 0.4515818
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 4.603073596954346
Epoch: 49, Steps: 28 | Train Loss: 0.5699323 Vali Loss: 1.4476821 Test Loss: 0.4512839
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.551701545715332
Epoch: 50, Steps: 28 | Train Loss: 0.5690120 Vali Loss: 1.4484601 Test Loss: 0.4510136
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4431130290031433, mae:0.46798720955848694, rse:0.6372490525245667, corr:[0.20656429 0.23176405 0.23256397 0.22585087 0.22626768 0.22997029
 0.232292   0.23220392 0.2310964  0.23054016 0.23012683 0.22937329
 0.22839437 0.22776489 0.2277264  0.22738916 0.22619882 0.22456521
 0.22360593 0.22394067 0.22485934 0.22507001 0.22436675 0.22348982
 0.22370107 0.22493047 0.2260906  0.22647859 0.22623731 0.2260083
 0.22621612 0.2265666  0.22670755 0.22636922 0.2257449  0.22529131
 0.2251213  0.22484133 0.22448812 0.22429433 0.2244826  0.22481342
 0.22494763 0.22495748 0.22496922 0.2253523  0.22608194 0.22672193
 0.22693144 0.226483   0.22570664 0.2250946  0.2246774  0.2241605
 0.22345994 0.22255376 0.22214463 0.22187182 0.22165945 0.22140206
 0.22113626 0.22090805 0.22077237 0.22068058 0.22054932 0.22044335
 0.22043122 0.22060506 0.22092462 0.2209823  0.22056018 0.2200661
 0.2196201  0.2193852  0.21932583 0.21917711 0.21881855 0.2184052
 0.21819346 0.21819864 0.21818654 0.21791874 0.21741015 0.21689422
 0.2164058  0.21589008 0.21533254 0.21498622 0.21484879 0.21483934
 0.2148398  0.21497941 0.2150414  0.21517727 0.21548648 0.21633764
 0.21755558 0.21864523 0.21940756 0.21993452 0.22025457 0.22041263
 0.22042087 0.22028564 0.22009963 0.21973856 0.21928167 0.21888444
 0.21856248 0.21846169 0.21840832 0.21835208 0.21827815 0.21811847
 0.21801426 0.2180403  0.2182186  0.21841419 0.21853974 0.21862167
 0.2185297  0.21819116 0.21774478 0.21747155 0.21719003 0.21686417
 0.21652074 0.21627185 0.21612948 0.21590392 0.21556759 0.21520512
 0.21498698 0.21481195 0.21465597 0.21451014 0.21429029 0.21409471
 0.21396753 0.21400507 0.21404786 0.21385008 0.21336892 0.21313567
 0.21312481 0.21309894 0.21288075 0.21248077 0.21211137 0.21191794
 0.21194696 0.2121294  0.2122238  0.21196657 0.211501   0.21116202
 0.21090682 0.210708   0.21059509 0.21073724 0.21090733 0.21105352
 0.21111421 0.21113293 0.21110818 0.21117581 0.21101558 0.21103072
 0.21122742 0.2116251  0.21204366 0.2125209  0.2128055  0.2128256
 0.21267454 0.21259359 0.21255037 0.2124565  0.21221475 0.21187328
 0.21155666 0.2114261  0.21158735 0.2119012  0.21220644 0.212375
 0.2124564  0.21261856 0.2128322  0.21273549 0.21242574 0.21212053
 0.21191844 0.21170864 0.21138187 0.21093644 0.21051465 0.21024005
 0.21013562 0.2101798  0.21014766 0.20985769 0.20963147 0.20968144
 0.21002729 0.21047674 0.21077143 0.21084242 0.21080786 0.21074544
 0.21051782 0.21005373 0.20946763 0.2089284  0.20860586 0.2086777
 0.20890687 0.20901754 0.2090096  0.20911455 0.20935696 0.20949061
 0.20935228 0.20901118 0.20857143 0.20824899 0.2079738  0.20777546
 0.20753276 0.20728408 0.20709494 0.20699115 0.20677032 0.20649044
 0.20632847 0.20633748 0.20637679 0.20628616 0.20607057 0.2059669
 0.20627399 0.20687495 0.20753768 0.20790368 0.20779139 0.20746416
 0.20735171 0.20741208 0.2072559  0.2066538  0.20591334 0.20550199
 0.2055331  0.20570627 0.20584008 0.20583174 0.2059085  0.20618317
 0.20648636 0.20646274 0.20627634 0.20605856 0.20606844 0.20630062
 0.20654048 0.20654753 0.20629825 0.20605043 0.20589969 0.2058613
 0.20581785 0.20563668 0.20535336 0.20525905 0.20539075 0.2055521
 0.20550616 0.2054743  0.20536022 0.20538671 0.20535932 0.20526423
 0.20497312 0.20486432 0.2049199  0.20498224 0.20492816 0.20502901
 0.20541595 0.20609683 0.2069056  0.20750706 0.2076805  0.20754012
 0.20741001 0.20758182 0.20778087 0.20776735 0.2075682  0.20747696
 0.20756531 0.20768093 0.20766893 0.20759694 0.20753735 0.20770536
 0.20816405 0.2083041  0.20799501 0.20759931 0.20755523 0.20819579
 0.20915355 0.20962599 0.2093493  0.20882156 0.2085353  0.20850629
 0.20826548 0.20760883 0.2067289  0.20609975 0.20605773 0.20633608
 0.20652655 0.20647931 0.20643258 0.20674688 0.20711178 0.20724283
 0.2070615  0.20679829 0.2068758  0.20726013 0.20743719 0.20731068
 0.20700622 0.20677805 0.20686466 0.20703064 0.20677598 0.206022
 0.20518933 0.20469777 0.20455158 0.20453557 0.20442456 0.20425119
 0.20419057 0.2043868  0.2046172  0.20471057 0.2046802  0.20487472
 0.20514601 0.2052335  0.20502405 0.20472342 0.20447598 0.20466635
 0.20517077 0.2053641  0.2049712  0.20428035 0.20374438 0.20347707
 0.20327562 0.20291713 0.2023498  0.20181747 0.20158102 0.20159255
 0.20132627 0.2006351  0.19995151 0.19966753 0.19985838 0.20010066
 0.1999482  0.19934678 0.198914   0.19919847 0.19996352 0.20099224
 0.20184214 0.20241733 0.20293768 0.20344774 0.20364831 0.20304963
 0.20194615 0.20092876 0.20034626 0.20004022 0.19960001 0.19904709
 0.19862287 0.1985217  0.1987639  0.19897403 0.19902648 0.1991238
 0.19945176 0.1998406  0.20007282 0.20000394 0.19970216 0.19979681
 0.2005277  0.20147783 0.20206693 0.20209898 0.20190936 0.20171756
 0.20170227 0.20151903 0.200974   0.20027395 0.19993007 0.20014949
 0.20044436 0.2003602  0.1996892  0.19913797 0.19916204 0.19964582
 0.19987585 0.19962083 0.19906619 0.1992384  0.20005703 0.2009756
 0.20144409 0.201265   0.2009375  0.20122387 0.20185818 0.20201018
 0.2014331  0.20068325 0.200267   0.20045532 0.20073217 0.20066622
 0.20014092 0.19958703 0.19943972 0.19944301 0.19928925 0.19896692
 0.19881132 0.19893248 0.1991875  0.19930263 0.19933824 0.19973348
 0.20062876 0.20161821 0.20203897 0.20154192 0.20054303 0.1996548
 0.19940326 0.19953197 0.19943155 0.19911382 0.19879574 0.19894078
 0.19936915 0.19967397 0.19962035 0.19948101 0.19956309 0.19990693
 0.20001726 0.19963665 0.19899411 0.19860788 0.19894558 0.1995277
 0.19973864 0.19938116 0.19881766 0.19870862 0.19903003 0.19910868
 0.19858626 0.19793797 0.19748157 0.1975147  0.19773014 0.19771433
 0.19740179 0.19706729 0.19703615 0.19715483 0.197082   0.19665772
 0.19621928 0.19612478 0.1964126  0.19680272 0.19709152 0.197477
 0.1981952  0.19924007 0.20007524 0.20027682 0.19989772 0.1992084
 0.19870843 0.19858272 0.19826674 0.19754092 0.19691378 0.19693662
 0.19741924 0.19784716 0.19810353 0.19810006 0.19840196 0.19901754
 0.19953473 0.19964144 0.19977954 0.19977246 0.20049648 0.20103796
 0.20144129 0.20121099 0.20061412 0.2004932  0.20062457 0.20070037
 0.20026003 0.19959123 0.19902846 0.19898264 0.19909969 0.1991546
 0.19900206 0.19890942 0.19944772 0.20025364 0.2008122  0.20066418
 0.20019914 0.20004743 0.20026174 0.20036738 0.19990179 0.19901371
 0.19827402 0.1982108  0.19836625 0.19804369 0.19703928 0.19588242
 0.19527622 0.19535379 0.19525966 0.19461393 0.19369465 0.19322748
 0.19323145 0.19312845 0.19299129 0.19248521 0.192435   0.19280246
 0.19329026 0.1930321  0.19230966 0.19173262 0.19178605 0.19216287
 0.19223955 0.19175334 0.19082369 0.19062403 0.19118902 0.19154695
 0.19122219 0.19002374 0.18903634 0.18887472 0.18954015 0.19003583
 0.1898696  0.18915834 0.18897083 0.18949199 0.19002664 0.18987356
 0.1892618  0.18904144 0.18958504 0.19024494 0.19002149 0.18887469
 0.18778534 0.18757088 0.18793233 0.18783925 0.18660678 0.18467814
 0.18343759 0.18354422 0.18405747 0.18393266 0.18293457 0.18213542
 0.18220194 0.18272041 0.1828768  0.18230012 0.18171018 0.1819038
 0.18276997 0.18342695 0.18331604 0.1828873  0.18306598 0.18385816
 0.18431889 0.18358293 0.1818704  0.18075338 0.18097112 0.18173915
 0.18186    0.18074581 0.1791721  0.17854026 0.17913018 0.18005541
 0.18017544 0.17928058 0.17872946 0.17919172 0.17992814 0.17985411
 0.17888504 0.17824602 0.17879863 0.17998397 0.18047772 0.17964834
 0.17826779 0.17780966 0.17846933 0.17936014 0.1787721  0.176751
 0.17495573 0.17467937 0.17553397 0.17578967 0.17479092 0.17364892
 0.17348276 0.17401622 0.17428432 0.17363238 0.17297533 0.17335786
 0.1744505  0.17506391 0.1741897  0.17268096 0.17195919 0.17246348
 0.17255487 0.17086563 0.16768642 0.16513029 0.16471769 0.16525516
 0.16532594 0.16389008 0.16215262 0.161397   0.16232567 0.1636453
 0.16348733 0.16204189 0.16147953 0.16244158 0.16366349 0.16342813
 0.16204263 0.16164447 0.16303386 0.16439024 0.16336635 0.16005795
 0.15736328 0.15801363 0.16042852 0.16150403 0.15887687 0.15461373
 0.15307239 0.15514056 0.15775935 0.15721723 0.15438025 0.15231499
 0.15267476 0.15335286 0.15200621 0.14935789 0.14869928 0.1504682
 0.15075593 0.14435126 0.136021   0.13572519 0.1426821  0.11895347]
