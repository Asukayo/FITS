Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  25200896.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5612000
	speed: 0.1504s/iter; left time: 872.5776s
Epoch: 1 cost time: 17.61461615562439
Epoch: 1, Steps: 118 | Train Loss: 0.6704748 Vali Loss: 1.3341717 Test Loss: 0.5464090
Validation loss decreased (inf --> 1.334172).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4827093
	speed: 0.3364s/iter; left time: 1911.6098s
Epoch: 2 cost time: 14.709903240203857
Epoch: 2, Steps: 118 | Train Loss: 0.5136964 Vali Loss: 1.2290423 Test Loss: 0.4673311
Validation loss decreased (1.334172 --> 1.229042).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4502082
	speed: 0.3213s/iter; left time: 1788.0036s
Epoch: 3 cost time: 14.250357627868652
Epoch: 3, Steps: 118 | Train Loss: 0.4730827 Vali Loss: 1.1923492 Test Loss: 0.4423936
Validation loss decreased (1.229042 --> 1.192349).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4534445
	speed: 0.2594s/iter; left time: 1412.6849s
Epoch: 4 cost time: 13.952231645584106
Epoch: 4, Steps: 118 | Train Loss: 0.4569022 Vali Loss: 1.1870310 Test Loss: 0.4358377
Validation loss decreased (1.192349 --> 1.187031).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4794579
	speed: 0.3547s/iter; left time: 1890.0619s
Epoch: 5 cost time: 17.231309175491333
Epoch: 5, Steps: 118 | Train Loss: 0.4501670 Vali Loss: 1.1875582 Test Loss: 0.4348245
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4629883
	speed: 0.3684s/iter; left time: 1919.7092s
Epoch: 6 cost time: 17.741697788238525
Epoch: 6, Steps: 118 | Train Loss: 0.4459016 Vali Loss: 1.1903509 Test Loss: 0.4352992
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4364696
	speed: 0.3351s/iter; left time: 1706.8737s
Epoch: 7 cost time: 13.759740829467773
Epoch: 7, Steps: 118 | Train Loss: 0.4435524 Vali Loss: 1.1922349 Test Loss: 0.4366346
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.43485936522483826, mae:0.44036999344825745, rse:0.6278074383735657, corr:[0.25153318 0.26077297 0.2540564  0.2561842  0.25646973 0.252803
 0.25215337 0.25416753 0.2541448  0.25264797 0.25257826 0.2530488
 0.2525293  0.25158617 0.2511226  0.25089446 0.25045094 0.24963701
 0.24859874 0.24829839 0.24871372 0.24828267 0.24712703 0.24620326
 0.24621211 0.24627557 0.24603719 0.24623927 0.24731565 0.24831575
 0.24847154 0.24813007 0.24802716 0.24791834 0.24757035 0.2471849
 0.24713741 0.24751447 0.24798629 0.2481766  0.24796708 0.24796686
 0.24852584 0.2489873  0.24893843 0.24920303 0.25007537 0.25052267
 0.2502213  0.24950022 0.2488695  0.2482029  0.24706395 0.24593891
 0.24566934 0.24576555 0.24524601 0.24411763 0.24353336 0.24392042
 0.24431184 0.24406096 0.24353947 0.24340348 0.24367483 0.24402103
 0.24413452 0.24381481 0.24358699 0.24350616 0.24336286 0.2430576
 0.24273859 0.24236041 0.24218951 0.24236393 0.24241674 0.24214107
 0.24181913 0.24183077 0.24196532 0.24158382 0.24061643 0.24011272
 0.24053621 0.24109313 0.24061033 0.23958303 0.23938946 0.24010707
 0.24026094 0.2394248  0.23838347 0.2380771  0.23828827 0.23897588
 0.2400348  0.24075617 0.2409166  0.24104324 0.24114558 0.24121715
 0.24143602 0.24159983 0.24106021 0.23984277 0.23888357 0.23881102
 0.23908351 0.23911378 0.23896302 0.23895948 0.23927563 0.2398607
 0.24034747 0.24042664 0.2403378  0.24034518 0.24035695 0.24028596
 0.24021894 0.24002507 0.23956434 0.23882379 0.23795827 0.23737247
 0.23747699 0.23795192 0.23795989 0.23719358 0.23614709 0.23575912
 0.23620097 0.23629838 0.23542807 0.23445275 0.23440981 0.23506242
 0.23533353 0.23517652 0.23514067 0.23511821 0.23476227 0.23446208
 0.23425873 0.23395    0.2336793  0.23334354 0.23272143 0.23198353
 0.23191933 0.2323335  0.23222102 0.23152162 0.23112687 0.23141208
 0.23169385 0.23169139 0.23161758 0.23132883 0.23082031 0.23086831
 0.2314044  0.23145384 0.23072238 0.23016632 0.22996153 0.22974472
 0.22950135 0.22986154 0.23075745 0.23148452 0.23159273 0.2315093
 0.23151393 0.23147151 0.23120297 0.23083833 0.23042639 0.23017919
 0.22997911 0.22956027 0.22909755 0.22916585 0.22965881 0.22966965
 0.22902253 0.22898068 0.22992818 0.23032379 0.22919862 0.227876
 0.22778355 0.2284503  0.22871265 0.2283307  0.22772858 0.22704667
 0.2264671  0.22625563 0.22631988 0.22625296 0.22563158 0.22477004
 0.22437875 0.22540355 0.22694604 0.22736573 0.22675298 0.22656316
 0.22719386 0.22724637 0.22644053 0.22597061 0.22615352 0.22618718
 0.22580333 0.22567336 0.22591776 0.22598974 0.225553   0.22485158
 0.22445443 0.22436309 0.2242185  0.22381836 0.22353135 0.22400482
 0.22473529 0.22488739 0.22431916 0.22378466 0.22377262 0.2237837
 0.22309905 0.22232302 0.22208111 0.22177228 0.22090042 0.22002088
 0.22013964 0.22088534 0.22190373 0.22278734 0.22276269 0.22157241
 0.22021048 0.21981218 0.22024962 0.22059433 0.22031829 0.21959946
 0.21884653 0.21891205 0.2201806  0.22140752 0.22176477 0.22165486
 0.22158523 0.22115906 0.22073865 0.22093439 0.22125395 0.22039922
 0.21926317 0.21961202 0.22072574 0.22073013 0.21955204 0.21868351
 0.21837938 0.21789044 0.21751371 0.21742666 0.21670514 0.21591265
 0.21651292 0.21775289 0.2178158  0.21709543 0.21694337 0.2170477
 0.21632326 0.21560045 0.21573342 0.21524978 0.2137745  0.21329086
 0.21444751 0.21551849 0.21584082 0.2167343  0.21728525 0.21632677
 0.21455249 0.21355428 0.21329099 0.2136009  0.21449493 0.21508914
 0.21440236 0.21376482 0.2146553  0.21515197 0.21486133 0.21512865
 0.21600036 0.21475339 0.21270326 0.21278627 0.21398096 0.21275958
 0.21055095 0.21093495 0.21237667 0.21174818 0.21072748 0.21086314
 0.21029638 0.20804045 0.20643383 0.20560963 0.20378749 0.2031842
 0.20471187 0.20410827 0.20149472 0.20174591 0.20165256 0.19680978
 0.19389482 0.19674893 0.1918522  0.17779365 0.18259525 0.18237217]
