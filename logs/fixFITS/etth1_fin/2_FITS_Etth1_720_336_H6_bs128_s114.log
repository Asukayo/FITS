Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  100803584.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.496317625045776
Epoch: 1, Steps: 29 | Train Loss: 0.7522225 Vali Loss: 1.9041773 Test Loss: 0.9178670
Validation loss decreased (inf --> 1.904177).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.0347983837127686
Epoch: 2, Steps: 29 | Train Loss: 0.6363432 Vali Loss: 1.7497067 Test Loss: 0.8330373
Validation loss decreased (1.904177 --> 1.749707).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.225145101547241
Epoch: 3, Steps: 29 | Train Loss: 0.5663775 Vali Loss: 1.6576979 Test Loss: 0.7829734
Validation loss decreased (1.749707 --> 1.657698).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.176228046417236
Epoch: 4, Steps: 29 | Train Loss: 0.5222301 Vali Loss: 1.6027994 Test Loss: 0.7563456
Validation loss decreased (1.657698 --> 1.602799).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.587115526199341
Epoch: 5, Steps: 29 | Train Loss: 0.4929875 Vali Loss: 1.5685364 Test Loss: 0.7398371
Validation loss decreased (1.602799 --> 1.568536).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.39982271194458
Epoch: 6, Steps: 29 | Train Loss: 0.4706884 Vali Loss: 1.5407473 Test Loss: 0.7289708
Validation loss decreased (1.568536 --> 1.540747).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.457022190093994
Epoch: 7, Steps: 29 | Train Loss: 0.4545088 Vali Loss: 1.5212646 Test Loss: 0.7210897
Validation loss decreased (1.540747 --> 1.521265).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.7711760997772217
Epoch: 8, Steps: 29 | Train Loss: 0.4402455 Vali Loss: 1.5090473 Test Loss: 0.7146574
Validation loss decreased (1.521265 --> 1.509047).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.574954509735107
Epoch: 9, Steps: 29 | Train Loss: 0.4285306 Vali Loss: 1.5025164 Test Loss: 0.7104365
Validation loss decreased (1.509047 --> 1.502516).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.117393255233765
Epoch: 10, Steps: 29 | Train Loss: 0.4186885 Vali Loss: 1.4930663 Test Loss: 0.7055168
Validation loss decreased (1.502516 --> 1.493066).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.019331455230713
Epoch: 11, Steps: 29 | Train Loss: 0.4089283 Vali Loss: 1.4850439 Test Loss: 0.7016273
Validation loss decreased (1.493066 --> 1.485044).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.101220369338989
Epoch: 12, Steps: 29 | Train Loss: 0.4009033 Vali Loss: 1.4767778 Test Loss: 0.6976930
Validation loss decreased (1.485044 --> 1.476778).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.615231990814209
Epoch: 13, Steps: 29 | Train Loss: 0.3931802 Vali Loss: 1.4718689 Test Loss: 0.6943270
Validation loss decreased (1.476778 --> 1.471869).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.519869089126587
Epoch: 14, Steps: 29 | Train Loss: 0.3864091 Vali Loss: 1.4675034 Test Loss: 0.6905975
Validation loss decreased (1.471869 --> 1.467503).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.634860277175903
Epoch: 15, Steps: 29 | Train Loss: 0.3808684 Vali Loss: 1.4587307 Test Loss: 0.6877083
Validation loss decreased (1.467503 --> 1.458731).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.914778470993042
Epoch: 16, Steps: 29 | Train Loss: 0.3750111 Vali Loss: 1.4556055 Test Loss: 0.6846380
Validation loss decreased (1.458731 --> 1.455606).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.5795981884002686
Epoch: 17, Steps: 29 | Train Loss: 0.3696847 Vali Loss: 1.4492024 Test Loss: 0.6810136
Validation loss decreased (1.455606 --> 1.449202).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.754101753234863
Epoch: 18, Steps: 29 | Train Loss: 0.3645712 Vali Loss: 1.4428173 Test Loss: 0.6793024
Validation loss decreased (1.449202 --> 1.442817).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.692937612533569
Epoch: 19, Steps: 29 | Train Loss: 0.3602902 Vali Loss: 1.4333606 Test Loss: 0.6764023
Validation loss decreased (1.442817 --> 1.433361).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.869166135787964
Epoch: 20, Steps: 29 | Train Loss: 0.3562866 Vali Loss: 1.4312570 Test Loss: 0.6739720
Validation loss decreased (1.433361 --> 1.431257).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 6.081212282180786
Epoch: 21, Steps: 29 | Train Loss: 0.3519091 Vali Loss: 1.4404861 Test Loss: 0.6708704
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 6.130944490432739
Epoch: 22, Steps: 29 | Train Loss: 0.3490369 Vali Loss: 1.4301616 Test Loss: 0.6688846
Validation loss decreased (1.431257 --> 1.430162).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 5.8855979442596436
Epoch: 23, Steps: 29 | Train Loss: 0.3452590 Vali Loss: 1.4259977 Test Loss: 0.6673309
Validation loss decreased (1.430162 --> 1.425998).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 5.689145088195801
Epoch: 24, Steps: 29 | Train Loss: 0.3421748 Vali Loss: 1.4265891 Test Loss: 0.6649138
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 5.44668984413147
Epoch: 25, Steps: 29 | Train Loss: 0.3388785 Vali Loss: 1.4208404 Test Loss: 0.6631496
Validation loss decreased (1.425998 --> 1.420840).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 5.693922519683838
Epoch: 26, Steps: 29 | Train Loss: 0.3360473 Vali Loss: 1.4174690 Test Loss: 0.6610413
Validation loss decreased (1.420840 --> 1.417469).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 5.557135343551636
Epoch: 27, Steps: 29 | Train Loss: 0.3332824 Vali Loss: 1.4159970 Test Loss: 0.6595666
Validation loss decreased (1.417469 --> 1.415997).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 5.875974893569946
Epoch: 28, Steps: 29 | Train Loss: 0.3312246 Vali Loss: 1.4127015 Test Loss: 0.6577159
Validation loss decreased (1.415997 --> 1.412701).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 5.805228233337402
Epoch: 29, Steps: 29 | Train Loss: 0.3286299 Vali Loss: 1.4212911 Test Loss: 0.6562079
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 5.330477476119995
Epoch: 30, Steps: 29 | Train Loss: 0.3269276 Vali Loss: 1.4084761 Test Loss: 0.6542239
Validation loss decreased (1.412701 --> 1.408476).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 5.9969916343688965
Epoch: 31, Steps: 29 | Train Loss: 0.3247490 Vali Loss: 1.4070323 Test Loss: 0.6528232
Validation loss decreased (1.408476 --> 1.407032).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 5.697885990142822
Epoch: 32, Steps: 29 | Train Loss: 0.3228376 Vali Loss: 1.4138227 Test Loss: 0.6516799
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 5.429925918579102
Epoch: 33, Steps: 29 | Train Loss: 0.3205480 Vali Loss: 1.4071867 Test Loss: 0.6505034
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.743868112564087
Epoch: 34, Steps: 29 | Train Loss: 0.3193175 Vali Loss: 1.3986036 Test Loss: 0.6491014
Validation loss decreased (1.407032 --> 1.398604).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 5.583294153213501
Epoch: 35, Steps: 29 | Train Loss: 0.3179549 Vali Loss: 1.3998827 Test Loss: 0.6480581
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.733480453491211
Epoch: 36, Steps: 29 | Train Loss: 0.3159265 Vali Loss: 1.4082247 Test Loss: 0.6468647
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.634137392044067
Epoch: 37, Steps: 29 | Train Loss: 0.3142990 Vali Loss: 1.3974684 Test Loss: 0.6459550
Validation loss decreased (1.398604 --> 1.397468).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.440439462661743
Epoch: 38, Steps: 29 | Train Loss: 0.3132503 Vali Loss: 1.3904155 Test Loss: 0.6448574
Validation loss decreased (1.397468 --> 1.390416).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 4.4507622718811035
Epoch: 39, Steps: 29 | Train Loss: 0.3120906 Vali Loss: 1.3953891 Test Loss: 0.6439821
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.3956451416015625
Epoch: 40, Steps: 29 | Train Loss: 0.3108635 Vali Loss: 1.3916869 Test Loss: 0.6430424
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 4.303661346435547
Epoch: 41, Steps: 29 | Train Loss: 0.3095398 Vali Loss: 1.3961387 Test Loss: 0.6422852
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  100803584.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.688750267028809
Epoch: 1, Steps: 29 | Train Loss: 0.5537342 Vali Loss: 1.3238065 Test Loss: 0.5830775
Validation loss decreased (inf --> 1.323807).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.519328355789185
Epoch: 2, Steps: 29 | Train Loss: 0.5189595 Vali Loss: 1.2809196 Test Loss: 0.5420134
Validation loss decreased (1.323807 --> 1.280920).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.320305109024048
Epoch: 3, Steps: 29 | Train Loss: 0.4971603 Vali Loss: 1.2585104 Test Loss: 0.5137243
Validation loss decreased (1.280920 --> 1.258510).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.574994325637817
Epoch: 4, Steps: 29 | Train Loss: 0.4817478 Vali Loss: 1.2238133 Test Loss: 0.4936216
Validation loss decreased (1.258510 --> 1.223813).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.3620874881744385
Epoch: 5, Steps: 29 | Train Loss: 0.4700497 Vali Loss: 1.2169769 Test Loss: 0.4788708
Validation loss decreased (1.223813 --> 1.216977).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.721469402313232
Epoch: 6, Steps: 29 | Train Loss: 0.4615055 Vali Loss: 1.2046145 Test Loss: 0.4679044
Validation loss decreased (1.216977 --> 1.204615).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.3542399406433105
Epoch: 7, Steps: 29 | Train Loss: 0.4555035 Vali Loss: 1.1987700 Test Loss: 0.4599547
Validation loss decreased (1.204615 --> 1.198770).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.850421667098999
Epoch: 8, Steps: 29 | Train Loss: 0.4509211 Vali Loss: 1.1974722 Test Loss: 0.4540939
Validation loss decreased (1.198770 --> 1.197472).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.733334064483643
Epoch: 9, Steps: 29 | Train Loss: 0.4468575 Vali Loss: 1.1883583 Test Loss: 0.4498921
Validation loss decreased (1.197472 --> 1.188358).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.838731288909912
Epoch: 10, Steps: 29 | Train Loss: 0.4445486 Vali Loss: 1.1880279 Test Loss: 0.4469223
Validation loss decreased (1.188358 --> 1.188028).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.7813401222229
Epoch: 11, Steps: 29 | Train Loss: 0.4422295 Vali Loss: 1.1909814 Test Loss: 0.4446321
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.394714593887329
Epoch: 12, Steps: 29 | Train Loss: 0.4406433 Vali Loss: 1.1890897 Test Loss: 0.4429137
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.48474383354187
Epoch: 13, Steps: 29 | Train Loss: 0.4394170 Vali Loss: 1.1842453 Test Loss: 0.4417697
Validation loss decreased (1.188028 --> 1.184245).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.569150447845459
Epoch: 14, Steps: 29 | Train Loss: 0.4376834 Vali Loss: 1.1912131 Test Loss: 0.4409713
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.465531826019287
Epoch: 15, Steps: 29 | Train Loss: 0.4378189 Vali Loss: 1.1858350 Test Loss: 0.4404797
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.637440204620361
Epoch: 16, Steps: 29 | Train Loss: 0.4370091 Vali Loss: 1.1874423 Test Loss: 0.4400908
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4400654435157776, mae:0.4415196478366852, rse:0.6315542459487915, corr:[0.2511059  0.25821283 0.2562131  0.25863087 0.2575172  0.25356433
 0.25244167 0.25378728 0.25372162 0.25238603 0.2517978  0.2519945
 0.25173923 0.2508273  0.25019687 0.2501793  0.2501882  0.24989241
 0.24937762 0.24891216 0.24868026 0.24837117 0.24800931 0.24822545
 0.24873291 0.2486255  0.24828471 0.24859151 0.24913377 0.24888928
 0.24804883 0.24753429 0.24760562 0.24748516 0.24685377 0.24642114
 0.24636544 0.24621896 0.2459395  0.24572131 0.24600098 0.24665968
 0.2472526  0.24731031 0.24701524 0.24722451 0.24815597 0.24881618
 0.24853402 0.24791116 0.24715778 0.24624984 0.24507605 0.24381784
 0.24307771 0.24275705 0.24244604 0.24221571 0.24191976 0.2417395
 0.24135433 0.24098937 0.24074559 0.24070112 0.24075195 0.24108942
 0.24183582 0.24232014 0.24232097 0.24202    0.24183153 0.24188145
 0.24170592 0.24106348 0.2403539  0.24010448 0.23982786 0.23926108
 0.23872161 0.23844244 0.23823625 0.23785959 0.23735891 0.23693599
 0.2365173  0.23613657 0.2359515  0.23578429 0.23554248 0.23555736
 0.2357564  0.23575321 0.23538747 0.23520994 0.23542917 0.23622854
 0.23747163 0.23844637 0.23864919 0.23843752 0.23825836 0.23823343
 0.23832262 0.23827124 0.23802233 0.23776811 0.23743102 0.2371661
 0.23719852 0.23727955 0.23725909 0.23735361 0.23769306 0.23800083
 0.23806643 0.23807882 0.23815647 0.23803374 0.23782031 0.23801349
 0.23834598 0.23777287 0.2364462  0.23555852 0.2351296  0.23446588
 0.23408878 0.23434941 0.23424272 0.23351115 0.23283046 0.23279065
 0.23289646 0.23265609 0.2324999  0.23277842 0.23306492 0.23300262
 0.23282637 0.23268075 0.2326606  0.23288709 0.23297209 0.23267931
 0.23218396 0.23162714 0.23110697 0.23035814 0.22947878 0.22871222
 0.22834653 0.22818157 0.22812696 0.22819662 0.22800495 0.22781526
 0.22814457 0.22870985 0.22865142 0.2279034  0.22739866 0.22781332
 0.22823231 0.22796199 0.22742906 0.22747569 0.22782925 0.22793502
 0.2276999  0.22751981 0.22763295 0.22787584 0.22763723 0.22684039
 0.22611216 0.22610235 0.22651352 0.22682677 0.22650222 0.22580105
 0.2254484  0.2257487  0.22597314 0.22553727 0.22525053 0.2260683
 0.22720398 0.22747125 0.22725984 0.22742026 0.22757715 0.22717759
 0.22667798 0.22664571 0.22635396 0.22530471 0.2242175  0.22344467
 0.2225603  0.2218733  0.22203967 0.22265798 0.22226405 0.22139294
 0.221194   0.22192942 0.2224081  0.22243595 0.22270963 0.22301269
 0.22301804 0.2228551  0.22284271 0.22276676 0.22249067 0.22220129
 0.22201233 0.22191745 0.22177535 0.22151645 0.22094592 0.21997343
 0.21956176 0.21980388 0.21987097 0.21967928 0.21975428 0.21984322
 0.21923108 0.21864718 0.21898945 0.21934633 0.21909444 0.21889225
 0.21879545 0.21840143 0.21822922 0.21878174 0.21937166 0.21903598
 0.2188355  0.21920343 0.21927221 0.2183968  0.217655   0.21782441
 0.21816555 0.2183073  0.21861877 0.21878195 0.21815664 0.21735172
 0.21719426 0.21722884 0.21710241 0.2169881  0.2172059  0.21748821
 0.2178896  0.21804364 0.21768504 0.21700048 0.21713422 0.21777906
 0.21758708 0.21649882 0.21573186 0.21574955 0.21534231 0.21449602
 0.21410665 0.2140035  0.21394733 0.21427353 0.21406372 0.21288702
 0.211954   0.21245265 0.21336244 0.21332717 0.21306215 0.2135615
 0.21374562 0.21312195 0.21323746 0.21390058 0.21341659 0.21214841
 0.21221982 0.21350567 0.21335033 0.21194834 0.21120031 0.21147932
 0.21132143 0.21118623 0.21180563 0.2123042  0.21202496 0.21200305
 0.21233898 0.2121616  0.21221578 0.21225993 0.21206957 0.21191114
 0.2128811  0.21330936 0.21246774 0.21152727 0.21190473 0.21183507
 0.21025293 0.20915434 0.20939456 0.20864053 0.20664096 0.20573013
 0.20595926 0.20445387 0.20277856 0.20342243 0.20365404 0.20231813
 0.20240262 0.20328628 0.20171374 0.20083322 0.20251888 0.20188415
 0.1976508  0.19750766 0.198714   0.18975857 0.18825531 0.20418836]
