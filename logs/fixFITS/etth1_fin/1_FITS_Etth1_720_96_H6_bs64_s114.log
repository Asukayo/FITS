Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  38986752.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.102254867553711
Epoch: 1, Steps: 61 | Train Loss: 0.5685008 Vali Loss: 0.9363230 Test Loss: 0.4918450
Validation loss decreased (inf --> 0.936323).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.987649917602539
Epoch: 2, Steps: 61 | Train Loss: 0.3965313 Vali Loss: 0.7974574 Test Loss: 0.4066410
Validation loss decreased (0.936323 --> 0.797457).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.482523441314697
Epoch: 3, Steps: 61 | Train Loss: 0.3606924 Vali Loss: 0.7550759 Test Loss: 0.3862153
Validation loss decreased (0.797457 --> 0.755076).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.249021768569946
Epoch: 4, Steps: 61 | Train Loss: 0.3491721 Vali Loss: 0.7385203 Test Loss: 0.3818085
Validation loss decreased (0.755076 --> 0.738520).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.1898775100708
Epoch: 5, Steps: 61 | Train Loss: 0.3445301 Vali Loss: 0.7324327 Test Loss: 0.3803210
Validation loss decreased (0.738520 --> 0.732433).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.564743280410767
Epoch: 6, Steps: 61 | Train Loss: 0.3418995 Vali Loss: 0.7245774 Test Loss: 0.3797082
Validation loss decreased (0.732433 --> 0.724577).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.436593294143677
Epoch: 7, Steps: 61 | Train Loss: 0.3398468 Vali Loss: 0.7220967 Test Loss: 0.3798560
Validation loss decreased (0.724577 --> 0.722097).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 7.501114845275879
Epoch: 8, Steps: 61 | Train Loss: 0.3385066 Vali Loss: 0.7157629 Test Loss: 0.3795915
Validation loss decreased (0.722097 --> 0.715763).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.140629291534424
Epoch: 9, Steps: 61 | Train Loss: 0.3374576 Vali Loss: 0.7157894 Test Loss: 0.3795984
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 7.9425225257873535
Epoch: 10, Steps: 61 | Train Loss: 0.3366096 Vali Loss: 0.7134547 Test Loss: 0.3791715
Validation loss decreased (0.715763 --> 0.713455).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.963159561157227
Epoch: 11, Steps: 61 | Train Loss: 0.3357724 Vali Loss: 0.7105839 Test Loss: 0.3794585
Validation loss decreased (0.713455 --> 0.710584).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 7.736696481704712
Epoch: 12, Steps: 61 | Train Loss: 0.3352885 Vali Loss: 0.7120267 Test Loss: 0.3791972
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.66213321685791
Epoch: 13, Steps: 61 | Train Loss: 0.3346446 Vali Loss: 0.7065989 Test Loss: 0.3796445
Validation loss decreased (0.710584 --> 0.706599).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.414589405059814
Epoch: 14, Steps: 61 | Train Loss: 0.3339804 Vali Loss: 0.7063606 Test Loss: 0.3795700
Validation loss decreased (0.706599 --> 0.706361).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.055951833724976
Epoch: 15, Steps: 61 | Train Loss: 0.3335793 Vali Loss: 0.7087850 Test Loss: 0.3791859
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.2712082862854
Epoch: 16, Steps: 61 | Train Loss: 0.3336740 Vali Loss: 0.7070720 Test Loss: 0.3792327
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.583664894104004
Epoch: 17, Steps: 61 | Train Loss: 0.3330534 Vali Loss: 0.7004861 Test Loss: 0.3790785
Validation loss decreased (0.706361 --> 0.700486).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.63398003578186
Epoch: 18, Steps: 61 | Train Loss: 0.3329386 Vali Loss: 0.7044963 Test Loss: 0.3792832
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.575706243515015
Epoch: 19, Steps: 61 | Train Loss: 0.3329013 Vali Loss: 0.7073022 Test Loss: 0.3791137
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.967676401138306
Epoch: 20, Steps: 61 | Train Loss: 0.3326267 Vali Loss: 0.7036482 Test Loss: 0.3793676
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.37878409028053284, mae:0.40192514657974243, rse:0.5845931172370911, corr:[0.26743516 0.27900502 0.27852124 0.27891797 0.27740586 0.27441508
 0.2729295  0.27294728 0.27251133 0.27184045 0.2717073  0.27173135
 0.2714518  0.27108353 0.27119252 0.27151507 0.2714853  0.27115977
 0.27073914 0.2702732  0.26990697 0.26977432 0.26965052 0.26977244
 0.26984298 0.26967615 0.26954043 0.26952696 0.26946256 0.26915058
 0.26872683 0.26827154 0.2681362  0.2682085  0.26801512 0.26759338
 0.2672816  0.26732698 0.2676078  0.26776162 0.26792797 0.2681001
 0.26808384 0.26789376 0.26765442 0.2676229  0.26806682 0.26862273
 0.26840243 0.26741576 0.26619753 0.2652337  0.2642761  0.26306197
 0.2621233  0.26160374 0.26135567 0.2614126  0.26116407 0.26086158
 0.26066798 0.26089486 0.26105678 0.26079723 0.26017302 0.25985947
 0.26041296 0.26073012 0.2602414  0.2598347  0.26025864 0.26048368
 0.25945172 0.25784457 0.25671753 0.25655332 0.2560585  0.25522932
 0.25455904 0.25346956 0.2520382  0.25175473 0.2523937  0.25192687
 0.25090024 0.25080758 0.25095484 0.25003046 0.2493107  0.24866016
 0.24662115 0.24542505 0.24550009 0.24115434 0.2386914  0.2486982 ]
