Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  77973504.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.221429347991943
Epoch: 1, Steps: 30 | Train Loss: 0.6575081 Vali Loss: 1.1246173 Test Loss: 0.6098849
Validation loss decreased (inf --> 1.124617).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.16547155380249
Epoch: 2, Steps: 30 | Train Loss: 0.4710290 Vali Loss: 0.9257209 Test Loss: 0.4932678
Validation loss decreased (1.124617 --> 0.925721).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.984039783477783
Epoch: 3, Steps: 30 | Train Loss: 0.4105474 Vali Loss: 0.8408303 Test Loss: 0.4398852
Validation loss decreased (0.925721 --> 0.840830).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.8153393268585205
Epoch: 4, Steps: 30 | Train Loss: 0.3809397 Vali Loss: 0.7973720 Test Loss: 0.4109804
Validation loss decreased (0.840830 --> 0.797372).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.817491292953491
Epoch: 5, Steps: 30 | Train Loss: 0.3650160 Vali Loss: 0.7740611 Test Loss: 0.3965670
Validation loss decreased (0.797372 --> 0.774061).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.908456087112427
Epoch: 6, Steps: 30 | Train Loss: 0.3558982 Vali Loss: 0.7625338 Test Loss: 0.3888985
Validation loss decreased (0.774061 --> 0.762534).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.112263917922974
Epoch: 7, Steps: 30 | Train Loss: 0.3508733 Vali Loss: 0.7505600 Test Loss: 0.3851839
Validation loss decreased (0.762534 --> 0.750560).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.809611797332764
Epoch: 8, Steps: 30 | Train Loss: 0.3478504 Vali Loss: 0.7430321 Test Loss: 0.3827611
Validation loss decreased (0.750560 --> 0.743032).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.869717359542847
Epoch: 9, Steps: 30 | Train Loss: 0.3443430 Vali Loss: 0.7354890 Test Loss: 0.3819625
Validation loss decreased (0.743032 --> 0.735489).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.2819437980651855
Epoch: 10, Steps: 30 | Train Loss: 0.3434876 Vali Loss: 0.7268539 Test Loss: 0.3812377
Validation loss decreased (0.735489 --> 0.726854).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.250150442123413
Epoch: 11, Steps: 30 | Train Loss: 0.3414861 Vali Loss: 0.7280366 Test Loss: 0.3807346
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.40093731880188
Epoch: 12, Steps: 30 | Train Loss: 0.3413781 Vali Loss: 0.7244042 Test Loss: 0.3806712
Validation loss decreased (0.726854 --> 0.724404).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.721733570098877
Epoch: 13, Steps: 30 | Train Loss: 0.3404017 Vali Loss: 0.7188196 Test Loss: 0.3801727
Validation loss decreased (0.724404 --> 0.718820).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.975740432739258
Epoch: 14, Steps: 30 | Train Loss: 0.3398485 Vali Loss: 0.7218493 Test Loss: 0.3804102
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.9350132942199707
Epoch: 15, Steps: 30 | Train Loss: 0.3382302 Vali Loss: 0.7224075 Test Loss: 0.3803072
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.2218496799468994
Epoch: 16, Steps: 30 | Train Loss: 0.3379468 Vali Loss: 0.7217127 Test Loss: 0.3800600
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.37929341197013855, mae:0.40326082706451416, rse:0.5849860310554504, corr:[0.27105057 0.28121042 0.27823216 0.27936846 0.27818346 0.27494588
 0.27361146 0.2739266  0.27370396 0.27289695 0.27254382 0.27236125
 0.27184016 0.27128395 0.27120543 0.27145126 0.2713926  0.27094963
 0.27035853 0.2699975  0.26969987 0.26910946 0.26870397 0.268918
 0.26927194 0.26903343 0.26869246 0.26883316 0.26922408 0.26910287
 0.2684364  0.26795766 0.26808712 0.26808944 0.26746815 0.26717633
 0.26762906 0.2681743  0.26805738 0.26771778 0.26800945 0.26852047
 0.26863718 0.26850045 0.26852793 0.26882085 0.26914608 0.269288
 0.2690827  0.268485   0.2675748  0.26657525 0.26534823 0.26410455
 0.2633453  0.2627023  0.26205647 0.2620843  0.2623397  0.2621947
 0.2614714  0.26143768 0.26204535 0.26194844 0.2611289  0.26118153
 0.26236725 0.26263896 0.26192015 0.26163614 0.26199406 0.2618497
 0.26101875 0.25994837 0.2589447  0.25872782 0.25863332 0.25783938
 0.25648576 0.2557317  0.25554687 0.25486803 0.25385863 0.25386107
 0.2541356  0.2527931  0.25153336 0.25198314 0.25185338 0.2490851
 0.24711584 0.24738021 0.2435909  0.23532297 0.23565908 0.23352957]
