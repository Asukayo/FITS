Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  87105536.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.732311010360718
Epoch: 1, Steps: 30 | Train Loss: 0.7288775 Vali Loss: 1.3684591 Test Loss: 0.6907902
Validation loss decreased (inf --> 1.368459).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.53654670715332
Epoch: 2, Steps: 30 | Train Loss: 0.5621724 Vali Loss: 1.1907966 Test Loss: 0.5792771
Validation loss decreased (1.368459 --> 1.190797).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.456768751144409
Epoch: 3, Steps: 30 | Train Loss: 0.4985977 Vali Loss: 1.1111296 Test Loss: 0.5244715
Validation loss decreased (1.190797 --> 1.111130).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.202271938323975
Epoch: 4, Steps: 30 | Train Loss: 0.4646122 Vali Loss: 1.0608538 Test Loss: 0.4907120
Validation loss decreased (1.111130 --> 1.060854).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.598886728286743
Epoch: 5, Steps: 30 | Train Loss: 0.4431162 Vali Loss: 1.0252398 Test Loss: 0.4678946
Validation loss decreased (1.060854 --> 1.025240).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.504729509353638
Epoch: 6, Steps: 30 | Train Loss: 0.4292642 Vali Loss: 1.0117639 Test Loss: 0.4521049
Validation loss decreased (1.025240 --> 1.011764).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.741382598876953
Epoch: 7, Steps: 30 | Train Loss: 0.4189112 Vali Loss: 0.9968834 Test Loss: 0.4416761
Validation loss decreased (1.011764 --> 0.996883).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.968102216720581
Epoch: 8, Steps: 30 | Train Loss: 0.4116507 Vali Loss: 0.9855924 Test Loss: 0.4343894
Validation loss decreased (0.996883 --> 0.985592).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.613102674484253
Epoch: 9, Steps: 30 | Train Loss: 0.4066043 Vali Loss: 0.9774016 Test Loss: 0.4294030
Validation loss decreased (0.985592 --> 0.977402).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.563251495361328
Epoch: 10, Steps: 30 | Train Loss: 0.4025488 Vali Loss: 0.9722188 Test Loss: 0.4261552
Validation loss decreased (0.977402 --> 0.972219).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.529466152191162
Epoch: 11, Steps: 30 | Train Loss: 0.3995539 Vali Loss: 0.9719651 Test Loss: 0.4238572
Validation loss decreased (0.972219 --> 0.971965).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.5028979778289795
Epoch: 12, Steps: 30 | Train Loss: 0.3978389 Vali Loss: 0.9671276 Test Loss: 0.4222613
Validation loss decreased (0.971965 --> 0.967128).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.7417473793029785
Epoch: 13, Steps: 30 | Train Loss: 0.3955162 Vali Loss: 0.9674567 Test Loss: 0.4211450
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.431296348571777
Epoch: 14, Steps: 30 | Train Loss: 0.3942107 Vali Loss: 0.9655389 Test Loss: 0.4202668
Validation loss decreased (0.967128 --> 0.965539).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.703215837478638
Epoch: 15, Steps: 30 | Train Loss: 0.3931792 Vali Loss: 0.9655841 Test Loss: 0.4196179
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.451486110687256
Epoch: 16, Steps: 30 | Train Loss: 0.3923626 Vali Loss: 0.9647447 Test Loss: 0.4194373
Validation loss decreased (0.965539 --> 0.964745).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.063745737075806
Epoch: 17, Steps: 30 | Train Loss: 0.3911596 Vali Loss: 0.9643529 Test Loss: 0.4190538
Validation loss decreased (0.964745 --> 0.964353).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.540935039520264
Epoch: 18, Steps: 30 | Train Loss: 0.3906122 Vali Loss: 0.9607805 Test Loss: 0.4187573
Validation loss decreased (0.964353 --> 0.960781).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.607599973678589
Epoch: 19, Steps: 30 | Train Loss: 0.3900862 Vali Loss: 0.9569305 Test Loss: 0.4186048
Validation loss decreased (0.960781 --> 0.956930).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.891352891921997
Epoch: 20, Steps: 30 | Train Loss: 0.3894562 Vali Loss: 0.9594750 Test Loss: 0.4185084
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.4739134311676025
Epoch: 21, Steps: 30 | Train Loss: 0.3887165 Vali Loss: 0.9618378 Test Loss: 0.4184192
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.612119197845459
Epoch: 22, Steps: 30 | Train Loss: 0.3883422 Vali Loss: 0.9646577 Test Loss: 0.4182627
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.41290247440338135, mae:0.4231650233268738, rse:0.6102124452590942, corr:[0.26008272 0.27310684 0.2687433  0.27072614 0.26993176 0.26633707
 0.2652862  0.2662164  0.26611012 0.26488692 0.2640809  0.26392275
 0.2638429  0.26346937 0.26274902 0.26233467 0.262583   0.2626838
 0.26190722 0.2611943  0.2613806  0.26136854 0.26071057 0.26022893
 0.26046607 0.26053068 0.2603613  0.26057243 0.26116547 0.261306
 0.26074046 0.26017344 0.26024443 0.26047656 0.2602419  0.25975704
 0.25956607 0.25990385 0.26027358 0.26028153 0.26024517 0.26057407
 0.2611485  0.26140717 0.2612091  0.26121762 0.26176313 0.26203504
 0.2616759  0.26092163 0.2602637  0.25960973 0.2584244  0.25707185
 0.25651446 0.2563277  0.25568298 0.25497073 0.25491336 0.25515378
 0.2548991  0.25453696 0.2547783  0.2552003  0.25519758 0.25532484
 0.25600788 0.2563347  0.25607938 0.25580537 0.25574234 0.25551564
 0.25502524 0.25447795 0.25397474 0.25365326 0.25349468 0.25328633
 0.25280538 0.2524066  0.25240058 0.2521679  0.25134134 0.2508188
 0.2511995  0.25152546 0.2510158  0.2504215  0.25058943 0.25098002
 0.25080362 0.25038975 0.2502548  0.25026578 0.25008222 0.25036854
 0.25119343 0.25157964 0.25155574 0.25163814 0.25161487 0.25144917
 0.2516203  0.25172588 0.25127608 0.25086835 0.25093588 0.25083598
 0.25023177 0.24983947 0.25028297 0.2507949  0.2506098  0.2505246
 0.25096497 0.25111842 0.25076884 0.25057447 0.25063202 0.2504202
 0.24995492 0.24949521 0.2488749  0.24820662 0.24757284 0.2467596
 0.24603128 0.24564356 0.2454053  0.24486831 0.24403352 0.24373
 0.24400435 0.24399881 0.24358717 0.24367948 0.24437061 0.24443866
 0.24392994 0.2438715  0.24422598 0.24404636 0.24314617 0.24248193
 0.24215882 0.24136496 0.24042399 0.23969509 0.23881684 0.23763998
 0.23699948 0.23686181 0.23664676 0.2366138  0.23695315 0.23712473
 0.23657002 0.23626629 0.23676665 0.2366661  0.2357071  0.23561431
 0.2364455  0.23622528 0.23489536 0.2344639  0.23495701 0.23410548
 0.23249342 0.2323871  0.23280664 0.23206766 0.23064743 0.23000814
 0.22975473 0.22930425 0.22868723 0.22837314 0.22780237 0.22752567
 0.22763787 0.22647    0.22438844 0.22455776 0.22531176 0.22194378
 0.21845733 0.22041304 0.21828079 0.2064334  0.20711279 0.21402065]
