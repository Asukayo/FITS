Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 7.912874460220337
Epoch: 1, Steps: 59 | Train Loss: 0.6972898 Vali Loss: 1.7439628 Test Loss: 0.8336298
Validation loss decreased (inf --> 1.743963).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.805646896362305
Epoch: 2, Steps: 59 | Train Loss: 0.5448621 Vali Loss: 1.5902860 Test Loss: 0.7523731
Validation loss decreased (1.743963 --> 1.590286).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.1669762134552
Epoch: 3, Steps: 59 | Train Loss: 0.4775600 Vali Loss: 1.5370713 Test Loss: 0.7241384
Validation loss decreased (1.590286 --> 1.537071).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.996106386184692
Epoch: 4, Steps: 59 | Train Loss: 0.4390399 Vali Loss: 1.5024250 Test Loss: 0.7098185
Validation loss decreased (1.537071 --> 1.502425).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.68207597732544
Epoch: 5, Steps: 59 | Train Loss: 0.4118537 Vali Loss: 1.4796511 Test Loss: 0.6965988
Validation loss decreased (1.502425 --> 1.479651).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.954625606536865
Epoch: 6, Steps: 59 | Train Loss: 0.3896029 Vali Loss: 1.4581313 Test Loss: 0.6859003
Validation loss decreased (1.479651 --> 1.458131).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.914650678634644
Epoch: 7, Steps: 59 | Train Loss: 0.3712651 Vali Loss: 1.4384146 Test Loss: 0.6764168
Validation loss decreased (1.458131 --> 1.438415).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.733980894088745
Epoch: 8, Steps: 59 | Train Loss: 0.3550440 Vali Loss: 1.4240794 Test Loss: 0.6668438
Validation loss decreased (1.438415 --> 1.424079).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.20778226852417
Epoch: 9, Steps: 59 | Train Loss: 0.3408564 Vali Loss: 1.4156084 Test Loss: 0.6577775
Validation loss decreased (1.424079 --> 1.415608).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 6.610500335693359
Epoch: 10, Steps: 59 | Train Loss: 0.3286909 Vali Loss: 1.4032580 Test Loss: 0.6503685
Validation loss decreased (1.415608 --> 1.403258).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.207611083984375
Epoch: 11, Steps: 59 | Train Loss: 0.3178524 Vali Loss: 1.3941361 Test Loss: 0.6426209
Validation loss decreased (1.403258 --> 1.394136).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.726792097091675
Epoch: 12, Steps: 59 | Train Loss: 0.3083536 Vali Loss: 1.3899472 Test Loss: 0.6347566
Validation loss decreased (1.394136 --> 1.389947).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.959175825119019
Epoch: 13, Steps: 59 | Train Loss: 0.2995149 Vali Loss: 1.3771263 Test Loss: 0.6282579
Validation loss decreased (1.389947 --> 1.377126).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.101819515228271
Epoch: 14, Steps: 59 | Train Loss: 0.2916750 Vali Loss: 1.3683099 Test Loss: 0.6206926
Validation loss decreased (1.377126 --> 1.368310).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.0469810962677
Epoch: 15, Steps: 59 | Train Loss: 0.2845381 Vali Loss: 1.3602605 Test Loss: 0.6145959
Validation loss decreased (1.368310 --> 1.360260).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.830038785934448
Epoch: 16, Steps: 59 | Train Loss: 0.2780200 Vali Loss: 1.3531462 Test Loss: 0.6094917
Validation loss decreased (1.360260 --> 1.353146).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.463419675827026
Epoch: 17, Steps: 59 | Train Loss: 0.2722595 Vali Loss: 1.3517215 Test Loss: 0.6035663
Validation loss decreased (1.353146 --> 1.351722).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.509965181350708
Epoch: 18, Steps: 59 | Train Loss: 0.2669629 Vali Loss: 1.3472823 Test Loss: 0.5982636
Validation loss decreased (1.351722 --> 1.347282).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.241589546203613
Epoch: 19, Steps: 59 | Train Loss: 0.2620166 Vali Loss: 1.3398430 Test Loss: 0.5941816
Validation loss decreased (1.347282 --> 1.339843).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 11.468507289886475
Epoch: 20, Steps: 59 | Train Loss: 0.2574825 Vali Loss: 1.3324746 Test Loss: 0.5893289
Validation loss decreased (1.339843 --> 1.332475).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.652223587036133
Epoch: 21, Steps: 59 | Train Loss: 0.2534100 Vali Loss: 1.3313769 Test Loss: 0.5859726
Validation loss decreased (1.332475 --> 1.331377).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.190443277359009
Epoch: 22, Steps: 59 | Train Loss: 0.2496904 Vali Loss: 1.3274704 Test Loss: 0.5814878
Validation loss decreased (1.331377 --> 1.327470).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 11.088149785995483
Epoch: 23, Steps: 59 | Train Loss: 0.2463594 Vali Loss: 1.3246529 Test Loss: 0.5775658
Validation loss decreased (1.327470 --> 1.324653).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 11.503495931625366
Epoch: 24, Steps: 59 | Train Loss: 0.2431819 Vali Loss: 1.3181318 Test Loss: 0.5739667
Validation loss decreased (1.324653 --> 1.318132).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 11.324138164520264
Epoch: 25, Steps: 59 | Train Loss: 0.2402860 Vali Loss: 1.3157943 Test Loss: 0.5713010
Validation loss decreased (1.318132 --> 1.315794).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 11.353830575942993
Epoch: 26, Steps: 59 | Train Loss: 0.2374534 Vali Loss: 1.3179752 Test Loss: 0.5682806
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.598955154418945
Epoch: 27, Steps: 59 | Train Loss: 0.2350316 Vali Loss: 1.3096126 Test Loss: 0.5653464
Validation loss decreased (1.315794 --> 1.309613).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.948117971420288
Epoch: 28, Steps: 59 | Train Loss: 0.2325347 Vali Loss: 1.3131803 Test Loss: 0.5627657
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 11.474534749984741
Epoch: 29, Steps: 59 | Train Loss: 0.2305051 Vali Loss: 1.3027960 Test Loss: 0.5605190
Validation loss decreased (1.309613 --> 1.302796).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 11.40410041809082
Epoch: 30, Steps: 59 | Train Loss: 0.2284464 Vali Loss: 1.3030524 Test Loss: 0.5579947
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 11.197005271911621
Epoch: 31, Steps: 59 | Train Loss: 0.2263940 Vali Loss: 1.3019422 Test Loss: 0.5554804
Validation loss decreased (1.302796 --> 1.301942).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 10.812242984771729
Epoch: 32, Steps: 59 | Train Loss: 0.2246964 Vali Loss: 1.3052547 Test Loss: 0.5534272
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 11.231132507324219
Epoch: 33, Steps: 59 | Train Loss: 0.2231238 Vali Loss: 1.3026925 Test Loss: 0.5512910
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 11.592044115066528
Epoch: 34, Steps: 59 | Train Loss: 0.2215539 Vali Loss: 1.3001274 Test Loss: 0.5497807
Validation loss decreased (1.301942 --> 1.300127).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 11.627756834030151
Epoch: 35, Steps: 59 | Train Loss: 0.2202362 Vali Loss: 1.2950456 Test Loss: 0.5478334
Validation loss decreased (1.300127 --> 1.295046).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 11.256476640701294
Epoch: 36, Steps: 59 | Train Loss: 0.2187685 Vali Loss: 1.2960730 Test Loss: 0.5462382
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 11.201642990112305
Epoch: 37, Steps: 59 | Train Loss: 0.2174530 Vali Loss: 1.2958900 Test Loss: 0.5445907
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 11.126910209655762
Epoch: 38, Steps: 59 | Train Loss: 0.2164966 Vali Loss: 1.2987547 Test Loss: 0.5432994
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.733402252197266
Epoch: 1, Steps: 59 | Train Loss: 0.4800700 Vali Loss: 1.2263629 Test Loss: 0.4813213
Validation loss decreased (inf --> 1.226363).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.120227098464966
Epoch: 2, Steps: 59 | Train Loss: 0.4535240 Vali Loss: 1.2029359 Test Loss: 0.4540719
Validation loss decreased (1.226363 --> 1.202936).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.719443321228027
Epoch: 3, Steps: 59 | Train Loss: 0.4419904 Vali Loss: 1.1992834 Test Loss: 0.4440395
Validation loss decreased (1.202936 --> 1.199283).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.702926635742188
Epoch: 4, Steps: 59 | Train Loss: 0.4371719 Vali Loss: 1.1955538 Test Loss: 0.4404411
Validation loss decreased (1.199283 --> 1.195554).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.69535756111145
Epoch: 5, Steps: 59 | Train Loss: 0.4353617 Vali Loss: 1.2033848 Test Loss: 0.4401529
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.293508768081665
Epoch: 6, Steps: 59 | Train Loss: 0.4339909 Vali Loss: 1.1984843 Test Loss: 0.4404714
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.131723880767822
Epoch: 7, Steps: 59 | Train Loss: 0.4337186 Vali Loss: 1.2011635 Test Loss: 0.4407231
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.43930989503860474, mae:0.4401049017906189, rse:0.6310118436813354, corr:[0.25335354 0.2603397  0.25823542 0.25921652 0.25846347 0.25479543
 0.2526467  0.2532549  0.25350034 0.25248942 0.25181395 0.25225985
 0.25275135 0.2522671  0.2513383  0.25090128 0.25109056 0.25120378
 0.25051633 0.24950853 0.24933049 0.24978648 0.24989057 0.24969105
 0.24992359 0.2501826  0.2498544  0.24914506 0.24867126 0.24833573
 0.24781309 0.24738021 0.24742079 0.24748425 0.24707028 0.24677685
 0.24700144 0.24735066 0.24732672 0.24703993 0.2470197  0.2472415
 0.24750029 0.24755628 0.24747464 0.24787878 0.24882706 0.24938455
 0.24898298 0.24817625 0.24736284 0.24655819 0.24540262 0.24423511
 0.2436934  0.24327825 0.24236654 0.24171488 0.24191175 0.24257861
 0.24246886 0.24180171 0.24120706 0.24082093 0.24041657 0.24017648
 0.24050526 0.24103509 0.24152124 0.24177444 0.2415166  0.24117832
 0.24138793 0.24164595 0.2412454  0.24064794 0.24037248 0.24035567
 0.23990116 0.23906949 0.2384696  0.23824896 0.23796113 0.23786372
 0.23825243 0.23837502 0.23751998 0.23619756 0.23544751 0.23551697
 0.23567154 0.2357404  0.23575155 0.23575541 0.23568344 0.23607881
 0.23728284 0.23852064 0.23901987 0.23898616 0.2388295  0.23863848
 0.23835209 0.23793522 0.23750558 0.23740274 0.23749368 0.2375517
 0.23755476 0.23754421 0.23758242 0.23748411 0.23717004 0.23679583
 0.23654614 0.23642965 0.23657796 0.23697585 0.23719579 0.23684524
 0.23589593 0.23475225 0.23431827 0.23461223 0.23463239 0.23409048
 0.23394728 0.23425786 0.2337436  0.2325184  0.23173614 0.2320164
 0.23262553 0.2327217  0.23253186 0.2324945  0.23238702 0.2320473
 0.23180212 0.23198009 0.23250477 0.23297153 0.23278588 0.23195249
 0.2311336  0.2308401  0.23066548 0.2296373  0.2282052  0.22748452
 0.22771072 0.22796133 0.22802064 0.22847424 0.22900857 0.22891648
 0.22830804 0.22790983 0.22788252 0.2276392  0.2271666  0.2273035
 0.2277691  0.22782396 0.22722137 0.2266328  0.22647077 0.22672926
 0.22686481 0.22663659 0.22662905 0.22731169 0.22778447 0.22738536
 0.22695285 0.22737674 0.22778833 0.2273093  0.22624187 0.22562054
 0.22551021 0.22544032 0.22537385 0.22550258 0.22582068 0.22622389
 0.22660828 0.22687297 0.22699001 0.22712383 0.22714399 0.22672288
 0.22606508 0.22594033 0.22592355 0.22504626 0.22359872 0.22270319
 0.22253996 0.22236899 0.22194457 0.22192046 0.22212297 0.22221424
 0.22210908 0.22225866 0.22227216 0.22173268 0.22132608 0.22172011
 0.2225477  0.22275531 0.22217315 0.22151807 0.22141904 0.2217581
 0.22165018 0.22060476 0.21916439 0.21851332 0.2189182  0.2192804
 0.21929629 0.21936963 0.21997404 0.22071207 0.22081926 0.22015265
 0.21935892 0.21918522 0.21936193 0.21914029 0.21890123 0.2191271
 0.21901277 0.21809131 0.21780747 0.21857324 0.21865718 0.21704878
 0.21594837 0.21659422 0.21737972 0.21669047 0.21587992 0.21631701
 0.21702313 0.21698926 0.21691303 0.21725963 0.2173369  0.21709819
 0.21724351 0.2175646  0.21784204 0.21824701 0.21885084 0.21902834
 0.2188815  0.21879748 0.2187851  0.2183903  0.21816665 0.21817271
 0.21763287 0.21657103 0.21612442 0.21640505 0.21587321 0.21463291
 0.2140668  0.21394186 0.2132437  0.21258022 0.21256596 0.21283437
 0.21283525 0.21296194 0.21336515 0.21309468 0.21231109 0.21241164
 0.21330808 0.21350528 0.21331695 0.2132129  0.21256757 0.2110621
 0.21032779 0.21141742 0.21215974 0.21141653 0.21055509 0.21118551
 0.2118819  0.21160373 0.21111324 0.21130234 0.21176845 0.21221286
 0.21237452 0.21176715 0.21134214 0.21157749 0.21251705 0.21271066
 0.2120408  0.21099496 0.21087565 0.21132661 0.21172315 0.21125287
 0.21031035 0.21030334 0.21092157 0.2098328  0.20727952 0.20611583
 0.20742188 0.20783488 0.20626524 0.20466451 0.20359974 0.20297727
 0.20346005 0.20422578 0.20366001 0.20332691 0.20318887 0.20131494
 0.1987633  0.20032804 0.20257981 0.19696347 0.1947093  0.20474033]
