Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  128708608.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.712740182876587
Epoch: 1, Steps: 14 | Train Loss: 1.0455420 Vali Loss: 2.2930632 Test Loss: 1.0029653
Validation loss decreased (inf --> 2.293063).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.8227810859680176
Epoch: 2, Steps: 14 | Train Loss: 0.9255441 Vali Loss: 2.0789213 Test Loss: 0.8787965
Validation loss decreased (2.293063 --> 2.078921).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.8386130332946777
Epoch: 3, Steps: 14 | Train Loss: 0.8461534 Vali Loss: 1.9369565 Test Loss: 0.7978818
Validation loss decreased (2.078921 --> 1.936957).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.7957117557525635
Epoch: 4, Steps: 14 | Train Loss: 0.7938414 Vali Loss: 1.8515732 Test Loss: 0.7453948
Validation loss decreased (1.936957 --> 1.851573).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.932729721069336
Epoch: 5, Steps: 14 | Train Loss: 0.7597909 Vali Loss: 1.7924117 Test Loss: 0.7102458
Validation loss decreased (1.851573 --> 1.792412).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.7163383960723877
Epoch: 6, Steps: 14 | Train Loss: 0.7363998 Vali Loss: 1.7532535 Test Loss: 0.6858424
Validation loss decreased (1.792412 --> 1.753253).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.81882905960083
Epoch: 7, Steps: 14 | Train Loss: 0.7196348 Vali Loss: 1.7268054 Test Loss: 0.6677905
Validation loss decreased (1.753253 --> 1.726805).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.2645645141601562
Epoch: 8, Steps: 14 | Train Loss: 0.7065885 Vali Loss: 1.7021866 Test Loss: 0.6539322
Validation loss decreased (1.726805 --> 1.702187).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.2800941467285156
Epoch: 9, Steps: 14 | Train Loss: 0.6964718 Vali Loss: 1.6861093 Test Loss: 0.6426293
Validation loss decreased (1.702187 --> 1.686109).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.2008543014526367
Epoch: 10, Steps: 14 | Train Loss: 0.6880285 Vali Loss: 1.6704437 Test Loss: 0.6332113
Validation loss decreased (1.686109 --> 1.670444).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.1368069648742676
Epoch: 11, Steps: 14 | Train Loss: 0.6808895 Vali Loss: 1.6547718 Test Loss: 0.6248706
Validation loss decreased (1.670444 --> 1.654772).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.2081353664398193
Epoch: 12, Steps: 14 | Train Loss: 0.6748015 Vali Loss: 1.6438490 Test Loss: 0.6175026
Validation loss decreased (1.654772 --> 1.643849).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.8178367614746094
Epoch: 13, Steps: 14 | Train Loss: 0.6695210 Vali Loss: 1.6405927 Test Loss: 0.6110020
Validation loss decreased (1.643849 --> 1.640593).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.1204605102539062
Epoch: 14, Steps: 14 | Train Loss: 0.6649326 Vali Loss: 1.6348792 Test Loss: 0.6050385
Validation loss decreased (1.640593 --> 1.634879).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.3727924823760986
Epoch: 15, Steps: 14 | Train Loss: 0.6601741 Vali Loss: 1.6235032 Test Loss: 0.5997308
Validation loss decreased (1.634879 --> 1.623503).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.5700786113739014
Epoch: 16, Steps: 14 | Train Loss: 0.6563740 Vali Loss: 1.6134748 Test Loss: 0.5947323
Validation loss decreased (1.623503 --> 1.613475).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.9877922534942627
Epoch: 17, Steps: 14 | Train Loss: 0.6529998 Vali Loss: 1.6052876 Test Loss: 0.5901335
Validation loss decreased (1.613475 --> 1.605288).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.623950719833374
Epoch: 18, Steps: 14 | Train Loss: 0.6498397 Vali Loss: 1.6007739 Test Loss: 0.5859694
Validation loss decreased (1.605288 --> 1.600774).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.741218090057373
Epoch: 19, Steps: 14 | Train Loss: 0.6468309 Vali Loss: 1.5966620 Test Loss: 0.5820752
Validation loss decreased (1.600774 --> 1.596662).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.688650131225586
Epoch: 20, Steps: 14 | Train Loss: 0.6439072 Vali Loss: 1.5871782 Test Loss: 0.5784782
Validation loss decreased (1.596662 --> 1.587178).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.7571046352386475
Epoch: 21, Steps: 14 | Train Loss: 0.6417337 Vali Loss: 1.5861089 Test Loss: 0.5750918
Validation loss decreased (1.587178 --> 1.586109).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.5413036346435547
Epoch: 22, Steps: 14 | Train Loss: 0.6393612 Vali Loss: 1.5841961 Test Loss: 0.5719612
Validation loss decreased (1.586109 --> 1.584196).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.572737693786621
Epoch: 23, Steps: 14 | Train Loss: 0.6372315 Vali Loss: 1.5811987 Test Loss: 0.5690342
Validation loss decreased (1.584196 --> 1.581199).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.702855348587036
Epoch: 24, Steps: 14 | Train Loss: 0.6352642 Vali Loss: 1.5728258 Test Loss: 0.5663055
Validation loss decreased (1.581199 --> 1.572826).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.9071011543273926
Epoch: 25, Steps: 14 | Train Loss: 0.6333986 Vali Loss: 1.5690513 Test Loss: 0.5637277
Validation loss decreased (1.572826 --> 1.569051).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.626511573791504
Epoch: 26, Steps: 14 | Train Loss: 0.6317268 Vali Loss: 1.5679544 Test Loss: 0.5613407
Validation loss decreased (1.569051 --> 1.567954).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.709364891052246
Epoch: 27, Steps: 14 | Train Loss: 0.6307419 Vali Loss: 1.5664008 Test Loss: 0.5590709
Validation loss decreased (1.567954 --> 1.566401).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.7294833660125732
Epoch: 28, Steps: 14 | Train Loss: 0.6286146 Vali Loss: 1.5565457 Test Loss: 0.5569802
Validation loss decreased (1.566401 --> 1.556546).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.8549704551696777
Epoch: 29, Steps: 14 | Train Loss: 0.6272373 Vali Loss: 1.5559886 Test Loss: 0.5550178
Validation loss decreased (1.556546 --> 1.555989).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.7331886291503906
Epoch: 30, Steps: 14 | Train Loss: 0.6260267 Vali Loss: 1.5583941 Test Loss: 0.5531766
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.8837053775787354
Epoch: 31, Steps: 14 | Train Loss: 0.6249967 Vali Loss: 1.5513582 Test Loss: 0.5514147
Validation loss decreased (1.555989 --> 1.551358).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.649367094039917
Epoch: 32, Steps: 14 | Train Loss: 0.6240917 Vali Loss: 1.5535138 Test Loss: 0.5497531
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.586052417755127
Epoch: 33, Steps: 14 | Train Loss: 0.6226794 Vali Loss: 1.5499005 Test Loss: 0.5482168
Validation loss decreased (1.551358 --> 1.549901).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.7076706886291504
Epoch: 34, Steps: 14 | Train Loss: 0.6219928 Vali Loss: 1.5470035 Test Loss: 0.5467644
Validation loss decreased (1.549901 --> 1.547004).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.6053667068481445
Epoch: 35, Steps: 14 | Train Loss: 0.6210873 Vali Loss: 1.5415142 Test Loss: 0.5454043
Validation loss decreased (1.547004 --> 1.541514).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.607461452484131
Epoch: 36, Steps: 14 | Train Loss: 0.6201499 Vali Loss: 1.5364031 Test Loss: 0.5441192
Validation loss decreased (1.541514 --> 1.536403).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.7484629154205322
Epoch: 37, Steps: 14 | Train Loss: 0.6190968 Vali Loss: 1.5450233 Test Loss: 0.5428822
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.835677146911621
Epoch: 38, Steps: 14 | Train Loss: 0.6187479 Vali Loss: 1.5393100 Test Loss: 0.5417407
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.746046781539917
Epoch: 39, Steps: 14 | Train Loss: 0.6177201 Vali Loss: 1.5390620 Test Loss: 0.5406303
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.519088864326477, mae:0.5153391361236572, rse:0.6897200345993042, corr:[0.17991519 0.21934585 0.22131316 0.21497959 0.21551222 0.22138092
 0.22587919 0.22700895 0.22659671 0.22606815 0.22572866 0.22484508
 0.22331677 0.22230548 0.22201341 0.22159673 0.22008167 0.21739721
 0.21507597 0.21425287 0.21476674 0.21479587 0.21370731 0.21195112
 0.21129079 0.21230413 0.2142306  0.21579118 0.21656904 0.21684082
 0.21714462 0.21769741 0.21831343 0.21847433 0.21794772 0.21704891
 0.21623205 0.21533339 0.2145287  0.21407622 0.21378353 0.21349683
 0.21293701 0.21234514 0.21185113 0.21203868 0.21272397 0.21371228
 0.21484743 0.21533889 0.21510619 0.21463743 0.21424672 0.21394809
 0.21364807 0.21310197 0.21275045 0.21207564 0.21138822 0.21083377
 0.21056886 0.21036    0.21015741 0.20974544 0.20902944 0.20844674
 0.2081786  0.2082442  0.20862195 0.20857374 0.20776437 0.20658548
 0.20557208 0.20524038 0.20572744 0.20644297 0.20677337 0.20662798
 0.20634477 0.20631656 0.20640914 0.20641546 0.2060457  0.20531766
 0.20401853 0.20248988 0.20124231 0.20082317 0.20090383 0.20090702
 0.20055209 0.20000966 0.19929385 0.19875278 0.19852789 0.19942428
 0.20123874 0.20326506 0.20488888 0.20589232 0.2063992  0.20673892
 0.20708878 0.20742086 0.20742078 0.20674393 0.2056803  0.2048479
 0.20429216 0.2042     0.20407102 0.20359239 0.20279267 0.20203872
 0.20168376 0.20157711 0.2015443  0.20151228 0.20142338 0.20139027
 0.20124063 0.20099598 0.20095089 0.20145105 0.20207128 0.20233646
 0.20207411 0.20160784 0.20146196 0.20162119 0.20174417 0.2014473
 0.20082776 0.20003873 0.19949372 0.19936082 0.1991821  0.19883478
 0.19826433 0.19780213 0.19751514 0.19703063 0.19607732 0.19555943
 0.19560061 0.19603947 0.19643126 0.1964496  0.19614057 0.1958798
 0.1960724  0.19661258 0.19692479 0.19639657 0.19536279 0.19487683
 0.1950068  0.19557087 0.19607258 0.19644214 0.19647709 0.19641332
 0.19641408 0.19619873 0.19566052 0.1951512  0.19428979 0.19385801
 0.19382793 0.19427615 0.19499893 0.19611348 0.19726202 0.19802369
 0.19821848 0.19823292 0.1983349  0.1986807  0.19898827 0.19877161
 0.19787768 0.19675761 0.19622354 0.19659173 0.19737346 0.19770929
 0.19731782 0.19664052 0.19608463 0.19552982 0.1950115  0.1946145
 0.19432956 0.19416612 0.19413438 0.19393894 0.19349194 0.19300781
 0.19284153 0.19315474 0.19344473 0.19305304 0.19227113 0.19175763
 0.1921094  0.19320907 0.19411626 0.19426186 0.19376484 0.193029
 0.19230045 0.19160905 0.19087847 0.19019938 0.18971293 0.1898585
 0.19043551 0.19092478 0.19117227 0.19159277 0.19238816 0.19315006
 0.1932612  0.19268103 0.1917354  0.19116254 0.1910399  0.19123529
 0.1908762  0.19005232 0.18928607 0.1891568  0.18947607 0.18984148
 0.1899259  0.18952514 0.18873157 0.1878603  0.18708844 0.1866625
 0.1871052  0.18839894 0.19034661 0.19205281 0.19278264 0.19257887
 0.19219895 0.19214214 0.19215687 0.19170873 0.19068404 0.18965597
 0.18901037 0.18918902 0.18982598 0.19033773 0.19049469 0.19047542
 0.19052675 0.19049409 0.19031149 0.18984158 0.18935902 0.18899141
 0.18892507 0.18899186 0.18904538 0.18921097 0.189529   0.19002426
 0.19039668 0.19019666 0.18966532 0.18946363 0.1899849  0.19090022
 0.19136445 0.19143496 0.19089587 0.19060458 0.19056875 0.19062498
 0.19020538 0.18972746 0.18923493 0.1889165  0.18853603 0.18834928
 0.18842252 0.18903248 0.19036126 0.19190231 0.19289088 0.19293971
 0.19245286 0.19234546 0.19268782 0.19293064 0.19257095 0.1918171
 0.19113643 0.19104773 0.19145626 0.192028   0.1922433  0.19226955
 0.19260502 0.19262725 0.19203247 0.19113919 0.19053073 0.19097011
 0.19246347 0.19394079 0.19465044 0.19459155 0.19438915 0.19452982
 0.19451512 0.19383447 0.1926762  0.19177699 0.1919049  0.19271742
 0.19345795 0.1933848  0.19294453 0.19302776 0.1935856  0.19422205
 0.19434269 0.1938341  0.19335306 0.19336048 0.19332035 0.1930267
 0.19237624 0.19177373 0.1919095  0.19276553 0.19333938 0.19293644
 0.19190018 0.1910317  0.19084106 0.19124177 0.1913258  0.19083363
 0.19013916 0.18989147 0.19012208 0.19045144 0.19051738 0.19034092
 0.18994893 0.18957724 0.18917352 0.18862349 0.1879486  0.18774387
 0.188409   0.18929148 0.1897365  0.18956907 0.18933304 0.18937318
 0.18959665 0.18950626 0.18882203 0.1879368  0.18767256 0.18834475
 0.18883932 0.18820328 0.1866249  0.18512185 0.18461193 0.1849947
 0.185207   0.18427123 0.18257464 0.18133235 0.1811363  0.18217859
 0.18385527 0.18546686 0.18710473 0.18888713 0.19045764 0.19070102
 0.18943986 0.18758959 0.18635137 0.18608168 0.18603532 0.18550332
 0.18428342 0.18312211 0.18297756 0.18372974 0.18467495 0.18506622
 0.18487112 0.18443349 0.18421152 0.18417698 0.18391718 0.18364485
 0.18394268 0.18498996 0.18632734 0.18715103 0.18748514 0.18747135
 0.1876926  0.18770857 0.1870677  0.18577833 0.1847405  0.18487628
 0.18580158 0.18651338 0.1859187  0.18477231 0.18420209 0.18470018
 0.1854202  0.18552768 0.18467808 0.18399075 0.18405911 0.18488479
 0.18604131 0.18678829 0.1870882  0.187973   0.1893242  0.19002898
 0.18964021 0.18875322 0.18830377 0.18901391 0.1899415  0.18999177
 0.18854676 0.1864753  0.18538544 0.18548584 0.18604475 0.18603192
 0.18532266 0.18440096 0.18381758 0.18346742 0.18329303 0.18341261
 0.18423593 0.18580087 0.18734145 0.18791738 0.1875275  0.18675937
 0.18671788 0.18721716 0.18734834 0.18689758 0.18600358 0.18570004
 0.18629745 0.1872783  0.18773106 0.18740614 0.18691476 0.18700689
 0.18741867 0.1874648  0.18664768 0.18537606 0.18445723 0.184181
 0.18431105 0.18428968 0.18397802 0.1841795  0.18525873 0.18605429
 0.18575597 0.18469222 0.18358292 0.18336223 0.18388942 0.18425229
 0.18373135 0.18251234 0.18168737 0.18176556 0.18222523 0.18202442
 0.18081208 0.17921586 0.17811053 0.1777596  0.17780073 0.17803575
 0.17858131 0.17963813 0.18083726 0.18162256 0.18166037 0.181121
 0.18061    0.18088312 0.18127729 0.18092334 0.18014911 0.17983194
 0.18034896 0.18150973 0.18271899 0.18306105 0.18295385 0.18290454
 0.18307033 0.1833518  0.183842   0.18380937 0.18399316 0.1837939
 0.18396349 0.18384519 0.18341592 0.18374926 0.18466875 0.18572368
 0.18602985 0.18542708 0.18458591 0.18444565 0.18497187 0.1858238
 0.18611796 0.18561213 0.18532196 0.18561056 0.18632025 0.18654695
 0.18610126 0.18554203 0.18523403 0.18499531 0.18432601 0.1831995
 0.18222988 0.18231815 0.1830736  0.18338895 0.18264689 0.18125637
 0.18040358 0.18063402 0.18089473 0.18037112 0.17902112 0.17783032
 0.17727606 0.17713012 0.17742799 0.17707494 0.17668687 0.17642641
 0.17646061 0.1760741  0.17524864 0.17428592 0.17344195 0.17281984
 0.17229335 0.17169988 0.17091532 0.17133224 0.17276086 0.17393817
 0.17396994 0.17234838 0.17075679 0.17052646 0.17189312 0.17327897
 0.17331888 0.17190075 0.17080167 0.17106706 0.1721153  0.17257437
 0.1719145  0.1708747  0.17031324 0.17008866 0.16927283 0.16764279
 0.16622204 0.16617115 0.16739313 0.16833629 0.16761255 0.16535585
 0.1637826  0.16427761 0.16564943 0.16615957 0.16500692 0.16369767
 0.16365112 0.16480915 0.16593698 0.16582826 0.1646925  0.16382432
 0.1641004  0.16509672 0.16582203 0.16598085 0.16606654 0.16627912
 0.16623336 0.16529998 0.1634991  0.16255419 0.1634073  0.1650766
 0.16593565 0.16493356 0.16311704 0.16262735 0.1640349  0.16611123
 0.16695008 0.1656544  0.16413972 0.16408992 0.16519651 0.16597612
 0.16543825 0.16431184 0.16376767 0.16389364 0.16379032 0.16278921
 0.16143322 0.16104665 0.16191854 0.16322836 0.16290864 0.1608577
 0.1592123  0.15966745 0.16162296 0.16291852 0.16229475 0.16094708
 0.16047208 0.1609522  0.1615893  0.1614468  0.16085777 0.16067882
 0.16090906 0.16097288 0.16023427 0.1592506  0.15874411 0.1588061
 0.15806371 0.1558172  0.1528978  0.15144044 0.15242045 0.15367249
 0.15374757 0.15191418 0.15048313 0.15076447 0.15319459 0.15565796
 0.15597066 0.1543287  0.15339395 0.15419693 0.15554847 0.15572672
 0.15478669 0.154425   0.15506843 0.15511121 0.15305829 0.1499053
 0.14862561 0.15073869 0.15344793 0.15382971 0.1507388  0.14737886
 0.14820103 0.15211567 0.15480615 0.15356015 0.1504789  0.14863896
 0.14889915 0.14863583 0.14696556 0.14542897 0.14562507 0.14558838
 0.14084198 0.13226978 0.1288748  0.13269277 0.12719126 0.05895621]
