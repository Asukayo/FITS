Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19493376.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4457396
	speed: 0.1494s/iter; left time: 896.3336s
Epoch: 1 cost time: 18.080676078796387
Epoch: 1, Steps: 122 | Train Loss: 0.4910232 Vali Loss: 0.8078561 Test Loss: 0.4093046
Validation loss decreased (inf --> 0.807856).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3145821
	speed: 0.3730s/iter; left time: 2192.5764s
Epoch: 2 cost time: 16.222581148147583
Epoch: 2, Steps: 122 | Train Loss: 0.3581587 Vali Loss: 0.7434952 Test Loss: 0.3820353
Validation loss decreased (0.807856 --> 0.743495).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3054696
	speed: 0.3729s/iter; left time: 2146.6459s
Epoch: 3 cost time: 17.48215627670288
Epoch: 3, Steps: 122 | Train Loss: 0.3454939 Vali Loss: 0.7297848 Test Loss: 0.3806458
Validation loss decreased (0.743495 --> 0.729785).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3706743
	speed: 0.3742s/iter; left time: 2108.3884s
Epoch: 4 cost time: 17.37870979309082
Epoch: 4, Steps: 122 | Train Loss: 0.3411009 Vali Loss: 0.7230207 Test Loss: 0.3805372
Validation loss decreased (0.729785 --> 0.723021).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2922643
	speed: 0.3737s/iter; left time: 2060.2629s
Epoch: 5 cost time: 17.63133144378662
Epoch: 5, Steps: 122 | Train Loss: 0.3387728 Vali Loss: 0.7141096 Test Loss: 0.3794072
Validation loss decreased (0.723021 --> 0.714110).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3486848
	speed: 0.3784s/iter; left time: 2040.0753s
Epoch: 6 cost time: 17.664372205734253
Epoch: 6, Steps: 122 | Train Loss: 0.3370802 Vali Loss: 0.7111790 Test Loss: 0.3797460
Validation loss decreased (0.714110 --> 0.711179).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3097140
	speed: 0.3198s/iter; left time: 1685.1372s
Epoch: 7 cost time: 13.526208400726318
Epoch: 7, Steps: 122 | Train Loss: 0.3361656 Vali Loss: 0.7082167 Test Loss: 0.3803169
Validation loss decreased (0.711179 --> 0.708217).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3218774
	speed: 0.3684s/iter; left time: 1896.1668s
Epoch: 8 cost time: 17.169944047927856
Epoch: 8, Steps: 122 | Train Loss: 0.3350222 Vali Loss: 0.7051985 Test Loss: 0.3802715
Validation loss decreased (0.708217 --> 0.705198).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3261530
	speed: 0.3215s/iter; left time: 1615.4140s
Epoch: 9 cost time: 15.048599004745483
Epoch: 9, Steps: 122 | Train Loss: 0.3345173 Vali Loss: 0.7062967 Test Loss: 0.3799441
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3241487
	speed: 0.3918s/iter; left time: 1921.1458s
Epoch: 10 cost time: 18.409008026123047
Epoch: 10, Steps: 122 | Train Loss: 0.3340079 Vali Loss: 0.7016164 Test Loss: 0.3800371
Validation loss decreased (0.705198 --> 0.701616).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3408270
	speed: 0.3994s/iter; left time: 1909.6343s
Epoch: 11 cost time: 17.81038498878479
Epoch: 11, Steps: 122 | Train Loss: 0.3336714 Vali Loss: 0.7046813 Test Loss: 0.3799099
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3557738
	speed: 0.3907s/iter; left time: 1820.2603s
Epoch: 12 cost time: 18.519052267074585
Epoch: 12, Steps: 122 | Train Loss: 0.3332056 Vali Loss: 0.7017238 Test Loss: 0.3809643
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3311449
	speed: 0.3959s/iter; left time: 1796.2990s
Epoch: 13 cost time: 18.23402500152588
Epoch: 13, Steps: 122 | Train Loss: 0.3328904 Vali Loss: 0.7033677 Test Loss: 0.3806751
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_96_FITS_ETTh1_ftM_sl720_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.3795756697654724, mae:0.4024958312511444, rse:0.5852036476135254, corr:[0.2670035  0.27894074 0.27883804 0.278917   0.27695423 0.2738116
 0.27256763 0.27274194 0.27228007 0.27169362 0.2715003  0.27138454
 0.27121878 0.27120438 0.27156168 0.27177638 0.27147123 0.27098852
 0.27055058 0.27026463 0.2702069  0.27017605 0.26963225 0.26927447
 0.26937312 0.26954034 0.2695001  0.26917455 0.2687492  0.2684973
 0.26862562 0.26867208 0.268382   0.2678138  0.26722753 0.26708314
 0.2671825  0.26730022 0.26750228 0.26776826 0.26801878 0.2680372
 0.26793852 0.26808393 0.26825762 0.26826626 0.26838443 0.26851195
 0.26814008 0.26719448 0.26595834 0.2649958  0.26390558 0.26213944
 0.26081023 0.26060593 0.2607807  0.26053938 0.25964198 0.25929165
 0.2594711  0.25962785 0.2596028  0.2597097  0.25964442 0.25937116
 0.25976446 0.26034    0.26029354 0.25993043 0.2602295  0.26046783
 0.25945008 0.25755963 0.25608364 0.25549635 0.25451002 0.25347695
 0.25330296 0.2529176  0.25167733 0.25055403 0.24984935 0.24888979
 0.24905603 0.2511568  0.2517691  0.24999295 0.24893346 0.2489208
 0.24692532 0.24525951 0.24634509 0.24387027 0.24259847 0.25142413]
