Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30898560.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.778375625610352
Epoch: 1, Steps: 60 | Train Loss: 0.6559154 Vali Loss: 1.5953671 Test Loss: 0.8149478
Validation loss decreased (inf --> 1.595367).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.97921371459961
Epoch: 2, Steps: 60 | Train Loss: 0.5155003 Vali Loss: 1.4386907 Test Loss: 0.7405695
Validation loss decreased (1.595367 --> 1.438691).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.712043046951294
Epoch: 3, Steps: 60 | Train Loss: 0.4476457 Vali Loss: 1.3736886 Test Loss: 0.7139548
Validation loss decreased (1.438691 --> 1.373689).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.908815145492554
Epoch: 4, Steps: 60 | Train Loss: 0.4078032 Vali Loss: 1.3383563 Test Loss: 0.6991531
Validation loss decreased (1.373689 --> 1.338356).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.644408702850342
Epoch: 5, Steps: 60 | Train Loss: 0.3789537 Vali Loss: 1.3148831 Test Loss: 0.6887500
Validation loss decreased (1.338356 --> 1.314883).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.966945171356201
Epoch: 6, Steps: 60 | Train Loss: 0.3557190 Vali Loss: 1.2976817 Test Loss: 0.6808510
Validation loss decreased (1.314883 --> 1.297682).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.78489875793457
Epoch: 7, Steps: 60 | Train Loss: 0.3362233 Vali Loss: 1.2786405 Test Loss: 0.6701085
Validation loss decreased (1.297682 --> 1.278641).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.049225568771362
Epoch: 8, Steps: 60 | Train Loss: 0.3193321 Vali Loss: 1.2635157 Test Loss: 0.6616023
Validation loss decreased (1.278641 --> 1.263516).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.079489707946777
Epoch: 9, Steps: 60 | Train Loss: 0.3048056 Vali Loss: 1.2485493 Test Loss: 0.6524246
Validation loss decreased (1.263516 --> 1.248549).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.909228801727295
Epoch: 10, Steps: 60 | Train Loss: 0.2916660 Vali Loss: 1.2338181 Test Loss: 0.6424322
Validation loss decreased (1.248549 --> 1.233818).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.602944374084473
Epoch: 11, Steps: 60 | Train Loss: 0.2801639 Vali Loss: 1.2234358 Test Loss: 0.6360887
Validation loss decreased (1.233818 --> 1.223436).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.546667575836182
Epoch: 12, Steps: 60 | Train Loss: 0.2699545 Vali Loss: 1.2126932 Test Loss: 0.6285150
Validation loss decreased (1.223436 --> 1.212693).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.777551651000977
Epoch: 13, Steps: 60 | Train Loss: 0.2606147 Vali Loss: 1.1991935 Test Loss: 0.6191250
Validation loss decreased (1.212693 --> 1.199193).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.84192705154419
Epoch: 14, Steps: 60 | Train Loss: 0.2521571 Vali Loss: 1.1903975 Test Loss: 0.6129946
Validation loss decreased (1.199193 --> 1.190398).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.855678081512451
Epoch: 15, Steps: 60 | Train Loss: 0.2445112 Vali Loss: 1.1809196 Test Loss: 0.6063893
Validation loss decreased (1.190398 --> 1.180920).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.503663539886475
Epoch: 16, Steps: 60 | Train Loss: 0.2377540 Vali Loss: 1.1712927 Test Loss: 0.5991071
Validation loss decreased (1.180920 --> 1.171293).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.768378496170044
Epoch: 17, Steps: 60 | Train Loss: 0.2314561 Vali Loss: 1.1644101 Test Loss: 0.5942045
Validation loss decreased (1.171293 --> 1.164410).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.130387306213379
Epoch: 18, Steps: 60 | Train Loss: 0.2255973 Vali Loss: 1.1548536 Test Loss: 0.5872914
Validation loss decreased (1.164410 --> 1.154854).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.359875679016113
Epoch: 19, Steps: 60 | Train Loss: 0.2203490 Vali Loss: 1.1474073 Test Loss: 0.5817282
Validation loss decreased (1.154854 --> 1.147407).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 8.672849416732788
Epoch: 20, Steps: 60 | Train Loss: 0.2155625 Vali Loss: 1.1408691 Test Loss: 0.5765116
Validation loss decreased (1.147407 --> 1.140869).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.54240345954895
Epoch: 21, Steps: 60 | Train Loss: 0.2111127 Vali Loss: 1.1348219 Test Loss: 0.5724134
Validation loss decreased (1.140869 --> 1.134822).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.913696050643921
Epoch: 22, Steps: 60 | Train Loss: 0.2070039 Vali Loss: 1.1283145 Test Loss: 0.5672126
Validation loss decreased (1.134822 --> 1.128314).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.49097728729248
Epoch: 23, Steps: 60 | Train Loss: 0.2031943 Vali Loss: 1.1236837 Test Loss: 0.5636142
Validation loss decreased (1.128314 --> 1.123684).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.843232154846191
Epoch: 24, Steps: 60 | Train Loss: 0.1996294 Vali Loss: 1.1178163 Test Loss: 0.5596029
Validation loss decreased (1.123684 --> 1.117816).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.464841842651367
Epoch: 25, Steps: 60 | Train Loss: 0.1962994 Vali Loss: 1.1125432 Test Loss: 0.5555943
Validation loss decreased (1.117816 --> 1.112543).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 8.970037698745728
Epoch: 26, Steps: 60 | Train Loss: 0.1933964 Vali Loss: 1.1083826 Test Loss: 0.5517631
Validation loss decreased (1.112543 --> 1.108383).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.978582382202148
Epoch: 27, Steps: 60 | Train Loss: 0.1906224 Vali Loss: 1.1036825 Test Loss: 0.5483722
Validation loss decreased (1.108383 --> 1.103683).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.553274869918823
Epoch: 28, Steps: 60 | Train Loss: 0.1879807 Vali Loss: 1.0996193 Test Loss: 0.5452007
Validation loss decreased (1.103683 --> 1.099619).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.604015350341797
Epoch: 29, Steps: 60 | Train Loss: 0.1855350 Vali Loss: 1.0960250 Test Loss: 0.5421601
Validation loss decreased (1.099619 --> 1.096025).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 7.5545759201049805
Epoch: 30, Steps: 60 | Train Loss: 0.1833642 Vali Loss: 1.0925918 Test Loss: 0.5396512
Validation loss decreased (1.096025 --> 1.092592).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 8.814509391784668
Epoch: 31, Steps: 60 | Train Loss: 0.1812231 Vali Loss: 1.0884283 Test Loss: 0.5365487
Validation loss decreased (1.092592 --> 1.088428).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.20141887664795
Epoch: 32, Steps: 60 | Train Loss: 0.1792103 Vali Loss: 1.0848951 Test Loss: 0.5342729
Validation loss decreased (1.088428 --> 1.084895).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.531902551651001
Epoch: 33, Steps: 60 | Train Loss: 0.1773315 Vali Loss: 1.0823958 Test Loss: 0.5320550
Validation loss decreased (1.084895 --> 1.082396).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 8.756476879119873
Epoch: 34, Steps: 60 | Train Loss: 0.1756490 Vali Loss: 1.0796511 Test Loss: 0.5299065
Validation loss decreased (1.082396 --> 1.079651).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 8.728038311004639
Epoch: 35, Steps: 60 | Train Loss: 0.1740872 Vali Loss: 1.0767177 Test Loss: 0.5274856
Validation loss decreased (1.079651 --> 1.076718).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.648817539215088
Epoch: 36, Steps: 60 | Train Loss: 0.1725408 Vali Loss: 1.0747687 Test Loss: 0.5259129
Validation loss decreased (1.076718 --> 1.074769).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 9.019338846206665
Epoch: 37, Steps: 60 | Train Loss: 0.1709683 Vali Loss: 1.0719368 Test Loss: 0.5237703
Validation loss decreased (1.074769 --> 1.071937).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 8.95676064491272
Epoch: 38, Steps: 60 | Train Loss: 0.1697641 Vali Loss: 1.0698034 Test Loss: 0.5219687
Validation loss decreased (1.071937 --> 1.069803).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 8.429708242416382
Epoch: 39, Steps: 60 | Train Loss: 0.1685088 Vali Loss: 1.0668358 Test Loss: 0.5203086
Validation loss decreased (1.069803 --> 1.066836).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 8.97212553024292
Epoch: 40, Steps: 60 | Train Loss: 0.1672403 Vali Loss: 1.0654699 Test Loss: 0.5188135
Validation loss decreased (1.066836 --> 1.065470).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 9.201218605041504
Epoch: 41, Steps: 60 | Train Loss: 0.1661034 Vali Loss: 1.0632521 Test Loss: 0.5172861
Validation loss decreased (1.065470 --> 1.063252).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 9.324973583221436
Epoch: 42, Steps: 60 | Train Loss: 0.1651339 Vali Loss: 1.0617797 Test Loss: 0.5158245
Validation loss decreased (1.063252 --> 1.061780).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 8.776001691818237
Epoch: 43, Steps: 60 | Train Loss: 0.1641178 Vali Loss: 1.0601771 Test Loss: 0.5144700
Validation loss decreased (1.061780 --> 1.060177).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 8.551135540008545
Epoch: 44, Steps: 60 | Train Loss: 0.1631582 Vali Loss: 1.0583063 Test Loss: 0.5134108
Validation loss decreased (1.060177 --> 1.058306).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 8.370650053024292
Epoch: 45, Steps: 60 | Train Loss: 0.1622416 Vali Loss: 1.0571445 Test Loss: 0.5120347
Validation loss decreased (1.058306 --> 1.057145).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.144117593765259
Epoch: 46, Steps: 60 | Train Loss: 0.1614124 Vali Loss: 1.0554575 Test Loss: 0.5108063
Validation loss decreased (1.057145 --> 1.055457).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 9.297695636749268
Epoch: 47, Steps: 60 | Train Loss: 0.1605974 Vali Loss: 1.0540270 Test Loss: 0.5097509
Validation loss decreased (1.055457 --> 1.054027).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 8.711907863616943
Epoch: 48, Steps: 60 | Train Loss: 0.1599172 Vali Loss: 1.0526066 Test Loss: 0.5090134
Validation loss decreased (1.054027 --> 1.052607).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 8.492565870285034
Epoch: 49, Steps: 60 | Train Loss: 0.1591772 Vali Loss: 1.0511526 Test Loss: 0.5077212
Validation loss decreased (1.052607 --> 1.051153).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 8.670205116271973
Epoch: 50, Steps: 60 | Train Loss: 0.1585417 Vali Loss: 1.0503017 Test Loss: 0.5069649
Validation loss decreased (1.051153 --> 1.050302).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30898560.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.780516147613525
Epoch: 1, Steps: 60 | Train Loss: 0.4166880 Vali Loss: 0.9633836 Test Loss: 0.4352342
Validation loss decreased (inf --> 0.963384).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.413074016571045
Epoch: 2, Steps: 60 | Train Loss: 0.3908495 Vali Loss: 0.9479467 Test Loss: 0.4198627
Validation loss decreased (0.963384 --> 0.947947).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 7.578599452972412
Epoch: 3, Steps: 60 | Train Loss: 0.3841329 Vali Loss: 0.9495101 Test Loss: 0.4180573
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.83240294456482
Epoch: 4, Steps: 60 | Train Loss: 0.3822206 Vali Loss: 0.9519637 Test Loss: 0.4186900
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.455112934112549
Epoch: 5, Steps: 60 | Train Loss: 0.3816774 Vali Loss: 0.9539887 Test Loss: 0.4189540
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.41748666763305664, mae:0.4263441562652588, rse:0.6135905385017395, corr:[0.2635161  0.27227932 0.2705704  0.26777378 0.2672515  0.26631364
 0.26500243 0.26408413 0.26418656 0.26479265 0.26479688 0.26388502
 0.26319113 0.26309398 0.2633558  0.26337555 0.26295638 0.26231965
 0.2614443  0.26053742 0.25987652 0.25949857 0.25919133 0.25911134
 0.2593094  0.25965676 0.2598306  0.2598857  0.2600334  0.26023415
 0.26018876 0.25966603 0.2590278  0.25871134 0.25867182 0.25882092
 0.25887117 0.25877553 0.25858387 0.2584541  0.2586816  0.25902203
 0.2591673  0.25908473 0.25891444 0.25884098 0.25922468 0.2597937
 0.25977534 0.25930864 0.2586005  0.25777113 0.25677517 0.25534686
 0.25401482 0.253336   0.25319567 0.25329447 0.25289622 0.25231388
 0.25196564 0.25196764 0.2518998  0.2517177  0.25165844 0.25198758
 0.25248194 0.25234577 0.25188404 0.2517235  0.25198743 0.25215065
 0.25199372 0.25155511 0.25099102 0.25070196 0.25044453 0.25020579
 0.25023356 0.25024268 0.24963945 0.24862203 0.24782155 0.24780975
 0.24818952 0.24817684 0.24777226 0.2472764  0.2467009  0.24599671
 0.24529232 0.24495739 0.24520889 0.24553196 0.24574105 0.24615733
 0.24695577 0.24766241 0.2476526  0.24748676 0.24785635 0.24848117
 0.24875858 0.24849282 0.24815738 0.24825399 0.24811333 0.2473411
 0.24666959 0.24678206 0.24748386 0.24783711 0.2472569  0.24628063
 0.24580672 0.24591047 0.24620312 0.24637905 0.24624164 0.24568895
 0.24479274 0.2437188  0.24298699 0.242606   0.24201962 0.24110015
 0.24082202 0.24135177 0.2416572  0.24144754 0.24095236 0.24073736
 0.24075788 0.24073367 0.24049439 0.24015468 0.24000043 0.2397648
 0.2393519  0.23900609 0.2390735  0.23934402 0.23944905 0.23918885
 0.23891498 0.23841953 0.23729014 0.23571862 0.2348067  0.23454343
 0.2342773  0.23358259 0.23279479 0.23251066 0.23237835 0.2322179
 0.23191467 0.23162018 0.23163305 0.23213486 0.23295867 0.23356643
 0.23316312 0.23204662 0.23145536 0.23185897 0.23253816 0.23212953
 0.23100202 0.23034792 0.23029245 0.23038213 0.2301511  0.22973451
 0.22925372 0.22870132 0.22773767 0.2272234  0.22701138 0.2264821
 0.22547635 0.22480771 0.22485869 0.22498666 0.22497338 0.22536469
 0.22566435 0.22379038 0.219927   0.2184138  0.22127353 0.21728387]
