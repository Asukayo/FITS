Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6166487040.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4375554
	speed: 0.3206s/iter; left time: 2132.6122s
Epoch: 1 cost time: 43.183889865875244
Epoch: 1, Steps: 135 | Train Loss: 0.6036354 Vali Loss: 0.3026169 Test Loss: 0.3629843
Validation loss decreased (inf --> 0.302617).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2461328
	speed: 0.7048s/iter; left time: 4592.2951s
Epoch: 2 cost time: 42.65004110336304
Epoch: 2, Steps: 135 | Train Loss: 0.2828202 Vali Loss: 0.1799903 Test Loss: 0.2189052
Validation loss decreased (0.302617 --> 0.179990).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1946849
	speed: 0.7375s/iter; left time: 4705.9189s
Epoch: 3 cost time: 46.470569133758545
Epoch: 3, Steps: 135 | Train Loss: 0.2005616 Vali Loss: 0.1498224 Test Loss: 0.1805231
Validation loss decreased (0.179990 --> 0.149822).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1685213
	speed: 0.7171s/iter; left time: 4478.9995s
Epoch: 4 cost time: 42.42457127571106
Epoch: 4, Steps: 135 | Train Loss: 0.1804360 Vali Loss: 0.1436748 Test Loss: 0.1716485
Validation loss decreased (0.149822 --> 0.143675).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1682100
	speed: 0.7316s/iter; left time: 4471.0527s
Epoch: 5 cost time: 43.388999700546265
Epoch: 5, Steps: 135 | Train Loss: 0.1760222 Vali Loss: 0.1423057 Test Loss: 0.1694778
Validation loss decreased (0.143675 --> 0.142306).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1658244
	speed: 0.7120s/iter; left time: 4255.1509s
Epoch: 6 cost time: 43.077816009521484
Epoch: 6, Steps: 135 | Train Loss: 0.1747424 Vali Loss: 0.1422069 Test Loss: 0.1687456
Validation loss decreased (0.142306 --> 0.142207).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1840975
	speed: 0.7657s/iter; left time: 4472.2353s
Epoch: 7 cost time: 45.81109046936035
Epoch: 7, Steps: 135 | Train Loss: 0.1741294 Vali Loss: 0.1420715 Test Loss: 0.1683385
Validation loss decreased (0.142207 --> 0.142071).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1722383
	speed: 0.7183s/iter; left time: 4098.6866s
Epoch: 8 cost time: 42.96961259841919
Epoch: 8, Steps: 135 | Train Loss: 0.1737572 Vali Loss: 0.1422198 Test Loss: 0.1681416
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1764408
	speed: 0.7195s/iter; left time: 4008.2070s
Epoch: 9 cost time: 43.9214608669281
Epoch: 9, Steps: 135 | Train Loss: 0.1735151 Vali Loss: 0.1416550 Test Loss: 0.1679889
Validation loss decreased (0.142071 --> 0.141655).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1922245
	speed: 0.7157s/iter; left time: 3890.6186s
Epoch: 10 cost time: 44.274975538253784
Epoch: 10, Steps: 135 | Train Loss: 0.1733677 Vali Loss: 0.1416959 Test Loss: 0.1679059
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1644817
	speed: 0.7096s/iter; left time: 3761.4449s
Epoch: 11 cost time: 42.8987352848053
Epoch: 11, Steps: 135 | Train Loss: 0.1731429 Vali Loss: 0.1415561 Test Loss: 0.1677886
Validation loss decreased (0.141655 --> 0.141556).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1826963
	speed: 0.7335s/iter; left time: 3789.1629s
Epoch: 12 cost time: 44.35836958885193
Epoch: 12, Steps: 135 | Train Loss: 0.1730568 Vali Loss: 0.1417613 Test Loss: 0.1677494
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1843881
	speed: 0.7558s/iter; left time: 3802.5941s
Epoch: 13 cost time: 46.367128133773804
Epoch: 13, Steps: 135 | Train Loss: 0.1729579 Vali Loss: 0.1415920 Test Loss: 0.1677001
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1804338
	speed: 0.8181s/iter; left time: 4005.3227s
Epoch: 14 cost time: 49.150617361068726
Epoch: 14, Steps: 135 | Train Loss: 0.1728174 Vali Loss: 0.1413099 Test Loss: 0.1676376
Validation loss decreased (0.141556 --> 0.141310).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1792087
	speed: 0.7360s/iter; left time: 3504.0722s
Epoch: 15 cost time: 42.95496845245361
Epoch: 15, Steps: 135 | Train Loss: 0.1728493 Vali Loss: 0.1413179 Test Loss: 0.1676227
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1850917
	speed: 0.7285s/iter; left time: 3370.2038s
Epoch: 16 cost time: 44.56857895851135
Epoch: 16, Steps: 135 | Train Loss: 0.1726217 Vali Loss: 0.1411728 Test Loss: 0.1675599
Validation loss decreased (0.141310 --> 0.141173).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1696152
	speed: 0.7270s/iter; left time: 3264.7553s
Epoch: 17 cost time: 45.07465958595276
Epoch: 17, Steps: 135 | Train Loss: 0.1726225 Vali Loss: 0.1415831 Test Loss: 0.1675584
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1635439
	speed: 0.7341s/iter; left time: 3197.6165s
Epoch: 18 cost time: 44.35766410827637
Epoch: 18, Steps: 135 | Train Loss: 0.1726266 Vali Loss: 0.1415569 Test Loss: 0.1675245
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1710525
	speed: 0.7255s/iter; left time: 3062.5426s
Epoch: 19 cost time: 43.66202425956726
Epoch: 19, Steps: 135 | Train Loss: 0.1725815 Vali Loss: 0.1414152 Test Loss: 0.1675406
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1811344
	speed: 0.7222s/iter; left time: 2951.0148s
Epoch: 20 cost time: 43.75416398048401
Epoch: 20, Steps: 135 | Train Loss: 0.1726110 Vali Loss: 0.1414808 Test Loss: 0.1675282
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1745820
	speed: 0.7871s/iter; left time: 3109.7110s
Epoch: 21 cost time: 48.400816202163696
Epoch: 21, Steps: 135 | Train Loss: 0.1724935 Vali Loss: 0.1412857 Test Loss: 0.1675131
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1666334
	speed: 0.8544s/iter; left time: 3260.4263s
Epoch: 22 cost time: 49.75588917732239
Epoch: 22, Steps: 135 | Train Loss: 0.1724196 Vali Loss: 0.1413139 Test Loss: 0.1674838
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1805141
	speed: 0.7499s/iter; left time: 2760.2043s
Epoch: 23 cost time: 44.08675789833069
Epoch: 23, Steps: 135 | Train Loss: 0.1724525 Vali Loss: 0.1412283 Test Loss: 0.1674755
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1699815
	speed: 0.7320s/iter; left time: 2595.8250s
Epoch: 24 cost time: 44.44722652435303
Epoch: 24, Steps: 135 | Train Loss: 0.1724358 Vali Loss: 0.1415296 Test Loss: 0.1674506
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1762960
	speed: 0.7142s/iter; left time: 2436.0477s
Epoch: 25 cost time: 44.28297019004822
Epoch: 25, Steps: 135 | Train Loss: 0.1724301 Vali Loss: 0.1411919 Test Loss: 0.1674774
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1679455
	speed: 0.7219s/iter; left time: 2365.0833s
Epoch: 26 cost time: 42.914358615875244
Epoch: 26, Steps: 135 | Train Loss: 0.1724051 Vali Loss: 0.1414663 Test Loss: 0.1674235
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1774634
	speed: 0.7056s/iter; left time: 2216.2389s
Epoch: 27 cost time: 43.94035792350769
Epoch: 27, Steps: 135 | Train Loss: 0.1724249 Vali Loss: 0.1413640 Test Loss: 0.1674384
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1689991
	speed: 0.7558s/iter; left time: 2271.9609s
Epoch: 28 cost time: 46.206772327423096
Epoch: 28, Steps: 135 | Train Loss: 0.1724132 Vali Loss: 0.1412553 Test Loss: 0.1674077
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1696941
	speed: 0.7639s/iter; left time: 2193.1324s
Epoch: 29 cost time: 44.98862028121948
Epoch: 29, Steps: 135 | Train Loss: 0.1723935 Vali Loss: 0.1411464 Test Loss: 0.1674351
Validation loss decreased (0.141173 --> 0.141146).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1723363
	speed: 0.7491s/iter; left time: 2049.5518s
Epoch: 30 cost time: 46.7166051864624
Epoch: 30, Steps: 135 | Train Loss: 0.1723946 Vali Loss: 0.1413917 Test Loss: 0.1673869
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1671976
	speed: 0.7928s/iter; left time: 2062.1152s
Epoch: 31 cost time: 47.4055392742157
Epoch: 31, Steps: 135 | Train Loss: 0.1723436 Vali Loss: 0.1413256 Test Loss: 0.1673892
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1688802
	speed: 0.7422s/iter; left time: 1830.1625s
Epoch: 32 cost time: 43.78215718269348
Epoch: 32, Steps: 135 | Train Loss: 0.1723325 Vali Loss: 0.1412113 Test Loss: 0.1673920
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1714934
	speed: 0.7292s/iter; left time: 1699.8004s
Epoch: 33 cost time: 44.566884994506836
Epoch: 33, Steps: 135 | Train Loss: 0.1722898 Vali Loss: 0.1412408 Test Loss: 0.1673728
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1694195
	speed: 0.7471s/iter; left time: 1640.7284s
Epoch: 34 cost time: 45.72243332862854
Epoch: 34, Steps: 135 | Train Loss: 0.1722574 Vali Loss: 0.1411574 Test Loss: 0.1673793
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1699057
	speed: 0.7797s/iter; left time: 1606.9904s
Epoch: 35 cost time: 48.9684898853302
Epoch: 35, Steps: 135 | Train Loss: 0.1722578 Vali Loss: 0.1412533 Test Loss: 0.1673881
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1659648
	speed: 0.7892s/iter; left time: 1519.9976s
Epoch: 36 cost time: 45.25703406333923
Epoch: 36, Steps: 135 | Train Loss: 0.1723033 Vali Loss: 0.1413016 Test Loss: 0.1673633
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1795070
	speed: 0.7368s/iter; left time: 1319.5542s
Epoch: 37 cost time: 44.43466114997864
Epoch: 37, Steps: 135 | Train Loss: 0.1723106 Vali Loss: 0.1414487 Test Loss: 0.1673539
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1738302
	speed: 0.6806s/iter; left time: 1127.0809s
Epoch: 38 cost time: 39.67789554595947
Epoch: 38, Steps: 135 | Train Loss: 0.1722374 Vali Loss: 0.1411152 Test Loss: 0.1673638
Validation loss decreased (0.141146 --> 0.141115).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1700078
	speed: 0.7331s/iter; left time: 1115.0045s
Epoch: 39 cost time: 45.3035306930542
Epoch: 39, Steps: 135 | Train Loss: 0.1723190 Vali Loss: 0.1411832 Test Loss: 0.1673451
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1697686
	speed: 0.7322s/iter; left time: 1014.8746s
Epoch: 40 cost time: 43.80015420913696
Epoch: 40, Steps: 135 | Train Loss: 0.1722953 Vali Loss: 0.1414641 Test Loss: 0.1673582
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1762549
	speed: 0.7066s/iter; left time: 884.0148s
Epoch: 41 cost time: 41.684247970581055
Epoch: 41, Steps: 135 | Train Loss: 0.1722651 Vali Loss: 0.1412340 Test Loss: 0.1673485
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1881366
	speed: 0.6900s/iter; left time: 770.0710s
Epoch: 42 cost time: 42.55681228637695
Epoch: 42, Steps: 135 | Train Loss: 0.1721735 Vali Loss: 0.1413939 Test Loss: 0.1673555
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1751681
	speed: 0.7376s/iter; left time: 723.5802s
Epoch: 43 cost time: 44.10571026802063
Epoch: 43, Steps: 135 | Train Loss: 0.1722330 Vali Loss: 0.1413798 Test Loss: 0.1673449
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.1721864
	speed: 0.7034s/iter; left time: 595.0871s
Epoch: 44 cost time: 42.2406907081604
Epoch: 44, Steps: 135 | Train Loss: 0.1722434 Vali Loss: 0.1414755 Test Loss: 0.1673426
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.1854585
	speed: 0.7283s/iter; left time: 517.8136s
Epoch: 45 cost time: 42.32975745201111
Epoch: 45, Steps: 135 | Train Loss: 0.1722378 Vali Loss: 0.1411945 Test Loss: 0.1673383
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.1741146
	speed: 0.6556s/iter; left time: 377.6007s
Epoch: 46 cost time: 39.76585268974304
Epoch: 46, Steps: 135 | Train Loss: 0.1722334 Vali Loss: 0.1411864 Test Loss: 0.1673409
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.1747258
	speed: 0.6855s/iter; left time: 302.3203s
Epoch: 47 cost time: 42.16783952713013
Epoch: 47, Steps: 135 | Train Loss: 0.1722625 Vali Loss: 0.1411343 Test Loss: 0.1673347
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.1715858
	speed: 0.7022s/iter; left time: 214.8850s
Epoch: 48 cost time: 41.835628032684326
Epoch: 48, Steps: 135 | Train Loss: 0.1721784 Vali Loss: 0.1410830 Test Loss: 0.1673404
Validation loss decreased (0.141115 --> 0.141083).  Saving model ...
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.1677025
	speed: 0.6998s/iter; left time: 119.6699s
Epoch: 49 cost time: 42.047236919403076
Epoch: 49, Steps: 135 | Train Loss: 0.1722444 Vali Loss: 0.1411338 Test Loss: 0.1673367
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.1831430
	speed: 0.7233s/iter; left time: 26.0386s
Epoch: 50 cost time: 45.869253158569336
Epoch: 50, Steps: 135 | Train Loss: 0.1721954 Vali Loss: 0.1412770 Test Loss: 0.1673343
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
>>>>>>>testing : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.1651274412870407, mae:0.26045331358909607, rse:0.40443703532218933, corr:[0.4596102  0.46164694 0.46191278 0.46331668 0.46315113 0.46389532
 0.46383408 0.46382773 0.46366677 0.46348226 0.46333125 0.46321025
 0.46321782 0.46315002 0.46315497 0.4632089  0.46321386 0.46325508
 0.46325842 0.46304858 0.463083   0.46303222 0.4631656  0.4633281
 0.46362644 0.4638103  0.463976   0.46406123 0.46381304 0.46382484
 0.46357647 0.46343523 0.46330738 0.46315932 0.46312204 0.46307707
 0.4631405  0.46313807 0.46315005 0.46315023 0.4631438  0.46316698
 0.4630969  0.46293953 0.4629035  0.4627878  0.46284348 0.4629454
 0.4629877  0.46320006 0.4632468  0.4632259  0.46308285 0.4629379
 0.46290103 0.46285424 0.4627987  0.46267667 0.46263263 0.46262026
 0.4626681  0.46269745 0.46270373 0.46274284 0.46274385 0.4627216
 0.46265665 0.46241474 0.46230337 0.46231315 0.46221873 0.46232298
 0.46231502 0.4624182  0.46249133 0.46235073 0.46225697 0.46212927
 0.46216497 0.46210524 0.46200088 0.46198305 0.4619431  0.4618988
 0.4618882  0.46189842 0.46188512 0.4619131  0.46193022 0.4619491
 0.46194875 0.46191546 0.46189362 0.46189216 0.46186012 0.46200126
 0.4620832  0.46206668 0.4621657  0.46201465 0.46194485 0.46191126
 0.46178642 0.4618431  0.46181136 0.4617637  0.4616899  0.46168035
 0.4617155  0.46168825 0.46176538 0.46176347 0.46173152 0.4617799
 0.46171302 0.46162608 0.4615446  0.46149716 0.46153808 0.46160683
 0.46184975 0.46188468 0.46186855 0.46186438 0.46179125 0.4618266
 0.46184096 0.46178687 0.46176332 0.46167526 0.46163306 0.4616129
 0.46159875 0.46162271 0.4615893  0.4616518  0.461657   0.46162912
 0.4616901  0.46151426 0.46131137 0.4612686  0.46122873 0.46115145
 0.46120194 0.46133444 0.4613322  0.461332   0.46137702 0.4613465
 0.46128935 0.46127927 0.46126315 0.4611772  0.461096   0.46110937
 0.46109638 0.46112686 0.46112695 0.46108273 0.46111965 0.461107
 0.46113235 0.46096927 0.4608343  0.4608333  0.46074316 0.46076643
 0.46070728 0.46072164 0.46079722 0.46063182 0.46052295 0.46047965
 0.46043536 0.46034557 0.46021762 0.46011433 0.45996714 0.45984367
 0.45978495 0.4596862  0.45958906 0.4594577  0.45933872 0.45924133
 0.45902362 0.45896482 0.45883876 0.4587493  0.45878822 0.45876643
 0.45886952 0.45898384 0.45912266 0.4591075  0.4590207  0.45897183
 0.4588765  0.45882615 0.4587191  0.45862922 0.45855445 0.45848793
 0.45854536 0.45851457 0.45842433 0.45845026 0.45836547 0.45837417
 0.45836934 0.45814413 0.4580768  0.45806858 0.4581307  0.4581468
 0.45823509 0.45847198 0.4585752  0.45859915 0.45854202 0.45847568
 0.4584341  0.4584162  0.45833275 0.45823658 0.45819843 0.45813113
 0.45808733 0.45808294 0.45804092 0.45796934 0.45801127 0.45799422
 0.45796493 0.45779696 0.45763668 0.45768034 0.45775184 0.4579074
 0.45794284 0.45811096 0.45824137 0.4581856  0.45816538 0.45809665
 0.4580324  0.45795935 0.4579027  0.4577602  0.4576151  0.45763788
 0.45759392 0.45751756 0.45752814 0.45746326 0.4574692  0.45741656
 0.4573619  0.45741546 0.45739427 0.4575378  0.45759204 0.4576937
 0.45782936 0.4578736  0.45801914 0.45795086 0.45789728 0.45787388
 0.45776755 0.45773473 0.45760748 0.4574508  0.45736846 0.45730358
 0.45734155 0.45726523 0.45723328 0.45730993 0.45722544 0.45730826
 0.45730072 0.45723334 0.4572265  0.4572692  0.45744386 0.4575258
 0.45778418 0.45785514 0.45790592 0.45784166 0.4577553  0.45770195
 0.45761484 0.4575313  0.45737275 0.45719782 0.4570461  0.45700964
 0.45698622 0.45702788 0.45692435 0.45697013 0.45699203 0.4570313
 0.45704985 0.4567433  0.4567158  0.45659557 0.4566775  0.45670494
 0.45671016 0.45680088 0.45675626 0.45681864 0.45660087 0.45645025
 0.45632532 0.45618713 0.45614424 0.45611894 0.45599875 0.45615232
 0.4561362  0.45631346 0.4563199  0.4563645  0.45648268 0.45621553
 0.4564182  0.45574972 0.4561447  0.45571327 0.45644462 0.45670635]
