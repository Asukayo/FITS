Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4544778
	speed: 0.0137s/iter; left time: 175.4489s
	iters: 200, epoch: 1 | loss: 0.7022394
	speed: 0.0107s/iter; left time: 136.5311s
Epoch: 1 cost time: 3.078392744064331
Epoch: 1, Steps: 258 | Train Loss: 0.5723936 Vali Loss: 0.2777416 Test Loss: 0.3739824
Validation loss decreased (inf --> 0.277742).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4349810
	speed: 0.0493s/iter; left time: 618.9259s
	iters: 200, epoch: 2 | loss: 0.4985230
	speed: 0.0102s/iter; left time: 126.4109s
Epoch: 2 cost time: 3.0557308197021484
Epoch: 2, Steps: 258 | Train Loss: 0.5172120 Vali Loss: 0.2686980 Test Loss: 0.3641858
Validation loss decreased (0.277742 --> 0.268698).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5926580
	speed: 0.0549s/iter; left time: 674.2722s
	iters: 200, epoch: 3 | loss: 0.3564458
	speed: 0.0099s/iter; left time: 120.7696s
Epoch: 3 cost time: 3.1820950508117676
Epoch: 3, Steps: 258 | Train Loss: 0.5083822 Vali Loss: 0.2664196 Test Loss: 0.3601144
Validation loss decreased (0.268698 --> 0.266420).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4143704
	speed: 0.0582s/iter; left time: 699.9052s
	iters: 200, epoch: 4 | loss: 0.6128854
	speed: 0.0097s/iter; left time: 116.1783s
Epoch: 4 cost time: 2.9117510318756104
Epoch: 4, Steps: 258 | Train Loss: 0.5042916 Vali Loss: 0.2644932 Test Loss: 0.3580889
Validation loss decreased (0.266420 --> 0.264493).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4208990
	speed: 0.0535s/iter; left time: 630.2092s
	iters: 200, epoch: 5 | loss: 0.6471041
	speed: 0.0097s/iter; left time: 113.1630s
Epoch: 5 cost time: 3.0345077514648438
Epoch: 5, Steps: 258 | Train Loss: 0.5019744 Vali Loss: 0.2632241 Test Loss: 0.3566158
Validation loss decreased (0.264493 --> 0.263224).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4641482
	speed: 0.0560s/iter; left time: 644.7394s
	iters: 200, epoch: 6 | loss: 0.4832663
	speed: 0.0099s/iter; left time: 112.4337s
Epoch: 6 cost time: 3.1229076385498047
Epoch: 6, Steps: 258 | Train Loss: 0.4997227 Vali Loss: 0.2625380 Test Loss: 0.3556511
Validation loss decreased (0.263224 --> 0.262538).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5310655
	speed: 0.0579s/iter; left time: 651.0887s
	iters: 200, epoch: 7 | loss: 0.4512278
	speed: 0.0101s/iter; left time: 112.7504s
Epoch: 7 cost time: 3.1543705463409424
Epoch: 7, Steps: 258 | Train Loss: 0.4988393 Vali Loss: 0.2624554 Test Loss: 0.3548595
Validation loss decreased (0.262538 --> 0.262455).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5921586
	speed: 0.0555s/iter; left time: 610.2646s
	iters: 200, epoch: 8 | loss: 0.4941877
	speed: 0.0097s/iter; left time: 105.7961s
Epoch: 8 cost time: 2.925082206726074
Epoch: 8, Steps: 258 | Train Loss: 0.4979616 Vali Loss: 0.2622595 Test Loss: 0.3542668
Validation loss decreased (0.262455 --> 0.262259).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5366186
	speed: 0.0559s/iter; left time: 600.3830s
	iters: 200, epoch: 9 | loss: 0.4637832
	speed: 0.0094s/iter; left time: 100.1127s
Epoch: 9 cost time: 3.0303540229797363
Epoch: 9, Steps: 258 | Train Loss: 0.4969715 Vali Loss: 0.2615247 Test Loss: 0.3542207
Validation loss decreased (0.262259 --> 0.261525).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5925203
	speed: 0.0550s/iter; left time: 575.9264s
	iters: 200, epoch: 10 | loss: 0.5905334
	speed: 0.0093s/iter; left time: 96.2647s
Epoch: 10 cost time: 2.912630319595337
Epoch: 10, Steps: 258 | Train Loss: 0.4967525 Vali Loss: 0.2614141 Test Loss: 0.3537720
Validation loss decreased (0.261525 --> 0.261414).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4411637
	speed: 0.0565s/iter; left time: 577.1023s
	iters: 200, epoch: 11 | loss: 0.6168321
	speed: 0.0095s/iter; left time: 96.4300s
Epoch: 11 cost time: 2.9783782958984375
Epoch: 11, Steps: 258 | Train Loss: 0.4954586 Vali Loss: 0.2612070 Test Loss: 0.3536832
Validation loss decreased (0.261414 --> 0.261207).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6347083
	speed: 0.0550s/iter; left time: 547.5004s
	iters: 200, epoch: 12 | loss: 0.4849268
	speed: 0.0094s/iter; left time: 92.8327s
Epoch: 12 cost time: 3.025322914123535
Epoch: 12, Steps: 258 | Train Loss: 0.4961481 Vali Loss: 0.2607695 Test Loss: 0.3535275
Validation loss decreased (0.261207 --> 0.260769).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4413612
	speed: 0.0533s/iter; left time: 516.8024s
	iters: 200, epoch: 13 | loss: 0.5066220
	speed: 0.0094s/iter; left time: 90.0555s
Epoch: 13 cost time: 2.912245750427246
Epoch: 13, Steps: 258 | Train Loss: 0.4957576 Vali Loss: 0.2611679 Test Loss: 0.3533006
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6367861
	speed: 0.0515s/iter; left time: 486.3923s
	iters: 200, epoch: 14 | loss: 0.4812858
	speed: 0.0089s/iter; left time: 83.4407s
Epoch: 14 cost time: 2.932244300842285
Epoch: 14, Steps: 258 | Train Loss: 0.4957763 Vali Loss: 0.2606065 Test Loss: 0.3529517
Validation loss decreased (0.260769 --> 0.260607).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4012551
	speed: 0.0547s/iter; left time: 503.0826s
	iters: 200, epoch: 15 | loss: 0.6354129
	speed: 0.0097s/iter; left time: 87.9257s
Epoch: 15 cost time: 3.007357358932495
Epoch: 15, Steps: 258 | Train Loss: 0.4955311 Vali Loss: 0.2608686 Test Loss: 0.3529368
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5374254
	speed: 0.0528s/iter; left time: 471.6026s
	iters: 200, epoch: 16 | loss: 0.4532540
	speed: 0.0097s/iter; left time: 85.6103s
Epoch: 16 cost time: 3.014601945877075
Epoch: 16, Steps: 258 | Train Loss: 0.4949697 Vali Loss: 0.2605938 Test Loss: 0.3530515
Validation loss decreased (0.260607 --> 0.260594).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5397234
	speed: 0.0521s/iter; left time: 451.5356s
	iters: 200, epoch: 17 | loss: 0.5074313
	speed: 0.0087s/iter; left time: 74.4384s
Epoch: 17 cost time: 2.842709541320801
Epoch: 17, Steps: 258 | Train Loss: 0.4950247 Vali Loss: 0.2607827 Test Loss: 0.3528188
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.6026906
	speed: 0.0509s/iter; left time: 428.1809s
	iters: 200, epoch: 18 | loss: 0.3719104
	speed: 0.0093s/iter; left time: 77.1171s
Epoch: 18 cost time: 2.9006526470184326
Epoch: 18, Steps: 258 | Train Loss: 0.4949632 Vali Loss: 0.2606691 Test Loss: 0.3528820
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3757881
	speed: 0.0528s/iter; left time: 430.3591s
	iters: 200, epoch: 19 | loss: 0.4356644
	speed: 0.0096s/iter; left time: 77.4506s
Epoch: 19 cost time: 2.9918177127838135
Epoch: 19, Steps: 258 | Train Loss: 0.4951749 Vali Loss: 0.2606961 Test Loss: 0.3527223
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.7398241
	speed: 0.0524s/iter; left time: 414.2800s
	iters: 200, epoch: 20 | loss: 0.6901695
	speed: 0.0092s/iter; left time: 71.4282s
Epoch: 20 cost time: 2.905477523803711
Epoch: 20, Steps: 258 | Train Loss: 0.4949707 Vali Loss: 0.2606030 Test Loss: 0.3526671
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3369539
	speed: 0.0538s/iter; left time: 411.0318s
	iters: 200, epoch: 21 | loss: 0.6480064
	speed: 0.0091s/iter; left time: 68.4728s
Epoch: 21 cost time: 2.9575300216674805
Epoch: 21, Steps: 258 | Train Loss: 0.4945073 Vali Loss: 0.2604475 Test Loss: 0.3525700
Validation loss decreased (0.260594 --> 0.260448).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4389731
	speed: 0.0546s/iter; left time: 403.0527s
	iters: 200, epoch: 22 | loss: 0.3643246
	speed: 0.0090s/iter; left time: 65.5133s
Epoch: 22 cost time: 2.8683855533599854
Epoch: 22, Steps: 258 | Train Loss: 0.4946775 Vali Loss: 0.2603291 Test Loss: 0.3526010
Validation loss decreased (0.260448 --> 0.260329).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4354681
	speed: 0.0564s/iter; left time: 401.8380s
	iters: 200, epoch: 23 | loss: 0.4899466
	speed: 0.0100s/iter; left time: 70.2236s
Epoch: 23 cost time: 3.0529634952545166
Epoch: 23, Steps: 258 | Train Loss: 0.4935116 Vali Loss: 0.2603980 Test Loss: 0.3525713
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5260977
	speed: 0.0506s/iter; left time: 347.2498s
	iters: 200, epoch: 24 | loss: 0.5335602
	speed: 0.0093s/iter; left time: 62.8430s
Epoch: 24 cost time: 2.868036985397339
Epoch: 24, Steps: 258 | Train Loss: 0.4943609 Vali Loss: 0.2603887 Test Loss: 0.3524088
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5058795
	speed: 0.0539s/iter; left time: 356.5385s
	iters: 200, epoch: 25 | loss: 0.6090977
	speed: 0.0103s/iter; left time: 67.0028s
Epoch: 25 cost time: 3.121081829071045
Epoch: 25, Steps: 258 | Train Loss: 0.4942349 Vali Loss: 0.2602887 Test Loss: 0.3524364
Validation loss decreased (0.260329 --> 0.260289).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4492454
	speed: 0.0557s/iter; left time: 353.9757s
	iters: 200, epoch: 26 | loss: 0.5044553
	speed: 0.0104s/iter; left time: 64.8378s
Epoch: 26 cost time: 3.273486852645874
Epoch: 26, Steps: 258 | Train Loss: 0.4943406 Vali Loss: 0.2605416 Test Loss: 0.3524809
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5310141
	speed: 0.0544s/iter; left time: 331.6311s
	iters: 200, epoch: 27 | loss: 0.4816548
	speed: 0.0094s/iter; left time: 56.0371s
Epoch: 27 cost time: 2.9538896083831787
Epoch: 27, Steps: 258 | Train Loss: 0.4937500 Vali Loss: 0.2602461 Test Loss: 0.3524487
Validation loss decreased (0.260289 --> 0.260246).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5185886
	speed: 0.0569s/iter; left time: 332.0217s
	iters: 200, epoch: 28 | loss: 0.5354443
	speed: 0.0102s/iter; left time: 58.6335s
Epoch: 28 cost time: 3.066657543182373
Epoch: 28, Steps: 258 | Train Loss: 0.4937155 Vali Loss: 0.2601746 Test Loss: 0.3524292
Validation loss decreased (0.260246 --> 0.260175).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.5080081
	speed: 0.0555s/iter; left time: 309.2975s
	iters: 200, epoch: 29 | loss: 0.4245389
	speed: 0.0094s/iter; left time: 51.5319s
Epoch: 29 cost time: 3.1293985843658447
Epoch: 29, Steps: 258 | Train Loss: 0.4944031 Vali Loss: 0.2603012 Test Loss: 0.3524177
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.6667404
	speed: 0.0541s/iter; left time: 287.4953s
	iters: 200, epoch: 30 | loss: 0.4574393
	speed: 0.0094s/iter; left time: 49.1859s
Epoch: 30 cost time: 2.9696455001831055
Epoch: 30, Steps: 258 | Train Loss: 0.4939747 Vali Loss: 0.2600221 Test Loss: 0.3524194
Validation loss decreased (0.260175 --> 0.260022).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4604883
	speed: 0.0552s/iter; left time: 279.3818s
	iters: 200, epoch: 31 | loss: 0.4732870
	speed: 0.0093s/iter; left time: 46.2454s
Epoch: 31 cost time: 2.9232981204986572
Epoch: 31, Steps: 258 | Train Loss: 0.4935969 Vali Loss: 0.2601946 Test Loss: 0.3524003
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.5314983
	speed: 0.0521s/iter; left time: 250.2691s
	iters: 200, epoch: 32 | loss: 0.5810678
	speed: 0.0094s/iter; left time: 44.2136s
Epoch: 32 cost time: 2.9896044731140137
Epoch: 32, Steps: 258 | Train Loss: 0.4931874 Vali Loss: 0.2601509 Test Loss: 0.3522999
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4551722
	speed: 0.0518s/iter; left time: 235.3187s
	iters: 200, epoch: 33 | loss: 0.4742486
	speed: 0.0106s/iter; left time: 46.9129s
Epoch: 33 cost time: 3.013939619064331
Epoch: 33, Steps: 258 | Train Loss: 0.4930892 Vali Loss: 0.2602394 Test Loss: 0.3522769
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3920030
	speed: 0.0538s/iter; left time: 230.4603s
	iters: 200, epoch: 34 | loss: 0.5601355
	speed: 0.0092s/iter; left time: 38.4619s
Epoch: 34 cost time: 2.8807179927825928
Epoch: 34, Steps: 258 | Train Loss: 0.4934773 Vali Loss: 0.2602104 Test Loss: 0.3523345
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.5245082
	speed: 0.0528s/iter; left time: 212.7755s
	iters: 200, epoch: 35 | loss: 0.5592085
	speed: 0.0102s/iter; left time: 40.0682s
Epoch: 35 cost time: 3.0729103088378906
Epoch: 35, Steps: 258 | Train Loss: 0.4932844 Vali Loss: 0.2604312 Test Loss: 0.3523383
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4593197
	speed: 0.0518s/iter; left time: 195.1605s
	iters: 200, epoch: 36 | loss: 0.4842672
	speed: 0.0088s/iter; left time: 32.4800s
Epoch: 36 cost time: 2.867990493774414
Epoch: 36, Steps: 258 | Train Loss: 0.4938116 Vali Loss: 0.2601168 Test Loss: 0.3522565
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3542092
	speed: 0.0510s/iter; left time: 179.0793s
	iters: 200, epoch: 37 | loss: 0.6133042
	speed: 0.0090s/iter; left time: 30.7282s
Epoch: 37 cost time: 2.8332722187042236
Epoch: 37, Steps: 258 | Train Loss: 0.4937472 Vali Loss: 0.2599384 Test Loss: 0.3522631
Validation loss decreased (0.260022 --> 0.259938).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4846775
	speed: 0.0576s/iter; left time: 187.3512s
	iters: 200, epoch: 38 | loss: 0.5129995
	speed: 0.0091s/iter; left time: 28.6094s
Epoch: 38 cost time: 2.9914731979370117
Epoch: 38, Steps: 258 | Train Loss: 0.4938539 Vali Loss: 0.2601809 Test Loss: 0.3522709
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.5048515
	speed: 0.0516s/iter; left time: 154.6280s
	iters: 200, epoch: 39 | loss: 0.4590266
	speed: 0.0094s/iter; left time: 27.1720s
Epoch: 39 cost time: 2.8398993015289307
Epoch: 39, Steps: 258 | Train Loss: 0.4935713 Vali Loss: 0.2597136 Test Loss: 0.3523196
Validation loss decreased (0.259938 --> 0.259714).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4454600
	speed: 0.0538s/iter; left time: 147.3906s
	iters: 200, epoch: 40 | loss: 0.5025565
	speed: 0.0087s/iter; left time: 22.9579s
Epoch: 40 cost time: 2.8019351959228516
Epoch: 40, Steps: 258 | Train Loss: 0.4934657 Vali Loss: 0.2600929 Test Loss: 0.3523543
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.6170201
	speed: 0.0517s/iter; left time: 128.3830s
	iters: 200, epoch: 41 | loss: 0.4472413
	speed: 0.0094s/iter; left time: 22.4295s
Epoch: 41 cost time: 2.9299371242523193
Epoch: 41, Steps: 258 | Train Loss: 0.4932178 Vali Loss: 0.2597906 Test Loss: 0.3522784
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4563319
	speed: 0.0534s/iter; left time: 118.6987s
	iters: 200, epoch: 42 | loss: 0.4517069
	speed: 0.0090s/iter; left time: 19.2026s
Epoch: 42 cost time: 2.91428804397583
Epoch: 42, Steps: 258 | Train Loss: 0.4940916 Vali Loss: 0.2601701 Test Loss: 0.3522672
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4163353
	speed: 0.0525s/iter; left time: 103.1990s
	iters: 200, epoch: 43 | loss: 0.5680019
	speed: 0.0096s/iter; left time: 17.9117s
Epoch: 43 cost time: 3.030035972595215
Epoch: 43, Steps: 258 | Train Loss: 0.4933743 Vali Loss: 0.2600936 Test Loss: 0.3522764
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.5364166
	speed: 0.0497s/iter; left time: 84.8721s
	iters: 200, epoch: 44 | loss: 0.6481620
	speed: 0.0093s/iter; left time: 14.9282s
Epoch: 44 cost time: 2.8978700637817383
Epoch: 44, Steps: 258 | Train Loss: 0.4933949 Vali Loss: 0.2599618 Test Loss: 0.3522465
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.5062079
	speed: 0.0522s/iter; left time: 75.6702s
	iters: 200, epoch: 45 | loss: 0.5197535
	speed: 0.0090s/iter; left time: 12.1638s
Epoch: 45 cost time: 2.8268635272979736
Epoch: 45, Steps: 258 | Train Loss: 0.4930939 Vali Loss: 0.2600175 Test Loss: 0.3522416
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.5717757
	speed: 0.0522s/iter; left time: 62.1329s
	iters: 200, epoch: 46 | loss: 0.4212204
	speed: 0.0092s/iter; left time: 10.0838s
Epoch: 46 cost time: 2.8288722038269043
Epoch: 46, Steps: 258 | Train Loss: 0.4935782 Vali Loss: 0.2601147 Test Loss: 0.3522245
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4494974
	speed: 0.0507s/iter; left time: 47.2836s
	iters: 200, epoch: 47 | loss: 0.4069218
	speed: 0.0091s/iter; left time: 7.5995s
Epoch: 47 cost time: 2.7811942100524902
Epoch: 47, Steps: 258 | Train Loss: 0.4935118 Vali Loss: 0.2601461 Test Loss: 0.3522356
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4382875
	speed: 0.0500s/iter; left time: 33.7598s
	iters: 200, epoch: 48 | loss: 0.5175716
	speed: 0.0095s/iter; left time: 5.4557s
Epoch: 48 cost time: 2.941869020462036
Epoch: 48, Steps: 258 | Train Loss: 0.4928470 Vali Loss: 0.2598949 Test Loss: 0.3522065
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4733551
	speed: 0.0531s/iter; left time: 22.1435s
	iters: 200, epoch: 49 | loss: 0.6515802
	speed: 0.0095s/iter; left time: 3.0242s
Epoch: 49 cost time: 3.0549263954162598
Epoch: 49, Steps: 258 | Train Loss: 0.4938956 Vali Loss: 0.2601530 Test Loss: 0.3522008
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.5077739
	speed: 0.0513s/iter; left time: 8.1504s
	iters: 200, epoch: 50 | loss: 0.4308816
	speed: 0.0097s/iter; left time: 0.5717s
Epoch: 50 cost time: 3.007110118865967
Epoch: 50, Steps: 258 | Train Loss: 0.4937892 Vali Loss: 0.2602675 Test Loss: 0.3521762
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.0497355408796396e-05
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3489380478858948, mae:0.37786510586738586, rse:0.47480860352516174, corr:[0.54430395 0.54343104 0.53717065 0.53593755 0.53780234 0.5382559
 0.53669167 0.53537965 0.53557307 0.53662026 0.53697395 0.5362212
 0.5353528  0.53521097 0.535759   0.5360503  0.5354891  0.5344443
 0.53361094 0.5333179  0.5332903  0.5329014  0.53204405 0.531168
 0.5306723  0.53061455 0.5306048  0.5302261  0.5294897  0.5287582
 0.5283193  0.5282339  0.52812445 0.5276256  0.5268345  0.52607423
 0.5255185  0.5251642  0.5247696  0.5242064  0.5235538  0.5229574
 0.522546   0.52221596 0.5217599  0.5210439  0.520167   0.51931095
 0.51858175 0.5179719  0.51737463 0.51671755 0.5159573  0.51517874
 0.5145178  0.51400375 0.5135266  0.5130234  0.51254874 0.5121795
 0.51200885 0.5119648  0.51184845 0.5115463  0.51113915 0.510846
 0.5106907  0.51062775 0.5104536  0.51007664 0.5095902  0.5091695
 0.5088907  0.5086405  0.5082192  0.5076114  0.5069682  0.50644535
 0.50608283 0.5056899  0.50503045 0.504131   0.503196   0.50247717
 0.50207543 0.501816   0.5014285  0.50088936 0.5002836  0.4997823
 0.49944845 0.49912104 0.49857947 0.49774632 0.49664375 0.49535125
 0.49394357 0.4925004  0.49102694 0.48963562 0.48846218 0.4875109
 0.48662108 0.48557708 0.48431048 0.48299572 0.4819052  0.48115146
 0.4805195  0.47977012 0.4788038  0.47768465 0.47664702 0.47577018
 0.4749766  0.47416633 0.4733026  0.4723925  0.4715307  0.4707075
 0.46994117 0.46916685 0.46845195 0.46781716 0.46723986 0.46658763
 0.46572992 0.46461126 0.4633694  0.46226218 0.46148348 0.46093875
 0.46038616 0.4596279  0.45872247 0.45787528 0.45722234 0.4567258
 0.45614696 0.45534292 0.45440105 0.45360157 0.4531246  0.45278543
 0.45228472 0.4514147  0.45040315 0.44953203 0.44896472 0.44852945
 0.44797018 0.44716045 0.44617492 0.44532397 0.4448169  0.44453037
 0.44415486 0.44353497 0.4428074  0.4423357  0.44216526 0.4420646
 0.44167268 0.44091022 0.4400117  0.43935278 0.4390875  0.43904293
 0.43889457 0.4385007  0.4379365  0.43751085 0.43730143 0.43711665
 0.43665683 0.4358428  0.43487704 0.43408597 0.43360913 0.4333246
 0.43288887 0.43217468 0.43136802 0.43069866 0.430252   0.42988455
 0.42941412 0.42871487 0.42787325 0.42699215 0.42608616 0.42500454
 0.42364946 0.42218447 0.42066225 0.41923335 0.41800892 0.4169016
 0.41582823 0.41473338 0.4136286  0.41249755 0.41127777 0.4099487
 0.40858844 0.40735805 0.4063727  0.40558183 0.40484762 0.40403858
 0.40302196 0.401943   0.40101495 0.4002634  0.39951056 0.39848462
 0.3972001  0.39587128 0.3948385  0.394054   0.3932182  0.3920995
 0.39073333 0.3894162  0.38843626 0.38764557 0.38679725 0.38575003
 0.38449675 0.38331875 0.3824136  0.3817625  0.38114893 0.38045886
 0.3797358  0.3792215  0.37901953 0.3789908  0.37890086 0.37854743
 0.37797397 0.3773087  0.37672007 0.3763169  0.37590006 0.37549248
 0.37521777 0.37520728 0.37533787 0.37532562 0.3750464  0.3745304
 0.37408128 0.37405455 0.3743412  0.37458527 0.37437385 0.37369254
 0.37282553 0.37217346 0.37186924 0.37176847 0.37157875 0.37129918
 0.37108177 0.37101978 0.3710559  0.37089455 0.37031886 0.36946133
 0.3688566  0.36882743 0.36914006 0.36929289 0.36884022 0.3678311
 0.3668279  0.36624417 0.36611366 0.36617282 0.36601633 0.3656554
 0.36532614 0.36527333 0.3653702  0.36517754 0.3643607  0.36291137
 0.3613562  0.36027366 0.35961264 0.35896546 0.35805634 0.35694575
 0.35610116 0.355712   0.35553524 0.35516536 0.35445228 0.3535313
 0.35285485 0.3525323  0.35239467 0.35190234 0.35092345 0.34973246
 0.34888294 0.3485744  0.34863243 0.3486248  0.34826568 0.34774226
 0.34726277 0.3470826  0.34706458 0.34694925 0.34667683 0.34641668
 0.34629786 0.34635124 0.34636068 0.34605616 0.34542435 0.34480256
 0.34451777 0.34466693 0.34496823 0.3450166  0.34481156 0.34457037
 0.34467357 0.34507576 0.3454751  0.34555987 0.345191   0.34470177
 0.34440824 0.344354   0.3444443  0.3444594  0.34421772 0.34374934
 0.34329712 0.34301096 0.34285498 0.34273764 0.34268397 0.34282085
 0.34310892 0.34336156 0.34328055 0.3427586  0.3420661  0.3416006
 0.34157985 0.34183943 0.34189263 0.34154284 0.34080902 0.34018958
 0.3400314  0.34028518 0.34054863 0.3404707  0.34002304 0.33959404
 0.33943993 0.3396342  0.33987293 0.33987638 0.33963683 0.3393301
 0.33921784 0.33935738 0.3395335  0.3395142  0.33927193 0.3390442
 0.3390264  0.33913535 0.33909625 0.33870918 0.33816916 0.33764893
 0.33734417 0.3371193  0.33668312 0.3358199  0.33462653 0.33354682
 0.33280772 0.3323583  0.33192772 0.33131614 0.33056346 0.32985544
 0.32937437 0.32912746 0.32882926 0.3283137  0.32766286 0.3269438
 0.32627952 0.32569182 0.32519752 0.3248238  0.32452637 0.3243597
 0.32423455 0.3241007  0.32386103 0.32350662 0.32321092 0.3230398
 0.32301974 0.32302794 0.32288966 0.32261437 0.32231542 0.32214782
 0.32214186 0.32215667 0.32205024 0.3218973  0.3218879  0.32212955
 0.32254878 0.32288373 0.3230016  0.32293537 0.32289588 0.32301873
 0.32322153 0.32320777 0.32286447 0.32230884 0.32181194 0.32151037
 0.32130995 0.3211767  0.32094458 0.32059288 0.32030123 0.32023433
 0.32023707 0.3201597  0.31984285 0.31929782 0.31875315 0.31831768
 0.3180125  0.31772095 0.3173891  0.31699404 0.3166775  0.3165388
 0.31648493 0.31637263 0.31611443 0.31578746 0.315602   0.31550068
 0.3154125  0.31521127 0.31485036 0.31441897 0.314051   0.31382218
 0.3136815  0.3134711  0.31306416 0.31246    0.31182632 0.311251
 0.31081566 0.31045726 0.3101457  0.309706   0.30901676 0.30795735
 0.30654055 0.30505094 0.30369118 0.3026189  0.30181187 0.30108744
 0.30026636 0.29941073 0.2986263  0.29806727 0.29761368 0.29705474
 0.29627508 0.2954218  0.29466617 0.29403788 0.2934848  0.2928184
 0.29193598 0.29085532 0.2898247  0.28913403 0.28871474 0.2884128
 0.28789052 0.28709564 0.28629616 0.28577593 0.28567448 0.28573725
 0.285631   0.28517008 0.28442508 0.28368303 0.28318694 0.2829077
 0.2826321  0.28217727 0.2817325  0.28138196 0.28117722 0.28108013
 0.28093693 0.28067434 0.28027764 0.27989945 0.27971715 0.2797409
 0.27984875 0.27994344 0.2799551  0.27989855 0.2798112  0.27966502
 0.27944276 0.27920648 0.2790982  0.2790731  0.2790289  0.27888474
 0.2786296  0.27835292 0.27807692 0.27782124 0.2774876  0.2770183
 0.2765553  0.27631342 0.27631086 0.27636352 0.27617365 0.27571058
 0.27510533 0.2746944  0.27466297 0.2749121  0.2750263  0.27489224
 0.2745411  0.27424738 0.27409232 0.27402255 0.27393314 0.27377018
 0.2736114  0.27350548 0.27345288 0.2733007  0.2729545  0.27250788
 0.27215725 0.27203596 0.27197292 0.27168807 0.2709164  0.26955134
 0.26789996 0.2665533  0.26563296 0.2648937  0.26398695 0.26285094
 0.26170886 0.26081038 0.26021227 0.25971597 0.25905484 0.25820285
 0.25727996 0.2564713  0.25593382 0.2555191  0.25500533 0.25431687
 0.25349185 0.2526576  0.25204596 0.25164622 0.25147352 0.2514496
 0.25145778 0.25147867 0.2513574  0.2510162  0.250573   0.25014797
 0.24976003 0.24949092 0.24936993 0.24921024 0.24899885 0.24876933
 0.24868254 0.24861257 0.248579   0.24859789 0.24859954 0.24857675
 0.24861874 0.24884613 0.24913736 0.24941365 0.24966305 0.25002107
 0.2505437  0.25114194 0.25160208 0.2517621  0.25153065 0.25106865
 0.2506607  0.25059998 0.25088865 0.251048   0.25105426 0.25087422
 0.2506896  0.25064394 0.25059244 0.25052297 0.25022927 0.24990445
 0.24975093 0.24993256 0.25010392 0.249844   0.24931848 0.24865124
 0.24833016 0.2484102  0.2486493  0.24865794 0.24841604 0.2479988
 0.2479108  0.24818216 0.24844058 0.2484569  0.24815175 0.24769361
 0.24739435 0.24756384 0.24785356 0.2479187  0.24777715 0.24762471
 0.24771121 0.24818008 0.24871331 0.24874158 0.24803281 0.24684386
 0.24561355 0.24463728 0.24385834 0.24301584 0.24208051 0.24110791
 0.2402855  0.23969842 0.23951328 0.23930745 0.2389118  0.23828724
 0.23762985 0.23708586 0.23658459 0.2359573  0.235314   0.23464005
 0.23414731 0.23380916 0.23335427 0.23285317 0.23223156 0.23164944
 0.2313412  0.23116079 0.23091978 0.2302852  0.22941631 0.22860347
 0.22813681 0.22757204 0.22674863 0.22585501 0.22532049 0.22537401
 0.22545114 0.22490427 0.22356632 0.2224394  0.22232944 0.22326513
 0.22372182 0.2224097  0.22020718 0.21985109 0.22169024 0.21969743]
