Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j192_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17501
val 2441
test 5069
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2662502400.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3731370
	speed: 0.2220s/iter; left time: 3008.4072s
	iters: 200, epoch: 1 | loss: 0.2123151
	speed: 0.2566s/iter; left time: 3451.2543s
Epoch: 1 cost time: 67.83889842033386
Epoch: 1, Steps: 273 | Train Loss: 0.3872842 Vali Loss: 0.1389556 Test Loss: 0.1704339
Validation loss decreased (inf --> 0.138956).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1514559
	speed: 0.9122s/iter; left time: 12112.1842s
	iters: 200, epoch: 2 | loss: 0.1655595
	speed: 0.2619s/iter; left time: 3451.4933s
Epoch: 2 cost time: 71.81095123291016
Epoch: 2, Steps: 273 | Train Loss: 0.1571299 Vali Loss: 0.1273317 Test Loss: 0.1545378
Validation loss decreased (0.138956 --> 0.127332).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1596905
	speed: 0.8878s/iter; left time: 11545.4328s
	iters: 200, epoch: 3 | loss: 0.1363087
	speed: 0.2546s/iter; left time: 3285.5627s
Epoch: 3 cost time: 69.84600186347961
Epoch: 3, Steps: 273 | Train Loss: 0.1525634 Vali Loss: 0.1265841 Test Loss: 0.1536092
Validation loss decreased (0.127332 --> 0.126584).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1454539
	speed: 0.9363s/iter; left time: 11921.2772s
	iters: 200, epoch: 4 | loss: 0.1618584
	speed: 0.2878s/iter; left time: 3635.2248s
Epoch: 4 cost time: 78.46339178085327
Epoch: 4, Steps: 273 | Train Loss: 0.1516344 Vali Loss: 0.1262113 Test Loss: 0.1532464
Validation loss decreased (0.126584 --> 0.126211).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1446045
	speed: 1.0105s/iter; left time: 12589.7135s
	iters: 200, epoch: 5 | loss: 0.1545451
	speed: 0.2775s/iter; left time: 3430.0635s
Epoch: 5 cost time: 76.78785800933838
Epoch: 5, Steps: 273 | Train Loss: 0.1511983 Vali Loss: 0.1260498 Test Loss: 0.1529341
Validation loss decreased (0.126211 --> 0.126050).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1535947
	speed: 0.9339s/iter; left time: 11381.0231s
	iters: 200, epoch: 6 | loss: 0.1561931
	speed: 0.2694s/iter; left time: 3255.5716s
Epoch: 6 cost time: 74.95199871063232
Epoch: 6, Steps: 273 | Train Loss: 0.1509212 Vali Loss: 0.1258683 Test Loss: 0.1528114
Validation loss decreased (0.126050 --> 0.125868).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1541382
	speed: 0.9444s/iter; left time: 11251.0016s
	iters: 200, epoch: 7 | loss: 0.1600340
	speed: 0.2731s/iter; left time: 3226.0872s
Epoch: 7 cost time: 75.58385729789734
Epoch: 7, Steps: 273 | Train Loss: 0.1506862 Vali Loss: 0.1258145 Test Loss: 0.1526649
Validation loss decreased (0.125868 --> 0.125814).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1447222
	speed: 0.9635s/iter; left time: 11214.6770s
	iters: 200, epoch: 8 | loss: 0.1410970
	speed: 0.2661s/iter; left time: 3070.7750s
Epoch: 8 cost time: 75.54883122444153
Epoch: 8, Steps: 273 | Train Loss: 0.1506467 Vali Loss: 0.1258210 Test Loss: 0.1526577
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1522949
	speed: 0.8665s/iter; left time: 9849.7861s
	iters: 200, epoch: 9 | loss: 0.1613826
	speed: 0.2150s/iter; left time: 2422.6890s
Epoch: 9 cost time: 63.459924936294556
Epoch: 9, Steps: 273 | Train Loss: 0.1505721 Vali Loss: 0.1257626 Test Loss: 0.1525802
Validation loss decreased (0.125814 --> 0.125763).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1463109
	speed: 0.9527s/iter; left time: 10569.2875s
	iters: 200, epoch: 10 | loss: 0.1638387
	speed: 0.3297s/iter; left time: 3625.2617s
Epoch: 10 cost time: 87.3568344116211
Epoch: 10, Steps: 273 | Train Loss: 0.1505146 Vali Loss: 0.1255470 Test Loss: 0.1525564
Validation loss decreased (0.125763 --> 0.125547).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1324777
	speed: 0.9453s/iter; left time: 10228.8431s
	iters: 200, epoch: 11 | loss: 0.1577504
	speed: 0.2364s/iter; left time: 2534.5717s
Epoch: 11 cost time: 66.74145197868347
Epoch: 11, Steps: 273 | Train Loss: 0.1504108 Vali Loss: 0.1257286 Test Loss: 0.1525450
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1488716
	speed: 0.8670s/iter; left time: 9145.4535s
	iters: 200, epoch: 12 | loss: 0.1601370
	speed: 0.2445s/iter; left time: 2554.6023s
Epoch: 12 cost time: 70.642737865448
Epoch: 12, Steps: 273 | Train Loss: 0.1503759 Vali Loss: 0.1256910 Test Loss: 0.1525016
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1649466
	speed: 0.8958s/iter; left time: 9203.9908s
	iters: 200, epoch: 13 | loss: 0.1375708
	speed: 0.2189s/iter; left time: 2227.2791s
Epoch: 13 cost time: 65.24319577217102
Epoch: 13, Steps: 273 | Train Loss: 0.1503097 Vali Loss: 0.1257349 Test Loss: 0.1524676
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5069
mse:0.1495036482810974, mae:0.2447860985994339, rse:0.38443973660469055, corr:[0.4632992  0.46532562 0.4664024  0.46742937 0.467636   0.4681897
 0.46820185 0.46812814 0.46827754 0.46802542 0.46818653 0.46774155
 0.4679425  0.4677988  0.46785358 0.4678074  0.46786648 0.46799695
 0.46787432 0.4676533  0.46759307 0.46763828 0.46779674 0.46811002
 0.46835455 0.46862346 0.46875185 0.4688578  0.46858874 0.46863958
 0.46840453 0.46817046 0.46813524 0.4679213  0.46787006 0.46757224
 0.4675331  0.4674159  0.46730056 0.46716192 0.4670577  0.4670713
 0.46688765 0.4667105  0.46662506 0.46662405 0.46657604 0.46684906
 0.46701732 0.467112   0.4670755  0.46719947 0.46730086 0.4671677
 0.4672367  0.4670184  0.4667739  0.4667383  0.46684414 0.466537
 0.46645647 0.4664616  0.46634012 0.4664927  0.46624374 0.46639937
 0.46644256 0.46608102 0.46593776 0.46584147 0.46586803 0.46610427
 0.46618277 0.46625328 0.46644816 0.4663412  0.46641234 0.46615192
 0.4661123  0.46629784 0.46609232 0.46604967 0.46605995 0.46606407
 0.46593675 0.4660279  0.4660562  0.46601367 0.46601957 0.46593633
 0.46578726 0.46551168 0.46547157 0.4654792  0.46557316 0.4656969
 0.46571693 0.46574515 0.4660342  0.4660464  0.46596268 0.46599597
 0.46590587 0.46596205 0.46573478 0.46588394 0.46588415 0.46560702
 0.46565956 0.46567118 0.46567816 0.46546522 0.46552202 0.46549678
 0.46545494 0.46536815 0.46513334 0.46532455 0.4655157  0.4656423
 0.4659829  0.4661691  0.46613985 0.4663413  0.4662774  0.46620288
 0.4662145  0.4660114  0.46593285 0.46589872 0.4659308  0.4657562
 0.4657821  0.46563956 0.4656157  0.46559724 0.46549076 0.46581846
 0.4656261  0.4653284  0.46502283 0.4650379  0.4650589  0.46499375
 0.46524447 0.4653407  0.46561638 0.46568155 0.4655957  0.46542883
 0.4652642  0.46518824 0.46531624 0.46515957 0.46499643 0.4650828
 0.4651322  0.46529183 0.46507567 0.46505097 0.46500292 0.46528178
 0.46515727 0.46470964 0.4647461  0.46444392 0.46451    0.4646168
 0.46455702 0.46457377 0.46466357 0.46456283 0.464366   0.4642345
 0.46385527 0.4637654  0.46387944 0.46371222 0.46352172 0.46362442
 0.463584   0.4639012  0.4633828  0.46353763 0.46357346 0.4634482
 0.46365786 0.46263033 0.4631375  0.46255806 0.46368712 0.46332693]
