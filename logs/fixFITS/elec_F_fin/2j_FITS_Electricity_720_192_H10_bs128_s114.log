Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j192_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17501
val 2441
test 5069
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10650009600.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 39.790014028549194
Epoch: 1, Steps: 68 | Train Loss: 0.9776076 Vali Loss: 0.7450992 Test Loss: 0.8577251
Validation loss decreased (inf --> 0.745099).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 42.074739933013916
Epoch: 2, Steps: 68 | Train Loss: 0.7637245 Vali Loss: 0.6613979 Test Loss: 0.7625767
Validation loss decreased (0.745099 --> 0.661398).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 42.59643197059631
Epoch: 3, Steps: 68 | Train Loss: 0.6903092 Vali Loss: 0.6228240 Test Loss: 0.7190135
Validation loss decreased (0.661398 --> 0.622824).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 44.08880615234375
Epoch: 4, Steps: 68 | Train Loss: 0.6395026 Vali Loss: 0.5917082 Test Loss: 0.6835735
Validation loss decreased (0.622824 --> 0.591708).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 44.391409397125244
Epoch: 5, Steps: 68 | Train Loss: 0.5963257 Vali Loss: 0.5598671 Test Loss: 0.6474890
Validation loss decreased (0.591708 --> 0.559867).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 43.34644031524658
Epoch: 6, Steps: 68 | Train Loss: 0.5583463 Vali Loss: 0.5358400 Test Loss: 0.6207095
Validation loss decreased (0.559867 --> 0.535840).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 40.85346579551697
Epoch: 7, Steps: 68 | Train Loss: 0.5247149 Vali Loss: 0.5102473 Test Loss: 0.5918390
Validation loss decreased (0.535840 --> 0.510247).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 45.46391987800598
Epoch: 8, Steps: 68 | Train Loss: 0.4947098 Vali Loss: 0.4872858 Test Loss: 0.5661816
Validation loss decreased (0.510247 --> 0.487286).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 47.3762149810791
Epoch: 9, Steps: 68 | Train Loss: 0.4676082 Vali Loss: 0.4694648 Test Loss: 0.5463029
Validation loss decreased (0.487286 --> 0.469465).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 44.298175573349
Epoch: 10, Steps: 68 | Train Loss: 0.4430168 Vali Loss: 0.4518788 Test Loss: 0.5267812
Validation loss decreased (0.469465 --> 0.451879).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 43.651864767074585
Epoch: 11, Steps: 68 | Train Loss: 0.4208733 Vali Loss: 0.4352295 Test Loss: 0.5079342
Validation loss decreased (0.451879 --> 0.435230).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 41.19013166427612
Epoch: 12, Steps: 68 | Train Loss: 0.4006992 Vali Loss: 0.4209696 Test Loss: 0.4920197
Validation loss decreased (0.435230 --> 0.420970).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 46.70669603347778
Epoch: 13, Steps: 68 | Train Loss: 0.3823894 Vali Loss: 0.4081788 Test Loss: 0.4778559
Validation loss decreased (0.420970 --> 0.408179).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 46.77883839607239
Epoch: 14, Steps: 68 | Train Loss: 0.3654729 Vali Loss: 0.3948396 Test Loss: 0.4630155
Validation loss decreased (0.408179 --> 0.394840).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 42.96214962005615
Epoch: 15, Steps: 68 | Train Loss: 0.3501189 Vali Loss: 0.3817762 Test Loss: 0.4483589
Validation loss decreased (0.394840 --> 0.381776).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 46.15067505836487
Epoch: 16, Steps: 68 | Train Loss: 0.3359681 Vali Loss: 0.3706822 Test Loss: 0.4359291
Validation loss decreased (0.381776 --> 0.370682).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 44.092334508895874
Epoch: 17, Steps: 68 | Train Loss: 0.3229397 Vali Loss: 0.3604703 Test Loss: 0.4245432
Validation loss decreased (0.370682 --> 0.360470).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 44.38157320022583
Epoch: 18, Steps: 68 | Train Loss: 0.3109228 Vali Loss: 0.3505444 Test Loss: 0.4133197
Validation loss decreased (0.360470 --> 0.350544).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 44.73500299453735
Epoch: 19, Steps: 68 | Train Loss: 0.2998385 Vali Loss: 0.3418658 Test Loss: 0.4033149
Validation loss decreased (0.350544 --> 0.341866).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 45.37140703201294
Epoch: 20, Steps: 68 | Train Loss: 0.2896494 Vali Loss: 0.3334463 Test Loss: 0.3942161
Validation loss decreased (0.341866 --> 0.333446).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 39.59762644767761
Epoch: 21, Steps: 68 | Train Loss: 0.2800851 Vali Loss: 0.3255223 Test Loss: 0.3854270
Validation loss decreased (0.333446 --> 0.325522).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 41.44435119628906
Epoch: 22, Steps: 68 | Train Loss: 0.2712747 Vali Loss: 0.3202991 Test Loss: 0.3792575
Validation loss decreased (0.325522 --> 0.320299).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 51.42654585838318
Epoch: 23, Steps: 68 | Train Loss: 0.2630521 Vali Loss: 0.3125179 Test Loss: 0.3705303
Validation loss decreased (0.320299 --> 0.312518).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 49.66475796699524
Epoch: 24, Steps: 68 | Train Loss: 0.2554933 Vali Loss: 0.3062161 Test Loss: 0.3631403
Validation loss decreased (0.312518 --> 0.306216).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 43.187597036361694
Epoch: 25, Steps: 68 | Train Loss: 0.2483816 Vali Loss: 0.3006674 Test Loss: 0.3573521
Validation loss decreased (0.306216 --> 0.300667).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 46.41642141342163
Epoch: 26, Steps: 68 | Train Loss: 0.2418273 Vali Loss: 0.2959557 Test Loss: 0.3516377
Validation loss decreased (0.300667 --> 0.295956).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 46.10450530052185
Epoch: 27, Steps: 68 | Train Loss: 0.2356120 Vali Loss: 0.2893482 Test Loss: 0.3444027
Validation loss decreased (0.295956 --> 0.289348).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 42.87448811531067
Epoch: 28, Steps: 68 | Train Loss: 0.2298599 Vali Loss: 0.2853977 Test Loss: 0.3400129
Validation loss decreased (0.289348 --> 0.285398).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 41.25858497619629
Epoch: 29, Steps: 68 | Train Loss: 0.2244613 Vali Loss: 0.2822759 Test Loss: 0.3362431
Validation loss decreased (0.285398 --> 0.282276).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 42.324265480041504
Epoch: 30, Steps: 68 | Train Loss: 0.2193959 Vali Loss: 0.2777192 Test Loss: 0.3309574
Validation loss decreased (0.282276 --> 0.277719).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 45.536160707473755
Epoch: 31, Steps: 68 | Train Loss: 0.2146788 Vali Loss: 0.2740477 Test Loss: 0.3271469
Validation loss decreased (0.277719 --> 0.274048).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 46.83729314804077
Epoch: 32, Steps: 68 | Train Loss: 0.2102502 Vali Loss: 0.2699555 Test Loss: 0.3223441
Validation loss decreased (0.274048 --> 0.269955).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 46.91644787788391
Epoch: 33, Steps: 68 | Train Loss: 0.2060606 Vali Loss: 0.2669819 Test Loss: 0.3188928
Validation loss decreased (0.269955 --> 0.266982).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 44.282634258270264
Epoch: 34, Steps: 68 | Train Loss: 0.2021357 Vali Loss: 0.2635909 Test Loss: 0.3153551
Validation loss decreased (0.266982 --> 0.263591).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 46.1777868270874
Epoch: 35, Steps: 68 | Train Loss: 0.1984320 Vali Loss: 0.2603583 Test Loss: 0.3116806
Validation loss decreased (0.263591 --> 0.260358).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 40.99943923950195
Epoch: 36, Steps: 68 | Train Loss: 0.1950016 Vali Loss: 0.2583780 Test Loss: 0.3092731
Validation loss decreased (0.260358 --> 0.258378).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 41.72193479537964
Epoch: 37, Steps: 68 | Train Loss: 0.1917619 Vali Loss: 0.2553459 Test Loss: 0.3060846
Validation loss decreased (0.258378 --> 0.255346).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 39.591073751449585
Epoch: 38, Steps: 68 | Train Loss: 0.1886771 Vali Loss: 0.2528515 Test Loss: 0.3028081
Validation loss decreased (0.255346 --> 0.252851).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 40.9109308719635
Epoch: 39, Steps: 68 | Train Loss: 0.1857813 Vali Loss: 0.2502376 Test Loss: 0.3003491
Validation loss decreased (0.252851 --> 0.250238).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 49.21188139915466
Epoch: 40, Steps: 68 | Train Loss: 0.1830290 Vali Loss: 0.2485960 Test Loss: 0.2979738
Validation loss decreased (0.250238 --> 0.248596).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 51.73431444168091
Epoch: 41, Steps: 68 | Train Loss: 0.1804727 Vali Loss: 0.2462198 Test Loss: 0.2952812
Validation loss decreased (0.248596 --> 0.246220).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 51.39647459983826
Epoch: 42, Steps: 68 | Train Loss: 0.1780171 Vali Loss: 0.2440107 Test Loss: 0.2930303
Validation loss decreased (0.246220 --> 0.244011).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 44.00145936012268
Epoch: 43, Steps: 68 | Train Loss: 0.1757241 Vali Loss: 0.2417462 Test Loss: 0.2903677
Validation loss decreased (0.244011 --> 0.241746).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 41.196940422058105
Epoch: 44, Steps: 68 | Train Loss: 0.1735421 Vali Loss: 0.2405062 Test Loss: 0.2888633
Validation loss decreased (0.241746 --> 0.240506).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 40.4746618270874
Epoch: 45, Steps: 68 | Train Loss: 0.1715071 Vali Loss: 0.2386249 Test Loss: 0.2866237
Validation loss decreased (0.240506 --> 0.238625).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 38.25022220611572
Epoch: 46, Steps: 68 | Train Loss: 0.1695588 Vali Loss: 0.2368722 Test Loss: 0.2850468
Validation loss decreased (0.238625 --> 0.236872).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 41.73379635810852
Epoch: 47, Steps: 68 | Train Loss: 0.1676898 Vali Loss: 0.2353285 Test Loss: 0.2829213
Validation loss decreased (0.236872 --> 0.235328).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 44.83928871154785
Epoch: 48, Steps: 68 | Train Loss: 0.1659632 Vali Loss: 0.2340415 Test Loss: 0.2816065
Validation loss decreased (0.235328 --> 0.234041).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 46.4137008190155
Epoch: 49, Steps: 68 | Train Loss: 0.1643211 Vali Loss: 0.2324065 Test Loss: 0.2799225
Validation loss decreased (0.234041 --> 0.232406).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 48.1217360496521
Epoch: 50, Steps: 68 | Train Loss: 0.1627621 Vali Loss: 0.2316531 Test Loss: 0.2785757
Validation loss decreased (0.232406 --> 0.231653).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 17501
val 2441
test 5069
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10650009600.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 41.077678203582764
Epoch: 1, Steps: 68 | Train Loss: 0.2054774 Vali Loss: 0.1349297 Test Loss: 0.1650058
Validation loss decreased (inf --> 0.134930).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 39.287792682647705
Epoch: 2, Steps: 68 | Train Loss: 0.1552243 Vali Loss: 0.1264383 Test Loss: 0.1527543
Validation loss decreased (0.134930 --> 0.126438).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 41.23629665374756
Epoch: 3, Steps: 68 | Train Loss: 0.1511266 Vali Loss: 0.1256870 Test Loss: 0.1521482
Validation loss decreased (0.126438 --> 0.125687).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 48.25681161880493
Epoch: 4, Steps: 68 | Train Loss: 0.1507682 Vali Loss: 0.1260127 Test Loss: 0.1519293
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 45.26378631591797
Epoch: 5, Steps: 68 | Train Loss: 0.1504851 Vali Loss: 0.1255108 Test Loss: 0.1518632
Validation loss decreased (0.125687 --> 0.125511).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 45.23810434341431
Epoch: 6, Steps: 68 | Train Loss: 0.1505002 Vali Loss: 0.1257620 Test Loss: 0.1517880
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 41.89541411399841
Epoch: 7, Steps: 68 | Train Loss: 0.1503923 Vali Loss: 0.1256231 Test Loss: 0.1517301
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 45.52125000953674
Epoch: 8, Steps: 68 | Train Loss: 0.1502803 Vali Loss: 0.1257568 Test Loss: 0.1516465
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5069
mse:0.14965839684009552, mae:0.2450065016746521, rse:0.38463863730430603, corr:[0.46296665 0.46491775 0.46710014 0.4677495  0.4688126  0.4686406
 0.46939906 0.468837   0.46928778 0.46867636 0.4687784  0.4686476
 0.46829945 0.46848688 0.4679787  0.46808326 0.46825904 0.46811798
 0.46806324 0.4678085  0.46771392 0.46770632 0.46772155 0.46804938
 0.46834296 0.46878943 0.4688686  0.46897396 0.4690267  0.46869653
 0.468769   0.46837962 0.46828195 0.46808544 0.46781185 0.46789232
 0.46757567 0.46756774 0.46748155 0.4673939  0.46745098 0.4671961
 0.46706042 0.46684015 0.4666013  0.4667488  0.4667661  0.4668952
 0.46701404 0.46720845 0.46751764 0.46731317 0.46729332 0.46709606
 0.46699914 0.4669345  0.46668082 0.46681377 0.46658632 0.46647623
 0.46654797 0.4664762  0.46662357 0.46659178 0.46646467 0.46655574
 0.46655846 0.46638218 0.46615127 0.46620116 0.46625096 0.46616745
 0.46638975 0.4665055  0.466545   0.4664938  0.46644008 0.46655494
 0.46620983 0.46603793 0.46593735 0.46586558 0.4659509  0.46594763
 0.46600887 0.4658935  0.46589765 0.46608135 0.46612442 0.46610457
 0.46597567 0.46574965 0.46569988 0.46565863 0.4655442  0.46549323
 0.46565893 0.4659428  0.4659126  0.4658299  0.4656765  0.4657012
 0.46566132 0.4655128  0.46554282 0.4652615  0.46537718 0.4654007
 0.4653264  0.46559197 0.4655157  0.4655022  0.4655917  0.46563563
 0.46568638 0.46559075 0.46567202 0.46577263 0.4655836  0.4657848
 0.46591684 0.46596596 0.46605536 0.46574408 0.46592522 0.46579257
 0.46567336 0.4657024  0.4655206  0.46565783 0.46547934 0.4653901
 0.46540964 0.46545586 0.46567285 0.4656525  0.4658163  0.46593672
 0.46587065 0.46583575 0.46553522 0.4654077  0.4654131  0.46532616
 0.46545553 0.46537837 0.46545112 0.4654217  0.4652273  0.46520388
 0.46501914 0.46521413 0.4651174  0.46494853 0.4650225  0.46501285
 0.46524352 0.46515062 0.4652011  0.46530777 0.46530738 0.46560913
 0.46546343 0.4652732  0.4650312  0.46471888 0.46477652 0.46480057
 0.4645313  0.4644409  0.4644811  0.4642212  0.46398866 0.4640352
 0.4636191  0.4636683  0.46380103 0.46354663 0.46378055 0.46363258
 0.46362934 0.46383825 0.46351236 0.46375963 0.46341938 0.4634197
 0.46309748 0.46277156 0.46269324 0.46242267 0.46309084 0.4644859 ]
