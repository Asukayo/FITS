Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j192_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17501
val 2441
test 5069
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21300019200.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 48.53050899505615
Epoch: 1, Steps: 34 | Train Loss: 0.8942961 Vali Loss: 0.5704190 Test Loss: 0.6654557
Validation loss decreased (inf --> 0.570419).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 41.965656757354736
Epoch: 2, Steps: 34 | Train Loss: 0.5800547 Vali Loss: 0.4099130 Test Loss: 0.4828905
Validation loss decreased (0.570419 --> 0.409913).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 42.86690640449524
Epoch: 3, Steps: 34 | Train Loss: 0.4306675 Vali Loss: 0.3098807 Test Loss: 0.3690791
Validation loss decreased (0.409913 --> 0.309881).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 43.60546040534973
Epoch: 4, Steps: 34 | Train Loss: 0.3349815 Vali Loss: 0.2456155 Test Loss: 0.2945002
Validation loss decreased (0.309881 --> 0.245616).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 43.67742848396301
Epoch: 5, Steps: 34 | Train Loss: 0.2720431 Vali Loss: 0.2039099 Test Loss: 0.2454031
Validation loss decreased (0.245616 --> 0.203910).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 44.94379997253418
Epoch: 6, Steps: 34 | Train Loss: 0.2308467 Vali Loss: 0.1760415 Test Loss: 0.2132667
Validation loss decreased (0.203910 --> 0.176042).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 44.053863763809204
Epoch: 7, Steps: 34 | Train Loss: 0.2040008 Vali Loss: 0.1585981 Test Loss: 0.1922901
Validation loss decreased (0.176042 --> 0.158598).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 45.26573967933655
Epoch: 8, Steps: 34 | Train Loss: 0.1864438 Vali Loss: 0.1468309 Test Loss: 0.1786507
Validation loss decreased (0.158598 --> 0.146831).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 47.08493256568909
Epoch: 9, Steps: 34 | Train Loss: 0.1751656 Vali Loss: 0.1396479 Test Loss: 0.1698005
Validation loss decreased (0.146831 --> 0.139648).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 41.28481578826904
Epoch: 10, Steps: 34 | Train Loss: 0.1677738 Vali Loss: 0.1354598 Test Loss: 0.1640558
Validation loss decreased (0.139648 --> 0.135460).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 41.7198805809021
Epoch: 11, Steps: 34 | Train Loss: 0.1631047 Vali Loss: 0.1322745 Test Loss: 0.1603027
Validation loss decreased (0.135460 --> 0.132275).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 41.65099573135376
Epoch: 12, Steps: 34 | Train Loss: 0.1599755 Vali Loss: 0.1308329 Test Loss: 0.1578231
Validation loss decreased (0.132275 --> 0.130833).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 40.094168186187744
Epoch: 13, Steps: 34 | Train Loss: 0.1579703 Vali Loss: 0.1294934 Test Loss: 0.1561598
Validation loss decreased (0.130833 --> 0.129493).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 39.834412813186646
Epoch: 14, Steps: 34 | Train Loss: 0.1564374 Vali Loss: 0.1285902 Test Loss: 0.1550259
Validation loss decreased (0.129493 --> 0.128590).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 41.743406534194946
Epoch: 15, Steps: 34 | Train Loss: 0.1555684 Vali Loss: 0.1282314 Test Loss: 0.1542351
Validation loss decreased (0.128590 --> 0.128231).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 38.17854142189026
Epoch: 16, Steps: 34 | Train Loss: 0.1548682 Vali Loss: 0.1276504 Test Loss: 0.1536589
Validation loss decreased (0.128231 --> 0.127650).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 40.47394609451294
Epoch: 17, Steps: 34 | Train Loss: 0.1543590 Vali Loss: 0.1276000 Test Loss: 0.1532345
Validation loss decreased (0.127650 --> 0.127600).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 45.52169895172119
Epoch: 18, Steps: 34 | Train Loss: 0.1539402 Vali Loss: 0.1273095 Test Loss: 0.1529178
Validation loss decreased (0.127600 --> 0.127310).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 45.10866165161133
Epoch: 19, Steps: 34 | Train Loss: 0.1536409 Vali Loss: 0.1275482 Test Loss: 0.1526584
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 40.7817268371582
Epoch: 20, Steps: 34 | Train Loss: 0.1533548 Vali Loss: 0.1274642 Test Loss: 0.1524596
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 41.58253741264343
Epoch: 21, Steps: 34 | Train Loss: 0.1531818 Vali Loss: 0.1269823 Test Loss: 0.1523008
Validation loss decreased (0.127310 --> 0.126982).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 49.37825655937195
Epoch: 22, Steps: 34 | Train Loss: 0.1530141 Vali Loss: 0.1264685 Test Loss: 0.1521702
Validation loss decreased (0.126982 --> 0.126468).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 47.517608404159546
Epoch: 23, Steps: 34 | Train Loss: 0.1528239 Vali Loss: 0.1267756 Test Loss: 0.1520513
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 48.06111168861389
Epoch: 24, Steps: 34 | Train Loss: 0.1527277 Vali Loss: 0.1268700 Test Loss: 0.1519437
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 48.881731033325195
Epoch: 25, Steps: 34 | Train Loss: 0.1525894 Vali Loss: 0.1263416 Test Loss: 0.1518676
Validation loss decreased (0.126468 --> 0.126342).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 40.84213614463806
Epoch: 26, Steps: 34 | Train Loss: 0.1525170 Vali Loss: 0.1267639 Test Loss: 0.1517844
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 43.12586045265198
Epoch: 27, Steps: 34 | Train Loss: 0.1523816 Vali Loss: 0.1265628 Test Loss: 0.1517138
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 43.90533113479614
Epoch: 28, Steps: 34 | Train Loss: 0.1523545 Vali Loss: 0.1267511 Test Loss: 0.1516496
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5069
mse:0.15095199644565582, mae:0.24740135669708252, rse:0.38629743456840515, corr:[0.46194798 0.45966578 0.46615213 0.46551958 0.46887144 0.46796235
 0.47009903 0.46929777 0.46954736 0.46936744 0.46865737 0.46862644
 0.4681139  0.4678852  0.46782523 0.46765026 0.46775594 0.467732
 0.46772692 0.4674646  0.46739313 0.467633   0.467572   0.46818346
 0.46833542 0.46884403 0.4692094  0.46895087 0.46930143 0.46877265
 0.4687752  0.4685619  0.46806002 0.46800464 0.46750277 0.46741948
 0.4672765  0.46718433 0.4671313  0.46704817 0.46706617 0.46694213
 0.4669163  0.4668212  0.46673438 0.46687984 0.46688116 0.4670558
 0.46744797 0.46745002 0.46782866 0.46757787 0.46749198 0.46742806
 0.4670059  0.46708795 0.46668714 0.46659186 0.46650514 0.466356
 0.4664127  0.46639827 0.46652412 0.4664817  0.4666047  0.4666671
 0.46665785 0.466641   0.46650118 0.46651432 0.46663573 0.46664393
 0.46689877 0.46694496 0.46689165 0.46700075 0.4665961  0.46662375
 0.46638048 0.4661498  0.4661164  0.4658386  0.46593347 0.4658231
 0.46592173 0.4659659  0.46602058 0.46611732 0.46609792 0.46625274
 0.46617898 0.46606594 0.46598518 0.46592316 0.46598658 0.4661229
 0.4661669  0.46638587 0.46623206 0.46619356 0.4660797  0.46574238
 0.46580267 0.46548492 0.46543348 0.46527025 0.46520126 0.46530542
 0.46525785 0.4655076  0.46545818 0.46561137 0.4656645  0.4656582
 0.46580514 0.4657783  0.4658209  0.46593413 0.4660111  0.4661593
 0.46628448 0.46630415 0.4663917  0.46610397 0.4661235  0.46582714
 0.4657589  0.4657473  0.46547374 0.46560913 0.4654259  0.46560514
 0.4656124  0.46576262 0.46595576 0.46593127 0.46617588 0.46612787
 0.4662104  0.46605778 0.46581963 0.46576658 0.46566314 0.4656323
 0.46570683 0.4655689  0.46563265 0.4655357  0.46527034 0.4652735
 0.46497    0.46512187 0.4649272  0.4649863  0.4650687  0.4650322
 0.46540758 0.4652867  0.46561918 0.46556363 0.46562612 0.4658434
 0.46555895 0.46548876 0.46501857 0.464804   0.4646584  0.46438864
 0.46444902 0.46427754 0.46415478 0.46410474 0.46366823 0.46372175
 0.46337026 0.46342462 0.4635047  0.46336314 0.46384498 0.46358716
 0.46410736 0.46396926 0.46372524 0.4639635  0.46265072 0.46301177
 0.461281   0.46110302 0.45997187 0.46019703 0.45816776 0.46451208]
