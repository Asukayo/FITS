Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j96_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17597
val 2537
test 5165
Model(
  (freq_upsampler): Linear(in_features=320, out_features=362, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2379816960.0
params:  116202.0
Trainable parameters:  116202
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2103621
	speed: 0.2582s/iter; left time: 3512.2656s
	iters: 200, epoch: 1 | loss: 0.1270591
	speed: 0.2744s/iter; left time: 3705.1539s
Epoch: 1 cost time: 73.44228029251099
Epoch: 1, Steps: 274 | Train Loss: 0.2616785 Vali Loss: 0.1164708 Test Loss: 0.1387775
Validation loss decreased (inf --> 0.116471).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1412593
	speed: 0.8792s/iter; left time: 11716.5525s
	iters: 200, epoch: 2 | loss: 0.1235709
	speed: 0.2690s/iter; left time: 3558.1834s
Epoch: 2 cost time: 74.12056994438171
Epoch: 2, Steps: 274 | Train Loss: 0.1381341 Vali Loss: 0.1149863 Test Loss: 0.1366932
Validation loss decreased (0.116471 --> 0.114986).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1363595
	speed: 1.0513s/iter; left time: 13722.5177s
	iters: 200, epoch: 3 | loss: 0.1187560
	speed: 0.2926s/iter; left time: 3790.0910s
Epoch: 3 cost time: 80.84934711456299
Epoch: 3, Steps: 274 | Train Loss: 0.1366657 Vali Loss: 0.1141175 Test Loss: 0.1359561
Validation loss decreased (0.114986 --> 0.114118).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1383215
	speed: 0.7681s/iter; left time: 9816.1080s
	iters: 200, epoch: 4 | loss: 0.1306354
	speed: 0.1719s/iter; left time: 2179.4670s
Epoch: 4 cost time: 49.9863817691803
Epoch: 4, Steps: 274 | Train Loss: 0.1360356 Vali Loss: 0.1136896 Test Loss: 0.1355594
Validation loss decreased (0.114118 --> 0.113690).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1353354
	speed: 0.6079s/iter; left time: 7601.4917s
	iters: 200, epoch: 5 | loss: 0.1277062
	speed: 0.1744s/iter; left time: 2163.9982s
Epoch: 5 cost time: 48.14797759056091
Epoch: 5, Steps: 274 | Train Loss: 0.1357609 Vali Loss: 0.1141380 Test Loss: 0.1356042
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1298215
	speed: 0.6401s/iter; left time: 7828.6072s
	iters: 200, epoch: 6 | loss: 0.1288027
	speed: 0.1863s/iter; left time: 2259.7955s
Epoch: 6 cost time: 51.11675667762756
Epoch: 6, Steps: 274 | Train Loss: 0.1355233 Vali Loss: 0.1137257 Test Loss: 0.1352149
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1178525
	speed: 0.6718s/iter; left time: 8032.7295s
	iters: 200, epoch: 7 | loss: 0.1258172
	speed: 0.2128s/iter; left time: 2523.6185s
Epoch: 7 cost time: 58.57758355140686
Epoch: 7, Steps: 274 | Train Loss: 0.1354003 Vali Loss: 0.1134272 Test Loss: 0.1351336
Validation loss decreased (0.113690 --> 0.113427).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1195423
	speed: 0.7219s/iter; left time: 8434.2164s
	iters: 200, epoch: 8 | loss: 0.1373908
	speed: 0.2091s/iter; left time: 2422.3415s
Epoch: 8 cost time: 57.569154262542725
Epoch: 8, Steps: 274 | Train Loss: 0.1353570 Vali Loss: 0.1136677 Test Loss: 0.1351214
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1424396
	speed: 0.7162s/iter; left time: 8171.1504s
	iters: 200, epoch: 9 | loss: 0.1307492
	speed: 0.1932s/iter; left time: 2185.4149s
Epoch: 9 cost time: 53.9279420375824
Epoch: 9, Steps: 274 | Train Loss: 0.1352435 Vali Loss: 0.1134926 Test Loss: 0.1350088
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1266377
	speed: 0.6375s/iter; left time: 7099.0994s
	iters: 200, epoch: 10 | loss: 0.1431676
	speed: 0.1799s/iter; left time: 1984.7111s
Epoch: 10 cost time: 50.13487362861633
Epoch: 10, Steps: 274 | Train Loss: 0.1351787 Vali Loss: 0.1134935 Test Loss: 0.1350175
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5165
mse:0.13500253856182098, mae:0.23222197592258453, rse:0.3651866018772125, corr:[0.4659475  0.46817377 0.47008327 0.47068903 0.47105768 0.4712244
 0.47177848 0.4713402  0.4714837  0.47104183 0.47119358 0.47070083
 0.47089443 0.47053343 0.47037402 0.47036737 0.47030804 0.4704344
 0.47056872 0.47067854 0.47054958 0.4705203  0.47055176 0.47088584
 0.47100368 0.4713087  0.47129706 0.47125632 0.47107875 0.47096324
 0.47089034 0.4706982  0.4707702  0.47059336 0.4704584  0.47040603
 0.47055805 0.4704499  0.4704228  0.4704454  0.4704818  0.4706658
 0.47041106 0.470285   0.47021705 0.47021163 0.47032753 0.47048527
 0.4703931  0.470583   0.4706026  0.47036898 0.47019866 0.47014308
 0.4701953  0.469884   0.47016895 0.47006962 0.46992812 0.46990886
 0.46974185 0.46987185 0.46963847 0.46973377 0.46964788 0.469797
 0.46993282 0.46966082 0.46958548 0.46946806 0.46940136 0.4696255
 0.4696707  0.46957037 0.46973297 0.46954638 0.4695576  0.46943858
 0.469345   0.4691978  0.46919417 0.46921968 0.46905458 0.46915603
 0.46903807 0.46916875 0.46867618 0.46896625 0.46870193 0.46861875
 0.46853414 0.46809345 0.4682818  0.4678763  0.46882275 0.46893013]
