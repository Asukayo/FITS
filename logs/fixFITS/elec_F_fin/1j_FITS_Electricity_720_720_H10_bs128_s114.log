Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16829644800.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 75.02504253387451
Epoch: 1, Steps: 66 | Train Loss: 0.9636715 Vali Loss: 0.6050131 Test Loss: 0.7210953
Validation loss decreased (inf --> 0.605013).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 66.13791370391846
Epoch: 2, Steps: 66 | Train Loss: 0.6584593 Vali Loss: 0.4809088 Test Loss: 0.5788404
Validation loss decreased (0.605013 --> 0.480909).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 51.30740237236023
Epoch: 3, Steps: 66 | Train Loss: 0.5388046 Vali Loss: 0.3962522 Test Loss: 0.4806689
Validation loss decreased (0.480909 --> 0.396252).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 51.35210299491882
Epoch: 4, Steps: 66 | Train Loss: 0.4535738 Vali Loss: 0.3354727 Test Loss: 0.4093691
Validation loss decreased (0.396252 --> 0.335473).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 55.07490682601929
Epoch: 5, Steps: 66 | Train Loss: 0.3918512 Vali Loss: 0.2910879 Test Loss: 0.3570630
Validation loss decreased (0.335473 --> 0.291088).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 64.87512254714966
Epoch: 6, Steps: 66 | Train Loss: 0.3465011 Vali Loss: 0.2591825 Test Loss: 0.3184637
Validation loss decreased (0.291088 --> 0.259183).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 67.95156955718994
Epoch: 7, Steps: 66 | Train Loss: 0.3131268 Vali Loss: 0.2356918 Test Loss: 0.2899362
Validation loss decreased (0.259183 --> 0.235692).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 79.44247174263
Epoch: 8, Steps: 66 | Train Loss: 0.2886189 Vali Loss: 0.2190862 Test Loss: 0.2687724
Validation loss decreased (0.235692 --> 0.219086).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 63.8745174407959
Epoch: 9, Steps: 66 | Train Loss: 0.2704877 Vali Loss: 0.2064073 Test Loss: 0.2530616
Validation loss decreased (0.219086 --> 0.206407).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 72.71828126907349
Epoch: 10, Steps: 66 | Train Loss: 0.2572438 Vali Loss: 0.1978193 Test Loss: 0.2414434
Validation loss decreased (0.206407 --> 0.197819).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 65.7167317867279
Epoch: 11, Steps: 66 | Train Loss: 0.2473775 Vali Loss: 0.1911566 Test Loss: 0.2327304
Validation loss decreased (0.197819 --> 0.191157).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 78.24128532409668
Epoch: 12, Steps: 66 | Train Loss: 0.2400631 Vali Loss: 0.1867728 Test Loss: 0.2263243
Validation loss decreased (0.191157 --> 0.186773).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 76.3027606010437
Epoch: 13, Steps: 66 | Train Loss: 0.2347856 Vali Loss: 0.1835216 Test Loss: 0.2215101
Validation loss decreased (0.186773 --> 0.183522).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 108.750248670578
Epoch: 14, Steps: 66 | Train Loss: 0.2307475 Vali Loss: 0.1812118 Test Loss: 0.2179014
Validation loss decreased (0.183522 --> 0.181212).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 72.06671833992004
Epoch: 15, Steps: 66 | Train Loss: 0.2277711 Vali Loss: 0.1791946 Test Loss: 0.2151836
Validation loss decreased (0.181212 --> 0.179195).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 73.04767537117004
Epoch: 16, Steps: 66 | Train Loss: 0.2256009 Vali Loss: 0.1783951 Test Loss: 0.2131389
Validation loss decreased (0.179195 --> 0.178395).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 75.59158706665039
Epoch: 17, Steps: 66 | Train Loss: 0.2239863 Vali Loss: 0.1773952 Test Loss: 0.2115933
Validation loss decreased (0.178395 --> 0.177395).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 67.86172366142273
Epoch: 18, Steps: 66 | Train Loss: 0.2227047 Vali Loss: 0.1770643 Test Loss: 0.2103718
Validation loss decreased (0.177395 --> 0.177064).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 73.78726172447205
Epoch: 19, Steps: 66 | Train Loss: 0.2218269 Vali Loss: 0.1765029 Test Loss: 0.2094664
Validation loss decreased (0.177064 --> 0.176503).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 75.97257494926453
Epoch: 20, Steps: 66 | Train Loss: 0.2211036 Vali Loss: 0.1760810 Test Loss: 0.2087794
Validation loss decreased (0.176503 --> 0.176081).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 61.93289613723755
Epoch: 21, Steps: 66 | Train Loss: 0.2205309 Vali Loss: 0.1761696 Test Loss: 0.2082249
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 65.23714900016785
Epoch: 22, Steps: 66 | Train Loss: 0.2201244 Vali Loss: 0.1762426 Test Loss: 0.2077877
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 67.29631900787354
Epoch: 23, Steps: 66 | Train Loss: 0.2197764 Vali Loss: 0.1758443 Test Loss: 0.2074471
Validation loss decreased (0.176081 --> 0.175844).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 65.57477807998657
Epoch: 24, Steps: 66 | Train Loss: 0.2195347 Vali Loss: 0.1754279 Test Loss: 0.2071653
Validation loss decreased (0.175844 --> 0.175428).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 70.94201231002808
Epoch: 25, Steps: 66 | Train Loss: 0.2192957 Vali Loss: 0.1754948 Test Loss: 0.2069778
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 68.27140760421753
Epoch: 26, Steps: 66 | Train Loss: 0.2191211 Vali Loss: 0.1759512 Test Loss: 0.2067998
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 70.48309278488159
Epoch: 27, Steps: 66 | Train Loss: 0.2189916 Vali Loss: 0.1755920 Test Loss: 0.2066457
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.20577962696552277, mae:0.2967856824398041, rse:0.45251041650772095, corr:[0.44657892 0.43531814 0.4464327  0.44608247 0.45049307 0.4495839
 0.45241463 0.45144102 0.4516032  0.45141116 0.4503756  0.4503148
 0.44982234 0.44938663 0.4497263  0.44919622 0.44972685 0.44954655
 0.4497928  0.4495941  0.44969457 0.4501364  0.44998163 0.45069176
 0.45049196 0.45126936 0.45162657 0.4513291  0.45167446 0.45112422
 0.45107847 0.45081982 0.45023608 0.45029247 0.44988292 0.44973946
 0.449836   0.44966093 0.4498222  0.44979194 0.44987434 0.4499136
 0.44993216 0.44998443 0.44981706 0.45015818 0.4501109  0.45027938
 0.45050803 0.45033237 0.45078295 0.45051855 0.45040601 0.4503658
 0.4499231  0.4499455  0.4496701  0.44952926 0.44955322 0.4493952
 0.4495061  0.4495293  0.4495515  0.4496282  0.4495838  0.44972953
 0.44964996 0.44964564 0.44967523 0.4496354  0.44980633 0.44963995
 0.44980103 0.44981977 0.44976267 0.44987315 0.44950867 0.4494974
 0.44932094 0.44903815 0.44900048 0.44882414 0.4488317  0.44880214
 0.44878742 0.4489583  0.44894812 0.4489643  0.4490381  0.44908702
 0.44914722 0.44905165 0.44910407 0.4489807  0.44899273 0.44903648
 0.44887686 0.44915473 0.4490131  0.44898906 0.44900262 0.44872442
 0.44880325 0.44858566 0.44847387 0.44846797 0.4483048  0.4483172
 0.4483205  0.4483324  0.44831663 0.44831565 0.4484008  0.4483894
 0.44844207 0.4483787  0.44826552 0.44840527 0.4483105  0.44839185
 0.4485098  0.44847596 0.44866973 0.44848904 0.4484825  0.44844252
 0.44830573 0.44829506 0.4481874  0.44822428 0.4481865  0.44813
 0.44819131 0.44823238 0.44830844 0.44835854 0.44840044 0.44847953
 0.44847032 0.44852668 0.44855037 0.44853497 0.4485293  0.4483199
 0.44837034 0.44831797 0.44835812 0.44844437 0.4482384  0.44831026
 0.4481433  0.44802916 0.44807333 0.44794905 0.44796047 0.44790557
 0.44792128 0.44800526 0.4480523  0.44805044 0.4480575  0.44813362
 0.44812536 0.4481237  0.4481335  0.4479592  0.44794738 0.44773737
 0.44737506 0.447491   0.4475396  0.4475371  0.4474525  0.4473133
 0.44734874 0.44715652 0.44707426 0.4471018  0.4469969  0.44697082
 0.44699627 0.4470128  0.44699916 0.44701868 0.44702533 0.44697338
 0.44695294 0.446723   0.4465322  0.4464377  0.44632107 0.4463103
 0.44614092 0.44624346 0.44640648 0.44628853 0.4463159  0.4461808
 0.4461429  0.44610944 0.44593608 0.4459621  0.4458966  0.44587627
 0.44593474 0.44589627 0.4459291  0.4459986  0.4460166  0.44598228
 0.44592088 0.44576702 0.44549423 0.4454262  0.44542804 0.44533333
 0.4454218  0.4454386  0.44556725 0.4456123  0.44551554 0.44559756
 0.44546527 0.44540098 0.44540977 0.44534847 0.44534153 0.44528642
 0.4452777  0.44528311 0.4452643  0.44526163 0.44527128 0.4452378
 0.44517505 0.44505572 0.44494787 0.44483015 0.4448884  0.4448297
 0.44476965 0.44494614 0.44494018 0.4450031  0.44494957 0.44488174
 0.44490257 0.44471788 0.44473055 0.44465846 0.4445332  0.44457728
 0.44456896 0.44455248 0.4445243  0.44457096 0.4446255  0.44460517
 0.44457072 0.44443923 0.44437656 0.4443154  0.44429946 0.4443754
 0.44431004 0.44442707 0.44456422 0.44452494 0.44462234 0.4445046
 0.44440803 0.44437993 0.4442438  0.44423097 0.4441733  0.44412008
 0.4441571  0.4441327  0.44411823 0.44410807 0.4440862  0.44405884
 0.44402853 0.44390666 0.44372752 0.44378227 0.44379425 0.44382074
 0.4440555  0.4440811  0.4442744  0.44433215 0.44431978 0.44442543
 0.44430026 0.44427097 0.44426107 0.44414905 0.44410086 0.44404906
 0.44406837 0.44406062 0.44403875 0.44408    0.444048   0.44403353
 0.44403118 0.44403505 0.4440698  0.44402212 0.44406617 0.44397196
 0.44389856 0.44400927 0.44402817 0.4441349  0.4440884  0.44401085
 0.44403416 0.44393027 0.4439479  0.44390002 0.44380754 0.44380614
 0.44377625 0.44380835 0.44384587 0.44384786 0.4438677  0.4438364
 0.44384086 0.4437244  0.44360313 0.4435333  0.4434167  0.4432662
 0.44292244 0.44299933 0.4432016  0.44307286 0.4430865  0.44300517
 0.442891   0.44283974 0.4427085  0.44268465 0.44256163 0.44244432
 0.4424858  0.4424672  0.44244662 0.4424599  0.4424412  0.44243157
 0.4423781  0.4422657  0.4420995  0.44200382 0.4419855  0.44189993
 0.44180492 0.4418405  0.44204226 0.44210958 0.4420342  0.4420262
 0.4419006  0.44180256 0.4417318  0.4416426  0.4415457  0.44143942
 0.44143885 0.4414371  0.4414179  0.44148657 0.44148323 0.44140375
 0.4413782  0.441266   0.4411781  0.44108355 0.44108438 0.44109103
 0.4410466  0.441161   0.44124526 0.44136286 0.44138488 0.44129202
 0.44125715 0.4411425  0.44106552 0.44107097 0.44089565 0.4407582
 0.4408387  0.4407906  0.4407537  0.44076645 0.44072378 0.4407191
 0.44063553 0.44055086 0.44048166 0.44038492 0.4404324  0.44046414
 0.44039452 0.44055864 0.440726   0.44076553 0.44080195 0.44069526
 0.44067785 0.44059822 0.44042927 0.4404003  0.44030756 0.4402576
 0.4402852  0.44028282 0.44021136 0.4402391  0.44031978 0.4402765
 0.44026983 0.44018543 0.44009182 0.44007114 0.44007915 0.44013035
 0.44018453 0.44025278 0.4403923  0.44046733 0.44049338 0.4405123
 0.44040278 0.44029891 0.44021738 0.44009817 0.43999407 0.43991518
 0.43986338 0.4398825  0.43990585 0.43983495 0.4398687  0.43984666
 0.43975252 0.43970245 0.43957642 0.4395624  0.4396435  0.43969363
 0.43995497 0.4401296  0.44025674 0.44038758 0.44034848 0.44037497
 0.44036764 0.44023338 0.44022918 0.44011578 0.43997225 0.43987906
 0.43986434 0.43984035 0.43983492 0.43990386 0.4398455  0.4398235
 0.43982273 0.43986532 0.43992305 0.4398861  0.439973   0.43995678
 0.43990797 0.4400657  0.44017822 0.44023478 0.44026116 0.4402104
 0.4402455  0.44021147 0.44011226 0.4400457  0.43993908 0.4398931
 0.43985423 0.43987963 0.43984744 0.43987837 0.43990815 0.43980655
 0.43987265 0.43975687 0.43962398 0.4395927  0.4395181  0.4394523
 0.43919766 0.43919417 0.43938768 0.4393769  0.43938377 0.4393399
 0.439174   0.4390951  0.4389748  0.43882412 0.4386438  0.4385597
 0.438563   0.438498   0.43849352 0.43847278 0.43847567 0.4384972
 0.43836206 0.43823248 0.4380712  0.43798044 0.4379667  0.43786475
 0.43785208 0.43789688 0.43805307 0.43819758 0.43812376 0.43806872
 0.43796867 0.4378127  0.43764687 0.43751615 0.4373498  0.43718952
 0.43717587 0.43711036 0.43707725 0.43710613 0.43704718 0.43704268
 0.43697193 0.43684912 0.43682444 0.43674672 0.43685028 0.43686008
 0.43677786 0.43698037 0.4370889  0.43710738 0.43705717 0.4369477
 0.43694538 0.43682316 0.4367077  0.4365493  0.4363491  0.4361738
 0.43608952 0.43606964 0.4359287  0.43591845 0.43586898 0.43569872
 0.4356605  0.43557158 0.43551958 0.43552482 0.4355991  0.4357083
 0.4356585  0.43582568 0.43603367 0.4359968  0.4360387  0.43589255
 0.43576112 0.4356841  0.43549395 0.43539155 0.4351175  0.43507904
 0.43509784 0.43499106 0.43504527 0.43500286 0.43506166 0.43501395
 0.43497977 0.43500742 0.43486357 0.43491283 0.43499675 0.4350039
 0.4351374  0.4352295  0.43539652 0.43540463 0.43531936 0.4352833
 0.43506202 0.43493396 0.43472943 0.43461546 0.43455648 0.4343439
 0.4344797  0.4344167  0.43441358 0.43456888 0.43448293 0.43458688
 0.4344804  0.43442523 0.43446785 0.43442264 0.4346336  0.43466663
 0.43490404 0.43514293 0.43514895 0.43520498 0.43515223 0.4350415
 0.4349293  0.43469387 0.434625   0.4343584  0.4342367  0.4341476
 0.43400878 0.43416286 0.43406627 0.43425727 0.4342913  0.43420506
 0.43443546 0.43436792 0.43446872 0.43452862 0.4346236  0.43473464
 0.43451956 0.4346499  0.43475825 0.43471298 0.43466    0.4343706
 0.43428916 0.43413237 0.43394744 0.43380725 0.4335332  0.43365705
 0.43355438 0.4337045  0.43382612 0.43377516 0.43412194 0.43397784
 0.43412194 0.43416896 0.43395904 0.4341389  0.43400607 0.43401635
 0.43366683 0.4333698  0.4336107  0.4332156  0.4330769  0.43279395
 0.4323422  0.43229666 0.4318524  0.43192583 0.4316313  0.43163383
 0.43197992 0.43181434 0.4321706  0.43215975 0.43227798 0.4324635
 0.43220806 0.43235427 0.4320027  0.4318882  0.43198973 0.43133938
 0.43156374 0.43099678 0.43101263 0.43076098 0.43008518 0.4303616
 0.42942855 0.42992035 0.42923686 0.42970598 0.43002033 0.42992333
 0.43117595 0.4309061  0.43140826 0.43171182 0.43043262 0.43120426
 0.42853686 0.42847934 0.42644686 0.42618397 0.41856903 0.43158507]
