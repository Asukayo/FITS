Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j96_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17597
val 2537
test 5165
Model(
  (freq_upsampler): Linear(in_features=320, out_features=362, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19038535680.0
params:  116202.0
Trainable parameters:  116202
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 36.41135931015015
Epoch: 1, Steps: 34 | Train Loss: 0.9834642 Vali Loss: 0.7630148 Test Loss: 0.8722492
Validation loss decreased (inf --> 0.763015).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 40.89033555984497
Epoch: 2, Steps: 34 | Train Loss: 0.8085093 Vali Loss: 0.6796156 Test Loss: 0.7772070
Validation loss decreased (0.763015 --> 0.679616).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 40.80201816558838
Epoch: 3, Steps: 34 | Train Loss: 0.7260087 Vali Loss: 0.6423950 Test Loss: 0.7352448
Validation loss decreased (0.679616 --> 0.642395).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 30.460290670394897
Epoch: 4, Steps: 34 | Train Loss: 0.6818771 Vali Loss: 0.6196764 Test Loss: 0.7082359
Validation loss decreased (0.642395 --> 0.619676).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 42.106231689453125
Epoch: 5, Steps: 34 | Train Loss: 0.6518496 Vali Loss: 0.6013948 Test Loss: 0.6887415
Validation loss decreased (0.619676 --> 0.601395).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 41.418506383895874
Epoch: 6, Steps: 34 | Train Loss: 0.6274554 Vali Loss: 0.5897173 Test Loss: 0.6749054
Validation loss decreased (0.601395 --> 0.589717).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 35.0103325843811
Epoch: 7, Steps: 34 | Train Loss: 0.6061231 Vali Loss: 0.5749872 Test Loss: 0.6584477
Validation loss decreased (0.589717 --> 0.574987).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 32.24299192428589
Epoch: 8, Steps: 34 | Train Loss: 0.5870251 Vali Loss: 0.5645044 Test Loss: 0.6469418
Validation loss decreased (0.574987 --> 0.564504).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 31.167784214019775
Epoch: 9, Steps: 34 | Train Loss: 0.5695516 Vali Loss: 0.5534290 Test Loss: 0.6344289
Validation loss decreased (0.564504 --> 0.553429).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 31.510572910308838
Epoch: 10, Steps: 34 | Train Loss: 0.5533769 Vali Loss: 0.5434585 Test Loss: 0.6237317
Validation loss decreased (0.553429 --> 0.543459).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 31.062779188156128
Epoch: 11, Steps: 34 | Train Loss: 0.5387178 Vali Loss: 0.5309894 Test Loss: 0.6103702
Validation loss decreased (0.543459 --> 0.530989).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 31.00233554840088
Epoch: 12, Steps: 34 | Train Loss: 0.5250108 Vali Loss: 0.5217161 Test Loss: 0.6003296
Validation loss decreased (0.530989 --> 0.521716).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 31.610738277435303
Epoch: 13, Steps: 34 | Train Loss: 0.5121132 Vali Loss: 0.5146192 Test Loss: 0.5924731
Validation loss decreased (0.521716 --> 0.514619).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 33.32517623901367
Epoch: 14, Steps: 34 | Train Loss: 0.5003740 Vali Loss: 0.5059526 Test Loss: 0.5831859
Validation loss decreased (0.514619 --> 0.505953).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 34.357810974121094
Epoch: 15, Steps: 34 | Train Loss: 0.4893060 Vali Loss: 0.4973816 Test Loss: 0.5738249
Validation loss decreased (0.505953 --> 0.497382).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 34.24876689910889
Epoch: 16, Steps: 34 | Train Loss: 0.4791247 Vali Loss: 0.4908644 Test Loss: 0.5663109
Validation loss decreased (0.497382 --> 0.490864).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 32.337873697280884
Epoch: 17, Steps: 34 | Train Loss: 0.4695262 Vali Loss: 0.4831786 Test Loss: 0.5581337
Validation loss decreased (0.490864 --> 0.483179).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 31.622271299362183
Epoch: 18, Steps: 34 | Train Loss: 0.4606166 Vali Loss: 0.4766497 Test Loss: 0.5508133
Validation loss decreased (0.483179 --> 0.476650).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 31.827754974365234
Epoch: 19, Steps: 34 | Train Loss: 0.4522227 Vali Loss: 0.4703435 Test Loss: 0.5436718
Validation loss decreased (0.476650 --> 0.470343).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 31.85344433784485
Epoch: 20, Steps: 34 | Train Loss: 0.4443106 Vali Loss: 0.4649470 Test Loss: 0.5370273
Validation loss decreased (0.470343 --> 0.464947).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 33.810835123062134
Epoch: 21, Steps: 34 | Train Loss: 0.4369802 Vali Loss: 0.4596311 Test Loss: 0.5319386
Validation loss decreased (0.464947 --> 0.459631).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 34.790300130844116
Epoch: 22, Steps: 34 | Train Loss: 0.4301455 Vali Loss: 0.4547371 Test Loss: 0.5267829
Validation loss decreased (0.459631 --> 0.454737).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 33.8475775718689
Epoch: 23, Steps: 34 | Train Loss: 0.4236274 Vali Loss: 0.4491033 Test Loss: 0.5207325
Validation loss decreased (0.454737 --> 0.449103).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 33.386019468307495
Epoch: 24, Steps: 34 | Train Loss: 0.4175974 Vali Loss: 0.4444908 Test Loss: 0.5156028
Validation loss decreased (0.449103 --> 0.444491).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 32.7318971157074
Epoch: 25, Steps: 34 | Train Loss: 0.4118030 Vali Loss: 0.4401828 Test Loss: 0.5112818
Validation loss decreased (0.444491 --> 0.440183).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 31.466383695602417
Epoch: 26, Steps: 34 | Train Loss: 0.4064550 Vali Loss: 0.4362556 Test Loss: 0.5068038
Validation loss decreased (0.440183 --> 0.436256).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 31.937085151672363
Epoch: 27, Steps: 34 | Train Loss: 0.4013268 Vali Loss: 0.4329530 Test Loss: 0.5029399
Validation loss decreased (0.436256 --> 0.432953).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 33.961933851242065
Epoch: 28, Steps: 34 | Train Loss: 0.3966164 Vali Loss: 0.4285424 Test Loss: 0.4983713
Validation loss decreased (0.432953 --> 0.428542).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 32.01042819023132
Epoch: 29, Steps: 34 | Train Loss: 0.3921083 Vali Loss: 0.4256651 Test Loss: 0.4947563
Validation loss decreased (0.428542 --> 0.425665).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 32.84618020057678
Epoch: 30, Steps: 34 | Train Loss: 0.3878783 Vali Loss: 0.4215394 Test Loss: 0.4907381
Validation loss decreased (0.425665 --> 0.421539).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 32.256951332092285
Epoch: 31, Steps: 34 | Train Loss: 0.3838046 Vali Loss: 0.4190677 Test Loss: 0.4881525
Validation loss decreased (0.421539 --> 0.419068).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 33.283103466033936
Epoch: 32, Steps: 34 | Train Loss: 0.3800681 Vali Loss: 0.4164830 Test Loss: 0.4847165
Validation loss decreased (0.419068 --> 0.416483).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 33.06167697906494
Epoch: 33, Steps: 34 | Train Loss: 0.3764806 Vali Loss: 0.4130343 Test Loss: 0.4815716
Validation loss decreased (0.416483 --> 0.413034).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 36.047417879104614
Epoch: 34, Steps: 34 | Train Loss: 0.3730592 Vali Loss: 0.4109835 Test Loss: 0.4784183
Validation loss decreased (0.413034 --> 0.410983).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 34.96398162841797
Epoch: 35, Steps: 34 | Train Loss: 0.3699248 Vali Loss: 0.4088572 Test Loss: 0.4759744
Validation loss decreased (0.410983 --> 0.408857).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 33.786917209625244
Epoch: 36, Steps: 34 | Train Loss: 0.3669991 Vali Loss: 0.4064443 Test Loss: 0.4735486
Validation loss decreased (0.408857 --> 0.406444).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 37.45983028411865
Epoch: 37, Steps: 34 | Train Loss: 0.3641263 Vali Loss: 0.4036257 Test Loss: 0.4711128
Validation loss decreased (0.406444 --> 0.403626).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 40.6648313999176
Epoch: 38, Steps: 34 | Train Loss: 0.3613306 Vali Loss: 0.4011352 Test Loss: 0.4684612
Validation loss decreased (0.403626 --> 0.401135).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 37.04272413253784
Epoch: 39, Steps: 34 | Train Loss: 0.3588123 Vali Loss: 0.3990865 Test Loss: 0.4664149
Validation loss decreased (0.401135 --> 0.399087).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 33.82226824760437
Epoch: 40, Steps: 34 | Train Loss: 0.3563521 Vali Loss: 0.3972090 Test Loss: 0.4641867
Validation loss decreased (0.399087 --> 0.397209).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 35.22553086280823
Epoch: 41, Steps: 34 | Train Loss: 0.3540681 Vali Loss: 0.3961936 Test Loss: 0.4623180
Validation loss decreased (0.397209 --> 0.396194).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 40.45385384559631
Epoch: 42, Steps: 34 | Train Loss: 0.3519251 Vali Loss: 0.3941332 Test Loss: 0.4608284
Validation loss decreased (0.396194 --> 0.394133).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 39.30984091758728
Epoch: 43, Steps: 34 | Train Loss: 0.3498363 Vali Loss: 0.3920396 Test Loss: 0.4584490
Validation loss decreased (0.394133 --> 0.392040).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 41.36457633972168
Epoch: 44, Steps: 34 | Train Loss: 0.3478786 Vali Loss: 0.3914109 Test Loss: 0.4569579
Validation loss decreased (0.392040 --> 0.391411).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 41.773967027664185
Epoch: 45, Steps: 34 | Train Loss: 0.3460465 Vali Loss: 0.3895532 Test Loss: 0.4554944
Validation loss decreased (0.391411 --> 0.389553).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 44.2789831161499
Epoch: 46, Steps: 34 | Train Loss: 0.3442854 Vali Loss: 0.3880812 Test Loss: 0.4540275
Validation loss decreased (0.389553 --> 0.388081).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 43.01084899902344
Epoch: 47, Steps: 34 | Train Loss: 0.3425729 Vali Loss: 0.3868542 Test Loss: 0.4524393
Validation loss decreased (0.388081 --> 0.386854).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 43.62628173828125
Epoch: 48, Steps: 34 | Train Loss: 0.3410469 Vali Loss: 0.3864156 Test Loss: 0.4512154
Validation loss decreased (0.386854 --> 0.386416).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 42.7211856842041
Epoch: 49, Steps: 34 | Train Loss: 0.3395774 Vali Loss: 0.3845371 Test Loss: 0.4495836
Validation loss decreased (0.386416 --> 0.384537).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 35.62679386138916
Epoch: 50, Steps: 34 | Train Loss: 0.3381784 Vali Loss: 0.3836224 Test Loss: 0.4486637
Validation loss decreased (0.384537 --> 0.383622).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 17597
val 2537
test 5165
Model(
  (freq_upsampler): Linear(in_features=320, out_features=362, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19038535680.0
params:  116202.0
Trainable parameters:  116202
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 40.08883619308472
Epoch: 1, Steps: 34 | Train Loss: 0.3284095 Vali Loss: 0.1959586 Test Loss: 0.2364508
Validation loss decreased (inf --> 0.195959).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 48.87751293182373
Epoch: 2, Steps: 34 | Train Loss: 0.1917013 Vali Loss: 0.1329638 Test Loss: 0.1632589
Validation loss decreased (0.195959 --> 0.132964).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 47.21540665626526
Epoch: 3, Steps: 34 | Train Loss: 0.1494432 Vali Loss: 0.1180531 Test Loss: 0.1445335
Validation loss decreased (0.132964 --> 0.118053).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 50.848531007766724
Epoch: 4, Steps: 34 | Train Loss: 0.1398289 Vali Loss: 0.1158967 Test Loss: 0.1405585
Validation loss decreased (0.118053 --> 0.115897).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 46.591593742370605
Epoch: 5, Steps: 34 | Train Loss: 0.1379557 Vali Loss: 0.1145314 Test Loss: 0.1395385
Validation loss decreased (0.115897 --> 0.114531).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 46.909491539001465
Epoch: 6, Steps: 34 | Train Loss: 0.1372420 Vali Loss: 0.1144881 Test Loss: 0.1390749
Validation loss decreased (0.114531 --> 0.114488).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 49.88589906692505
Epoch: 7, Steps: 34 | Train Loss: 0.1367255 Vali Loss: 0.1147444 Test Loss: 0.1388372
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 47.77041149139404
Epoch: 8, Steps: 34 | Train Loss: 0.1363774 Vali Loss: 0.1145384 Test Loss: 0.1386704
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 46.63608694076538
Epoch: 9, Steps: 34 | Train Loss: 0.1362279 Vali Loss: 0.1140473 Test Loss: 0.1385617
Validation loss decreased (0.114488 --> 0.114047).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 42.85653114318848
Epoch: 10, Steps: 34 | Train Loss: 0.1360935 Vali Loss: 0.1138873 Test Loss: 0.1383868
Validation loss decreased (0.114047 --> 0.113887).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 42.00546312332153
Epoch: 11, Steps: 34 | Train Loss: 0.1359502 Vali Loss: 0.1136946 Test Loss: 0.1383127
Validation loss decreased (0.113887 --> 0.113695).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 49.122822284698486
Epoch: 12, Steps: 34 | Train Loss: 0.1358279 Vali Loss: 0.1141975 Test Loss: 0.1382325
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 49.58732628822327
Epoch: 13, Steps: 34 | Train Loss: 0.1357671 Vali Loss: 0.1133484 Test Loss: 0.1382043
Validation loss decreased (0.113695 --> 0.113348).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 47.776848554611206
Epoch: 14, Steps: 34 | Train Loss: 0.1356365 Vali Loss: 0.1135185 Test Loss: 0.1380916
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 50.60492420196533
Epoch: 15, Steps: 34 | Train Loss: 0.1356233 Vali Loss: 0.1136109 Test Loss: 0.1380800
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 49.465457916259766
Epoch: 16, Steps: 34 | Train Loss: 0.1355782 Vali Loss: 0.1139177 Test Loss: 0.1380249
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5165
mse:0.1353611946105957, mae:0.23276320099830627, rse:0.3656713664531708, corr:[0.46537527 0.46757215 0.4701098  0.47053835 0.47202903 0.4716406
 0.47270435 0.47196886 0.4722387  0.47173867 0.4715278  0.47125682
 0.47115797 0.4709589  0.47075716 0.47077906 0.4707568  0.47074848
 0.47074494 0.47077683 0.47073534 0.4708578  0.47088164 0.47123775
 0.4713162  0.47170034 0.47171107 0.47172323 0.47179168 0.47132713
 0.47132397 0.4709015  0.47065726 0.47053337 0.47030416 0.47038168
 0.47036505 0.47029886 0.47034755 0.47038382 0.47035435 0.47044808
 0.47043344 0.47041947 0.47022384 0.47025156 0.47025722 0.47056773
 0.47073126 0.47075114 0.47094107 0.4706057  0.47054726 0.4702811
 0.4701221  0.47006068 0.46972778 0.46986148 0.46979168 0.46969646
 0.46976843 0.4696984  0.4698545  0.4700257  0.46985656 0.46998468
 0.46991238 0.4697062  0.4695424  0.4694936  0.46951792 0.4693665
 0.46954617 0.46942768 0.46940723 0.4692799  0.46908373 0.46909577
 0.46873438 0.4688433  0.4688032  0.46863708 0.46891925 0.46888283
 0.46898645 0.46907106 0.4686769  0.46903735 0.4684926  0.46843958
 0.46812347 0.46766475 0.46781108 0.46750534 0.46756992 0.46949255]
