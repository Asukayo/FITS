Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4207411200.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6492628
	speed: 0.3353s/iter; left time: 4409.0474s
	iters: 200, epoch: 1 | loss: 0.4824705
	speed: 0.2824s/iter; left time: 3685.5652s
Epoch: 1 cost time: 79.96021938323975
Epoch: 1, Steps: 265 | Train Loss: 0.6544610 Vali Loss: 0.3273172 Test Loss: 0.3993095
Validation loss decreased (inf --> 0.327317).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3283523
	speed: 1.0134s/iter; left time: 13059.0473s
	iters: 200, epoch: 2 | loss: 0.2883842
	speed: 0.2760s/iter; left time: 3528.3288s
Epoch: 2 cost time: 74.22850394248962
Epoch: 2, Steps: 265 | Train Loss: 0.3193470 Vali Loss: 0.2043477 Test Loss: 0.2486711
Validation loss decreased (0.327317 --> 0.204348).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2398324
	speed: 0.9040s/iter; left time: 11408.8271s
	iters: 200, epoch: 3 | loss: 0.2474757
	speed: 0.2653s/iter; left time: 3322.1599s
Epoch: 3 cost time: 73.4470067024231
Epoch: 3, Steps: 265 | Train Loss: 0.2390183 Vali Loss: 0.1794095 Test Loss: 0.2132956
Validation loss decreased (0.204348 --> 0.179409).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2135326
	speed: 1.0658s/iter; left time: 13169.3247s
	iters: 200, epoch: 4 | loss: 0.2219110
	speed: 0.3119s/iter; left time: 3822.6825s
Epoch: 4 cost time: 85.26612901687622
Epoch: 4, Steps: 265 | Train Loss: 0.2224145 Vali Loss: 0.1764146 Test Loss: 0.2065580
Validation loss decreased (0.179409 --> 0.176415).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2351313
	speed: 1.0401s/iter; left time: 12576.4151s
	iters: 200, epoch: 5 | loss: 0.2139881
	speed: 0.2889s/iter; left time: 3464.2775s
Epoch: 5 cost time: 78.79833579063416
Epoch: 5, Steps: 265 | Train Loss: 0.2194742 Vali Loss: 0.1759223 Test Loss: 0.2052229
Validation loss decreased (0.176415 --> 0.175922).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2403245
	speed: 1.0315s/iter; left time: 12198.7857s
	iters: 200, epoch: 6 | loss: 0.2288739
	speed: 0.2999s/iter; left time: 3516.2423s
Epoch: 6 cost time: 81.68437242507935
Epoch: 6, Steps: 265 | Train Loss: 0.2188068 Vali Loss: 0.1757381 Test Loss: 0.2047935
Validation loss decreased (0.175922 --> 0.175738).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2178432
	speed: 1.0301s/iter; left time: 11908.6314s
	iters: 200, epoch: 7 | loss: 0.2305728
	speed: 0.2900s/iter; left time: 3323.7733s
Epoch: 7 cost time: 79.87348222732544
Epoch: 7, Steps: 265 | Train Loss: 0.2184432 Vali Loss: 0.1756195 Test Loss: 0.2045828
Validation loss decreased (0.175738 --> 0.175619).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2262556
	speed: 1.0911s/iter; left time: 12324.9709s
	iters: 200, epoch: 8 | loss: 0.2304334
	speed: 0.3351s/iter; left time: 3751.5873s
Epoch: 8 cost time: 87.52313876152039
Epoch: 8, Steps: 265 | Train Loss: 0.2183174 Vali Loss: 0.1757266 Test Loss: 0.2045024
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2150709
	speed: 1.1621s/iter; left time: 12819.5444s
	iters: 200, epoch: 9 | loss: 0.2005155
	speed: 0.3130s/iter; left time: 3421.0822s
Epoch: 9 cost time: 85.8513731956482
Epoch: 9, Steps: 265 | Train Loss: 0.2180946 Vali Loss: 0.1753021 Test Loss: 0.2045182
Validation loss decreased (0.175619 --> 0.175302).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2365803
	speed: 1.0179s/iter; left time: 10958.9631s
	iters: 200, epoch: 10 | loss: 0.2133600
	speed: 0.2900s/iter; left time: 3092.8205s
Epoch: 10 cost time: 78.65315127372742
Epoch: 10, Steps: 265 | Train Loss: 0.2180818 Vali Loss: 0.1754043 Test Loss: 0.2044421
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2354835
	speed: 1.0358s/iter; left time: 10876.6538s
	iters: 200, epoch: 11 | loss: 0.2065685
	speed: 0.2850s/iter; left time: 2964.2239s
Epoch: 11 cost time: 78.87868452072144
Epoch: 11, Steps: 265 | Train Loss: 0.2179721 Vali Loss: 0.1752767 Test Loss: 0.2043782
Validation loss decreased (0.175302 --> 0.175277).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2154496
	speed: 1.0004s/iter; left time: 10239.6139s
	iters: 200, epoch: 12 | loss: 0.2166985
	speed: 0.2821s/iter; left time: 2859.0973s
Epoch: 12 cost time: 76.97287607192993
Epoch: 12, Steps: 265 | Train Loss: 0.2178920 Vali Loss: 0.1752079 Test Loss: 0.2042938
Validation loss decreased (0.175277 --> 0.175208).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2286452
	speed: 1.0949s/iter; left time: 10917.3279s
	iters: 200, epoch: 13 | loss: 0.2205158
	speed: 0.3110s/iter; left time: 3069.9903s
Epoch: 13 cost time: 89.02754092216492
Epoch: 13, Steps: 265 | Train Loss: 0.2178486 Vali Loss: 0.1752269 Test Loss: 0.2043020
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2121662
	speed: 1.1958s/iter; left time: 11606.3619s
	iters: 200, epoch: 14 | loss: 0.2190779
	speed: 0.3213s/iter; left time: 3086.8799s
Epoch: 14 cost time: 88.95487070083618
Epoch: 14, Steps: 265 | Train Loss: 0.2177881 Vali Loss: 0.1750991 Test Loss: 0.2042511
Validation loss decreased (0.175208 --> 0.175099).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1981242
	speed: 1.1532s/iter; left time: 10887.5976s
	iters: 200, epoch: 15 | loss: 0.2068583
	speed: 0.4030s/iter; left time: 3763.9833s
Epoch: 15 cost time: 105.57447934150696
Epoch: 15, Steps: 265 | Train Loss: 0.2176780 Vali Loss: 0.1751617 Test Loss: 0.2042027
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2242808
	speed: 1.1103s/iter; left time: 10188.1123s
	iters: 200, epoch: 16 | loss: 0.2180915
	speed: 0.2527s/iter; left time: 2293.2197s
Epoch: 16 cost time: 68.91037082672119
Epoch: 16, Steps: 265 | Train Loss: 0.2176942 Vali Loss: 0.1752453 Test Loss: 0.2041651
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2059895
	speed: 0.8117s/iter; left time: 7233.3052s
	iters: 200, epoch: 17 | loss: 0.2146166
	speed: 0.2283s/iter; left time: 2011.7450s
Epoch: 17 cost time: 62.085365533828735
Epoch: 17, Steps: 265 | Train Loss: 0.2176400 Vali Loss: 0.1749906 Test Loss: 0.2041663
Validation loss decreased (0.175099 --> 0.174991).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2237756
	speed: 0.8253s/iter; left time: 7135.4221s
	iters: 200, epoch: 18 | loss: 0.2373205
	speed: 0.3640s/iter; left time: 3111.0144s
Epoch: 18 cost time: 90.5075900554657
Epoch: 18, Steps: 265 | Train Loss: 0.2176089 Vali Loss: 0.1748739 Test Loss: 0.2041431
Validation loss decreased (0.174991 --> 0.174874).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2189617
	speed: 1.3440s/iter; left time: 11264.0478s
	iters: 200, epoch: 19 | loss: 0.2179203
	speed: 0.3659s/iter; left time: 3029.6753s
Epoch: 19 cost time: 98.84866213798523
Epoch: 19, Steps: 265 | Train Loss: 0.2175727 Vali Loss: 0.1750818 Test Loss: 0.2041856
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2171488
	speed: 1.1757s/iter; left time: 9541.7511s
	iters: 200, epoch: 20 | loss: 0.2368678
	speed: 0.3494s/iter; left time: 2800.6923s
Epoch: 20 cost time: 91.75229334831238
Epoch: 20, Steps: 265 | Train Loss: 0.2175706 Vali Loss: 0.1749544 Test Loss: 0.2041605
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1992792
	speed: 1.2128s/iter; left time: 9522.0594s
	iters: 200, epoch: 21 | loss: 0.2306061
	speed: 0.3864s/iter; left time: 2994.6351s
Epoch: 21 cost time: 102.05255246162415
Epoch: 21, Steps: 265 | Train Loss: 0.2175215 Vali Loss: 0.1749113 Test Loss: 0.2041882
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.2040667086839676, mae:0.293356329202652, rse:0.45062312483787537, corr:[0.44744894 0.4483261  0.44891846 0.45082146 0.45072544 0.4516757
 0.45171028 0.4514714  0.45145693 0.45102304 0.45073485 0.45044273
 0.45013347 0.4502122  0.45002615 0.45013872 0.45009264 0.4501907
 0.45012867 0.44999906 0.45020866 0.45021266 0.45057327 0.45062438
 0.45092005 0.45135882 0.45133922 0.4513412  0.4512092  0.45115817
 0.45095858 0.4506972  0.4505241  0.45035595 0.45018008 0.45006853
 0.45004824 0.45014444 0.45014182 0.45011494 0.45014507 0.45021206
 0.45024863 0.45016322 0.45019794 0.45015544 0.45024696 0.4503212
 0.45037848 0.45061547 0.4506028  0.45050457 0.45036772 0.4503171
 0.45028645 0.45000786 0.4499161  0.4500098  0.4499588  0.44986188
 0.4498261  0.44990057 0.4499039  0.44980723 0.44986814 0.4498413
 0.44983795 0.44976842 0.44967827 0.44974315 0.44972587 0.44972488
 0.449644   0.44976166 0.44972694 0.44952315 0.44955656 0.4494543
 0.4493492  0.4492507  0.44916752 0.4491122  0.4491297  0.44912037
 0.4490208  0.4490838  0.44910255 0.44912666 0.44915596 0.44908512
 0.449192   0.44920886 0.4489673  0.44900206 0.4490812  0.449026
 0.44905248 0.44913208 0.44918334 0.4489395  0.44885355 0.4488959
 0.4488988  0.4489424  0.44873896 0.44877833 0.44883868 0.44872573
 0.44872555 0.4487073  0.44873968 0.44873717 0.44863868 0.44868347
 0.44875193 0.44867224 0.44853145 0.44849834 0.44852206 0.44854894
 0.44875363 0.44878447 0.44885194 0.44883102 0.44872883 0.4487292
 0.44861576 0.44852746 0.44843784 0.4483463  0.4484598  0.44843504
 0.44847667 0.44858947 0.4484729  0.44848594 0.44851473 0.448534
 0.4485947  0.44857624 0.44862986 0.44852126 0.4485345  0.4486165
 0.44851992 0.44854647 0.4484714  0.44853485 0.4485869  0.44847614
 0.44841534 0.44823295 0.44825098 0.44837394 0.44831318 0.44821802
 0.4483567  0.4485376  0.44838348 0.44832098 0.44849566 0.44860312
 0.44868028 0.44868758 0.4486267  0.44843414 0.44832438 0.44826347
 0.44794083 0.4479094  0.4480964  0.4479741  0.44791493 0.44777834
 0.44760343 0.44752926 0.44744402 0.44741848 0.44740078 0.44744825
 0.4474945  0.44753024 0.44747794 0.44741473 0.447473   0.4474346
 0.44743186 0.44732562 0.44697776 0.4468877  0.44693694 0.4468766
 0.446867   0.4468226  0.44685453 0.4469516  0.4469304  0.44684362
 0.44662    0.44654316 0.44649112 0.44630206 0.44620672 0.44617924
 0.44622332 0.44616753 0.44618395 0.44629252 0.44622618 0.4462872
 0.44629988 0.44609717 0.44607428 0.44603258 0.44586897 0.44579214
 0.44591364 0.4460952  0.4460595  0.4460529  0.44609058 0.4460642
 0.44600168 0.4458244  0.44583192 0.44581336 0.44575372 0.44563594
 0.44541678 0.44555846 0.44561172 0.4455583  0.445585   0.44550487
 0.4455899  0.44540694 0.44530165 0.44529042 0.44514444 0.44521824
 0.44518313 0.44531924 0.44547752 0.4453248  0.4454106  0.44546068
 0.44538122 0.4452316  0.4450861  0.4451763  0.4451192  0.4450084
 0.44508716 0.4450995  0.4450505  0.4450149  0.44499797 0.44501457
 0.44502258 0.44494292 0.4447797  0.44468176 0.44472915 0.444718
 0.44475174 0.44492462 0.44503558 0.44505316 0.44508797 0.44514096
 0.44500744 0.44495994 0.44490734 0.44469577 0.44465122 0.44462138
 0.44464344 0.44462633 0.44452724 0.44450518 0.44447237 0.44451162
 0.4445266  0.44452548 0.44449437 0.44441926 0.44441447 0.444394
 0.44463742 0.44478104 0.44475904 0.44483343 0.4449371  0.4449304
 0.44488907 0.44476733 0.44467312 0.4446452  0.44464156 0.44456476
 0.4443711  0.44437343 0.4444255  0.44441333 0.44433042 0.44426036
 0.4443887  0.44448295 0.4445304  0.4444394  0.44437018 0.44438648
 0.44436863 0.4445172  0.44456875 0.4444804  0.44458267 0.44462058
 0.44454527 0.44463497 0.44465283 0.44448724 0.44434443 0.4442526
 0.44426817 0.44434592 0.44435367 0.44422725 0.44414535 0.44421026
 0.44429573 0.44420996 0.44399208 0.4439619  0.44393456 0.4436589
 0.44341627 0.44350344 0.44368723 0.4437496  0.44384795 0.4437698
 0.44350982 0.44346488 0.44337535 0.44318378 0.44296327 0.4428214
 0.44294623 0.44298244 0.44287762 0.4428507  0.44274563 0.44272783
 0.44272408 0.4426083  0.44254392 0.4424548  0.4425139  0.44241646
 0.44228134 0.44247192 0.44238266 0.44241375 0.44257835 0.44251132
 0.44241846 0.44228274 0.44211996 0.44202387 0.44186515 0.44175014
 0.44185442 0.44179225 0.44164568 0.44165134 0.44156963 0.44150096
 0.44156566 0.44151312 0.44144607 0.44140556 0.44144237 0.4415313
 0.44153845 0.44161472 0.44166803 0.44169447 0.44174904 0.44174883
 0.44174263 0.44161457 0.44154072 0.44156334 0.44147137 0.4414036
 0.44129223 0.44127536 0.4412286  0.44105366 0.44101092 0.44096723
 0.44103557 0.44105086 0.44086874 0.44085073 0.44092786 0.44086722
 0.4408572  0.44103035 0.4412105  0.4413474  0.44142166 0.44122458
 0.4411149  0.44117367 0.44100952 0.44106898 0.44101927 0.4407954
 0.44092947 0.44092083 0.44084606 0.44064337 0.4405703  0.4407574
 0.44063112 0.44060054 0.44051504 0.44039187 0.44055775 0.44043583
 0.44052634 0.44090003 0.4409982  0.4410744  0.44108227 0.44103718
 0.44101778 0.44093817 0.44082943 0.44053748 0.44045216 0.44043243
 0.4403621  0.44049612 0.440369   0.44022623 0.4401642  0.4401134
 0.44028014 0.4402029  0.4400553  0.44006437 0.44009757 0.44014972
 0.44039038 0.4406492  0.44079646 0.44075692 0.44085833 0.4410184
 0.44092998 0.44078895 0.4406478  0.440579   0.44055957 0.44042468
 0.44032368 0.44039452 0.44042033 0.4403608  0.44035846 0.4402055
 0.44030175 0.44055933 0.4404495  0.44037986 0.44046578 0.44037893
 0.44028026 0.44047794 0.44059244 0.4405882  0.44067797 0.44077155
 0.44081604 0.44083712 0.44076264 0.440648   0.44059244 0.44046444
 0.44045657 0.44052702 0.44041854 0.4403606  0.44038355 0.44040215
 0.44047868 0.44035482 0.44003522 0.43998596 0.43999118 0.43973547
 0.43965274 0.4396969  0.439792   0.43998206 0.43999216 0.43992862
 0.4398549  0.439795   0.43965915 0.43948016 0.43931007 0.43906364
 0.43912628 0.4392251  0.43899572 0.4389816  0.43891975 0.43877104
 0.4387961  0.43862852 0.4385091  0.43841165 0.43832758 0.4383743
 0.43829378 0.43841007 0.43854177 0.4386185  0.4386977  0.43852997
 0.4384875  0.43846744 0.4381538  0.43791908 0.43778956 0.4376591
 0.43762472 0.43764904 0.43752733 0.43742096 0.43739    0.43727624
 0.43721437 0.43716294 0.437132   0.4371063  0.43719304 0.43724504
 0.43720353 0.43740538 0.43747425 0.43737957 0.4375763  0.43756574
 0.4374143  0.43746546 0.4372737  0.43708578 0.43702796 0.43685836
 0.43677333 0.43674254 0.43661383 0.43640655 0.43629664 0.43603995
 0.43582228 0.4358486  0.43576163 0.4357465  0.4358435  0.4358991
 0.43597266 0.43603927 0.43619075 0.43639678 0.4364045  0.4363184
 0.43635398 0.4362499  0.43607792 0.43598497 0.4358149  0.43567812
 0.43571907 0.43571246 0.4355308  0.43538338 0.43538848 0.43542853
 0.43531132 0.43526465 0.4353039  0.4352705  0.43543756 0.4354631
 0.43544883 0.4355581  0.4357208  0.43591827 0.43591368 0.43577006
 0.43563837 0.4354875  0.4353804  0.435275   0.4350341  0.4348513
 0.43485972 0.43484995 0.43471062 0.4345714  0.43442634 0.43449068
 0.43462107 0.4345118  0.434528   0.43457755 0.43474615 0.4349062
 0.4350517  0.43527836 0.43537337 0.43542325 0.4354928  0.4353793
 0.43517876 0.4350537  0.4348926  0.43462837 0.43448925 0.43444553
 0.4344083  0.43444374 0.434425   0.43440753 0.43463004 0.43442026
 0.43439013 0.434759   0.43465453 0.43466327 0.434743   0.43477413
 0.43485194 0.43498197 0.43508843 0.43502507 0.4350326  0.43494797
 0.4348203  0.43473136 0.4345123  0.43432143 0.43425342 0.4340813
 0.43398085 0.43409    0.43418038 0.4341524  0.43415132 0.4342417
 0.434239   0.43434    0.4342056  0.4341347  0.4343085  0.4341069
 0.4339173  0.43377584 0.43374148 0.43364108 0.43351206 0.43363366
 0.43331587 0.43317774 0.43292278 0.43256453 0.4325119  0.43228942
 0.4323873  0.4325175  0.43250018 0.4327475  0.43272474 0.4327212
 0.43280327 0.4325462  0.4325471  0.43246472 0.43252075 0.4323817
 0.4321931  0.43229407 0.43198675 0.43197584 0.431639   0.43156597
 0.43125242 0.43102142 0.43114707 0.43077102 0.43110466 0.4309802
 0.43139532 0.4318658  0.43193    0.43241176 0.4322074  0.43205988
 0.43204784 0.43076092 0.42706767 0.4298429  0.42622814 0.42701393]
