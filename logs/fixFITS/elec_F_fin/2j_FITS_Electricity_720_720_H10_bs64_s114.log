Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8414822400.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7978461
	speed: 0.4913s/iter; left time: 3194.1030s
Epoch: 1 cost time: 65.58736252784729
Epoch: 1, Steps: 132 | Train Loss: 0.9481212 Vali Loss: 0.6874281 Test Loss: 0.8092502
Validation loss decreased (inf --> 0.687428).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6590961
	speed: 1.0795s/iter; left time: 6875.1286s
Epoch: 2 cost time: 67.08106470108032
Epoch: 2, Steps: 132 | Train Loss: 0.6952132 Vali Loss: 0.6072754 Test Loss: 0.7187949
Validation loss decreased (0.687428 --> 0.607275).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5913050
	speed: 1.1126s/iter; left time: 6939.2056s
Epoch: 3 cost time: 66.95636892318726
Epoch: 3, Steps: 132 | Train Loss: 0.6069724 Vali Loss: 0.5470710 Test Loss: 0.6506842
Validation loss decreased (0.607275 --> 0.547071).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5114778
	speed: 1.0890s/iter; left time: 6648.2831s
Epoch: 4 cost time: 65.73382878303528
Epoch: 4, Steps: 132 | Train Loss: 0.5365404 Vali Loss: 0.4953300 Test Loss: 0.5920591
Validation loss decreased (0.547071 --> 0.495330).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4652142
	speed: 1.1384s/iter; left time: 6799.3739s
Epoch: 5 cost time: 71.5136125087738
Epoch: 5, Steps: 132 | Train Loss: 0.4783011 Vali Loss: 0.4525211 Test Loss: 0.5432646
Validation loss decreased (0.495330 --> 0.452521).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4166102
	speed: 1.1451s/iter; left time: 6688.7419s
Epoch: 6 cost time: 67.85386562347412
Epoch: 6, Steps: 132 | Train Loss: 0.4294711 Vali Loss: 0.4157009 Test Loss: 0.5009783
Validation loss decreased (0.452521 --> 0.415701).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3802603
	speed: 1.0932s/iter; left time: 6241.0043s
Epoch: 7 cost time: 65.53376889228821
Epoch: 7, Steps: 132 | Train Loss: 0.3882400 Vali Loss: 0.3843556 Test Loss: 0.4648145
Validation loss decreased (0.415701 --> 0.384356).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3425629
	speed: 1.0911s/iter; left time: 6085.3428s
Epoch: 8 cost time: 64.30764532089233
Epoch: 8, Steps: 132 | Train Loss: 0.3529737 Vali Loss: 0.3568584 Test Loss: 0.4332637
Validation loss decreased (0.384356 --> 0.356858).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3114154
	speed: 1.0592s/iter; left time: 5767.2509s
Epoch: 9 cost time: 63.703805685043335
Epoch: 9, Steps: 132 | Train Loss: 0.3228324 Vali Loss: 0.3343502 Test Loss: 0.4069546
Validation loss decreased (0.356858 --> 0.334350).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2864716
	speed: 1.0860s/iter; left time: 5769.8073s
Epoch: 10 cost time: 66.3272476196289
Epoch: 10, Steps: 132 | Train Loss: 0.2969121 Vali Loss: 0.3141672 Test Loss: 0.3838427
Validation loss decreased (0.334350 --> 0.314167).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2677057
	speed: 1.1278s/iter; left time: 5843.0336s
Epoch: 11 cost time: 69.56744337081909
Epoch: 11, Steps: 132 | Train Loss: 0.2745721 Vali Loss: 0.2961442 Test Loss: 0.3621133
Validation loss decreased (0.314167 --> 0.296144).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2439407
	speed: 1.1160s/iter; left time: 5634.7875s
Epoch: 12 cost time: 63.572627544403076
Epoch: 12, Steps: 132 | Train Loss: 0.2550606 Vali Loss: 0.2809271 Test Loss: 0.3444587
Validation loss decreased (0.296144 --> 0.280927).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2342402
	speed: 1.0666s/iter; left time: 5244.3050s
Epoch: 13 cost time: 67.08877086639404
Epoch: 13, Steps: 132 | Train Loss: 0.2382095 Vali Loss: 0.2676837 Test Loss: 0.3285299
Validation loss decreased (0.280927 --> 0.267684).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2156464
	speed: 1.0686s/iter; left time: 5113.2571s
Epoch: 14 cost time: 65.68610262870789
Epoch: 14, Steps: 132 | Train Loss: 0.2235027 Vali Loss: 0.2567654 Test Loss: 0.3154704
Validation loss decreased (0.267684 --> 0.256765).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2091759
	speed: 1.0775s/iter; left time: 5013.4550s
Epoch: 15 cost time: 65.0912709236145
Epoch: 15, Steps: 132 | Train Loss: 0.2106764 Vali Loss: 0.2465999 Test Loss: 0.3031912
Validation loss decreased (0.256765 --> 0.246600).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2055451
	speed: 1.0978s/iter; left time: 4963.1814s
Epoch: 16 cost time: 68.29733967781067
Epoch: 16, Steps: 132 | Train Loss: 0.1993953 Vali Loss: 0.2379185 Test Loss: 0.2927706
Validation loss decreased (0.246600 --> 0.237919).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1896481
	speed: 1.1557s/iter; left time: 5072.1710s
Epoch: 17 cost time: 72.31698060035706
Epoch: 17, Steps: 132 | Train Loss: 0.1895485 Vali Loss: 0.2301819 Test Loss: 0.2832516
Validation loss decreased (0.237919 --> 0.230182).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1813734
	speed: 1.1060s/iter; left time: 4708.0424s
Epoch: 18 cost time: 64.53086423873901
Epoch: 18, Steps: 132 | Train Loss: 0.1808716 Vali Loss: 0.2237148 Test Loss: 0.2751287
Validation loss decreased (0.230182 --> 0.223715).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1675183
	speed: 1.0901s/iter; left time: 4496.8574s
Epoch: 19 cost time: 65.89268946647644
Epoch: 19, Steps: 132 | Train Loss: 0.1731838 Vali Loss: 0.2175864 Test Loss: 0.2678861
Validation loss decreased (0.223715 --> 0.217586).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1639997
	speed: 1.0599s/iter; left time: 4232.2369s
Epoch: 20 cost time: 65.9267680644989
Epoch: 20, Steps: 132 | Train Loss: 0.1664511 Vali Loss: 0.2128943 Test Loss: 0.2616519
Validation loss decreased (0.217586 --> 0.212894).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1592143
	speed: 1.0462s/iter; left time: 4039.4716s
Epoch: 21 cost time: 63.525413513183594
Epoch: 21, Steps: 132 | Train Loss: 0.1604959 Vali Loss: 0.2081466 Test Loss: 0.2558485
Validation loss decreased (0.212894 --> 0.208147).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1468683
	speed: 1.1165s/iter; left time: 4163.2555s
Epoch: 22 cost time: 68.60329127311707
Epoch: 22, Steps: 132 | Train Loss: 0.1552704 Vali Loss: 0.2042168 Test Loss: 0.2506489
Validation loss decreased (0.208147 --> 0.204217).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1517323
	speed: 1.1143s/iter; left time: 4008.0101s
Epoch: 23 cost time: 70.13253259658813
Epoch: 23, Steps: 132 | Train Loss: 0.1506791 Vali Loss: 0.2005987 Test Loss: 0.2461815
Validation loss decreased (0.204217 --> 0.200599).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1432399
	speed: 1.1303s/iter; left time: 3916.4816s
Epoch: 24 cost time: 67.5154538154602
Epoch: 24, Steps: 132 | Train Loss: 0.1465065 Vali Loss: 0.1979218 Test Loss: 0.2425378
Validation loss decreased (0.200599 --> 0.197922).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1473332
	speed: 1.0832s/iter; left time: 3610.2414s
Epoch: 25 cost time: 66.8830976486206
Epoch: 25, Steps: 132 | Train Loss: 0.1428716 Vali Loss: 0.1952387 Test Loss: 0.2388602
Validation loss decreased (0.197922 --> 0.195239).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1470348
	speed: 1.1199s/iter; left time: 3584.9038s
Epoch: 26 cost time: 67.1106219291687
Epoch: 26, Steps: 132 | Train Loss: 0.1395912 Vali Loss: 0.1924924 Test Loss: 0.2356683
Validation loss decreased (0.195239 --> 0.192492).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1391836
	speed: 1.1198s/iter; left time: 3436.6574s
Epoch: 27 cost time: 67.60381388664246
Epoch: 27, Steps: 132 | Train Loss: 0.1366921 Vali Loss: 0.1909043 Test Loss: 0.2329322
Validation loss decreased (0.192492 --> 0.190904).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1317876
	speed: 1.0912s/iter; left time: 3204.9513s
Epoch: 28 cost time: 66.99153780937195
Epoch: 28, Steps: 132 | Train Loss: 0.1340843 Vali Loss: 0.1889102 Test Loss: 0.2303738
Validation loss decreased (0.190904 --> 0.188910).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1338459
	speed: 1.2399s/iter; left time: 3478.0566s
Epoch: 29 cost time: 70.37166166305542
Epoch: 29, Steps: 132 | Train Loss: 0.1318006 Vali Loss: 0.1871366 Test Loss: 0.2280735
Validation loss decreased (0.188910 --> 0.187137).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1293195
	speed: 1.0794s/iter; left time: 2885.1294s
Epoch: 30 cost time: 65.70310378074646
Epoch: 30, Steps: 132 | Train Loss: 0.1297131 Vali Loss: 0.1856661 Test Loss: 0.2259790
Validation loss decreased (0.187137 --> 0.185666).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1288409
	speed: 1.0862s/iter; left time: 2760.1221s
Epoch: 31 cost time: 67.08025860786438
Epoch: 31, Steps: 132 | Train Loss: 0.1279138 Vali Loss: 0.1846329 Test Loss: 0.2242598
Validation loss decreased (0.185666 --> 0.184633).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1234392
	speed: 1.0892s/iter; left time: 2623.8548s
Epoch: 32 cost time: 65.25686168670654
Epoch: 32, Steps: 132 | Train Loss: 0.1262651 Vali Loss: 0.1830632 Test Loss: 0.2224979
Validation loss decreased (0.184633 --> 0.183063).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1246827
	speed: 1.0655s/iter; left time: 2426.0403s
Epoch: 33 cost time: 63.197633266448975
Epoch: 33, Steps: 132 | Train Loss: 0.1248458 Vali Loss: 0.1826561 Test Loss: 0.2210903
Validation loss decreased (0.183063 --> 0.182656).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1272747
	speed: 0.9945s/iter; left time: 2133.1583s
Epoch: 34 cost time: 64.36462140083313
Epoch: 34, Steps: 132 | Train Loss: 0.1235310 Vali Loss: 0.1817027 Test Loss: 0.2198132
Validation loss decreased (0.182656 --> 0.181703).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1270196
	speed: 1.0900s/iter; left time: 2194.2155s
Epoch: 35 cost time: 65.0529260635376
Epoch: 35, Steps: 132 | Train Loss: 0.1223014 Vali Loss: 0.1806007 Test Loss: 0.2186336
Validation loss decreased (0.181703 --> 0.180601).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1155063
	speed: 1.0646s/iter; left time: 2002.5014s
Epoch: 36 cost time: 63.32456016540527
Epoch: 36, Steps: 132 | Train Loss: 0.1213072 Vali Loss: 0.1802325 Test Loss: 0.2176413
Validation loss decreased (0.180601 --> 0.180233).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1178961
	speed: 1.1318s/iter; left time: 1979.5259s
Epoch: 37 cost time: 68.53689074516296
Epoch: 37, Steps: 132 | Train Loss: 0.1203049 Vali Loss: 0.1794812 Test Loss: 0.2166529
Validation loss decreased (0.180233 --> 0.179481).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1229018
	speed: 1.1216s/iter; left time: 1813.6450s
Epoch: 38 cost time: 66.40032505989075
Epoch: 38, Steps: 132 | Train Loss: 0.1194835 Vali Loss: 0.1787130 Test Loss: 0.2158708
Validation loss decreased (0.179481 --> 0.178713).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1144838
	speed: 1.1331s/iter; left time: 1682.7031s
Epoch: 39 cost time: 68.85033583641052
Epoch: 39, Steps: 132 | Train Loss: 0.1186988 Vali Loss: 0.1786608 Test Loss: 0.2149490
Validation loss decreased (0.178713 --> 0.178661).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1146999
	speed: 1.1728s/iter; left time: 1586.8268s
Epoch: 40 cost time: 71.24322366714478
Epoch: 40, Steps: 132 | Train Loss: 0.1179950 Vali Loss: 0.1782822 Test Loss: 0.2143009
Validation loss decreased (0.178661 --> 0.178282).  Saving model ...
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1164202
	speed: 1.1908s/iter; left time: 1453.9738s
Epoch: 41 cost time: 70.14573431015015
Epoch: 41, Steps: 132 | Train Loss: 0.1174562 Vali Loss: 0.1778881 Test Loss: 0.2135704
Validation loss decreased (0.178282 --> 0.177888).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1176223
	speed: 1.1525s/iter; left time: 1255.1012s
Epoch: 42 cost time: 70.80751585960388
Epoch: 42, Steps: 132 | Train Loss: 0.1169294 Vali Loss: 0.1775458 Test Loss: 0.2130068
Validation loss decreased (0.177888 --> 0.177546).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1148891
	speed: 1.1056s/iter; left time: 1058.0334s
Epoch: 43 cost time: 66.07555913925171
Epoch: 43, Steps: 132 | Train Loss: 0.1163878 Vali Loss: 0.1771474 Test Loss: 0.2125059
Validation loss decreased (0.177546 --> 0.177147).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.1127420
	speed: 1.1058s/iter; left time: 912.2706s
Epoch: 44 cost time: 68.59807324409485
Epoch: 44, Steps: 132 | Train Loss: 0.1159286 Vali Loss: 0.1769900 Test Loss: 0.2120246
Validation loss decreased (0.177147 --> 0.176990).  Saving model ...
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.1115230
	speed: 1.1368s/iter; left time: 787.8201s
Epoch: 45 cost time: 69.93323945999146
Epoch: 45, Steps: 132 | Train Loss: 0.1155118 Vali Loss: 0.1765628 Test Loss: 0.2116278
Validation loss decreased (0.176990 --> 0.176563).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.1151818
	speed: 1.1076s/iter; left time: 621.3552s
Epoch: 46 cost time: 66.48547291755676
Epoch: 46, Steps: 132 | Train Loss: 0.1151514 Vali Loss: 0.1764399 Test Loss: 0.2112199
Validation loss decreased (0.176563 --> 0.176440).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.1114672
	speed: 1.1049s/iter; left time: 474.0020s
Epoch: 47 cost time: 67.31806349754333
Epoch: 47, Steps: 132 | Train Loss: 0.1148278 Vali Loss: 0.1764550 Test Loss: 0.2108926
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.1103004
	speed: 1.1249s/iter; left time: 334.0819s
Epoch: 48 cost time: 68.54407095909119
Epoch: 48, Steps: 132 | Train Loss: 0.1145880 Vali Loss: 0.1763152 Test Loss: 0.2105380
Validation loss decreased (0.176440 --> 0.176315).  Saving model ...
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.1209729
	speed: 1.0465s/iter; left time: 172.6678s
Epoch: 49 cost time: 62.83626079559326
Epoch: 49, Steps: 132 | Train Loss: 0.1142679 Vali Loss: 0.1760377 Test Loss: 0.2102206
Validation loss decreased (0.176315 --> 0.176038).  Saving model ...
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.1163838
	speed: 1.1220s/iter; left time: 37.0247s
Epoch: 50 cost time: 68.22394156455994
Epoch: 50, Steps: 132 | Train Loss: 0.1140585 Vali Loss: 0.1760969 Test Loss: 0.2100234
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.0497355408796396e-05
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8414822400.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2097842
	speed: 0.4822s/iter; left time: 3134.5342s
Epoch: 1 cost time: 63.564629793167114
Epoch: 1, Steps: 132 | Train Loss: 0.2187047 Vali Loss: 0.1750474 Test Loss: 0.2062104
Validation loss decreased (inf --> 0.175047).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2158549
	speed: 1.0891s/iter; left time: 6936.3367s
Epoch: 2 cost time: 65.29214882850647
Epoch: 2, Steps: 132 | Train Loss: 0.2179965 Vali Loss: 0.1751998 Test Loss: 0.2060121
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2255243
	speed: 1.0832s/iter; left time: 6756.1409s
Epoch: 3 cost time: 65.69257664680481
Epoch: 3, Steps: 132 | Train Loss: 0.2178010 Vali Loss: 0.1752095 Test Loss: 0.2057487
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1992095
	speed: 1.0753s/iter; left time: 6564.4092s
Epoch: 4 cost time: 65.21270227432251
Epoch: 4, Steps: 132 | Train Loss: 0.2177810 Vali Loss: 0.1752083 Test Loss: 0.2058676
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.20451620221138, mae:0.29394638538360596, rse:0.4511191248893738, corr:[0.44481918 0.44967386 0.45114386 0.4513369  0.4517224  0.45143926
 0.4518606  0.45152658 0.4515533  0.451604   0.4515041  0.4516764
 0.45139387 0.4514948  0.45111603 0.4514392  0.45140314 0.45123503
 0.45115    0.45115647 0.4511449  0.4509919  0.45095694 0.45090783
 0.45096806 0.45102304 0.45142084 0.451299   0.45128074 0.45130187
 0.45111147 0.45090833 0.45095608 0.4510325  0.45076784 0.45075327
 0.45062765 0.45070288 0.45060104 0.4505444  0.45073366 0.45057848
 0.45037603 0.450127   0.45000434 0.45003027 0.4501274  0.45006642
 0.45016208 0.4503498  0.4505527  0.4505889  0.45045573 0.45052314
 0.45050806 0.45041293 0.45016268 0.45015422 0.45011425 0.45003113
 0.450143   0.45011202 0.45014042 0.45017323 0.45021984 0.45018214
 0.4500632  0.4499921  0.4499997  0.44982043 0.44963154 0.4495785
 0.44957396 0.4496572  0.44974947 0.44997138 0.44987506 0.44982368
 0.44977546 0.4494987  0.4493804  0.44925    0.4493705  0.44932887
 0.44912353 0.44926372 0.44948223 0.44944495 0.44942793 0.44946358
 0.449376   0.4493234  0.44929072 0.44925562 0.44924808 0.44928366
 0.44917333 0.44925362 0.44945288 0.449305   0.44914326 0.4491594
 0.4491032  0.44902933 0.4490402  0.44886222 0.44889492 0.44897693
 0.44882342 0.4487389  0.44867086 0.44873434 0.44873923 0.44880736
 0.4486966  0.44850066 0.44846725 0.4484005  0.4485551  0.4486535
 0.44881344 0.4489515  0.44902834 0.44904348 0.44905177 0.44896996
 0.44896808 0.44897768 0.44873008 0.4486117  0.44865385 0.44868687
 0.44872242 0.44886294 0.4487625  0.4485349  0.44868892 0.44872087
 0.4486267  0.44872093 0.4487812  0.4487234  0.44880024 0.4486682
 0.4486413  0.44877014 0.44878688 0.4488768  0.44869444 0.4486953
 0.44869128 0.4484674  0.44835085 0.4482689  0.44837183 0.44846633
 0.44843256 0.44830334 0.44838238 0.44853035 0.44841647 0.4484619
 0.4486443  0.44855267 0.4485688  0.4484938  0.4482503  0.4482577
 0.44789293 0.4477914  0.44813436 0.44800857 0.4479677  0.44794932
 0.44778162 0.44775698 0.44764552 0.447647   0.44773096 0.44769108
 0.44777405 0.44769645 0.44748873 0.44747126 0.4474687  0.4474462
 0.44740307 0.44734165 0.44711772 0.44702533 0.44723096 0.44720262
 0.44696036 0.44698682 0.44724002 0.4472277  0.44710925 0.4470099
 0.44688427 0.4467797  0.44668007 0.4466332  0.44660613 0.44664615
 0.44656828 0.44653574 0.44652516 0.4464173  0.4464372  0.44643542
 0.44636965 0.4461515  0.44595897 0.44594356 0.4461006  0.4460238
 0.4458093  0.44599432 0.44620296 0.44629967 0.44623545 0.44613662
 0.4459828  0.44591767 0.44585493 0.44570148 0.44583553 0.44578376
 0.4456105  0.4456697  0.4457609  0.44583857 0.44577113 0.44565082
 0.44562995 0.4454317  0.44528374 0.44515765 0.44506732 0.44504756
 0.44500902 0.44525686 0.4455654  0.44563088 0.4456582  0.44567287
 0.44569886 0.44556704 0.44529685 0.44520274 0.44516906 0.44504124
 0.4450489  0.4451808  0.44531006 0.44526312 0.44515002 0.44510582
 0.4450225  0.44499716 0.44472393 0.4445489  0.44463423 0.4445424
 0.44466293 0.44474536 0.4448147  0.4449052  0.4448214  0.4449466
 0.4449741  0.4448053  0.44468302 0.44464594 0.44477862 0.44472197
 0.44458625 0.44466168 0.44475833 0.4447559  0.4445917  0.44446453
 0.44437847 0.44429076 0.4441963  0.44405428 0.44413027 0.4441764
 0.44439638 0.4446116  0.44466415 0.44470546 0.4447798  0.44471964
 0.44475815 0.44480932 0.44462135 0.44451463 0.44452694 0.44463038
 0.44460556 0.4445687  0.4446824  0.44463456 0.44455898 0.44442457
 0.44442195 0.4444846  0.44457647 0.444633   0.44448233 0.44449213
 0.44433123 0.44442534 0.44468185 0.44462952 0.44460133 0.44458312
 0.4445004  0.44452983 0.44447    0.4442282  0.44432342 0.4444012
 0.4443074  0.44428042 0.44434112 0.44431993 0.4441076  0.44411007
 0.4441856  0.44422776 0.44412342 0.44388336 0.44394937 0.44362697
 0.44311404 0.44330418 0.44350085 0.4433581  0.4433443  0.44327933
 0.4431571  0.44303232 0.4427736  0.44272244 0.44273382 0.44268322
 0.44272435 0.44274032 0.44270706 0.4427026  0.44258776 0.44253016
 0.44254106 0.44245827 0.44250968 0.44244933 0.4424549  0.44243518
 0.4422841  0.44244516 0.44255617 0.44262004 0.44257239 0.44246787
 0.44236982 0.442229   0.44227442 0.44209698 0.44191217 0.44196135
 0.44187132 0.44198862 0.44214234 0.44197968 0.44205025 0.44199798
 0.44179663 0.4417208  0.44160753 0.44162926 0.4415124  0.44143724
 0.4415803  0.4416398  0.4418203  0.44200933 0.44204876 0.44188884
 0.44167513 0.4416027  0.4414931  0.44142222 0.4412218  0.44101328
 0.44116005 0.44143698 0.44145697 0.4411642  0.44113934 0.44120878
 0.44099364 0.44102424 0.44095653 0.44072676 0.44075942 0.44062328
 0.44052362 0.44080916 0.44113085 0.4411965  0.44114447 0.44118327
 0.4409735  0.44086272 0.44080573 0.44057742 0.44060984 0.44050464
 0.44051445 0.44078737 0.44066307 0.4406706  0.44069952 0.44058585
 0.4406044  0.44042987 0.44042432 0.4403731  0.44029632 0.44031665
 0.44030693 0.44062296 0.44068807 0.44046763 0.4406752  0.4406835
 0.44055578 0.44066662 0.440545   0.4406019  0.44053885 0.4404068
 0.44048247 0.4404433  0.4407439  0.44068828 0.44036785 0.4405229
 0.44056925 0.4404574  0.44020143 0.43998587 0.43993738 0.4399972
 0.4402325  0.44043052 0.44065288 0.4407304  0.44059417 0.4405801
 0.44066474 0.44053766 0.44050068 0.44054553 0.44050768 0.44039547
 0.44028643 0.44034374 0.44052806 0.440512   0.44047356 0.4405502
 0.44047663 0.44049484 0.4405139  0.44041282 0.440432   0.44040668
 0.440369   0.44039539 0.44045955 0.44052982 0.44051403 0.44051188
 0.44039908 0.44035834 0.44033507 0.44012833 0.44028717 0.4404493
 0.4402564  0.4403459  0.4402691  0.44022322 0.44045424 0.44043794
 0.44048643 0.4405258  0.44045275 0.44024387 0.4400924  0.43994325
 0.43960774 0.4396865  0.4398204  0.43969786 0.4396216  0.4395303
 0.43941948 0.43924826 0.4390154  0.43897825 0.43893844 0.43894243
 0.4390427  0.43903315 0.43906403 0.43906483 0.43903148 0.4389235
 0.4389364  0.43890208 0.43850532 0.43835703 0.4382353  0.43802124
 0.4380713  0.43819755 0.4384291  0.43842688 0.43836254 0.4385213
 0.4382743  0.43810123 0.43794844 0.43774426 0.43785393 0.43786976
 0.43783587 0.43771613 0.43767923 0.43796024 0.43784463 0.4377184
 0.4378004  0.4375547  0.43726513 0.43711042 0.43718004 0.43721458
 0.43704993 0.43742576 0.43775    0.43756557 0.43750513 0.43738863
 0.43726048 0.43716457 0.43710628 0.43702078 0.43685022 0.43674675
 0.43668166 0.43677106 0.43675268 0.43653235 0.4364439  0.436359
 0.4363056  0.43616205 0.436036   0.4360608  0.43587235 0.43560457
 0.4356385  0.43582165 0.4359868  0.43600926 0.4359987  0.4358857
 0.43576595 0.43576977 0.43553728 0.4353752  0.43551555 0.4356126
 0.43551555 0.43554378 0.4356769  0.4356278  0.43567193 0.43565017
 0.43562233 0.4356295  0.4354663  0.43543777 0.43539324 0.4353033
 0.43532673 0.43546462 0.43574178 0.4356043  0.4354081  0.43546924
 0.43535268 0.43526083 0.43512198 0.43494067 0.43491527 0.43495756
 0.4350816  0.43490517 0.43479443 0.4350201  0.4350221  0.43501014
 0.43492457 0.4348227  0.43481582 0.43468624 0.43470314 0.43476167
 0.4349563  0.43527257 0.4352705  0.4352015  0.43517303 0.43492454
 0.43499184 0.4350241  0.43490538 0.4347771  0.43472278 0.4347639
 0.43468788 0.43478096 0.43482065 0.43481386 0.43483454 0.43471703
 0.4346832  0.43473658 0.43480158 0.43469962 0.43460852 0.43437698
 0.4342915  0.43454778 0.43452826 0.43450546 0.43459252 0.43454137
 0.43451315 0.43449858 0.4343442  0.43433806 0.43433493 0.43440524
 0.43445143 0.43434665 0.43437988 0.43431994 0.4343791  0.4344536
 0.43446296 0.4343365  0.4342022  0.43420032 0.43411788 0.4341019
 0.4335434  0.43322593 0.43363273 0.4334435  0.4331976  0.43321356
 0.4329284  0.43270472 0.43240756 0.43223894 0.4324216  0.43223453
 0.43241474 0.43246397 0.43230164 0.43251595 0.43234518 0.43234307
 0.43214008 0.43200332 0.43192676 0.4318439  0.43183553 0.43177527
 0.43177155 0.43164185 0.43194076 0.43166724 0.43153477 0.4315371
 0.431211   0.43087095 0.43064302 0.43058017 0.43051746 0.4304757
 0.4303619  0.43059328 0.43037334 0.4305561  0.43016148 0.43037844
 0.4300506  0.4303041  0.4302005  0.43086386 0.43112758 0.4308822 ]
