Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j192_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17501
val 2441
test 5069
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5325004800.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3663308
	speed: 0.3898s/iter; left time: 2611.7660s
Epoch: 1 cost time: 51.740556716918945
Epoch: 1, Steps: 136 | Train Loss: 0.5605721 Vali Loss: 0.2384217 Test Loss: 0.2876537
Validation loss decreased (inf --> 0.238422).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1947405
	speed: 0.8132s/iter; left time: 5338.8687s
Epoch: 2 cost time: 50.110777854919434
Epoch: 2, Steps: 136 | Train Loss: 0.2117124 Vali Loss: 0.1386990 Test Loss: 0.1699839
Validation loss decreased (0.238422 --> 0.138699).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1583138
	speed: 0.7898s/iter; left time: 5077.4133s
Epoch: 3 cost time: 47.190921783447266
Epoch: 3, Steps: 136 | Train Loss: 0.1600525 Vali Loss: 0.1283269 Test Loss: 0.1561435
Validation loss decreased (0.138699 --> 0.128327).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1591530
	speed: 0.7403s/iter; left time: 4658.5891s
Epoch: 4 cost time: 44.802225828170776
Epoch: 4, Steps: 136 | Train Loss: 0.1540054 Vali Loss: 0.1271500 Test Loss: 0.1543038
Validation loss decreased (0.128327 --> 0.127150).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1537805
	speed: 0.8406s/iter; left time: 5175.7495s
Epoch: 5 cost time: 53.96820282936096
Epoch: 5, Steps: 136 | Train Loss: 0.1527953 Vali Loss: 0.1267007 Test Loss: 0.1536825
Validation loss decreased (0.127150 --> 0.126701).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1549111
	speed: 0.8608s/iter; left time: 5183.1447s
Epoch: 6 cost time: 55.12947916984558
Epoch: 6, Steps: 136 | Train Loss: 0.1520603 Vali Loss: 0.1263235 Test Loss: 0.1532919
Validation loss decreased (0.126701 --> 0.126324).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1492247
	speed: 0.8566s/iter; left time: 5040.8883s
Epoch: 7 cost time: 54.099252223968506
Epoch: 7, Steps: 136 | Train Loss: 0.1516546 Vali Loss: 0.1262316 Test Loss: 0.1530851
Validation loss decreased (0.126324 --> 0.126232).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1491908
	speed: 0.8620s/iter; left time: 4955.3614s
Epoch: 8 cost time: 50.13322305679321
Epoch: 8, Steps: 136 | Train Loss: 0.1513985 Vali Loss: 0.1261067 Test Loss: 0.1529091
Validation loss decreased (0.126232 --> 0.126107).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1584432
	speed: 0.8018s/iter; left time: 4500.2619s
Epoch: 9 cost time: 50.701558351516724
Epoch: 9, Steps: 136 | Train Loss: 0.1511205 Vali Loss: 0.1259168 Test Loss: 0.1527378
Validation loss decreased (0.126107 --> 0.125917).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1605521
	speed: 0.8372s/iter; left time: 4585.1301s
Epoch: 10 cost time: 54.59412622451782
Epoch: 10, Steps: 136 | Train Loss: 0.1509265 Vali Loss: 0.1258931 Test Loss: 0.1526569
Validation loss decreased (0.125917 --> 0.125893).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1625321
	speed: 0.8263s/iter; left time: 4413.0893s
Epoch: 11 cost time: 49.65610384941101
Epoch: 11, Steps: 136 | Train Loss: 0.1507627 Vali Loss: 0.1258539 Test Loss: 0.1525820
Validation loss decreased (0.125893 --> 0.125854).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1497782
	speed: 0.7836s/iter; left time: 4078.4268s
Epoch: 12 cost time: 47.84067249298096
Epoch: 12, Steps: 136 | Train Loss: 0.1506812 Vali Loss: 0.1257607 Test Loss: 0.1525584
Validation loss decreased (0.125854 --> 0.125761).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1469118
	speed: 0.8634s/iter; left time: 4376.5363s
Epoch: 13 cost time: 54.39877438545227
Epoch: 13, Steps: 136 | Train Loss: 0.1506199 Vali Loss: 0.1257057 Test Loss: 0.1524499
Validation loss decreased (0.125761 --> 0.125706).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1496736
	speed: 0.7637s/iter; left time: 3767.5699s
Epoch: 14 cost time: 41.620314836502075
Epoch: 14, Steps: 136 | Train Loss: 0.1505319 Vali Loss: 0.1256806 Test Loss: 0.1524412
Validation loss decreased (0.125706 --> 0.125681).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1627796
	speed: 0.8126s/iter; left time: 3898.0056s
Epoch: 15 cost time: 55.74621343612671
Epoch: 15, Steps: 136 | Train Loss: 0.1505334 Vali Loss: 0.1256020 Test Loss: 0.1523704
Validation loss decreased (0.125681 --> 0.125602).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1448064
	speed: 0.9167s/iter; left time: 4272.5772s
Epoch: 16 cost time: 50.40607309341431
Epoch: 16, Steps: 136 | Train Loss: 0.1504316 Vali Loss: 0.1255924 Test Loss: 0.1523037
Validation loss decreased (0.125602 --> 0.125592).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1456109
	speed: 0.7367s/iter; left time: 3333.6157s
Epoch: 17 cost time: 47.04040789604187
Epoch: 17, Steps: 136 | Train Loss: 0.1503549 Vali Loss: 0.1256068 Test Loss: 0.1523455
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1391965
	speed: 0.7208s/iter; left time: 3163.7862s
Epoch: 18 cost time: 45.08562254905701
Epoch: 18, Steps: 136 | Train Loss: 0.1504065 Vali Loss: 0.1255193 Test Loss: 0.1523029
Validation loss decreased (0.125592 --> 0.125519).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1475557
	speed: 0.7543s/iter; left time: 3208.0889s
Epoch: 19 cost time: 43.53407597541809
Epoch: 19, Steps: 136 | Train Loss: 0.1503281 Vali Loss: 0.1256027 Test Loss: 0.1522921
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1563723
	speed: 0.7623s/iter; left time: 3138.5154s
Epoch: 20 cost time: 48.50680184364319
Epoch: 20, Steps: 136 | Train Loss: 0.1502520 Vali Loss: 0.1255679 Test Loss: 0.1522674
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1544440
	speed: 0.8346s/iter; left time: 3322.5834s
Epoch: 21 cost time: 51.51740503311157
Epoch: 21, Steps: 136 | Train Loss: 0.1503236 Vali Loss: 0.1255093 Test Loss: 0.1522729
Validation loss decreased (0.125519 --> 0.125509).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1579686
	speed: 0.8463s/iter; left time: 3253.9631s
Epoch: 22 cost time: 53.42769980430603
Epoch: 22, Steps: 136 | Train Loss: 0.1502697 Vali Loss: 0.1255029 Test Loss: 0.1522559
Validation loss decreased (0.125509 --> 0.125503).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1461715
	speed: 0.8295s/iter; left time: 3076.5635s
Epoch: 23 cost time: 50.84343695640564
Epoch: 23, Steps: 136 | Train Loss: 0.1502592 Vali Loss: 0.1255220 Test Loss: 0.1522087
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1421712
	speed: 0.7457s/iter; left time: 2664.4669s
Epoch: 24 cost time: 43.003525495529175
Epoch: 24, Steps: 136 | Train Loss: 0.1501432 Vali Loss: 0.1254721 Test Loss: 0.1522153
Validation loss decreased (0.125503 --> 0.125472).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1432084
	speed: 0.7062s/iter; left time: 2427.0862s
Epoch: 25 cost time: 41.08499622344971
Epoch: 25, Steps: 136 | Train Loss: 0.1501662 Vali Loss: 0.1255157 Test Loss: 0.1522100
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1454143
	speed: 0.6945s/iter; left time: 2292.5937s
Epoch: 26 cost time: 40.14056634902954
Epoch: 26, Steps: 136 | Train Loss: 0.1501710 Vali Loss: 0.1255236 Test Loss: 0.1522199
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1497026
	speed: 0.5943s/iter; left time: 1880.8958s
Epoch: 27 cost time: 35.77051877975464
Epoch: 27, Steps: 136 | Train Loss: 0.1501622 Vali Loss: 0.1254605 Test Loss: 0.1521755
Validation loss decreased (0.125472 --> 0.125460).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1540311
	speed: 0.6035s/iter; left time: 1828.0279s
Epoch: 28 cost time: 36.82838463783264
Epoch: 28, Steps: 136 | Train Loss: 0.1501485 Vali Loss: 0.1253721 Test Loss: 0.1521782
Validation loss decreased (0.125460 --> 0.125372).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1428828
	speed: 0.6794s/iter; left time: 1965.6119s
Epoch: 29 cost time: 48.48488521575928
Epoch: 29, Steps: 136 | Train Loss: 0.1500531 Vali Loss: 0.1254573 Test Loss: 0.1521237
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1438818
	speed: 0.7941s/iter; left time: 2189.2498s
Epoch: 30 cost time: 45.968318939208984
Epoch: 30, Steps: 136 | Train Loss: 0.1501814 Vali Loss: 0.1254339 Test Loss: 0.1521488
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1460288
	speed: 0.7396s/iter; left time: 1938.3863s
Epoch: 31 cost time: 44.26089382171631
Epoch: 31, Steps: 136 | Train Loss: 0.1501106 Vali Loss: 0.1254574 Test Loss: 0.1521268
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5069
mse:0.14929434657096863, mae:0.2443339079618454, rse:0.3841705322265625, corr:[0.46394214 0.46616203 0.4667044  0.46790186 0.467965   0.46850607
 0.46855068 0.46845305 0.4684568  0.46820998 0.468208   0.46795455
 0.46806937 0.46786726 0.4680481  0.46800363 0.4678932  0.46798468
 0.46785796 0.46773964 0.46765178 0.46762875 0.46783093 0.46794546
 0.46827692 0.46858084 0.46864718 0.4688237  0.46854132 0.46848384
 0.46824378 0.46808195 0.46795264 0.46771848 0.46772635 0.46752486
 0.4674562  0.46726367 0.4672595  0.46719286 0.467056   0.46707496
 0.46685985 0.46675292 0.4667167  0.46659264 0.46667486 0.4668556
 0.46692553 0.4672605  0.46731693 0.46728393 0.4672307  0.46704584
 0.46701285 0.46681106 0.4668257  0.4667896  0.4667847  0.46666515
 0.46664062 0.46665302 0.4665184  0.46661055 0.46639788 0.46641308
 0.46645227 0.46623778 0.46620512 0.46614406 0.4660902  0.4662506
 0.46630237 0.46642917 0.46661368 0.46647125 0.46643272 0.46632645
 0.46634844 0.46625727 0.46614066 0.46616748 0.46605623 0.46608454
 0.46595034 0.46604428 0.46598    0.4658237  0.46589717 0.46585414
 0.46586344 0.46559593 0.4654034  0.4654302  0.46545145 0.46553808
 0.46572542 0.46582457 0.4659296  0.46593243 0.46582788 0.46573284
 0.46560276 0.4656338  0.46550646 0.46549663 0.46538082 0.465327
 0.46542487 0.46527752 0.4653841  0.46537784 0.46533263 0.46528465
 0.46525908 0.46530274 0.4653306  0.46548733 0.4655238  0.465684
 0.4659283  0.46602693 0.46602595 0.4660808  0.46608925 0.46605048
 0.46592632 0.4658337  0.46583256 0.4657074  0.46573552 0.46553424
 0.4655712  0.4656052  0.46560878 0.465684   0.46550727 0.46568188
 0.46562007 0.46553734 0.46541983 0.4652616  0.46526384 0.46526173
 0.46538153 0.4654099  0.46543726 0.4654822  0.46548656 0.46537885
 0.46530145 0.46518546 0.46516013 0.46491832 0.46496433 0.46505553
 0.46484047 0.46507564 0.46492168 0.46505022 0.4650193  0.46491426
 0.46504292 0.46479365 0.4646988  0.46443245 0.46451795 0.46461603
 0.46455085 0.46468067 0.46475664 0.46462417 0.4644377  0.46425804
 0.46417364 0.46403632 0.463783   0.46361724 0.46351215 0.46365106
 0.46339965 0.4636977  0.46343756 0.46347132 0.46337572 0.46307307
 0.46320918 0.46251228 0.46291637 0.46241862 0.46362686 0.4635788 ]
