Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8414822400.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6589374
	speed: 0.4850s/iter; left time: 3152.9368s
Epoch: 1 cost time: 62.82058596611023
Epoch: 1, Steps: 132 | Train Loss: 0.8137545 Vali Loss: 0.4816649 Test Loss: 0.5806739
Validation loss decreased (inf --> 0.481665).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4219459
	speed: 1.0764s/iter; left time: 6855.8313s
Epoch: 2 cost time: 67.62087416648865
Epoch: 2, Steps: 132 | Train Loss: 0.4926358 Vali Loss: 0.3275862 Test Loss: 0.4009639
Validation loss decreased (0.481665 --> 0.327586).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3240111
	speed: 1.1287s/iter; left time: 7039.8740s
Epoch: 3 cost time: 66.40432500839233
Epoch: 3, Steps: 132 | Train Loss: 0.3580753 Vali Loss: 0.2480149 Test Loss: 0.3054498
Validation loss decreased (0.327586 --> 0.248015).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2575290
	speed: 1.1061s/iter; left time: 6752.6192s
Epoch: 4 cost time: 65.47242999076843
Epoch: 4, Steps: 132 | Train Loss: 0.2875190 Vali Loss: 0.2081973 Test Loss: 0.2555170
Validation loss decreased (0.248015 --> 0.208197).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2538275
	speed: 1.1012s/iter; left time: 6577.4279s
Epoch: 5 cost time: 69.34334635734558
Epoch: 5, Steps: 132 | Train Loss: 0.2515650 Vali Loss: 0.1892601 Test Loss: 0.2301757
Validation loss decreased (0.208197 --> 0.189260).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2306989
	speed: 1.1769s/iter; left time: 6874.3734s
Epoch: 6 cost time: 72.17004060745239
Epoch: 6, Steps: 132 | Train Loss: 0.2340171 Vali Loss: 0.1810927 Test Loss: 0.2178036
Validation loss decreased (0.189260 --> 0.181093).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2292382
	speed: 1.2315s/iter; left time: 7030.8859s
Epoch: 7 cost time: 76.02629113197327
Epoch: 7, Steps: 132 | Train Loss: 0.2258182 Vali Loss: 0.1776637 Test Loss: 0.2118319
Validation loss decreased (0.181093 --> 0.177664).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2192130
	speed: 1.2192s/iter; left time: 6799.6379s
Epoch: 8 cost time: 67.93932271003723
Epoch: 8, Steps: 132 | Train Loss: 0.2219206 Vali Loss: 0.1761249 Test Loss: 0.2090770
Validation loss decreased (0.177664 --> 0.176125).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2162636
	speed: 1.0379s/iter; left time: 5651.3451s
Epoch: 9 cost time: 61.608267068862915
Epoch: 9, Steps: 132 | Train Loss: 0.2201855 Vali Loss: 0.1759907 Test Loss: 0.2077698
Validation loss decreased (0.176125 --> 0.175991).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2127883
	speed: 1.0349s/iter; left time: 5498.1961s
Epoch: 10 cost time: 60.40521788597107
Epoch: 10, Steps: 132 | Train Loss: 0.2193846 Vali Loss: 0.1755020 Test Loss: 0.2071067
Validation loss decreased (0.175991 --> 0.175502).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2181262
	speed: 1.0361s/iter; left time: 5368.2660s
Epoch: 11 cost time: 63.72407341003418
Epoch: 11, Steps: 132 | Train Loss: 0.2190297 Vali Loss: 0.1758899 Test Loss: 0.2066770
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2142608
	speed: 1.0781s/iter; left time: 5443.3424s
Epoch: 12 cost time: 64.50618767738342
Epoch: 12, Steps: 132 | Train Loss: 0.2186491 Vali Loss: 0.1754694 Test Loss: 0.2064670
Validation loss decreased (0.175502 --> 0.175469).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2150149
	speed: 1.0554s/iter; left time: 5189.5485s
Epoch: 13 cost time: 63.54126024246216
Epoch: 13, Steps: 132 | Train Loss: 0.2185347 Vali Loss: 0.1755703 Test Loss: 0.2063287
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2101608
	speed: 1.0546s/iter; left time: 5046.4077s
Epoch: 14 cost time: 64.59239101409912
Epoch: 14, Steps: 132 | Train Loss: 0.2184024 Vali Loss: 0.1755098 Test Loss: 0.2062106
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2238202
	speed: 1.1075s/iter; left time: 5153.3542s
Epoch: 15 cost time: 69.03167295455933
Epoch: 15, Steps: 132 | Train Loss: 0.2183422 Vali Loss: 0.1754784 Test Loss: 0.2061171
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.20480278134346008, mae:0.2946530878543854, rse:0.4514350891113281, corr:[0.4470753  0.43976027 0.44722092 0.44782    0.45064893 0.45029703
 0.45233676 0.4512678  0.4516189  0.45119688 0.45039204 0.45044184
 0.44985998 0.44984287 0.44984737 0.44968164 0.4500682  0.44993624
 0.4503124  0.44984853 0.4501154  0.45026997 0.450255   0.450941
 0.45070025 0.45146278 0.45157373 0.45151976 0.4516639  0.45101044
 0.45112944 0.4506879  0.45039737 0.4504806  0.44999993 0.450156
 0.45006508 0.4499824  0.45018685 0.450127   0.4503088  0.45026535
 0.45046997 0.4504712  0.45022762 0.4505681  0.45046324 0.4506557
 0.4507155  0.45066643 0.45105624 0.45064893 0.45061305 0.45042917
 0.4501147  0.45016378 0.44983736 0.44982412 0.44969836 0.44957027
 0.44967586 0.44962677 0.4497778  0.449836   0.44982877 0.44995734
 0.44992638 0.44997627 0.4498474  0.44987372 0.44997373 0.4498135
 0.45006698 0.44993007 0.44995338 0.45001405 0.44972378 0.44986984
 0.44955894 0.4493853  0.449394   0.4491593  0.4492041  0.44917557
 0.44921958 0.44922912 0.44909358 0.44914886 0.44918326 0.44934085
 0.44939768 0.44925225 0.44935063 0.44924468 0.44926906 0.4492666
 0.44919205 0.4493403  0.4491661  0.4491892  0.44907805 0.44899818
 0.44907475 0.44878918 0.44882953 0.44876525 0.4486554  0.44863954
 0.44864687 0.4487728  0.44860297 0.44865015 0.4487408  0.4486449
 0.44870752 0.44849935 0.4485061  0.44860214 0.448403   0.44859388
 0.44866604 0.4486503  0.44877136 0.44866112 0.44872147 0.44858932
 0.448674   0.4487097  0.448497   0.44856375 0.44847965 0.4485028
 0.4485657  0.44855988 0.44861534 0.4485586  0.4486444  0.44864115
 0.44862148 0.44874874 0.44869164 0.44869396 0.44865146 0.4485935
 0.44867653 0.44853747 0.44861856 0.44861606 0.44847605 0.44852725
 0.4483417  0.4483846  0.44837105 0.4482011  0.44829577 0.44823164
 0.44831738 0.4484527  0.44847617 0.4484914  0.4484069  0.44844395
 0.44851553 0.4485301  0.4484508  0.4482665  0.44822815 0.4479366
 0.4477228  0.44778103 0.44777504 0.44786024 0.44771895 0.44769573
 0.4477223  0.44748095 0.44747183 0.4473827  0.44733456 0.44734082
 0.44727483 0.44735762 0.44726866 0.4473468  0.44736978 0.44724253
 0.44736513 0.44708797 0.44687372 0.44675624 0.44656003 0.44665152
 0.44649085 0.44662914 0.4467614  0.4466929  0.4467286  0.44658104
 0.44670206 0.44657153 0.44637692 0.44641688 0.4462276  0.4462747
 0.44632053 0.44629362 0.44631228 0.44630733 0.44640842 0.4463583
 0.44636625 0.44615635 0.44581303 0.4458958  0.4457786  0.4456849
 0.44577563 0.44570544 0.44591916 0.44596153 0.4458407  0.44590068
 0.44575667 0.44570082 0.44570398 0.44574082 0.44576225 0.44561467
 0.44563258 0.445545   0.44551933 0.4456072  0.44555923 0.44561458
 0.44553417 0.4454228  0.44538206 0.4452376  0.4452226  0.44510162
 0.44518477 0.44530508 0.44530582 0.44542968 0.4453642  0.44541502
 0.44536018 0.44515803 0.44525585 0.44515124 0.44501156 0.44500092
 0.44492456 0.44488636 0.444876   0.44498083 0.44493896 0.4449123
 0.4449042  0.44466108 0.44473925 0.44470313 0.4446095  0.44470587
 0.44465578 0.4447726  0.44479632 0.44489235 0.44502196 0.4448068
 0.44480777 0.44474855 0.44469744 0.4448     0.44463798 0.44455904
 0.44452375 0.4444446  0.4445203  0.4445056  0.44446883 0.44444007
 0.4444437  0.4443291  0.44420967 0.4443199  0.44422725 0.4442933
 0.44452563 0.44452673 0.44465467 0.44460547 0.4447236  0.4448669
 0.44472575 0.44467676 0.44462427 0.4446943  0.44467044 0.44443116
 0.44444686 0.4444469  0.44442722 0.44443908 0.4443706  0.44437537
 0.444298   0.44443765 0.44449514 0.44437957 0.4444988  0.44446713
 0.4445121  0.44458163 0.44461548 0.44470736 0.4446786  0.44475108
 0.44467127 0.44450003 0.44452587 0.4444275  0.44440544 0.44431865
 0.44418848 0.44428733 0.44420615 0.44421333 0.44419673 0.44403964
 0.44412658 0.44404206 0.4439975  0.4438767  0.44373187 0.44370085
 0.4433858  0.4435116  0.44363543 0.44353136 0.44362688 0.44356146
 0.44348493 0.44337013 0.4433165  0.44323596 0.44303793 0.4429618
 0.44283062 0.44284728 0.4428669  0.44277832 0.44283682 0.44271666
 0.44262657 0.4425883  0.44239342 0.44237116 0.44227594 0.44226122
 0.4422891  0.442351   0.4425752  0.4425063  0.44248787 0.44251496
 0.44232675 0.44230896 0.44225326 0.44221613 0.4421462  0.44199386
 0.44197327 0.4418797  0.44183135 0.44181013 0.44176748 0.44178328
 0.4416709  0.44154438 0.4414175  0.4413138  0.44131687 0.44124782
 0.4413965  0.44152588 0.4416478  0.44172457 0.4415645  0.44167438
 0.44162086 0.4414523  0.44150797 0.44143787 0.4414026  0.44129765
 0.44127873 0.4413117  0.44115794 0.44121584 0.44120196 0.44117957
 0.44117886 0.44096687 0.44096994 0.440821   0.44081518 0.44092688
 0.4407876  0.44104543 0.44113994 0.44115388 0.44131693 0.44123635
 0.4412446  0.44111028 0.44095588 0.4409589  0.44084403 0.44080216
 0.44072852 0.44075954 0.44078404 0.44074783 0.44080618 0.44065288
 0.4407009  0.44061854 0.44045585 0.44057631 0.440448   0.44050848
 0.44051418 0.44050044 0.4408399  0.44079128 0.44079784 0.4408959
 0.44085005 0.44090208 0.4407719  0.44069892 0.44055942 0.44040543
 0.4404576  0.44039828 0.44051334 0.4405055  0.4403518  0.4403928
 0.44020754 0.44009593 0.4400824  0.4400151  0.44007385 0.44011942
 0.44035503 0.44048396 0.44063246 0.44073737 0.44068775 0.4407947
 0.44073924 0.4406573  0.44071448 0.44053254 0.44044796 0.4403001
 0.44027212 0.44040954 0.44036573 0.44038045 0.44027525 0.4403211
 0.44034427 0.44026828 0.44041517 0.44031313 0.4404192  0.44055983
 0.44044197 0.44059166 0.4406943  0.4407319  0.44078317 0.44072047
 0.44076592 0.4406835  0.44060808 0.44053695 0.44034058 0.44030836
 0.44019294 0.4401895  0.44021845 0.44022685 0.44030014 0.44024318
 0.44030708 0.44016537 0.44010434 0.44012997 0.43999925 0.44004297
 0.43971837 0.43961865 0.4399018  0.4398085  0.43981186 0.4397684
 0.43958166 0.4395427  0.43939823 0.43936923 0.4391886  0.43898112
 0.43895426 0.43883872 0.4388739  0.4388217  0.43884963 0.43886146
 0.4386061  0.43861246 0.43843803 0.4383631  0.43842486 0.43832624
 0.43847498 0.43843353 0.43855846 0.438692   0.43846962 0.43848023
 0.43830273 0.43814367 0.43807724 0.43780136 0.43770674 0.4375492
 0.43750614 0.43751147 0.4373638  0.43749136 0.43743423 0.43739164
 0.43732196 0.43702713 0.43713003 0.4370968  0.43719995 0.43734637
 0.4372679  0.43750408 0.43762583 0.43766266 0.4376004  0.4374634
 0.4374146  0.437139   0.43703791 0.43690547 0.43667495 0.43657622
 0.43636438 0.4363432  0.4362442  0.43615898 0.43617824 0.4359579
 0.4359287  0.43576077 0.43570906 0.43589595 0.43585396 0.4360546
 0.4360609  0.4361404  0.4364519  0.43640724 0.43652984 0.4364642
 0.43626612 0.43617433 0.43596295 0.43590835 0.4356073  0.43542346
 0.4353513  0.43521136 0.43540213 0.4352949  0.43530688 0.43531784
 0.43515262 0.43521145 0.43507987 0.43520564 0.43537644 0.43535838
 0.43554586 0.435495   0.43565497 0.43578842 0.43565968 0.43568152
 0.43547836 0.43534714 0.43516445 0.43497485 0.43495846 0.43464878
 0.43469402 0.4346405  0.43456972 0.4347832  0.43462706 0.4347744
 0.43469694 0.43458042 0.4347816  0.4346541  0.4348363  0.43491438
 0.43503004 0.4352806  0.43537304 0.43551424 0.43539783 0.43525654
 0.43522894 0.43493333 0.4349653  0.434682   0.43440607 0.43438423
 0.43416008 0.43444782 0.4343443  0.43439975 0.43461996 0.43451124
 0.4347875  0.43462303 0.4346943  0.43485522 0.43481553 0.43504593
 0.43483195 0.43492666 0.43515027 0.43508795 0.43514335 0.43482774
 0.43471286 0.43452185 0.4342095  0.43430522 0.4339229  0.43394595
 0.4338943  0.4338807  0.43411574 0.43395132 0.43432608 0.43425423
 0.43430606 0.43442622 0.43416804 0.43449953 0.43440735 0.43435812
 0.43416727 0.43374884 0.43398696 0.43362093 0.43338493 0.43321732
 0.43253523 0.43257427 0.43215156 0.43211973 0.4319871  0.4316394
 0.43200538 0.43176842 0.43217087 0.43228945 0.4322003  0.43253818
 0.43219474 0.43240634 0.4322352  0.43206915 0.43249232 0.43189245
 0.4320709  0.4315993  0.43145528 0.43157163 0.43061745 0.43096554
 0.43007472 0.43028212 0.42996475 0.4299097  0.43063223 0.430063
 0.43121424 0.43119505 0.4312818  0.4319404  0.43074638 0.43136206
 0.42976347 0.42900807 0.4286695  0.42732403 0.42287874 0.43183154]
