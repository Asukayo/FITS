Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j192_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17501
val 2441
test 5069
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10650009600.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 46.002081632614136
Epoch: 1, Steps: 68 | Train Loss: 0.7386838 Vali Loss: 0.4106281 Test Loss: 0.4845559
Validation loss decreased (inf --> 0.410628).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 46.626386642456055
Epoch: 2, Steps: 68 | Train Loss: 0.3781762 Vali Loss: 0.2375291 Test Loss: 0.2863654
Validation loss decreased (0.410628 --> 0.237529).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 43.38299775123596
Epoch: 3, Steps: 68 | Train Loss: 0.2410992 Vali Loss: 0.1670879 Test Loss: 0.2038323
Validation loss decreased (0.237529 --> 0.167088).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 42.98081684112549
Epoch: 4, Steps: 68 | Train Loss: 0.1857804 Vali Loss: 0.1407339 Test Loss: 0.1719054
Validation loss decreased (0.167088 --> 0.140734).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 44.6529803276062
Epoch: 5, Steps: 68 | Train Loss: 0.1651481 Vali Loss: 0.1315747 Test Loss: 0.1603250
Validation loss decreased (0.140734 --> 0.131575).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 46.12247943878174
Epoch: 6, Steps: 68 | Train Loss: 0.1576262 Vali Loss: 0.1286250 Test Loss: 0.1561821
Validation loss decreased (0.131575 --> 0.128625).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 39.951571464538574
Epoch: 7, Steps: 68 | Train Loss: 0.1549575 Vali Loss: 0.1277240 Test Loss: 0.1545652
Validation loss decreased (0.128625 --> 0.127724).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 42.187509298324585
Epoch: 8, Steps: 68 | Train Loss: 0.1537630 Vali Loss: 0.1273652 Test Loss: 0.1538141
Validation loss decreased (0.127724 --> 0.127365).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 43.39034843444824
Epoch: 9, Steps: 68 | Train Loss: 0.1530906 Vali Loss: 0.1270821 Test Loss: 0.1533692
Validation loss decreased (0.127365 --> 0.127082).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 41.289562702178955
Epoch: 10, Steps: 68 | Train Loss: 0.1525930 Vali Loss: 0.1266457 Test Loss: 0.1530723
Validation loss decreased (0.127082 --> 0.126646).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 43.08969855308533
Epoch: 11, Steps: 68 | Train Loss: 0.1523323 Vali Loss: 0.1266051 Test Loss: 0.1528628
Validation loss decreased (0.126646 --> 0.126605).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 46.99533772468567
Epoch: 12, Steps: 68 | Train Loss: 0.1520617 Vali Loss: 0.1264657 Test Loss: 0.1527170
Validation loss decreased (0.126605 --> 0.126466).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 45.219759464263916
Epoch: 13, Steps: 68 | Train Loss: 0.1518648 Vali Loss: 0.1263442 Test Loss: 0.1525604
Validation loss decreased (0.126466 --> 0.126344).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 40.90759086608887
Epoch: 14, Steps: 68 | Train Loss: 0.1515687 Vali Loss: 0.1262091 Test Loss: 0.1524248
Validation loss decreased (0.126344 --> 0.126209).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 35.81034326553345
Epoch: 15, Steps: 68 | Train Loss: 0.1515331 Vali Loss: 0.1261817 Test Loss: 0.1523570
Validation loss decreased (0.126209 --> 0.126182).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 42.82144117355347
Epoch: 16, Steps: 68 | Train Loss: 0.1513408 Vali Loss: 0.1260781 Test Loss: 0.1522643
Validation loss decreased (0.126182 --> 0.126078).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 41.81269359588623
Epoch: 17, Steps: 68 | Train Loss: 0.1512346 Vali Loss: 0.1260207 Test Loss: 0.1522056
Validation loss decreased (0.126078 --> 0.126021).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 47.32603144645691
Epoch: 18, Steps: 68 | Train Loss: 0.1511758 Vali Loss: 0.1261282 Test Loss: 0.1521452
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 45.25092315673828
Epoch: 19, Steps: 68 | Train Loss: 0.1510374 Vali Loss: 0.1261199 Test Loss: 0.1520883
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 42.232879400253296
Epoch: 20, Steps: 68 | Train Loss: 0.1510592 Vali Loss: 0.1258690 Test Loss: 0.1520416
Validation loss decreased (0.126021 --> 0.125869).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 40.6000497341156
Epoch: 21, Steps: 68 | Train Loss: 0.1509512 Vali Loss: 0.1256979 Test Loss: 0.1519975
Validation loss decreased (0.125869 --> 0.125698).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 40.18982648849487
Epoch: 22, Steps: 68 | Train Loss: 0.1508653 Vali Loss: 0.1258290 Test Loss: 0.1519691
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 40.44867181777954
Epoch: 23, Steps: 68 | Train Loss: 0.1508071 Vali Loss: 0.1258477 Test Loss: 0.1519428
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 40.98526620864868
Epoch: 24, Steps: 68 | Train Loss: 0.1508185 Vali Loss: 0.1261826 Test Loss: 0.1519024
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5069
mse:0.14979444444179535, mae:0.24525988101959229, rse:0.38481342792510986, corr:[0.46374464 0.46306872 0.46611908 0.46663222 0.4683113  0.46795493
 0.46940613 0.46862733 0.46915594 0.46867284 0.46847555 0.46845675
 0.46809646 0.46812072 0.4679673  0.46790394 0.46803755 0.4678797
 0.46792457 0.46750098 0.4675985  0.46767947 0.4676584  0.4682163
 0.46817732 0.4687421  0.46884665 0.46883392 0.468919   0.46849892
 0.46870914 0.46841463 0.4681656  0.46808165 0.46766958 0.46773383
 0.46758354 0.46739292 0.4673951  0.46722084 0.46722713 0.4671367
 0.4669822  0.4668103  0.46668464 0.46683216 0.46671572 0.4670458
 0.46719006 0.4672758  0.46758342 0.46734443 0.46743065 0.46727106
 0.46703556 0.46710438 0.4668434  0.46687362 0.46677488 0.46664646
 0.46676856 0.4665945  0.46669567 0.46674305 0.4667054  0.4668665
 0.4666822  0.4666529  0.46648088 0.46643278 0.46658075 0.466555
 0.46675372 0.46669525 0.46682006 0.4667462  0.46649837 0.46663558
 0.46637687 0.46636152 0.46634552 0.46611223 0.46619996 0.46606842
 0.46608815 0.4660558  0.4659801  0.46617934 0.46608675 0.46616665
 0.46609893 0.46587947 0.46587294 0.46563616 0.46576786 0.46578985
 0.46581417 0.4660229  0.46586886 0.4659652  0.46590063 0.4658068
 0.46587166 0.4656288  0.46574396 0.4656258  0.4654791  0.4656349
 0.4654582  0.4655916  0.46563897 0.4655702  0.46565643 0.4655341
 0.4656531  0.4656487  0.46567404 0.4657436  0.46570137 0.46595687
 0.46602228 0.4661548  0.4662466  0.46608457 0.46617466 0.46594334
 0.46589294 0.46593168 0.46570805 0.4657891  0.46556833 0.46564674
 0.46577018 0.46563673 0.46585593 0.46577922 0.46589038 0.46596882
 0.46590018 0.465853   0.4655541  0.4656243  0.46544254 0.46545032
 0.46558467 0.46546963 0.46565568 0.4654724  0.4653755  0.4653858
 0.4650828  0.46526363 0.46508822 0.4650522  0.46524206 0.46497068
 0.4653022  0.46517074 0.46521935 0.46538994 0.46520376 0.465513
 0.4652058  0.46512184 0.46488762 0.46457967 0.4646915  0.46441677
 0.46457145 0.4644747  0.4644713  0.4644431  0.46400577 0.46408784
 0.46370402 0.4636105  0.4637697  0.4633986  0.46369883 0.46354347
 0.46365616 0.4638623  0.46338126 0.46385485 0.46290165 0.46316752
 0.4623693  0.46173322 0.4618828  0.46137828 0.46208143 0.46423888]
