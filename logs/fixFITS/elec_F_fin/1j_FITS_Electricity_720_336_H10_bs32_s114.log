Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3083243520.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4651595
	speed: 0.2971s/iter; left time: 3996.8556s
	iters: 200, epoch: 1 | loss: 0.2729471
	speed: 0.2832s/iter; left time: 3781.0656s
Epoch: 1 cost time: 78.28471684455872
Epoch: 1, Steps: 271 | Train Loss: 0.4430832 Vali Loss: 0.1799658 Test Loss: 0.2166807
Validation loss decreased (inf --> 0.179966).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1935141
	speed: 0.9671s/iter; left time: 12746.3581s
	iters: 200, epoch: 2 | loss: 0.1692801
	speed: 0.2510s/iter; left time: 3282.5509s
Epoch: 2 cost time: 72.49284982681274
Epoch: 2, Steps: 271 | Train Loss: 0.1898662 Vali Loss: 0.1436833 Test Loss: 0.1692444
Validation loss decreased (0.179966 --> 0.143683).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1820504
	speed: 0.8954s/iter; left time: 11558.3465s
	iters: 200, epoch: 3 | loss: 0.1804622
	speed: 0.2350s/iter; left time: 3010.3138s
Epoch: 3 cost time: 66.02290272712708
Epoch: 3, Steps: 271 | Train Loss: 0.1753467 Vali Loss: 0.1423309 Test Loss: 0.1666203
Validation loss decreased (0.143683 --> 0.142331).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1836685
	speed: 0.7753s/iter; left time: 9797.6913s
	iters: 200, epoch: 4 | loss: 0.1566189
	speed: 0.1591s/iter; left time: 1994.5780s
Epoch: 4 cost time: 47.35526633262634
Epoch: 4, Steps: 271 | Train Loss: 0.1740768 Vali Loss: 0.1420314 Test Loss: 0.1661115
Validation loss decreased (0.142331 --> 0.142031).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1706410
	speed: 0.6382s/iter; left time: 7892.9361s
	iters: 200, epoch: 5 | loss: 0.1775988
	speed: 0.2894s/iter; left time: 3549.9026s
Epoch: 5 cost time: 78.9580397605896
Epoch: 5, Steps: 271 | Train Loss: 0.1735367 Vali Loss: 0.1418787 Test Loss: 0.1658538
Validation loss decreased (0.142031 --> 0.141879).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1606413
	speed: 1.0930s/iter; left time: 13220.8881s
	iters: 200, epoch: 6 | loss: 0.1766655
	speed: 0.3209s/iter; left time: 3849.9100s
Epoch: 6 cost time: 88.15911841392517
Epoch: 6, Steps: 271 | Train Loss: 0.1732061 Vali Loss: 0.1416375 Test Loss: 0.1656918
Validation loss decreased (0.141879 --> 0.141637).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1571745
	speed: 1.0754s/iter; left time: 12716.2821s
	iters: 200, epoch: 7 | loss: 0.1689482
	speed: 0.3267s/iter; left time: 3830.3186s
Epoch: 7 cost time: 90.65042495727539
Epoch: 7, Steps: 271 | Train Loss: 0.1730239 Vali Loss: 0.1415992 Test Loss: 0.1656213
Validation loss decreased (0.141637 --> 0.141599).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1831798
	speed: 1.1062s/iter; left time: 12780.7870s
	iters: 200, epoch: 8 | loss: 0.1860490
	speed: 0.3226s/iter; left time: 3694.8531s
Epoch: 8 cost time: 90.35840487480164
Epoch: 8, Steps: 271 | Train Loss: 0.1728539 Vali Loss: 0.1415271 Test Loss: 0.1656570
Validation loss decreased (0.141599 --> 0.141527).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1667543
	speed: 1.0635s/iter; left time: 11999.6960s
	iters: 200, epoch: 9 | loss: 0.1695793
	speed: 0.3095s/iter; left time: 3460.7803s
Epoch: 9 cost time: 85.94736552238464
Epoch: 9, Steps: 271 | Train Loss: 0.1727687 Vali Loss: 0.1415166 Test Loss: 0.1654979
Validation loss decreased (0.141527 --> 0.141517).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1872940
	speed: 1.0540s/iter; left time: 11606.7387s
	iters: 200, epoch: 10 | loss: 0.1754482
	speed: 0.2900s/iter; left time: 3164.3378s
Epoch: 10 cost time: 80.68980646133423
Epoch: 10, Steps: 271 | Train Loss: 0.1727068 Vali Loss: 0.1415678 Test Loss: 0.1654648
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1644669
	speed: 0.9988s/iter; left time: 10727.6429s
	iters: 200, epoch: 11 | loss: 0.1715447
	speed: 0.3035s/iter; left time: 3229.0848s
Epoch: 11 cost time: 83.26271080970764
Epoch: 11, Steps: 271 | Train Loss: 0.1726537 Vali Loss: 0.1417061 Test Loss: 0.1655086
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1795303
	speed: 1.1302s/iter; left time: 11832.7417s
	iters: 200, epoch: 12 | loss: 0.1828772
	speed: 0.3356s/iter; left time: 3480.5715s
Epoch: 12 cost time: 93.15216732025146
Epoch: 12, Steps: 271 | Train Loss: 0.1725897 Vali Loss: 0.1414421 Test Loss: 0.1654681
Validation loss decreased (0.141517 --> 0.141442).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1874043
	speed: 1.0778s/iter; left time: 10992.8637s
	iters: 200, epoch: 13 | loss: 0.1852631
	speed: 0.3108s/iter; left time: 3138.2914s
Epoch: 13 cost time: 85.63439154624939
Epoch: 13, Steps: 271 | Train Loss: 0.1725540 Vali Loss: 0.1414631 Test Loss: 0.1654341
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1525115
	speed: 1.0665s/iter; left time: 10588.6710s
	iters: 200, epoch: 14 | loss: 0.1653229
	speed: 0.3144s/iter; left time: 3089.7157s
Epoch: 14 cost time: 85.85175609588623
Epoch: 14, Steps: 271 | Train Loss: 0.1725124 Vali Loss: 0.1412003 Test Loss: 0.1654587
Validation loss decreased (0.141442 --> 0.141200).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1719143
	speed: 1.0824s/iter; left time: 10452.8835s
	iters: 200, epoch: 15 | loss: 0.1709081
	speed: 0.3054s/iter; left time: 2918.5289s
Epoch: 15 cost time: 86.08708953857422
Epoch: 15, Steps: 271 | Train Loss: 0.1724768 Vali Loss: 0.1414819 Test Loss: 0.1654000
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1764101
	speed: 1.0243s/iter; left time: 9613.8294s
	iters: 200, epoch: 16 | loss: 0.1741727
	speed: 0.2640s/iter; left time: 2451.0840s
Epoch: 16 cost time: 76.36610054969788
Epoch: 16, Steps: 271 | Train Loss: 0.1724612 Vali Loss: 0.1414896 Test Loss: 0.1653706
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1801510
	speed: 1.0598s/iter; left time: 9660.4379s
	iters: 200, epoch: 17 | loss: 0.1815139
	speed: 0.3352s/iter; left time: 3021.8203s
Epoch: 17 cost time: 92.88734173774719
Epoch: 17, Steps: 271 | Train Loss: 0.1724109 Vali Loss: 0.1413022 Test Loss: 0.1653488
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.16535930335521698, mae:0.2608308494091034, rse:0.40472090244293213, corr:[0.4591664  0.46125606 0.46170512 0.46313724 0.46309826 0.46383587
 0.46383563 0.4639016  0.4638709  0.46375853 0.46375862 0.46323818
 0.46331128 0.46323594 0.46313608 0.46333614 0.46333468 0.46324286
 0.46339682 0.46331617 0.4631315  0.46310523 0.46332923 0.46342948
 0.46374813 0.46400043 0.4640772  0.46422142 0.46399513 0.46399176
 0.46376386 0.46361765 0.46351725 0.4632657  0.4631623  0.46324015
 0.4634409  0.4632832  0.463261   0.4632871  0.4632454  0.46335945
 0.46321958 0.4629559  0.46292365 0.46281502 0.4628834  0.46303114
 0.46295303 0.4632075  0.4633013  0.4632409  0.46321398 0.46301773
 0.46296766 0.46294177 0.46295834 0.4627809  0.46264595 0.46265978
 0.46259758 0.46261278 0.46263608 0.46267536 0.4627234  0.4627078
 0.46264675 0.46245992 0.46222708 0.46218666 0.4622092  0.46229458
 0.46236223 0.46246517 0.46239376 0.4622377  0.46229514 0.46207723
 0.4619197  0.46181047 0.46188274 0.46205756 0.46183294 0.46185717
 0.461827   0.4616385  0.46160284 0.46158987 0.46165413 0.46163592
 0.46169245 0.46168545 0.4615275  0.46162775 0.4618344  0.46195403
 0.46202505 0.46211052 0.46221977 0.46211544 0.46196204 0.46185234
 0.46179587 0.4618888  0.46176898 0.46173704 0.46185365 0.46187854
 0.4618055  0.4616121  0.46174663 0.46182433 0.46162516 0.46163633
 0.461608   0.46146235 0.46146482 0.4615666  0.46163017 0.46177682
 0.4620794  0.46218622 0.46223128 0.4622253  0.4620832  0.46204656
 0.46203807 0.4618442  0.46188974 0.46191332 0.46182337 0.46183395
 0.46173334 0.4616529  0.4617873  0.4618879  0.4617367  0.46174696
 0.461762   0.4614874  0.4612919  0.46125388 0.46126625 0.46116537
 0.46116176 0.4612709  0.46139765 0.4614174  0.4613585  0.4613358
 0.46135187 0.461367   0.46131393 0.46129254 0.46114773 0.4611521
 0.46120295 0.46105212 0.46115732 0.4613155  0.4611452  0.46114227
 0.46122065 0.46094146 0.46091366 0.4609715  0.4609002  0.46085784
 0.46079868 0.46095634 0.4609109  0.46069464 0.46069485 0.46053785
 0.46036366 0.46044758 0.4604411  0.46026996 0.46014804 0.46000084
 0.4598277  0.4596724  0.45957622 0.4595376  0.4593332  0.45923743
 0.4592155  0.45898807 0.45882565 0.45885462 0.45883596 0.45883927
 0.45897016 0.45909268 0.45915133 0.4590925  0.45922828 0.45926952
 0.45915473 0.4591148  0.45882094 0.45872706 0.45877016 0.45862812
 0.45861825 0.45862204 0.45862982 0.4586207  0.45851693 0.4584281
 0.45827815 0.45811066 0.4579745  0.45784864 0.45793077 0.45811048
 0.45825922 0.45851347 0.45863655 0.45864505 0.458705   0.45860386
 0.45838004 0.45824766 0.45836934 0.45844385 0.45823506 0.4581279
 0.45824912 0.45823318 0.45814154 0.45823318 0.45817    0.45812023
 0.45799872 0.4576479  0.45760635 0.4575568  0.4575723  0.4576867
 0.45775023 0.45792893 0.45802492 0.4580547  0.45802465 0.4579658
 0.45797384 0.4578324  0.45763344 0.4576825  0.45760557 0.45757818
 0.45760816 0.45741457 0.45737955 0.4573629  0.45748794 0.4574617
 0.45744726 0.45755196 0.4573895  0.45747647 0.45751703 0.45756245
 0.45768142 0.45782715 0.45793492 0.4577908  0.4578043  0.45776564
 0.45760736 0.45755917 0.45748115 0.45739195 0.45733404 0.4573455
 0.45728222 0.45714483 0.457263   0.45710337 0.45700535 0.45725444
 0.45720065 0.45724708 0.4571374  0.45722342 0.45752993 0.45754802
 0.45784032 0.45791355 0.45794964 0.45787084 0.4577017  0.45770484
 0.45760944 0.45749086 0.45752773 0.4572497  0.4571855  0.45734745
 0.45716694 0.45703483 0.45694393 0.4572531  0.4570941  0.45690963
 0.45686647 0.45649645 0.4566497  0.45655173 0.45667347 0.45665994
 0.45661235 0.45680714 0.45661554 0.4565507  0.4564283  0.45627853
 0.4562495  0.45615435 0.4561784  0.45620468 0.4561218  0.4563378
 0.4561055  0.4563347  0.45622936 0.45621362 0.4565049  0.45620674
 0.4563802  0.45571792 0.4559457  0.45571128 0.45601377 0.45604697]
