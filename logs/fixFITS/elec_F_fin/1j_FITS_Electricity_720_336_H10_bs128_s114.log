Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12332974080.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 47.49908447265625
Epoch: 1, Steps: 67 | Train Loss: 0.7579151 Vali Loss: 0.4483990 Test Loss: 0.5337711
Validation loss decreased (inf --> 0.448399).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 52.425904989242554
Epoch: 2, Steps: 67 | Train Loss: 0.4474816 Vali Loss: 0.3031331 Test Loss: 0.3668419
Validation loss decreased (0.448399 --> 0.303133).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 49.7475905418396
Epoch: 3, Steps: 67 | Train Loss: 0.3211653 Vali Loss: 0.2266790 Test Loss: 0.2776435
Validation loss decreased (0.303133 --> 0.226679).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 47.128440856933594
Epoch: 4, Steps: 67 | Train Loss: 0.2524606 Vali Loss: 0.1855205 Test Loss: 0.2287565
Validation loss decreased (0.226679 --> 0.185520).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 50.12980890274048
Epoch: 5, Steps: 67 | Train Loss: 0.2154208 Vali Loss: 0.1635158 Test Loss: 0.2023173
Validation loss decreased (0.185520 --> 0.163516).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 49.80633068084717
Epoch: 6, Steps: 67 | Train Loss: 0.1957766 Vali Loss: 0.1526732 Test Loss: 0.1883774
Validation loss decreased (0.163516 --> 0.152673).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 47.48566150665283
Epoch: 7, Steps: 67 | Train Loss: 0.1855807 Vali Loss: 0.1471854 Test Loss: 0.1811303
Validation loss decreased (0.152673 --> 0.147185).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 47.45511341094971
Epoch: 8, Steps: 67 | Train Loss: 0.1803912 Vali Loss: 0.1450160 Test Loss: 0.1773730
Validation loss decreased (0.147185 --> 0.145016).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 50.9991455078125
Epoch: 9, Steps: 67 | Train Loss: 0.1776435 Vali Loss: 0.1433432 Test Loss: 0.1753727
Validation loss decreased (0.145016 --> 0.143343).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 48.743084192276
Epoch: 10, Steps: 67 | Train Loss: 0.1762548 Vali Loss: 0.1430548 Test Loss: 0.1742632
Validation loss decreased (0.143343 --> 0.143055).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 45.93360090255737
Epoch: 11, Steps: 67 | Train Loss: 0.1753985 Vali Loss: 0.1425179 Test Loss: 0.1736106
Validation loss decreased (0.143055 --> 0.142518).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 43.69623279571533
Epoch: 12, Steps: 67 | Train Loss: 0.1748551 Vali Loss: 0.1422421 Test Loss: 0.1731914
Validation loss decreased (0.142518 --> 0.142242).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 44.65731334686279
Epoch: 13, Steps: 67 | Train Loss: 0.1744881 Vali Loss: 0.1422907 Test Loss: 0.1729169
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 48.77547597885132
Epoch: 14, Steps: 67 | Train Loss: 0.1743967 Vali Loss: 0.1419643 Test Loss: 0.1727142
Validation loss decreased (0.142242 --> 0.141964).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 47.199378490448
Epoch: 15, Steps: 67 | Train Loss: 0.1741234 Vali Loss: 0.1417887 Test Loss: 0.1725589
Validation loss decreased (0.141964 --> 0.141789).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 54.14512801170349
Epoch: 16, Steps: 67 | Train Loss: 0.1739246 Vali Loss: 0.1416652 Test Loss: 0.1724528
Validation loss decreased (0.141789 --> 0.141665).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 56.77856659889221
Epoch: 17, Steps: 67 | Train Loss: 0.1736964 Vali Loss: 0.1419366 Test Loss: 0.1723296
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 51.06683969497681
Epoch: 18, Steps: 67 | Train Loss: 0.1736173 Vali Loss: 0.1421506 Test Loss: 0.1722588
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 55.294206619262695
Epoch: 19, Steps: 67 | Train Loss: 0.1735316 Vali Loss: 0.1415126 Test Loss: 0.1721914
Validation loss decreased (0.141665 --> 0.141513).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 52.240795850753784
Epoch: 20, Steps: 67 | Train Loss: 0.1734211 Vali Loss: 0.1418217 Test Loss: 0.1721209
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 77.61952209472656
Epoch: 21, Steps: 67 | Train Loss: 0.1732778 Vali Loss: 0.1420736 Test Loss: 0.1720670
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 74.05001330375671
Epoch: 22, Steps: 67 | Train Loss: 0.1734263 Vali Loss: 0.1415606 Test Loss: 0.1720094
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.1658264547586441, mae:0.2616764307022095, rse:0.4052921533584595, corr:[0.4583438  0.4570463  0.46108145 0.46135744 0.4636798  0.46301305
 0.4649641  0.4638596  0.4646467  0.46407065 0.46388498 0.46392637
 0.46341166 0.46360996 0.46344912 0.4634281  0.46353796 0.4633513
 0.46349406 0.46316314 0.46310887 0.46318403 0.46302268 0.46365333
 0.46352515 0.464167   0.46442854 0.46431303 0.46458375 0.4640665
 0.46424636 0.4638629  0.4635953  0.46361583 0.4631877  0.46333945
 0.46316612 0.46316683 0.4633133  0.46317264 0.46333915 0.46330193
 0.46322876 0.46312338 0.4629277  0.4631006  0.46297154 0.4632262
 0.46343926 0.4634851  0.46391106 0.46356514 0.46362063 0.46350113
 0.46321896 0.4633328  0.4629175  0.4629772  0.4628728  0.46271205
 0.4628351  0.46266726 0.4628178  0.462848   0.4628087  0.4629214
 0.46270403 0.46262136 0.46246985 0.46239188 0.46254408 0.46245375
 0.46273428 0.46274558 0.46280304 0.4628653  0.46256152 0.46270284
 0.46247265 0.46239582 0.4623511  0.46209872 0.46224853 0.46203303
 0.46206945 0.46216577 0.4620962  0.4622737  0.46212688 0.46212232
 0.46216592 0.46203136 0.46215373 0.46202606 0.46209034 0.4621873
 0.46213728 0.46245757 0.46235025 0.46241146 0.46239036 0.46218944
 0.46227637 0.4620035  0.46206465 0.46196666 0.46185282 0.46201572
 0.46182895 0.46191084 0.46191347 0.4619167  0.46199116 0.46190238
 0.46205673 0.4619028  0.4617776  0.46181092 0.4616467  0.46192858
 0.462011   0.46208155 0.462277   0.46208796 0.46221492 0.46201545
 0.461896   0.46196967 0.46175075 0.46188098 0.46173486 0.46171102
 0.4618051  0.46167758 0.46186343 0.46179044 0.46183252 0.4619325
 0.46180415 0.4618038  0.46156654 0.46150875 0.46145406 0.46137378
 0.46157613 0.46139815 0.4614907  0.46152258 0.46137285 0.46149975
 0.46125278 0.46125036 0.46126285 0.46115324 0.4612672  0.46106327
 0.46117464 0.4611952  0.461139   0.4613313  0.46118265 0.4612997
 0.46126834 0.46105596 0.46106613 0.46085152 0.46093342 0.46080673
 0.4606847  0.46087077 0.46075022 0.4607597  0.46062312 0.46047014
 0.460467   0.4602227  0.46025658 0.46006286 0.45988783 0.45989192
 0.45969263 0.45977622 0.45964205 0.45960617 0.45961416 0.45940349
 0.45939448 0.45916235 0.4590919  0.4590366  0.4589032  0.45910487
 0.4589858  0.45916933 0.45932353 0.45919064 0.45925975 0.4590238
 0.45896342 0.4588524  0.45861086 0.4586731  0.45847502 0.45850113
 0.45847818 0.45837635 0.45856157 0.4584049  0.4585068  0.4585286
 0.4583994  0.45842695 0.45812416 0.4581827  0.4581546  0.45818573
 0.45840672 0.45840675 0.45871162 0.45861125 0.45852238 0.45861763
 0.4583703  0.45849016 0.4583611  0.45823634 0.45830408 0.45806602
 0.45822293 0.4581885  0.4581326  0.45827574 0.458107   0.4582783
 0.45812476 0.457892   0.45791265 0.4576871  0.4579179  0.45786905
 0.45796934 0.4582197  0.4581742  0.45833015 0.45806754 0.4580681
 0.45812336 0.45791647 0.45800796 0.45771086 0.45770925 0.45772597
 0.45757067 0.4577994  0.45764685 0.45770335 0.45769712 0.45757473
 0.45778933 0.45757324 0.45769823 0.4577239  0.45763588 0.45796114
 0.45784527 0.45811725 0.45811918 0.45795757 0.45808697 0.45780358
 0.457885   0.45770803 0.45755336 0.45762628 0.4573288  0.45754355
 0.4574625  0.4574     0.45758086 0.45743784 0.4576536  0.45749858
 0.4575351  0.4575772  0.45731503 0.4575644  0.45744896 0.45770845
 0.45793507 0.45779356 0.4580441  0.45774528 0.45779064 0.4577168
 0.4573994  0.4575952  0.4572389  0.45731586 0.45724922 0.4570707
 0.45743823 0.45713317 0.45737776 0.4573152  0.45720142 0.45748755
 0.4569915  0.4570653  0.45671985 0.45658866 0.45681632 0.4564921
 0.45682275 0.45649767 0.45656386 0.45656362 0.45610297 0.4563692
 0.4558572  0.45603678 0.4559911  0.45576373 0.45633057 0.45584372
 0.45640597 0.45627207 0.45584545 0.4563784  0.45510066 0.45583594
 0.45444092 0.454378   0.4538809  0.4537703  0.45317683 0.45745414]
