Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6166487040.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4375554
	speed: 0.4561s/iter; left time: 3033.3520s
Epoch: 1 cost time: 60.42111802101135
Epoch: 1, Steps: 135 | Train Loss: 0.6036354 Vali Loss: 0.3026169 Test Loss: 0.3629843
Validation loss decreased (inf --> 0.302617).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2461328
	speed: 0.9035s/iter; left time: 5886.9881s
Epoch: 2 cost time: 55.313273191452026
Epoch: 2, Steps: 135 | Train Loss: 0.2828202 Vali Loss: 0.1799903 Test Loss: 0.2189052
Validation loss decreased (0.302617 --> 0.179990).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1946849
	speed: 0.9154s/iter; left time: 5841.3914s
Epoch: 3 cost time: 55.329461336135864
Epoch: 3, Steps: 135 | Train Loss: 0.2005616 Vali Loss: 0.1498224 Test Loss: 0.1805231
Validation loss decreased (0.179990 --> 0.149822).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1685213
	speed: 0.9343s/iter; left time: 5835.5492s
Epoch: 4 cost time: 54.24667048454285
Epoch: 4, Steps: 135 | Train Loss: 0.1804360 Vali Loss: 0.1436748 Test Loss: 0.1716485
Validation loss decreased (0.149822 --> 0.143675).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1682100
	speed: 0.9306s/iter; left time: 5686.6217s
Epoch: 5 cost time: 58.70360255241394
Epoch: 5, Steps: 135 | Train Loss: 0.1760222 Vali Loss: 0.1423057 Test Loss: 0.1694778
Validation loss decreased (0.143675 --> 0.142306).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1658244
	speed: 1.0289s/iter; left time: 6148.5793s
Epoch: 6 cost time: 61.79373264312744
Epoch: 6, Steps: 135 | Train Loss: 0.1747424 Vali Loss: 0.1422069 Test Loss: 0.1687456
Validation loss decreased (0.142306 --> 0.142207).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1840975
	speed: 0.9542s/iter; left time: 5573.6345s
Epoch: 7 cost time: 59.09368872642517
Epoch: 7, Steps: 135 | Train Loss: 0.1741294 Vali Loss: 0.1420715 Test Loss: 0.1683385
Validation loss decreased (0.142207 --> 0.142071).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1722383
	speed: 0.9962s/iter; left time: 5684.2767s
Epoch: 8 cost time: 59.57091665267944
Epoch: 8, Steps: 135 | Train Loss: 0.1737572 Vali Loss: 0.1422198 Test Loss: 0.1681416
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1764408
	speed: 0.9674s/iter; left time: 5389.3439s
Epoch: 9 cost time: 59.144678354263306
Epoch: 9, Steps: 135 | Train Loss: 0.1735151 Vali Loss: 0.1416550 Test Loss: 0.1679889
Validation loss decreased (0.142071 --> 0.141655).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1922245
	speed: 0.9750s/iter; left time: 5300.3374s
Epoch: 10 cost time: 58.860480546951294
Epoch: 10, Steps: 135 | Train Loss: 0.1733677 Vali Loss: 0.1416959 Test Loss: 0.1679059
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1644817
	speed: 0.9311s/iter; left time: 4935.5455s
Epoch: 11 cost time: 57.583810329437256
Epoch: 11, Steps: 135 | Train Loss: 0.1731429 Vali Loss: 0.1415561 Test Loss: 0.1677886
Validation loss decreased (0.141655 --> 0.141556).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1826963
	speed: 0.9560s/iter; left time: 4938.7765s
Epoch: 12 cost time: 58.14289164543152
Epoch: 12, Steps: 135 | Train Loss: 0.1730568 Vali Loss: 0.1417613 Test Loss: 0.1677494
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1843881
	speed: 0.9176s/iter; left time: 4616.5013s
Epoch: 13 cost time: 54.084413290023804
Epoch: 13, Steps: 135 | Train Loss: 0.1729579 Vali Loss: 0.1415920 Test Loss: 0.1677001
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1804338
	speed: 0.8831s/iter; left time: 4323.7031s
Epoch: 14 cost time: 53.473236322402954
Epoch: 14, Steps: 135 | Train Loss: 0.1728174 Vali Loss: 0.1413099 Test Loss: 0.1676376
Validation loss decreased (0.141556 --> 0.141310).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1792087
	speed: 0.9789s/iter; left time: 4660.6783s
Epoch: 15 cost time: 61.75850462913513
Epoch: 15, Steps: 135 | Train Loss: 0.1728493 Vali Loss: 0.1413179 Test Loss: 0.1676227
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1850917
	speed: 0.9482s/iter; left time: 4386.2278s
Epoch: 16 cost time: 56.6871554851532
Epoch: 16, Steps: 135 | Train Loss: 0.1726217 Vali Loss: 0.1411728 Test Loss: 0.1675599
Validation loss decreased (0.141310 --> 0.141173).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1696152
	speed: 0.9181s/iter; left time: 4123.2652s
Epoch: 17 cost time: 56.9416024684906
Epoch: 17, Steps: 135 | Train Loss: 0.1726225 Vali Loss: 0.1415831 Test Loss: 0.1675584
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1635439
	speed: 0.9460s/iter; left time: 4120.9858s
Epoch: 18 cost time: 59.31410455703735
Epoch: 18, Steps: 135 | Train Loss: 0.1726266 Vali Loss: 0.1415569 Test Loss: 0.1675245
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1710525
	speed: 0.9179s/iter; left time: 3874.4866s
Epoch: 19 cost time: 56.847625970840454
Epoch: 19, Steps: 135 | Train Loss: 0.1725815 Vali Loss: 0.1414152 Test Loss: 0.1675406
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.16533632576465607, mae:0.26085925102233887, rse:0.40469276905059814, corr:[0.4591811  0.4594842  0.4613378  0.46237716 0.4631404  0.46338832
 0.46423355 0.46358097 0.46401125 0.46348462 0.46350253 0.46341926
 0.46344927 0.46330115 0.46340838 0.46329403 0.46344915 0.4634289
 0.46339586 0.46316427 0.46309924 0.46314096 0.46305442 0.46346873
 0.46350202 0.46402535 0.46405306 0.4641031  0.46403807 0.46386337
 0.4638877  0.46359378 0.46352297 0.4634146  0.46326977 0.46319982
 0.4632466  0.46317837 0.46329138 0.46330065 0.46331528 0.4633683
 0.46324572 0.46315178 0.4629623  0.4630635  0.4630023  0.46323067
 0.46331373 0.46342093 0.46360868 0.4634689  0.46350008 0.46327922
 0.46324205 0.46311834 0.46285868 0.4628122  0.4628166  0.46273416
 0.46279153 0.4627845  0.46275717 0.46291775 0.4627641  0.4628721
 0.46275985 0.46249682 0.46245125 0.46235716 0.46231857 0.46229342
 0.46238863 0.46243355 0.46262327 0.46246353 0.46237367 0.46238232
 0.46227735 0.46221176 0.46206537 0.46212143 0.46206492 0.46200007
 0.46203122 0.46208403 0.462017   0.4621172  0.46213305 0.46204054
 0.4621841  0.46200684 0.46199736 0.46197203 0.46199045 0.46205142
 0.46209553 0.46219406 0.46218124 0.46226898 0.46212313 0.46206558
 0.4620471  0.46197334 0.4619256  0.46189556 0.46195143 0.46191448
 0.46182382 0.46179765 0.46185187 0.46178707 0.46181044 0.46175256
 0.46179876 0.46166325 0.46151316 0.46169057 0.46164176 0.46185753
 0.4619657  0.46207377 0.46218047 0.4621058  0.46205524 0.4619221
 0.46196258 0.46184722 0.46176884 0.46181402 0.4617525  0.46165946
 0.4616882  0.46156603 0.46163163 0.46183154 0.46168017 0.46174896
 0.46167192 0.46153197 0.46141413 0.4611869  0.4611463  0.46116504
 0.46120584 0.46122634 0.4614655  0.461423   0.46130705 0.46130076
 0.46120065 0.46117374 0.4610995  0.46103117 0.4610903  0.46105212
 0.46095714 0.46108425 0.46104437 0.4611343  0.46115187 0.46111244
 0.4611766  0.4608892  0.46085823 0.46078548 0.46065155 0.46062407
 0.46061233 0.46060088 0.46067446 0.46067628 0.4604396  0.4603738
 0.46029437 0.46015182 0.4601311  0.46005473 0.45986724 0.45978102
 0.45979285 0.45969617 0.4596123  0.45959145 0.45953268 0.45936903
 0.45928603 0.45905176 0.45879897 0.45882532 0.4588288  0.45891607
 0.4589662  0.45912448 0.45916587 0.45911488 0.45913467 0.45903036
 0.45894644 0.45883253 0.4587696  0.45875725 0.4586172  0.4584882
 0.4585181  0.4584891  0.4584942  0.45845848 0.45837328 0.45837814
 0.4583119  0.45828357 0.45808738 0.45805544 0.45812878 0.45813438
 0.45829135 0.45845485 0.45858222 0.45853335 0.45849413 0.45838773
 0.45832732 0.45835552 0.45824897 0.45818254 0.45823956 0.45812595
 0.45807406 0.45810416 0.45790604 0.45796382 0.4579272  0.45792013
 0.45793223 0.4576772  0.457573   0.4574827  0.45757565 0.45761
 0.4577083  0.45777076 0.4578318  0.4579123  0.4578882  0.4579275
 0.4578782  0.45785135 0.45773795 0.45766369 0.45764038 0.45764136
 0.4575681  0.45750007 0.45747417 0.45744586 0.4575363  0.45737308
 0.4574288  0.45743212 0.45746633 0.45762175 0.45760375 0.45769373
 0.45770064 0.45789945 0.4578583  0.4578034  0.45779982 0.45776403
 0.4578895  0.4576995  0.45749387 0.45755515 0.45747936 0.45738676
 0.45741504 0.45729843 0.45745608 0.45746633 0.4574568  0.457504
 0.45744696 0.45741755 0.4572513  0.45743597 0.45743185 0.45761803
 0.4577693  0.45771182 0.45779476 0.4576081  0.457645   0.45750025
 0.4574594  0.4573952  0.45719096 0.45722076 0.45716357 0.45696223
 0.45698342 0.45689854 0.4568756  0.45706734 0.4569429  0.45712063
 0.45691878 0.45687705 0.45682925 0.45663723 0.45674303 0.45654196
 0.45673308 0.45654628 0.45658126 0.45640528 0.45622733 0.4562099
 0.45596892 0.45598787 0.45575252 0.4557347  0.45591486 0.45583534
 0.45574358 0.45603636 0.4556624  0.45608425 0.45558903 0.45572978
 0.45531136 0.45493522 0.45534262 0.45499647 0.4554312  0.45701143]
