Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j192_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17501
val 2441
test 5069
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21300019200.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 46.08782887458801
Epoch: 1, Steps: 34 | Train Loss: 1.0740828 Vali Loss: 0.8520448 Test Loss: 0.9769711
Validation loss decreased (inf --> 0.852045).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 48.26642346382141
Epoch: 2, Steps: 34 | Train Loss: 0.8797387 Vali Loss: 0.7432978 Test Loss: 0.8554996
Validation loss decreased (0.852045 --> 0.743298).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 46.72243809700012
Epoch: 3, Steps: 34 | Train Loss: 0.7897852 Vali Loss: 0.6939239 Test Loss: 0.8003637
Validation loss decreased (0.743298 --> 0.693924).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 46.44316291809082
Epoch: 4, Steps: 34 | Train Loss: 0.7422088 Vali Loss: 0.6661687 Test Loss: 0.7677560
Validation loss decreased (0.693924 --> 0.666169).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 43.35473608970642
Epoch: 5, Steps: 34 | Train Loss: 0.7102022 Vali Loss: 0.6466655 Test Loss: 0.7454869
Validation loss decreased (0.666169 --> 0.646666).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 44.517794609069824
Epoch: 6, Steps: 34 | Train Loss: 0.6845457 Vali Loss: 0.6302772 Test Loss: 0.7268204
Validation loss decreased (0.646666 --> 0.630277).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 53.9663610458374
Epoch: 7, Steps: 34 | Train Loss: 0.6625743 Vali Loss: 0.6152992 Test Loss: 0.7089568
Validation loss decreased (0.630277 --> 0.615299).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 45.61685824394226
Epoch: 8, Steps: 34 | Train Loss: 0.6424031 Vali Loss: 0.6025953 Test Loss: 0.6951597
Validation loss decreased (0.615299 --> 0.602595).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 45.13933277130127
Epoch: 9, Steps: 34 | Train Loss: 0.6243351 Vali Loss: 0.5902368 Test Loss: 0.6814052
Validation loss decreased (0.602595 --> 0.590237).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 46.67319345474243
Epoch: 10, Steps: 34 | Train Loss: 0.6075493 Vali Loss: 0.5775963 Test Loss: 0.6674183
Validation loss decreased (0.590237 --> 0.577596).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 46.1716570854187
Epoch: 11, Steps: 34 | Train Loss: 0.5921659 Vali Loss: 0.5672888 Test Loss: 0.6556970
Validation loss decreased (0.577596 --> 0.567289).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 47.10722208023071
Epoch: 12, Steps: 34 | Train Loss: 0.5780345 Vali Loss: 0.5587813 Test Loss: 0.6456498
Validation loss decreased (0.567289 --> 0.558781).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 47.505812883377075
Epoch: 13, Steps: 34 | Train Loss: 0.5648780 Vali Loss: 0.5484474 Test Loss: 0.6339811
Validation loss decreased (0.558781 --> 0.548447).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 42.89075422286987
Epoch: 14, Steps: 34 | Train Loss: 0.5525954 Vali Loss: 0.5393595 Test Loss: 0.6239604
Validation loss decreased (0.548447 --> 0.539359).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 44.86695098876953
Epoch: 15, Steps: 34 | Train Loss: 0.5412292 Vali Loss: 0.5321299 Test Loss: 0.6155853
Validation loss decreased (0.539359 --> 0.532130).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 51.22474503517151
Epoch: 16, Steps: 34 | Train Loss: 0.5305469 Vali Loss: 0.5249293 Test Loss: 0.6069663
Validation loss decreased (0.532130 --> 0.524929).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 46.40305495262146
Epoch: 17, Steps: 34 | Train Loss: 0.5207046 Vali Loss: 0.5171296 Test Loss: 0.5991553
Validation loss decreased (0.524929 --> 0.517130).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 50.029335260391235
Epoch: 18, Steps: 34 | Train Loss: 0.5114662 Vali Loss: 0.5103616 Test Loss: 0.5915452
Validation loss decreased (0.517130 --> 0.510362).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 49.98940062522888
Epoch: 19, Steps: 34 | Train Loss: 0.5028322 Vali Loss: 0.5052355 Test Loss: 0.5850479
Validation loss decreased (0.510362 --> 0.505235).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 45.203280210494995
Epoch: 20, Steps: 34 | Train Loss: 0.4947212 Vali Loss: 0.4980779 Test Loss: 0.5776489
Validation loss decreased (0.505235 --> 0.498078).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 52.06241488456726
Epoch: 21, Steps: 34 | Train Loss: 0.4871758 Vali Loss: 0.4919039 Test Loss: 0.5712218
Validation loss decreased (0.498078 --> 0.491904).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 48.42943549156189
Epoch: 22, Steps: 34 | Train Loss: 0.4799971 Vali Loss: 0.4865312 Test Loss: 0.5650642
Validation loss decreased (0.491904 --> 0.486531).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 52.23297905921936
Epoch: 23, Steps: 34 | Train Loss: 0.4733983 Vali Loss: 0.4817105 Test Loss: 0.5595492
Validation loss decreased (0.486531 --> 0.481711).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 59.9968798160553
Epoch: 24, Steps: 34 | Train Loss: 0.4670048 Vali Loss: 0.4772323 Test Loss: 0.5542139
Validation loss decreased (0.481711 --> 0.477232).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 57.3598256111145
Epoch: 25, Steps: 34 | Train Loss: 0.4611515 Vali Loss: 0.4719056 Test Loss: 0.5491036
Validation loss decreased (0.477232 --> 0.471906).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 53.83619236946106
Epoch: 26, Steps: 34 | Train Loss: 0.4556014 Vali Loss: 0.4698856 Test Loss: 0.5457681
Validation loss decreased (0.471906 --> 0.469886).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 40.397435903549194
Epoch: 27, Steps: 34 | Train Loss: 0.4503964 Vali Loss: 0.4656406 Test Loss: 0.5409755
Validation loss decreased (0.469886 --> 0.465641).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 43.712133169174194
Epoch: 28, Steps: 34 | Train Loss: 0.4454365 Vali Loss: 0.4614520 Test Loss: 0.5363148
Validation loss decreased (0.465641 --> 0.461452).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 43.00631403923035
Epoch: 29, Steps: 34 | Train Loss: 0.4407770 Vali Loss: 0.4578792 Test Loss: 0.5329558
Validation loss decreased (0.461452 --> 0.457879).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 48.23798608779907
Epoch: 30, Steps: 34 | Train Loss: 0.4363363 Vali Loss: 0.4548354 Test Loss: 0.5291063
Validation loss decreased (0.457879 --> 0.454835).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 52.922873735427856
Epoch: 31, Steps: 34 | Train Loss: 0.4322299 Vali Loss: 0.4521660 Test Loss: 0.5259990
Validation loss decreased (0.454835 --> 0.452166).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 55.507712602615356
Epoch: 32, Steps: 34 | Train Loss: 0.4282914 Vali Loss: 0.4483516 Test Loss: 0.5216522
Validation loss decreased (0.452166 --> 0.448352).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 48.99550676345825
Epoch: 33, Steps: 34 | Train Loss: 0.4246208 Vali Loss: 0.4455140 Test Loss: 0.5184869
Validation loss decreased (0.448352 --> 0.445514).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 48.74455714225769
Epoch: 34, Steps: 34 | Train Loss: 0.4211189 Vali Loss: 0.4423851 Test Loss: 0.5161786
Validation loss decreased (0.445514 --> 0.442385).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 50.778287410736084
Epoch: 35, Steps: 34 | Train Loss: 0.4178920 Vali Loss: 0.4401677 Test Loss: 0.5133305
Validation loss decreased (0.442385 --> 0.440168).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 49.90715432167053
Epoch: 36, Steps: 34 | Train Loss: 0.4147888 Vali Loss: 0.4382185 Test Loss: 0.5105110
Validation loss decreased (0.440168 --> 0.438219).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 56.184930086135864
Epoch: 37, Steps: 34 | Train Loss: 0.4118520 Vali Loss: 0.4361126 Test Loss: 0.5084790
Validation loss decreased (0.438219 --> 0.436113).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 66.59331250190735
Epoch: 38, Steps: 34 | Train Loss: 0.4089179 Vali Loss: 0.4341512 Test Loss: 0.5057448
Validation loss decreased (0.436113 --> 0.434151).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 65.77270531654358
Epoch: 39, Steps: 34 | Train Loss: 0.4063882 Vali Loss: 0.4310425 Test Loss: 0.5031139
Validation loss decreased (0.434151 --> 0.431042).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 51.542428493499756
Epoch: 40, Steps: 34 | Train Loss: 0.4038669 Vali Loss: 0.4299183 Test Loss: 0.5013276
Validation loss decreased (0.431042 --> 0.429918).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 48.73279857635498
Epoch: 41, Steps: 34 | Train Loss: 0.4014336 Vali Loss: 0.4278991 Test Loss: 0.4992918
Validation loss decreased (0.429918 --> 0.427899).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 51.73346161842346
Epoch: 42, Steps: 34 | Train Loss: 0.3992010 Vali Loss: 0.4260373 Test Loss: 0.4970624
Validation loss decreased (0.427899 --> 0.426037).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 54.80374598503113
Epoch: 43, Steps: 34 | Train Loss: 0.3970725 Vali Loss: 0.4243050 Test Loss: 0.4955748
Validation loss decreased (0.426037 --> 0.424305).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 60.8978796005249
Epoch: 44, Steps: 34 | Train Loss: 0.3949791 Vali Loss: 0.4233356 Test Loss: 0.4938713
Validation loss decreased (0.424305 --> 0.423336).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 60.77783226966858
Epoch: 45, Steps: 34 | Train Loss: 0.3931442 Vali Loss: 0.4220547 Test Loss: 0.4921935
Validation loss decreased (0.423336 --> 0.422055).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 61.30930709838867
Epoch: 46, Steps: 34 | Train Loss: 0.3913026 Vali Loss: 0.4204924 Test Loss: 0.4907019
Validation loss decreased (0.422055 --> 0.420492).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 53.709901094436646
Epoch: 47, Steps: 34 | Train Loss: 0.3896207 Vali Loss: 0.4185944 Test Loss: 0.4888269
Validation loss decreased (0.420492 --> 0.418594).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 60.21745848655701
Epoch: 48, Steps: 34 | Train Loss: 0.3879748 Vali Loss: 0.4178429 Test Loss: 0.4879096
Validation loss decreased (0.418594 --> 0.417843).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 57.32579970359802
Epoch: 49, Steps: 34 | Train Loss: 0.3863555 Vali Loss: 0.4164133 Test Loss: 0.4863477
Validation loss decreased (0.417843 --> 0.416413).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 59.55090069770813
Epoch: 50, Steps: 34 | Train Loss: 0.3848846 Vali Loss: 0.4151066 Test Loss: 0.4850314
Validation loss decreased (0.416413 --> 0.415107).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 17501
val 2441
test 5069
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21300019200.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 51.6455237865448
Epoch: 1, Steps: 34 | Train Loss: 0.4075860 Vali Loss: 0.2772468 Test Loss: 0.3289414
Validation loss decreased (inf --> 0.277247).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 50.39881253242493
Epoch: 2, Steps: 34 | Train Loss: 0.2867484 Vali Loss: 0.2013021 Test Loss: 0.2425206
Validation loss decreased (0.277247 --> 0.201302).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 47.220991134643555
Epoch: 3, Steps: 34 | Train Loss: 0.2196823 Vali Loss: 0.1620438 Test Loss: 0.1965601
Validation loss decreased (0.201302 --> 0.162044).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 51.58403182029724
Epoch: 4, Steps: 34 | Train Loss: 0.1846203 Vali Loss: 0.1424543 Test Loss: 0.1729727
Validation loss decreased (0.162044 --> 0.142454).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 46.8730993270874
Epoch: 5, Steps: 34 | Train Loss: 0.1672232 Vali Loss: 0.1331528 Test Loss: 0.1614185
Validation loss decreased (0.142454 --> 0.133153).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 50.143394947052
Epoch: 6, Steps: 34 | Train Loss: 0.1586639 Vali Loss: 0.1282058 Test Loss: 0.1559612
Validation loss decreased (0.133153 --> 0.128206).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 49.91609263420105
Epoch: 7, Steps: 34 | Train Loss: 0.1548880 Vali Loss: 0.1275017 Test Loss: 0.1534559
Validation loss decreased (0.128206 --> 0.127502).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 47.86845111846924
Epoch: 8, Steps: 34 | Train Loss: 0.1530640 Vali Loss: 0.1261538 Test Loss: 0.1522857
Validation loss decreased (0.127502 --> 0.126154).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 50.205023765563965
Epoch: 9, Steps: 34 | Train Loss: 0.1521471 Vali Loss: 0.1261710 Test Loss: 0.1516874
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 51.61916446685791
Epoch: 10, Steps: 34 | Train Loss: 0.1516696 Vali Loss: 0.1262462 Test Loss: 0.1513534
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 54.3969943523407
Epoch: 11, Steps: 34 | Train Loss: 0.1514429 Vali Loss: 0.1261916 Test Loss: 0.1511775
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5069
mse:0.15137438476085663, mae:0.2490667700767517, rse:0.3868374824523926, corr:[0.46278018 0.46043766 0.46584538 0.4660958  0.468697   0.4682044
 0.47024253 0.4693492  0.46993604 0.46933073 0.46881342 0.46885818
 0.46804413 0.46805853 0.46755585 0.4675822  0.46765047 0.46755177
 0.46769857 0.4670382  0.46718624 0.46721312 0.4672195  0.4678226
 0.46783814 0.46853092 0.46888664 0.4689052  0.4691092  0.4685258
 0.4686735  0.46818754 0.46787566 0.46791166 0.46721113 0.46716693
 0.46683192 0.46687943 0.46671274 0.4665244  0.46671432 0.46638027
 0.46648782 0.4664011  0.4662757  0.46632624 0.4662045  0.4666365
 0.46691164 0.46699867 0.46742016 0.4671657  0.46718195 0.46700934
 0.46663338 0.46664417 0.46614322 0.46618715 0.46594974 0.46580634
 0.4659377  0.46572167 0.46595386 0.4659437  0.46603942 0.46610892
 0.4660832  0.4661603  0.46589383 0.46597216 0.4661314  0.4661476
 0.46651423 0.46640915 0.46637908 0.46650946 0.4661764  0.46618107
 0.46579656 0.4656935  0.465537   0.4651589  0.46531078 0.46503302
 0.46520972 0.46532187 0.46534348 0.46544763 0.46532848 0.465572
 0.46545118 0.4653245  0.46535465 0.46524063 0.46535328 0.4653815
 0.4654792  0.4657298  0.46549284 0.46553108 0.4653089  0.4650648
 0.46514723 0.46465477 0.46468782 0.46445113 0.4644218  0.46458387
 0.46438903 0.46474206 0.46475357 0.46493793 0.46494597 0.46490058
 0.46506315 0.4649186  0.46515265 0.4652638  0.4652532  0.4655926
 0.46571144 0.46571577 0.46567455 0.46542677 0.46547848 0.465083
 0.4651006  0.46498698 0.46468237 0.46486297 0.46460906 0.4648778
 0.46474487 0.46485615 0.46524853 0.46507728 0.46543235 0.4654394
 0.46546867 0.46544123 0.46514955 0.4651779  0.46501175 0.46498552
 0.46505022 0.46494192 0.4651619  0.46495593 0.46476856 0.46476355
 0.46442646 0.4645613  0.46428207 0.4643958  0.4644938  0.46445024
 0.46482238 0.46468422 0.46509415 0.46515888 0.4651631  0.4653683
 0.46502495 0.4650447  0.464602   0.4643605  0.46446645 0.46404588
 0.4641055  0.46385399 0.46384084 0.46382394 0.46324357 0.4633954
 0.46282732 0.46314913 0.46311414 0.46313217 0.46369857 0.46340016
 0.46411493 0.4639181  0.46366072 0.464058   0.4626945  0.4631512
 0.46173602 0.46113414 0.46092135 0.46025234 0.45893607 0.46427527]
