Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  24665948160.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 44.98431324958801
Epoch: 1, Steps: 33 | Train Loss: 0.9066778 Vali Loss: 0.5827028 Test Loss: 0.6842694
Validation loss decreased (inf --> 0.582703).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 48.23382592201233
Epoch: 2, Steps: 33 | Train Loss: 0.6131411 Vali Loss: 0.4513050 Test Loss: 0.5344718
Validation loss decreased (0.582703 --> 0.451305).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 47.77154088020325
Epoch: 3, Steps: 33 | Train Loss: 0.4948705 Vali Loss: 0.3715167 Test Loss: 0.4428833
Validation loss decreased (0.451305 --> 0.371517).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 51.85771131515503
Epoch: 4, Steps: 33 | Train Loss: 0.4153128 Vali Loss: 0.3144960 Test Loss: 0.3771832
Validation loss decreased (0.371517 --> 0.314496).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 49.80472445487976
Epoch: 5, Steps: 33 | Train Loss: 0.3576180 Vali Loss: 0.2720656 Test Loss: 0.3285456
Validation loss decreased (0.314496 --> 0.272066).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 50.180272579193115
Epoch: 6, Steps: 33 | Train Loss: 0.3145541 Vali Loss: 0.2406606 Test Loss: 0.2920579
Validation loss decreased (0.272066 --> 0.240661).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 51.09728980064392
Epoch: 7, Steps: 33 | Train Loss: 0.2819003 Vali Loss: 0.2175919 Test Loss: 0.2643542
Validation loss decreased (0.240661 --> 0.217592).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 48.330153703689575
Epoch: 8, Steps: 33 | Train Loss: 0.2577769 Vali Loss: 0.1995966 Test Loss: 0.2432458
Validation loss decreased (0.217592 --> 0.199597).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 48.302651166915894
Epoch: 9, Steps: 33 | Train Loss: 0.2390103 Vali Loss: 0.1860432 Test Loss: 0.2271000
Validation loss decreased (0.199597 --> 0.186043).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 51.005656480789185
Epoch: 10, Steps: 33 | Train Loss: 0.2243651 Vali Loss: 0.1755560 Test Loss: 0.2146950
Validation loss decreased (0.186043 --> 0.175556).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 52.12159490585327
Epoch: 11, Steps: 33 | Train Loss: 0.2134230 Vali Loss: 0.1680060 Test Loss: 0.2051514
Validation loss decreased (0.175556 --> 0.168006).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 54.59022259712219
Epoch: 12, Steps: 33 | Train Loss: 0.2050417 Vali Loss: 0.1622101 Test Loss: 0.1977962
Validation loss decreased (0.168006 --> 0.162210).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 49.36612010002136
Epoch: 13, Steps: 33 | Train Loss: 0.1986446 Vali Loss: 0.1581019 Test Loss: 0.1920886
Validation loss decreased (0.162210 --> 0.158102).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 50.38686418533325
Epoch: 14, Steps: 33 | Train Loss: 0.1939240 Vali Loss: 0.1544316 Test Loss: 0.1876502
Validation loss decreased (0.158102 --> 0.154432).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 47.719372034072876
Epoch: 15, Steps: 33 | Train Loss: 0.1898159 Vali Loss: 0.1515774 Test Loss: 0.1841787
Validation loss decreased (0.154432 --> 0.151577).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 47.551795959472656
Epoch: 16, Steps: 33 | Train Loss: 0.1866493 Vali Loss: 0.1494000 Test Loss: 0.1814480
Validation loss decreased (0.151577 --> 0.149400).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 45.75051784515381
Epoch: 17, Steps: 33 | Train Loss: 0.1842177 Vali Loss: 0.1483161 Test Loss: 0.1792946
Validation loss decreased (0.149400 --> 0.148316).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 54.05336093902588
Epoch: 18, Steps: 33 | Train Loss: 0.1826715 Vali Loss: 0.1470822 Test Loss: 0.1775875
Validation loss decreased (0.148316 --> 0.147082).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 41.42716145515442
Epoch: 19, Steps: 33 | Train Loss: 0.1811648 Vali Loss: 0.1459382 Test Loss: 0.1762158
Validation loss decreased (0.147082 --> 0.145938).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 42.73911190032959
Epoch: 20, Steps: 33 | Train Loss: 0.1799185 Vali Loss: 0.1448487 Test Loss: 0.1751278
Validation loss decreased (0.145938 --> 0.144849).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 37.136200189590454
Epoch: 21, Steps: 33 | Train Loss: 0.1789153 Vali Loss: 0.1443443 Test Loss: 0.1742399
Validation loss decreased (0.144849 --> 0.144344).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 33.740492820739746
Epoch: 22, Steps: 33 | Train Loss: 0.1782964 Vali Loss: 0.1441935 Test Loss: 0.1735085
Validation loss decreased (0.144344 --> 0.144194).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 34.84895467758179
Epoch: 23, Steps: 33 | Train Loss: 0.1777473 Vali Loss: 0.1437184 Test Loss: 0.1729089
Validation loss decreased (0.144194 --> 0.143718).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 35.3590612411499
Epoch: 24, Steps: 33 | Train Loss: 0.1772295 Vali Loss: 0.1437611 Test Loss: 0.1724153
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 32.71252965927124
Epoch: 25, Steps: 33 | Train Loss: 0.1765039 Vali Loss: 0.1432630 Test Loss: 0.1720135
Validation loss decreased (0.143718 --> 0.143263).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 35.431931257247925
Epoch: 26, Steps: 33 | Train Loss: 0.1762893 Vali Loss: 0.1432586 Test Loss: 0.1716635
Validation loss decreased (0.143263 --> 0.143259).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 31.99770736694336
Epoch: 27, Steps: 33 | Train Loss: 0.1761093 Vali Loss: 0.1433515 Test Loss: 0.1713779
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 35.95139670372009
Epoch: 28, Steps: 33 | Train Loss: 0.1758392 Vali Loss: 0.1426849 Test Loss: 0.1711245
Validation loss decreased (0.143259 --> 0.142685).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 43.17302894592285
Epoch: 29, Steps: 33 | Train Loss: 0.1756872 Vali Loss: 0.1428772 Test Loss: 0.1709108
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 42.54305934906006
Epoch: 30, Steps: 33 | Train Loss: 0.1755006 Vali Loss: 0.1424202 Test Loss: 0.1707333
Validation loss decreased (0.142685 --> 0.142420).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 44.03379535675049
Epoch: 31, Steps: 33 | Train Loss: 0.1751870 Vali Loss: 0.1425509 Test Loss: 0.1705703
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 43.478025913238525
Epoch: 32, Steps: 33 | Train Loss: 0.1752738 Vali Loss: 0.1425742 Test Loss: 0.1704354
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 48.6661639213562
Epoch: 33, Steps: 33 | Train Loss: 0.1749171 Vali Loss: 0.1426633 Test Loss: 0.1703122
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.16764123737812042, mae:0.2650928199291229, rse:0.4075038731098175, corr:[0.45664313 0.4530377  0.46096006 0.45990032 0.4638737  0.46274185
 0.4652536  0.46447712 0.46486217 0.46471342 0.46402323 0.46396625
 0.46342406 0.46321243 0.46313903 0.46298873 0.46302417 0.46301004
 0.46303633 0.46284273 0.46263775 0.46287912 0.46269062 0.46327284
 0.4633635  0.46379286 0.46437976 0.46412405 0.46457553 0.46415398
 0.46413592 0.4639466  0.46347103 0.46345335 0.46301615 0.46297732
 0.4628118  0.46275377 0.46285057 0.46279702 0.46284765 0.46284214
 0.46287546 0.46282053 0.462621   0.46271396 0.4627265  0.46285787
 0.46328115 0.46322632 0.4636796  0.4635567  0.46343428 0.46347663
 0.46306786 0.4630761  0.46270755 0.46258074 0.46243495 0.4622325
 0.46227553 0.46221712 0.46233985 0.462304   0.46235475 0.46242633
 0.46232268 0.46227586 0.4621569  0.46208316 0.46227413 0.4622296
 0.46242172 0.46258157 0.46251822 0.46269816 0.46239567 0.46241653
 0.4622169  0.4619618  0.46193954 0.46161997 0.46165392 0.46148098
 0.46148497 0.46151808 0.46149507 0.46160448 0.46153286 0.46165186
 0.4616952  0.4616262  0.46174657 0.4617341  0.46177244 0.46198267
 0.4619191  0.4621923  0.46216834 0.46209842 0.462143   0.46182877
 0.461867   0.4615753  0.46146533 0.46137354 0.4611938  0.4612591
 0.4611104  0.46124616 0.46125633 0.46130112 0.4613819  0.46136576
 0.46147627 0.46137497 0.4612975  0.4613952  0.46139458 0.46153387
 0.4617134  0.46171537 0.46191987 0.461749   0.4617788  0.46166658
 0.46144342 0.4614736  0.46123776 0.46124735 0.46112925 0.46110323
 0.46112505 0.46109375 0.46124193 0.4612125  0.46136448 0.46143156
 0.46138647 0.46135664 0.4611384  0.4610475  0.46105406 0.46097213
 0.46108335 0.4610521  0.4610911  0.4611724  0.46096614 0.4610009
 0.46077457 0.4606332  0.46058282 0.46045193 0.46048746 0.4603595
 0.46046084 0.46045604 0.46053132 0.4606581  0.4606044  0.46077156
 0.4607207  0.46057445 0.4605453  0.4603805  0.46037102 0.460302
 0.4601885  0.46034902 0.4602891  0.46025908 0.4601099  0.45984474
 0.4598093  0.45957935 0.4594797  0.45930666 0.45919827 0.45914432
 0.45901576 0.45912296 0.4590279  0.4590479  0.45899862 0.4588442
 0.45882612 0.45863023 0.4585551  0.45851675 0.45841938 0.4585377
 0.458543   0.45863754 0.4588219  0.45868796 0.45866615 0.45846367
 0.4583242  0.45822448 0.45802912 0.45802134 0.4578675  0.45793167
 0.45789623 0.45796525 0.45808887 0.45799795 0.45816654 0.45808125
 0.45806232 0.45804152 0.45781538 0.45786014 0.45782867 0.4578659
 0.4580535  0.45807818 0.45826247 0.45823523 0.45809516 0.45807034
 0.45780307 0.45782295 0.4576796  0.45761544 0.4575974  0.4574735
 0.4576924  0.4576175  0.45775518 0.45783287 0.45781347 0.45798373
 0.45779246 0.4577188  0.45763355 0.4575175  0.45767108 0.4576472
 0.45768878 0.45785975 0.45779768 0.45789438 0.45771158 0.45762068
 0.45756766 0.45736545 0.45742896 0.45722157 0.4573318  0.4572598
 0.4572623  0.45748416 0.45734066 0.4575648  0.45750862 0.45749572
 0.457583   0.45741102 0.45753875 0.4575356  0.45757398 0.45773917
 0.45764083 0.45779476 0.45783877 0.45768404 0.45768103 0.45740926
 0.45742798 0.4572388  0.4571591  0.457185   0.4570286  0.4572723
 0.4571348  0.45736057 0.457431   0.45735857 0.4576103  0.4573964
 0.45754844 0.4574252  0.4572322  0.4574403  0.45737964 0.45756492
 0.45775637 0.45759466 0.45775515 0.45757014 0.45748806 0.4573697
 0.4571658  0.45723933 0.45694616 0.45712477 0.4569311  0.45704123
 0.45730528 0.45707247 0.45751864 0.45729282 0.45740613 0.45748186
 0.4570802  0.45717084 0.45666054 0.4566602  0.4566119  0.4563715
 0.4565932  0.45625618 0.45635462 0.45626113 0.45598084 0.456084
 0.45568016 0.45600978 0.45573053 0.45593712 0.45634273 0.45598882
 0.45689663 0.45627186 0.45640096 0.45644882 0.45492908 0.45576552
 0.45315948 0.45387912 0.4516629  0.45276633 0.44818458 0.4571093 ]
