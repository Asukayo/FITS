Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j96_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17597
val 2537
test 5165
Model(
  (freq_upsampler): Linear(in_features=320, out_features=362, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9519267840.0
params:  116202.0
Trainable parameters:  116202
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 33.507428884506226
Epoch: 1, Steps: 68 | Train Loss: 0.8968278 Vali Loss: 0.6800535 Test Loss: 0.7766492
Validation loss decreased (inf --> 0.680053).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 31.2960786819458
Epoch: 2, Steps: 68 | Train Loss: 0.7020900 Vali Loss: 0.6180156 Test Loss: 0.7050573
Validation loss decreased (0.680053 --> 0.618016).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 31.308476448059082
Epoch: 3, Steps: 68 | Train Loss: 0.6328995 Vali Loss: 0.5820863 Test Loss: 0.6643499
Validation loss decreased (0.618016 --> 0.582086).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 33.443851947784424
Epoch: 4, Steps: 68 | Train Loss: 0.5842612 Vali Loss: 0.5530600 Test Loss: 0.6327590
Validation loss decreased (0.582086 --> 0.553060).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 31.77885103225708
Epoch: 5, Steps: 68 | Train Loss: 0.5426070 Vali Loss: 0.5266211 Test Loss: 0.6026225
Validation loss decreased (0.553060 --> 0.526621).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 29.531686305999756
Epoch: 6, Steps: 68 | Train Loss: 0.5059811 Vali Loss: 0.5018484 Test Loss: 0.5767644
Validation loss decreased (0.526621 --> 0.501848).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 30.41495656967163
Epoch: 7, Steps: 68 | Train Loss: 0.4734775 Vali Loss: 0.4784558 Test Loss: 0.5506644
Validation loss decreased (0.501848 --> 0.478456).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 28.06642174720764
Epoch: 8, Steps: 68 | Train Loss: 0.4443218 Vali Loss: 0.4576585 Test Loss: 0.5282083
Validation loss decreased (0.478456 --> 0.457658).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 29.341084480285645
Epoch: 9, Steps: 68 | Train Loss: 0.4181221 Vali Loss: 0.4374361 Test Loss: 0.5053595
Validation loss decreased (0.457658 --> 0.437436).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 30.42277717590332
Epoch: 10, Steps: 68 | Train Loss: 0.3944230 Vali Loss: 0.4207267 Test Loss: 0.4870169
Validation loss decreased (0.437436 --> 0.420727).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 29.539264917373657
Epoch: 11, Steps: 68 | Train Loss: 0.3730316 Vali Loss: 0.4028746 Test Loss: 0.4681233
Validation loss decreased (0.420727 --> 0.402875).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 29.713056564331055
Epoch: 12, Steps: 68 | Train Loss: 0.3535374 Vali Loss: 0.3885291 Test Loss: 0.4513472
Validation loss decreased (0.402875 --> 0.388529).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 31.128528833389282
Epoch: 13, Steps: 68 | Train Loss: 0.3358610 Vali Loss: 0.3738163 Test Loss: 0.4351600
Validation loss decreased (0.388529 --> 0.373816).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 30.70212435722351
Epoch: 14, Steps: 68 | Train Loss: 0.3196167 Vali Loss: 0.3621358 Test Loss: 0.4219090
Validation loss decreased (0.373816 --> 0.362136).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 31.046200275421143
Epoch: 15, Steps: 68 | Train Loss: 0.3049193 Vali Loss: 0.3510506 Test Loss: 0.4096213
Validation loss decreased (0.362136 --> 0.351051).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 32.53173637390137
Epoch: 16, Steps: 68 | Train Loss: 0.2912751 Vali Loss: 0.3398732 Test Loss: 0.3978929
Validation loss decreased (0.351051 --> 0.339873).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 34.5063533782959
Epoch: 17, Steps: 68 | Train Loss: 0.2788117 Vali Loss: 0.3308930 Test Loss: 0.3871813
Validation loss decreased (0.339873 --> 0.330893).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 36.902925968170166
Epoch: 18, Steps: 68 | Train Loss: 0.2674103 Vali Loss: 0.3210105 Test Loss: 0.3767405
Validation loss decreased (0.330893 --> 0.321011).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 31.618934631347656
Epoch: 19, Steps: 68 | Train Loss: 0.2568467 Vali Loss: 0.3135025 Test Loss: 0.3678699
Validation loss decreased (0.321011 --> 0.313502).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 31.098087072372437
Epoch: 20, Steps: 68 | Train Loss: 0.2470946 Vali Loss: 0.3047429 Test Loss: 0.3583843
Validation loss decreased (0.313502 --> 0.304743).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 33.28368639945984
Epoch: 21, Steps: 68 | Train Loss: 0.2380482 Vali Loss: 0.2972505 Test Loss: 0.3501850
Validation loss decreased (0.304743 --> 0.297251).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 32.444554567337036
Epoch: 22, Steps: 68 | Train Loss: 0.2296732 Vali Loss: 0.2918617 Test Loss: 0.3437951
Validation loss decreased (0.297251 --> 0.291862).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 33.55220985412598
Epoch: 23, Steps: 68 | Train Loss: 0.2219691 Vali Loss: 0.2851951 Test Loss: 0.3368515
Validation loss decreased (0.291862 --> 0.285195).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 31.595882177352905
Epoch: 24, Steps: 68 | Train Loss: 0.2147540 Vali Loss: 0.2784120 Test Loss: 0.3290560
Validation loss decreased (0.285195 --> 0.278412).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 30.896889209747314
Epoch: 25, Steps: 68 | Train Loss: 0.2080818 Vali Loss: 0.2732953 Test Loss: 0.3231771
Validation loss decreased (0.278412 --> 0.273295).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 34.53398323059082
Epoch: 26, Steps: 68 | Train Loss: 0.2017798 Vali Loss: 0.2689693 Test Loss: 0.3178898
Validation loss decreased (0.273295 --> 0.268969).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 35.10434293746948
Epoch: 27, Steps: 68 | Train Loss: 0.1960292 Vali Loss: 0.2630372 Test Loss: 0.3114275
Validation loss decreased (0.268969 --> 0.263037).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 33.19738245010376
Epoch: 28, Steps: 68 | Train Loss: 0.1906464 Vali Loss: 0.2590386 Test Loss: 0.3066352
Validation loss decreased (0.263037 --> 0.259039).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 33.56064319610596
Epoch: 29, Steps: 68 | Train Loss: 0.1855261 Vali Loss: 0.2555729 Test Loss: 0.3028670
Validation loss decreased (0.259039 --> 0.255573).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 34.15311598777771
Epoch: 30, Steps: 68 | Train Loss: 0.1808236 Vali Loss: 0.2506843 Test Loss: 0.2975119
Validation loss decreased (0.255573 --> 0.250684).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 32.41050720214844
Epoch: 31, Steps: 68 | Train Loss: 0.1763777 Vali Loss: 0.2483110 Test Loss: 0.2947004
Validation loss decreased (0.250684 --> 0.248311).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 31.829418182373047
Epoch: 32, Steps: 68 | Train Loss: 0.1722189 Vali Loss: 0.2442750 Test Loss: 0.2899678
Validation loss decreased (0.248311 --> 0.244275).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 35.26103758811951
Epoch: 33, Steps: 68 | Train Loss: 0.1683225 Vali Loss: 0.2408941 Test Loss: 0.2864726
Validation loss decreased (0.244275 --> 0.240894).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 31.83687973022461
Epoch: 34, Steps: 68 | Train Loss: 0.1646615 Vali Loss: 0.2384062 Test Loss: 0.2838227
Validation loss decreased (0.240894 --> 0.238406).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 34.766825914382935
Epoch: 35, Steps: 68 | Train Loss: 0.1611730 Vali Loss: 0.2351540 Test Loss: 0.2800206
Validation loss decreased (0.238406 --> 0.235154).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 35.12150430679321
Epoch: 36, Steps: 68 | Train Loss: 0.1580008 Vali Loss: 0.2321639 Test Loss: 0.2770162
Validation loss decreased (0.235154 --> 0.232164).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 32.827152967453
Epoch: 37, Steps: 68 | Train Loss: 0.1549690 Vali Loss: 0.2298906 Test Loss: 0.2740476
Validation loss decreased (0.232164 --> 0.229891).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 32.62704396247864
Epoch: 38, Steps: 68 | Train Loss: 0.1520977 Vali Loss: 0.2273667 Test Loss: 0.2710501
Validation loss decreased (0.229891 --> 0.227367).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 32.94207048416138
Epoch: 39, Steps: 68 | Train Loss: 0.1494153 Vali Loss: 0.2253546 Test Loss: 0.2690255
Validation loss decreased (0.227367 --> 0.225355).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 33.59530210494995
Epoch: 40, Steps: 68 | Train Loss: 0.1468492 Vali Loss: 0.2233676 Test Loss: 0.2662687
Validation loss decreased (0.225355 --> 0.223368).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 35.43248248100281
Epoch: 41, Steps: 68 | Train Loss: 0.1444654 Vali Loss: 0.2212548 Test Loss: 0.2642783
Validation loss decreased (0.223368 --> 0.221255).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 35.61138987541199
Epoch: 42, Steps: 68 | Train Loss: 0.1422128 Vali Loss: 0.2192420 Test Loss: 0.2621895
Validation loss decreased (0.221255 --> 0.219242).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 31.112172603607178
Epoch: 43, Steps: 68 | Train Loss: 0.1400624 Vali Loss: 0.2177504 Test Loss: 0.2604032
Validation loss decreased (0.219242 --> 0.217750).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 33.44644904136658
Epoch: 44, Steps: 68 | Train Loss: 0.1380557 Vali Loss: 0.2166459 Test Loss: 0.2588505
Validation loss decreased (0.217750 --> 0.216646).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 32.43555212020874
Epoch: 45, Steps: 68 | Train Loss: 0.1361613 Vali Loss: 0.2150250 Test Loss: 0.2572595
Validation loss decreased (0.216646 --> 0.215025).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 33.4011652469635
Epoch: 46, Steps: 68 | Train Loss: 0.1343742 Vali Loss: 0.2130033 Test Loss: 0.2548184
Validation loss decreased (0.215025 --> 0.213003).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 28.73536968231201
Epoch: 47, Steps: 68 | Train Loss: 0.1326771 Vali Loss: 0.2118032 Test Loss: 0.2536150
Validation loss decreased (0.213003 --> 0.211803).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 29.783584594726562
Epoch: 48, Steps: 68 | Train Loss: 0.1310618 Vali Loss: 0.2100537 Test Loss: 0.2516552
Validation loss decreased (0.211803 --> 0.210054).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 30.84017515182495
Epoch: 49, Steps: 68 | Train Loss: 0.1295259 Vali Loss: 0.2088621 Test Loss: 0.2499099
Validation loss decreased (0.210054 --> 0.208862).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 30.55314016342163
Epoch: 50, Steps: 68 | Train Loss: 0.1280815 Vali Loss: 0.2080468 Test Loss: 0.2490637
Validation loss decreased (0.208862 --> 0.208047).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 17597
val 2537
test 5165
Model(
  (freq_upsampler): Linear(in_features=320, out_features=362, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9519267840.0
params:  116202.0
Trainable parameters:  116202
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 37.97700262069702
Epoch: 1, Steps: 68 | Train Loss: 0.1648730 Vali Loss: 0.1152347 Test Loss: 0.1384238
Validation loss decreased (inf --> 0.115235).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 33.14217138290405
Epoch: 2, Steps: 68 | Train Loss: 0.1366927 Vali Loss: 0.1144920 Test Loss: 0.1372588
Validation loss decreased (0.115235 --> 0.114492).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 43.833757162094116
Epoch: 3, Steps: 68 | Train Loss: 0.1359220 Vali Loss: 0.1137137 Test Loss: 0.1367971
Validation loss decreased (0.114492 --> 0.113714).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 35.61250686645508
Epoch: 4, Steps: 68 | Train Loss: 0.1356956 Vali Loss: 0.1134380 Test Loss: 0.1365907
Validation loss decreased (0.113714 --> 0.113438).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 34.84114623069763
Epoch: 5, Steps: 68 | Train Loss: 0.1354919 Vali Loss: 0.1134346 Test Loss: 0.1365468
Validation loss decreased (0.113438 --> 0.113435).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 34.131468296051025
Epoch: 6, Steps: 68 | Train Loss: 0.1352266 Vali Loss: 0.1137140 Test Loss: 0.1364249
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 37.58586573600769
Epoch: 7, Steps: 68 | Train Loss: 0.1352540 Vali Loss: 0.1135558 Test Loss: 0.1362374
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 30.91127300262451
Epoch: 8, Steps: 68 | Train Loss: 0.1350574 Vali Loss: 0.1136022 Test Loss: 0.1363277
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5165
mse:0.1351747214794159, mae:0.23228947818279266, rse:0.3654194176197052, corr:[0.46527386 0.46875566 0.47096384 0.4711887  0.47180572 0.47167265
 0.4721085  0.47160408 0.47185856 0.47127903 0.4713773  0.4712739
 0.47098777 0.47084558 0.47087222 0.47078922 0.47074118 0.47084817
 0.47078162 0.47075626 0.4705449  0.47073093 0.4707301  0.47099113
 0.4712585  0.4713967  0.4714918  0.47135326 0.47121775 0.4710083
 0.47087657 0.4707006  0.47071457 0.47040737 0.4702355  0.47036377
 0.4702972  0.47018653 0.47020927 0.4701357  0.47008777 0.47022456
 0.4701253  0.47013482 0.46996954 0.47004285 0.47012234 0.47029018
 0.47063473 0.47079554 0.47075352 0.4705587  0.47057036 0.4703672
 0.47009045 0.46993446 0.46996805 0.4698473  0.4696652  0.46979064
 0.46986163 0.4698468  0.46971053 0.4697825  0.46970156 0.4696427
 0.4694006  0.46920785 0.46916315 0.46890494 0.46896103 0.46915984
 0.4693908  0.469418   0.46944797 0.46935537 0.4692315  0.46904704
 0.46883005 0.46885958 0.46886218 0.4688026  0.46876    0.46890643
 0.46875954 0.46883547 0.4684528  0.46859735 0.46851915 0.4683148
 0.4683486  0.4679789  0.46795812 0.46809366 0.46825647 0.4695693 ]
