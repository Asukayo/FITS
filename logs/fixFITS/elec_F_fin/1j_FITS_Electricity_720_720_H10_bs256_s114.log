Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  33659289600.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 54.84568238258362
Epoch: 1, Steps: 33 | Train Loss: 1.1264134 Vali Loss: 0.7351589 Test Loss: 0.8649963
Validation loss decreased (inf --> 0.735159).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 53.64652132987976
Epoch: 2, Steps: 33 | Train Loss: 0.7973323 Vali Loss: 0.6044957 Test Loss: 0.7193463
Validation loss decreased (0.735159 --> 0.604496).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 55.9305956363678
Epoch: 3, Steps: 33 | Train Loss: 0.6951901 Vali Loss: 0.5396546 Test Loss: 0.6449886
Validation loss decreased (0.604496 --> 0.539655).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 58.39960527420044
Epoch: 4, Steps: 33 | Train Loss: 0.6282421 Vali Loss: 0.4887031 Test Loss: 0.5862345
Validation loss decreased (0.539655 --> 0.488703).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 63.007256746292114
Epoch: 5, Steps: 33 | Train Loss: 0.5738966 Vali Loss: 0.4467382 Test Loss: 0.5374523
Validation loss decreased (0.488703 --> 0.446738).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 54.17068600654602
Epoch: 6, Steps: 33 | Train Loss: 0.5284016 Vali Loss: 0.4109627 Test Loss: 0.4963870
Validation loss decreased (0.446738 --> 0.410963).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 53.813015937805176
Epoch: 7, Steps: 33 | Train Loss: 0.4900119 Vali Loss: 0.3814035 Test Loss: 0.4616113
Validation loss decreased (0.410963 --> 0.381404).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 52.992761850357056
Epoch: 8, Steps: 33 | Train Loss: 0.4574979 Vali Loss: 0.3558319 Test Loss: 0.4319604
Validation loss decreased (0.381404 --> 0.355832).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 56.36122536659241
Epoch: 9, Steps: 33 | Train Loss: 0.4298863 Vali Loss: 0.3337111 Test Loss: 0.4065084
Validation loss decreased (0.355832 --> 0.333711).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 58.34171390533447
Epoch: 10, Steps: 33 | Train Loss: 0.4060534 Vali Loss: 0.3151131 Test Loss: 0.3846002
Validation loss decreased (0.333711 --> 0.315113).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 56.28596210479736
Epoch: 11, Steps: 33 | Train Loss: 0.3854908 Vali Loss: 0.2992285 Test Loss: 0.3656857
Validation loss decreased (0.315113 --> 0.299229).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 58.780518770217896
Epoch: 12, Steps: 33 | Train Loss: 0.3677957 Vali Loss: 0.2861155 Test Loss: 0.3492322
Validation loss decreased (0.299229 --> 0.286115).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 55.252005100250244
Epoch: 13, Steps: 33 | Train Loss: 0.3523855 Vali Loss: 0.2734442 Test Loss: 0.3349397
Validation loss decreased (0.286115 --> 0.273444).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 57.96731925010681
Epoch: 14, Steps: 33 | Train Loss: 0.3388484 Vali Loss: 0.2632404 Test Loss: 0.3224386
Validation loss decreased (0.273444 --> 0.263240).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 59.80758833885193
Epoch: 15, Steps: 33 | Train Loss: 0.3273402 Vali Loss: 0.2537576 Test Loss: 0.3115028
Validation loss decreased (0.263240 --> 0.253758).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 57.874204874038696
Epoch: 16, Steps: 33 | Train Loss: 0.3169265 Vali Loss: 0.2464686 Test Loss: 0.3018713
Validation loss decreased (0.253758 --> 0.246469).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 60.157095432281494
Epoch: 17, Steps: 33 | Train Loss: 0.3080079 Vali Loss: 0.2393831 Test Loss: 0.2933882
Validation loss decreased (0.246469 --> 0.239383).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 59.310425996780396
Epoch: 18, Steps: 33 | Train Loss: 0.2999781 Vali Loss: 0.2334860 Test Loss: 0.2858854
Validation loss decreased (0.239383 --> 0.233486).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 56.85810685157776
Epoch: 19, Steps: 33 | Train Loss: 0.2929017 Vali Loss: 0.2283137 Test Loss: 0.2792358
Validation loss decreased (0.233486 --> 0.228314).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 57.1132709980011
Epoch: 20, Steps: 33 | Train Loss: 0.2868130 Vali Loss: 0.2225974 Test Loss: 0.2733121
Validation loss decreased (0.228314 --> 0.222597).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 55.99718356132507
Epoch: 21, Steps: 33 | Train Loss: 0.2812774 Vali Loss: 0.2192314 Test Loss: 0.2680500
Validation loss decreased (0.222597 --> 0.219231).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 54.280643463134766
Epoch: 22, Steps: 33 | Train Loss: 0.2762889 Vali Loss: 0.2153121 Test Loss: 0.2633366
Validation loss decreased (0.219231 --> 0.215312).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 54.91136598587036
Epoch: 23, Steps: 33 | Train Loss: 0.2719226 Vali Loss: 0.2119009 Test Loss: 0.2591098
Validation loss decreased (0.215312 --> 0.211901).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 61.2271146774292
Epoch: 24, Steps: 33 | Train Loss: 0.2680900 Vali Loss: 0.2092512 Test Loss: 0.2553430
Validation loss decreased (0.211901 --> 0.209251).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 61.234949350357056
Epoch: 25, Steps: 33 | Train Loss: 0.2644326 Vali Loss: 0.2067234 Test Loss: 0.2519450
Validation loss decreased (0.209251 --> 0.206723).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 59.028910636901855
Epoch: 26, Steps: 33 | Train Loss: 0.2613023 Vali Loss: 0.2046171 Test Loss: 0.2488626
Validation loss decreased (0.206723 --> 0.204617).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 59.84426689147949
Epoch: 27, Steps: 33 | Train Loss: 0.2585009 Vali Loss: 0.2022012 Test Loss: 0.2460881
Validation loss decreased (0.204617 --> 0.202201).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 58.49438834190369
Epoch: 28, Steps: 33 | Train Loss: 0.2558687 Vali Loss: 0.1997935 Test Loss: 0.2435733
Validation loss decreased (0.202201 --> 0.199794).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 59.83993577957153
Epoch: 29, Steps: 33 | Train Loss: 0.2535706 Vali Loss: 0.1985147 Test Loss: 0.2412906
Validation loss decreased (0.199794 --> 0.198515).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 55.28795123100281
Epoch: 30, Steps: 33 | Train Loss: 0.2514269 Vali Loss: 0.1966472 Test Loss: 0.2392174
Validation loss decreased (0.198515 --> 0.196647).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 58.744717597961426
Epoch: 31, Steps: 33 | Train Loss: 0.2494743 Vali Loss: 0.1953748 Test Loss: 0.2373347
Validation loss decreased (0.196647 --> 0.195375).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 59.53387236595154
Epoch: 32, Steps: 33 | Train Loss: 0.2477197 Vali Loss: 0.1946955 Test Loss: 0.2356147
Validation loss decreased (0.195375 --> 0.194695).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 58.974135398864746
Epoch: 33, Steps: 33 | Train Loss: 0.2461766 Vali Loss: 0.1930467 Test Loss: 0.2340345
Validation loss decreased (0.194695 --> 0.193047).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 60.30317544937134
Epoch: 34, Steps: 33 | Train Loss: 0.2447372 Vali Loss: 0.1920376 Test Loss: 0.2325904
Validation loss decreased (0.193047 --> 0.192038).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 58.35292196273804
Epoch: 35, Steps: 33 | Train Loss: 0.2433676 Vali Loss: 0.1905883 Test Loss: 0.2312693
Validation loss decreased (0.192038 --> 0.190588).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 59.006356954574585
Epoch: 36, Steps: 33 | Train Loss: 0.2421120 Vali Loss: 0.1901844 Test Loss: 0.2300509
Validation loss decreased (0.190588 --> 0.190184).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 63.51381254196167
Epoch: 37, Steps: 33 | Train Loss: 0.2409547 Vali Loss: 0.1891833 Test Loss: 0.2289266
Validation loss decreased (0.190184 --> 0.189183).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 59.799949169158936
Epoch: 38, Steps: 33 | Train Loss: 0.2400318 Vali Loss: 0.1882655 Test Loss: 0.2278970
Validation loss decreased (0.189183 --> 0.188266).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 52.65506863594055
Epoch: 39, Steps: 33 | Train Loss: 0.2390403 Vali Loss: 0.1879303 Test Loss: 0.2269392
Validation loss decreased (0.188266 --> 0.187930).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 48.32655572891235
Epoch: 40, Steps: 33 | Train Loss: 0.2381507 Vali Loss: 0.1872462 Test Loss: 0.2260622
Validation loss decreased (0.187930 --> 0.187246).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 53.97057318687439
Epoch: 41, Steps: 33 | Train Loss: 0.2372409 Vali Loss: 0.1871877 Test Loss: 0.2252452
Validation loss decreased (0.187246 --> 0.187188).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 54.975860357284546
Epoch: 42, Steps: 33 | Train Loss: 0.2366016 Vali Loss: 0.1857567 Test Loss: 0.2244905
Validation loss decreased (0.187188 --> 0.185757).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 55.30920338630676
Epoch: 43, Steps: 33 | Train Loss: 0.2358554 Vali Loss: 0.1857906 Test Loss: 0.2237790
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 54.04094648361206
Epoch: 44, Steps: 33 | Train Loss: 0.2350908 Vali Loss: 0.1845554 Test Loss: 0.2231310
Validation loss decreased (0.185757 --> 0.184555).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 56.85251069068909
Epoch: 45, Steps: 33 | Train Loss: 0.2346812 Vali Loss: 0.1853004 Test Loss: 0.2225205
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 54.221057176589966
Epoch: 46, Steps: 33 | Train Loss: 0.2340522 Vali Loss: 0.1846070 Test Loss: 0.2219525
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 55.748942613601685
Epoch: 47, Steps: 33 | Train Loss: 0.2335781 Vali Loss: 0.1838400 Test Loss: 0.2214203
Validation loss decreased (0.184555 --> 0.183840).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 56.492135763168335
Epoch: 48, Steps: 33 | Train Loss: 0.2330501 Vali Loss: 0.1840827 Test Loss: 0.2209286
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 55.6805579662323
Epoch: 49, Steps: 33 | Train Loss: 0.2325401 Vali Loss: 0.1833774 Test Loss: 0.2204679
Validation loss decreased (0.183840 --> 0.183377).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 64.2792501449585
Epoch: 50, Steps: 33 | Train Loss: 0.2321588 Vali Loss: 0.1830309 Test Loss: 0.2200307
Validation loss decreased (0.183377 --> 0.183031).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
>>>>>>>testing : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.2195705622434616, mae:0.31895318627357483, rse:0.46742770075798035, corr:[0.44382942 0.423118   0.44559395 0.4416775  0.4500169  0.4477468
 0.45208734 0.4514     0.45140135 0.45173317 0.4503538  0.4499052
 0.449731   0.4482693  0.44926384 0.44805598 0.44872096 0.44849822
 0.44844973 0.44869995 0.4480857  0.44916987 0.44851646 0.44950035
 0.4494879  0.44993472 0.4509608  0.4503639  0.45100486 0.45062694
 0.45021892 0.4503989  0.44943786 0.4493733  0.44911608 0.44843787
 0.44887054 0.4483106  0.4485384  0.4485801  0.44841632 0.44882923
 0.44849303 0.44880542 0.4485043  0.44879153 0.44901296 0.44885993
 0.449512   0.44922864 0.4497692  0.44981784 0.44942275 0.44966415
 0.44909313 0.44898972 0.44884834 0.44827235 0.4484608  0.44805935
 0.4480201  0.4481945  0.44784564 0.44822094 0.44804534 0.44821426
 0.4482597  0.44808266 0.4483533  0.44811377 0.44847697 0.44836193
 0.4484387  0.44878674 0.44850126 0.44881934 0.4484891  0.44832528
 0.44837677 0.44780025 0.4478723  0.44763198 0.44733965 0.44751146
 0.44719732 0.4473958  0.44740722 0.44730082 0.44754273 0.44738334
 0.44759342 0.44742176 0.4474948  0.4476317  0.44743842 0.44774294
 0.44753587 0.4478384  0.44785994 0.44759002 0.44782484 0.4473832
 0.44736904 0.44727835 0.44685948 0.44699332 0.44671762 0.44663355
 0.44672197 0.44650766 0.44670212 0.44663566 0.44673708 0.4468425
 0.4467316  0.44681254 0.44654083 0.44676965 0.44671094 0.44667736
 0.44708028 0.44686583 0.44718218 0.4470235  0.4468966  0.44702673
 0.4466358  0.44668993 0.44650862 0.44626537 0.44637915 0.44610772
 0.44621408 0.4463147  0.44625008 0.4464742  0.4463868  0.4465411
 0.44655335 0.44662058 0.44679278 0.4465935  0.44677192 0.44653586
 0.44656554 0.4466496  0.44646865 0.44675434 0.44646326 0.44645295
 0.44640788 0.44606203 0.44617552 0.44589478 0.4457887  0.44586965
 0.4456778  0.44588292 0.44588888 0.44589782 0.44606727 0.4460195
 0.4462139  0.44608015 0.4460888  0.44599316 0.44576603 0.4457704
 0.44534853 0.44559664 0.44568664 0.4455763  0.44569618 0.4453087
 0.44536507 0.44515923 0.44487968 0.44501296 0.4447278  0.44475958
 0.4447756  0.4446778  0.4448684  0.44476074 0.44487002 0.44485584
 0.44477817 0.44472566 0.44436064 0.44444275 0.44430053 0.44420364
 0.44429418 0.4441665  0.4445156  0.44438308 0.44431677 0.44429317
 0.44394052 0.44398683 0.44371417 0.44358748 0.44363803 0.44342318
 0.44355023 0.44347167 0.4434623  0.44361803 0.44350183 0.44364506
 0.44353342 0.44338602 0.44328138 0.44308323 0.4432455  0.44303325
 0.44313982 0.44325495 0.44318962 0.44348246 0.44320124 0.4432007
 0.4431392  0.4428168  0.44295216 0.4427477  0.4426939  0.44272646
 0.44253847 0.44272903 0.44264182 0.44265777 0.4427743  0.44263443
 0.442745   0.44248122 0.44241267 0.44240722 0.44234592 0.44242853
 0.4422052  0.44246784 0.44250858 0.44251454 0.44258043 0.44228557
 0.44236952 0.44216433 0.44202164 0.44208172 0.44181913 0.44188854
 0.4418816  0.44181332 0.44195575 0.4418165  0.44193903 0.4419673
 0.4419109  0.44191927 0.44171566 0.4418502  0.44175696 0.44181007
 0.441898   0.44181463 0.44215763 0.44193572 0.44193718 0.4419149
 0.4415895  0.44170108 0.44144133 0.441341   0.44137192 0.44114685
 0.44131768 0.4412549  0.44126108 0.44141686 0.4412579  0.44145438
 0.44134414 0.44120806 0.44115645 0.4410013  0.44126868 0.44113848
 0.44140592 0.44153246 0.44146246 0.44173148 0.44146028 0.4415032
 0.44152382 0.44127813 0.4413773  0.44109938 0.44106227 0.4411151
 0.4409421  0.44118744 0.4410823  0.4411278  0.44128478 0.4411642
 0.44136763 0.44127023 0.44136584 0.44136995 0.44123363 0.4413546
 0.44105235 0.44122317 0.4412559  0.4412021  0.44134018 0.44104332
 0.44113767 0.4409951  0.44083813 0.4410053  0.4407101  0.44078174
 0.44084004 0.4407215  0.44093168 0.4408116  0.44098127 0.4409939
 0.44090396 0.44098717 0.44069052 0.44075954 0.44055504 0.44025177
 0.44010645 0.43996188 0.440307   0.44012272 0.44009298 0.44014475
 0.4397555  0.43987015 0.43959862 0.43943706 0.4395654  0.43924993
 0.43940738 0.43940842 0.4393603  0.43952906 0.4393511  0.4395662
 0.4394938  0.43930832 0.43929756 0.43909544 0.43926632 0.43902156
 0.4389378  0.439095   0.43907824 0.4393199  0.43905532 0.43905658
 0.4390038  0.43869406 0.4388147  0.438518   0.43844542 0.43848196
 0.43828338 0.43847862 0.4383453  0.4384158  0.43851456 0.43831968
 0.43848243 0.4381939  0.43819004 0.43821692 0.43802568 0.4382297
 0.4379953  0.4382073  0.4383261  0.43822378 0.43839008 0.43807974
 0.43811274 0.43803343 0.4377932  0.4379299  0.4376176  0.4376369
 0.43770236 0.43753657 0.4377668  0.4376491  0.4377067  0.43771762
 0.4376104  0.4376987  0.43742758 0.43751192 0.4374816  0.43741104
 0.43751812 0.4374311  0.4377893  0.4377037  0.43764988 0.4377485
 0.4374517  0.43752605 0.43727717 0.4371731  0.4373411  0.43710926
 0.43728718 0.43724474 0.4371959  0.43741664 0.43723112 0.43741432
 0.43735245 0.43718776 0.43723178 0.43703657 0.43727377 0.43715248
 0.43719897 0.43739605 0.4372709  0.43750292 0.4373366  0.43731615
 0.43734702 0.43706593 0.43720326 0.4369359  0.43683463 0.43699598
 0.43675876 0.43697783 0.43697724 0.4369517  0.437125   0.4369598
 0.43708202 0.43687144 0.4367774  0.4368829  0.43678695 0.43700653
 0.43705696 0.43726602 0.437373   0.43730244 0.4374208  0.4372137
 0.43728432 0.4372149  0.436953   0.43703735 0.43676192 0.43678877
 0.4369365  0.43677577 0.4370886  0.43703482 0.43707457 0.43723142
 0.4371189  0.4372905  0.43721008 0.43728727 0.4373032  0.43718904
 0.43726745 0.43718624 0.4374322  0.4374024  0.43734455 0.43740872
 0.43717992 0.43728352 0.4371816  0.43703264 0.43713245 0.4369111
 0.4370851  0.4371476  0.43708667 0.43733886 0.4372378  0.43737903
 0.43738422 0.4372615  0.43725234 0.43697646 0.43709564 0.43683028
 0.4365109  0.43661076 0.43656707 0.43677643 0.43661338 0.43652782
 0.4365216  0.43620196 0.43625793 0.43599293 0.435822   0.43595898
 0.4357977  0.43600193 0.43597567 0.43597883 0.4361074  0.4359604
 0.4360565  0.43584594 0.4357233  0.4357187  0.43555903 0.43564895
 0.43542475 0.43554842 0.43568236 0.43559322 0.4357182  0.43546107
 0.4354146  0.43527025 0.4349288  0.43507144 0.43474773 0.43471467
 0.43485722 0.43471113 0.43494877 0.43483606 0.43488407 0.43494114
 0.43474913 0.43479884 0.43462485 0.4346701  0.43474954 0.43467298
 0.4347171  0.4346437  0.43486094 0.43478534 0.4345917  0.43462765
 0.43441436 0.43441403 0.43420196 0.43400657 0.4340615  0.43375403
 0.43391863 0.43389282 0.43381992 0.4340185  0.43381512 0.43387568
 0.43374747 0.43363538 0.43372393 0.43356508 0.43374744 0.43364567
 0.43359774 0.43378285 0.43378857 0.43398887 0.4337896  0.43365183
 0.43369445 0.43338084 0.43342653 0.4332109  0.43308714 0.43325648
 0.43308607 0.43330505 0.43326458 0.43329674 0.43341556 0.43328482
 0.43347842 0.43327457 0.43329626 0.43335304 0.43326762 0.4334569
 0.4332983  0.43350887 0.4335999  0.43342966 0.43354926 0.43323356
 0.43318474 0.43308738 0.4327146  0.43281475 0.43254846 0.43260732
 0.43275616 0.43267453 0.43301466 0.4329129  0.43305847 0.43318093
 0.43303058 0.43314055 0.43294716 0.43311208 0.43321335 0.43320018
 0.43351066 0.43343943 0.43361554 0.4334749  0.43329635 0.4332956
 0.4329581  0.432977   0.4327318  0.4325502  0.4327223  0.43243635
 0.43271938 0.43274775 0.43277127 0.43310758 0.4329383  0.43317977
 0.43315443 0.4331363  0.4332936  0.43311155 0.43340772 0.43322632
 0.43308946 0.43321478 0.43305042 0.4332376  0.43288305 0.43276045
 0.43273988 0.4323506  0.43254486 0.43216452 0.4322843  0.4324892
 0.4323052  0.4328556  0.43269408 0.4329533  0.4331862  0.4330213
 0.4333593  0.4330438  0.4330456  0.43307158 0.4328089  0.43294233
 0.43212926 0.43209505 0.4320657  0.431568   0.4316821  0.43094906
 0.4310322  0.43070304 0.43041256 0.43067604 0.43013233 0.43075955
 0.4307381  0.43096268 0.4314364  0.43121874 0.43163055 0.43152666
 0.43137893 0.43145904 0.43072832 0.4309793  0.4305154  0.430019
 0.43016773 0.4291036  0.42982885 0.42878687 0.42882937 0.42863303
 0.42798832 0.42888495 0.42762855 0.4292292  0.42872608 0.4296458
 0.43066427 0.43034488 0.43118742 0.4308247  0.42905453 0.43006125
 0.42467284 0.426929   0.4197224  0.42435023 0.40330374 0.42892796]
