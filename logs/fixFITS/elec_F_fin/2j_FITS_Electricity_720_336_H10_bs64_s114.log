Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6166487040.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6996817
	speed: 0.4030s/iter; left time: 2680.5541s
Epoch: 1 cost time: 54.559818267822266
Epoch: 1, Steps: 135 | Train Loss: 0.8302913 Vali Loss: 0.6294428 Test Loss: 0.7300373
Validation loss decreased (inf --> 0.629443).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5878541
	speed: 0.9178s/iter; left time: 5980.1339s
Epoch: 2 cost time: 57.734195947647095
Epoch: 2, Steps: 135 | Train Loss: 0.6151779 Vali Loss: 0.5540031 Test Loss: 0.6443761
Validation loss decreased (0.629443 --> 0.554003).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4976290
	speed: 0.9618s/iter; left time: 6136.9757s
Epoch: 3 cost time: 56.59298062324524
Epoch: 3, Steps: 135 | Train Loss: 0.5261951 Vali Loss: 0.4919574 Test Loss: 0.5748482
Validation loss decreased (0.554003 --> 0.491957).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4357173
	speed: 0.8956s/iter; left time: 5593.7788s
Epoch: 4 cost time: 55.092324018478394
Epoch: 4, Steps: 135 | Train Loss: 0.4556733 Vali Loss: 0.4437802 Test Loss: 0.5209824
Validation loss decreased (0.491957 --> 0.443780).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3832786
	speed: 0.8721s/iter; left time: 5329.2377s
Epoch: 5 cost time: 49.67770338058472
Epoch: 5, Steps: 135 | Train Loss: 0.3977253 Vali Loss: 0.4028412 Test Loss: 0.4753466
Validation loss decreased (0.443780 --> 0.402841).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3319526
	speed: 0.8292s/iter; left time: 4955.0425s
Epoch: 6 cost time: 53.84448313713074
Epoch: 6, Steps: 135 | Train Loss: 0.3494926 Vali Loss: 0.3645346 Test Loss: 0.4320406
Validation loss decreased (0.402841 --> 0.364535).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3025044
	speed: 0.8831s/iter; left time: 5158.0016s
Epoch: 7 cost time: 53.28141641616821
Epoch: 7, Steps: 135 | Train Loss: 0.3088966 Vali Loss: 0.3344750 Test Loss: 0.3979784
Validation loss decreased (0.364535 --> 0.334475).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2658999
	speed: 0.8700s/iter; left time: 4964.2373s
Epoch: 8 cost time: 51.54489755630493
Epoch: 8, Steps: 135 | Train Loss: 0.2745857 Vali Loss: 0.3088096 Test Loss: 0.3683173
Validation loss decreased (0.334475 --> 0.308810).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2357946
	speed: 0.8668s/iter; left time: 4829.0885s
Epoch: 9 cost time: 55.06777095794678
Epoch: 9, Steps: 135 | Train Loss: 0.2454755 Vali Loss: 0.2849837 Test Loss: 0.3416396
Validation loss decreased (0.308810 --> 0.284984).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2228222
	speed: 0.9353s/iter; left time: 5084.1531s
Epoch: 10 cost time: 60.03145456314087
Epoch: 10, Steps: 135 | Train Loss: 0.2206118 Vali Loss: 0.2668913 Test Loss: 0.3209486
Validation loss decreased (0.284984 --> 0.266891).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1905887
	speed: 0.9419s/iter; left time: 4992.9591s
Epoch: 11 cost time: 54.18382692337036
Epoch: 11, Steps: 135 | Train Loss: 0.1993097 Vali Loss: 0.2508262 Test Loss: 0.3024429
Validation loss decreased (0.266891 --> 0.250826).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1804764
	speed: 0.9043s/iter; left time: 4671.6658s
Epoch: 12 cost time: 55.22221541404724
Epoch: 12, Steps: 135 | Train Loss: 0.1810050 Vali Loss: 0.2362047 Test Loss: 0.2851862
Validation loss decreased (0.250826 --> 0.236205).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1652115
	speed: 0.8584s/iter; left time: 4318.7312s
Epoch: 13 cost time: 52.39182925224304
Epoch: 13, Steps: 135 | Train Loss: 0.1652408 Vali Loss: 0.2241696 Test Loss: 0.2715096
Validation loss decreased (0.236205 --> 0.224170).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1546085
	speed: 0.7628s/iter; left time: 3734.4368s
Epoch: 14 cost time: 48.52090620994568
Epoch: 14, Steps: 135 | Train Loss: 0.1515986 Vali Loss: 0.2127269 Test Loss: 0.2581614
Validation loss decreased (0.224170 --> 0.212727).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1407567
	speed: 0.8692s/iter; left time: 4138.2438s
Epoch: 15 cost time: 55.67864203453064
Epoch: 15, Steps: 135 | Train Loss: 0.1398379 Vali Loss: 0.2036296 Test Loss: 0.2475351
Validation loss decreased (0.212727 --> 0.203630).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1325325
	speed: 0.8902s/iter; left time: 4118.2917s
Epoch: 16 cost time: 56.37591814994812
Epoch: 16, Steps: 135 | Train Loss: 0.1295611 Vali Loss: 0.1958103 Test Loss: 0.2382372
Validation loss decreased (0.203630 --> 0.195810).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1178999
	speed: 0.9014s/iter; left time: 4048.1219s
Epoch: 17 cost time: 56.26380395889282
Epoch: 17, Steps: 135 | Train Loss: 0.1207133 Vali Loss: 0.1894743 Test Loss: 0.2303241
Validation loss decreased (0.195810 --> 0.189474).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1093275
	speed: 0.9088s/iter; left time: 3958.9175s
Epoch: 18 cost time: 56.45497965812683
Epoch: 18, Steps: 135 | Train Loss: 0.1129997 Vali Loss: 0.1825820 Test Loss: 0.2220037
Validation loss decreased (0.189474 --> 0.182582).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1043127
	speed: 0.8681s/iter; left time: 3664.1476s
Epoch: 19 cost time: 51.95264434814453
Epoch: 19, Steps: 135 | Train Loss: 0.1062613 Vali Loss: 0.1781787 Test Loss: 0.2169261
Validation loss decreased (0.182582 --> 0.178179).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1015771
	speed: 0.8885s/iter; left time: 3630.3429s
Epoch: 20 cost time: 56.68562960624695
Epoch: 20, Steps: 135 | Train Loss: 0.1003990 Vali Loss: 0.1734099 Test Loss: 0.2109646
Validation loss decreased (0.178179 --> 0.173410).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.0943994
	speed: 0.9215s/iter; left time: 3640.8689s
Epoch: 21 cost time: 57.71874165534973
Epoch: 21, Steps: 135 | Train Loss: 0.0952379 Vali Loss: 0.1692990 Test Loss: 0.2061324
Validation loss decreased (0.173410 --> 0.169299).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.0871131
	speed: 0.9154s/iter; left time: 3493.3063s
Epoch: 22 cost time: 56.07671403884888
Epoch: 22, Steps: 135 | Train Loss: 0.0907286 Vali Loss: 0.1660116 Test Loss: 0.2020980
Validation loss decreased (0.169299 --> 0.166012).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.0887295
	speed: 0.8644s/iter; left time: 3181.9425s
Epoch: 23 cost time: 49.65540266036987
Epoch: 23, Steps: 135 | Train Loss: 0.0868084 Vali Loss: 0.1625210 Test Loss: 0.1978997
Validation loss decreased (0.166012 --> 0.162521).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.0814133
	speed: 0.8229s/iter; left time: 2918.0058s
Epoch: 24 cost time: 50.46054816246033
Epoch: 24, Steps: 135 | Train Loss: 0.0833647 Vali Loss: 0.1602114 Test Loss: 0.1945146
Validation loss decreased (0.162521 --> 0.160211).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.0807090
	speed: 0.9009s/iter; left time: 3072.8827s
Epoch: 25 cost time: 55.53563642501831
Epoch: 25, Steps: 135 | Train Loss: 0.0803453 Vali Loss: 0.1576473 Test Loss: 0.1917477
Validation loss decreased (0.160211 --> 0.157647).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.0759550
	speed: 0.8827s/iter; left time: 2891.8192s
Epoch: 26 cost time: 51.633256912231445
Epoch: 26, Steps: 135 | Train Loss: 0.0776861 Vali Loss: 0.1558972 Test Loss: 0.1890924
Validation loss decreased (0.157647 --> 0.155897).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.0762625
	speed: 0.8338s/iter; left time: 2618.9696s
Epoch: 27 cost time: 52.90318989753723
Epoch: 27, Steps: 135 | Train Loss: 0.0753580 Vali Loss: 0.1541276 Test Loss: 0.1869094
Validation loss decreased (0.155897 --> 0.154128).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.0716570
	speed: 0.8547s/iter; left time: 2569.1224s
Epoch: 28 cost time: 54.026809215545654
Epoch: 28, Steps: 135 | Train Loss: 0.0733129 Vali Loss: 0.1523959 Test Loss: 0.1847856
Validation loss decreased (0.154128 --> 0.152396).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.0697703
	speed: 0.8892s/iter; left time: 2552.9509s
Epoch: 29 cost time: 52.2398624420166
Epoch: 29, Steps: 135 | Train Loss: 0.0715012 Vali Loss: 0.1511335 Test Loss: 0.1833141
Validation loss decreased (0.152396 --> 0.151134).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.0690518
	speed: 0.8611s/iter; left time: 2356.0991s
Epoch: 30 cost time: 55.436328411102295
Epoch: 30, Steps: 135 | Train Loss: 0.0699176 Vali Loss: 0.1499584 Test Loss: 0.1813833
Validation loss decreased (0.151134 --> 0.149958).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.0666757
	speed: 0.8744s/iter; left time: 2274.3313s
Epoch: 31 cost time: 54.29071497917175
Epoch: 31, Steps: 135 | Train Loss: 0.0685028 Vali Loss: 0.1488919 Test Loss: 0.1799847
Validation loss decreased (0.149958 --> 0.148892).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.0658153
	speed: 0.8486s/iter; left time: 2092.6394s
Epoch: 32 cost time: 51.73355197906494
Epoch: 32, Steps: 135 | Train Loss: 0.0672671 Vali Loss: 0.1480582 Test Loss: 0.1790104
Validation loss decreased (0.148892 --> 0.148058).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.0656111
	speed: 0.9616s/iter; left time: 2241.5906s
Epoch: 33 cost time: 62.33628273010254
Epoch: 33, Steps: 135 | Train Loss: 0.0661701 Vali Loss: 0.1472712 Test Loss: 0.1778636
Validation loss decreased (0.148058 --> 0.147271).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.0639993
	speed: 0.9246s/iter; left time: 2030.3548s
Epoch: 34 cost time: 51.79004406929016
Epoch: 34, Steps: 135 | Train Loss: 0.0652047 Vali Loss: 0.1464524 Test Loss: 0.1768324
Validation loss decreased (0.147271 --> 0.146452).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.0633416
	speed: 0.8078s/iter; left time: 1664.8425s
Epoch: 35 cost time: 53.139435052871704
Epoch: 35, Steps: 135 | Train Loss: 0.0643618 Vali Loss: 0.1458937 Test Loss: 0.1758868
Validation loss decreased (0.146452 --> 0.145894).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.0613444
	speed: 0.8868s/iter; left time: 1707.9351s
Epoch: 36 cost time: 55.02493715286255
Epoch: 36, Steps: 135 | Train Loss: 0.0636316 Vali Loss: 0.1456136 Test Loss: 0.1753633
Validation loss decreased (0.145894 --> 0.145614).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.0650385
	speed: 0.9195s/iter; left time: 1646.8533s
Epoch: 37 cost time: 56.064462661743164
Epoch: 37, Steps: 135 | Train Loss: 0.0629792 Vali Loss: 0.1451815 Test Loss: 0.1745282
Validation loss decreased (0.145614 --> 0.145181).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.0629157
	speed: 0.9058s/iter; left time: 1500.0207s
Epoch: 38 cost time: 55.772472858428955
Epoch: 38, Steps: 135 | Train Loss: 0.0623750 Vali Loss: 0.1444961 Test Loss: 0.1739517
Validation loss decreased (0.145181 --> 0.144496).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.0612170
	speed: 0.9036s/iter; left time: 1374.3089s
Epoch: 39 cost time: 55.09800839424133
Epoch: 39, Steps: 135 | Train Loss: 0.0618911 Vali Loss: 0.1441870 Test Loss: 0.1733642
Validation loss decreased (0.144496 --> 0.144187).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.0606785
	speed: 0.8755s/iter; left time: 1213.3742s
Epoch: 40 cost time: 56.809635162353516
Epoch: 40, Steps: 135 | Train Loss: 0.0614334 Vali Loss: 0.1442179 Test Loss: 0.1729556
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.0624503
	speed: 0.9076s/iter; left time: 1135.3767s
Epoch: 41 cost time: 55.178229331970215
Epoch: 41, Steps: 135 | Train Loss: 0.0610248 Vali Loss: 0.1437391 Test Loss: 0.1725526
Validation loss decreased (0.144187 --> 0.143739).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.0655509
	speed: 0.8819s/iter; left time: 984.2274s
Epoch: 42 cost time: 55.19690227508545
Epoch: 42, Steps: 135 | Train Loss: 0.0606462 Vali Loss: 0.1437382 Test Loss: 0.1722359
Validation loss decreased (0.143739 --> 0.143738).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.0613457
	speed: 0.8932s/iter; left time: 876.1957s
Epoch: 43 cost time: 56.541977643966675
Epoch: 43, Steps: 135 | Train Loss: 0.0603565 Vali Loss: 0.1434651 Test Loss: 0.1718438
Validation loss decreased (0.143738 --> 0.143465).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.0601091
	speed: 0.9419s/iter; left time: 796.8373s
Epoch: 44 cost time: 57.56540083885193
Epoch: 44, Steps: 135 | Train Loss: 0.0600831 Vali Loss: 0.1433756 Test Loss: 0.1715133
Validation loss decreased (0.143465 --> 0.143376).  Saving model ...
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.0641098
	speed: 0.9372s/iter; left time: 666.3739s
Epoch: 45 cost time: 56.93886375427246
Epoch: 45, Steps: 135 | Train Loss: 0.0598402 Vali Loss: 0.1429824 Test Loss: 0.1712596
Validation loss decreased (0.143376 --> 0.142982).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.0603849
	speed: 0.8503s/iter; left time: 489.7712s
Epoch: 46 cost time: 53.97474002838135
Epoch: 46, Steps: 135 | Train Loss: 0.0596263 Vali Loss: 0.1429197 Test Loss: 0.1710929
Validation loss decreased (0.142982 --> 0.142920).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.0600669
	speed: 0.9230s/iter; left time: 407.0378s
Epoch: 47 cost time: 57.812676668167114
Epoch: 47, Steps: 135 | Train Loss: 0.0594477 Vali Loss: 0.1426591 Test Loss: 0.1707884
Validation loss decreased (0.142920 --> 0.142659).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.0590439
	speed: 0.9276s/iter; left time: 283.8371s
Epoch: 48 cost time: 54.805644512176514
Epoch: 48, Steps: 135 | Train Loss: 0.0592561 Vali Loss: 0.1425899 Test Loss: 0.1706832
Validation loss decreased (0.142659 --> 0.142590).  Saving model ...
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.0575958
	speed: 0.8794s/iter; left time: 150.3700s
Epoch: 49 cost time: 54.50184988975525
Epoch: 49, Steps: 135 | Train Loss: 0.0591312 Vali Loss: 0.1425634 Test Loss: 0.1704680
Validation loss decreased (0.142590 --> 0.142563).  Saving model ...
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.0623015
	speed: 0.8784s/iter; left time: 31.6223s
Epoch: 50 cost time: 52.001168966293335
Epoch: 50, Steps: 135 | Train Loss: 0.0589852 Vali Loss: 0.1426213 Test Loss: 0.1703087
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.0497355408796396e-05
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6166487040.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.1937474
	speed: 0.3853s/iter; left time: 2562.9150s
Epoch: 1 cost time: 51.14853358268738
Epoch: 1, Steps: 135 | Train Loss: 0.1734543 Vali Loss: 0.1418030 Test Loss: 0.1679826
Validation loss decreased (inf --> 0.141803).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1632368
	speed: 0.8839s/iter; left time: 5759.3666s
Epoch: 2 cost time: 53.66581869125366
Epoch: 2, Steps: 135 | Train Loss: 0.1729075 Vali Loss: 0.1412925 Test Loss: 0.1678026
Validation loss decreased (0.141803 --> 0.141292).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1683001
	speed: 0.8723s/iter; left time: 5566.1612s
Epoch: 3 cost time: 52.63683342933655
Epoch: 3, Steps: 135 | Train Loss: 0.1726386 Vali Loss: 0.1417120 Test Loss: 0.1676448
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1781410
	speed: 0.8742s/iter; left time: 5460.2016s
Epoch: 4 cost time: 54.232176780700684
Epoch: 4, Steps: 135 | Train Loss: 0.1725631 Vali Loss: 0.1413971 Test Loss: 0.1677424
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1734135
	speed: 0.9923s/iter; left time: 6064.0953s
Epoch: 5 cost time: 59.040579319000244
Epoch: 5, Steps: 135 | Train Loss: 0.1724916 Vali Loss: 0.1415567 Test Loss: 0.1676994
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.1655811220407486, mae:0.261080801486969, rse:0.4049922227859497, corr:[0.45792854 0.46178195 0.46360362 0.46373212 0.46427572 0.46421996
 0.46433887 0.4642336  0.46428657 0.46408182 0.464204   0.4639247
 0.463887   0.46388263 0.46374202 0.46394894 0.46387    0.46393538
 0.4638061  0.46371642 0.46339992 0.46355608 0.4636958  0.463695
 0.46397692 0.46399063 0.46404573 0.4640664  0.4641122  0.4638529
 0.46373817 0.4636578  0.46357635 0.46338725 0.4633363  0.46331093
 0.46313483 0.46322012 0.4630868  0.4631885  0.46331272 0.46346828
 0.46351    0.4632897  0.46303955 0.46291924 0.46322596 0.46327785
 0.46349967 0.46348953 0.46344942 0.46371123 0.46347147 0.46327776
 0.46332565 0.46319336 0.4631304  0.46308288 0.46287024 0.4628856
 0.462821   0.46266204 0.46266776 0.4628578  0.46292332 0.462784
 0.4627687  0.46242297 0.46217516 0.4622254  0.46230152 0.46248385
 0.46252513 0.46260452 0.46272704 0.46264955 0.4625155  0.46252513
 0.4623596  0.46232894 0.46224526 0.462077   0.46212944 0.4620411
 0.4620802  0.4620117  0.46181983 0.46183032 0.46196464 0.46201056
 0.46193287 0.46188083 0.46176088 0.46177143 0.46191046 0.46192637
 0.46196046 0.46204105 0.4619886  0.46216428 0.46219215 0.4620065
 0.46207735 0.46203312 0.46209303 0.46205404 0.46199927 0.4619665
 0.4618093  0.4618302  0.461762   0.4617536  0.4617833  0.46181026
 0.4617198  0.4615399  0.461422   0.4612165  0.46146545 0.46176106
 0.46177027 0.46200278 0.46211958 0.46189925 0.46188864 0.4619927
 0.46201319 0.46176752 0.461622   0.46169186 0.461663   0.46168968
 0.46150684 0.46151885 0.46165746 0.46156815 0.4616201  0.46161205
 0.4616679  0.46147728 0.46108833 0.46097073 0.4610172  0.46122593
 0.46132904 0.4612499  0.4612511  0.4613381  0.46144322 0.46153906
 0.4613547  0.46130356 0.461375   0.46133593 0.4613213  0.46111202
 0.4610972  0.46108648 0.46104366 0.46119887 0.46114573 0.46116522
 0.46125948 0.46100184 0.46095046 0.46114683 0.46093345 0.46075094
 0.4608093  0.4606719  0.46073684 0.46078947 0.4605495  0.46035197
 0.4603269  0.4602648  0.46019816 0.4601157  0.4598733  0.45962408
 0.4596584  0.45958766 0.45939952 0.4594673  0.45937407 0.4592313
 0.45912814 0.45891485 0.4587076  0.45865783 0.45864806 0.4586901
 0.45880595 0.45889503 0.45907938 0.45903453 0.45881948 0.45865515
 0.45866755 0.4586393  0.45833674 0.45837915 0.45836306 0.45831928
 0.45846048 0.45832923 0.45849663 0.4587863  0.45873377 0.4587618
 0.4588046  0.45859832 0.45842347 0.45846358 0.45853373 0.45854446
 0.45849165 0.45845714 0.45859537 0.45874232 0.4585686  0.45844042
 0.45835418 0.45822415 0.45814243 0.45806557 0.45795557 0.4578076
 0.45792502 0.45800596 0.45791125 0.4579669  0.45816135 0.45829394
 0.45811838 0.45780045 0.4576133  0.45761785 0.45774254 0.45773223
 0.45776072 0.4579562  0.4580842  0.4581662  0.45796496 0.4578889
 0.45781916 0.4576261  0.4577735  0.45771217 0.45765132 0.45755085
 0.4573708  0.45752656 0.45759714 0.45761102 0.45763198 0.4576629
 0.4576549  0.45749596 0.4574964  0.45764068 0.45777202 0.4579241
 0.45786032 0.4577989  0.4579389  0.4579657  0.45782855 0.45759836
 0.45764446 0.4575406  0.45742118 0.4576266  0.4575051  0.45745245
 0.45745713 0.45744148 0.45752934 0.45743784 0.45755994 0.45754904
 0.45735663 0.45729712 0.45712507 0.4571236  0.45750102 0.45765123
 0.45771074 0.45780417 0.4578745  0.45780882 0.45768893 0.45763937
 0.45751402 0.45750666 0.45739964 0.45723554 0.4571775  0.45703688
 0.45713514 0.4570411  0.45708928 0.45726806 0.45729682 0.45736217
 0.45701042 0.45705864 0.45674112 0.45620456 0.45668933 0.45693168
 0.45693997 0.4570431  0.45691127 0.45695978 0.45679283 0.45650068
 0.45650423 0.4561367  0.45619056 0.45633876 0.45632362 0.45624447
 0.45595077 0.45614    0.45598087 0.4562848  0.4558605  0.45588297
 0.45606136 0.45555845 0.45571372 0.45573148 0.4558303  0.45675594]
