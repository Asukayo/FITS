Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3083243520.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7130747
	speed: 0.2790s/iter; left time: 3752.5349s
	iters: 200, epoch: 1 | loss: 0.6177164
	speed: 0.2829s/iter; left time: 3776.9454s
Epoch: 1 cost time: 77.84480023384094
Epoch: 1, Steps: 271 | Train Loss: 0.7228120 Vali Loss: 0.5520736 Test Loss: 0.6405698
Validation loss decreased (inf --> 0.552074).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4954758
	speed: 1.0075s/iter; left time: 13278.6103s
	iters: 200, epoch: 2 | loss: 0.4286770
	speed: 0.2573s/iter; left time: 3365.0753s
Epoch: 2 cost time: 73.45742726325989
Epoch: 2, Steps: 271 | Train Loss: 0.4858088 Vali Loss: 0.4360707 Test Loss: 0.5106096
Validation loss decreased (0.552074 --> 0.436071).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3783407
	speed: 0.8958s/iter; left time: 11564.1441s
	iters: 200, epoch: 3 | loss: 0.3504195
	speed: 0.2345s/iter; left time: 3003.4328s
Epoch: 3 cost time: 66.03789591789246
Epoch: 3, Steps: 271 | Train Loss: 0.3591739 Vali Loss: 0.3505473 Test Loss: 0.4142119
Validation loss decreased (0.436071 --> 0.350547).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2783544
	speed: 0.7905s/iter; left time: 9990.5116s
	iters: 200, epoch: 4 | loss: 0.2448124
	speed: 0.2243s/iter; left time: 2812.1051s
Epoch: 4 cost time: 65.05879926681519
Epoch: 4, Steps: 271 | Train Loss: 0.2697071 Vali Loss: 0.2894845 Test Loss: 0.3450337
Validation loss decreased (0.350547 --> 0.289485).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2132429
	speed: 0.8387s/iter; left time: 10372.1241s
	iters: 200, epoch: 5 | loss: 0.1954606
	speed: 0.1962s/iter; left time: 2406.6831s
Epoch: 5 cost time: 60.153213024139404
Epoch: 5, Steps: 271 | Train Loss: 0.2055943 Vali Loss: 0.2420971 Test Loss: 0.2905064
Validation loss decreased (0.289485 --> 0.242097).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1573724
	speed: 0.8595s/iter; left time: 10396.7529s
	iters: 200, epoch: 6 | loss: 0.1542425
	speed: 0.2254s/iter; left time: 2703.4514s
Epoch: 6 cost time: 64.38950371742249
Epoch: 6, Steps: 271 | Train Loss: 0.1596129 Vali Loss: 0.2101710 Test Loss: 0.2532099
Validation loss decreased (0.242097 --> 0.210171).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1198975
	speed: 0.8714s/iter; left time: 10304.3116s
	iters: 200, epoch: 7 | loss: 0.1184019
	speed: 0.2290s/iter; left time: 2684.9655s
Epoch: 7 cost time: 65.74372029304504
Epoch: 7, Steps: 271 | Train Loss: 0.1268461 Vali Loss: 0.1872173 Test Loss: 0.2259961
Validation loss decreased (0.210171 --> 0.187217).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1096940
	speed: 0.8882s/iter; left time: 10262.4139s
	iters: 200, epoch: 8 | loss: 0.1020901
	speed: 0.2409s/iter; left time: 2759.7452s
Epoch: 8 cost time: 71.2873055934906
Epoch: 8, Steps: 271 | Train Loss: 0.1037192 Vali Loss: 0.1701722 Test Loss: 0.2053408
Validation loss decreased (0.187217 --> 0.170172).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.0860827
	speed: 0.9349s/iter; left time: 10548.4787s
	iters: 200, epoch: 9 | loss: 0.0850842
	speed: 0.2705s/iter; left time: 3024.9555s
Epoch: 9 cost time: 74.93363237380981
Epoch: 9, Steps: 271 | Train Loss: 0.0876814 Vali Loss: 0.1597333 Test Loss: 0.1922068
Validation loss decreased (0.170172 --> 0.159733).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.0833191
	speed: 0.8382s/iter; left time: 9230.3694s
	iters: 200, epoch: 10 | loss: 0.0753524
	speed: 0.2654s/iter; left time: 2896.1331s
Epoch: 10 cost time: 68.19391965866089
Epoch: 10, Steps: 271 | Train Loss: 0.0767773 Vali Loss: 0.1527503 Test Loss: 0.1831066
Validation loss decreased (0.159733 --> 0.152750).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.0675985
	speed: 0.8211s/iter; left time: 8819.4130s
	iters: 200, epoch: 11 | loss: 0.0676225
	speed: 0.2619s/iter; left time: 2786.3764s
Epoch: 11 cost time: 69.67879223823547
Epoch: 11, Steps: 271 | Train Loss: 0.0695334 Vali Loss: 0.1484315 Test Loss: 0.1771393
Validation loss decreased (0.152750 --> 0.148431).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.0679495
	speed: 0.8079s/iter; left time: 8458.4875s
	iters: 200, epoch: 12 | loss: 0.0672964
	speed: 0.2184s/iter; left time: 2265.0600s
Epoch: 12 cost time: 61.540573596954346
Epoch: 12, Steps: 271 | Train Loss: 0.0648588 Vali Loss: 0.1454928 Test Loss: 0.1732060
Validation loss decreased (0.148431 --> 0.145493).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.0672460
	speed: 0.7930s/iter; left time: 8088.2567s
	iters: 200, epoch: 13 | loss: 0.0654734
	speed: 0.2279s/iter; left time: 2302.0263s
Epoch: 13 cost time: 65.85694766044617
Epoch: 13, Steps: 271 | Train Loss: 0.0619468 Vali Loss: 0.1439092 Test Loss: 0.1706917
Validation loss decreased (0.145493 --> 0.143909).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.0537707
	speed: 0.8765s/iter; left time: 8701.4645s
	iters: 200, epoch: 14 | loss: 0.0576863
	speed: 0.2463s/iter; left time: 2420.2136s
Epoch: 14 cost time: 68.55087804794312
Epoch: 14, Steps: 271 | Train Loss: 0.0601977 Vali Loss: 0.1428869 Test Loss: 0.1692554
Validation loss decreased (0.143909 --> 0.142887).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.0590934
	speed: 0.8100s/iter; left time: 7822.3307s
	iters: 200, epoch: 15 | loss: 0.0585945
	speed: 0.2411s/iter; left time: 2304.1061s
Epoch: 15 cost time: 67.49809193611145
Epoch: 15, Steps: 271 | Train Loss: 0.0591917 Vali Loss: 0.1427684 Test Loss: 0.1683032
Validation loss decreased (0.142887 --> 0.142768).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.0602710
	speed: 0.8833s/iter; left time: 8290.9554s
	iters: 200, epoch: 16 | loss: 0.0590880
	speed: 0.2298s/iter; left time: 2134.2093s
Epoch: 16 cost time: 67.18411231040955
Epoch: 16, Steps: 271 | Train Loss: 0.0586460 Vali Loss: 0.1425782 Test Loss: 0.1676319
Validation loss decreased (0.142768 --> 0.142578).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.0606251
	speed: 0.8347s/iter; left time: 7608.3962s
	iters: 200, epoch: 17 | loss: 0.0613529
	speed: 0.2303s/iter; left time: 2076.2631s
Epoch: 17 cost time: 64.41159653663635
Epoch: 17, Steps: 271 | Train Loss: 0.0583511 Vali Loss: 0.1423844 Test Loss: 0.1672992
Validation loss decreased (0.142578 --> 0.142384).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.0556457
	speed: 0.8122s/iter; left time: 7182.7214s
	iters: 200, epoch: 18 | loss: 0.0529701
	speed: 0.2391s/iter; left time: 2091.0949s
Epoch: 18 cost time: 65.50774574279785
Epoch: 18, Steps: 271 | Train Loss: 0.0582131 Vali Loss: 0.1425074 Test Loss: 0.1671461
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.0525816
	speed: 0.8391s/iter; left time: 7193.3372s
	iters: 200, epoch: 19 | loss: 0.0599835
	speed: 0.2564s/iter; left time: 2172.5406s
Epoch: 19 cost time: 68.18798232078552
Epoch: 19, Steps: 271 | Train Loss: 0.0581452 Vali Loss: 0.1425077 Test Loss: 0.1670118
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.0549439
	speed: 0.8686s/iter; left time: 7210.9999s
	iters: 200, epoch: 20 | loss: 0.0583807
	speed: 0.2482s/iter; left time: 2035.3324s
Epoch: 20 cost time: 70.59038591384888
Epoch: 20, Steps: 271 | Train Loss: 0.0581193 Vali Loss: 0.1424489 Test Loss: 0.1668770
EarlyStopping counter: 3 out of 3
Early stopping
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3083243520.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.1881762
	speed: 0.2765s/iter; left time: 3718.6591s
	iters: 200, epoch: 1 | loss: 0.1686913
	speed: 0.2520s/iter; left time: 3364.2552s
Epoch: 1 cost time: 71.47090029716492
Epoch: 1, Steps: 271 | Train Loss: 0.1733486 Vali Loss: 0.1415483 Test Loss: 0.1660241
Validation loss decreased (inf --> 0.141548).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1715077
	speed: 0.8255s/iter; left time: 10880.3786s
	iters: 200, epoch: 2 | loss: 0.1681061
	speed: 0.2312s/iter; left time: 3023.7444s
Epoch: 2 cost time: 64.31519269943237
Epoch: 2, Steps: 271 | Train Loss: 0.1728924 Vali Loss: 0.1417312 Test Loss: 0.1658701
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1747301
	speed: 0.8166s/iter; left time: 10541.3363s
	iters: 200, epoch: 3 | loss: 0.1841143
	speed: 0.2247s/iter; left time: 2878.6125s
Epoch: 3 cost time: 63.280104637145996
Epoch: 3, Steps: 271 | Train Loss: 0.1727291 Vali Loss: 0.1415977 Test Loss: 0.1656741
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1717884
	speed: 0.8116s/iter; left time: 10257.3349s
	iters: 200, epoch: 4 | loss: 0.1704568
	speed: 0.2273s/iter; left time: 2849.2655s
Epoch: 4 cost time: 63.239362716674805
Epoch: 4, Steps: 271 | Train Loss: 0.1726571 Vali Loss: 0.1417261 Test Loss: 0.1655698
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.16592416167259216, mae:0.2616627812385559, rse:0.4054115414619446, corr:[0.45668194 0.46225876 0.463333   0.46357223 0.4639481  0.46400356
 0.46414682 0.4638318  0.46408805 0.4639927  0.46411243 0.46383616
 0.46376914 0.46418303 0.46361622 0.46355957 0.46358004 0.46385744
 0.4636189  0.46347252 0.46345997 0.46309695 0.4631848  0.46341428
 0.4636817  0.4639475  0.46406618 0.46373004 0.46382204 0.4637403
 0.4634918  0.4634099  0.46333644 0.46330383 0.46320283 0.4631124
 0.46312597 0.46339607 0.46318015 0.46307844 0.46312273 0.4630452
 0.46310344 0.46281937 0.46259323 0.46253753 0.46252394 0.46267647
 0.46301684 0.46319762 0.46334457 0.46332008 0.46318397 0.4631348
 0.4630314  0.46293265 0.46270645 0.4626974  0.4626536  0.46261182
 0.4625924  0.46255493 0.4626326  0.46271625 0.4627899  0.46260747
 0.46244404 0.46243644 0.46235374 0.46200943 0.46199307 0.46229228
 0.46241465 0.46242496 0.46236086 0.4624377  0.46236023 0.46239296
 0.46206096 0.46179104 0.46203345 0.46182755 0.46175507 0.46169958
 0.46166098 0.4616837  0.46162346 0.46179202 0.46179453 0.46155632
 0.4613689  0.46135113 0.46148807 0.4616619  0.4617051  0.46167934
 0.46178293 0.4620818  0.4621847  0.46214703 0.46220946 0.4621572
 0.46202564 0.4618435  0.4617995  0.46171612 0.46179417 0.46179065
 0.46146116 0.46153608 0.46170112 0.4615711  0.46147218 0.46161047
 0.46139085 0.46128055 0.46155792 0.46131328 0.4613934  0.46164337
 0.46164432 0.4618315  0.46206883 0.4619661  0.4620423  0.46210554
 0.46192747 0.46191534 0.4618294  0.46180144 0.46177277 0.461613
 0.4615862  0.46165016 0.46155074 0.46157622 0.46159574 0.46140978
 0.46133363 0.4611886  0.4613052  0.4611667  0.4609286  0.46119806
 0.46123043 0.46134213 0.46129584 0.4612563  0.46165052 0.4618405
 0.4615829  0.46170378 0.46172872 0.46136093 0.4614905  0.46145263
 0.46140584 0.46153155 0.46127734 0.4612522  0.46133226 0.46132696
 0.46151504 0.46135196 0.4611847  0.46127906 0.4609777  0.46097842
 0.46099028 0.46077865 0.46073535 0.46058404 0.4603539  0.46019432
 0.46024448 0.46012458 0.4599407  0.45993194 0.45987934 0.45974466
 0.4597378  0.45980433 0.4596147  0.4595215  0.45957184 0.4594291
 0.45915952 0.45893246 0.4588578  0.45882323 0.45866412 0.45879152
 0.45913586 0.45924363 0.45931605 0.4592995  0.4591973  0.45895794
 0.45873782 0.45858932 0.4583674  0.45842612 0.45831397 0.45813847
 0.45828417 0.45829275 0.45811662 0.45804912 0.45805988 0.4580148
 0.4580269  0.45804843 0.4580001  0.45780638 0.4580196  0.45823705
 0.4582155  0.45841545 0.45861444 0.45871666 0.458401   0.45825008
 0.45851675 0.4585716  0.45832297 0.45824093 0.45828348 0.45811567
 0.45791683 0.45775005 0.45787284 0.45788932 0.45788527 0.45797166
 0.45772466 0.4575686  0.4575776  0.4574565  0.4573498  0.4575675
 0.45781738 0.45803496 0.45796522 0.45786846 0.4581111  0.45812517
 0.45787925 0.4575577  0.4574712  0.4574613  0.4573921  0.45740938
 0.45727894 0.45711398 0.45696315 0.45686728 0.4569281  0.45700687
 0.45675543 0.45685336 0.4572795  0.45707068 0.4570472  0.4573016
 0.4572669  0.45744365 0.45776674 0.45773947 0.45774466 0.45769414
 0.45772478 0.45780188 0.45742366 0.45732403 0.4574734  0.4575223
 0.45741332 0.45738387 0.45743316 0.45731205 0.457345   0.45703658
 0.45693785 0.45719025 0.45701322 0.45681912 0.45678496 0.45708874
 0.457596   0.45749754 0.45752758 0.45777667 0.45771095 0.4577794
 0.45769146 0.4573683  0.45716983 0.45722035 0.4573659  0.45699978
 0.4569815  0.45699114 0.45686212 0.45687303 0.45662504 0.45658064
 0.45627335 0.45607165 0.45623264 0.45596713 0.45596585 0.4562286
 0.4560977  0.45629346 0.45626345 0.45630458 0.45648265 0.456266
 0.45645455 0.4561324  0.45617962 0.4560579  0.45576602 0.45590836
 0.45561862 0.45591593 0.455455   0.45528919 0.4552723  0.45553732
 0.45541403 0.45522594 0.45535147 0.45495552 0.45508465 0.4563175 ]
