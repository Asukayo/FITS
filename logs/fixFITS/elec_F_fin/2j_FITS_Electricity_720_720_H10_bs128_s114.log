Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16829644800.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 63.83193778991699
Epoch: 1, Steps: 66 | Train Loss: 1.0921538 Vali Loss: 0.7869294 Test Loss: 0.9202493
Validation loss decreased (inf --> 0.786929).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 64.07655596733093
Epoch: 2, Steps: 66 | Train Loss: 0.7998154 Vali Loss: 0.6877987 Test Loss: 0.8092363
Validation loss decreased (0.786929 --> 0.687799).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 67.03141021728516
Epoch: 3, Steps: 66 | Train Loss: 0.7213522 Vali Loss: 0.6456352 Test Loss: 0.7613842
Validation loss decreased (0.687799 --> 0.645635).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 74.059561252594
Epoch: 4, Steps: 66 | Train Loss: 0.6737416 Vali Loss: 0.6137778 Test Loss: 0.7250905
Validation loss decreased (0.645635 --> 0.613778).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 49.760608196258545
Epoch: 5, Steps: 66 | Train Loss: 0.6340646 Vali Loss: 0.5836544 Test Loss: 0.6917385
Validation loss decreased (0.613778 --> 0.583654).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 51.888094902038574
Epoch: 6, Steps: 66 | Train Loss: 0.5991551 Vali Loss: 0.5567594 Test Loss: 0.6609480
Validation loss decreased (0.583654 --> 0.556759).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 48.26183605194092
Epoch: 7, Steps: 66 | Train Loss: 0.5681437 Vali Loss: 0.5340120 Test Loss: 0.6352853
Validation loss decreased (0.556759 --> 0.534012).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 69.03981614112854
Epoch: 8, Steps: 66 | Train Loss: 0.5402906 Vali Loss: 0.5140709 Test Loss: 0.6123211
Validation loss decreased (0.534012 --> 0.514071).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 69.13214159011841
Epoch: 9, Steps: 66 | Train Loss: 0.5153249 Vali Loss: 0.4947999 Test Loss: 0.5909947
Validation loss decreased (0.514071 --> 0.494800).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 69.2158944606781
Epoch: 10, Steps: 66 | Train Loss: 0.4928492 Vali Loss: 0.4770330 Test Loss: 0.5703330
Validation loss decreased (0.494800 --> 0.477033).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 60.34800124168396
Epoch: 11, Steps: 66 | Train Loss: 0.4723986 Vali Loss: 0.4607565 Test Loss: 0.5524218
Validation loss decreased (0.477033 --> 0.460757).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 66.19915080070496
Epoch: 12, Steps: 66 | Train Loss: 0.4537742 Vali Loss: 0.4471121 Test Loss: 0.5362188
Validation loss decreased (0.460757 --> 0.447112).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 66.15237951278687
Epoch: 13, Steps: 66 | Train Loss: 0.4368773 Vali Loss: 0.4337236 Test Loss: 0.5211647
Validation loss decreased (0.447112 --> 0.433724).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 68.4069709777832
Epoch: 14, Steps: 66 | Train Loss: 0.4213595 Vali Loss: 0.4216947 Test Loss: 0.5073055
Validation loss decreased (0.433724 --> 0.421695).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 70.38417959213257
Epoch: 15, Steps: 66 | Train Loss: 0.4071789 Vali Loss: 0.4094227 Test Loss: 0.4936369
Validation loss decreased (0.421695 --> 0.409423).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 69.43102359771729
Epoch: 16, Steps: 66 | Train Loss: 0.3941769 Vali Loss: 0.4000553 Test Loss: 0.4823931
Validation loss decreased (0.409423 --> 0.400055).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 76.07312965393066
Epoch: 17, Steps: 66 | Train Loss: 0.3821981 Vali Loss: 0.3909495 Test Loss: 0.4721825
Validation loss decreased (0.400055 --> 0.390949).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 71.65079641342163
Epoch: 18, Steps: 66 | Train Loss: 0.3711422 Vali Loss: 0.3818606 Test Loss: 0.4613434
Validation loss decreased (0.390949 --> 0.381861).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 68.25439739227295
Epoch: 19, Steps: 66 | Train Loss: 0.3609562 Vali Loss: 0.3734029 Test Loss: 0.4516668
Validation loss decreased (0.381861 --> 0.373403).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 72.36463046073914
Epoch: 20, Steps: 66 | Train Loss: 0.3514984 Vali Loss: 0.3660624 Test Loss: 0.4433546
Validation loss decreased (0.373403 --> 0.366062).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 72.10329532623291
Epoch: 21, Steps: 66 | Train Loss: 0.3427618 Vali Loss: 0.3588487 Test Loss: 0.4348182
Validation loss decreased (0.366062 --> 0.358849).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 74.34258580207825
Epoch: 22, Steps: 66 | Train Loss: 0.3346679 Vali Loss: 0.3530319 Test Loss: 0.4278127
Validation loss decreased (0.358849 --> 0.353032).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 71.54074144363403
Epoch: 23, Steps: 66 | Train Loss: 0.3271160 Vali Loss: 0.3463365 Test Loss: 0.4203510
Validation loss decreased (0.353032 --> 0.346336).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 70.86465883255005
Epoch: 24, Steps: 66 | Train Loss: 0.3201270 Vali Loss: 0.3405361 Test Loss: 0.4139614
Validation loss decreased (0.346336 --> 0.340536).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 73.0233383178711
Epoch: 25, Steps: 66 | Train Loss: 0.3136062 Vali Loss: 0.3352718 Test Loss: 0.4078036
Validation loss decreased (0.340536 --> 0.335272).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 69.15540432929993
Epoch: 26, Steps: 66 | Train Loss: 0.3075038 Vali Loss: 0.3309097 Test Loss: 0.4021932
Validation loss decreased (0.335272 --> 0.330910).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 75.06895613670349
Epoch: 27, Steps: 66 | Train Loss: 0.3018426 Vali Loss: 0.3256034 Test Loss: 0.3964075
Validation loss decreased (0.330910 --> 0.325603).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 68.09813809394836
Epoch: 28, Steps: 66 | Train Loss: 0.2965393 Vali Loss: 0.3211781 Test Loss: 0.3913730
Validation loss decreased (0.325603 --> 0.321178).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 70.58067512512207
Epoch: 29, Steps: 66 | Train Loss: 0.2915987 Vali Loss: 0.3173892 Test Loss: 0.3866891
Validation loss decreased (0.321178 --> 0.317389).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 65.02508401870728
Epoch: 30, Steps: 66 | Train Loss: 0.2869162 Vali Loss: 0.3135841 Test Loss: 0.3827451
Validation loss decreased (0.317389 --> 0.313584).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 80.38965368270874
Epoch: 31, Steps: 66 | Train Loss: 0.2825782 Vali Loss: 0.3099728 Test Loss: 0.3779225
Validation loss decreased (0.313584 --> 0.309973).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 76.14589071273804
Epoch: 32, Steps: 66 | Train Loss: 0.2784827 Vali Loss: 0.3067905 Test Loss: 0.3744577
Validation loss decreased (0.309973 --> 0.306791).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 63.25075006484985
Epoch: 33, Steps: 66 | Train Loss: 0.2746042 Vali Loss: 0.3034754 Test Loss: 0.3705728
Validation loss decreased (0.306791 --> 0.303475).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 65.97444486618042
Epoch: 34, Steps: 66 | Train Loss: 0.2710378 Vali Loss: 0.3007681 Test Loss: 0.3670852
Validation loss decreased (0.303475 --> 0.300768).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 74.89431834220886
Epoch: 35, Steps: 66 | Train Loss: 0.2676135 Vali Loss: 0.2979258 Test Loss: 0.3641167
Validation loss decreased (0.300768 --> 0.297926).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 78.53999471664429
Epoch: 36, Steps: 66 | Train Loss: 0.2644525 Vali Loss: 0.2953492 Test Loss: 0.3610069
Validation loss decreased (0.297926 --> 0.295349).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 85.40652084350586
Epoch: 37, Steps: 66 | Train Loss: 0.2614167 Vali Loss: 0.2930552 Test Loss: 0.3581776
Validation loss decreased (0.295349 --> 0.293055).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 79.69805645942688
Epoch: 38, Steps: 66 | Train Loss: 0.2585723 Vali Loss: 0.2907138 Test Loss: 0.3555639
Validation loss decreased (0.293055 --> 0.290714).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 81.4004979133606
Epoch: 39, Steps: 66 | Train Loss: 0.2558572 Vali Loss: 0.2885186 Test Loss: 0.3527465
Validation loss decreased (0.290714 --> 0.288519).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 70.52997541427612
Epoch: 40, Steps: 66 | Train Loss: 0.2534318 Vali Loss: 0.2863973 Test Loss: 0.3503791
Validation loss decreased (0.288519 --> 0.286397).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 74.80317091941833
Epoch: 41, Steps: 66 | Train Loss: 0.2510478 Vali Loss: 0.2844854 Test Loss: 0.3481159
Validation loss decreased (0.286397 --> 0.284485).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 80.78983330726624
Epoch: 42, Steps: 66 | Train Loss: 0.2488174 Vali Loss: 0.2825723 Test Loss: 0.3459527
Validation loss decreased (0.284485 --> 0.282572).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 67.08455848693848
Epoch: 43, Steps: 66 | Train Loss: 0.2466705 Vali Loss: 0.2809519 Test Loss: 0.3438223
Validation loss decreased (0.282572 --> 0.280952).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 62.36499309539795
Epoch: 44, Steps: 66 | Train Loss: 0.2446870 Vali Loss: 0.2793177 Test Loss: 0.3418347
Validation loss decreased (0.280952 --> 0.279318).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 58.17371129989624
Epoch: 45, Steps: 66 | Train Loss: 0.2428012 Vali Loss: 0.2776510 Test Loss: 0.3401187
Validation loss decreased (0.279318 --> 0.277651).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 69.68697667121887
Epoch: 46, Steps: 66 | Train Loss: 0.2409553 Vali Loss: 0.2762357 Test Loss: 0.3383931
Validation loss decreased (0.277651 --> 0.276236).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 63.77582406997681
Epoch: 47, Steps: 66 | Train Loss: 0.2393087 Vali Loss: 0.2749390 Test Loss: 0.3366776
Validation loss decreased (0.276236 --> 0.274939).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 70.03754472732544
Epoch: 48, Steps: 66 | Train Loss: 0.2376102 Vali Loss: 0.2733116 Test Loss: 0.3351708
Validation loss decreased (0.274939 --> 0.273312).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 61.31649351119995
Epoch: 49, Steps: 66 | Train Loss: 0.2361270 Vali Loss: 0.2723737 Test Loss: 0.3337251
Validation loss decreased (0.273312 --> 0.272374).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 64.1616861820221
Epoch: 50, Steps: 66 | Train Loss: 0.2347057 Vali Loss: 0.2713954 Test Loss: 0.3323591
Validation loss decreased (0.272374 --> 0.271395).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16829644800.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 55.50871777534485
Epoch: 1, Steps: 66 | Train Loss: 0.3054852 Vali Loss: 0.2173466 Test Loss: 0.2668968
Validation loss decreased (inf --> 0.217347).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 57.273348569869995
Epoch: 2, Steps: 66 | Train Loss: 0.2583480 Vali Loss: 0.1918137 Test Loss: 0.2338483
Validation loss decreased (0.217347 --> 0.191814).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 58.58758473396301
Epoch: 3, Steps: 66 | Train Loss: 0.2355108 Vali Loss: 0.1806519 Test Loss: 0.2182625
Validation loss decreased (0.191814 --> 0.180652).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 61.99552512168884
Epoch: 4, Steps: 66 | Train Loss: 0.2252034 Vali Loss: 0.1764708 Test Loss: 0.2110702
Validation loss decreased (0.180652 --> 0.176471).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 59.001221895217896
Epoch: 5, Steps: 66 | Train Loss: 0.2207759 Vali Loss: 0.1751268 Test Loss: 0.2079158
Validation loss decreased (0.176471 --> 0.175127).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 58.06943488121033
Epoch: 6, Steps: 66 | Train Loss: 0.2189049 Vali Loss: 0.1752567 Test Loss: 0.2065899
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 70.14539861679077
Epoch: 7, Steps: 66 | Train Loss: 0.2182023 Vali Loss: 0.1750290 Test Loss: 0.2060009
Validation loss decreased (0.175127 --> 0.175029).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 58.207929849624634
Epoch: 8, Steps: 66 | Train Loss: 0.2179046 Vali Loss: 0.1750843 Test Loss: 0.2057350
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 55.095009088516235
Epoch: 9, Steps: 66 | Train Loss: 0.2178149 Vali Loss: 0.1749559 Test Loss: 0.2056281
Validation loss decreased (0.175029 --> 0.174956).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 58.48375082015991
Epoch: 10, Steps: 66 | Train Loss: 0.2176975 Vali Loss: 0.1745498 Test Loss: 0.2055294
Validation loss decreased (0.174956 --> 0.174550).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 59.726651191711426
Epoch: 11, Steps: 66 | Train Loss: 0.2177064 Vali Loss: 0.1750898 Test Loss: 0.2055160
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 60.897685527801514
Epoch: 12, Steps: 66 | Train Loss: 0.2176030 Vali Loss: 0.1748703 Test Loss: 0.2054591
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 65.34006071090698
Epoch: 13, Steps: 66 | Train Loss: 0.2175308 Vali Loss: 0.1747286 Test Loss: 0.2054256
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.2041293978691101, mae:0.29360997676849365, rse:0.4506923258304596, corr:[0.4470869  0.44692114 0.44836894 0.4500022  0.45068094 0.45110488
 0.4520513  0.45143908 0.45214075 0.45139292 0.45137137 0.45106554
 0.45078054 0.45077392 0.45041868 0.45060056 0.4504341  0.45047256
 0.45042855 0.4500305  0.45016474 0.45004302 0.4503076  0.45050967
 0.45053068 0.45129225 0.4514748  0.4516139  0.45147073 0.4513776
 0.4514139  0.45088363 0.45079735 0.45050728 0.4503978  0.45036528
 0.45016345 0.45028415 0.4502015  0.45029294 0.4503575  0.45030105
 0.45041883 0.45016515 0.44998395 0.4500508  0.45002452 0.45018253
 0.45016065 0.45047435 0.4507968  0.45070258 0.45074093 0.45048916
 0.45049635 0.4503337  0.45009267 0.45021415 0.44997066 0.44979137
 0.4497714  0.44976875 0.4498411  0.4498789  0.45001155 0.45003608
 0.44990766 0.44989455 0.44979477 0.44977966 0.44975302 0.44979423
 0.44984388 0.44979927 0.45002186 0.44984385 0.44980735 0.4498455
 0.44965693 0.44973767 0.44954863 0.44939783 0.44926643 0.44919056
 0.44918704 0.44908226 0.44915515 0.4491824  0.44916788 0.44928873
 0.4492595  0.449143   0.44913885 0.44912958 0.44912732 0.4491231
 0.4492251  0.44920626 0.4492561  0.4493012  0.4490763  0.44910282
 0.44902423 0.44898558 0.4489071  0.4487238  0.44882166 0.44867602
 0.44859245 0.44869322 0.4486045  0.44863662 0.44867432 0.44866404
 0.4486386  0.4484227  0.4484436  0.44845903 0.44845915 0.44867438
 0.44877997 0.44888934 0.4488687  0.44882992 0.4487789  0.448586
 0.4486981  0.44859335 0.44845027 0.44846976 0.44841936 0.44844383
 0.44840452 0.44840842 0.4485319  0.44857398 0.44863945 0.44864085
 0.44856608 0.44857532 0.4485985  0.44866306 0.44860366 0.44862887
 0.44864735 0.44862515 0.44879046 0.44872168 0.44870463 0.44873434
 0.44865134 0.4486093  0.44850004 0.4485109  0.44851264 0.44839576
 0.4483707  0.4483741  0.448382   0.44840685 0.44839844 0.4484956
 0.44850898 0.44842777 0.4484475  0.44836348 0.44835246 0.44812766
 0.4478297  0.447859   0.44804332 0.44808435 0.44793215 0.44791228
 0.44783354 0.44766745 0.4475807  0.44757515 0.44749996 0.44729525
 0.44729632 0.44727215 0.4472444  0.44738546 0.44731167 0.4472621
 0.44729847 0.44708487 0.44688797 0.44674563 0.4467045  0.44668558
 0.44663733 0.4468354  0.44697183 0.44703642 0.44699466 0.4467606
 0.44673654 0.44669044 0.44653797 0.44648954 0.4463321  0.44627774
 0.44625956 0.4461564  0.44631633 0.446336   0.4462281  0.44632986
 0.44630063 0.4461289  0.44602454 0.44598997 0.44593158 0.4458659
 0.44584003 0.44600776 0.4462134  0.44607833 0.4460633  0.44607097
 0.44589335 0.44586375 0.4457068  0.4456356  0.44571856 0.44552004
 0.4455484  0.4456839  0.44563246 0.44562134 0.44569346 0.44576403
 0.4456721  0.44559255 0.44541445 0.44520888 0.44532135 0.44532424
 0.44532263 0.44539356 0.44545668 0.44545636 0.44541857 0.4454305
 0.44537464 0.44522834 0.44504276 0.44496146 0.44495073 0.44497576
 0.44489402 0.4448638  0.444877   0.4448052  0.44486576 0.44488752
 0.4449163  0.44489723 0.44485235 0.44485652 0.4448286  0.444792
 0.44480187 0.4449722  0.4451383  0.44519657 0.44509256 0.44493976
 0.44492286 0.44489828 0.44476622 0.44461364 0.44451118 0.44447458
 0.44446272 0.44437712 0.44442058 0.4445024  0.44450414 0.44445544
 0.44443512 0.44439715 0.4442331  0.444218   0.4443264  0.44445217
 0.4447091  0.4448319  0.44490194 0.4448962  0.4448272  0.44473955
 0.44471154 0.44465542 0.44454357 0.44446823 0.44453865 0.4444281
 0.44428444 0.44445693 0.44457114 0.44453144 0.44451404 0.44453388
 0.44452873 0.4445556  0.44453043 0.4445478  0.4445708  0.44448158
 0.4445391  0.44459262 0.44470036 0.44469315 0.44456047 0.4445507
 0.44455516 0.4444648  0.44432387 0.4443311  0.4443456  0.44425657
 0.44419014 0.44412345 0.4440808  0.44408992 0.44404143 0.44411066
 0.4442533  0.44417697 0.44410875 0.4439722  0.44396624 0.44385734
 0.4434707  0.4435796  0.44371417 0.44367605 0.44363838 0.44342434
 0.443261   0.4432064  0.4430421  0.4428703  0.44280997 0.4427571
 0.44265574 0.44251782 0.4425004  0.44250128 0.44246662 0.4425621
 0.44269657 0.44257486 0.44244874 0.44258055 0.4425222  0.44242853
 0.44236717 0.44242647 0.44265217 0.44263366 0.4425781  0.44247708
 0.44235346 0.44236457 0.4421856  0.44205442 0.44204086 0.44190496
 0.44186568 0.4417433  0.44162035 0.441668   0.44157457 0.44163817
 0.441685   0.44149527 0.44146186 0.44150898 0.44151762 0.44140714
 0.44147715 0.4415841  0.44158453 0.44156933 0.44150603 0.44148907
 0.441433   0.4414243  0.44127685 0.441141   0.44115508 0.4410952
 0.4410947  0.44113165 0.4410632  0.44105536 0.44108856 0.44109413
 0.4411237  0.4410091  0.44095317 0.44091448 0.44083935 0.44086608
 0.44087267 0.4409713  0.44112915 0.4411808  0.44107318 0.44101286
 0.4410455  0.44095877 0.44082564 0.4407342  0.4406076  0.4405809
 0.44066906 0.44072813 0.44069883 0.44060808 0.44070515 0.44069326
 0.44059965 0.44059816 0.44056037 0.440507   0.4403987  0.44044113
 0.44042867 0.44048998 0.44077918 0.44073963 0.44066322 0.44068336
 0.4406766  0.4406977  0.44055334 0.44038063 0.4404273  0.44040328
 0.44040066 0.4404315  0.44039673 0.44044194 0.4403621  0.44041047
 0.44049072 0.44035783 0.44015563 0.4401182  0.44028652 0.4402388
 0.4404946  0.44067124 0.44062603 0.4407295  0.44077408 0.44068667
 0.44061065 0.44066557 0.4405718  0.4403504  0.4403465  0.44028804
 0.4402615  0.4404037  0.44038317 0.4403826  0.44047302 0.44043666
 0.44049522 0.44053808 0.44054198 0.4405572  0.44054195 0.44053966
 0.44061247 0.44066465 0.44070095 0.4408077  0.44077688 0.44070694
 0.44072336 0.44061756 0.4404726  0.44041896 0.44035435 0.44033134
 0.44026184 0.44025767 0.44031706 0.4402057  0.44018903 0.44027168
 0.4403776  0.4403002  0.44016734 0.4401739  0.440076   0.4399812
 0.43973118 0.43964037 0.43980858 0.43980983 0.43976328 0.43963134
 0.43950552 0.43949142 0.4392749  0.43919295 0.43912834 0.43896362
 0.43899715 0.4389667  0.43896723 0.43899426 0.4389086  0.43894657
 0.43883717 0.43874395 0.43866152 0.43846804 0.43850756 0.43840647
 0.43824613 0.4382602  0.43837252 0.43856362 0.43846065 0.43828014
 0.43819484 0.43814513 0.43807834 0.43783963 0.43779293 0.43781886
 0.43768802 0.4377024  0.43766952 0.43765435 0.43762672 0.43762904
 0.43771034 0.4374645  0.43737388 0.4373593  0.43733943 0.43738493
 0.4373111  0.4373424  0.43736666 0.43738282 0.43725142 0.4370621
 0.4370698  0.43698412 0.43689618 0.43668804 0.43652079 0.436586
 0.43637156 0.4363317  0.43641308 0.4362406  0.43632364 0.43616942
 0.43608096 0.4361724  0.4360152  0.43602562 0.4359925  0.43593904
 0.43592644 0.43597513 0.436254   0.43625844 0.4362331  0.43622065
 0.4360232  0.43602923 0.43582624 0.43564755 0.43572646 0.4356036
 0.43550465 0.43548128 0.43552038 0.43545973 0.43538725 0.43548346
 0.43528152 0.43518263 0.43526188 0.4352718  0.43538505 0.43537685
 0.43546832 0.43546116 0.4353662  0.43551382 0.43553522 0.43545914
 0.43537468 0.4352552  0.43514788 0.43481553 0.43487594 0.43496582
 0.4346012  0.4347521  0.43480793 0.4347569  0.43482113 0.43475354
 0.43482155 0.43471527 0.43466154 0.43468535 0.4348144  0.43488565
 0.43503606 0.4351436  0.43510276 0.43523127 0.43523455 0.4350065
 0.43493938 0.4348593  0.43477237 0.4346751  0.4345119  0.4346243
 0.4346213  0.43452093 0.4346428  0.43462884 0.4346254  0.43455306
 0.43463764 0.43464231 0.434572   0.4347192  0.43459082 0.43464205
 0.4345588  0.43458733 0.43481386 0.4347235  0.43469334 0.43453592
 0.43434787 0.43427193 0.43410942 0.4340537  0.43405294 0.434057
 0.43408304 0.43412018 0.43428317 0.43422115 0.4342056  0.434394
 0.43431652 0.43435997 0.43426874 0.43423957 0.43426767 0.433951
 0.4337831  0.4335634  0.43363318 0.43353483 0.43320596 0.4331021
 0.43274584 0.43282023 0.43263653 0.43244117 0.43248832 0.4322699
 0.43240663 0.43237126 0.43235773 0.43266338 0.4325872  0.43271118
 0.43272617 0.43247813 0.43248188 0.43221888 0.43242854 0.43207648
 0.43184608 0.4319236  0.43171015 0.43194607 0.43121096 0.43131545
 0.43094906 0.43083653 0.43125147 0.4307838  0.4315315  0.4313735
 0.4314588  0.4318334  0.43131682 0.43194747 0.43105292 0.43091428
 0.43090972 0.4292795  0.43033782 0.42802924 0.42468876 0.42736626]
