Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4207411200.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7863497
	speed: 0.3650s/iter; left time: 4800.3003s
	iters: 200, epoch: 1 | loss: 0.6930218
	speed: 0.3279s/iter; left time: 4279.4890s
Epoch: 1 cost time: 88.8783745765686
Epoch: 1, Steps: 265 | Train Loss: 0.8228108 Vali Loss: 0.6075426 Test Loss: 0.7174208
Validation loss decreased (inf --> 0.607543).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5769674
	speed: 1.0919s/iter; left time: 14069.6157s
	iters: 200, epoch: 2 | loss: 0.5318202
	speed: 0.3359s/iter; left time: 4294.4954s
Epoch: 2 cost time: 88.32087564468384
Epoch: 2, Steps: 265 | Train Loss: 0.5670707 Vali Loss: 0.4878242 Test Loss: 0.5819598
Validation loss decreased (0.607543 --> 0.487824).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4538110
	speed: 1.0625s/iter; left time: 13410.4025s
	iters: 200, epoch: 3 | loss: 0.4250458
	speed: 0.3090s/iter; left time: 3869.5960s
Epoch: 3 cost time: 83.30377531051636
Epoch: 3, Steps: 265 | Train Loss: 0.4396393 Vali Loss: 0.4007660 Test Loss: 0.4825023
Validation loss decreased (0.487824 --> 0.400766).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3447378
	speed: 1.0976s/iter; left time: 13561.7999s
	iters: 200, epoch: 4 | loss: 0.3207931
	speed: 0.3555s/iter; left time: 4357.2074s
Epoch: 4 cost time: 94.42345309257507
Epoch: 4, Steps: 265 | Train Loss: 0.3480430 Vali Loss: 0.3347703 Test Loss: 0.4058756
Validation loss decreased (0.400766 --> 0.334770).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2995374
	speed: 1.1552s/iter; left time: 13967.6480s
	iters: 200, epoch: 5 | loss: 0.2633356
	speed: 0.3231s/iter; left time: 3874.4808s
Epoch: 5 cost time: 86.57848644256592
Epoch: 5, Steps: 265 | Train Loss: 0.2811130 Vali Loss: 0.2872418 Test Loss: 0.3501693
Validation loss decreased (0.334770 --> 0.287242).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2521855
	speed: 1.0284s/iter; left time: 12162.1146s
	iters: 200, epoch: 6 | loss: 0.2281788
	speed: 0.3305s/iter; left time: 3875.0000s
Epoch: 6 cost time: 82.36324167251587
Epoch: 6, Steps: 265 | Train Loss: 0.2319997 Vali Loss: 0.2531309 Test Loss: 0.3094712
Validation loss decreased (0.287242 --> 0.253131).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1969950
	speed: 0.9787s/iter; left time: 11314.9246s
	iters: 200, epoch: 7 | loss: 0.1932652
	speed: 0.3129s/iter; left time: 3586.5760s
Epoch: 7 cost time: 79.43373847007751
Epoch: 7, Steps: 265 | Train Loss: 0.1960137 Vali Loss: 0.2283349 Test Loss: 0.2791379
Validation loss decreased (0.253131 --> 0.228335).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1762718
	speed: 1.0092s/iter; left time: 11399.7673s
	iters: 200, epoch: 8 | loss: 0.1716301
	speed: 0.3054s/iter; left time: 3419.8022s
Epoch: 8 cost time: 80.9820830821991
Epoch: 8, Steps: 265 | Train Loss: 0.1699306 Vali Loss: 0.2092309 Test Loss: 0.2551648
Validation loss decreased (0.228335 --> 0.209231).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1504551
	speed: 1.0729s/iter; left time: 11834.7973s
	iters: 200, epoch: 9 | loss: 0.1381504
	speed: 0.3160s/iter; left time: 3453.8776s
Epoch: 9 cost time: 87.206547498703
Epoch: 9, Steps: 265 | Train Loss: 0.1511287 Vali Loss: 0.1966196 Test Loss: 0.2392761
Validation loss decreased (0.209231 --> 0.196620).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1473431
	speed: 1.1604s/iter; left time: 12493.0318s
	iters: 200, epoch: 10 | loss: 0.1325143
	speed: 0.2950s/iter; left time: 3146.8587s
Epoch: 10 cost time: 84.87266230583191
Epoch: 10, Steps: 265 | Train Loss: 0.1379034 Vali Loss: 0.1888203 Test Loss: 0.2285137
Validation loss decreased (0.196620 --> 0.188820).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1390913
	speed: 1.0245s/iter; left time: 10758.2774s
	iters: 200, epoch: 11 | loss: 0.1212483
	speed: 0.3177s/iter; left time: 3303.9942s
Epoch: 11 cost time: 84.96013402938843
Epoch: 11, Steps: 265 | Train Loss: 0.1286787 Vali Loss: 0.1832095 Test Loss: 0.2205727
Validation loss decreased (0.188820 --> 0.183210).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1226254
	speed: 1.1142s/iter; left time: 11404.4885s
	iters: 200, epoch: 12 | loss: 0.1215898
	speed: 0.3606s/iter; left time: 3655.4005s
Epoch: 12 cost time: 93.69129347801208
Epoch: 12, Steps: 265 | Train Loss: 0.1224140 Vali Loss: 0.1797534 Test Loss: 0.2151851
Validation loss decreased (0.183210 --> 0.179753).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1240338
	speed: 1.0772s/iter; left time: 10740.5487s
	iters: 200, epoch: 13 | loss: 0.1191656
	speed: 0.2933s/iter; left time: 2894.7656s
Epoch: 13 cost time: 81.96092653274536
Epoch: 13, Steps: 265 | Train Loss: 0.1182858 Vali Loss: 0.1776894 Test Loss: 0.2116068
Validation loss decreased (0.179753 --> 0.177689).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1135650
	speed: 1.0533s/iter; left time: 10222.8831s
	iters: 200, epoch: 14 | loss: 0.1152266
	speed: 0.3402s/iter; left time: 3267.9754s
Epoch: 14 cost time: 87.16343307495117
Epoch: 14, Steps: 265 | Train Loss: 0.1156263 Vali Loss: 0.1763841 Test Loss: 0.2091504
Validation loss decreased (0.177689 --> 0.176384).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1046391
	speed: 1.1135s/iter; left time: 10512.6832s
	iters: 200, epoch: 15 | loss: 0.1075336
	speed: 0.3204s/iter; left time: 2992.6865s
Epoch: 15 cost time: 84.28814625740051
Epoch: 15, Steps: 265 | Train Loss: 0.1139483 Vali Loss: 0.1759538 Test Loss: 0.2077285
Validation loss decreased (0.176384 --> 0.175954).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1160912
	speed: 1.0952s/iter; left time: 10049.3656s
	iters: 200, epoch: 16 | loss: 0.1134012
	speed: 0.3523s/iter; left time: 3197.4696s
Epoch: 16 cost time: 95.67042899131775
Epoch: 16, Steps: 265 | Train Loss: 0.1129957 Vali Loss: 0.1757966 Test Loss: 0.2067322
Validation loss decreased (0.175954 --> 0.175797).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1061219
	speed: 1.1867s/iter; left time: 10574.3370s
	iters: 200, epoch: 17 | loss: 0.1107498
	speed: 0.3244s/iter; left time: 2858.2342s
Epoch: 17 cost time: 87.9493100643158
Epoch: 17, Steps: 265 | Train Loss: 0.1124249 Vali Loss: 0.1756088 Test Loss: 0.2062714
Validation loss decreased (0.175797 --> 0.175609).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1155229
	speed: 1.0951s/iter; left time: 9468.5887s
	iters: 200, epoch: 18 | loss: 0.1220854
	speed: 0.3339s/iter; left time: 2853.2677s
Epoch: 18 cost time: 86.30592393875122
Epoch: 18, Steps: 265 | Train Loss: 0.1121212 Vali Loss: 0.1755604 Test Loss: 0.2059315
Validation loss decreased (0.175609 --> 0.175560).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1127418
	speed: 1.1987s/iter; left time: 10046.7147s
	iters: 200, epoch: 19 | loss: 0.1116818
	speed: 0.3637s/iter; left time: 3011.4239s
Epoch: 19 cost time: 95.55739879608154
Epoch: 19, Steps: 265 | Train Loss: 0.1119583 Vali Loss: 0.1758626 Test Loss: 0.2057917
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1117141
	speed: 1.1268s/iter; left time: 9144.8950s
	iters: 200, epoch: 20 | loss: 0.1218394
	speed: 0.3390s/iter; left time: 2717.7147s
Epoch: 20 cost time: 90.26359844207764
Epoch: 20, Steps: 265 | Train Loss: 0.1118905 Vali Loss: 0.1758023 Test Loss: 0.2056733
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1027733
	speed: 1.1866s/iter; left time: 9315.7973s
	iters: 200, epoch: 21 | loss: 0.1180737
	speed: 0.3324s/iter; left time: 2576.3036s
Epoch: 21 cost time: 92.35612416267395
Epoch: 21, Steps: 265 | Train Loss: 0.1118386 Vali Loss: 0.1758539 Test Loss: 0.2056514
EarlyStopping counter: 3 out of 3
Early stopping
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4207411200.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2176826
	speed: 0.3618s/iter; left time: 4758.0329s
	iters: 200, epoch: 1 | loss: 0.2002592
	speed: 0.3598s/iter; left time: 4695.1738s
Epoch: 1 cost time: 93.5451729297638
Epoch: 1, Steps: 265 | Train Loss: 0.2184453 Vali Loss: 0.1755100 Test Loss: 0.2044541
Validation loss decreased (inf --> 0.175510).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2157015
	speed: 1.1944s/iter; left time: 15391.4823s
	iters: 200, epoch: 2 | loss: 0.2218624
	speed: 0.3166s/iter; left time: 4047.4147s
Epoch: 2 cost time: 89.3215548992157
Epoch: 2, Steps: 265 | Train Loss: 0.2180056 Vali Loss: 0.1751639 Test Loss: 0.2044789
Validation loss decreased (0.175510 --> 0.175164).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2170640
	speed: 1.1205s/iter; left time: 14141.6797s
	iters: 200, epoch: 3 | loss: 0.2199971
	speed: 0.3258s/iter; left time: 4079.5562s
Epoch: 3 cost time: 91.1600341796875
Epoch: 3, Steps: 265 | Train Loss: 0.2178582 Vali Loss: 0.1750714 Test Loss: 0.2042334
Validation loss decreased (0.175164 --> 0.175071).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2212114
	speed: 1.1037s/iter; left time: 13637.6264s
	iters: 200, epoch: 4 | loss: 0.2166148
	speed: 0.3487s/iter; left time: 4274.0094s
Epoch: 4 cost time: 92.32854866981506
Epoch: 4, Steps: 265 | Train Loss: 0.2177216 Vali Loss: 0.1751254 Test Loss: 0.2044736
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2312388
	speed: 1.0438s/iter; left time: 12620.1663s
	iters: 200, epoch: 5 | loss: 0.2385925
	speed: 0.2994s/iter; left time: 3590.4313s
Epoch: 5 cost time: 78.93889307975769
Epoch: 5, Steps: 265 | Train Loss: 0.2176790 Vali Loss: 0.1749588 Test Loss: 0.2043013
Validation loss decreased (0.175071 --> 0.174959).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2361367
	speed: 1.0373s/iter; left time: 12267.5459s
	iters: 200, epoch: 6 | loss: 0.2117489
	speed: 0.3206s/iter; left time: 3759.0811s
Epoch: 6 cost time: 85.47842979431152
Epoch: 6, Steps: 265 | Train Loss: 0.2176216 Vali Loss: 0.1748930 Test Loss: 0.2040880
Validation loss decreased (0.174959 --> 0.174893).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2133532
	speed: 1.0733s/iter; left time: 12408.5710s
	iters: 200, epoch: 7 | loss: 0.2211769
	speed: 0.2860s/iter; left time: 3278.2538s
Epoch: 7 cost time: 76.22883367538452
Epoch: 7, Steps: 265 | Train Loss: 0.2175503 Vali Loss: 0.1747482 Test Loss: 0.2041552
Validation loss decreased (0.174893 --> 0.174748).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2219493
	speed: 1.0983s/iter; left time: 12406.2322s
	iters: 200, epoch: 8 | loss: 0.2053048
	speed: 0.3574s/iter; left time: 4001.5537s
Epoch: 8 cost time: 98.46796464920044
Epoch: 8, Steps: 265 | Train Loss: 0.2175294 Vali Loss: 0.1747370 Test Loss: 0.2041216
Validation loss decreased (0.174748 --> 0.174737).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2233812
	speed: 1.2165s/iter; left time: 13418.6727s
	iters: 200, epoch: 9 | loss: 0.2189585
	speed: 0.3087s/iter; left time: 3373.8933s
Epoch: 9 cost time: 84.1704568862915
Epoch: 9, Steps: 265 | Train Loss: 0.2174987 Vali Loss: 0.1749217 Test Loss: 0.2042117
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2130738
	speed: 1.0486s/iter; left time: 11289.2371s
	iters: 200, epoch: 10 | loss: 0.2275908
	speed: 0.3110s/iter; left time: 3317.1110s
Epoch: 10 cost time: 82.66588020324707
Epoch: 10, Steps: 265 | Train Loss: 0.2174680 Vali Loss: 0.1748562 Test Loss: 0.2041233
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2222493
	speed: 1.1467s/iter; left time: 12041.9891s
	iters: 200, epoch: 11 | loss: 0.2006184
	speed: 0.3345s/iter; left time: 3479.5445s
Epoch: 11 cost time: 88.96477794647217
Epoch: 11, Steps: 265 | Train Loss: 0.2174230 Vali Loss: 0.1747729 Test Loss: 0.2040051
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.20404285192489624, mae:0.2932490408420563, rse:0.45059674978256226, corr:[0.44660515 0.44924575 0.45077628 0.45112595 0.45128807 0.45114163
 0.45137608 0.45126826 0.4508562  0.45108476 0.45095995 0.4507825
 0.45086685 0.45073488 0.45076698 0.45054913 0.45062888 0.45075393
 0.4504543  0.45037016 0.4504678  0.45048687 0.45070612 0.4508477
 0.4507985  0.45102504 0.45129    0.45138672 0.4512809  0.4512149
 0.45105407 0.45090163 0.4506997  0.45059833 0.45063308 0.45026746
 0.45008558 0.45025963 0.45034683 0.45045736 0.45043644 0.45029876
 0.45018727 0.45000178 0.44996428 0.4500395  0.45009327 0.45005858
 0.4500876  0.45047653 0.4507314  0.45067316 0.45076537 0.45070213
 0.4505097  0.45044374 0.45035285 0.45045942 0.45039672 0.45001772
 0.45007366 0.45011783 0.45013258 0.45022085 0.45005083 0.44988132
 0.4497602  0.44964835 0.44947326 0.4492806  0.44938162 0.4497346
 0.4497268  0.4498971  0.45003432 0.44993263 0.45000172 0.44966778
 0.44949758 0.44950068 0.4493219  0.44936147 0.44938678 0.44942558
 0.4493155  0.4492456  0.44935083 0.44937664 0.44941357 0.44935107
 0.4491774  0.4491256  0.44907218 0.44888526 0.44885042 0.4490079
 0.449124   0.44924903 0.4494733  0.44945034 0.44920287 0.44918358
 0.44912088 0.44896686 0.44891602 0.4487672  0.44872195 0.44863907
 0.44850504 0.4487331  0.44873258 0.44868696 0.44871837 0.44858754
 0.44857574 0.44840968 0.44833755 0.44831297 0.44831336 0.44847876
 0.44866717 0.4488906  0.4490103  0.44902477 0.4489565  0.44885284
 0.44872722 0.44840315 0.44840914 0.44858953 0.4485514  0.4485208
 0.44840994 0.44840956 0.44860825 0.44843024 0.44836318 0.44841194
 0.44826254 0.44841638 0.4485809  0.4485474  0.4484245  0.44859
 0.44872576 0.4485428  0.44869822 0.4487361  0.44859374 0.4485532
 0.44855925 0.44861752 0.44852397 0.44818214 0.44812748 0.44833714
 0.44834957 0.44855216 0.44858035 0.44837287 0.44846928 0.44858977
 0.44869846 0.44871432 0.44847223 0.44831526 0.44826305 0.44797587
 0.44784603 0.4481245  0.4482703  0.44815844 0.44801882 0.44792318
 0.44782233 0.4476809  0.44747248 0.4473019  0.4471477  0.44714773
 0.44719917 0.44719198 0.44725323 0.4471165  0.44708338 0.44732782
 0.44736275 0.44714138 0.44695175 0.4468451  0.44691592 0.4467744
 0.44649827 0.44678938 0.44685915 0.44670615 0.44680044 0.44670588
 0.44660112 0.44652128 0.44641092 0.44620708 0.44607368 0.44603792
 0.44588652 0.44606113 0.44625294 0.446073   0.44607407 0.44619042
 0.4462909  0.4461805  0.4458855  0.4459336  0.44602633 0.44591102
 0.44588432 0.44607708 0.44623056 0.4460524  0.44593164 0.4460884
 0.44606993 0.44587097 0.44580168 0.4457377  0.4456573  0.4455673
 0.4455545  0.44571018 0.44566244 0.44561908 0.44565737 0.44555384
 0.44553325 0.4453981  0.4452253  0.44512197 0.44522497 0.44523254
 0.44525683 0.44546828 0.44541457 0.44554287 0.44556078 0.4453204
 0.44527805 0.44513148 0.44502398 0.44494128 0.44488716 0.44486463
 0.44479153 0.4449221  0.44500828 0.44496158 0.44496262 0.44504344
 0.44504932 0.4449403  0.4448275  0.44457728 0.44447517 0.44464958
 0.44477195 0.44491154 0.44512314 0.44512913 0.44501486 0.44500887
 0.44496536 0.44481757 0.4447508  0.44466338 0.44443598 0.44430995
 0.4443264  0.44444788 0.44453833 0.4445136  0.44463924 0.4447202
 0.44467127 0.44454828 0.44444335 0.44451925 0.44450217 0.4444674
 0.44472334 0.44485193 0.44498846 0.44504222 0.4447913  0.4446689
 0.4446459  0.44456434 0.44442475 0.44422343 0.4441512  0.44409308
 0.44408977 0.44417608 0.44438103 0.44438627 0.444106   0.4441212
 0.44429004 0.44442993 0.44450384 0.44454992 0.44464085 0.44455174
 0.44437197 0.44452643 0.4446948  0.44447818 0.44448462 0.44445968
 0.44432035 0.44411767 0.44384304 0.44393545 0.44392043 0.44388026
 0.44381663 0.44385895 0.44405943 0.44379357 0.4438143  0.44409606
 0.44409698 0.44411755 0.44397333 0.4437699  0.443883   0.44384754
 0.44366798 0.44374812 0.44377363 0.4437101  0.4435066  0.44336078
 0.44325727 0.44295096 0.44276744 0.44262865 0.4424303  0.44240043
 0.44232213 0.44240057 0.44249213 0.44237116 0.44251177 0.44262585
 0.44257727 0.4425819  0.4425214  0.44252264 0.4425825  0.44252348
 0.4425243  0.44265902 0.44269958 0.44272816 0.44246218 0.44232586
 0.4422966  0.44197032 0.44176883 0.4416859  0.44159093 0.44141516
 0.44153133 0.44182467 0.4416104  0.44140604 0.4414511  0.4416818
 0.44187185 0.44166163 0.44153404 0.44146258 0.4413745  0.44149235
 0.44161546 0.44174552 0.4417958  0.44169766 0.44162813 0.44155574
 0.44153085 0.44144025 0.4412506  0.44122112 0.4410235  0.4410681
 0.44120544 0.44106528 0.44105822 0.44100836 0.44098112 0.44111675
 0.44116583 0.44100302 0.44095156 0.44093725 0.44075087 0.44073504
 0.4408218  0.44094935 0.4409781  0.4409448  0.44094217 0.4408991
 0.44063407 0.44050184 0.44044945 0.44036335 0.44045618 0.4403261
 0.44031864 0.44055974 0.44063318 0.44057548 0.44042414 0.44045815
 0.44053024 0.44041893 0.44048038 0.4404988  0.44042194 0.4404496
 0.4405538  0.44073328 0.44073582 0.44064733 0.440751   0.44076294
 0.44077072 0.44044372 0.44017947 0.4403425  0.44014406 0.44018582
 0.44043577 0.44036287 0.4404867  0.44034335 0.4401801  0.44042322
 0.44042975 0.44040877 0.4402127  0.4399244  0.44006097 0.440179
 0.44045302 0.44060197 0.44060206 0.44071642 0.44080845 0.44081
 0.4407521  0.44064364 0.4404716  0.44035706 0.4402529  0.44021133
 0.4402514  0.44021565 0.4402702  0.44035888 0.44022536 0.4402274
 0.44049135 0.44041628 0.44046733 0.44058093 0.44031537 0.44039673
 0.4407157  0.44072008 0.4405722  0.4406476  0.44063368 0.44053254
 0.44063583 0.4404846  0.44021833 0.44011748 0.4400376  0.43998453
 0.4400211  0.44012558 0.44024903 0.4401438  0.44005206 0.44012922
 0.44019687 0.44022194 0.44007546 0.43996492 0.43984053 0.43959838
 0.4395425  0.43959534 0.43951347 0.4395521  0.4395365  0.43937957
 0.4393399  0.4392403  0.43916097 0.43912023 0.4388014  0.43867704
 0.43876654 0.43874797 0.43872797 0.43869495 0.43869045 0.4388229
 0.43890244 0.4387931  0.43851173 0.4384053  0.43844545 0.4382153
 0.43820518 0.43839926 0.4382658  0.4383955  0.43847528 0.4382161
 0.4380082  0.43781024 0.4377217  0.43763077 0.4375455  0.43763557
 0.4376521  0.43766758 0.4377304  0.43766788 0.4375372  0.43740726
 0.43742698 0.43738776 0.43716472 0.4372017  0.43733084 0.43718904
 0.43713602 0.4372592  0.4372211  0.43723628 0.4374068  0.43730262
 0.43715438 0.43703833 0.43693173 0.43691394 0.43671393 0.4365895
 0.4366214  0.4366131  0.4366164  0.4364356  0.4363643  0.43626586
 0.43611187 0.43603083 0.43581352 0.4357616  0.43583387 0.4357338
 0.43577036 0.43599132 0.43587032 0.43577412 0.43575904 0.43555805
 0.43567282 0.43573236 0.43560496 0.435556   0.43536136 0.43524572
 0.43527234 0.43526435 0.43531698 0.43531615 0.43518725 0.43527687
 0.43538436 0.43524754 0.43515548 0.43509796 0.4350469  0.43498617
 0.43509996 0.43531722 0.4353529  0.4353387  0.43530264 0.43519062
 0.43507597 0.43498453 0.43490732 0.43477154 0.43471712 0.4346913
 0.43448767 0.43455905 0.43460837 0.43449134 0.4345559  0.43463317
 0.43477634 0.43464932 0.43459776 0.43475226 0.43467715 0.43467522
 0.43509012 0.43538153 0.43536484 0.43537834 0.43542072 0.4353562
 0.435278   0.43514663 0.43500426 0.4349122  0.4346736  0.4345808
 0.4346992  0.43447903 0.4345284  0.43448427 0.43431616 0.4343387
 0.4340894  0.43416217 0.434281   0.43431586 0.43444455 0.43441617
 0.43445876 0.43461072 0.43476993 0.43489823 0.43482774 0.4346962
 0.43461454 0.43454054 0.43451613 0.43440682 0.43433464 0.43399337
 0.4339483  0.43428183 0.434197   0.4343534  0.43415996 0.43405417
 0.43417132 0.43398577 0.4340838  0.43383074 0.4337296  0.43375126
 0.43340945 0.4335278  0.43349555 0.43323347 0.43327174 0.43332
 0.43325934 0.43304843 0.43307203 0.4331325  0.43262297 0.43266714
 0.4327815  0.43274137 0.43287608 0.43252578 0.4323384  0.4323142
 0.43233898 0.432103   0.43191296 0.43202808 0.4321472  0.43204737
 0.43193224 0.43219146 0.43205145 0.4318716  0.4320374  0.43185645
 0.43163252 0.43185234 0.4317205  0.43168303 0.4314002  0.43116924
 0.4313315  0.43109736 0.43135592 0.43113485 0.4311644  0.4311653
 0.42669836 0.43126976 0.4271117  0.42705938 0.42717463 0.42650962]
