Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12332974080.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 50.10613656044006
Epoch: 1, Steps: 67 | Train Loss: 0.9449665 Vali Loss: 0.7127386 Test Loss: 0.8273463
Validation loss decreased (inf --> 0.712739).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 52.22168707847595
Epoch: 2, Steps: 67 | Train Loss: 0.7144550 Vali Loss: 0.6279929 Test Loss: 0.7320775
Validation loss decreased (0.712739 --> 0.627993).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 56.079994916915894
Epoch: 3, Steps: 67 | Train Loss: 0.6424797 Vali Loss: 0.5890051 Test Loss: 0.6874892
Validation loss decreased (0.627993 --> 0.589005).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 52.098312854766846
Epoch: 4, Steps: 67 | Train Loss: 0.5949516 Vali Loss: 0.5586748 Test Loss: 0.6530678
Validation loss decreased (0.589005 --> 0.558675).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 49.98809552192688
Epoch: 5, Steps: 67 | Train Loss: 0.5550373 Vali Loss: 0.5297321 Test Loss: 0.6204265
Validation loss decreased (0.558675 --> 0.529732).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 47.10915684700012
Epoch: 6, Steps: 67 | Train Loss: 0.5199639 Vali Loss: 0.5051867 Test Loss: 0.5927858
Validation loss decreased (0.529732 --> 0.505187).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 47.23251557350159
Epoch: 7, Steps: 67 | Train Loss: 0.4889372 Vali Loss: 0.4818765 Test Loss: 0.5669092
Validation loss decreased (0.505187 --> 0.481876).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 54.15291142463684
Epoch: 8, Steps: 67 | Train Loss: 0.4612971 Vali Loss: 0.4631975 Test Loss: 0.5462893
Validation loss decreased (0.481876 --> 0.463198).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 51.61105275154114
Epoch: 9, Steps: 67 | Train Loss: 0.4364275 Vali Loss: 0.4437966 Test Loss: 0.5242568
Validation loss decreased (0.463198 --> 0.443797).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 51.529911518096924
Epoch: 10, Steps: 67 | Train Loss: 0.4141074 Vali Loss: 0.4263739 Test Loss: 0.5045601
Validation loss decreased (0.443797 --> 0.426374).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 46.961506843566895
Epoch: 11, Steps: 67 | Train Loss: 0.3937611 Vali Loss: 0.4117034 Test Loss: 0.4881926
Validation loss decreased (0.426374 --> 0.411703).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 46.3117413520813
Epoch: 12, Steps: 67 | Train Loss: 0.3754388 Vali Loss: 0.3977584 Test Loss: 0.4725098
Validation loss decreased (0.411703 --> 0.397758).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 47.06767559051514
Epoch: 13, Steps: 67 | Train Loss: 0.3587254 Vali Loss: 0.3853909 Test Loss: 0.4585149
Validation loss decreased (0.397758 --> 0.385391).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 45.820756673812866
Epoch: 14, Steps: 67 | Train Loss: 0.3434830 Vali Loss: 0.3728112 Test Loss: 0.4445417
Validation loss decreased (0.385391 --> 0.372811).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 46.236916065216064
Epoch: 15, Steps: 67 | Train Loss: 0.3295260 Vali Loss: 0.3616627 Test Loss: 0.4318550
Validation loss decreased (0.372811 --> 0.361663).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 50.86463189125061
Epoch: 16, Steps: 67 | Train Loss: 0.3167677 Vali Loss: 0.3515078 Test Loss: 0.4209340
Validation loss decreased (0.361663 --> 0.351508).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 52.90393829345703
Epoch: 17, Steps: 67 | Train Loss: 0.3050237 Vali Loss: 0.3430469 Test Loss: 0.4109294
Validation loss decreased (0.351508 --> 0.343047).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 51.016969203948975
Epoch: 18, Steps: 67 | Train Loss: 0.2941897 Vali Loss: 0.3341459 Test Loss: 0.4004230
Validation loss decreased (0.343047 --> 0.334146).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 49.10538363456726
Epoch: 19, Steps: 67 | Train Loss: 0.2842947 Vali Loss: 0.3256866 Test Loss: 0.3915357
Validation loss decreased (0.334146 --> 0.325687).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 51.95079970359802
Epoch: 20, Steps: 67 | Train Loss: 0.2751368 Vali Loss: 0.3189675 Test Loss: 0.3836556
Validation loss decreased (0.325687 --> 0.318968).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 50.444483518600464
Epoch: 21, Steps: 67 | Train Loss: 0.2665695 Vali Loss: 0.3119404 Test Loss: 0.3752507
Validation loss decreased (0.318968 --> 0.311940).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 51.5184965133667
Epoch: 22, Steps: 67 | Train Loss: 0.2588460 Vali Loss: 0.3046829 Test Loss: 0.3673609
Validation loss decreased (0.311940 --> 0.304683).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 53.9906587600708
Epoch: 23, Steps: 67 | Train Loss: 0.2514948 Vali Loss: 0.2996212 Test Loss: 0.3615521
Validation loss decreased (0.304683 --> 0.299621).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 59.56682991981506
Epoch: 24, Steps: 67 | Train Loss: 0.2447415 Vali Loss: 0.2949553 Test Loss: 0.3559635
Validation loss decreased (0.299621 --> 0.294955).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 55.26313781738281
Epoch: 25, Steps: 67 | Train Loss: 0.2384329 Vali Loss: 0.2893470 Test Loss: 0.3499093
Validation loss decreased (0.294955 --> 0.289347).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 53.24703359603882
Epoch: 26, Steps: 67 | Train Loss: 0.2325534 Vali Loss: 0.2842160 Test Loss: 0.3439787
Validation loss decreased (0.289347 --> 0.284216).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 51.41468667984009
Epoch: 27, Steps: 67 | Train Loss: 0.2271202 Vali Loss: 0.2798665 Test Loss: 0.3393101
Validation loss decreased (0.284216 --> 0.279866).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 55.77888369560242
Epoch: 28, Steps: 67 | Train Loss: 0.2219517 Vali Loss: 0.2756833 Test Loss: 0.3340681
Validation loss decreased (0.279866 --> 0.275683).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 51.16192603111267
Epoch: 29, Steps: 67 | Train Loss: 0.2172024 Vali Loss: 0.2719442 Test Loss: 0.3299219
Validation loss decreased (0.275683 --> 0.271944).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 49.128955364227295
Epoch: 30, Steps: 67 | Train Loss: 0.2127568 Vali Loss: 0.2685744 Test Loss: 0.3258304
Validation loss decreased (0.271944 --> 0.268574).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 48.35240936279297
Epoch: 31, Steps: 67 | Train Loss: 0.2085463 Vali Loss: 0.2646252 Test Loss: 0.3219911
Validation loss decreased (0.268574 --> 0.264625).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 50.982829332351685
Epoch: 32, Steps: 67 | Train Loss: 0.2046030 Vali Loss: 0.2619382 Test Loss: 0.3186029
Validation loss decreased (0.264625 --> 0.261938).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 54.78192734718323
Epoch: 33, Steps: 67 | Train Loss: 0.2009303 Vali Loss: 0.2584989 Test Loss: 0.3147678
Validation loss decreased (0.261938 --> 0.258499).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 44.688279151916504
Epoch: 34, Steps: 67 | Train Loss: 0.1974934 Vali Loss: 0.2561461 Test Loss: 0.3118473
Validation loss decreased (0.258499 --> 0.256146).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 45.2994065284729
Epoch: 35, Steps: 67 | Train Loss: 0.1942796 Vali Loss: 0.2530578 Test Loss: 0.3080970
Validation loss decreased (0.256146 --> 0.253058).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 45.53084588050842
Epoch: 36, Steps: 67 | Train Loss: 0.1912705 Vali Loss: 0.2503550 Test Loss: 0.3053236
Validation loss decreased (0.253058 --> 0.250355).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 47.17526865005493
Epoch: 37, Steps: 67 | Train Loss: 0.1883934 Vali Loss: 0.2488283 Test Loss: 0.3033179
Validation loss decreased (0.250355 --> 0.248828).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 49.57541584968567
Epoch: 38, Steps: 67 | Train Loss: 0.1856508 Vali Loss: 0.2463753 Test Loss: 0.3005051
Validation loss decreased (0.248828 --> 0.246375).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 47.90116357803345
Epoch: 39, Steps: 67 | Train Loss: 0.1831261 Vali Loss: 0.2444001 Test Loss: 0.2982888
Validation loss decreased (0.246375 --> 0.244400).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 46.44631266593933
Epoch: 40, Steps: 67 | Train Loss: 0.1807408 Vali Loss: 0.2425394 Test Loss: 0.2959144
Validation loss decreased (0.244400 --> 0.242539).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 45.656068563461304
Epoch: 41, Steps: 67 | Train Loss: 0.1784920 Vali Loss: 0.2404230 Test Loss: 0.2937053
Validation loss decreased (0.242539 --> 0.240423).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 45.22386288642883
Epoch: 42, Steps: 67 | Train Loss: 0.1763137 Vali Loss: 0.2386872 Test Loss: 0.2917378
Validation loss decreased (0.240423 --> 0.238687).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 44.97727417945862
Epoch: 43, Steps: 67 | Train Loss: 0.1743157 Vali Loss: 0.2370439 Test Loss: 0.2894662
Validation loss decreased (0.238687 --> 0.237044).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 47.35032677650452
Epoch: 44, Steps: 67 | Train Loss: 0.1724074 Vali Loss: 0.2358618 Test Loss: 0.2880057
Validation loss decreased (0.237044 --> 0.235862).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 47.54341459274292
Epoch: 45, Steps: 67 | Train Loss: 0.1706006 Vali Loss: 0.2342917 Test Loss: 0.2864212
Validation loss decreased (0.235862 --> 0.234292).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 45.736807346343994
Epoch: 46, Steps: 67 | Train Loss: 0.1689316 Vali Loss: 0.2329580 Test Loss: 0.2846393
Validation loss decreased (0.234292 --> 0.232958).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 43.59319090843201
Epoch: 47, Steps: 67 | Train Loss: 0.1673516 Vali Loss: 0.2310335 Test Loss: 0.2828871
Validation loss decreased (0.232958 --> 0.231033).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 49.21139574050903
Epoch: 48, Steps: 67 | Train Loss: 0.1657634 Vali Loss: 0.2302685 Test Loss: 0.2813900
Validation loss decreased (0.231033 --> 0.230268).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 48.76932740211487
Epoch: 49, Steps: 67 | Train Loss: 0.1643934 Vali Loss: 0.2287404 Test Loss: 0.2798971
Validation loss decreased (0.230268 --> 0.228740).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 51.361828565597534
Epoch: 50, Steps: 67 | Train Loss: 0.1629659 Vali Loss: 0.2277747 Test Loss: 0.2787435
Validation loss decreased (0.228740 --> 0.227775).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 17357
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12332974080.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 52.728878021240234
Epoch: 1, Steps: 67 | Train Loss: 0.2273128 Vali Loss: 0.1577376 Test Loss: 0.1951702
Validation loss decreased (inf --> 0.157738).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 57.090790033340454
Epoch: 2, Steps: 67 | Train Loss: 0.1829565 Vali Loss: 0.1429035 Test Loss: 0.1756252
Validation loss decreased (0.157738 --> 0.142903).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 50.98994421958923
Epoch: 3, Steps: 67 | Train Loss: 0.1740877 Vali Loss: 0.1415118 Test Loss: 0.1723942
Validation loss decreased (0.142903 --> 0.141512).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 53.3406708240509
Epoch: 4, Steps: 67 | Train Loss: 0.1728593 Vali Loss: 0.1413662 Test Loss: 0.1718951
Validation loss decreased (0.141512 --> 0.141366).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 55.2664577960968
Epoch: 5, Steps: 67 | Train Loss: 0.1726040 Vali Loss: 0.1415949 Test Loss: 0.1717092
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 59.117578744888306
Epoch: 6, Steps: 67 | Train Loss: 0.1725530 Vali Loss: 0.1416406 Test Loss: 0.1716399
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 69.47312378883362
Epoch: 7, Steps: 67 | Train Loss: 0.1725385 Vali Loss: 0.1411482 Test Loss: 0.1716008
Validation loss decreased (0.141366 --> 0.141148).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 53.99749755859375
Epoch: 8, Steps: 67 | Train Loss: 0.1722991 Vali Loss: 0.1415720 Test Loss: 0.1715660
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 55.476940631866455
Epoch: 9, Steps: 67 | Train Loss: 0.1723588 Vali Loss: 0.1414022 Test Loss: 0.1715381
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 58.65924143791199
Epoch: 10, Steps: 67 | Train Loss: 0.1724789 Vali Loss: 0.1412326 Test Loss: 0.1714930
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Electricity_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.1652904599905014, mae:0.26079118251800537, rse:0.40463659167289734, corr:[0.45841643 0.46057314 0.4619289  0.46304348 0.46405113 0.46396264
 0.4647686  0.4641254  0.46458563 0.4640134  0.4640645  0.46392658
 0.46374178 0.46365467 0.46353588 0.4634909  0.4633952  0.46354488
 0.4633575  0.46310702 0.46305877 0.46303734 0.46307352 0.46337926
 0.46361154 0.46404174 0.46410492 0.46421972 0.46426073 0.46404594
 0.46411657 0.4639234  0.4638776  0.46369594 0.4634927  0.46327063
 0.46324104 0.46335357 0.46335045 0.4633514  0.46316868 0.46322986
 0.46314842 0.4629478  0.4627982  0.46281406 0.46292105 0.4629419
 0.4629257  0.46309635 0.46330535 0.4632924  0.46334076 0.4631008
 0.46308503 0.46300143 0.4628347  0.4628824  0.46269011 0.4626498
 0.4627541  0.46284378 0.46275592 0.46269405 0.46269995 0.4627239
 0.46258694 0.4622813  0.46217284 0.46219215 0.46223888 0.46232477
 0.46240076 0.46234024 0.46250346 0.462525   0.46241125 0.46241882
 0.4622314  0.46211183 0.4620027  0.46191698 0.46186155 0.46195126
 0.46203125 0.46199805 0.46205986 0.46206364 0.4619859  0.46194026
 0.46194854 0.46182123 0.46176958 0.46179757 0.46199843 0.46210107
 0.46201625 0.4621661  0.46220973 0.46220642 0.4620888  0.46208572
 0.46205148 0.46192843 0.46191314 0.46182853 0.46173227 0.4616434
 0.46171027 0.46180183 0.4618896  0.46176982 0.46176502 0.46189412
 0.4617251  0.46151456 0.46145847 0.46161583 0.46156815 0.4616299
 0.46176696 0.46181887 0.46190557 0.46179014 0.46175495 0.46168998
 0.46183118 0.46187896 0.46171978 0.46168345 0.46166727 0.46150383
 0.46152553 0.46168005 0.46160537 0.46174818 0.4617401  0.46166605
 0.46168116 0.4615008  0.46131638 0.46120316 0.46121517 0.46115625
 0.4610521  0.46111217 0.4612752  0.46124378 0.46123457 0.4611971
 0.46121523 0.46132505 0.46110147 0.4610371  0.4610498  0.4609879
 0.46103168 0.46113458 0.46115616 0.46113467 0.461252   0.46135268
 0.4614032  0.4611156  0.4609219  0.46091804 0.4607414  0.46072108
 0.46061048 0.46050167 0.46065658 0.46061245 0.46045676 0.46034855
 0.460316   0.46024778 0.46007606 0.46003205 0.4598788  0.4596781
 0.45969653 0.45959187 0.4595516  0.45955503 0.45945868 0.45940852
 0.45921606 0.4590152  0.4589209  0.45879972 0.45873642 0.45876583
 0.4588114  0.45900187 0.45905757 0.4590408  0.4590454  0.45893383
 0.45886615 0.45883274 0.45873487 0.45853567 0.45842704 0.45834446
 0.45833167 0.45844668 0.45845428 0.4583726  0.45838267 0.45851025
 0.4583258  0.45811    0.45806494 0.45796835 0.4580012  0.45819423
 0.4583301  0.4583707  0.4585175  0.45860404 0.45850953 0.45841452
 0.45840812 0.4582478  0.45807338 0.45809004 0.45804054 0.457916
 0.45786962 0.45797935 0.45794642 0.45785913 0.45792446 0.45806724
 0.45803973 0.4577237  0.45765835 0.45764205 0.45768854 0.45789105
 0.4578904  0.45795667 0.45819312 0.45822662 0.4581351  0.4580752
 0.45799768 0.45795733 0.4577385  0.4576381  0.45759684 0.45748642
 0.45757163 0.45767382 0.45769766 0.45751476 0.45742628 0.45751163
 0.4575959  0.457488   0.45740122 0.45755026 0.45761794 0.4577355
 0.4578308  0.45778728 0.4577254  0.4577448  0.45763835 0.45767775
 0.457742   0.45751315 0.45742843 0.45735526 0.45729122 0.4572065
 0.45726654 0.45733568 0.45728713 0.4573369  0.45738086 0.45746738
 0.4574645  0.45727873 0.4571697  0.4572832  0.4574524  0.4576366
 0.45773304 0.45769262 0.4576242  0.4574765  0.45747185 0.4572758
 0.4573412  0.4574605  0.45729965 0.45712474 0.4568991  0.4568623
 0.45694357 0.456961   0.4570536  0.4570329  0.4568369  0.4571121
 0.45698935 0.45659012 0.45647484 0.45631602 0.4563596  0.4564262
 0.4565475  0.45647445 0.4564677  0.4564179  0.45632464 0.45636988
 0.4562596  0.45613655 0.45601207 0.4559908  0.45600563 0.45592886
 0.45607534 0.4564885  0.45604107 0.45621872 0.4558753  0.45588276
 0.45567426 0.45527458 0.4557271  0.45527896 0.45619065 0.45678252]
