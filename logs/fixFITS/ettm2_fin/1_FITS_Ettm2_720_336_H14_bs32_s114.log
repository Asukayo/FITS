Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9728768.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4155731
	speed: 0.1464s/iter; left time: 3814.5134s
	iters: 200, epoch: 1 | loss: 0.4317849
	speed: 0.1554s/iter; left time: 4031.5145s
	iters: 300, epoch: 1 | loss: 0.3302385
	speed: 0.1551s/iter; left time: 4008.9581s
	iters: 400, epoch: 1 | loss: 0.5075269
	speed: 0.1591s/iter; left time: 4097.3640s
	iters: 500, epoch: 1 | loss: 0.5016910
	speed: 0.1443s/iter; left time: 3702.5080s
Epoch: 1 cost time: 78.7704873085022
Epoch: 1, Steps: 523 | Train Loss: 0.4364967 Vali Loss: 0.2036384 Test Loss: 0.2819586
Validation loss decreased (inf --> 0.203638).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3789754
	speed: 0.7658s/iter; left time: 19548.6510s
	iters: 200, epoch: 2 | loss: 0.3225738
	speed: 0.1128s/iter; left time: 2868.5670s
	iters: 300, epoch: 2 | loss: 0.2344372
	speed: 0.1115s/iter; left time: 2823.2907s
	iters: 400, epoch: 2 | loss: 0.3843108
	speed: 0.1084s/iter; left time: 2733.9658s
	iters: 500, epoch: 2 | loss: 0.2229471
	speed: 0.1080s/iter; left time: 2713.9057s
Epoch: 2 cost time: 58.824888944625854
Epoch: 2, Steps: 523 | Train Loss: 0.3927837 Vali Loss: 0.1977223 Test Loss: 0.2748801
Validation loss decreased (0.203638 --> 0.197722).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6192068
	speed: 0.7217s/iter; left time: 18046.8837s
	iters: 200, epoch: 3 | loss: 0.4254215
	speed: 0.1037s/iter; left time: 2583.5646s
	iters: 300, epoch: 3 | loss: 0.4554194
	speed: 0.0936s/iter; left time: 2321.3804s
	iters: 400, epoch: 3 | loss: 0.3268971
	speed: 0.0927s/iter; left time: 2291.2967s
	iters: 500, epoch: 3 | loss: 0.4488814
	speed: 0.0982s/iter; left time: 2417.3790s
Epoch: 3 cost time: 52.830363750457764
Epoch: 3, Steps: 523 | Train Loss: 0.3851393 Vali Loss: 0.1957098 Test Loss: 0.2725494
Validation loss decreased (0.197722 --> 0.195710).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4660522
	speed: 0.6420s/iter; left time: 15717.2983s
	iters: 200, epoch: 4 | loss: 0.2872012
	speed: 0.1103s/iter; left time: 2689.4284s
	iters: 300, epoch: 4 | loss: 0.2855597
	speed: 0.1060s/iter; left time: 2574.8513s
	iters: 400, epoch: 4 | loss: 0.3579636
	speed: 0.1096s/iter; left time: 2649.9967s
	iters: 500, epoch: 4 | loss: 0.4259650
	speed: 0.1039s/iter; left time: 2501.1928s
Epoch: 4 cost time: 58.447993516922
Epoch: 4, Steps: 523 | Train Loss: 0.3815419 Vali Loss: 0.1944030 Test Loss: 0.2707721
Validation loss decreased (0.195710 --> 0.194403).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2550521
	speed: 0.7072s/iter; left time: 16943.3180s
	iters: 200, epoch: 5 | loss: 0.3021967
	speed: 0.1007s/iter; left time: 2401.9479s
	iters: 300, epoch: 5 | loss: 0.2889359
	speed: 0.1010s/iter; left time: 2398.6642s
	iters: 400, epoch: 5 | loss: 0.1882085
	speed: 0.1019s/iter; left time: 2411.0533s
	iters: 500, epoch: 5 | loss: 0.4228868
	speed: 0.0996s/iter; left time: 2347.3363s
Epoch: 5 cost time: 53.41042900085449
Epoch: 5, Steps: 523 | Train Loss: 0.3795720 Vali Loss: 0.1941965 Test Loss: 0.2706351
Validation loss decreased (0.194403 --> 0.194196).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3133126
	speed: 0.6884s/iter; left time: 16133.3586s
	iters: 200, epoch: 6 | loss: 0.2360664
	speed: 0.0940s/iter; left time: 2194.4645s
	iters: 300, epoch: 6 | loss: 0.5279418
	speed: 0.0702s/iter; left time: 1632.2172s
	iters: 400, epoch: 6 | loss: 0.3396530
	speed: 0.0701s/iter; left time: 1620.8277s
	iters: 500, epoch: 6 | loss: 0.3305524
	speed: 0.0674s/iter; left time: 1552.5176s
Epoch: 6 cost time: 42.36145567893982
Epoch: 6, Steps: 523 | Train Loss: 0.3782014 Vali Loss: 0.1937850 Test Loss: 0.2699007
Validation loss decreased (0.194196 --> 0.193785).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4235034
	speed: 0.6527s/iter; left time: 14954.5900s
	iters: 200, epoch: 7 | loss: 0.3529963
	speed: 0.1063s/iter; left time: 2425.5596s
	iters: 300, epoch: 7 | loss: 0.2602620
	speed: 0.1053s/iter; left time: 2392.3401s
	iters: 400, epoch: 7 | loss: 0.5612440
	speed: 0.1080s/iter; left time: 2441.5090s
	iters: 500, epoch: 7 | loss: 0.3935459
	speed: 0.1084s/iter; left time: 2439.6050s
Epoch: 7 cost time: 56.705541372299194
Epoch: 7, Steps: 523 | Train Loss: 0.3769093 Vali Loss: 0.1934277 Test Loss: 0.2693929
Validation loss decreased (0.193785 --> 0.193428).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4601536
	speed: 0.6996s/iter; left time: 15664.0572s
	iters: 200, epoch: 8 | loss: 0.3009521
	speed: 0.1079s/iter; left time: 2404.1136s
	iters: 300, epoch: 8 | loss: 0.6275182
	speed: 0.1019s/iter; left time: 2262.2462s
	iters: 400, epoch: 8 | loss: 0.3385197
	speed: 0.1070s/iter; left time: 2362.5669s
	iters: 500, epoch: 8 | loss: 0.2692643
	speed: 0.1060s/iter; left time: 2331.4039s
Epoch: 8 cost time: 55.97697830200195
Epoch: 8, Steps: 523 | Train Loss: 0.3767513 Vali Loss: 0.1930867 Test Loss: 0.2689924
Validation loss decreased (0.193428 --> 0.193087).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3054409
	speed: 0.7164s/iter; left time: 15664.5067s
	iters: 200, epoch: 9 | loss: 0.3360246
	speed: 0.1025s/iter; left time: 2231.2687s
	iters: 300, epoch: 9 | loss: 0.4132037
	speed: 0.1044s/iter; left time: 2262.3069s
	iters: 400, epoch: 9 | loss: 0.5244650
	speed: 0.1043s/iter; left time: 2250.1674s
	iters: 500, epoch: 9 | loss: 0.4822395
	speed: 0.1037s/iter; left time: 2226.8450s
Epoch: 9 cost time: 54.77560472488403
Epoch: 9, Steps: 523 | Train Loss: 0.3763654 Vali Loss: 0.1929794 Test Loss: 0.2689265
Validation loss decreased (0.193087 --> 0.192979).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4890608
	speed: 0.7200s/iter; left time: 15367.2942s
	iters: 200, epoch: 10 | loss: 0.2588432
	speed: 0.1051s/iter; left time: 2232.6306s
	iters: 300, epoch: 10 | loss: 0.3524638
	speed: 0.1069s/iter; left time: 2259.7539s
	iters: 400, epoch: 10 | loss: 0.2430370
	speed: 0.1050s/iter; left time: 2210.1725s
	iters: 500, epoch: 10 | loss: 0.3705607
	speed: 0.1073s/iter; left time: 2248.0071s
Epoch: 10 cost time: 56.524733781814575
Epoch: 10, Steps: 523 | Train Loss: 0.3759184 Vali Loss: 0.1923964 Test Loss: 0.2686333
Validation loss decreased (0.192979 --> 0.192396).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4147528
	speed: 0.7227s/iter; left time: 15046.9373s
	iters: 200, epoch: 11 | loss: 0.2193878
	speed: 0.1084s/iter; left time: 2246.0236s
	iters: 300, epoch: 11 | loss: 0.4681136
	speed: 0.1051s/iter; left time: 2167.8536s
	iters: 400, epoch: 11 | loss: 0.3893496
	speed: 0.1057s/iter; left time: 2169.6589s
	iters: 500, epoch: 11 | loss: 0.2825382
	speed: 0.1046s/iter; left time: 2136.7526s
Epoch: 11 cost time: 56.43776512145996
Epoch: 11, Steps: 523 | Train Loss: 0.3747924 Vali Loss: 0.1928171 Test Loss: 0.2688803
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2535767
	speed: 0.7013s/iter; left time: 14235.0659s
	iters: 200, epoch: 12 | loss: 0.2719497
	speed: 0.0835s/iter; left time: 1685.8247s
	iters: 300, epoch: 12 | loss: 0.6958150
	speed: 0.0806s/iter; left time: 1619.0139s
	iters: 400, epoch: 12 | loss: 0.3958949
	speed: 0.0803s/iter; left time: 1606.4450s
	iters: 500, epoch: 12 | loss: 0.3823076
	speed: 0.0758s/iter; left time: 1507.2892s
Epoch: 12 cost time: 44.10331702232361
Epoch: 12, Steps: 523 | Train Loss: 0.3753122 Vali Loss: 0.1927769 Test Loss: 0.2684836
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3783193
	speed: 0.5184s/iter; left time: 10250.7380s
	iters: 200, epoch: 13 | loss: 0.2765118
	speed: 0.0994s/iter; left time: 1955.2644s
	iters: 300, epoch: 13 | loss: 0.4485333
	speed: 0.0968s/iter; left time: 1894.8525s
	iters: 400, epoch: 13 | loss: 0.2078210
	speed: 0.0960s/iter; left time: 1869.5537s
	iters: 500, epoch: 13 | loss: 0.3971070
	speed: 0.1026s/iter; left time: 1988.3655s
Epoch: 13 cost time: 50.79609155654907
Epoch: 13, Steps: 523 | Train Loss: 0.3751397 Vali Loss: 0.1926453 Test Loss: 0.2686783
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2688300609588623, mae:0.32606542110443115, rse:0.4187931418418884, corr:[0.55152243 0.5578073  0.55293137 0.5510458  0.55258244 0.55394405
 0.5531359  0.5516307  0.5512815  0.5523788  0.55364496 0.55371517
 0.552848   0.5521521  0.5522714  0.55280656 0.55279475 0.55185914
 0.5506392  0.54987025 0.5496901  0.54965395 0.5492695  0.54853153
 0.54786897 0.54763514 0.54773194 0.5476746  0.5470565  0.5460076
 0.54502344 0.54450047 0.5443168  0.5440508  0.54343027 0.5425554
 0.5416806  0.54102844 0.54057014 0.54011494 0.5395273  0.5388479
 0.5382465  0.5377963  0.5374122  0.5368998  0.5361566  0.535252
 0.53433627 0.5335843  0.5330578  0.53264713 0.5321228  0.5314329
 0.5306963  0.5300973  0.52966857 0.5293625  0.52909875 0.5288278
 0.5285684  0.5283643  0.52816033 0.5278849  0.52756613 0.52730036
 0.5271297  0.52706033 0.5269597  0.5266881  0.52623785 0.52576673
 0.5254342  0.5252431  0.52499527 0.52453667 0.5238126  0.52292347
 0.5220545  0.5213676  0.5207827  0.5202081  0.51959676 0.51902616
 0.5185809  0.5182099  0.5177836  0.5172954  0.51684606 0.5165765
 0.51648486 0.51631874 0.5157505  0.5146612  0.5132026  0.5117375
 0.51058686 0.5097189  0.5087828  0.507563   0.5061057  0.5046839
 0.5035968  0.50277466 0.501904   0.50069463 0.49920794 0.49776298
 0.49666032 0.4959304  0.49531636 0.49455556 0.49368834 0.49290457
 0.4923423  0.4919112  0.49132177 0.49035704 0.48918328 0.48818064
 0.48767787 0.48742855 0.48700827 0.48606116 0.48458862 0.483046
 0.4820437  0.4816742  0.4815035  0.48113376 0.48046604 0.47959423
 0.47866026 0.47765937 0.47649944 0.47522953 0.47415626 0.47366095
 0.47365576 0.47362915 0.47301394 0.47167224 0.46998847 0.46857807
 0.46783257 0.46743625 0.46690118 0.46595123 0.46496418 0.46457088
 0.4649614  0.46552652 0.4653844  0.46420473 0.46241006 0.46099922
 0.46068382 0.46122557 0.46172145 0.4615494  0.4606391  0.45953944
 0.4588735  0.4588283  0.4590319  0.45906    0.45876682 0.45841175
 0.45832    0.45850283 0.45859903 0.45839763 0.4579929  0.45765862
 0.45752656 0.45741454 0.45704684 0.45635155 0.4554957  0.4548051
 0.45421246 0.45342937 0.45228937 0.45089424 0.44968668 0.4490945
 0.44906554 0.44901693 0.44827178 0.4465824  0.44438913 0.44254023
 0.44162235 0.44150797 0.4414516  0.44097027 0.44006786 0.43884665
 0.43738452 0.43560967 0.43359965 0.43168497 0.4303678  0.42986906
 0.42988154 0.42974907 0.4289585  0.42756012 0.42610264 0.4250847
 0.42440978 0.42380518 0.42311668 0.42244324 0.4220148  0.42175204
 0.4213615  0.42050362 0.41921413 0.4177077  0.41624048 0.41495395
 0.4137093  0.41236392 0.41100672 0.4097661  0.408915   0.40847656
 0.40804386 0.40741462 0.40655738 0.40570232 0.404916   0.4040548
 0.4028603  0.40145716 0.4003175  0.40010917 0.40101993 0.40237987
 0.40324    0.40297943 0.40186542 0.40074015 0.40005648 0.39985344
 0.399779   0.39962745 0.39948463 0.39959148 0.40005106 0.40032917
 0.39982373 0.39855263 0.39701805 0.39611316 0.3961347  0.39667353
 0.39698577 0.39677987 0.3964597  0.39663276 0.39735663 0.39813104
 0.39824653 0.39739144 0.39608032 0.39501685 0.39445394 0.39410406
 0.393687   0.39338747 0.39355537 0.39440188 0.3954065  0.39580452
 0.39528164 0.393901   0.3923032  0.39121398 0.3906472  0.39029643
 0.38995403 0.38989982 0.39045078 0.3912824  0.39144623 0.3899819
 0.38708147 0.38433537 0.38322654 0.38385314 0.38489574 0.38491303
 0.38364866 0.3817759  0.38037834 0.3799202  0.37988424 0.37930956
 0.37841374 0.3779948  0.37871101 0.37971917 0.37977803 0.37834156
 0.3763141  0.37531677 0.3764083  0.37850738 0.37946972 0.37835014
 0.37567112 0.3738103  0.37436688 0.37634704 0.37759253 0.37702236
 0.3752608  0.37440363 0.37560198 0.37762117 0.37849322 0.37781405
 0.37678412 0.37704188 0.37844297 0.37900645 0.37755024 0.37494677
 0.37487158 0.37813884 0.3810873  0.38004634 0.3731304  0.36780882]
