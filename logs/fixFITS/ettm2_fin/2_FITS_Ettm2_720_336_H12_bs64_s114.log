Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14721280.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3113401
	speed: 0.1186s/iter; left time: 1535.5191s
	iters: 200, epoch: 1 | loss: 0.2402176
	speed: 0.1329s/iter; left time: 1707.5702s
Epoch: 1 cost time: 30.720040559768677
Epoch: 1, Steps: 261 | Train Loss: 0.3276317 Vali Loss: 0.2356373 Test Loss: 0.3148991
Validation loss decreased (inf --> 0.235637).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2555903
	speed: 0.4558s/iter; left time: 5783.8096s
	iters: 200, epoch: 2 | loss: 0.1813067
	speed: 0.1145s/iter; left time: 1441.6548s
Epoch: 2 cost time: 31.03833818435669
Epoch: 2, Steps: 261 | Train Loss: 0.2101242 Vali Loss: 0.2179049 Test Loss: 0.2929634
Validation loss decreased (0.235637 --> 0.217905).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1841419
	speed: 0.5471s/iter; left time: 6800.4754s
	iters: 200, epoch: 3 | loss: 0.1478557
	speed: 0.1415s/iter; left time: 1744.6395s
Epoch: 3 cost time: 37.10064339637756
Epoch: 3, Steps: 261 | Train Loss: 0.1721713 Vali Loss: 0.2099639 Test Loss: 0.2841465
Validation loss decreased (0.217905 --> 0.209964).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1443801
	speed: 0.6366s/iter; left time: 7745.9676s
	iters: 200, epoch: 4 | loss: 0.1617542
	speed: 0.1322s/iter; left time: 1595.0770s
Epoch: 4 cost time: 35.732877254486084
Epoch: 4, Steps: 261 | Train Loss: 0.1532596 Vali Loss: 0.2053750 Test Loss: 0.2790276
Validation loss decreased (0.209964 --> 0.205375).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1162869
	speed: 0.6823s/iter; left time: 8123.6961s
	iters: 200, epoch: 5 | loss: 0.0957612
	speed: 0.1299s/iter; left time: 1533.6563s
Epoch: 5 cost time: 37.9552206993103
Epoch: 5, Steps: 261 | Train Loss: 0.1429670 Vali Loss: 0.2020573 Test Loss: 0.2757199
Validation loss decreased (0.205375 --> 0.202057).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1600588
	speed: 0.5926s/iter; left time: 6901.0104s
	iters: 200, epoch: 6 | loss: 0.1842584
	speed: 0.1147s/iter; left time: 1323.8117s
Epoch: 6 cost time: 30.46558380126953
Epoch: 6, Steps: 261 | Train Loss: 0.1371939 Vali Loss: 0.2002128 Test Loss: 0.2741831
Validation loss decreased (0.202057 --> 0.200213).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1315093
	speed: 0.5281s/iter; left time: 6011.8943s
	iters: 200, epoch: 7 | loss: 0.1538586
	speed: 0.1136s/iter; left time: 1281.9342s
Epoch: 7 cost time: 31.02258586883545
Epoch: 7, Steps: 261 | Train Loss: 0.1343694 Vali Loss: 0.1987175 Test Loss: 0.2730821
Validation loss decreased (0.200213 --> 0.198718).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.0915793
	speed: 0.4953s/iter; left time: 5509.7054s
	iters: 200, epoch: 8 | loss: 0.1870024
	speed: 0.1198s/iter; left time: 1320.2135s
Epoch: 8 cost time: 31.532636642456055
Epoch: 8, Steps: 261 | Train Loss: 0.1327095 Vali Loss: 0.1978803 Test Loss: 0.2726291
Validation loss decreased (0.198718 --> 0.197880).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1224403
	speed: 0.5228s/iter; left time: 5679.6432s
	iters: 200, epoch: 9 | loss: 0.1207751
	speed: 0.1134s/iter; left time: 1220.6819s
Epoch: 9 cost time: 30.289839267730713
Epoch: 9, Steps: 261 | Train Loss: 0.1317810 Vali Loss: 0.1974227 Test Loss: 0.2724286
Validation loss decreased (0.197880 --> 0.197423).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.0991362
	speed: 0.5508s/iter; left time: 5839.0953s
	iters: 200, epoch: 10 | loss: 0.1000278
	speed: 0.1365s/iter; left time: 1433.9437s
Epoch: 10 cost time: 35.46759819984436
Epoch: 10, Steps: 261 | Train Loss: 0.1311335 Vali Loss: 0.1967245 Test Loss: 0.2720666
Validation loss decreased (0.197423 --> 0.196724).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1163282
	speed: 0.6050s/iter; left time: 6256.6637s
	iters: 200, epoch: 11 | loss: 0.1272179
	speed: 0.1295s/iter; left time: 1326.0803s
Epoch: 11 cost time: 35.76137161254883
Epoch: 11, Steps: 261 | Train Loss: 0.1309661 Vali Loss: 0.1963137 Test Loss: 0.2719690
Validation loss decreased (0.196724 --> 0.196314).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1500290
	speed: 0.6021s/iter; left time: 6069.2233s
	iters: 200, epoch: 12 | loss: 0.1307569
	speed: 0.1103s/iter; left time: 1101.1483s
Epoch: 12 cost time: 30.260016918182373
Epoch: 12, Steps: 261 | Train Loss: 0.1306656 Vali Loss: 0.1963823 Test Loss: 0.2718974
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1439043
	speed: 0.5124s/iter; left time: 5031.5059s
	iters: 200, epoch: 13 | loss: 0.1175665
	speed: 0.1181s/iter; left time: 1147.4318s
Epoch: 13 cost time: 31.436286687850952
Epoch: 13, Steps: 261 | Train Loss: 0.1305804 Vali Loss: 0.1963998 Test Loss: 0.2721363
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1412771
	speed: 0.5352s/iter; left time: 5115.4532s
	iters: 200, epoch: 14 | loss: 0.1458353
	speed: 0.1247s/iter; left time: 1179.1452s
Epoch: 14 cost time: 34.162718057632446
Epoch: 14, Steps: 261 | Train Loss: 0.1304137 Vali Loss: 0.1961887 Test Loss: 0.2719053
Validation loss decreased (0.196314 --> 0.196189).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1753997
	speed: 0.5738s/iter; left time: 5334.9134s
	iters: 200, epoch: 15 | loss: 0.1785049
	speed: 0.1149s/iter; left time: 1056.8783s
Epoch: 15 cost time: 32.79670524597168
Epoch: 15, Steps: 261 | Train Loss: 0.1303999 Vali Loss: 0.1962834 Test Loss: 0.2720904
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1148000
	speed: 0.4788s/iter; left time: 4326.3108s
	iters: 200, epoch: 16 | loss: 0.1192760
	speed: 0.1042s/iter; left time: 931.0462s
Epoch: 16 cost time: 29.505535125732422
Epoch: 16, Steps: 261 | Train Loss: 0.1304347 Vali Loss: 0.1962440 Test Loss: 0.2720584
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1517517
	speed: 0.5958s/iter; left time: 5228.4629s
	iters: 200, epoch: 17 | loss: 0.1450862
	speed: 0.1292s/iter; left time: 1120.8661s
Epoch: 17 cost time: 35.331111431121826
Epoch: 17, Steps: 261 | Train Loss: 0.1303767 Vali Loss: 0.1963108 Test Loss: 0.2720351
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14721280.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3990442
	speed: 0.1391s/iter; left time: 1801.9191s
	iters: 200, epoch: 1 | loss: 0.3206516
	speed: 0.1310s/iter; left time: 1683.1049s
Epoch: 1 cost time: 35.25675320625305
Epoch: 1, Steps: 261 | Train Loss: 0.3796106 Vali Loss: 0.1934831 Test Loss: 0.2693311
Validation loss decreased (inf --> 0.193483).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4038810
	speed: 0.5281s/iter; left time: 6702.0252s
	iters: 200, epoch: 2 | loss: 0.3020734
	speed: 0.1226s/iter; left time: 1543.5655s
Epoch: 2 cost time: 31.403111934661865
Epoch: 2, Steps: 261 | Train Loss: 0.3780065 Vali Loss: 0.1933953 Test Loss: 0.2688026
Validation loss decreased (0.193483 --> 0.193395).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4646789
	speed: 0.5285s/iter; left time: 6568.2277s
	iters: 200, epoch: 3 | loss: 0.3340819
	speed: 0.1226s/iter; left time: 1511.4881s
Epoch: 3 cost time: 32.82966136932373
Epoch: 3, Steps: 261 | Train Loss: 0.3769227 Vali Loss: 0.1932334 Test Loss: 0.2683507
Validation loss decreased (0.193395 --> 0.193233).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4066117
	speed: 0.5546s/iter; left time: 6748.1292s
	iters: 200, epoch: 4 | loss: 0.2933810
	speed: 0.1252s/iter; left time: 1511.4739s
Epoch: 4 cost time: 32.79014182090759
Epoch: 4, Steps: 261 | Train Loss: 0.3762611 Vali Loss: 0.1929686 Test Loss: 0.2680561
Validation loss decreased (0.193233 --> 0.192969).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4108924
	speed: 0.5384s/iter; left time: 6410.7747s
	iters: 200, epoch: 5 | loss: 0.4099561
	speed: 0.1018s/iter; left time: 1202.0726s
Epoch: 5 cost time: 30.65199112892151
Epoch: 5, Steps: 261 | Train Loss: 0.3754857 Vali Loss: 0.1927761 Test Loss: 0.2680607
Validation loss decreased (0.192969 --> 0.192776).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2994891
	speed: 0.5483s/iter; left time: 6385.4833s
	iters: 200, epoch: 6 | loss: 0.5312894
	speed: 0.1437s/iter; left time: 1659.3395s
Epoch: 6 cost time: 37.86328673362732
Epoch: 6, Steps: 261 | Train Loss: 0.3757305 Vali Loss: 0.1929730 Test Loss: 0.2680931
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2464463
	speed: 0.6617s/iter; left time: 7533.3433s
	iters: 200, epoch: 7 | loss: 0.3371881
	speed: 0.1427s/iter; left time: 1609.8160s
Epoch: 7 cost time: 38.77042865753174
Epoch: 7, Steps: 261 | Train Loss: 0.3751756 Vali Loss: 0.1926994 Test Loss: 0.2680281
Validation loss decreased (0.192776 --> 0.192699).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2677419
	speed: 0.6191s/iter; left time: 6886.7097s
	iters: 200, epoch: 8 | loss: 0.3052390
	speed: 0.1248s/iter; left time: 1375.5588s
Epoch: 8 cost time: 34.372819662094116
Epoch: 8, Steps: 261 | Train Loss: 0.3748641 Vali Loss: 0.1927667 Test Loss: 0.2679133
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2919100
	speed: 0.5199s/iter; left time: 5647.1814s
	iters: 200, epoch: 9 | loss: 0.3635419
	speed: 0.0944s/iter; left time: 1016.4558s
Epoch: 9 cost time: 27.034460306167603
Epoch: 9, Steps: 261 | Train Loss: 0.3749803 Vali Loss: 0.1923503 Test Loss: 0.2675404
Validation loss decreased (0.192699 --> 0.192350).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4299316
	speed: 0.4916s/iter; left time: 5212.1394s
	iters: 200, epoch: 10 | loss: 0.2518165
	speed: 0.1044s/iter; left time: 1096.7209s
Epoch: 10 cost time: 26.907535314559937
Epoch: 10, Steps: 261 | Train Loss: 0.3742923 Vali Loss: 0.1922859 Test Loss: 0.2672730
Validation loss decreased (0.192350 --> 0.192286).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2488983
	speed: 0.5098s/iter; left time: 5271.4327s
	iters: 200, epoch: 11 | loss: 0.3159926
	speed: 0.1220s/iter; left time: 1249.1330s
Epoch: 11 cost time: 33.46896505355835
Epoch: 11, Steps: 261 | Train Loss: 0.3746153 Vali Loss: 0.1927291 Test Loss: 0.2678752
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4909225
	speed: 0.3913s/iter; left time: 3944.5764s
	iters: 200, epoch: 12 | loss: 0.4394078
	speed: 0.1002s/iter; left time: 999.8259s
Epoch: 12 cost time: 25.829594612121582
Epoch: 12, Steps: 261 | Train Loss: 0.3744422 Vali Loss: 0.1925483 Test Loss: 0.2677417
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2640960
	speed: 0.5517s/iter; left time: 5416.9291s
	iters: 200, epoch: 13 | loss: 0.2491558
	speed: 0.1443s/iter; left time: 1402.1786s
Epoch: 13 cost time: 36.46709442138672
Epoch: 13, Steps: 261 | Train Loss: 0.3738467 Vali Loss: 0.1924795 Test Loss: 0.2674265
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2682975232601166, mae:0.32591986656188965, rse:0.41837814450263977, corr:[0.5567161  0.5599813  0.55774575 0.5551743  0.55443263 0.5550963
 0.55569786 0.55521023 0.5540256  0.5531289  0.5530569  0.5536346
 0.5541839  0.5540459  0.5532653  0.55228513 0.5515548  0.5511871
 0.55092347 0.5504093  0.5495645  0.5486136  0.5478949  0.5475893
 0.54756296 0.54751664 0.5471828  0.54652345 0.5457483  0.54512066
 0.5447408  0.5445596  0.5443176  0.54378545 0.5429766  0.5420928
 0.54128873 0.54063606 0.54005533 0.5394532  0.53880125 0.5381441
 0.5375889  0.5371879  0.5368864  0.5365053  0.5359035  0.535068
 0.53405184 0.53302103 0.53218234 0.53164864 0.53126734 0.53082407
 0.53020865 0.52951205 0.5288753  0.52845377 0.52829766 0.52827996
 0.52820426 0.52798146 0.5275974  0.527111   0.5266552  0.5263544
 0.5261665  0.5260456  0.5259335  0.5258009  0.5256512  0.525495
 0.5252832  0.52496064 0.5244838  0.5239362  0.5234052  0.52295834
 0.5225782  0.52220845 0.52168465 0.5210026  0.5202212  0.5194664
 0.51886475 0.5184409  0.51809764 0.5177497  0.5173394  0.5168756
 0.51640576 0.5159094  0.5152656  0.51437765 0.5131902  0.51172847
 0.51017827 0.5087859  0.50759006 0.5065451  0.5055617  0.50456196
 0.5035383  0.502482   0.5014246  0.50033075 0.49919027 0.49799824
 0.49676913 0.4956277  0.49474233 0.49414653 0.49375087 0.4932859
 0.49255046 0.49159563 0.49060932 0.48974705 0.4891201  0.48858842
 0.48800075 0.48713997 0.4860265  0.48477152 0.48355418 0.48254362
 0.48179862 0.48117945 0.48050317 0.47970068 0.47885123 0.47807413
 0.47745952 0.47696218 0.47644252 0.47579345 0.47501543 0.47426113
 0.47363612 0.4731696  0.47272727 0.47219637 0.4715562  0.47083738
 0.47022337 0.46977973 0.46956444 0.46934512 0.46890846 0.4681232
 0.46711752 0.46615344 0.46534854 0.46469232 0.46406817 0.46336344
 0.46250555 0.4615914  0.46081176 0.46039027 0.46024466 0.46014994
 0.45983037 0.45923486 0.45847368 0.45777243 0.45724154 0.45689628
 0.45665368 0.45642242 0.45608664 0.4557502  0.45558813 0.45565677
 0.45582178 0.4558331  0.45547992 0.45472312 0.45367587 0.45267296
 0.451864   0.45129645 0.45096916 0.45075315 0.45051602 0.45019165
 0.44978604 0.449291   0.44861284 0.44766033 0.44646403 0.4451813
 0.44405177 0.4432571  0.44259378 0.44181666 0.44086373 0.4397239
 0.4385427  0.43743032 0.43644416 0.43548578 0.4344259  0.43316707
 0.4317397  0.43027183 0.42892393 0.42790496 0.42739937 0.42736885
 0.42738852 0.42711607 0.42637634 0.4251666  0.423698   0.42218897
 0.42091498 0.4200045  0.41949353 0.41910854 0.4184673  0.41741297
 0.41602167 0.41461033 0.4135695  0.41291022 0.41249815 0.4120786
 0.41127986 0.41007617 0.40859258 0.40718994 0.40614218 0.4055623
 0.4052698  0.4050632  0.40470862 0.40416834 0.40354487 0.40289846
 0.40227908 0.4015991  0.4009507  0.40056872 0.4003928  0.40042812
 0.40057892 0.40069622 0.40056965 0.4001431  0.39971188 0.39946038
 0.39942503 0.39955264 0.39952105 0.3992324  0.39869097 0.39815876
 0.3978435  0.39778343 0.39783958 0.3978314  0.39762968 0.39734375
 0.39712715 0.3970489  0.3971709  0.39732897 0.39727053 0.39684826
 0.39615318 0.39547363 0.39493316 0.39466372 0.39459935 0.39468008
 0.39498168 0.3953094  0.39546573 0.395337   0.39474785 0.39378992
 0.39269146 0.3918361  0.39147314 0.39148352 0.39141753 0.39065367
 0.38908756 0.38730133 0.3858925  0.3852823  0.38542724 0.3857487
 0.3857485  0.38507983 0.38385138 0.38262334 0.38206407 0.38207176
 0.38228312 0.38201284 0.3810796  0.37968254 0.3786187  0.37848726
 0.37923437 0.38000545 0.3800892  0.37922198 0.37779582 0.37686038
 0.37682807 0.37760192 0.37842894 0.37866375 0.37830332 0.37791178
 0.37795573 0.37852916 0.37914497 0.37911946 0.3780869  0.37660456
 0.3756439  0.37581784 0.37677062 0.3773889  0.3770151  0.37566796
 0.3745531  0.37446207 0.3751594  0.375283   0.37229362 0.36483538]
