Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13336064.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5145226
	speed: 0.1356s/iter; left time: 3492.7631s
	iters: 200, epoch: 1 | loss: 0.3599589
	speed: 0.1020s/iter; left time: 2616.7967s
	iters: 300, epoch: 1 | loss: 0.2955089
	speed: 0.0895s/iter; left time: 2286.7797s
	iters: 400, epoch: 1 | loss: 0.2245820
	speed: 0.1315s/iter; left time: 3346.7741s
	iters: 500, epoch: 1 | loss: 0.3676367
	speed: 0.1416s/iter; left time: 3588.4626s
Epoch: 1 cost time: 62.37689018249512
Epoch: 1, Steps: 517 | Train Loss: 0.3805715 Vali Loss: 0.2818477 Test Loss: 0.3745894
Validation loss decreased (inf --> 0.281848).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2128899
	speed: 0.8802s/iter; left time: 22211.2113s
	iters: 200, epoch: 2 | loss: 0.1913822
	speed: 0.1374s/iter; left time: 3452.3030s
	iters: 300, epoch: 2 | loss: 0.2807985
	speed: 0.1277s/iter; left time: 3197.7587s
	iters: 400, epoch: 2 | loss: 0.2993738
	speed: 0.1219s/iter; left time: 3039.5817s
	iters: 500, epoch: 2 | loss: 0.2206583
	speed: 0.1268s/iter; left time: 3149.1252s
Epoch: 2 cost time: 68.67578148841858
Epoch: 2, Steps: 517 | Train Loss: 0.2835833 Vali Loss: 0.2709703 Test Loss: 0.3611572
Validation loss decreased (0.281848 --> 0.270970).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2893238
	speed: 0.7749s/iter; left time: 19152.0746s
	iters: 200, epoch: 3 | loss: 0.2761161
	speed: 0.1028s/iter; left time: 2530.4455s
	iters: 300, epoch: 3 | loss: 0.1989127
	speed: 0.1255s/iter; left time: 3076.0817s
	iters: 400, epoch: 3 | loss: 0.2101480
	speed: 0.1374s/iter; left time: 3353.8178s
	iters: 500, epoch: 3 | loss: 0.1985292
	speed: 0.1416s/iter; left time: 3443.6020s
Epoch: 3 cost time: 65.34272384643555
Epoch: 3, Steps: 517 | Train Loss: 0.2639480 Vali Loss: 0.2668017 Test Loss: 0.3554057
Validation loss decreased (0.270970 --> 0.266802).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3545159
	speed: 0.8674s/iter; left time: 20992.2105s
	iters: 200, epoch: 4 | loss: 0.2298841
	speed: 0.1344s/iter; left time: 3239.7407s
	iters: 300, epoch: 4 | loss: 0.2131346
	speed: 0.1243s/iter; left time: 2982.3831s
	iters: 400, epoch: 4 | loss: 0.2639917
	speed: 0.1267s/iter; left time: 3028.0603s
	iters: 500, epoch: 4 | loss: 0.2006239
	speed: 0.1240s/iter; left time: 2951.5123s
Epoch: 4 cost time: 67.68578004837036
Epoch: 4, Steps: 517 | Train Loss: 0.2582368 Vali Loss: 0.2648053 Test Loss: 0.3532599
Validation loss decreased (0.266802 --> 0.264805).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3866055
	speed: 0.8289s/iter; left time: 19630.3814s
	iters: 200, epoch: 5 | loss: 0.3706649
	speed: 0.1249s/iter; left time: 2945.4587s
	iters: 300, epoch: 5 | loss: 0.2809221
	speed: 0.1239s/iter; left time: 2908.7252s
	iters: 400, epoch: 5 | loss: 0.2939477
	speed: 0.1344s/iter; left time: 3142.8806s
	iters: 500, epoch: 5 | loss: 0.2991474
	speed: 0.1264s/iter; left time: 2942.9741s
Epoch: 5 cost time: 65.87959241867065
Epoch: 5, Steps: 517 | Train Loss: 0.2565223 Vali Loss: 0.2638629 Test Loss: 0.3528349
Validation loss decreased (0.264805 --> 0.263863).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1956890
	speed: 0.7978s/iter; left time: 18482.5346s
	iters: 200, epoch: 6 | loss: 0.2766467
	speed: 0.1117s/iter; left time: 2575.7614s
	iters: 300, epoch: 6 | loss: 0.3294736
	speed: 0.1213s/iter; left time: 2785.3288s
	iters: 400, epoch: 6 | loss: 0.2638393
	speed: 0.1151s/iter; left time: 2632.8079s
	iters: 500, epoch: 6 | loss: 0.2793467
	speed: 0.1237s/iter; left time: 2815.5308s
Epoch: 6 cost time: 62.81490468978882
Epoch: 6, Steps: 517 | Train Loss: 0.2556565 Vali Loss: 0.2631588 Test Loss: 0.3519878
Validation loss decreased (0.263863 --> 0.263159).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2413855
	speed: 0.8033s/iter; left time: 18194.6659s
	iters: 200, epoch: 7 | loss: 0.1906896
	speed: 0.1138s/iter; left time: 2565.5043s
	iters: 300, epoch: 7 | loss: 0.1895429
	speed: 0.0977s/iter; left time: 2193.3199s
	iters: 400, epoch: 7 | loss: 0.2251600
	speed: 0.0830s/iter; left time: 1855.6556s
	iters: 500, epoch: 7 | loss: 0.2349084
	speed: 0.1213s/iter; left time: 2699.1251s
Epoch: 7 cost time: 56.331175565719604
Epoch: 7, Steps: 517 | Train Loss: 0.2556995 Vali Loss: 0.2637137 Test Loss: 0.3516396
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2489477
	speed: 0.7217s/iter; left time: 15972.6818s
	iters: 200, epoch: 8 | loss: 0.2628604
	speed: 0.1312s/iter; left time: 2889.9562s
	iters: 300, epoch: 8 | loss: 0.1716177
	speed: 0.1376s/iter; left time: 3018.3530s
	iters: 400, epoch: 8 | loss: 0.2135005
	speed: 0.1379s/iter; left time: 3009.8882s
	iters: 500, epoch: 8 | loss: 0.2627319
	speed: 0.1276s/iter; left time: 2772.7262s
Epoch: 8 cost time: 68.53322768211365
Epoch: 8, Steps: 517 | Train Loss: 0.2554935 Vali Loss: 0.2628586 Test Loss: 0.3519169
Validation loss decreased (0.263159 --> 0.262859).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1500653
	speed: 0.8371s/iter; left time: 18093.4309s
	iters: 200, epoch: 9 | loss: 0.1843261
	speed: 0.0868s/iter; left time: 1867.2727s
	iters: 300, epoch: 9 | loss: 0.4277348
	speed: 0.1011s/iter; left time: 2164.6260s
	iters: 400, epoch: 9 | loss: 0.2095498
	speed: 0.1148s/iter; left time: 2446.7240s
	iters: 500, epoch: 9 | loss: 0.1833039
	speed: 0.1183s/iter; left time: 2510.6528s
Epoch: 9 cost time: 57.679991483688354
Epoch: 9, Steps: 517 | Train Loss: 0.2552215 Vali Loss: 0.2631487 Test Loss: 0.3515699
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2141209
	speed: 0.8261s/iter; left time: 17429.3086s
	iters: 200, epoch: 10 | loss: 0.2656175
	speed: 0.1260s/iter; left time: 2644.9292s
	iters: 300, epoch: 10 | loss: 0.2124977
	speed: 0.1300s/iter; left time: 2715.7830s
	iters: 400, epoch: 10 | loss: 0.2473775
	speed: 0.1311s/iter; left time: 2727.2248s
	iters: 500, epoch: 10 | loss: 0.2363093
	speed: 0.1325s/iter; left time: 2741.9609s
Epoch: 10 cost time: 67.82367825508118
Epoch: 10, Steps: 517 | Train Loss: 0.2553569 Vali Loss: 0.2630007 Test Loss: 0.3513826
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2058995
	speed: 0.8511s/iter; left time: 17516.4516s
	iters: 200, epoch: 11 | loss: 0.3259569
	speed: 0.1301s/iter; left time: 2663.7781s
	iters: 300, epoch: 11 | loss: 0.1962858
	speed: 0.1247s/iter; left time: 2542.1995s
	iters: 400, epoch: 11 | loss: 0.3821381
	speed: 0.1130s/iter; left time: 2292.7646s
	iters: 500, epoch: 11 | loss: 0.2467701
	speed: 0.0853s/iter; left time: 1721.7474s
Epoch: 11 cost time: 61.27028155326843
Epoch: 11, Steps: 517 | Train Loss: 0.2551131 Vali Loss: 0.2626656 Test Loss: 0.3513390
Validation loss decreased (0.262859 --> 0.262666).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2702533
	speed: 0.6352s/iter; left time: 12745.2030s
	iters: 200, epoch: 12 | loss: 0.2368709
	speed: 0.1106s/iter; left time: 2208.7696s
	iters: 300, epoch: 12 | loss: 0.2751554
	speed: 0.1241s/iter; left time: 2465.1774s
	iters: 400, epoch: 12 | loss: 0.2786480
	speed: 0.1303s/iter; left time: 2575.6209s
	iters: 500, epoch: 12 | loss: 0.3129382
	speed: 0.1321s/iter; left time: 2598.1311s
Epoch: 12 cost time: 62.82520508766174
Epoch: 12, Steps: 517 | Train Loss: 0.2552002 Vali Loss: 0.2631517 Test Loss: 0.3512665
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1909174
	speed: 0.7014s/iter; left time: 13709.3512s
	iters: 200, epoch: 13 | loss: 0.2772122
	speed: 0.0830s/iter; left time: 1613.9343s
	iters: 300, epoch: 13 | loss: 0.2772269
	speed: 0.1191s/iter; left time: 2304.1119s
	iters: 400, epoch: 13 | loss: 0.1947940
	speed: 0.1194s/iter; left time: 2298.7693s
	iters: 500, epoch: 13 | loss: 0.2282027
	speed: 0.1240s/iter; left time: 2374.1289s
Epoch: 13 cost time: 55.81743502616882
Epoch: 13, Steps: 517 | Train Loss: 0.2550547 Vali Loss: 0.2625682 Test Loss: 0.3514533
Validation loss decreased (0.262666 --> 0.262568).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2909373
	speed: 0.7647s/iter; left time: 14553.1186s
	iters: 200, epoch: 14 | loss: 0.2199002
	speed: 0.1028s/iter; left time: 1946.5547s
	iters: 300, epoch: 14 | loss: 0.2253835
	speed: 0.1270s/iter; left time: 2391.7841s
	iters: 400, epoch: 14 | loss: 0.3579715
	speed: 0.1323s/iter; left time: 2478.3843s
	iters: 500, epoch: 14 | loss: 0.1918019
	speed: 0.1183s/iter; left time: 2203.1538s
Epoch: 14 cost time: 60.488560914993286
Epoch: 14, Steps: 517 | Train Loss: 0.2549889 Vali Loss: 0.2629031 Test Loss: 0.3511794
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2303313
	speed: 0.8291s/iter; left time: 15348.4263s
	iters: 200, epoch: 15 | loss: 0.2391043
	speed: 0.1273s/iter; left time: 2344.0912s
	iters: 300, epoch: 15 | loss: 0.2948607
	speed: 0.1250s/iter; left time: 2289.6714s
	iters: 400, epoch: 15 | loss: 0.1667322
	speed: 0.1243s/iter; left time: 2264.6370s
	iters: 500, epoch: 15 | loss: 0.2857326
	speed: 0.1255s/iter; left time: 2272.3318s
Epoch: 15 cost time: 66.12299609184265
Epoch: 15, Steps: 517 | Train Loss: 0.2550560 Vali Loss: 0.2624488 Test Loss: 0.3514094
Validation loss decreased (0.262568 --> 0.262449).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2830607
	speed: 0.8160s/iter; left time: 14684.1163s
	iters: 200, epoch: 16 | loss: 0.3552282
	speed: 0.1227s/iter; left time: 2195.5035s
	iters: 300, epoch: 16 | loss: 0.2122366
	speed: 0.1221s/iter; left time: 2172.5137s
	iters: 400, epoch: 16 | loss: 0.2753180
	speed: 0.1318s/iter; left time: 2332.3130s
	iters: 500, epoch: 16 | loss: 0.4678307
	speed: 0.1332s/iter; left time: 2343.0587s
Epoch: 16 cost time: 66.29728317260742
Epoch: 16, Steps: 517 | Train Loss: 0.2550009 Vali Loss: 0.2624217 Test Loss: 0.3511484
Validation loss decreased (0.262449 --> 0.262422).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3572662
	speed: 0.8124s/iter; left time: 14200.2562s
	iters: 200, epoch: 17 | loss: 0.3203289
	speed: 0.1306s/iter; left time: 2269.3151s
	iters: 300, epoch: 17 | loss: 0.1769898
	speed: 0.1268s/iter; left time: 2190.7953s
	iters: 400, epoch: 17 | loss: 0.2571029
	speed: 0.1109s/iter; left time: 1904.8915s
	iters: 500, epoch: 17 | loss: 0.2979660
	speed: 0.1111s/iter; left time: 1896.8087s
Epoch: 17 cost time: 63.798954486846924
Epoch: 17, Steps: 517 | Train Loss: 0.2549927 Vali Loss: 0.2624575 Test Loss: 0.3511041
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3280950
	speed: 0.8355s/iter; left time: 14172.5068s
	iters: 200, epoch: 18 | loss: 0.2005274
	speed: 0.1346s/iter; left time: 2269.9285s
	iters: 300, epoch: 18 | loss: 0.2262096
	speed: 0.1335s/iter; left time: 2237.2367s
	iters: 400, epoch: 18 | loss: 0.2330664
	speed: 0.1343s/iter; left time: 2238.3484s
	iters: 500, epoch: 18 | loss: 0.1588973
	speed: 0.1404s/iter; left time: 2325.3781s
Epoch: 18 cost time: 71.03735446929932
Epoch: 18, Steps: 517 | Train Loss: 0.2550508 Vali Loss: 0.2626229 Test Loss: 0.3512338
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2151217
	speed: 0.8724s/iter; left time: 14346.3088s
	iters: 200, epoch: 19 | loss: 0.2172444
	speed: 0.1344s/iter; left time: 2197.4572s
	iters: 300, epoch: 19 | loss: 0.2366919
	speed: 0.1381s/iter; left time: 2242.8204s
	iters: 400, epoch: 19 | loss: 0.2900245
	speed: 0.1279s/iter; left time: 2065.0741s
	iters: 500, epoch: 19 | loss: 0.2247094
	speed: 0.1296s/iter; left time: 2079.9386s
Epoch: 19 cost time: 69.78428030014038
Epoch: 19, Steps: 517 | Train Loss: 0.2550546 Vali Loss: 0.2624444 Test Loss: 0.3512357
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13336064.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4233419
	speed: 0.1254s/iter; left time: 3228.6519s
	iters: 200, epoch: 1 | loss: 0.4487519
	speed: 0.1342s/iter; left time: 3442.7416s
	iters: 300, epoch: 1 | loss: 0.4546696
	speed: 0.1314s/iter; left time: 3356.3201s
	iters: 400, epoch: 1 | loss: 0.6829394
	speed: 0.1394s/iter; left time: 3548.9954s
	iters: 500, epoch: 1 | loss: 0.3809615
	speed: 0.1428s/iter; left time: 3620.4328s
Epoch: 1 cost time: 69.88623356819153
Epoch: 1, Steps: 517 | Train Loss: 0.4971184 Vali Loss: 0.2613227 Test Loss: 0.3505892
Validation loss decreased (inf --> 0.261323).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6843681
	speed: 0.8841s/iter; left time: 22308.6966s
	iters: 200, epoch: 2 | loss: 0.3089835
	speed: 0.1390s/iter; left time: 3492.5746s
	iters: 300, epoch: 2 | loss: 0.4100853
	speed: 0.1362s/iter; left time: 3409.2443s
	iters: 400, epoch: 2 | loss: 0.6778284
	speed: 0.1290s/iter; left time: 3215.7897s
	iters: 500, epoch: 2 | loss: 0.4695919
	speed: 0.1282s/iter; left time: 3184.4338s
Epoch: 2 cost time: 69.68971848487854
Epoch: 2, Steps: 517 | Train Loss: 0.4959302 Vali Loss: 0.2606675 Test Loss: 0.3503645
Validation loss decreased (0.261323 --> 0.260668).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4294110
	speed: 0.8283s/iter; left time: 20472.2173s
	iters: 200, epoch: 3 | loss: 0.3608905
	speed: 0.1297s/iter; left time: 3191.7065s
	iters: 300, epoch: 3 | loss: 0.5252577
	speed: 0.1293s/iter; left time: 3169.3519s
	iters: 400, epoch: 3 | loss: 0.3296409
	speed: 0.1389s/iter; left time: 3392.3096s
	iters: 500, epoch: 3 | loss: 0.3047031
	speed: 0.1403s/iter; left time: 3410.6806s
Epoch: 3 cost time: 70.40440917015076
Epoch: 3, Steps: 517 | Train Loss: 0.4952884 Vali Loss: 0.2602605 Test Loss: 0.3501329
Validation loss decreased (0.260668 --> 0.260261).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3139839
	speed: 0.8939s/iter; left time: 21632.5437s
	iters: 200, epoch: 4 | loss: 0.3090141
	speed: 0.1385s/iter; left time: 3336.7396s
	iters: 300, epoch: 4 | loss: 0.3752058
	speed: 0.1287s/iter; left time: 3088.1561s
	iters: 400, epoch: 4 | loss: 0.6734915
	speed: 0.1339s/iter; left time: 3200.2171s
	iters: 500, epoch: 4 | loss: 0.3901947
	speed: 0.1207s/iter; left time: 2872.0575s
Epoch: 4 cost time: 68.71749019622803
Epoch: 4, Steps: 517 | Train Loss: 0.4943946 Vali Loss: 0.2605169 Test Loss: 0.3498960
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5651154
	speed: 0.7873s/iter; left time: 18644.6835s
	iters: 200, epoch: 5 | loss: 0.6193368
	speed: 0.1311s/iter; left time: 3090.6376s
	iters: 300, epoch: 5 | loss: 0.5849960
	speed: 0.1279s/iter; left time: 3004.0962s
	iters: 400, epoch: 5 | loss: 0.3336304
	speed: 0.1363s/iter; left time: 3188.0663s
	iters: 500, epoch: 5 | loss: 0.4156259
	speed: 0.1388s/iter; left time: 3232.3862s
Epoch: 5 cost time: 69.12836265563965
Epoch: 5, Steps: 517 | Train Loss: 0.4943998 Vali Loss: 0.2609920 Test Loss: 0.3491429
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3788611
	speed: 0.8760s/iter; left time: 20294.2560s
	iters: 200, epoch: 6 | loss: 0.6379675
	speed: 0.1389s/iter; left time: 3204.7467s
	iters: 300, epoch: 6 | loss: 0.5503097
	speed: 0.1314s/iter; left time: 3017.5057s
	iters: 400, epoch: 6 | loss: 0.4781863
	speed: 0.1317s/iter; left time: 3010.7629s
	iters: 500, epoch: 6 | loss: 0.3933970
	speed: 0.1299s/iter; left time: 2958.0819s
Epoch: 6 cost time: 70.20904016494751
Epoch: 6, Steps: 517 | Train Loss: 0.4943307 Vali Loss: 0.2602719 Test Loss: 0.3495567
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.34948036074638367, mae:0.37821221351623535, rse:0.4751774072647095, corr:[0.54174256 0.5465311  0.54369926 0.5409418  0.5409783  0.54244435
 0.5428857  0.5416227  0.54020154 0.5398622  0.54043263 0.5408739
 0.5403492  0.53913087 0.5381187  0.53774875 0.53769135 0.53730035
 0.53638345 0.5353887  0.5348308  0.5347213  0.5346399  0.53415215
 0.5332321  0.53231204 0.53177834 0.5315856  0.53133523 0.5307
 0.52975094 0.52888095 0.5283729  0.5281791  0.5279742  0.5274594
 0.52662027 0.5256857  0.52486116 0.5242469  0.5237792  0.52332866
 0.52284336 0.52233315 0.5218037  0.5211748  0.5203253  0.5192589
 0.5181551  0.51725537 0.51672226 0.5164599  0.5161549  0.5156051
 0.5148412  0.5140468  0.5134201  0.5130306  0.5127884  0.5125461
 0.51229984 0.5120545  0.51181436 0.5115838  0.5113454  0.51111984
 0.5108849  0.5106853  0.5104322  0.51005185 0.5095467  0.50900304
 0.5085666  0.50828946 0.5080261  0.50756824 0.5068456  0.5059717
 0.5052537  0.5048157  0.504522   0.5041006  0.50339746 0.5024685
 0.50157994 0.5010373  0.5009013  0.50091726 0.50066537 0.499994
 0.49905527 0.49810854 0.49739623 0.49691218 0.49637285 0.49551705
 0.49427664 0.4927817  0.49119785 0.48968706 0.48832315 0.48713183
 0.48612094 0.48523197 0.4843295  0.483277   0.4819982  0.48055735
 0.479173   0.47814184 0.47755057 0.4771973  0.4768212  0.4762164
 0.47543797 0.4746827  0.47400108 0.47319883 0.47202623 0.47039196
 0.46867418 0.46732855 0.46656165 0.4661375  0.4657044  0.46500662
 0.4640875  0.4631667  0.4623825  0.46168578 0.46093234 0.45997965
 0.45891818 0.4579829  0.45732433 0.456817   0.4562442  0.45555022
 0.45485088 0.45435396 0.4540919  0.453904   0.45360073 0.4529752
 0.45201144 0.45095003 0.45018482 0.4497439  0.4494086  0.44889075
 0.4480556  0.44693482 0.44565487 0.44448462 0.44366333 0.44332376
 0.44342887 0.44374022 0.4438813  0.44361246 0.4428666  0.44184574
 0.44084784 0.44019794 0.43994462 0.4398286  0.43954033 0.43896273
 0.4382134  0.4375947  0.4372656  0.43716928 0.43702084 0.43665165
 0.43607417 0.43545586 0.43496558 0.43464932 0.43443686 0.4342342
 0.4339052  0.4334484  0.43291995 0.4322584  0.43138114 0.43021405
 0.4287915  0.42716643 0.42555967 0.42421252 0.42330375 0.42277214
 0.42240012 0.42195678 0.42099914 0.41935512 0.41726816 0.41526026
 0.41393396 0.41347167 0.41346565 0.41320425 0.4121432  0.41015804
 0.40757173 0.4050586  0.40336615 0.40281793 0.4030733  0.40334436
 0.40291843 0.40174446 0.40025237 0.3989979  0.39831746 0.3979877
 0.39753658 0.39659962 0.39532313 0.39401388 0.39297396 0.39229357
 0.39177334 0.3911065  0.39012095 0.3887363  0.38718235 0.38583222
 0.38483012 0.38414922 0.3835219  0.38267913 0.38145137 0.3798853
 0.37821135 0.37689394 0.37629503 0.37650755 0.37732577 0.37822822
 0.3787022  0.3784402  0.3775899  0.37660983 0.37573487 0.3751391
 0.37476158 0.37446612 0.37411967 0.3737115  0.37339756 0.3732297
 0.37325263 0.37354243 0.3739735  0.3744237  0.3747057  0.37474975
 0.3745307  0.37413174 0.37362346 0.3730429  0.37231869 0.3715133
 0.37076563 0.37030682 0.37043098 0.37102306 0.37153858 0.37144256
 0.370731   0.36983752 0.3693505  0.36964804 0.37039816 0.37087134
 0.37055656 0.36932436 0.367627   0.36626068 0.36565918 0.36578402
 0.3661292  0.36631075 0.36626047 0.3660882  0.36583138 0.36521247
 0.36399767 0.36219978 0.36002257 0.3580028  0.3568005  0.35667822
 0.35741875 0.3582289  0.35833555 0.35743198 0.35574317 0.35371557
 0.3519916  0.35087606 0.35041466 0.35016072 0.34979677 0.34917194
 0.34845316 0.34781852 0.3474884  0.3474869  0.34761187 0.34772354
 0.3476525  0.34756735 0.34754506 0.34750378 0.34721756 0.3464446
 0.34509537 0.34354553 0.34233695 0.34179717 0.34185097 0.3421444
 0.34232238 0.3424209  0.34271693 0.3433716  0.34435266 0.3452235
 0.34559664 0.3452328  0.34431553 0.34322405 0.3421927  0.34149012
 0.34109682 0.3408063  0.34056103 0.3403597  0.34014708 0.33989048
 0.3397382  0.33980355 0.33999217 0.34009117 0.33994883 0.33963874
 0.33934727 0.3392884  0.3394044  0.33942485 0.33915532 0.338664
 0.3383645  0.33868524 0.33954856 0.34054318 0.3409731  0.34064502
 0.3398649  0.3392438  0.33914024 0.3393713  0.33940032 0.3389788
 0.33829397 0.338048   0.33860224 0.3396784  0.3405605  0.34050837
 0.33944306 0.33804625 0.33716434 0.33711794 0.3375663  0.33802143
 0.33826983 0.3384685  0.33887348 0.33947748 0.34005344 0.34010738
 0.33948207 0.33841163 0.3374284  0.33683562 0.33651775 0.3362402
 0.33571377 0.33493656 0.33408085 0.333275   0.33245325 0.3314414
 0.33028018 0.3293095  0.3287778  0.32870203 0.32883555 0.32871482
 0.32814503 0.32728484 0.3266043  0.3264667  0.32672185 0.32708475
 0.3272278  0.32708046 0.32664573 0.32598156 0.32523432 0.3244968
 0.3240614  0.32417563 0.32476553 0.3254118  0.32549402 0.32471314
 0.32339975 0.3223963  0.32246238 0.32364523 0.32514748 0.3259214
 0.32536522 0.32364586 0.32177624 0.32085368 0.32128805 0.32255343
 0.3237557  0.32424435 0.3240632  0.3236301  0.32330656 0.3230719
 0.32267338 0.32213584 0.32150942 0.3209065  0.32040426 0.31994802
 0.3192954  0.3185341  0.3179006  0.3175885  0.3176155  0.3177312
 0.31772304 0.31749806 0.31722417 0.31702107 0.31690058 0.31671432
 0.31629092 0.3156816  0.31516647 0.31508395 0.3155367  0.3161238
 0.3164497  0.31628335 0.31577468 0.31532785 0.31515855 0.3151344
 0.31492588 0.31426698 0.31322703 0.3122713  0.31188983 0.31201288
 0.31224957 0.31215253 0.3116214  0.31076202 0.30989203 0.30915475
 0.3084795  0.30781114 0.3070045  0.30611637 0.30524763 0.30436435
 0.30327427 0.30187753 0.3002508  0.29884598 0.29791647 0.29736537
 0.29689378 0.29629543 0.29543316 0.29437733 0.2934712  0.29288018
 0.29251385 0.2920586  0.29126915 0.29016355 0.28892162 0.28797147
 0.28738767 0.28704894 0.28670344 0.28611422 0.28538024 0.28467327
 0.28419444 0.28405157 0.28414845 0.28429142 0.28436777 0.284386
 0.28440166 0.28439867 0.28439268 0.28410572 0.28342077 0.28257236
 0.28196093 0.2818965  0.2822352  0.28252947 0.2823203  0.28140897
 0.2800806  0.2789548  0.2784549  0.27858868 0.27907634 0.27954486
 0.27974176 0.27967006 0.27956298 0.2795222  0.27956867 0.2796707
 0.2797652  0.27980945 0.2796943  0.27938032 0.27882645 0.2781105
 0.27750015 0.27722996 0.2772594  0.2773314  0.27707943 0.2764507
 0.27564028 0.27511027 0.27516344 0.2757243  0.27620888 0.27619144
 0.27544266 0.27431774 0.27337757 0.27318913 0.273893   0.27500448
 0.27581686 0.27585077 0.27515325 0.27408838 0.27313507 0.27257246
 0.27233595 0.27223477 0.2721405  0.27211905 0.27217603 0.27202365
 0.27136403 0.27020437 0.26864454 0.26700923 0.26561823 0.26462132
 0.26386458 0.26305676 0.26208383 0.26108065 0.2602794  0.25980213
 0.25936848 0.25854442 0.2572092  0.2555547  0.2540576  0.25316694
 0.2528657  0.2527217  0.25234923 0.25151533 0.25052688 0.249864
 0.24981935 0.25037992 0.2510706  0.25144118 0.25143    0.2511818
 0.250851   0.25062203 0.25052977 0.25031567 0.24987884 0.24921426
 0.24851942 0.24778047 0.2472432  0.24716596 0.24754694 0.24821079
 0.24895999 0.24965464 0.25007886 0.25023168 0.25023976 0.25030828
 0.2505078  0.25084338 0.2513572  0.2521343  0.2529724  0.25348693
 0.25330263 0.252427   0.25131    0.25044876 0.25050694 0.25125653
 0.25201318 0.25209895 0.25125158 0.25013125 0.24943975 0.24974102
 0.25079492 0.25197205 0.25255227 0.25240093 0.2521202  0.25191247
 0.25197765 0.2520096  0.25168863 0.25097492 0.2503313  0.25002974
 0.25036424 0.25103703 0.2515604  0.2517816  0.2516687  0.25124884
 0.25058934 0.24995558 0.24939269 0.24910247 0.24930005 0.2498219
 0.25027835 0.25036734 0.2499904  0.24933061 0.24878325 0.24857272
 0.24847537 0.2480013  0.24679701 0.24498916 0.24321893 0.24197619
 0.241397   0.24122068 0.2413009  0.24115716 0.24068609 0.23990627
 0.23899518 0.23815688 0.23757356 0.23731753 0.2374066  0.23742875
 0.23709512 0.236239   0.23499052 0.23400442 0.23355423 0.23353197
 0.23350473 0.23289241 0.2316165  0.22990023 0.22840291 0.22753902
 0.227469   0.22772709 0.2281079  0.22848164 0.2286465  0.22838709
 0.22750753 0.22637203 0.22562674 0.22596256 0.22676367 0.22695303
 0.22557192 0.22307616 0.2213175  0.22119966 0.21993631 0.21125963]
