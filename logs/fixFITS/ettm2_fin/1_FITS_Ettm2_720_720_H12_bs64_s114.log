Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20134912.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4635371
	speed: 0.1845s/iter; left time: 2361.6548s
	iters: 200, epoch: 1 | loss: 0.5438815
	speed: 0.1627s/iter; left time: 2066.8368s
Epoch: 1 cost time: 44.79358506202698
Epoch: 1, Steps: 258 | Train Loss: 0.5751630 Vali Loss: 0.2786936 Test Loss: 0.3735374
Validation loss decreased (inf --> 0.278694).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5808871
	speed: 0.7482s/iter; left time: 9384.3515s
	iters: 200, epoch: 2 | loss: 0.4827980
	speed: 0.1696s/iter; left time: 2110.0771s
Epoch: 2 cost time: 44.76646947860718
Epoch: 2, Steps: 258 | Train Loss: 0.5173751 Vali Loss: 0.2690624 Test Loss: 0.3640226
Validation loss decreased (0.278694 --> 0.269062).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4503457
	speed: 0.7073s/iter; left time: 8689.4256s
	iters: 200, epoch: 3 | loss: 0.6386450
	speed: 0.1666s/iter; left time: 2030.3291s
Epoch: 3 cost time: 43.550809383392334
Epoch: 3, Steps: 258 | Train Loss: 0.5083155 Vali Loss: 0.2661079 Test Loss: 0.3599859
Validation loss decreased (0.269062 --> 0.266108).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3671085
	speed: 0.7357s/iter; left time: 8847.8562s
	iters: 200, epoch: 4 | loss: 0.5175946
	speed: 0.1622s/iter; left time: 1934.8089s
Epoch: 4 cost time: 43.45723342895508
Epoch: 4, Steps: 258 | Train Loss: 0.5037142 Vali Loss: 0.2644572 Test Loss: 0.3581561
Validation loss decreased (0.266108 --> 0.264457).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4653016
	speed: 0.6891s/iter; left time: 8110.3788s
	iters: 200, epoch: 5 | loss: 0.5842043
	speed: 0.1453s/iter; left time: 1694.9894s
Epoch: 5 cost time: 39.4261577129364
Epoch: 5, Steps: 258 | Train Loss: 0.5016872 Vali Loss: 0.2636735 Test Loss: 0.3564713
Validation loss decreased (0.264457 --> 0.263674).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5358415
	speed: 0.7325s/iter; left time: 8431.9769s
	iters: 200, epoch: 6 | loss: 0.4483637
	speed: 0.1898s/iter; left time: 2165.4456s
Epoch: 6 cost time: 48.11920523643494
Epoch: 6, Steps: 258 | Train Loss: 0.5003397 Vali Loss: 0.2630665 Test Loss: 0.3554808
Validation loss decreased (0.263674 --> 0.263067).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6606080
	speed: 0.7154s/iter; left time: 8050.8837s
	iters: 200, epoch: 7 | loss: 0.5333021
	speed: 0.1443s/iter; left time: 1608.9568s
Epoch: 7 cost time: 36.93208074569702
Epoch: 7, Steps: 258 | Train Loss: 0.4989310 Vali Loss: 0.2623230 Test Loss: 0.3550739
Validation loss decreased (0.263067 --> 0.262323).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5174621
	speed: 0.5624s/iter; left time: 6183.3522s
	iters: 200, epoch: 8 | loss: 0.4871202
	speed: 0.0961s/iter; left time: 1046.6239s
Epoch: 8 cost time: 24.656019687652588
Epoch: 8, Steps: 258 | Train Loss: 0.4982787 Vali Loss: 0.2620040 Test Loss: 0.3545384
Validation loss decreased (0.262323 --> 0.262004).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3968214
	speed: 0.3715s/iter; left time: 3988.9164s
	iters: 200, epoch: 9 | loss: 0.4264001
	speed: 0.0823s/iter; left time: 875.2517s
Epoch: 9 cost time: 21.950058460235596
Epoch: 9, Steps: 258 | Train Loss: 0.4973656 Vali Loss: 0.2616517 Test Loss: 0.3542859
Validation loss decreased (0.262004 --> 0.261652).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5551780
	speed: 0.3781s/iter; left time: 3962.5628s
	iters: 200, epoch: 10 | loss: 0.4448959
	speed: 0.0862s/iter; left time: 894.8349s
Epoch: 10 cost time: 22.105103254318237
Epoch: 10, Steps: 258 | Train Loss: 0.4970279 Vali Loss: 0.2612039 Test Loss: 0.3540833
Validation loss decreased (0.261652 --> 0.261204).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4871192
	speed: 0.3395s/iter; left time: 3470.1294s
	iters: 200, epoch: 11 | loss: 0.3677740
	speed: 0.0657s/iter; left time: 665.0890s
Epoch: 11 cost time: 19.706857442855835
Epoch: 11, Steps: 258 | Train Loss: 0.4963107 Vali Loss: 0.2614178 Test Loss: 0.3536199
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5803165
	speed: 0.4507s/iter; left time: 4489.9268s
	iters: 200, epoch: 12 | loss: 0.7413377
	speed: 0.1530s/iter; left time: 1508.9739s
Epoch: 12 cost time: 40.38615942001343
Epoch: 12, Steps: 258 | Train Loss: 0.4961749 Vali Loss: 0.2613284 Test Loss: 0.3535037
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4242962
	speed: 0.7467s/iter; left time: 7246.8059s
	iters: 200, epoch: 13 | loss: 0.6042295
	speed: 0.1789s/iter; left time: 1718.6795s
Epoch: 13 cost time: 47.168142557144165
Epoch: 13, Steps: 258 | Train Loss: 0.4961077 Vali Loss: 0.2611617 Test Loss: 0.3533415
Validation loss decreased (0.261204 --> 0.261162).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3726468
	speed: 0.7454s/iter; left time: 7041.4238s
	iters: 200, epoch: 14 | loss: 0.4603989
	speed: 0.1496s/iter; left time: 1398.7176s
Epoch: 14 cost time: 41.13960361480713
Epoch: 14, Steps: 258 | Train Loss: 0.4952662 Vali Loss: 0.2607957 Test Loss: 0.3533469
Validation loss decreased (0.261162 --> 0.260796).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4448492
	speed: 0.6625s/iter; left time: 6087.4751s
	iters: 200, epoch: 15 | loss: 0.5199741
	speed: 0.1421s/iter; left time: 1291.4965s
Epoch: 15 cost time: 37.59274744987488
Epoch: 15, Steps: 258 | Train Loss: 0.4955604 Vali Loss: 0.2609763 Test Loss: 0.3529965
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4589243
	speed: 0.6309s/iter; left time: 5634.7163s
	iters: 200, epoch: 16 | loss: 0.4382912
	speed: 0.1359s/iter; left time: 1199.9695s
Epoch: 16 cost time: 36.87899088859558
Epoch: 16, Steps: 258 | Train Loss: 0.4946630 Vali Loss: 0.2604860 Test Loss: 0.3531143
Validation loss decreased (0.260796 --> 0.260486).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3460967
	speed: 0.6578s/iter; left time: 5705.3316s
	iters: 200, epoch: 17 | loss: 0.4637965
	speed: 0.1613s/iter; left time: 1382.9557s
Epoch: 17 cost time: 41.400298833847046
Epoch: 17, Steps: 258 | Train Loss: 0.4948215 Vali Loss: 0.2608901 Test Loss: 0.3527269
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4853757
	speed: 0.7029s/iter; left time: 5915.2293s
	iters: 200, epoch: 18 | loss: 0.4285213
	speed: 0.1681s/iter; left time: 1397.5978s
Epoch: 18 cost time: 43.6707558631897
Epoch: 18, Steps: 258 | Train Loss: 0.4947360 Vali Loss: 0.2606505 Test Loss: 0.3528437
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5062560
	speed: 0.7331s/iter; left time: 5980.1087s
	iters: 200, epoch: 19 | loss: 0.5348747
	speed: 0.1547s/iter; left time: 1246.0249s
Epoch: 19 cost time: 43.03121280670166
Epoch: 19, Steps: 258 | Train Loss: 0.4947706 Vali Loss: 0.2607088 Test Loss: 0.3528318
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3496686816215515, mae:0.37823325395584106, rse:0.475305438041687, corr:[0.53320783 0.5415458  0.53750616 0.5341696  0.5344654  0.536843
 0.53841823 0.5378873  0.5364856  0.5358015  0.5363099  0.5373783
 0.5379193  0.53737444 0.53626925 0.5353788  0.53514427 0.5354199
 0.5356253  0.5352528  0.53436893 0.5334092  0.5328102  0.5326785
 0.5327188  0.53253543 0.53193766 0.5310901  0.53030306 0.5297941
 0.52953815 0.529369   0.5290333  0.52841944 0.5276707  0.5269941
 0.52645993 0.5260256  0.5255323  0.5248992  0.52416605 0.52342457
 0.52280396 0.52232933 0.5219236  0.52143013 0.5207717  0.519988
 0.51915497 0.5183702  0.5177479  0.51729465 0.51685905 0.5163303
 0.51570195 0.51505303 0.5144968  0.5140977  0.5138148  0.51351136
 0.51314443 0.51270926 0.51226765 0.51191896 0.51173913 0.51172596
 0.51168096 0.51152134 0.5111863  0.51069295 0.51012594 0.50958383
 0.5091246  0.5087263  0.5083358  0.50792944 0.5075064  0.5070352
 0.50653756 0.5059513  0.5052453  0.50448036 0.50374883 0.5031024
 0.5025552  0.50205684 0.50153595 0.50101894 0.5005014  0.5000296
 0.4996214  0.4991788  0.4985912  0.49775907 0.49661174 0.4952029
 0.4936945  0.49227732 0.4909956  0.4898238  0.48868704 0.48751777
 0.486291   0.48503852 0.48380667 0.48264158 0.4815522  0.480539
 0.47956356 0.47864264 0.477799   0.47700492 0.47622204 0.47533503
 0.4743027  0.47321543 0.47223002 0.47144222 0.47085956 0.47033867
 0.4697741  0.4690642  0.4682291  0.46728778 0.46630642 0.46534055
 0.4644137  0.46347404 0.46249664 0.4615153  0.4606311  0.45989385
 0.4592835  0.4586764  0.4579671  0.4571152  0.4561979  0.45540527
 0.45483738 0.45443386 0.4540051  0.45339537 0.45258293 0.45163786
 0.4507423  0.4499982  0.44946268 0.44896615 0.44835785 0.44758308
 0.44675982 0.44603238 0.44541556 0.44487122 0.44430327 0.44366032
 0.44297552 0.44234934 0.4418683  0.4415737  0.44130984 0.44091177
 0.4402852  0.43951452 0.4387675  0.43817738 0.4377604  0.437435
 0.43708006 0.43664393 0.4361259  0.4357058  0.43546417 0.4353659
 0.43523476 0.4349173  0.43436652 0.43366665 0.43297756 0.43242347
 0.4319381  0.43140328 0.43074676 0.4299634  0.4291679  0.4284754
 0.427951   0.42741197 0.42664227 0.42547023 0.42395544 0.4223193
 0.42086112 0.41981587 0.41896197 0.41803172 0.41686308 0.41543022
 0.41392413 0.41258207 0.41152465 0.4106852  0.40988216 0.40893567
 0.40777862 0.4064713  0.4051432  0.40393856 0.40296805 0.40221575
 0.40146276 0.4005902  0.3995946  0.39854717 0.39751655 0.39646032
 0.39536658 0.39421612 0.39312708 0.39212376 0.3911703  0.3902013
 0.38916343 0.3880791  0.38706753 0.38614458 0.3853748  0.38479674
 0.3842582  0.3836456  0.3828504  0.38187265 0.38079214 0.37975833
 0.3788587  0.3781848  0.37773076 0.37744823 0.37729523 0.37715733
 0.37692314 0.3764596  0.3757764  0.37507445 0.37445056 0.37408087
 0.373991   0.37407127 0.3740825  0.37384844 0.37341928 0.37292022
 0.37253314 0.37240264 0.37244374 0.3725127  0.3724359  0.37219504
 0.37183097 0.37144402 0.37109163 0.37079683 0.37045923 0.37005186
 0.3695471  0.36895472 0.36840105 0.36796358 0.36764312 0.36741096
 0.36732823 0.3674048  0.36756563 0.36773232 0.36770916 0.36731333
 0.36653414 0.36543372 0.36427787 0.36347637 0.3631915  0.36338934
 0.36375815 0.3639851  0.36388263 0.3634247  0.3627317  0.361864
 0.36093932 0.3600189  0.35895544 0.35764033 0.35617763 0.35479906
 0.3538346  0.35333735 0.3531484  0.35303447 0.35284194 0.35246304
 0.3520023  0.35146388 0.3509406  0.35032585 0.34964234 0.3488944
 0.34818503 0.3475255  0.346957   0.34647778 0.34605044 0.3457383
 0.34547794 0.3453335  0.34525028 0.345135   0.34492105 0.3445878
 0.34415114 0.34379503 0.34371647 0.34398678 0.34450707 0.3450824
 0.34545967 0.3455307  0.3453075  0.34487504 0.34446406 0.3441819
 0.34411365 0.34415767 0.344232   0.34429088 0.34423465 0.34409282
 0.3438262  0.34339103 0.34293738 0.34267253 0.3427045  0.34296942
 0.34330028 0.34345025 0.34326664 0.3427786  0.3422433  0.3419985
 0.34216028 0.34258625 0.3428991  0.34275985 0.34209564 0.34111208
 0.3402361  0.33984718 0.33994335 0.3402955  0.340429   0.34015223
 0.33951718 0.33879527 0.3382601  0.33801067 0.33790493 0.3377849
 0.33748326 0.33712146 0.33687508 0.33690464 0.33718723 0.3374627
 0.33750087 0.3372401  0.3368103  0.33648777 0.3364943  0.33687413
 0.337413   0.33774778 0.33759788 0.33691496 0.33601955 0.3351836
 0.33467305 0.33445042 0.334285   0.33389512 0.33315814 0.33226067
 0.33135474 0.33054295 0.32977107 0.32890967 0.32789654 0.3267888
 0.32581764 0.32525244 0.32508716 0.32517672 0.32533243 0.32526112
 0.32486287 0.32417682 0.3233884  0.32268366 0.32204694 0.3215031
 0.32101673 0.32065216 0.32043648 0.3203387  0.32033592 0.32031092
 0.3202697  0.32025275 0.32026958 0.32036725 0.3204997  0.32059315
 0.3205685  0.32036567 0.32001984 0.31971207 0.3196429  0.31989074
 0.3203794  0.32086903 0.32117423 0.32119521 0.32100725 0.3207639
 0.32059443 0.32048994 0.32045567 0.3204627  0.32048023 0.3204177
 0.3201678  0.31978816 0.31925222 0.3185841  0.31794542 0.31755865
 0.3174478  0.3176071  0.31786248 0.3179845  0.31788412 0.31754827
 0.31712538 0.31673425 0.31644595 0.31615183 0.3157828  0.31533816
 0.31485766 0.31445503 0.3142171  0.31416476 0.31425014 0.31423032
 0.3139968  0.31353402 0.31297123 0.3124831  0.31218013 0.31204405
 0.31195852 0.3117476  0.3112988  0.31065163 0.31002754 0.3095753
 0.3093577  0.30919984 0.3089192  0.3083001  0.307332   0.30612198
 0.3048813  0.30388206 0.3031025  0.30241337 0.30163345 0.30064362
 0.29944628 0.2982478  0.2972312  0.296563   0.29613954 0.29573894
 0.29516238 0.29439995 0.29349414 0.2925275  0.29166064 0.29096174
 0.29042992 0.28989634 0.28924504 0.28849375 0.2876362  0.2868308
 0.28608847 0.2854524  0.28496245 0.284566   0.2842438  0.28386733
 0.28339005 0.28282395 0.2822153  0.28165406 0.28121272 0.2808794
 0.2805764  0.280235   0.27999613 0.27984697 0.2797726  0.27975255
 0.27972    0.2796168  0.27937686 0.2790396  0.278726   0.27850223
 0.27833802 0.27818772 0.2779624  0.27764267 0.27730325 0.27702948
 0.27690175 0.27698386 0.27732804 0.27777314 0.2781365  0.27828455
 0.27818787 0.27793363 0.2775925  0.27727652 0.27697435 0.27663377
 0.2762448  0.27588597 0.2756075  0.27543056 0.27528378 0.27515963
 0.27497146 0.27473998 0.27451596 0.27436584 0.2741992  0.274055
 0.2738657  0.27364108 0.27332    0.27293047 0.27259633 0.2724192
 0.2724245  0.27249253 0.2725245  0.27245    0.27232113 0.27223092
 0.27220213 0.27212816 0.27177438 0.27100202 0.2698042  0.26825368
 0.26662397 0.2653353  0.2644511  0.26379868 0.263103   0.26223162
 0.26121113 0.26016283 0.2592135  0.25838003 0.2575918  0.25681776
 0.25601622 0.25519285 0.25448027 0.25389194 0.25340196 0.2530078
 0.25266978 0.2523412  0.25208417 0.2518193  0.25155494 0.25125808
 0.2508677  0.25043467 0.2499344  0.24938284 0.24889114 0.2484736
 0.248045   0.24764474 0.24741381 0.2473558  0.2475355  0.2478687
 0.24823976 0.24832828 0.24811214 0.2477614  0.24744907 0.24733149
 0.2475118  0.24804594 0.24877332 0.24954103 0.2502052  0.25069705
 0.25094903 0.25097048 0.25086257 0.2508136  0.25087067 0.25099027
 0.25107062 0.25113276 0.2512657  0.25133654 0.25153902 0.25177735
 0.25195965 0.2520274  0.2518922  0.25172228 0.25145122 0.25117368
 0.25088373 0.25069338 0.25051457 0.2502414  0.25004587 0.24978921
 0.24959591 0.24945316 0.24938501 0.2493623  0.24942446 0.2493221
 0.24907434 0.24858648 0.24787576 0.24729764 0.2471055  0.24734017
 0.24786724 0.24857722 0.24909177 0.2492711  0.24926884 0.24919848
 0.24914458 0.24918722 0.24917985 0.2488438  0.24806528 0.246973
 0.2457754  0.24464843 0.2436162  0.24259181 0.24164002 0.24077499
 0.240029   0.23939988 0.2391049  0.23887666 0.23858272 0.23810141
 0.23751412 0.23697923 0.23657726 0.23624301 0.23598585 0.2355974
 0.23512088 0.23459594 0.23401323 0.23361796 0.23327337 0.23284622
 0.2323601  0.23182559 0.23136637 0.23077676 0.2299863  0.22896871
 0.22804669 0.22729251 0.22694959 0.2269543  0.22692111 0.2264908
 0.22548035 0.22439727 0.22383894 0.22411135 0.2243406  0.22377527
 0.22202799 0.21992938 0.2193376  0.22117567 0.2234831  0.22150552]
