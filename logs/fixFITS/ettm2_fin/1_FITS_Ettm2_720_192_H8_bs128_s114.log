Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=74, out_features=93, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12332544.0
params:  6975.0
Trainable parameters:  6975
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5154589
	speed: 0.1523s/iter; left time: 982.3818s
Epoch: 1 cost time: 19.76905107498169
Epoch: 1, Steps: 131 | Train Loss: 0.4194721 Vali Loss: 0.1836117 Test Loss: 0.2477531
Validation loss decreased (inf --> 0.183612).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2968359
	speed: 0.4079s/iter; left time: 2577.9743s
Epoch: 2 cost time: 19.536813974380493
Epoch: 2, Steps: 131 | Train Loss: 0.3404907 Vali Loss: 0.1701532 Test Loss: 0.2341708
Validation loss decreased (0.183612 --> 0.170153).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2480713
	speed: 0.4256s/iter; left time: 2634.2150s
Epoch: 3 cost time: 20.591710567474365
Epoch: 3, Steps: 131 | Train Loss: 0.3239323 Vali Loss: 0.1646190 Test Loss: 0.2288192
Validation loss decreased (0.170153 --> 0.164619).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5081204
	speed: 0.4155s/iter; left time: 2517.3646s
Epoch: 4 cost time: 20.42225193977356
Epoch: 4, Steps: 131 | Train Loss: 0.3145218 Vali Loss: 0.1616492 Test Loss: 0.2257911
Validation loss decreased (0.164619 --> 0.161649).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3256479
	speed: 0.4297s/iter; left time: 2546.7024s
Epoch: 5 cost time: 20.480002880096436
Epoch: 5, Steps: 131 | Train Loss: 0.3091521 Vali Loss: 0.1596380 Test Loss: 0.2237872
Validation loss decreased (0.161649 --> 0.159638).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3387027
	speed: 0.4033s/iter; left time: 2337.7556s
Epoch: 6 cost time: 18.496846914291382
Epoch: 6, Steps: 131 | Train Loss: 0.3055659 Vali Loss: 0.1581694 Test Loss: 0.2222517
Validation loss decreased (0.159638 --> 0.158169).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2218309
	speed: 0.3937s/iter; left time: 2230.5633s
Epoch: 7 cost time: 18.31145715713501
Epoch: 7, Steps: 131 | Train Loss: 0.3035051 Vali Loss: 0.1572746 Test Loss: 0.2213533
Validation loss decreased (0.158169 --> 0.157275).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2632241
	speed: 0.3824s/iter; left time: 2115.9949s
Epoch: 8 cost time: 18.678610801696777
Epoch: 8, Steps: 131 | Train Loss: 0.3010577 Vali Loss: 0.1567637 Test Loss: 0.2208182
Validation loss decreased (0.157275 --> 0.156764).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2810989
	speed: 0.3937s/iter; left time: 2127.2834s
Epoch: 9 cost time: 18.73875880241394
Epoch: 9, Steps: 131 | Train Loss: 0.2999877 Vali Loss: 0.1562208 Test Loss: 0.2203019
Validation loss decreased (0.156764 --> 0.156221).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3455376
	speed: 0.3858s/iter; left time: 2034.1007s
Epoch: 10 cost time: 18.013187170028687
Epoch: 10, Steps: 131 | Train Loss: 0.2988852 Vali Loss: 0.1556533 Test Loss: 0.2198059
Validation loss decreased (0.156221 --> 0.155653).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3540324
	speed: 0.3872s/iter; left time: 1990.5584s
Epoch: 11 cost time: 18.324514150619507
Epoch: 11, Steps: 131 | Train Loss: 0.2977748 Vali Loss: 0.1554933 Test Loss: 0.2193914
Validation loss decreased (0.155653 --> 0.155493).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3441841
	speed: 0.3605s/iter; left time: 1805.8760s
Epoch: 12 cost time: 17.761898279190063
Epoch: 12, Steps: 131 | Train Loss: 0.2974645 Vali Loss: 0.1551887 Test Loss: 0.2190723
Validation loss decreased (0.155493 --> 0.155189).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2826034
	speed: 0.3753s/iter; left time: 1830.8974s
Epoch: 13 cost time: 18.150739431381226
Epoch: 13, Steps: 131 | Train Loss: 0.2965881 Vali Loss: 0.1549129 Test Loss: 0.2186872
Validation loss decreased (0.155189 --> 0.154913).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2908811
	speed: 0.3810s/iter; left time: 1809.0748s
Epoch: 14 cost time: 18.38156270980835
Epoch: 14, Steps: 131 | Train Loss: 0.2963126 Vali Loss: 0.1547479 Test Loss: 0.2185825
Validation loss decreased (0.154913 --> 0.154748).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2928596
	speed: 0.3926s/iter; left time: 1812.4692s
Epoch: 15 cost time: 18.776893377304077
Epoch: 15, Steps: 131 | Train Loss: 0.2958434 Vali Loss: 0.1547943 Test Loss: 0.2185266
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3068209
	speed: 0.3892s/iter; left time: 1746.1027s
Epoch: 16 cost time: 18.52631163597107
Epoch: 16, Steps: 131 | Train Loss: 0.2954391 Vali Loss: 0.1545405 Test Loss: 0.2183797
Validation loss decreased (0.154748 --> 0.154540).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3800738
	speed: 0.3873s/iter; left time: 1686.6568s
Epoch: 17 cost time: 18.786810159683228
Epoch: 17, Steps: 131 | Train Loss: 0.2944924 Vali Loss: 0.1544190 Test Loss: 0.2182198
Validation loss decreased (0.154540 --> 0.154419).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3286138
	speed: 0.3827s/iter; left time: 1616.4801s
Epoch: 18 cost time: 18.27768588066101
Epoch: 18, Steps: 131 | Train Loss: 0.2942063 Vali Loss: 0.1543052 Test Loss: 0.2179022
Validation loss decreased (0.154419 --> 0.154305).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3188708
	speed: 0.3551s/iter; left time: 1453.3589s
Epoch: 19 cost time: 16.89473605155945
Epoch: 19, Steps: 131 | Train Loss: 0.2937761 Vali Loss: 0.1541003 Test Loss: 0.2177705
Validation loss decreased (0.154305 --> 0.154100).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2961681
	speed: 0.4270s/iter; left time: 1691.7971s
Epoch: 20 cost time: 20.8988938331604
Epoch: 20, Steps: 131 | Train Loss: 0.2942834 Vali Loss: 0.1540794 Test Loss: 0.2177842
Validation loss decreased (0.154100 --> 0.154079).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2922106
	speed: 0.4421s/iter; left time: 1693.6838s
Epoch: 21 cost time: 21.438982725143433
Epoch: 21, Steps: 131 | Train Loss: 0.2937369 Vali Loss: 0.1539472 Test Loss: 0.2177896
Validation loss decreased (0.154079 --> 0.153947).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2790737
	speed: 0.4393s/iter; left time: 1625.2480s
Epoch: 22 cost time: 20.470940113067627
Epoch: 22, Steps: 131 | Train Loss: 0.2934976 Vali Loss: 0.1539584 Test Loss: 0.2175783
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2323879
	speed: 0.4170s/iter; left time: 1488.3602s
Epoch: 23 cost time: 20.020723581314087
Epoch: 23, Steps: 131 | Train Loss: 0.2933262 Vali Loss: 0.1540343 Test Loss: 0.2175492
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2416944
	speed: 0.4056s/iter; left time: 1394.6095s
Epoch: 24 cost time: 19.176520109176636
Epoch: 24, Steps: 131 | Train Loss: 0.2933427 Vali Loss: 0.1540693 Test Loss: 0.2175092
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.21946600079536438, mae:0.29368722438812256, rse:0.37920910120010376, corr:[0.5515019  0.5591933  0.56312954 0.56275654 0.56068486 0.5590331
 0.55849284 0.5589645  0.5600113  0.56105095 0.56152934 0.561236
 0.5604059  0.5594384  0.5587058  0.5583121  0.55822533 0.5582579
 0.55818874 0.5578128  0.5571072  0.5561721  0.5552197  0.55444545
 0.5539228  0.5536385  0.5535025  0.5533823  0.55315316 0.5527325
 0.552103   0.5513793  0.5506326  0.5498975  0.54922205 0.54865897
 0.5481784  0.54773396 0.5472605  0.5467122  0.5460767  0.5453657
 0.5446239  0.5438886  0.543201   0.542575   0.5420171  0.54151386
 0.54095936 0.5402792  0.5394643  0.53858614 0.53768885 0.53684956
 0.53614014 0.5356535  0.5353673  0.5351979  0.53505296 0.53484935
 0.53453606 0.5341546  0.53373647 0.53333634 0.53298163 0.5327471
 0.53258944 0.5324959  0.53241646 0.5322635  0.5319707  0.5315439
 0.5310155  0.53045017 0.5298918  0.5293915  0.52894807 0.5285006
 0.52799267 0.5274002  0.52666056 0.5258079  0.52491677 0.5240895
 0.5234156  0.5229824  0.5226907  0.52248245 0.52224827 0.5218913
 0.52137554 0.5207006  0.5198053  0.5187535  0.5175992  0.5163655
 0.5151033  0.51389927 0.5127246  0.5115262  0.5102886  0.5090137
 0.50777584 0.5066216  0.505557   0.5045841  0.50370324 0.50292665
 0.5021249  0.5012275  0.5002271  0.49915436 0.4981895  0.49737224
 0.496706   0.49617636 0.4957239  0.4952235  0.49464    0.49386507
 0.4929754  0.49200132 0.49111572 0.4903667  0.48971522 0.48908907
 0.48843446 0.4876906  0.48680836 0.48578298 0.48470294 0.48366818
 0.4828411  0.48230797 0.48206243 0.48195824 0.48175314 0.4813665
 0.48068428 0.47975314 0.47865996 0.47759423 0.47674167 0.47609007
 0.47556305 0.47492912 0.474211   0.47334325 0.47240344 0.47141135
 0.4705466  0.46997866 0.46966732 0.46941832 0.46904868 0.46851078
 0.46774414 0.46678305 0.46566    0.46471426 0.4640631  0.46387815
 0.46406493 0.4644532  0.46477115 0.4647887  0.4642798  0.46321782
 0.46182093 0.46053508 0.45966846 0.45946565 0.4598144  0.46032828
 0.46056738 0.46016863 0.45901367 0.45723712 0.45514995 0.45359093
 0.4530319  0.4534859  0.4545519  0.4554047  0.4553085  0.45396078
 0.45158085 0.44925505 0.4486777  0.45109257 0.45512787 0.45660213]
