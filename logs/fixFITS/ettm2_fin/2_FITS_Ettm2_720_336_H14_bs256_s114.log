Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  77830144.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 12.368006706237793
Epoch: 1, Steps: 65 | Train Loss: 0.4403448 Vali Loss: 0.2878555 Test Loss: 0.3871819
Validation loss decreased (inf --> 0.287856).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 12.44767451286316
Epoch: 2, Steps: 65 | Train Loss: 0.3407865 Vali Loss: 0.2585250 Test Loss: 0.3486449
Validation loss decreased (0.287856 --> 0.258525).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 12.03297758102417
Epoch: 3, Steps: 65 | Train Loss: 0.2911903 Vali Loss: 0.2448031 Test Loss: 0.3301825
Validation loss decreased (0.258525 --> 0.244803).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 12.178112506866455
Epoch: 4, Steps: 65 | Train Loss: 0.2618970 Vali Loss: 0.2360123 Test Loss: 0.3199866
Validation loss decreased (0.244803 --> 0.236012).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 12.335520029067993
Epoch: 5, Steps: 65 | Train Loss: 0.2413808 Vali Loss: 0.2306222 Test Loss: 0.3133402
Validation loss decreased (0.236012 --> 0.230622).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 12.220171213150024
Epoch: 6, Steps: 65 | Train Loss: 0.2266471 Vali Loss: 0.2270284 Test Loss: 0.3086299
Validation loss decreased (0.230622 --> 0.227028).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.337173700332642
Epoch: 7, Steps: 65 | Train Loss: 0.2150173 Vali Loss: 0.2236521 Test Loss: 0.3050185
Validation loss decreased (0.227028 --> 0.223652).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 13.042656183242798
Epoch: 8, Steps: 65 | Train Loss: 0.2051185 Vali Loss: 0.2209333 Test Loss: 0.3019080
Validation loss decreased (0.223652 --> 0.220933).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 13.251125812530518
Epoch: 9, Steps: 65 | Train Loss: 0.1971821 Vali Loss: 0.2197035 Test Loss: 0.2993563
Validation loss decreased (0.220933 --> 0.219704).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 13.724996328353882
Epoch: 10, Steps: 65 | Train Loss: 0.1900991 Vali Loss: 0.2172855 Test Loss: 0.2971825
Validation loss decreased (0.219704 --> 0.217286).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 13.269415378570557
Epoch: 11, Steps: 65 | Train Loss: 0.1842763 Vali Loss: 0.2160704 Test Loss: 0.2953108
Validation loss decreased (0.217286 --> 0.216070).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 12.141222715377808
Epoch: 12, Steps: 65 | Train Loss: 0.1789456 Vali Loss: 0.2142293 Test Loss: 0.2936950
Validation loss decreased (0.216070 --> 0.214229).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.9337797164917
Epoch: 13, Steps: 65 | Train Loss: 0.1748719 Vali Loss: 0.2133023 Test Loss: 0.2921652
Validation loss decreased (0.214229 --> 0.213302).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.04589581489563
Epoch: 14, Steps: 65 | Train Loss: 0.1710556 Vali Loss: 0.2120164 Test Loss: 0.2908276
Validation loss decreased (0.213302 --> 0.212016).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 12.295886754989624
Epoch: 15, Steps: 65 | Train Loss: 0.1674980 Vali Loss: 0.2111945 Test Loss: 0.2895975
Validation loss decreased (0.212016 --> 0.211195).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 10.285262107849121
Epoch: 16, Steps: 65 | Train Loss: 0.1645933 Vali Loss: 0.2101950 Test Loss: 0.2885445
Validation loss decreased (0.211195 --> 0.210195).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.317135095596313
Epoch: 17, Steps: 65 | Train Loss: 0.1618547 Vali Loss: 0.2095889 Test Loss: 0.2875792
Validation loss decreased (0.210195 --> 0.209589).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 12.532238960266113
Epoch: 18, Steps: 65 | Train Loss: 0.1593270 Vali Loss: 0.2088987 Test Loss: 0.2867197
Validation loss decreased (0.209589 --> 0.208899).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 12.174789428710938
Epoch: 19, Steps: 65 | Train Loss: 0.1570333 Vali Loss: 0.2074995 Test Loss: 0.2858639
Validation loss decreased (0.208899 --> 0.207500).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 12.038861274719238
Epoch: 20, Steps: 65 | Train Loss: 0.1552949 Vali Loss: 0.2076583 Test Loss: 0.2851251
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 12.43096113204956
Epoch: 21, Steps: 65 | Train Loss: 0.1534592 Vali Loss: 0.2074091 Test Loss: 0.2844553
Validation loss decreased (0.207500 --> 0.207409).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 12.645270109176636
Epoch: 22, Steps: 65 | Train Loss: 0.1521155 Vali Loss: 0.2058304 Test Loss: 0.2838091
Validation loss decreased (0.207409 --> 0.205830).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 13.60767388343811
Epoch: 23, Steps: 65 | Train Loss: 0.1505835 Vali Loss: 0.2063051 Test Loss: 0.2832945
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 12.811388969421387
Epoch: 24, Steps: 65 | Train Loss: 0.1491423 Vali Loss: 0.2053424 Test Loss: 0.2827629
Validation loss decreased (0.205830 --> 0.205342).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 13.20809292793274
Epoch: 25, Steps: 65 | Train Loss: 0.1481482 Vali Loss: 0.2048339 Test Loss: 0.2822686
Validation loss decreased (0.205342 --> 0.204834).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 12.317131519317627
Epoch: 26, Steps: 65 | Train Loss: 0.1472013 Vali Loss: 0.2048423 Test Loss: 0.2818091
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 12.216805458068848
Epoch: 27, Steps: 65 | Train Loss: 0.1461501 Vali Loss: 0.2041484 Test Loss: 0.2814383
Validation loss decreased (0.204834 --> 0.204148).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 12.028705596923828
Epoch: 28, Steps: 65 | Train Loss: 0.1453107 Vali Loss: 0.2042471 Test Loss: 0.2810285
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 12.293740272521973
Epoch: 29, Steps: 65 | Train Loss: 0.1448167 Vali Loss: 0.2038943 Test Loss: 0.2806933
Validation loss decreased (0.204148 --> 0.203894).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 12.818756341934204
Epoch: 30, Steps: 65 | Train Loss: 0.1439957 Vali Loss: 0.2036115 Test Loss: 0.2803936
Validation loss decreased (0.203894 --> 0.203612).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 10.813735008239746
Epoch: 31, Steps: 65 | Train Loss: 0.1431848 Vali Loss: 0.2030617 Test Loss: 0.2800450
Validation loss decreased (0.203612 --> 0.203062).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 12.477709770202637
Epoch: 32, Steps: 65 | Train Loss: 0.1425508 Vali Loss: 0.2028580 Test Loss: 0.2797656
Validation loss decreased (0.203062 --> 0.202858).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.578741550445557
Epoch: 33, Steps: 65 | Train Loss: 0.1421065 Vali Loss: 0.2028876 Test Loss: 0.2795406
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 9.656076192855835
Epoch: 34, Steps: 65 | Train Loss: 0.1413878 Vali Loss: 0.2028915 Test Loss: 0.2792703
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 11.26471495628357
Epoch: 35, Steps: 65 | Train Loss: 0.1409292 Vali Loss: 0.2024367 Test Loss: 0.2790639
Validation loss decreased (0.202858 --> 0.202437).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 10.448453903198242
Epoch: 36, Steps: 65 | Train Loss: 0.1404082 Vali Loss: 0.2019821 Test Loss: 0.2788354
Validation loss decreased (0.202437 --> 0.201982).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 13.424491167068481
Epoch: 37, Steps: 65 | Train Loss: 0.1401373 Vali Loss: 0.2013348 Test Loss: 0.2786406
Validation loss decreased (0.201982 --> 0.201335).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 13.879274129867554
Epoch: 38, Steps: 65 | Train Loss: 0.1398773 Vali Loss: 0.2017406 Test Loss: 0.2784448
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 13.57600998878479
Epoch: 39, Steps: 65 | Train Loss: 0.1394829 Vali Loss: 0.2018870 Test Loss: 0.2782838
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 10.545435190200806
Epoch: 40, Steps: 65 | Train Loss: 0.1388092 Vali Loss: 0.2016291 Test Loss: 0.2781276
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  77830144.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.2623770236969
Epoch: 1, Steps: 65 | Train Loss: 0.3866896 Vali Loss: 0.1961520 Test Loss: 0.2725564
Validation loss decreased (inf --> 0.196152).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.964292287826538
Epoch: 2, Steps: 65 | Train Loss: 0.3809485 Vali Loss: 0.1950207 Test Loss: 0.2710656
Validation loss decreased (0.196152 --> 0.195021).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.10837984085083
Epoch: 3, Steps: 65 | Train Loss: 0.3785908 Vali Loss: 0.1937520 Test Loss: 0.2704554
Validation loss decreased (0.195021 --> 0.193752).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.190769672393799
Epoch: 4, Steps: 65 | Train Loss: 0.3788254 Vali Loss: 0.1939078 Test Loss: 0.2699207
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.781135320663452
Epoch: 5, Steps: 65 | Train Loss: 0.3767893 Vali Loss: 0.1935557 Test Loss: 0.2697050
Validation loss decreased (0.193752 --> 0.193556).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.821460962295532
Epoch: 6, Steps: 65 | Train Loss: 0.3761442 Vali Loss: 0.1933727 Test Loss: 0.2695878
Validation loss decreased (0.193556 --> 0.193373).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.137877702713013
Epoch: 7, Steps: 65 | Train Loss: 0.3765504 Vali Loss: 0.1931827 Test Loss: 0.2692998
Validation loss decreased (0.193373 --> 0.193183).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.288721561431885
Epoch: 8, Steps: 65 | Train Loss: 0.3760313 Vali Loss: 0.1926297 Test Loss: 0.2691229
Validation loss decreased (0.193183 --> 0.192630).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.965996026992798
Epoch: 9, Steps: 65 | Train Loss: 0.3755571 Vali Loss: 0.1928301 Test Loss: 0.2690130
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.35443639755249
Epoch: 10, Steps: 65 | Train Loss: 0.3756787 Vali Loss: 0.1928253 Test Loss: 0.2690464
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.577494382858276
Epoch: 11, Steps: 65 | Train Loss: 0.3756754 Vali Loss: 0.1929763 Test Loss: 0.2690002
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2691617012023926, mae:0.3266174793243408, rse:0.4190513789653778, corr:[0.5537432  0.56051946 0.55917865 0.55647427 0.55614233 0.55743766
 0.55823755 0.55749345 0.5562739  0.5557973  0.55616117 0.55661196
 0.55637085 0.55552    0.55476093 0.5544579  0.55438954 0.55404043
 0.5531601  0.5520662  0.5512421  0.5508292  0.55060285 0.550192
 0.5494582  0.54864836 0.5480392  0.54765725 0.54732126 0.5468256
 0.5461439  0.5454568  0.54486185 0.54435605 0.54384667 0.5432188
 0.5424329  0.5416051  0.54086137 0.5402793  0.5398415  0.5394281
 0.538942   0.5383398  0.5376391  0.53688335 0.53611845 0.535342
 0.534544   0.53378123 0.53314126 0.53265715 0.5322141  0.5317034
 0.5311003  0.5304888  0.5299418  0.5295131  0.5291778  0.52885514
 0.52851236 0.5282037  0.5279632  0.52776897 0.52756596 0.5272964
 0.5269382  0.52658045 0.52629733 0.5260729  0.5258156  0.5254727
 0.5250679  0.52469057 0.52437043 0.5240753  0.52367646 0.5230951
 0.52237827 0.52167535 0.52106524 0.52055925 0.5200375  0.51940393
 0.518686   0.5179825  0.51739675 0.5169744  0.51663107 0.5162628
 0.51580495 0.515207   0.5144607  0.51358384 0.51254976 0.51133686
 0.5100351  0.5087852  0.5076117  0.50649655 0.5053776  0.5042061
 0.5029949  0.50178075 0.5006478  0.49958768 0.49860454 0.49767098
 0.4967797  0.49597993 0.49529275 0.49461994 0.49387512 0.492973
 0.49197054 0.4910407  0.49025175 0.48947567 0.48860803 0.48761594
 0.48668125 0.48591504 0.48536372 0.4848436  0.48418158 0.48338094
 0.48258647 0.48186883 0.48118037 0.4804097  0.47950202 0.47853035
 0.47767752 0.4770368  0.47655392 0.47604853 0.47535503 0.47449207
 0.47356632 0.47271663 0.47192293 0.47109544 0.47013828 0.468998
 0.46782213 0.4667629  0.46602458 0.4655363  0.4651822  0.46484932
 0.46453562 0.464195   0.46365932 0.46291128 0.46211734 0.4615249
 0.46120295 0.46102527 0.4607516  0.46028754 0.45964378 0.4590235
 0.45857212 0.45829067 0.45796946 0.45742595 0.45665407 0.4559132
 0.45549446 0.45545796 0.4554835  0.4552904  0.4548032  0.4541689
 0.4535671  0.45304462 0.45252645 0.45199278 0.45152318 0.45129234
 0.4511507  0.45083758 0.4502296  0.4493704  0.44856104 0.44814643
 0.4482082  0.44841006 0.44823468 0.4473753  0.44594702 0.4443566
 0.44299856 0.44196525 0.44091618 0.43965638 0.43833864 0.43711367
 0.43600944 0.43483004 0.43343556 0.43188778 0.43049803 0.42952242
 0.42890543 0.42829874 0.42739645 0.42628396 0.42533672 0.42474782
 0.42415258 0.42318425 0.4218061  0.4204122  0.41955736 0.41931334
 0.41923374 0.418637   0.41728637 0.415371   0.41341585 0.41193947
 0.41096747 0.41019616 0.40944332 0.40871397 0.40832722 0.40836817
 0.4083303  0.40772495 0.40630886 0.40449008 0.40296018 0.4022185
 0.40208375 0.40205607 0.4017054  0.40113762 0.40077734 0.4007892
 0.40091455 0.40059334 0.3996908  0.39865562 0.39791903 0.3977899
 0.39803478 0.39816883 0.39787197 0.39736697 0.39729875 0.39781493
 0.39850637 0.39884898 0.39839634 0.39742973 0.39655703 0.39631006
 0.3965781  0.39686733 0.39683196 0.39651507 0.39614996 0.39596042
 0.3958399  0.39553565 0.39511603 0.39476815 0.39461154 0.394555
 0.39439186 0.39402565 0.3934577  0.39303252 0.39292416 0.39305523
 0.39325958 0.39316824 0.39274365 0.3922619  0.39182007 0.39142585
 0.3909727  0.39057076 0.39047003 0.3907249  0.39098766 0.39055103
 0.38914123 0.38722044 0.38542876 0.38435757 0.3840924  0.38411155
 0.38404152 0.3836776  0.3831377  0.3826793  0.38240623 0.38190985
 0.38120666 0.3803764  0.37978423 0.3793249  0.37882152 0.3780484
 0.37714225 0.37635475 0.3761301  0.37640887 0.37661254 0.37646228
 0.3757968  0.37526298 0.37526146 0.37556484 0.37560862 0.37505993
 0.3740777  0.3735164  0.37389222 0.37479758 0.3753213  0.37522438
 0.37489843 0.37498596 0.37562343 0.37607226 0.37594402 0.37536478
 0.3754476  0.3761493  0.3764844  0.37550634 0.373319   0.3723443 ]
