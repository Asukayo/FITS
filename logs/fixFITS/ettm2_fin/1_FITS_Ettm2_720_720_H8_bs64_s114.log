Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=74, out_features=148, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9812992.0
params:  11100.0
Trainable parameters:  11100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5723025
	speed: 0.1683s/iter; left time: 2154.3188s
	iters: 200, epoch: 1 | loss: 0.3835304
	speed: 0.1518s/iter; left time: 1927.5142s
Epoch: 1 cost time: 41.172977447509766
Epoch: 1, Steps: 258 | Train Loss: 0.5786951 Vali Loss: 0.2797790 Test Loss: 0.3749194
Validation loss decreased (inf --> 0.279779).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5471435
	speed: 0.6466s/iter; left time: 8110.6424s
	iters: 200, epoch: 2 | loss: 0.5507796
	speed: 0.1367s/iter; left time: 1701.3881s
Epoch: 2 cost time: 35.67186117172241
Epoch: 2, Steps: 258 | Train Loss: 0.5190485 Vali Loss: 0.2701917 Test Loss: 0.3653002
Validation loss decreased (0.279779 --> 0.270192).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5354676
	speed: 0.5886s/iter; left time: 7230.7790s
	iters: 200, epoch: 3 | loss: 0.5294587
	speed: 0.1325s/iter; left time: 1614.9614s
Epoch: 3 cost time: 34.854631423950195
Epoch: 3, Steps: 258 | Train Loss: 0.5108751 Vali Loss: 0.2666575 Test Loss: 0.3608850
Validation loss decreased (0.270192 --> 0.266658).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5927936
	speed: 0.5704s/iter; left time: 6860.3172s
	iters: 200, epoch: 4 | loss: 0.4027170
	speed: 0.1296s/iter; left time: 1545.4794s
Epoch: 4 cost time: 34.096667528152466
Epoch: 4, Steps: 258 | Train Loss: 0.5058869 Vali Loss: 0.2656313 Test Loss: 0.3589295
Validation loss decreased (0.266658 --> 0.265631).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6847497
	speed: 0.5634s/iter; left time: 6630.2185s
	iters: 200, epoch: 5 | loss: 0.5654822
	speed: 0.1270s/iter; left time: 1482.4014s
Epoch: 5 cost time: 34.68672323226929
Epoch: 5, Steps: 258 | Train Loss: 0.5030504 Vali Loss: 0.2649981 Test Loss: 0.3572748
Validation loss decreased (0.265631 --> 0.264998).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5048472
	speed: 0.5916s/iter; left time: 6809.4273s
	iters: 200, epoch: 6 | loss: 0.5734402
	speed: 0.1370s/iter; left time: 1563.6918s
Epoch: 6 cost time: 36.111430406570435
Epoch: 6, Steps: 258 | Train Loss: 0.5019877 Vali Loss: 0.2640150 Test Loss: 0.3564015
Validation loss decreased (0.264998 --> 0.264015).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3834324
	speed: 0.6452s/iter; left time: 7260.3829s
	iters: 200, epoch: 7 | loss: 0.5243036
	speed: 0.1508s/iter; left time: 1681.3370s
Epoch: 7 cost time: 39.89907455444336
Epoch: 7, Steps: 258 | Train Loss: 0.5005502 Vali Loss: 0.2638271 Test Loss: 0.3559043
Validation loss decreased (0.264015 --> 0.263827).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4508181
	speed: 0.7628s/iter; left time: 8387.1057s
	iters: 200, epoch: 8 | loss: 0.4622983
	speed: 0.1761s/iter; left time: 1918.2762s
Epoch: 8 cost time: 46.94202899932861
Epoch: 8, Steps: 258 | Train Loss: 0.4987968 Vali Loss: 0.2633460 Test Loss: 0.3551974
Validation loss decreased (0.263827 --> 0.263346).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4527164
	speed: 0.6717s/iter; left time: 7212.4529s
	iters: 200, epoch: 9 | loss: 0.6116245
	speed: 0.1292s/iter; left time: 1374.3310s
Epoch: 9 cost time: 35.361817836761475
Epoch: 9, Steps: 258 | Train Loss: 0.4992045 Vali Loss: 0.2631739 Test Loss: 0.3549723
Validation loss decreased (0.263346 --> 0.263174).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4507921
	speed: 0.5865s/iter; left time: 6145.4734s
	iters: 200, epoch: 10 | loss: 0.4948841
	speed: 0.1309s/iter; left time: 1358.8935s
Epoch: 10 cost time: 34.59572649002075
Epoch: 10, Steps: 258 | Train Loss: 0.4989162 Vali Loss: 0.2625958 Test Loss: 0.3546655
Validation loss decreased (0.263174 --> 0.262596).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3536406
	speed: 0.5798s/iter; left time: 5926.2866s
	iters: 200, epoch: 11 | loss: 0.5545729
	speed: 0.1397s/iter; left time: 1413.8393s
Epoch: 11 cost time: 35.99605321884155
Epoch: 11, Steps: 258 | Train Loss: 0.4980206 Vali Loss: 0.2632019 Test Loss: 0.3542251
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4611574
	speed: 0.5783s/iter; left time: 5761.5354s
	iters: 200, epoch: 12 | loss: 0.4159724
	speed: 0.1109s/iter; left time: 1093.4195s
Epoch: 12 cost time: 30.173590898513794
Epoch: 12, Steps: 258 | Train Loss: 0.4977095 Vali Loss: 0.2625731 Test Loss: 0.3540376
Validation loss decreased (0.262596 --> 0.262573).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3668264
	speed: 0.5726s/iter; left time: 5557.2060s
	iters: 200, epoch: 13 | loss: 0.4289390
	speed: 0.1322s/iter; left time: 1269.3374s
Epoch: 13 cost time: 34.96055245399475
Epoch: 13, Steps: 258 | Train Loss: 0.4976235 Vali Loss: 0.2626001 Test Loss: 0.3541201
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4526050
	speed: 0.5878s/iter; left time: 5553.2930s
	iters: 200, epoch: 14 | loss: 0.6011019
	speed: 0.1298s/iter; left time: 1213.6719s
Epoch: 14 cost time: 34.89707970619202
Epoch: 14, Steps: 258 | Train Loss: 0.4974575 Vali Loss: 0.2621864 Test Loss: 0.3540339
Validation loss decreased (0.262573 --> 0.262186).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4773563
	speed: 0.5966s/iter; left time: 5482.1635s
	iters: 200, epoch: 15 | loss: 0.5615231
	speed: 0.1423s/iter; left time: 1293.6751s
Epoch: 15 cost time: 37.35783386230469
Epoch: 15, Steps: 258 | Train Loss: 0.4968314 Vali Loss: 0.2622613 Test Loss: 0.3537068
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4509334
	speed: 0.6277s/iter; left time: 5605.7218s
	iters: 200, epoch: 16 | loss: 0.4341423
	speed: 0.1359s/iter; left time: 1200.4632s
Epoch: 16 cost time: 36.76714611053467
Epoch: 16, Steps: 258 | Train Loss: 0.4971250 Vali Loss: 0.2621754 Test Loss: 0.3536456
Validation loss decreased (0.262186 --> 0.262175).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4422752
	speed: 0.6337s/iter; left time: 5496.0596s
	iters: 200, epoch: 17 | loss: 0.3434706
	speed: 0.1420s/iter; left time: 1217.3939s
Epoch: 17 cost time: 38.113200187683105
Epoch: 17, Steps: 258 | Train Loss: 0.4965818 Vali Loss: 0.2621946 Test Loss: 0.3536798
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5141928
	speed: 0.6334s/iter; left time: 5329.8874s
	iters: 200, epoch: 18 | loss: 0.4268408
	speed: 0.1401s/iter; left time: 1164.5988s
Epoch: 18 cost time: 36.217304706573486
Epoch: 18, Steps: 258 | Train Loss: 0.4965829 Vali Loss: 0.2621485 Test Loss: 0.3535705
Validation loss decreased (0.262175 --> 0.262148).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5175174
	speed: 0.5782s/iter; left time: 4716.2489s
	iters: 200, epoch: 19 | loss: 0.4608019
	speed: 0.1353s/iter; left time: 1089.8549s
Epoch: 19 cost time: 35.59665775299072
Epoch: 19, Steps: 258 | Train Loss: 0.4963526 Vali Loss: 0.2620614 Test Loss: 0.3535795
Validation loss decreased (0.262148 --> 0.262061).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5050087
	speed: 0.6128s/iter; left time: 4840.1988s
	iters: 200, epoch: 20 | loss: 0.3797352
	speed: 0.1328s/iter; left time: 1035.6442s
Epoch: 20 cost time: 35.55964708328247
Epoch: 20, Steps: 258 | Train Loss: 0.4967739 Vali Loss: 0.2619655 Test Loss: 0.3535215
Validation loss decreased (0.262061 --> 0.261966).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4227964
	speed: 0.5937s/iter; left time: 4536.6062s
	iters: 200, epoch: 21 | loss: 0.5517148
	speed: 0.1375s/iter; left time: 1036.7236s
Epoch: 21 cost time: 35.656067848205566
Epoch: 21, Steps: 258 | Train Loss: 0.4960518 Vali Loss: 0.2618012 Test Loss: 0.3532771
Validation loss decreased (0.261966 --> 0.261801).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5968061
	speed: 0.5715s/iter; left time: 4219.2873s
	iters: 200, epoch: 22 | loss: 0.5032013
	speed: 0.1272s/iter; left time: 926.2063s
Epoch: 22 cost time: 34.69867563247681
Epoch: 22, Steps: 258 | Train Loss: 0.4965758 Vali Loss: 0.2620782 Test Loss: 0.3532268
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5009083
	speed: 0.5933s/iter; left time: 4227.3305s
	iters: 200, epoch: 23 | loss: 0.5117233
	speed: 0.1334s/iter; left time: 937.0480s
Epoch: 23 cost time: 35.740166664123535
Epoch: 23, Steps: 258 | Train Loss: 0.4961131 Vali Loss: 0.2617081 Test Loss: 0.3533283
Validation loss decreased (0.261801 --> 0.261708).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4472099
	speed: 0.6048s/iter; left time: 4153.2396s
	iters: 200, epoch: 24 | loss: 0.5489059
	speed: 0.1400s/iter; left time: 947.3326s
Epoch: 24 cost time: 36.588998794555664
Epoch: 24, Steps: 258 | Train Loss: 0.4960191 Vali Loss: 0.2618588 Test Loss: 0.3532982
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5223233
	speed: 0.6052s/iter; left time: 3999.7648s
	iters: 200, epoch: 25 | loss: 0.3547261
	speed: 0.1366s/iter; left time: 889.0614s
Epoch: 25 cost time: 36.22406339645386
Epoch: 25, Steps: 258 | Train Loss: 0.4963199 Vali Loss: 0.2616819 Test Loss: 0.3532861
Validation loss decreased (0.261708 --> 0.261682).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4201558
	speed: 0.5868s/iter; left time: 3726.8876s
	iters: 200, epoch: 26 | loss: 0.5333872
	speed: 0.1354s/iter; left time: 846.5607s
Epoch: 26 cost time: 34.73117661476135
Epoch: 26, Steps: 258 | Train Loss: 0.4961215 Vali Loss: 0.2614941 Test Loss: 0.3532588
Validation loss decreased (0.261682 --> 0.261494).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4076527
	speed: 0.5772s/iter; left time: 3516.9940s
	iters: 200, epoch: 27 | loss: 0.4754615
	speed: 0.1258s/iter; left time: 754.1367s
Epoch: 27 cost time: 34.15333342552185
Epoch: 27, Steps: 258 | Train Loss: 0.4959531 Vali Loss: 0.2614111 Test Loss: 0.3532103
Validation loss decreased (0.261494 --> 0.261411).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5276283
	speed: 0.5733s/iter; left time: 3344.9410s
	iters: 200, epoch: 28 | loss: 0.5361345
	speed: 0.1255s/iter; left time: 719.6345s
Epoch: 28 cost time: 33.10674548149109
Epoch: 28, Steps: 258 | Train Loss: 0.4960596 Vali Loss: 0.2617285 Test Loss: 0.3532542
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.6325688
	speed: 0.5810s/iter; left time: 3239.9659s
	iters: 200, epoch: 29 | loss: 0.6081091
	speed: 0.1308s/iter; left time: 716.3958s
Epoch: 29 cost time: 35.00561547279358
Epoch: 29, Steps: 258 | Train Loss: 0.4958934 Vali Loss: 0.2617656 Test Loss: 0.3531759
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4507520
	speed: 0.5564s/iter; left time: 2959.3258s
	iters: 200, epoch: 30 | loss: 0.5833489
	speed: 0.1199s/iter; left time: 625.9090s
Epoch: 30 cost time: 32.76498889923096
Epoch: 30, Steps: 258 | Train Loss: 0.4953461 Vali Loss: 0.2614861 Test Loss: 0.3531959
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.34982600808143616, mae:0.3786821663379669, rse:0.4754123091697693, corr:[0.53299904 0.539577   0.5397515  0.53730714 0.5350962  0.5341394
 0.53441566 0.53537256 0.53628105 0.5366324  0.5363594  0.535733
 0.5351312  0.5347803  0.5347797  0.534934   0.5350224  0.53482795
 0.534228   0.53330755 0.5323261  0.5314735  0.5309017  0.53065646
 0.53061277 0.53060937 0.53046745 0.5300931  0.5294913  0.5287798
 0.5280771  0.5275466  0.52719533 0.5269129  0.52661824 0.5262531
 0.5257461  0.5251197  0.52440166 0.5236749  0.52300787 0.52241373
 0.5219164  0.521486   0.521079   0.5206075  0.52003944 0.51938576
 0.51862186 0.51774305 0.51682305 0.51597756 0.5152392  0.5146265
 0.51413596 0.51374185 0.51338506 0.51302254 0.5126555  0.512279
 0.51193684 0.51167107 0.5114883  0.5113415  0.5111708  0.51097333
 0.51065344 0.5102675  0.5098434  0.5094058  0.50897443 0.5085693
 0.5082151  0.50788593 0.50752187 0.50709164 0.5065875  0.5059994
 0.5053728  0.50471824 0.50400853 0.5032865  0.5025989  0.5019631
 0.5013958  0.5008945  0.5004046  0.4999457  0.49947956 0.498997
 0.49850532 0.49796733 0.49730417 0.496457   0.4953739  0.49405417
 0.49256986 0.49106196 0.4895745  0.48815703 0.4868687  0.48573768
 0.48474956 0.48384652 0.48295036 0.48201495 0.4810334  0.48004565
 0.47904274 0.4780484  0.47709602 0.4761941  0.47539976 0.47464988
 0.47386613 0.47301397 0.4720931  0.47108698 0.47005156 0.4690024
 0.468048   0.46720982 0.46653605 0.46596628 0.4654101  0.46477512
 0.46403953 0.46319246 0.4622586  0.46128085 0.46033877 0.45947593
 0.45874575 0.4581352  0.45762384 0.4571341  0.45658356 0.45596504
 0.4552777  0.45455587 0.45382258 0.45311236 0.4524492  0.4517685
 0.4510373  0.4501934  0.44933298 0.44848728 0.4477129  0.44700527
 0.44639105 0.4458598  0.44531307 0.44470632 0.44403788 0.44335625
 0.44270855 0.44212013 0.441571   0.4411098  0.44068035 0.4402628
 0.43982235 0.43937695 0.43893737 0.43852046 0.4381052  0.43768644
 0.4372788  0.43690544 0.43649867 0.4360733  0.4355874  0.43505535
 0.4344747  0.43386626 0.4332348  0.43258148 0.43190062 0.431252
 0.430631   0.4300482  0.4295356  0.42906877 0.4285967  0.42805725
 0.42744502 0.4266857  0.42577198 0.42467138 0.42340022 0.42198107
 0.42050532 0.41918027 0.417955   0.41679224 0.4156965  0.41463283
 0.41360494 0.41258734 0.41154602 0.41044864 0.4092696  0.40801537
 0.40671715 0.40541217 0.4041189  0.40286383 0.40174648 0.40085506
 0.40007544 0.39930636 0.39847577 0.39755902 0.39659294 0.39554507
 0.394426   0.39321804 0.39201885 0.39086342 0.38975948 0.38871455
 0.38771793 0.38677257 0.38591248 0.3850279  0.38408226 0.38312793
 0.38216126 0.38127115 0.3804906  0.379842   0.3792862  0.37879294
 0.3782857  0.37776008 0.37719828 0.37659675 0.37599242 0.3753834
 0.3747861  0.37416852 0.37355763 0.37311152 0.37281036 0.37271038
 0.372789   0.3729951  0.37320778 0.37330866 0.37328467 0.37310424
 0.37276995 0.37235564 0.37185812 0.37133628 0.3708085  0.37034214
 0.3699522  0.36963278 0.36934504 0.3690862  0.36881843 0.36859074
 0.36839363 0.36817658 0.36794847 0.36770365 0.3674104  0.3670358
 0.3666397  0.36623725 0.36581114 0.36540213 0.3650016  0.36460093
 0.3642585  0.36393228 0.36360708 0.36335975 0.3631538  0.36298975
 0.36279547 0.36251873 0.3620998  0.36147213 0.36063343 0.3595475
 0.3583545  0.35728392 0.35641903 0.35576022 0.3552911  0.35491124
 0.3546165  0.35431057 0.35389635 0.35332337 0.35262    0.35177252
 0.35089073 0.34997812 0.34916186 0.34841532 0.34781435 0.3473512
 0.3470527  0.34680274 0.3465391  0.34622908 0.34586203 0.34554425
 0.34522295 0.34495324 0.34469086 0.3443883  0.34405613 0.34371433
 0.34333438 0.34296656 0.34266582 0.34244394 0.3422903  0.34222233
 0.34222132 0.34231424 0.34247118 0.3425806  0.34264073 0.3426049
 0.34254158 0.34243146 0.34230503 0.34220767 0.34208915 0.3419854
 0.34185898 0.3416126  0.34130958 0.3410612  0.34092742 0.34089392
 0.3409693  0.34110627 0.34124118 0.34128064 0.34117946 0.3409732
 0.34069094 0.34041357 0.34018046 0.3400021  0.33988652 0.33979258
 0.3396882  0.33957237 0.33938366 0.3391871  0.3389198  0.33864158
 0.33838764 0.33818027 0.3380225  0.33790153 0.3377692  0.33764067
 0.33743918 0.33719152 0.3368909  0.3365973  0.33639258 0.33629724
 0.3363418  0.33655658 0.33689064 0.337251   0.33752826 0.33768728
 0.33772132 0.33760688 0.33730656 0.33677027 0.3361191  0.33535036
 0.33459643 0.3339249  0.33336672 0.3328675  0.3323238  0.33177134
 0.33116564 0.3305273  0.3298928  0.32930472 0.32879642 0.32832688
 0.3278678  0.32742637 0.32689872 0.32626492 0.32561672 0.32493848
 0.32426882 0.3236103  0.32301405 0.32253048 0.32206738 0.321617
 0.3211332  0.32066977 0.32025883 0.31991082 0.3196948  0.31959838
 0.31967562 0.3199152  0.32020107 0.32044578 0.32055828 0.32053936
 0.3204493  0.32033777 0.32020533 0.32007766 0.31997523 0.3199057
 0.3198913  0.31990588 0.31997293 0.3200802  0.3202321  0.32041022
 0.3205648  0.320589   0.32047033 0.32023418 0.3199531  0.31964436
 0.31928086 0.318968   0.3186722  0.3183332  0.31794736 0.3175716
 0.31715196 0.3167473  0.3163668  0.31599548 0.31568116 0.31540698
 0.31520244 0.31504378 0.31492388 0.31475163 0.31451365 0.31423783
 0.3139314  0.31362465 0.31333107 0.3130663  0.31289226 0.31273514
 0.31259143 0.31242386 0.31221268 0.3119342  0.31157196 0.31115296
 0.3107353  0.31034997 0.30999753 0.30964983 0.30932835 0.30897227
 0.30855247 0.30796388 0.3072269  0.30629668 0.30522457 0.30402952
 0.30275574 0.3015321  0.30036148 0.2992737  0.29830074 0.29743725
 0.29663908 0.29592773 0.29525515 0.2946573  0.29406467 0.29341158
 0.2926597  0.29189608 0.29116425 0.29045913 0.28983155 0.28929532
 0.28886133 0.28841493 0.28788805 0.28731453 0.28667313 0.28605908
 0.28542796 0.28477705 0.28415224 0.28355944 0.28306687 0.28263697
 0.2822562  0.281891   0.28149626 0.28106052 0.28061634 0.2801839
 0.27975512 0.27930295 0.27896532 0.27872005 0.27853543 0.27840403
 0.27829772 0.2782145  0.27811182 0.2779803  0.27783749 0.2776514
 0.2773689  0.2770229  0.27663857 0.27628514 0.27604675 0.2759389
 0.27589914 0.27587023 0.2758785  0.27584943 0.27575085 0.2755718
 0.27532813 0.27507183 0.2748007  0.2745717  0.27436915 0.27416605
 0.27394408 0.27374074 0.27354714 0.2733468  0.27308482 0.27280304
 0.27248532 0.27219918 0.27200425 0.27195188 0.27192548 0.271943
 0.2719658  0.27200076 0.27198192 0.27186468 0.27168047 0.2714829
 0.27132875 0.27123386 0.27120483 0.27118313 0.2710963  0.2708761
 0.27045873 0.26979744 0.26883295 0.26762307 0.26628023 0.26485342
 0.26344708 0.2622926  0.26141188 0.26073393 0.26009104 0.25936946
 0.25853342 0.25762075 0.25671488 0.25587568 0.25513065 0.25449955
 0.25392306 0.25330427 0.25266728 0.25199568 0.25131264 0.2506991
 0.25018474 0.24973938 0.24940674 0.24911135 0.2488719  0.24868162
 0.24850576 0.24836431 0.24819459 0.24795958 0.24773511 0.24754153
 0.24731374 0.24704929 0.24679549 0.24648416 0.24617192 0.24589004
 0.24574976 0.24564274 0.24561767 0.24573503 0.24595611 0.24620633
 0.24644473 0.24671263 0.2469975  0.24732149 0.24771343 0.24816725
 0.24858128 0.24884354 0.24890888 0.24888271 0.24883734 0.24882734
 0.24881725 0.2488046  0.2488114  0.24865696 0.24848297 0.24827746
 0.24808231 0.24795888 0.24785328 0.24785931 0.24780488 0.24768496
 0.24744399 0.24721865 0.24699147 0.24670911 0.24653865 0.2463559
 0.24623443 0.24614011 0.24609125 0.2460882  0.24621023 0.24630557
 0.24645166 0.24654546 0.24647129 0.24633054 0.24617025 0.24596575
 0.2457155  0.24559096 0.24555206 0.24561828 0.24585679 0.24618231
 0.24647029 0.24668664 0.2467286  0.24641687 0.24568973 0.24462813
 0.2433681  0.24206708 0.24083431 0.2397041  0.23877332 0.23800386
 0.23734564 0.23674634 0.23638998 0.23613995 0.23595825 0.23571356
 0.23533961 0.23483557 0.23423526 0.23357289 0.23300648 0.23246194
 0.23195754 0.23145536 0.23089062 0.23044981 0.23009396 0.2297929
 0.22955623 0.22929059 0.22901228 0.22856045 0.22794773 0.22717866
 0.22643098 0.22561553 0.22486894 0.2243228  0.22399932 0.22382535
 0.22354534 0.22304788 0.22221619 0.22122315 0.22008379 0.2192439
 0.21908687 0.2197682  0.22096294 0.22176723 0.22061461 0.21530493]
