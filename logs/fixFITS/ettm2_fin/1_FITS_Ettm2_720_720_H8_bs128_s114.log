Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=74, out_features=148, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19625984.0
params:  11100.0
Trainable parameters:  11100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6186039
	speed: 0.1777s/iter; left time: 1128.5808s
Epoch: 1 cost time: 22.520938396453857
Epoch: 1, Steps: 129 | Train Loss: 0.6167686 Vali Loss: 0.2951058 Test Loss: 0.3981323
Validation loss decreased (inf --> 0.295106).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5330470
	speed: 0.4514s/iter; left time: 2808.7374s
Epoch: 2 cost time: 22.163968801498413
Epoch: 2, Steps: 129 | Train Loss: 0.5345370 Vali Loss: 0.2778148 Test Loss: 0.3800303
Validation loss decreased (0.295106 --> 0.277815).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4586806
	speed: 0.4407s/iter; left time: 2685.3151s
Epoch: 3 cost time: 20.79567241668701
Epoch: 3, Steps: 129 | Train Loss: 0.5201456 Vali Loss: 0.2722880 Test Loss: 0.3735005
Validation loss decreased (0.277815 --> 0.272288).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5467230
	speed: 0.4174s/iter; left time: 2489.5477s
Epoch: 4 cost time: 20.035152912139893
Epoch: 4, Steps: 129 | Train Loss: 0.5140081 Vali Loss: 0.2692533 Test Loss: 0.3700920
Validation loss decreased (0.272288 --> 0.269253).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3795946
	speed: 0.3910s/iter; left time: 2281.3550s
Epoch: 5 cost time: 19.097095251083374
Epoch: 5, Steps: 129 | Train Loss: 0.5095757 Vali Loss: 0.2678420 Test Loss: 0.3676482
Validation loss decreased (0.269253 --> 0.267842).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6143701
	speed: 0.4053s/iter; left time: 2312.8317s
Epoch: 6 cost time: 19.915923595428467
Epoch: 6, Steps: 129 | Train Loss: 0.5075052 Vali Loss: 0.2667261 Test Loss: 0.3658813
Validation loss decreased (0.267842 --> 0.266726).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4327073
	speed: 0.4200s/iter; left time: 2342.2771s
Epoch: 7 cost time: 20.969667434692383
Epoch: 7, Steps: 129 | Train Loss: 0.5056053 Vali Loss: 0.2653678 Test Loss: 0.3648411
Validation loss decreased (0.266726 --> 0.265368).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5397828
	speed: 0.4323s/iter; left time: 2355.0381s
Epoch: 8 cost time: 20.800827026367188
Epoch: 8, Steps: 129 | Train Loss: 0.5030931 Vali Loss: 0.2651965 Test Loss: 0.3636693
Validation loss decreased (0.265368 --> 0.265197).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4737082
	speed: 0.4789s/iter; left time: 2547.2410s
Epoch: 9 cost time: 21.74335217475891
Epoch: 9, Steps: 129 | Train Loss: 0.5027986 Vali Loss: 0.2645888 Test Loss: 0.3632993
Validation loss decreased (0.265197 --> 0.264589).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5070336
	speed: 0.4588s/iter; left time: 2380.9409s
Epoch: 10 cost time: 22.247413396835327
Epoch: 10, Steps: 129 | Train Loss: 0.5015247 Vali Loss: 0.2644621 Test Loss: 0.3624164
Validation loss decreased (0.264589 --> 0.264462).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4503697
	speed: 0.4798s/iter; left time: 2428.1134s
Epoch: 11 cost time: 23.316757917404175
Epoch: 11, Steps: 129 | Train Loss: 0.5011431 Vali Loss: 0.2641768 Test Loss: 0.3619538
Validation loss decreased (0.264462 --> 0.264177).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4205019
	speed: 0.4481s/iter; left time: 2209.9069s
Epoch: 12 cost time: 21.39417576789856
Epoch: 12, Steps: 129 | Train Loss: 0.5000782 Vali Loss: 0.2637736 Test Loss: 0.3616075
Validation loss decreased (0.264177 --> 0.263774).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4440910
	speed: 0.4344s/iter; left time: 2086.5587s
Epoch: 13 cost time: 20.8283052444458
Epoch: 13, Steps: 129 | Train Loss: 0.5004546 Vali Loss: 0.2636368 Test Loss: 0.3613451
Validation loss decreased (0.263774 --> 0.263637).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4262827
	speed: 0.4480s/iter; left time: 2093.7392s
Epoch: 14 cost time: 21.91841435432434
Epoch: 14, Steps: 129 | Train Loss: 0.4995770 Vali Loss: 0.2634194 Test Loss: 0.3609244
Validation loss decreased (0.263637 --> 0.263419).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5318410
	speed: 0.4556s/iter; left time: 2070.7687s
Epoch: 15 cost time: 21.744423866271973
Epoch: 15, Steps: 129 | Train Loss: 0.4994134 Vali Loss: 0.2635154 Test Loss: 0.3607178
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5470161
	speed: 0.4557s/iter; left time: 2012.5296s
Epoch: 16 cost time: 21.353890419006348
Epoch: 16, Steps: 129 | Train Loss: 0.4992366 Vali Loss: 0.2630343 Test Loss: 0.3605632
Validation loss decreased (0.263419 --> 0.263034).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5326424
	speed: 0.4309s/iter; left time: 1847.3946s
Epoch: 17 cost time: 20.971478939056396
Epoch: 17, Steps: 129 | Train Loss: 0.4985921 Vali Loss: 0.2630888 Test Loss: 0.3603431
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4816104
	speed: 0.4274s/iter; left time: 1777.2302s
Epoch: 18 cost time: 20.197741270065308
Epoch: 18, Steps: 129 | Train Loss: 0.4988410 Vali Loss: 0.2630726 Test Loss: 0.3602518
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5929952
	speed: 0.4349s/iter; left time: 1752.1197s
Epoch: 19 cost time: 20.952698707580566
Epoch: 19, Steps: 129 | Train Loss: 0.4986214 Vali Loss: 0.2627029 Test Loss: 0.3601215
Validation loss decreased (0.263034 --> 0.262703).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5121229
	speed: 0.4378s/iter; left time: 1707.3915s
Epoch: 20 cost time: 20.712514638900757
Epoch: 20, Steps: 129 | Train Loss: 0.4984056 Vali Loss: 0.2630295 Test Loss: 0.3599747
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.6319339
	speed: 0.4256s/iter; left time: 1604.9335s
Epoch: 21 cost time: 20.255027770996094
Epoch: 21, Steps: 129 | Train Loss: 0.4982966 Vali Loss: 0.2627330 Test Loss: 0.3598189
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5063219
	speed: 0.4305s/iter; left time: 1567.7527s
Epoch: 22 cost time: 21.401236057281494
Epoch: 22, Steps: 129 | Train Loss: 0.4982757 Vali Loss: 0.2626626 Test Loss: 0.3597987
Validation loss decreased (0.262703 --> 0.262663).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4318635
	speed: 0.4324s/iter; left time: 1519.1637s
Epoch: 23 cost time: 21.29214334487915
Epoch: 23, Steps: 129 | Train Loss: 0.4978161 Vali Loss: 0.2626605 Test Loss: 0.3596949
Validation loss decreased (0.262663 --> 0.262661).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5625956
	speed: 0.4584s/iter; left time: 1551.3781s
Epoch: 24 cost time: 22.64851450920105
Epoch: 24, Steps: 129 | Train Loss: 0.4972386 Vali Loss: 0.2625602 Test Loss: 0.3595665
Validation loss decreased (0.262661 --> 0.262560).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4592918
	speed: 0.4789s/iter; left time: 1558.8109s
Epoch: 25 cost time: 23.497774839401245
Epoch: 25, Steps: 129 | Train Loss: 0.4973803 Vali Loss: 0.2623825 Test Loss: 0.3595001
Validation loss decreased (0.262560 --> 0.262382).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.5722463
	speed: 0.4942s/iter; left time: 1544.7956s
Epoch: 26 cost time: 23.983407974243164
Epoch: 26, Steps: 129 | Train Loss: 0.4976499 Vali Loss: 0.2623023 Test Loss: 0.3594714
Validation loss decreased (0.262382 --> 0.262302).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4298640
	speed: 0.4646s/iter; left time: 1392.3908s
Epoch: 27 cost time: 22.127058506011963
Epoch: 27, Steps: 129 | Train Loss: 0.4973126 Vali Loss: 0.2624572 Test Loss: 0.3595073
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5258803
	speed: 0.4444s/iter; left time: 1274.6568s
Epoch: 28 cost time: 20.678409099578857
Epoch: 28, Steps: 129 | Train Loss: 0.4964524 Vali Loss: 0.2622935 Test Loss: 0.3594095
Validation loss decreased (0.262302 --> 0.262294).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.5322852
	speed: 0.4241s/iter; left time: 1161.5277s
Epoch: 29 cost time: 19.282028675079346
Epoch: 29, Steps: 129 | Train Loss: 0.4971711 Vali Loss: 0.2621263 Test Loss: 0.3593650
Validation loss decreased (0.262294 --> 0.262126).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.5623202
	speed: 0.3758s/iter; left time: 980.8198s
Epoch: 30 cost time: 19.99152946472168
Epoch: 30, Steps: 129 | Train Loss: 0.4971935 Vali Loss: 0.2625259 Test Loss: 0.3592916
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4378578
	speed: 0.4286s/iter; left time: 1063.3329s
Epoch: 31 cost time: 20.26281476020813
Epoch: 31, Steps: 129 | Train Loss: 0.4973161 Vali Loss: 0.2623606 Test Loss: 0.3592725
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4399317
	speed: 0.4321s/iter; left time: 1016.3472s
Epoch: 32 cost time: 21.432674884796143
Epoch: 32, Steps: 129 | Train Loss: 0.4967936 Vali Loss: 0.2622972 Test Loss: 0.3592278
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3504730463027954, mae:0.37900978326797485, rse:0.47585180401802063, corr:[0.527521   0.53686196 0.53979516 0.5381289  0.5356384  0.5342137
 0.53418404 0.5352076  0.53661007 0.5376172  0.5377529  0.5371035
 0.53614885 0.5353362  0.53492665 0.53487027 0.53501034 0.5351038
 0.53490347 0.5343065  0.53346086 0.53257024 0.5318584  0.53144956
 0.5313026  0.5313079  0.5312933  0.53109914 0.53063846 0.5299578
 0.52917504 0.5284679  0.5278944  0.5274105  0.52698404 0.5265873
 0.5261462  0.52562976 0.5250116  0.52433896 0.5236733  0.5230379
 0.5224648  0.5219376  0.52143437 0.5209029  0.5203153  0.5196616
 0.5189147  0.5180679  0.51718086 0.5163475  0.51559895 0.51498145
 0.5145246  0.5142021  0.5139438  0.5136792  0.5133773  0.51300853
 0.5126184  0.51224    0.5119079  0.51162744 0.51138616 0.51119334
 0.51094437 0.5106529  0.5102999  0.50987875 0.5093931  0.5088739
 0.5083791  0.50792825 0.50750446 0.5070749  0.5066113  0.50607073
 0.5054759  0.50479925 0.504026   0.5032067  0.5024243  0.50173616
 0.501169   0.5006983  0.5002539  0.4998243  0.4993445  0.49879485
 0.4981781  0.49747768 0.49667856 0.49576738 0.49470615 0.49349812
 0.4921807  0.49083287 0.48947075 0.48812935 0.48686695 0.48573455
 0.4847475  0.48387113 0.48302507 0.48214006 0.48117775 0.4801407
 0.47902706 0.47787818 0.47677055 0.47577    0.47495806 0.47428092
 0.47363317 0.47293365 0.47213277 0.47119132 0.47014788 0.4690408
 0.4679975  0.4670821  0.466366   0.46579108 0.46524844 0.464614
 0.4638355  0.46288878 0.46181005 0.46067142 0.45959923 0.45867887
 0.45796818 0.45742148 0.4569649  0.45646796 0.45582736 0.45503426
 0.4541207  0.4531734  0.45228076 0.45151833 0.45091254 0.45037058
 0.44980082 0.4490919  0.4482824  0.44739115 0.44650188 0.44567692
 0.44500133 0.4444946  0.44405854 0.44362435 0.44312346 0.44253933
 0.4418875  0.441207   0.44053835 0.43998617 0.43955263 0.43923095
 0.43896076 0.4386837  0.43833208 0.4378617  0.43726048 0.43657997
 0.4359191  0.4353849  0.43496966 0.4346831  0.4344165  0.43408012
 0.43357456 0.43286338 0.4319636  0.43095255 0.42995274 0.42912418
 0.42853    0.42817333 0.42799276 0.42784283 0.4275805  0.42708635
 0.42636874 0.42540294 0.42427742 0.42304993 0.4217942  0.42051607
 0.41923305 0.4180377  0.41682285 0.41554618 0.4142534  0.41298637
 0.4118211  0.41077846 0.40981188 0.40884003 0.40776572 0.40652773
 0.4051369  0.4036648  0.4022111  0.40090355 0.3999024  0.3992813
 0.39887062 0.3984646  0.39788786 0.39704794 0.39596978 0.3946894
 0.39333242 0.39199927 0.3908596  0.3899572  0.38923466 0.38857684
 0.38785696 0.3869944  0.38601318 0.384878   0.3836633  0.38253483
 0.38155597 0.38080677 0.38023898 0.37976387 0.37924746 0.37862143
 0.37784502 0.3770117  0.37623712 0.37563336 0.37527287 0.37510467
 0.3750146  0.37481743 0.37440178 0.3738334  0.37312216 0.37242877
 0.37189472 0.37162456 0.3716032  0.37173942 0.37194484 0.37207857
 0.37204197 0.37183106 0.37144777 0.3709965  0.37057117 0.37027958
 0.37012878 0.370048   0.3699122  0.3696345  0.36914253 0.36851215
 0.3678321  0.3671983  0.36674553 0.3665335  0.36649883 0.36650488
 0.36645824 0.3662409  0.36577797 0.36513633 0.36441317 0.3637442
 0.36329642 0.3630832  0.36304486 0.36313164 0.36316642 0.36304066
 0.3626603  0.3620295  0.36120826 0.36027506 0.35933986 0.35840055
 0.35752195 0.35675663 0.35603344 0.35525286 0.3543917  0.35345218
 0.3525827  0.35185194 0.35128227 0.35084757 0.35049704 0.35009372
 0.34960064 0.34892565 0.34816223 0.34735128 0.34665793 0.3461611
 0.3459312  0.34584132 0.34575155 0.3455361  0.34510496 0.3445346
 0.34383658 0.3431824  0.3426711  0.34236407 0.34229362 0.34240395
 0.3425395  0.34259236 0.34249187 0.3422003  0.34174454 0.3412458
 0.3408291  0.34064087 0.3407081  0.3409098  0.34115642 0.34130454
 0.3413255  0.3411744  0.34091145 0.34065345 0.34044564 0.34037277
 0.34039894 0.34038162 0.34026965 0.3400423  0.33968914 0.3392138
 0.33871588 0.33830446 0.33808392 0.33806425 0.3382086  0.3384552
 0.3386765  0.33878374 0.3387054  0.33842808 0.33802542 0.3375925
 0.33724552 0.3370789  0.33705604 0.33716545 0.3372309  0.33718067
 0.3369678  0.3366123  0.33619538 0.33583194 0.33560628 0.3355979
 0.33572865 0.33593243 0.33606514 0.33605188 0.33588102 0.3355712
 0.33522952 0.3350114  0.334993   0.33516413 0.33543286 0.33570254
 0.33586872 0.33580866 0.3354382  0.3347291  0.33386028 0.33292964
 0.3321325  0.33155063 0.33116984 0.3308551  0.33041957 0.3298273
 0.32901642 0.32805    0.32705146 0.32615593 0.32546946 0.32497084
 0.3245953  0.32427466 0.32383126 0.32318705 0.3224179  0.32155898
 0.32073054 0.32002285 0.31954044 0.31933314 0.3192618  0.31923386
 0.31911623 0.31888807 0.31855333 0.3181474  0.31778258 0.31750718
 0.31741542 0.3175204  0.31769696 0.31783962 0.31784424 0.3177052
 0.31750044 0.31731972 0.31721666 0.31724018 0.31740052 0.31764743
 0.31791905 0.31811228 0.31819588 0.3181486  0.31801873 0.31786793
 0.31773683 0.31760255 0.31748194 0.31736597 0.31725276 0.31707937
 0.3167683  0.3163969  0.3159775  0.31553876 0.31515315 0.31491965
 0.31478027 0.31472832 0.31468278 0.31454754 0.31431073 0.3139597
 0.3135763  0.31322044 0.31297484 0.31281033 0.31271353 0.3126608
 0.31257588 0.31240243 0.3121055  0.31170237 0.3112918  0.31088653
 0.31055924 0.31031325 0.31013536 0.30997038 0.30974448 0.3094231
 0.3090265  0.30858317 0.30813092 0.30768886 0.30731323 0.30696473
 0.30661583 0.3061501  0.30556253 0.30479684 0.30389172 0.3028603
 0.3017418  0.30064714 0.29956108 0.29848668 0.29742908 0.29638326
 0.29533315 0.29434747 0.29344964 0.29272768 0.29214904 0.29164192
 0.29111826 0.29058367 0.28999346 0.2892868  0.2884838  0.28761795
 0.2867781  0.28594893 0.28515568 0.28448457 0.2839211  0.2835115
 0.28315067 0.2827644  0.28233773 0.28184783 0.2813655  0.2808818
 0.28043365 0.28003332 0.27966475 0.27931973 0.2790134  0.27873728
 0.27846283 0.27814895 0.27790907 0.27772447 0.27757517 0.27746046
 0.27735835 0.27726117 0.27712834 0.27695236 0.2767599  0.27654162
 0.2762659  0.27597788 0.27569768 0.27546203 0.27530715 0.2752017
 0.27507508 0.27488488 0.2747068  0.27453414 0.2744018  0.2743278
 0.27431342 0.2743513  0.2743548  0.27429783 0.27411532 0.2737913
 0.27336112 0.27294588 0.27262342 0.27242753 0.27230498 0.27223927
 0.27213317 0.27196625 0.27174547 0.27152815 0.2712639  0.27105346
 0.27092946 0.27092668 0.27096134 0.27092776 0.27077863 0.2705084
 0.27016306 0.26981017 0.2695519  0.26943737 0.269467   0.26957735
 0.26965654 0.26955536 0.26910087 0.26824605 0.26704392 0.2655487
 0.26393577 0.2625121  0.26137602 0.2605054  0.25975    0.25898084
 0.25812304 0.2571848  0.25623152 0.25533214 0.25453764 0.25388536
 0.25331706 0.25272244 0.25209773 0.25139806 0.25062788 0.2498699
 0.24918498 0.24859619 0.24819355 0.24792422 0.24779063 0.24774459
 0.24769896 0.24761917 0.24741507 0.2470535  0.24664538 0.24627566
 0.24595821 0.24573898 0.24567312 0.24566251 0.24569777 0.24572633
 0.24578288 0.24572754 0.24561414 0.24555469 0.24559069 0.24572231
 0.2459512  0.24630977 0.24673928 0.24718404 0.24759978 0.24794254
 0.24812897 0.24811326 0.24794237 0.24780157 0.24780813 0.24800293
 0.24829806 0.24861036 0.2488817  0.24890178 0.24880205 0.24860165
 0.24838741 0.2482514  0.24815755 0.24817552 0.24810214 0.24791303
 0.24755274 0.2471928  0.2468811  0.24662204 0.24658936 0.24663873
 0.24676803 0.24684279 0.24678949 0.24657121 0.24629642 0.24592818
 0.24567935 0.24556802 0.24553737 0.24566203 0.24589345 0.24607971
 0.24610469 0.24607305 0.2459608  0.24586475 0.24595363 0.24622297
 0.24657896 0.24695678 0.24717471 0.24696043 0.24619113 0.244951
 0.2434438  0.24192064 0.24057001 0.23946166 0.23867594 0.23812057
 0.2376622  0.23716143 0.2367449  0.2362895  0.23582263 0.23530062
 0.23474537 0.23421067 0.2337495  0.23335955 0.2331123  0.23284177
 0.2325011  0.23203738 0.23141128 0.23086073 0.23039821 0.2300474
 0.22983567 0.22964501 0.22943568 0.22899902 0.22834332 0.22751737
 0.22677316 0.2260793  0.22560586 0.2254546  0.22554673 0.22569284
 0.22556771 0.22507352 0.22416303 0.22312315 0.22207387 0.2215244
 0.22174229 0.22257474 0.2233846  0.22317363 0.22051397 0.21381998]
