Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  40269824.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3972686
	speed: 0.1369s/iter; left time: 869.6174s
Epoch: 1 cost time: 17.827844858169556
Epoch: 1, Steps: 129 | Train Loss: 0.4942232 Vali Loss: 0.3191187 Test Loss: 0.4291774
Validation loss decreased (inf --> 0.319119).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3220869
	speed: 0.4063s/iter; left time: 2528.0690s
Epoch: 2 cost time: 20.545964241027832
Epoch: 2, Steps: 129 | Train Loss: 0.3727429 Vali Loss: 0.2960465 Test Loss: 0.4005874
Validation loss decreased (0.319119 --> 0.296047).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2890846
	speed: 0.3783s/iter; left time: 2305.1671s
Epoch: 3 cost time: 16.53275227546692
Epoch: 3, Steps: 129 | Train Loss: 0.3332402 Vali Loss: 0.2870325 Test Loss: 0.3900532
Validation loss decreased (0.296047 --> 0.287032).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3021320
	speed: 0.3462s/iter; left time: 2064.7150s
Epoch: 4 cost time: 18.309691667556763
Epoch: 4, Steps: 129 | Train Loss: 0.3124029 Vali Loss: 0.2818765 Test Loss: 0.3840365
Validation loss decreased (0.287032 --> 0.281877).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3102239
	speed: 0.4113s/iter; left time: 2400.1806s
Epoch: 5 cost time: 20.996066570281982
Epoch: 5, Steps: 129 | Train Loss: 0.2983024 Vali Loss: 0.2785435 Test Loss: 0.3796479
Validation loss decreased (0.281877 --> 0.278544).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3373289
	speed: 0.3880s/iter; left time: 2213.6501s
Epoch: 6 cost time: 17.9987850189209
Epoch: 6, Steps: 129 | Train Loss: 0.2881967 Vali Loss: 0.2757514 Test Loss: 0.3761322
Validation loss decreased (0.278544 --> 0.275751).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2532879
	speed: 0.3806s/iter; left time: 2122.5283s
Epoch: 7 cost time: 19.424038648605347
Epoch: 7, Steps: 129 | Train Loss: 0.2806777 Vali Loss: 0.2736433 Test Loss: 0.3734355
Validation loss decreased (0.275751 --> 0.273643).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2644336
	speed: 0.4377s/iter; left time: 2384.4842s
Epoch: 8 cost time: 22.428101301193237
Epoch: 8, Steps: 129 | Train Loss: 0.2753864 Vali Loss: 0.2720174 Test Loss: 0.3711975
Validation loss decreased (0.273643 --> 0.272017).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2325510
	speed: 0.4273s/iter; left time: 2272.5619s
Epoch: 9 cost time: 20.123474597930908
Epoch: 9, Steps: 129 | Train Loss: 0.2710438 Vali Loss: 0.2702385 Test Loss: 0.3693599
Validation loss decreased (0.272017 --> 0.270238).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2720074
	speed: 0.4237s/iter; left time: 2198.9321s
Epoch: 10 cost time: 19.920142889022827
Epoch: 10, Steps: 129 | Train Loss: 0.2678544 Vali Loss: 0.2694679 Test Loss: 0.3677147
Validation loss decreased (0.270238 --> 0.269468).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2726321
	speed: 0.3850s/iter; left time: 1948.4047s
Epoch: 11 cost time: 19.260241985321045
Epoch: 11, Steps: 129 | Train Loss: 0.2652650 Vali Loss: 0.2681764 Test Loss: 0.3665913
Validation loss decreased (0.269468 --> 0.268176).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2533793
	speed: 0.4202s/iter; left time: 2072.5678s
Epoch: 12 cost time: 20.341135263442993
Epoch: 12, Steps: 129 | Train Loss: 0.2634697 Vali Loss: 0.2672516 Test Loss: 0.3655400
Validation loss decreased (0.268176 --> 0.267252).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3227190
	speed: 0.3911s/iter; left time: 1878.6127s
Epoch: 13 cost time: 18.84433627128601
Epoch: 13, Steps: 129 | Train Loss: 0.2621595 Vali Loss: 0.2667762 Test Loss: 0.3646233
Validation loss decreased (0.267252 --> 0.266776).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2456837
	speed: 0.3461s/iter; left time: 1617.8190s
Epoch: 14 cost time: 14.07879638671875
Epoch: 14, Steps: 129 | Train Loss: 0.2611124 Vali Loss: 0.2661256 Test Loss: 0.3639780
Validation loss decreased (0.266776 --> 0.266126).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2587019
	speed: 0.4059s/iter; left time: 1844.6111s
Epoch: 15 cost time: 20.67769169807434
Epoch: 15, Steps: 129 | Train Loss: 0.2599425 Vali Loss: 0.2657198 Test Loss: 0.3633654
Validation loss decreased (0.266126 --> 0.265720).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2517406
	speed: 0.3773s/iter; left time: 1665.9438s
Epoch: 16 cost time: 18.745867252349854
Epoch: 16, Steps: 129 | Train Loss: 0.2592868 Vali Loss: 0.2653839 Test Loss: 0.3628838
Validation loss decreased (0.265720 --> 0.265384).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2186846
	speed: 0.3734s/iter; left time: 1600.7154s
Epoch: 17 cost time: 18.015274047851562
Epoch: 17, Steps: 129 | Train Loss: 0.2586129 Vali Loss: 0.2649376 Test Loss: 0.3624564
Validation loss decreased (0.265384 --> 0.264938).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2529003
	speed: 0.3955s/iter; left time: 1644.4525s
Epoch: 18 cost time: 20.220680952072144
Epoch: 18, Steps: 129 | Train Loss: 0.2582602 Vali Loss: 0.2647910 Test Loss: 0.3621218
Validation loss decreased (0.264938 --> 0.264791).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2010036
	speed: 0.4051s/iter; left time: 1632.2754s
Epoch: 19 cost time: 19.09996771812439
Epoch: 19, Steps: 129 | Train Loss: 0.2578559 Vali Loss: 0.2644346 Test Loss: 0.3618252
Validation loss decreased (0.264791 --> 0.264435).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2979515
	speed: 0.3743s/iter; left time: 1459.7612s
Epoch: 20 cost time: 19.468369007110596
Epoch: 20, Steps: 129 | Train Loss: 0.2576683 Vali Loss: 0.2641356 Test Loss: 0.3615936
Validation loss decreased (0.264435 --> 0.264136).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2665349
	speed: 0.3899s/iter; left time: 1470.1624s
Epoch: 21 cost time: 18.853019952774048
Epoch: 21, Steps: 129 | Train Loss: 0.2574469 Vali Loss: 0.2640309 Test Loss: 0.3613930
Validation loss decreased (0.264136 --> 0.264031).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2288191
	speed: 0.3865s/iter; left time: 1407.6296s
Epoch: 22 cost time: 18.252957105636597
Epoch: 22, Steps: 129 | Train Loss: 0.2572522 Vali Loss: 0.2640727 Test Loss: 0.3612204
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2580527
	speed: 0.3943s/iter; left time: 1385.0113s
Epoch: 23 cost time: 20.8954119682312
Epoch: 23, Steps: 129 | Train Loss: 0.2569979 Vali Loss: 0.2637379 Test Loss: 0.3611172
Validation loss decreased (0.264031 --> 0.263738).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2153784
	speed: 0.4204s/iter; left time: 1422.6128s
Epoch: 24 cost time: 20.52125883102417
Epoch: 24, Steps: 129 | Train Loss: 0.2566664 Vali Loss: 0.2636491 Test Loss: 0.3609110
Validation loss decreased (0.263738 --> 0.263649).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2376185
	speed: 0.4119s/iter; left time: 1340.8302s
Epoch: 25 cost time: 20.048778295516968
Epoch: 25, Steps: 129 | Train Loss: 0.2566144 Vali Loss: 0.2634490 Test Loss: 0.3608377
Validation loss decreased (0.263649 --> 0.263449).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2516232
	speed: 0.4010s/iter; left time: 1253.4266s
Epoch: 26 cost time: 19.15939998626709
Epoch: 26, Steps: 129 | Train Loss: 0.2566029 Vali Loss: 0.2637198 Test Loss: 0.3607098
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2512152
	speed: 0.3916s/iter; left time: 1173.7664s
Epoch: 27 cost time: 19.45672082901001
Epoch: 27, Steps: 129 | Train Loss: 0.2564988 Vali Loss: 0.2632126 Test Loss: 0.3606140
Validation loss decreased (0.263449 --> 0.263213).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2589805
	speed: 0.3922s/iter; left time: 1124.8018s
Epoch: 28 cost time: 19.024359464645386
Epoch: 28, Steps: 129 | Train Loss: 0.2564102 Vali Loss: 0.2633753 Test Loss: 0.3605444
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2217366
	speed: 0.4161s/iter; left time: 1139.6140s
Epoch: 29 cost time: 19.874632358551025
Epoch: 29, Steps: 129 | Train Loss: 0.2558777 Vali Loss: 0.2632644 Test Loss: 0.3605215
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2416607
	speed: 0.4083s/iter; left time: 1065.5709s
Epoch: 30 cost time: 21.48352074623108
Epoch: 30, Steps: 129 | Train Loss: 0.2563836 Vali Loss: 0.2633810 Test Loss: 0.3604032
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  40269824.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5181404
	speed: 0.1715s/iter; left time: 1089.1227s
Epoch: 1 cost time: 21.676039695739746
Epoch: 1, Steps: 129 | Train Loss: 0.4980026 Vali Loss: 0.2620412 Test Loss: 0.3594768
Validation loss decreased (inf --> 0.262041).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4149165
	speed: 0.4280s/iter; left time: 2663.3178s
Epoch: 2 cost time: 20.4127094745636
Epoch: 2, Steps: 129 | Train Loss: 0.4969539 Vali Loss: 0.2615173 Test Loss: 0.3591110
Validation loss decreased (0.262041 --> 0.261517).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6363406
	speed: 0.3785s/iter; left time: 2306.1515s
Epoch: 3 cost time: 18.60831928253174
Epoch: 3, Steps: 129 | Train Loss: 0.4957101 Vali Loss: 0.2612171 Test Loss: 0.3584976
Validation loss decreased (0.261517 --> 0.261217).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5272384
	speed: 0.3922s/iter; left time: 2339.0199s
Epoch: 4 cost time: 19.62461280822754
Epoch: 4, Steps: 129 | Train Loss: 0.4952942 Vali Loss: 0.2608118 Test Loss: 0.3585624
Validation loss decreased (0.261217 --> 0.260812).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4379324
	speed: 0.4189s/iter; left time: 2444.1356s
Epoch: 5 cost time: 20.012927055358887
Epoch: 5, Steps: 129 | Train Loss: 0.4953026 Vali Loss: 0.2606650 Test Loss: 0.3581799
Validation loss decreased (0.260812 --> 0.260665).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5782026
	speed: 0.4166s/iter; left time: 2376.9994s
Epoch: 6 cost time: 20.16515874862671
Epoch: 6, Steps: 129 | Train Loss: 0.4951784 Vali Loss: 0.2606288 Test Loss: 0.3581556
Validation loss decreased (0.260665 --> 0.260629).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3871233
	speed: 0.4103s/iter; left time: 2288.2312s
Epoch: 7 cost time: 20.089295148849487
Epoch: 7, Steps: 129 | Train Loss: 0.4947723 Vali Loss: 0.2605174 Test Loss: 0.3580887
Validation loss decreased (0.260629 --> 0.260517).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4228350
	speed: 0.3882s/iter; left time: 2114.9392s
Epoch: 8 cost time: 20.46119523048401
Epoch: 8, Steps: 129 | Train Loss: 0.4946400 Vali Loss: 0.2605087 Test Loss: 0.3579912
Validation loss decreased (0.260517 --> 0.260509).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5074083
	speed: 0.4347s/iter; left time: 2311.9497s
Epoch: 9 cost time: 19.476340770721436
Epoch: 9, Steps: 129 | Train Loss: 0.4945546 Vali Loss: 0.2603411 Test Loss: 0.3578810
Validation loss decreased (0.260509 --> 0.260341).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4830216
	speed: 0.4055s/iter; left time: 2104.6047s
Epoch: 10 cost time: 20.196268796920776
Epoch: 10, Steps: 129 | Train Loss: 0.4939963 Vali Loss: 0.2605544 Test Loss: 0.3579170
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4876013
	speed: 0.4071s/iter; left time: 2060.4240s
Epoch: 11 cost time: 20.942067861557007
Epoch: 11, Steps: 129 | Train Loss: 0.4940784 Vali Loss: 0.2605278 Test Loss: 0.3577962
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5256643
	speed: 0.4476s/iter; left time: 2207.5530s
Epoch: 12 cost time: 21.24191117286682
Epoch: 12, Steps: 129 | Train Loss: 0.4940492 Vali Loss: 0.2604992 Test Loss: 0.3576996
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3490242660045624, mae:0.3779105544090271, rse:0.47486722469329834, corr:[0.54029286 0.5452901  0.54383004 0.54093546 0.53983617 0.54053515
 0.5415891  0.54159516 0.54050756 0.53937525 0.53901446 0.5394404
 0.5400394  0.5400284  0.539196   0.5379851  0.53701574 0.5365279
 0.5363226  0.53599113 0.53530496 0.5343531  0.5335056  0.53305787
 0.5329624  0.5329207  0.5325664  0.5317611  0.5307064  0.5297729
 0.529192   0.5289736  0.52885956 0.5285389  0.52791387 0.52710897
 0.52630067 0.52563024 0.52508503 0.52459246 0.52406275 0.5234478
 0.5228131  0.5222171  0.5216817  0.5211465  0.5205278  0.51976407
 0.5188549  0.5178809  0.5170007  0.51630574 0.5157429  0.5152301
 0.51470304 0.5141383  0.5135799  0.51310647 0.5127782  0.51254934
 0.512384   0.5122029  0.51196706 0.5116799  0.51138645 0.51116204
 0.51092803 0.51068205 0.5103803  0.5100136  0.50960976 0.50920933
 0.5088387  0.50846535 0.50802183 0.5074796  0.5068577  0.5061798
 0.5055239  0.504877   0.50420326 0.5035225  0.5028828  0.50231594
 0.5018456  0.5014345  0.5009964  0.500527   0.4999859  0.49941683
 0.49887294 0.49832523 0.4977154  0.49695238 0.4959152  0.4945893
 0.49309272 0.49163532 0.49033636 0.48926073 0.4883682  0.48753977
 0.48663732 0.48557642 0.48435602 0.4830801  0.48187578 0.48081717
 0.47986358 0.47898403 0.47816214 0.47738218 0.4766587  0.47589684
 0.47502103 0.47403422 0.4729877  0.4719216  0.4709209  0.46998677
 0.46913058 0.46825752 0.46735328 0.4663976  0.46543315 0.46452197
 0.4637055  0.4629072  0.46202135 0.46099833 0.45991874 0.4589078
 0.4580923  0.45746833 0.45693606 0.45636517 0.45570293 0.45506003
 0.45455542 0.45423147 0.4539765  0.45360118 0.45294195 0.45189565
 0.45057726 0.44922242 0.44819328 0.4475674  0.44721162 0.44687402
 0.4464169  0.44581297 0.4450644  0.44428608 0.44358408 0.44300753
 0.4425462  0.442182   0.4419119  0.44178593 0.44170496 0.44156188
 0.44123667 0.4407276  0.44011223 0.43951273 0.43902758 0.4387045
 0.43849474 0.43829584 0.43795112 0.43746677 0.43685943 0.4361618
 0.43535239 0.43442377 0.43341276 0.43244722 0.43170688 0.43132254
 0.4311571  0.43096942 0.43053713 0.42973182 0.42862764 0.4274867
 0.42664343 0.42614704 0.4258141  0.42530093 0.42438537 0.42302775
 0.4214558  0.42006552 0.41889656 0.41788092 0.416915   0.41586474
 0.4147234  0.4135384  0.4123461  0.41112316 0.4098113  0.40839496
 0.4069634  0.40568623 0.40472087 0.40410906 0.4037569  0.40343517
 0.40280223 0.40177453 0.40050006 0.3992565  0.39825296 0.39738798
 0.39645717 0.3952611  0.3938986  0.39254126 0.39139873 0.39059305
 0.39004984 0.38957816 0.3890329  0.38824347 0.38725254 0.3862369
 0.3852529  0.38433987 0.38342506 0.3824722  0.3815214  0.38070118
 0.3800787  0.37969664 0.37946    0.37922442 0.37892368 0.37853357
 0.378117   0.37768337 0.37725803 0.37691462 0.37650597 0.37603518
 0.37552196 0.37505373 0.37467536 0.37440887 0.37431872 0.3743285
 0.37432796 0.3742695  0.3740589  0.37373656 0.3733439  0.3730119
 0.37279755 0.37270907 0.3726641  0.37258196 0.3723345  0.37192965
 0.3713962  0.37079445 0.37029305 0.36999053 0.36985072 0.36972442
 0.36952662 0.3691735  0.36863467 0.3680377  0.36745617 0.3669262
 0.36649737 0.3660786  0.36565447 0.36535838 0.36520144 0.36520302
 0.36520758 0.36505336 0.36460194 0.3638057  0.36277148 0.36160204
 0.3605406  0.3597853  0.3592085  0.3586081  0.35792118 0.3571947
 0.35664526 0.3562816  0.35594454 0.35541782 0.35458648 0.3533916
 0.35206568 0.3507921  0.34985057 0.34920657 0.34883106 0.3485706
 0.3483424  0.3480106  0.34755528 0.34698147 0.34631157 0.34571278
 0.34523693 0.34507385 0.34523803 0.34559348 0.34593022 0.3460412
 0.34577236 0.34526297 0.3448011  0.3446192  0.34473908 0.34499624
 0.3450896  0.34488574 0.34444952 0.34400085 0.34392282 0.3443017
 0.34499815 0.34554338 0.34559086 0.34509295 0.3442282  0.34346354
 0.34307048 0.34299797 0.34309345 0.34315914 0.34305015 0.34279156
 0.3425928  0.34259662 0.34277588 0.34294376 0.34292302 0.34269187
 0.34232643 0.3419972  0.3417446  0.34147656 0.3410844  0.34051302
 0.33988437 0.33945954 0.33937496 0.33969474 0.34011745 0.34041357
 0.3403998  0.34006324 0.33953708 0.3390009  0.338536   0.33818716
 0.3378308  0.33749783 0.33724588 0.3372081  0.3374555  0.33784693
 0.33823693 0.3385423  0.3387611  0.33898687 0.33931595 0.33979052
 0.3402718  0.3404691  0.34013242 0.33923134 0.33813986 0.33718926
 0.33669254 0.33657208 0.3364844  0.33603844 0.33508334 0.33391574
 0.33289722 0.33230752 0.33209392 0.33193725 0.3315162  0.33068722
 0.3296332  0.32873765 0.3281837  0.3279628  0.32789788 0.32761824
 0.32698005 0.3260786  0.32522804 0.32471794 0.32450587 0.32445624
 0.32432535 0.3240694  0.32374585 0.3234651  0.32336485 0.3233835
 0.32345092 0.32346737 0.32335234 0.3231659  0.3229734  0.3228469
 0.3227926  0.32271796 0.32254755 0.32235882 0.32231402 0.32252777
 0.32300225 0.32355    0.32400393 0.32423115 0.32425597 0.32417858
 0.32408404 0.3239193  0.32368618 0.32339835 0.32312393 0.32291147
 0.3227731  0.3228024  0.32284853 0.32270923 0.32232013 0.3217794
 0.3211614  0.3206933  0.3204802  0.32048142 0.32059574 0.32061836
 0.3204624  0.32011387 0.31967008 0.31914884 0.3186134  0.31812534
 0.3177004  0.31738445 0.31720892 0.31719255 0.3173459  0.31746167
 0.3174208  0.31715113 0.31670746 0.31624568 0.3159159  0.31577498
 0.3157426  0.3156173  0.31521097 0.31448138 0.31360525 0.3127569
 0.31210068 0.3115937  0.31115752 0.31059673 0.3098512  0.30893815
 0.3079425  0.30703077 0.30612788 0.30513132 0.3039651  0.30262822
 0.30121455 0.29995447 0.29897448 0.29835317 0.29793715 0.29752043
 0.29696348 0.29630858 0.29559585 0.29483166 0.29407206 0.29331118
 0.2925719  0.2918     0.29101047 0.2902943  0.28961155 0.28901228
 0.2883732  0.2876726  0.28699145 0.28639477 0.28598964 0.28569546
 0.28541183 0.2850497  0.28456974 0.28404316 0.28359044 0.28326675
 0.28302595 0.28276804 0.28257906 0.28239316 0.28219655 0.28202322
 0.2818593  0.2816839  0.2814005  0.28098986 0.2805286  0.2800931
 0.2797355  0.27951968 0.27942243 0.27939674 0.2793937  0.27932993
 0.2791519  0.27889892 0.2787291  0.27865434 0.2786431  0.27862424
 0.27852172 0.2783237  0.27803177 0.27776134 0.2775645  0.27743882
 0.27737433 0.27736008 0.27729696 0.277095   0.27667254 0.2761264
 0.27552012 0.2749991  0.2746713  0.2745821  0.27457353 0.27462626
 0.27463338 0.27457854 0.27439085 0.27410954 0.27389526 0.2738922
 0.2741417  0.27447256 0.27466303 0.27450186 0.27397525 0.27327272
 0.27266705 0.2723472  0.27222735 0.27208382 0.27163935 0.27063462
 0.26911733 0.26753452 0.2662129  0.26529476 0.26466307 0.2641302
 0.26353133 0.26278293 0.26189756 0.26090604 0.2598378  0.2587759
 0.25774774 0.25679544 0.25608143 0.2556375  0.25540978 0.25529772
 0.25510946 0.2546818  0.25407496 0.253344   0.25270608 0.25230113
 0.25210923 0.2520619  0.25193638 0.25159234 0.25113183 0.25069338
 0.25032824 0.250128   0.2501402  0.2501712  0.25012878 0.24993087
 0.24967101 0.24930698 0.24901916 0.24898556 0.24918313 0.24947357
 0.24975097 0.25005612 0.25039622 0.2508409  0.25139332 0.25196794
 0.2523426  0.25236082 0.25208586 0.25185758 0.2519381  0.2523899
 0.25299466 0.25347838 0.25364316 0.253262   0.25270435 0.25221252
 0.25199598 0.2520824  0.2522268  0.25232893 0.25212285 0.25168446
 0.2511622  0.25085926 0.25077188 0.2507508  0.25087422 0.25089678
 0.25085232 0.2507007  0.2504961  0.2502432  0.25003073 0.24974772
 0.24959378 0.24954942 0.24954782 0.2497069  0.2499753  0.2501453
 0.25007185 0.24990056 0.24962316 0.2493811  0.24941781 0.24975179
 0.25025445 0.25081033 0.25119114 0.25110844 0.25044706 0.24930501
 0.24785288 0.24629262 0.24477704 0.2433735  0.24220058 0.2412295
 0.24043043 0.23977908 0.23948802 0.23935686 0.23927188 0.23905095
 0.23864484 0.23811597 0.23755397 0.23701337 0.23660281 0.23612256
 0.23548406 0.23460323 0.23348083 0.23250453 0.23183176 0.23155703
 0.23165482 0.23180547 0.23172468 0.23102982 0.22977558 0.22827466
 0.22713675 0.2265038  0.22640494 0.22650503 0.22634655 0.22577763
 0.22497088 0.22453342 0.22477865 0.2256026  0.22606763 0.22569005
 0.22455682 0.22363557 0.22413315 0.22587357 0.22611986 0.22027911]
