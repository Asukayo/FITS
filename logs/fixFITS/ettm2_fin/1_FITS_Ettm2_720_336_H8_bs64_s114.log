Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=74, out_features=108, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7160832.0
params:  8100.0
Trainable parameters:  8100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4771334
	speed: 0.1522s/iter; left time: 1971.7640s
	iters: 200, epoch: 1 | loss: 0.3866547
	speed: 0.1403s/iter; left time: 1803.0552s
Epoch: 1 cost time: 37.93935966491699
Epoch: 1, Steps: 261 | Train Loss: 0.4603358 Vali Loss: 0.2115189 Test Loss: 0.2894266
Validation loss decreased (inf --> 0.211519).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3096789
	speed: 0.6296s/iter; left time: 7989.0360s
	iters: 200, epoch: 2 | loss: 0.3655562
	speed: 0.1479s/iter; left time: 1861.5672s
Epoch: 2 cost time: 38.770352363586426
Epoch: 2, Steps: 261 | Train Loss: 0.4042436 Vali Loss: 0.2031191 Test Loss: 0.2795654
Validation loss decreased (0.211519 --> 0.203119).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3615538
	speed: 0.6496s/iter; left time: 8074.2278s
	iters: 200, epoch: 3 | loss: 0.4272136
	speed: 0.1349s/iter; left time: 1663.4840s
Epoch: 3 cost time: 36.39877438545227
Epoch: 3, Steps: 261 | Train Loss: 0.3938360 Vali Loss: 0.1998658 Test Loss: 0.2755734
Validation loss decreased (0.203119 --> 0.199866).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5286094
	speed: 0.6124s/iter; left time: 7451.5093s
	iters: 200, epoch: 4 | loss: 0.2760998
	speed: 0.1340s/iter; left time: 1617.0204s
Epoch: 4 cost time: 35.6895546913147
Epoch: 4, Steps: 261 | Train Loss: 0.3890636 Vali Loss: 0.1980179 Test Loss: 0.2735780
Validation loss decreased (0.199866 --> 0.198018).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3876929
	speed: 0.6317s/iter; left time: 7521.8485s
	iters: 200, epoch: 5 | loss: 0.3222198
	speed: 0.1257s/iter; left time: 1484.6206s
Epoch: 5 cost time: 34.655884742736816
Epoch: 5, Steps: 261 | Train Loss: 0.3852567 Vali Loss: 0.1972289 Test Loss: 0.2722662
Validation loss decreased (0.198018 --> 0.197229).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4048499
	speed: 0.6273s/iter; left time: 7305.2186s
	iters: 200, epoch: 6 | loss: 0.4503385
	speed: 0.1404s/iter; left time: 1621.0822s
Epoch: 6 cost time: 37.59706687927246
Epoch: 6, Steps: 261 | Train Loss: 0.3839792 Vali Loss: 0.1966000 Test Loss: 0.2718648
Validation loss decreased (0.197229 --> 0.196600).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3909602
	speed: 0.6523s/iter; left time: 7426.9055s
	iters: 200, epoch: 7 | loss: 0.2959635
	speed: 0.1392s/iter; left time: 1570.5903s
Epoch: 7 cost time: 37.496254682540894
Epoch: 7, Steps: 261 | Train Loss: 0.3822785 Vali Loss: 0.1955049 Test Loss: 0.2707738
Validation loss decreased (0.196600 --> 0.195505).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4088932
	speed: 0.6210s/iter; left time: 6908.0750s
	iters: 200, epoch: 8 | loss: 0.4008376
	speed: 0.1300s/iter; left time: 1433.6350s
Epoch: 8 cost time: 35.2331702709198
Epoch: 8, Steps: 261 | Train Loss: 0.3815339 Vali Loss: 0.1954811 Test Loss: 0.2704842
Validation loss decreased (0.195505 --> 0.195481).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3609035
	speed: 0.6051s/iter; left time: 6573.5856s
	iters: 200, epoch: 9 | loss: 0.3432474
	speed: 0.1218s/iter; left time: 1311.2471s
Epoch: 9 cost time: 34.425782203674316
Epoch: 9, Steps: 261 | Train Loss: 0.3801514 Vali Loss: 0.1953963 Test Loss: 0.2702852
Validation loss decreased (0.195481 --> 0.195396).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3228374
	speed: 0.5951s/iter; left time: 6309.2551s
	iters: 200, epoch: 10 | loss: 0.4590720
	speed: 0.1294s/iter; left time: 1359.1153s
Epoch: 10 cost time: 35.008673667907715
Epoch: 10, Steps: 261 | Train Loss: 0.3803433 Vali Loss: 0.1948799 Test Loss: 0.2698501
Validation loss decreased (0.195396 --> 0.194880).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4639940
	speed: 0.6175s/iter; left time: 6385.7882s
	iters: 200, epoch: 11 | loss: 0.3411875
	speed: 0.1471s/iter; left time: 1506.3254s
Epoch: 11 cost time: 38.798015117645264
Epoch: 11, Steps: 261 | Train Loss: 0.3798217 Vali Loss: 0.1947349 Test Loss: 0.2697097
Validation loss decreased (0.194880 --> 0.194735).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4263204
	speed: 0.6612s/iter; left time: 6664.9510s
	iters: 200, epoch: 12 | loss: 0.3383080
	speed: 0.1458s/iter; left time: 1455.4027s
Epoch: 12 cost time: 38.67533040046692
Epoch: 12, Steps: 261 | Train Loss: 0.3789649 Vali Loss: 0.1947234 Test Loss: 0.2695405
Validation loss decreased (0.194735 --> 0.194723).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3831068
	speed: 0.6398s/iter; left time: 6282.5039s
	iters: 200, epoch: 13 | loss: 0.3537156
	speed: 0.1342s/iter; left time: 1304.1565s
Epoch: 13 cost time: 36.16228485107422
Epoch: 13, Steps: 261 | Train Loss: 0.3782099 Vali Loss: 0.1945556 Test Loss: 0.2691242
Validation loss decreased (0.194723 --> 0.194556).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2859146
	speed: 0.6490s/iter; left time: 6202.7003s
	iters: 200, epoch: 14 | loss: 0.2577924
	speed: 0.1504s/iter; left time: 1422.0872s
Epoch: 14 cost time: 39.10677719116211
Epoch: 14, Steps: 261 | Train Loss: 0.3780229 Vali Loss: 0.1944946 Test Loss: 0.2692835
Validation loss decreased (0.194556 --> 0.194495).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4697744
	speed: 0.6479s/iter; left time: 6023.1415s
	iters: 200, epoch: 15 | loss: 0.5092019
	speed: 0.1323s/iter; left time: 1216.3981s
Epoch: 15 cost time: 36.6551947593689
Epoch: 15, Steps: 261 | Train Loss: 0.3778273 Vali Loss: 0.1948011 Test Loss: 0.2692345
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2973556
	speed: 0.6252s/iter; left time: 5649.4041s
	iters: 200, epoch: 16 | loss: 0.4553224
	speed: 0.1360s/iter; left time: 1215.2498s
Epoch: 16 cost time: 35.83718395233154
Epoch: 16, Steps: 261 | Train Loss: 0.3780177 Vali Loss: 0.1944009 Test Loss: 0.2688522
Validation loss decreased (0.194495 --> 0.194401).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3508568
	speed: 0.6053s/iter; left time: 5311.9383s
	iters: 200, epoch: 17 | loss: 0.3295276
	speed: 0.1368s/iter; left time: 1186.3773s
Epoch: 17 cost time: 36.57653856277466
Epoch: 17, Steps: 261 | Train Loss: 0.3780070 Vali Loss: 0.1942996 Test Loss: 0.2688831
Validation loss decreased (0.194401 --> 0.194300).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2861784
	speed: 0.6040s/iter; left time: 5142.7791s
	iters: 200, epoch: 18 | loss: 0.4552150
	speed: 0.1307s/iter; left time: 1099.4500s
Epoch: 18 cost time: 35.176663398742676
Epoch: 18, Steps: 261 | Train Loss: 0.3771815 Vali Loss: 0.1946768 Test Loss: 0.2693044
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3314047
	speed: 0.5892s/iter; left time: 4862.4364s
	iters: 200, epoch: 19 | loss: 0.2451101
	speed: 0.1343s/iter; left time: 1095.3163s
Epoch: 19 cost time: 36.45914626121521
Epoch: 19, Steps: 261 | Train Loss: 0.3775610 Vali Loss: 0.1943732 Test Loss: 0.2689246
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4893793
	speed: 0.6363s/iter; left time: 5085.5671s
	iters: 200, epoch: 20 | loss: 0.4834327
	speed: 0.1449s/iter; left time: 1143.6241s
Epoch: 20 cost time: 38.41748666763306
Epoch: 20, Steps: 261 | Train Loss: 0.3775618 Vali Loss: 0.1943933 Test Loss: 0.2689248
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.26989853382110596, mae:0.32719510793685913, rse:0.41962459683418274, corr:[0.5447769  0.5530794  0.555615   0.5541857  0.5519948  0.550686
 0.550597   0.5514249  0.5525803  0.55340475 0.5534949  0.5529263
 0.5520913  0.5513406  0.55092216 0.55081254 0.5508809  0.55091804
 0.5507048  0.5501478  0.5493481  0.5484695  0.5477051  0.5471858
 0.5468981  0.5467578  0.54663306 0.5463916  0.54595757 0.5453464
 0.54462147 0.54392606 0.54331684 0.54278237 0.5423044  0.5418763
 0.5414246  0.5409182  0.5403265  0.5396625  0.5389712  0.5382837
 0.53765064 0.53709495 0.5366143  0.53616583 0.53570694 0.5351944
 0.5345591  0.53378034 0.53290474 0.5320399  0.5312433  0.5305686
 0.5300344  0.5296537  0.52935535 0.52907306 0.52876586 0.5284267
 0.52806824 0.52775705 0.52752894 0.52737445 0.5272701  0.5271914
 0.52706933 0.5268891  0.52664405 0.52633363 0.52596277 0.5255709
 0.5251847  0.5248215  0.52445006 0.52405703 0.5236028  0.5230612
 0.5224207  0.5217279  0.5209766  0.5202442  0.51957667 0.5190169
 0.51858026 0.51823914 0.5179199  0.5175785  0.5171703  0.51666373
 0.5160552  0.51533484 0.5144657  0.5134475  0.51229966 0.51103425
 0.5097293  0.50850564 0.5073693  0.5063049  0.50529176 0.5042972
 0.50329256 0.50222546 0.5011024  0.4999141  0.49873412 0.49764487
 0.4966761  0.49581727 0.49505192 0.49433205 0.49365118 0.4929224
 0.49206316 0.49107975 0.49003482 0.48896784 0.48799142 0.48712516
 0.48644084 0.48587853 0.48540273 0.48489416 0.48423225 0.48335752
 0.48230875 0.4811509  0.47997874 0.47890848 0.47803092 0.4773529
 0.47684547 0.47642276 0.47599632 0.47546336 0.47476444 0.47394004
 0.473044   0.47219265 0.47144243 0.47082934 0.470336   0.46982887
 0.4692147  0.46837676 0.4674004  0.4663462  0.46533534 0.46443415
 0.4637276  0.46324584 0.46288028 0.46253303 0.46214262 0.4617362
 0.4613119  0.4608795  0.46042368 0.45999083 0.4595363  0.45903295
 0.4584489  0.45781124 0.45716032 0.4565704  0.45605907 0.45566368
 0.45544305 0.45543072 0.45549273 0.45552403 0.45542994 0.45515442
 0.45469695 0.45410421 0.45344082 0.4527847  0.4521593  0.45166007
 0.4512384  0.4508308  0.4504349  0.4500088  0.4495042  0.44890675
 0.4482614  0.44761187 0.44694722 0.44620723 0.44533244 0.44426548
 0.44302264 0.4417176  0.4402968  0.43873084 0.4371157  0.43551838
 0.43405518 0.43277073 0.43167236 0.4306915  0.4297396  0.42873877
 0.42766058 0.426504   0.42526218 0.42401254 0.4229192  0.42211702
 0.42149028 0.42093486 0.420361   0.41970974 0.418993   0.4181335
 0.41711283 0.41590124 0.41459444 0.41325107 0.41188225 0.41056404
 0.4093591  0.40832886 0.4075372  0.40686312 0.40625346 0.40570712
 0.40511134 0.40451697 0.40386003 0.4031634  0.40239406 0.4015683
 0.40069425 0.3998817  0.39917785 0.3986639  0.39837706 0.39824924
 0.39820093 0.3980479  0.39774936 0.39740863 0.39697424 0.39653388
 0.39615488 0.39587918 0.39564487 0.39538425 0.39517954 0.3950268
 0.39491484 0.39489815 0.39487368 0.39483282 0.3947242  0.3945624
 0.39436552 0.39412388 0.3938501  0.3935892  0.3933555  0.3932201
 0.39316437 0.393109   0.3930535  0.39295626 0.39273536 0.39235094
 0.39183733 0.3913324  0.3908692  0.39053303 0.39029425 0.39010128
 0.38998112 0.38978294 0.38944328 0.3890269  0.38854632 0.3880648
 0.38756695 0.38705266 0.38650227 0.3858781  0.38516656 0.38429347
 0.38336125 0.38261563 0.38204893 0.3815569  0.38106963 0.3804812
 0.3798843  0.37929744 0.37873614 0.3782462  0.3779541  0.37769985
 0.37750697 0.37722388 0.37683478 0.3761991  0.37543467 0.37473452
 0.37435645 0.37428087 0.37451133 0.37492085 0.37525317 0.37546757
 0.3752888  0.37477592 0.37401736 0.37321275 0.37269497 0.37273583
 0.37324718 0.3740412  0.3747721  0.37505952 0.374627   0.3736402
 0.37254095 0.37203082 0.37268087 0.3743387  0.37650767 0.37831938
 0.37930682 0.37890586 0.37703058 0.37465122 0.37347534 0.3741686 ]
