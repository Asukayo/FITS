Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_96_FITS_ETTm2_ftM_sl720_ll48_pl96_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 11425
test 11425
Model(
  (freq_upsampler): Linear(in_features=106, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  45588480.0
params:  12840.0
Trainable parameters:  12840
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.835027933120728
Epoch: 1, Steps: 65 | Train Loss: 0.3940126 Vali Loss: 0.1603752 Test Loss: 0.2133058
Validation loss decreased (inf --> 0.160375).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.509474515914917
Epoch: 2, Steps: 65 | Train Loss: 0.2845433 Vali Loss: 0.1401196 Test Loss: 0.1892971
Validation loss decreased (0.160375 --> 0.140120).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.304154396057129
Epoch: 3, Steps: 65 | Train Loss: 0.2576862 Vali Loss: 0.1318943 Test Loss: 0.1802747
Validation loss decreased (0.140120 --> 0.131894).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.363356351852417
Epoch: 4, Steps: 65 | Train Loss: 0.2449906 Vali Loss: 0.1273960 Test Loss: 0.1753526
Validation loss decreased (0.131894 --> 0.127396).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.985666275024414
Epoch: 5, Steps: 65 | Train Loss: 0.2365740 Vali Loss: 0.1248439 Test Loss: 0.1723819
Validation loss decreased (0.127396 --> 0.124844).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.360961198806763
Epoch: 6, Steps: 65 | Train Loss: 0.2315609 Vali Loss: 0.1225576 Test Loss: 0.1703078
Validation loss decreased (0.124844 --> 0.122558).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.358603715896606
Epoch: 7, Steps: 65 | Train Loss: 0.2276354 Vali Loss: 0.1214975 Test Loss: 0.1688586
Validation loss decreased (0.122558 --> 0.121497).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.377753973007202
Epoch: 8, Steps: 65 | Train Loss: 0.2247207 Vali Loss: 0.1202202 Test Loss: 0.1676176
Validation loss decreased (0.121497 --> 0.120220).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.147913455963135
Epoch: 9, Steps: 65 | Train Loss: 0.2223552 Vali Loss: 0.1194481 Test Loss: 0.1668094
Validation loss decreased (0.120220 --> 0.119448).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.262130737304688
Epoch: 10, Steps: 65 | Train Loss: 0.2202884 Vali Loss: 0.1188463 Test Loss: 0.1660598
Validation loss decreased (0.119448 --> 0.118846).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.70167851448059
Epoch: 11, Steps: 65 | Train Loss: 0.2180286 Vali Loss: 0.1183013 Test Loss: 0.1655062
Validation loss decreased (0.118846 --> 0.118301).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.348809480667114
Epoch: 12, Steps: 65 | Train Loss: 0.2174374 Vali Loss: 0.1179602 Test Loss: 0.1649060
Validation loss decreased (0.118301 --> 0.117960).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.076874494552612
Epoch: 13, Steps: 65 | Train Loss: 0.2169063 Vali Loss: 0.1177585 Test Loss: 0.1646189
Validation loss decreased (0.117960 --> 0.117759).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.931026697158813
Epoch: 14, Steps: 65 | Train Loss: 0.2153344 Vali Loss: 0.1170305 Test Loss: 0.1641534
Validation loss decreased (0.117759 --> 0.117030).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.030814409255981
Epoch: 15, Steps: 65 | Train Loss: 0.2151836 Vali Loss: 0.1170577 Test Loss: 0.1639216
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.591683864593506
Epoch: 16, Steps: 65 | Train Loss: 0.2140598 Vali Loss: 0.1167997 Test Loss: 0.1636750
Validation loss decreased (0.117030 --> 0.116800).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.73899221420288
Epoch: 17, Steps: 65 | Train Loss: 0.2141183 Vali Loss: 0.1167216 Test Loss: 0.1635349
Validation loss decreased (0.116800 --> 0.116722).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.747968435287476
Epoch: 18, Steps: 65 | Train Loss: 0.2139276 Vali Loss: 0.1163856 Test Loss: 0.1633329
Validation loss decreased (0.116722 --> 0.116386).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.591304779052734
Epoch: 19, Steps: 65 | Train Loss: 0.2134531 Vali Loss: 0.1165086 Test Loss: 0.1631841
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.426778793334961
Epoch: 20, Steps: 65 | Train Loss: 0.2130562 Vali Loss: 0.1161753 Test Loss: 0.1629635
Validation loss decreased (0.116386 --> 0.116175).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 10.123220443725586
Epoch: 21, Steps: 65 | Train Loss: 0.2124469 Vali Loss: 0.1162299 Test Loss: 0.1628966
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 11.766679525375366
Epoch: 22, Steps: 65 | Train Loss: 0.2120208 Vali Loss: 0.1160321 Test Loss: 0.1627813
Validation loss decreased (0.116175 --> 0.116032).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.75517225265503
Epoch: 23, Steps: 65 | Train Loss: 0.2123289 Vali Loss: 0.1159842 Test Loss: 0.1626688
Validation loss decreased (0.116032 --> 0.115984).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 10.438676595687866
Epoch: 24, Steps: 65 | Train Loss: 0.2115162 Vali Loss: 0.1158462 Test Loss: 0.1626788
Validation loss decreased (0.115984 --> 0.115846).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 9.9725661277771
Epoch: 25, Steps: 65 | Train Loss: 0.2114772 Vali Loss: 0.1160834 Test Loss: 0.1624961
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.927648305892944
Epoch: 26, Steps: 65 | Train Loss: 0.2110121 Vali Loss: 0.1158993 Test Loss: 0.1624894
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.16352653503418
Epoch: 27, Steps: 65 | Train Loss: 0.2109470 Vali Loss: 0.1157006 Test Loss: 0.1624625
Validation loss decreased (0.115846 --> 0.115701).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.07179570198059
Epoch: 28, Steps: 65 | Train Loss: 0.2104988 Vali Loss: 0.1155898 Test Loss: 0.1622772
Validation loss decreased (0.115701 --> 0.115590).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 10.854281663894653
Epoch: 29, Steps: 65 | Train Loss: 0.2110582 Vali Loss: 0.1158265 Test Loss: 0.1622916
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 10.662719488143921
Epoch: 30, Steps: 65 | Train Loss: 0.2103648 Vali Loss: 0.1154680 Test Loss: 0.1621991
Validation loss decreased (0.115590 --> 0.115468).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 10.72320818901062
Epoch: 31, Steps: 65 | Train Loss: 0.2103010 Vali Loss: 0.1155234 Test Loss: 0.1621622
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 10.526869058609009
Epoch: 32, Steps: 65 | Train Loss: 0.2100946 Vali Loss: 0.1155412 Test Loss: 0.1621575
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 10.649815559387207
Epoch: 33, Steps: 65 | Train Loss: 0.2099386 Vali Loss: 0.1154776 Test Loss: 0.1621002
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_96_FITS_ETTm2_ftM_sl720_ll48_pl96_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
mse:0.16448524594306946, mae:0.2557220160961151, rse:0.3288210928440094, corr:[0.56010264 0.5691043  0.5690767  0.5665528  0.5658572  0.5669647
 0.56833637 0.56849515 0.56749016 0.5664593  0.5661083  0.56636465
 0.5667336  0.5666549  0.566059   0.565277   0.5646543  0.56421953
 0.5638481  0.56333447 0.5626226  0.5618299  0.5611427  0.5606257
 0.5602306  0.55988574 0.5594593  0.55887413 0.55817163 0.5574465
 0.55674744 0.55621034 0.5557602  0.55523705 0.5546099  0.55389804
 0.5530837  0.55225873 0.5515063  0.55084455 0.5502413  0.549645
 0.54898924 0.5482694  0.5475818  0.5469134  0.5462152  0.545461
 0.54454565 0.54349244 0.5425043  0.5417753  0.54117405 0.5404803
 0.5395764  0.5385719  0.5376564  0.53706026 0.5367524  0.5365091
 0.5361213  0.53564423 0.5352161  0.53500706 0.53500277 0.53501534
 0.5347053  0.53406906 0.53334695 0.5329121  0.5329485  0.5331429
 0.5329997  0.5323365  0.53125566 0.5302951  0.5299296  0.5300233
 0.5298905  0.52915865 0.5277009  0.526105   0.52516884 0.5250683
 0.5249718  0.5241101  0.52225065 0.52016515 0.5192066  0.51970243
 0.520072   0.5188122  0.5158243  0.51342654 0.5154738  0.5178289 ]
