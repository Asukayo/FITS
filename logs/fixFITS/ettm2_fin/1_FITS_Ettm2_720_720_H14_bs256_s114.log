Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  106688512.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.594555616378784
Epoch: 1, Steps: 64 | Train Loss: 0.6541152 Vali Loss: 0.3147770 Test Loss: 0.4355678
Validation loss decreased (inf --> 0.314777).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.25044870376587
Epoch: 2, Steps: 64 | Train Loss: 0.5559934 Vali Loss: 0.2894286 Test Loss: 0.4061669
Validation loss decreased (0.314777 --> 0.289429).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.831761598587036
Epoch: 3, Steps: 64 | Train Loss: 0.5344204 Vali Loss: 0.2806076 Test Loss: 0.3964057
Validation loss decreased (0.289429 --> 0.280608).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 12.100900411605835
Epoch: 4, Steps: 64 | Train Loss: 0.5244179 Vali Loss: 0.2756264 Test Loss: 0.3910869
Validation loss decreased (0.280608 --> 0.275626).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.089908361434937
Epoch: 5, Steps: 64 | Train Loss: 0.5192365 Vali Loss: 0.2728607 Test Loss: 0.3874095
Validation loss decreased (0.275626 --> 0.272861).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.53122878074646
Epoch: 6, Steps: 64 | Train Loss: 0.5163215 Vali Loss: 0.2709047 Test Loss: 0.3848978
Validation loss decreased (0.272861 --> 0.270905).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.77305555343628
Epoch: 7, Steps: 64 | Train Loss: 0.5118763 Vali Loss: 0.2691003 Test Loss: 0.3827810
Validation loss decreased (0.270905 --> 0.269100).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.979267597198486
Epoch: 8, Steps: 64 | Train Loss: 0.5095693 Vali Loss: 0.2685381 Test Loss: 0.3811680
Validation loss decreased (0.269100 --> 0.268538).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.363981485366821
Epoch: 9, Steps: 64 | Train Loss: 0.5084484 Vali Loss: 0.2672951 Test Loss: 0.3798468
Validation loss decreased (0.268538 --> 0.267295).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.299416303634644
Epoch: 10, Steps: 64 | Train Loss: 0.5069777 Vali Loss: 0.2664771 Test Loss: 0.3787950
Validation loss decreased (0.267295 --> 0.266477).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.213284730911255
Epoch: 11, Steps: 64 | Train Loss: 0.5060483 Vali Loss: 0.2661107 Test Loss: 0.3777762
Validation loss decreased (0.266477 --> 0.266111).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.50179672241211
Epoch: 12, Steps: 64 | Train Loss: 0.5048196 Vali Loss: 0.2654938 Test Loss: 0.3770437
Validation loss decreased (0.266111 --> 0.265494).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.796164989471436
Epoch: 13, Steps: 64 | Train Loss: 0.5039319 Vali Loss: 0.2651502 Test Loss: 0.3763927
Validation loss decreased (0.265494 --> 0.265150).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.468013048171997
Epoch: 14, Steps: 64 | Train Loss: 0.5026892 Vali Loss: 0.2648035 Test Loss: 0.3758143
Validation loss decreased (0.265150 --> 0.264803).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.516030311584473
Epoch: 15, Steps: 64 | Train Loss: 0.5032533 Vali Loss: 0.2644368 Test Loss: 0.3752931
Validation loss decreased (0.264803 --> 0.264437).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.06107759475708
Epoch: 16, Steps: 64 | Train Loss: 0.5024216 Vali Loss: 0.2644011 Test Loss: 0.3748176
Validation loss decreased (0.264437 --> 0.264401).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.909364461898804
Epoch: 17, Steps: 64 | Train Loss: 0.5009118 Vali Loss: 0.2640154 Test Loss: 0.3744241
Validation loss decreased (0.264401 --> 0.264015).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.684209108352661
Epoch: 18, Steps: 64 | Train Loss: 0.5016493 Vali Loss: 0.2637937 Test Loss: 0.3740963
Validation loss decreased (0.264015 --> 0.263794).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.574437141418457
Epoch: 19, Steps: 64 | Train Loss: 0.4997683 Vali Loss: 0.2634557 Test Loss: 0.3738586
Validation loss decreased (0.263794 --> 0.263456).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 11.738661050796509
Epoch: 20, Steps: 64 | Train Loss: 0.5005257 Vali Loss: 0.2635705 Test Loss: 0.3734967
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.726553201675415
Epoch: 21, Steps: 64 | Train Loss: 0.4994391 Vali Loss: 0.2633023 Test Loss: 0.3732227
Validation loss decreased (0.263456 --> 0.263302).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 11.660377025604248
Epoch: 22, Steps: 64 | Train Loss: 0.4997668 Vali Loss: 0.2633386 Test Loss: 0.3729803
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 11.773480653762817
Epoch: 23, Steps: 64 | Train Loss: 0.4995261 Vali Loss: 0.2631362 Test Loss: 0.3728481
Validation loss decreased (0.263302 --> 0.263136).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 11.302997589111328
Epoch: 24, Steps: 64 | Train Loss: 0.4996015 Vali Loss: 0.2627782 Test Loss: 0.3726206
Validation loss decreased (0.263136 --> 0.262778).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 11.207958936691284
Epoch: 25, Steps: 64 | Train Loss: 0.4992526 Vali Loss: 0.2627673 Test Loss: 0.3724466
Validation loss decreased (0.262778 --> 0.262767).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.824642896652222
Epoch: 26, Steps: 64 | Train Loss: 0.4994917 Vali Loss: 0.2627470 Test Loss: 0.3723145
Validation loss decreased (0.262767 --> 0.262747).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 11.369879245758057
Epoch: 27, Steps: 64 | Train Loss: 0.4998858 Vali Loss: 0.2624753 Test Loss: 0.3721910
Validation loss decreased (0.262747 --> 0.262475).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 11.675643920898438
Epoch: 28, Steps: 64 | Train Loss: 0.4985445 Vali Loss: 0.2626075 Test Loss: 0.3720329
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 11.925644159317017
Epoch: 29, Steps: 64 | Train Loss: 0.4980980 Vali Loss: 0.2625058 Test Loss: 0.3719289
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 11.772638320922852
Epoch: 30, Steps: 64 | Train Loss: 0.4971555 Vali Loss: 0.2626678 Test Loss: 0.3718015
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.35196083784103394, mae:0.37977999448776245, rse:0.47686073184013367, corr:[0.5261983  0.53992236 0.5372976  0.5339568  0.5351995  0.5386331
 0.5402385  0.5387977  0.53715104 0.53726697 0.53867286 0.5397359
 0.53927195 0.5379934  0.5373104  0.5375722  0.5380758  0.5378875
 0.5368419  0.5357165  0.53520465 0.53522485 0.535232   0.53476995
 0.53393644 0.5332442  0.53292936 0.532765   0.53240013 0.531676
 0.53084016 0.530231   0.5298556  0.5295004  0.5289723  0.5282105
 0.5273804  0.52666897 0.5260771  0.52555084 0.52504694 0.5245334
 0.5240274  0.5234801  0.5227941  0.5220029  0.521173   0.52034366
 0.51957834 0.5188575  0.5182046  0.5176306  0.51708657 0.51655006
 0.5160083  0.5154223  0.5148345  0.51435685 0.514004   0.51369184
 0.5134051  0.5130634  0.5126824  0.51236194 0.5121034  0.5118309
 0.5114209  0.5109037  0.5103845  0.50997293 0.509638   0.5092709
 0.50881183 0.5082524  0.5077197  0.5072876  0.50687784 0.5063317
 0.5056534  0.5048543  0.5041157  0.5035272  0.5030076  0.50239897
 0.5016587  0.5008784  0.50023896 0.49981913 0.49943197 0.49890804
 0.49817637 0.4972818  0.49643087 0.49563298 0.49465963 0.49337366
 0.49184695 0.4903354  0.48909342 0.48814842 0.48725635 0.48620185
 0.48495758 0.48373705 0.48274383 0.48194572 0.48110595 0.48005167
 0.47884983 0.47776732 0.4769827  0.4763397  0.47552666 0.47435892
 0.4730309  0.47194025 0.4712702  0.47079608 0.47013888 0.46914446
 0.46798626 0.46695575 0.46620014 0.46550223 0.46458355 0.46338755
 0.46215838 0.46119636 0.46052784 0.45986563 0.45894623 0.45779017
 0.4567286  0.45606253 0.45575783 0.4554064  0.45463553 0.4534522
 0.45220396 0.451288   0.45075157 0.4502561  0.44944254 0.44825682
 0.44705653 0.4462841  0.44601747 0.44582972 0.445306   0.44442657
 0.44354126 0.44293514 0.44249502 0.44194713 0.44109806 0.44008762
 0.43929073 0.4389326  0.43885455 0.43865293 0.43806452 0.43726265
 0.43668446 0.43656957 0.4366577  0.43644708 0.43572405 0.43474582
 0.43402302 0.4338093  0.4338113  0.4335626  0.43278584 0.43170592
 0.4308078  0.430383   0.4302111  0.42985165 0.4291103  0.42817056
 0.42739245 0.42699182 0.4267796  0.42637488 0.42564085 0.42474434
 0.42402518 0.42346781 0.4228045  0.42170864 0.42020446 0.4186158
 0.41730213 0.41633368 0.4153371  0.4140907  0.41270605 0.4114693
 0.41052702 0.40969187 0.40859196 0.40708274 0.40542457 0.40407985
 0.40325713 0.40270105 0.40194294 0.40077525 0.39943308 0.39831448
 0.39750114 0.39679936 0.39594093 0.39489663 0.39391    0.3931568
 0.39255327 0.3916997  0.39036363 0.38864207 0.38703978 0.38602355
 0.38558227 0.38523713 0.3845331  0.3833474  0.38209605 0.3812975
 0.38094023 0.38060918 0.37983572 0.37860164 0.37734827 0.37658927
 0.37635997 0.37625122 0.37576598 0.37482214 0.37382665 0.37323025
 0.37312132 0.37314358 0.37294048 0.3725132  0.3720938  0.37199077
 0.372151   0.37223288 0.37193123 0.371352   0.37094912 0.3709844
 0.37129787 0.37145844 0.37112457 0.37044042 0.3698507  0.36970684
 0.36986908 0.36989442 0.36947605 0.3687454  0.36805692 0.36767763
 0.3674334  0.36694613 0.36615023 0.36536154 0.36500064 0.3651465
 0.36547336 0.36547035 0.36494243 0.36424872 0.36386135 0.36392128
 0.36412445 0.36394772 0.3632384  0.36240318 0.36190647 0.36193192
 0.36213443 0.3620358  0.36140817 0.36042026 0.3594512  0.35862935
 0.35787365 0.35704246 0.35608292 0.35517105 0.3545222  0.35408664
 0.3536414  0.35289755 0.35186672 0.35086414 0.35020515 0.3498174
 0.3494614  0.34882686 0.3480011  0.34717727 0.34660944 0.34620178
 0.34574157 0.34502476 0.3441752  0.34352028 0.34323576 0.3432357
 0.34311268 0.3427138  0.34212363 0.34161988 0.34138453 0.34129566
 0.34104732 0.3405713  0.34006464 0.33980548 0.33987132 0.34005496
 0.34002373 0.33971643 0.33935648 0.3392422  0.339509   0.33988428
 0.340024   0.33971804 0.33917543 0.3387955  0.33873272 0.3388511
 0.33879444 0.33835724 0.33778995 0.33747166 0.3375091  0.3376348
 0.3374992  0.33698547 0.3363757  0.3360424  0.3361156  0.33635056
 0.3363491  0.33600566 0.33556154 0.33537176 0.33552584 0.33572814
 0.33562687 0.33517182 0.33464745 0.33444485 0.33451685 0.33456796
 0.33426487 0.33361384 0.33299556 0.33280098 0.33302456 0.33331165
 0.3332221  0.3327589  0.33225456 0.33209023 0.33229622 0.33251482
 0.3324443  0.33213273 0.33190325 0.33202812 0.33242804 0.33277884
 0.33279526 0.33245003 0.33198893 0.33163074 0.3313735  0.33086646
 0.3299372  0.3287273  0.32763922 0.32694614 0.32654914 0.32620338
 0.3255831  0.3246628  0.32368168 0.3229018  0.32240093 0.32203496
 0.32165396 0.32122672 0.32071903 0.320167   0.31960568 0.31893694
 0.31817266 0.31738818 0.31672734 0.31625962 0.3158587  0.31548908
 0.31513372 0.3148796  0.31472415 0.31457123 0.31436244 0.31407335
 0.31386775 0.31386593 0.31398243 0.31405917 0.3139344  0.313625
 0.313319   0.31320605 0.31331223 0.31350142 0.3136034  0.3135267
 0.31336287 0.31326598 0.31335136 0.31352267 0.31363282 0.31358767
 0.31341782 0.31321478 0.31311718 0.3130917  0.31298852 0.31269252
 0.31226134 0.3119694  0.31187856 0.31184435 0.3116787  0.3113354
 0.3108838  0.31059194 0.31054857 0.31059048 0.31050995 0.3101881
 0.3097737  0.30946004 0.3093305  0.30920157 0.30892214 0.30854088
 0.30826214 0.30826944 0.30847332 0.3085625  0.308289   0.30764824
 0.3070621  0.30689377 0.3071439  0.30742785 0.30734667 0.30684802
 0.30629522 0.30606142 0.30615553 0.30622488 0.30594754 0.30527005
 0.30453497 0.30404854 0.30384418 0.3035377  0.302767   0.3014204
 0.29979274 0.29838476 0.29735446 0.2965983  0.29588145 0.2950461
 0.29408994 0.293146   0.29224283 0.29144993 0.2907813  0.29029483
 0.2899886  0.28978515 0.28941873 0.28869626 0.2877011  0.28667238
 0.2858708  0.28525308 0.284661   0.28403425 0.28336346 0.28282553
 0.2823963  0.2819589  0.2813854  0.28064585 0.2799556  0.27946055
 0.27920532 0.27900192 0.27858716 0.27790293 0.2771812  0.2767242
 0.27663168 0.27669236 0.27668032 0.27635315 0.27578542 0.27528715
 0.27508754 0.2751254  0.27510303 0.27481458 0.2743275  0.27389655
 0.2737131  0.2737846  0.27387452 0.27376705 0.273486   0.27322552
 0.27317438 0.27330518 0.27345186 0.27334216 0.27298737 0.27263948
 0.27254212 0.2726866  0.27277374 0.27261248 0.27222183 0.27188084
 0.27182785 0.2720158  0.2721074  0.2718585  0.27134347 0.27097583
 0.27098033 0.27125058 0.27139533 0.27113894 0.2705075  0.27000225
 0.26993972 0.27026522 0.2705269  0.27040318 0.27000114 0.26968893
 0.2697317  0.26999113 0.27012557 0.26988107 0.26938486 0.26898298
 0.26889554 0.26900288 0.2689258  0.2684087  0.2674336  0.26611587
 0.26466542 0.26334617 0.26217875 0.2611549  0.26025683 0.2594827
 0.25875425 0.2579491  0.2570222  0.2560533  0.25520223 0.2545785
 0.254074   0.2534898  0.25278914 0.25196943 0.25111338 0.2503301
 0.24961305 0.248927   0.24838428 0.24799375 0.24781972 0.24777767
 0.24768245 0.24748886 0.24717638 0.24685045 0.24668768 0.24662945
 0.24643296 0.24601898 0.24553876 0.24512756 0.24498506 0.24506503
 0.24520344 0.245065   0.24471739 0.24447367 0.24455912 0.24497339
 0.24550554 0.24593401 0.24610281 0.246168   0.24641077 0.2469653
 0.24760464 0.24794902 0.24785125 0.24762918 0.24767101 0.24809936
 0.248606   0.24882992 0.24869491 0.24833634 0.24835327 0.2487941
 0.24929616 0.24940158 0.24894598 0.24844144 0.24824719 0.24850512
 0.24878632 0.24873531 0.24824719 0.24770343 0.24777296 0.24832876
 0.24891916 0.24892777 0.2483436  0.24773285 0.24777542 0.24836455
 0.24906056 0.24916784 0.24850763 0.24773666 0.24751338 0.2479253
 0.24851136 0.24885191 0.2486723  0.24827893 0.24822626 0.2486325
 0.24921128 0.24966009 0.24975613 0.24944353 0.24881604 0.24791229
 0.24668156 0.245202   0.24370365 0.24241224 0.24154466 0.24100688
 0.24057247 0.24002773 0.23953126 0.23895983 0.23843881 0.23801401
 0.23769449 0.23737814 0.23688945 0.23614761 0.23542741 0.23483306
 0.23449355 0.23422702 0.23367776 0.23295785 0.23214939 0.23153299
 0.23134075 0.23132557 0.23116195 0.23046283 0.22941038 0.22840561
 0.22798234 0.22780925 0.22757503 0.22711544 0.2264952  0.22601688
 0.22577314 0.22576931 0.22575429 0.2257737  0.22552383 0.22540817
 0.22566217 0.22636983 0.22733447 0.22772136 0.22665705 0.22317216]
