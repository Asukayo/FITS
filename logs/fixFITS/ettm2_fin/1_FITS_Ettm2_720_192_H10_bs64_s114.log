Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=90, out_features=114, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9192960.0
params:  10374.0
Trainable parameters:  10374
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4764147
	speed: 0.1096s/iter; left time: 1425.4583s
	iters: 200, epoch: 1 | loss: 0.3599075
	speed: 0.0979s/iter; left time: 1263.4634s
Epoch: 1 cost time: 27.314767837524414
Epoch: 1, Steps: 262 | Train Loss: 0.3911123 Vali Loss: 0.1708846 Test Loss: 0.2377017
Validation loss decreased (inf --> 0.170885).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3212576
	speed: 0.4114s/iter; left time: 5240.2395s
	iters: 200, epoch: 2 | loss: 0.3324361
	speed: 0.0522s/iter; left time: 659.3965s
Epoch: 2 cost time: 19.02279233932495
Epoch: 2, Steps: 262 | Train Loss: 0.3232506 Vali Loss: 0.1617494 Test Loss: 0.2277761
Validation loss decreased (0.170885 --> 0.161749).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3129435
	speed: 0.3168s/iter; left time: 3952.3920s
	iters: 200, epoch: 3 | loss: 0.3802685
	speed: 0.0985s/iter; left time: 1218.9353s
Epoch: 3 cost time: 27.317745447158813
Epoch: 3, Steps: 262 | Train Loss: 0.3098510 Vali Loss: 0.1583743 Test Loss: 0.2241130
Validation loss decreased (0.161749 --> 0.158374).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2176446
	speed: 0.4348s/iter; left time: 5310.6758s
	iters: 200, epoch: 4 | loss: 0.4125383
	speed: 0.1037s/iter; left time: 1255.9014s
Epoch: 4 cost time: 27.857889413833618
Epoch: 4, Steps: 262 | Train Loss: 0.3034231 Vali Loss: 0.1563666 Test Loss: 0.2217524
Validation loss decreased (0.158374 --> 0.156367).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2864519
	speed: 0.4171s/iter; left time: 4986.1914s
	iters: 200, epoch: 5 | loss: 0.3578883
	speed: 0.0738s/iter; left time: 874.1940s
Epoch: 5 cost time: 20.63863182067871
Epoch: 5, Steps: 262 | Train Loss: 0.2998127 Vali Loss: 0.1551285 Test Loss: 0.2205049
Validation loss decreased (0.156367 --> 0.155128).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2786506
	speed: 0.3069s/iter; left time: 3587.9246s
	iters: 200, epoch: 6 | loss: 0.3248518
	speed: 0.0790s/iter; left time: 916.1877s
Epoch: 6 cost time: 23.682464361190796
Epoch: 6, Steps: 262 | Train Loss: 0.2972819 Vali Loss: 0.1546898 Test Loss: 0.2195017
Validation loss decreased (0.155128 --> 0.154690).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2467873
	speed: 0.4272s/iter; left time: 4882.3554s
	iters: 200, epoch: 7 | loss: 0.2143693
	speed: 0.0889s/iter; left time: 1007.3464s
Epoch: 7 cost time: 25.27702784538269
Epoch: 7, Steps: 262 | Train Loss: 0.2960295 Vali Loss: 0.1540194 Test Loss: 0.2191410
Validation loss decreased (0.154690 --> 0.154019).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4222830
	speed: 0.4215s/iter; left time: 4706.4051s
	iters: 200, epoch: 8 | loss: 0.3331644
	speed: 0.0928s/iter; left time: 1027.4788s
Epoch: 8 cost time: 25.8862042427063
Epoch: 8, Steps: 262 | Train Loss: 0.2948871 Vali Loss: 0.1535174 Test Loss: 0.2186012
Validation loss decreased (0.154019 --> 0.153517).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3205390
	speed: 0.4176s/iter; left time: 4554.1906s
	iters: 200, epoch: 9 | loss: 0.2143188
	speed: 0.0895s/iter; left time: 966.7204s
Epoch: 9 cost time: 23.782424688339233
Epoch: 9, Steps: 262 | Train Loss: 0.2938077 Vali Loss: 0.1537554 Test Loss: 0.2187120
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4415461
	speed: 0.3684s/iter; left time: 3920.6921s
	iters: 200, epoch: 10 | loss: 0.3228790
	speed: 0.0737s/iter; left time: 776.8570s
Epoch: 10 cost time: 22.527993202209473
Epoch: 10, Steps: 262 | Train Loss: 0.2931334 Vali Loss: 0.1532706 Test Loss: 0.2180752
Validation loss decreased (0.153517 --> 0.153271).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3069228
	speed: 0.4005s/iter; left time: 4158.0261s
	iters: 200, epoch: 11 | loss: 0.3631580
	speed: 0.1086s/iter; left time: 1117.0043s
Epoch: 11 cost time: 27.781562089920044
Epoch: 11, Steps: 262 | Train Loss: 0.2926564 Vali Loss: 0.1531663 Test Loss: 0.2180213
Validation loss decreased (0.153271 --> 0.153166).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3425641
	speed: 0.4569s/iter; left time: 4623.3926s
	iters: 200, epoch: 12 | loss: 0.2736116
	speed: 0.1058s/iter; left time: 1060.3509s
Epoch: 12 cost time: 29.718247413635254
Epoch: 12, Steps: 262 | Train Loss: 0.2924980 Vali Loss: 0.1529583 Test Loss: 0.2177180
Validation loss decreased (0.153166 --> 0.152958).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3028198
	speed: 0.4714s/iter; left time: 4646.9470s
	iters: 200, epoch: 13 | loss: 0.2986552
	speed: 0.0970s/iter; left time: 946.2278s
Epoch: 13 cost time: 26.517712593078613
Epoch: 13, Steps: 262 | Train Loss: 0.2921340 Vali Loss: 0.1530224 Test Loss: 0.2180798
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2761974
	speed: 0.3718s/iter; left time: 3567.6629s
	iters: 200, epoch: 14 | loss: 0.2359506
	speed: 0.0859s/iter; left time: 816.0812s
Epoch: 14 cost time: 18.207181215286255
Epoch: 14, Steps: 262 | Train Loss: 0.2918031 Vali Loss: 0.1528721 Test Loss: 0.2175165
Validation loss decreased (0.152958 --> 0.152872).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3049940
	speed: 0.1599s/iter; left time: 1492.2577s
	iters: 200, epoch: 15 | loss: 0.2486891
	speed: 0.0531s/iter; left time: 490.4795s
Epoch: 15 cost time: 15.940634727478027
Epoch: 15, Steps: 262 | Train Loss: 0.2911779 Vali Loss: 0.1528749 Test Loss: 0.2175360
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3051504
	speed: 0.4111s/iter; left time: 3729.1614s
	iters: 200, epoch: 16 | loss: 0.4133955
	speed: 0.1039s/iter; left time: 931.8990s
Epoch: 16 cost time: 27.434351921081543
Epoch: 16, Steps: 262 | Train Loss: 0.2907836 Vali Loss: 0.1527690 Test Loss: 0.2172862
Validation loss decreased (0.152872 --> 0.152769).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3395308
	speed: 0.4849s/iter; left time: 4271.3039s
	iters: 200, epoch: 17 | loss: 0.2858877
	speed: 0.1129s/iter; left time: 983.0135s
Epoch: 17 cost time: 29.385267972946167
Epoch: 17, Steps: 262 | Train Loss: 0.2911312 Vali Loss: 0.1529220 Test Loss: 0.2174883
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2485123
	speed: 0.4512s/iter; left time: 3856.2329s
	iters: 200, epoch: 18 | loss: 0.3840952
	speed: 0.1027s/iter; left time: 867.4860s
Epoch: 18 cost time: 26.89750909805298
Epoch: 18, Steps: 262 | Train Loss: 0.2906405 Vali Loss: 0.1526365 Test Loss: 0.2173398
Validation loss decreased (0.152769 --> 0.152637).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2494535
	speed: 0.4641s/iter; left time: 3844.9538s
	iters: 200, epoch: 19 | loss: 0.2412991
	speed: 0.1106s/iter; left time: 905.1339s
Epoch: 19 cost time: 29.18784999847412
Epoch: 19, Steps: 262 | Train Loss: 0.2903150 Vali Loss: 0.1524493 Test Loss: 0.2172458
Validation loss decreased (0.152637 --> 0.152449).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3198545
	speed: 0.4545s/iter; left time: 3646.3213s
	iters: 200, epoch: 20 | loss: 0.2838578
	speed: 0.1032s/iter; left time: 817.5885s
Epoch: 20 cost time: 27.271694660186768
Epoch: 20, Steps: 262 | Train Loss: 0.2904657 Vali Loss: 0.1526783 Test Loss: 0.2173121
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2791020
	speed: 0.4681s/iter; left time: 3633.2102s
	iters: 200, epoch: 21 | loss: 0.2306535
	speed: 0.1069s/iter; left time: 818.8964s
Epoch: 21 cost time: 28.7454514503479
Epoch: 21, Steps: 262 | Train Loss: 0.2905915 Vali Loss: 0.1524594 Test Loss: 0.2172403
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1817858
	speed: 0.4768s/iter; left time: 3575.4634s
	iters: 200, epoch: 22 | loss: 0.2664238
	speed: 0.0961s/iter; left time: 711.2352s
Epoch: 22 cost time: 27.10658597946167
Epoch: 22, Steps: 262 | Train Loss: 0.2904050 Vali Loss: 0.1525915 Test Loss: 0.2171447
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.2178170382976532, mae:0.2922031879425049, rse:0.37778180837631226, corr:[0.55529785 0.5623986  0.5620277  0.5595574  0.5582856  0.55875176
 0.5601232  0.56107706 0.560884   0.55989397 0.55889267 0.55841815
 0.558609   0.55915797 0.5595945  0.559448   0.55874544 0.5578043
 0.55695575 0.55636615 0.5560567  0.55584586 0.5555321  0.55500966
 0.55433154 0.5536757  0.5531785  0.5528582  0.5526332  0.55237585
 0.5519632  0.551441   0.55084974 0.55020845 0.5495827  0.5490341
 0.54851866 0.5479736  0.547334   0.5465993  0.54583764 0.54511195
 0.544484   0.54395807 0.54349774 0.54301107 0.5424307  0.5417505
 0.54093647 0.540028   0.53913766 0.5384137  0.5378561  0.53739405
 0.5369147  0.53640413 0.53583646 0.53524226 0.5347311  0.53439844
 0.5342358  0.53418916 0.5341108  0.53390056 0.5335219  0.5330888
 0.53265136 0.53234047 0.53219146 0.53209955 0.5319034  0.5315238
 0.5309677  0.5303357  0.5297367  0.5292978  0.52901304 0.5287546
 0.5283764  0.5278354  0.52707833 0.5262116  0.5253852  0.5247273
 0.5242757  0.5239951  0.5236726  0.52325445 0.5227475  0.5222357
 0.5218311  0.5215484  0.5211743  0.5205587  0.5195622  0.5181129
 0.5163464  0.5146169  0.51318455 0.51215875 0.5114361  0.51074266
 0.5098716  0.5086999  0.50723976 0.50569737 0.5044113  0.503642
 0.5032494  0.5029535  0.5024921  0.50170267 0.5006855  0.49952868
 0.49839228 0.49748042 0.49683478 0.49627417 0.4956644  0.49480805
 0.4938129  0.49278626 0.49200812 0.49151793 0.49114603 0.49066687
 0.48996723 0.48903343 0.4879542  0.48688105 0.48598796 0.48529643
 0.48473975 0.48415038 0.48341346 0.48251092 0.48152137 0.48075256
 0.4803188  0.4801896  0.48010188 0.4798301  0.47925514 0.47829854
 0.47712713 0.47592264 0.47507998 0.47456035 0.47417766 0.47357807
 0.4727062  0.47173807 0.4708577  0.47021028 0.4698514  0.4696882
 0.46938914 0.46874216 0.4676362  0.46647266 0.46557316 0.4653464
 0.4656784  0.46618122 0.46638277 0.46606487 0.4652172  0.4642172
 0.4636649  0.4639641  0.46471733 0.46522817 0.46487406 0.46349737
 0.4615275  0.45986748 0.4595006  0.460577   0.46200415 0.46276325
 0.46207103 0.45991504 0.45728478 0.45566443 0.45597103 0.4574594
 0.45834136 0.45749584 0.45473704 0.4519633  0.45386982 0.45926967]
