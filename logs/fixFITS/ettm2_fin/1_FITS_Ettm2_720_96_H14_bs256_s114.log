Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_96_FITS_ETTm2_ftM_sl720_ll48_pl96_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 11425
test 11425
Model(
  (freq_upsampler): Linear(in_features=122, out_features=138, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  60340224.0
params:  16974.0
Trainable parameters:  16974
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.191444158554077
Epoch: 1, Steps: 65 | Train Loss: 0.3714045 Vali Loss: 0.1549760 Test Loss: 0.2050314
Validation loss decreased (inf --> 0.154976).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.694525003433228
Epoch: 2, Steps: 65 | Train Loss: 0.2768198 Vali Loss: 0.1378430 Test Loss: 0.1856541
Validation loss decreased (0.154976 --> 0.137843).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.075180053710938
Epoch: 3, Steps: 65 | Train Loss: 0.2546319 Vali Loss: 0.1305075 Test Loss: 0.1783648
Validation loss decreased (0.137843 --> 0.130507).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.736280918121338
Epoch: 4, Steps: 65 | Train Loss: 0.2426546 Vali Loss: 0.1268156 Test Loss: 0.1743236
Validation loss decreased (0.130507 --> 0.126816).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.88791799545288
Epoch: 5, Steps: 65 | Train Loss: 0.2346730 Vali Loss: 0.1241944 Test Loss: 0.1714468
Validation loss decreased (0.126816 --> 0.124194).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 12.068251848220825
Epoch: 6, Steps: 65 | Train Loss: 0.2297941 Vali Loss: 0.1222923 Test Loss: 0.1695589
Validation loss decreased (0.124194 --> 0.122292).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.638367891311646
Epoch: 7, Steps: 65 | Train Loss: 0.2262631 Vali Loss: 0.1208841 Test Loss: 0.1681152
Validation loss decreased (0.122292 --> 0.120884).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.213130474090576
Epoch: 8, Steps: 65 | Train Loss: 0.2233967 Vali Loss: 0.1197477 Test Loss: 0.1669111
Validation loss decreased (0.120884 --> 0.119748).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.372466802597046
Epoch: 9, Steps: 65 | Train Loss: 0.2208981 Vali Loss: 0.1190614 Test Loss: 0.1661266
Validation loss decreased (0.119748 --> 0.119061).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.606345891952515
Epoch: 10, Steps: 65 | Train Loss: 0.2191999 Vali Loss: 0.1186236 Test Loss: 0.1654627
Validation loss decreased (0.119061 --> 0.118624).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.18757700920105
Epoch: 11, Steps: 65 | Train Loss: 0.2184282 Vali Loss: 0.1181151 Test Loss: 0.1649176
Validation loss decreased (0.118624 --> 0.118115).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.705028057098389
Epoch: 12, Steps: 65 | Train Loss: 0.2162202 Vali Loss: 0.1176934 Test Loss: 0.1644746
Validation loss decreased (0.118115 --> 0.117693).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.30908727645874
Epoch: 13, Steps: 65 | Train Loss: 0.2157565 Vali Loss: 0.1172626 Test Loss: 0.1641194
Validation loss decreased (0.117693 --> 0.117263).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.36251187324524
Epoch: 14, Steps: 65 | Train Loss: 0.2142591 Vali Loss: 0.1170789 Test Loss: 0.1637450
Validation loss decreased (0.117263 --> 0.117079).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.162400007247925
Epoch: 15, Steps: 65 | Train Loss: 0.2144541 Vali Loss: 0.1169008 Test Loss: 0.1635530
Validation loss decreased (0.117079 --> 0.116901).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.566878318786621
Epoch: 16, Steps: 65 | Train Loss: 0.2138511 Vali Loss: 0.1167720 Test Loss: 0.1634307
Validation loss decreased (0.116901 --> 0.116772).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.197890520095825
Epoch: 17, Steps: 65 | Train Loss: 0.2134961 Vali Loss: 0.1163179 Test Loss: 0.1632625
Validation loss decreased (0.116772 --> 0.116318).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.335037231445312
Epoch: 18, Steps: 65 | Train Loss: 0.2130124 Vali Loss: 0.1165409 Test Loss: 0.1630522
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.329718828201294
Epoch: 19, Steps: 65 | Train Loss: 0.2123830 Vali Loss: 0.1161156 Test Loss: 0.1629398
Validation loss decreased (0.116318 --> 0.116116).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 11.406026363372803
Epoch: 20, Steps: 65 | Train Loss: 0.2122449 Vali Loss: 0.1161726 Test Loss: 0.1628501
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.732086896896362
Epoch: 21, Steps: 65 | Train Loss: 0.2118251 Vali Loss: 0.1159280 Test Loss: 0.1627260
Validation loss decreased (0.116116 --> 0.115928).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.335694313049316
Epoch: 22, Steps: 65 | Train Loss: 0.2118035 Vali Loss: 0.1159175 Test Loss: 0.1627362
Validation loss decreased (0.115928 --> 0.115918).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 11.25789499282837
Epoch: 23, Steps: 65 | Train Loss: 0.2118847 Vali Loss: 0.1158875 Test Loss: 0.1625828
Validation loss decreased (0.115918 --> 0.115888).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 11.172205686569214
Epoch: 24, Steps: 65 | Train Loss: 0.2113087 Vali Loss: 0.1160879 Test Loss: 0.1624656
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.769381046295166
Epoch: 25, Steps: 65 | Train Loss: 0.2108633 Vali Loss: 0.1158966 Test Loss: 0.1624047
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 11.39056134223938
Epoch: 26, Steps: 65 | Train Loss: 0.2110429 Vali Loss: 0.1156989 Test Loss: 0.1623642
Validation loss decreased (0.115888 --> 0.115699).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.978252649307251
Epoch: 27, Steps: 65 | Train Loss: 0.2108871 Vali Loss: 0.1156112 Test Loss: 0.1622453
Validation loss decreased (0.115699 --> 0.115611).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.926899433135986
Epoch: 28, Steps: 65 | Train Loss: 0.2101264 Vali Loss: 0.1158157 Test Loss: 0.1621730
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 10.586462497711182
Epoch: 29, Steps: 65 | Train Loss: 0.2105807 Vali Loss: 0.1156534 Test Loss: 0.1621441
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 10.761591911315918
Epoch: 30, Steps: 65 | Train Loss: 0.2103481 Vali Loss: 0.1153291 Test Loss: 0.1620803
Validation loss decreased (0.115611 --> 0.115329).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 10.664397239685059
Epoch: 31, Steps: 65 | Train Loss: 0.2098372 Vali Loss: 0.1155110 Test Loss: 0.1621127
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 10.866036653518677
Epoch: 32, Steps: 65 | Train Loss: 0.2098113 Vali Loss: 0.1154269 Test Loss: 0.1620217
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 11.195942401885986
Epoch: 33, Steps: 65 | Train Loss: 0.2094437 Vali Loss: 0.1154784 Test Loss: 0.1619638
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_96_FITS_ETTm2_ftM_sl720_ll48_pl96_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
mse:0.16438305377960205, mae:0.25571876764297485, rse:0.3287189304828644, corr:[0.56063306 0.56960315 0.56688935 0.56456196 0.565693   0.5680776
 0.56854296 0.5669431  0.5656305  0.56574434 0.5667057  0.56714725
 0.566411   0.56531197 0.56484413 0.5650489  0.56532735 0.5649302
 0.56384236 0.56273377 0.5621011  0.56187195 0.5616403  0.56100404
 0.56008005 0.5593109  0.55889195 0.55868447 0.55832386 0.5575899
 0.55666906 0.55594933 0.5554896  0.5551553  0.55472225 0.5540228
 0.5531202  0.5522115  0.55144936 0.5509195  0.5504493  0.54982
 0.54903316 0.54821527 0.5475823  0.54711556 0.5465569  0.5457109
 0.54454345 0.5433573  0.5425638  0.5421558  0.5416175  0.54062617
 0.5393069  0.53823036 0.5377727  0.53765565 0.53729916 0.53652525
 0.5356088  0.53522724 0.5355175  0.53581846 0.5355367  0.5347541
 0.5338814  0.5336178  0.5339691  0.5341311  0.5335803  0.5325332
 0.5317448  0.5317928  0.53207403 0.5317011  0.530427   0.528915
 0.5282614  0.52870584 0.5287586  0.5274929  0.5251541  0.5231991
 0.5230483  0.52384174 0.52328885 0.5208783  0.5181021  0.5176138
 0.51935405 0.5195797  0.5162713  0.5117901  0.5137683  0.5189864 ]
