Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_96_FITS_ETTm2_ftM_sl720_ll48_pl96_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 11425
test 11425
Model(
  (freq_upsampler): Linear(in_features=74, out_features=83, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  22012928.0
params:  6225.0
Trainable parameters:  6225
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.987065315246582
Epoch: 1, Steps: 65 | Train Loss: 0.4134872 Vali Loss: 0.1681457 Test Loss: 0.2210431
Validation loss decreased (inf --> 0.168146).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.654934167861938
Epoch: 2, Steps: 65 | Train Loss: 0.2935354 Vali Loss: 0.1440961 Test Loss: 0.1917884
Validation loss decreased (0.168146 --> 0.144096).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.117924451828003
Epoch: 3, Steps: 65 | Train Loss: 0.2644483 Vali Loss: 0.1350026 Test Loss: 0.1824405
Validation loss decreased (0.144096 --> 0.135003).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.246764659881592
Epoch: 4, Steps: 65 | Train Loss: 0.2511464 Vali Loss: 0.1303970 Test Loss: 0.1777047
Validation loss decreased (0.135003 --> 0.130397).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.96276307106018
Epoch: 5, Steps: 65 | Train Loss: 0.2422800 Vali Loss: 0.1274279 Test Loss: 0.1745194
Validation loss decreased (0.130397 --> 0.127428).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.813416481018066
Epoch: 6, Steps: 65 | Train Loss: 0.2365979 Vali Loss: 0.1254982 Test Loss: 0.1724325
Validation loss decreased (0.127428 --> 0.125498).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.247302055358887
Epoch: 7, Steps: 65 | Train Loss: 0.2313625 Vali Loss: 0.1236643 Test Loss: 0.1707069
Validation loss decreased (0.125498 --> 0.123664).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.375501871109009
Epoch: 8, Steps: 65 | Train Loss: 0.2283852 Vali Loss: 0.1227204 Test Loss: 0.1694653
Validation loss decreased (0.123664 --> 0.122720).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.038862705230713
Epoch: 9, Steps: 65 | Train Loss: 0.2265871 Vali Loss: 0.1216280 Test Loss: 0.1684754
Validation loss decreased (0.122720 --> 0.121628).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 12.454592227935791
Epoch: 10, Steps: 65 | Train Loss: 0.2236735 Vali Loss: 0.1208740 Test Loss: 0.1677071
Validation loss decreased (0.121628 --> 0.120874).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.10596251487732
Epoch: 11, Steps: 65 | Train Loss: 0.2227342 Vali Loss: 0.1205971 Test Loss: 0.1672112
Validation loss decreased (0.120874 --> 0.120597).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 12.275748014450073
Epoch: 12, Steps: 65 | Train Loss: 0.2212179 Vali Loss: 0.1198809 Test Loss: 0.1667088
Validation loss decreased (0.120597 --> 0.119881).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.895616292953491
Epoch: 13, Steps: 65 | Train Loss: 0.2199771 Vali Loss: 0.1197890 Test Loss: 0.1663299
Validation loss decreased (0.119881 --> 0.119789).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.885569095611572
Epoch: 14, Steps: 65 | Train Loss: 0.2196766 Vali Loss: 0.1192265 Test Loss: 0.1659470
Validation loss decreased (0.119789 --> 0.119227).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.576988458633423
Epoch: 15, Steps: 65 | Train Loss: 0.2186629 Vali Loss: 0.1193000 Test Loss: 0.1658531
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 10.367577075958252
Epoch: 16, Steps: 65 | Train Loss: 0.2187010 Vali Loss: 0.1190429 Test Loss: 0.1655772
Validation loss decreased (0.119227 --> 0.119043).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.925647258758545
Epoch: 17, Steps: 65 | Train Loss: 0.2177275 Vali Loss: 0.1187010 Test Loss: 0.1653840
Validation loss decreased (0.119043 --> 0.118701).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.833595991134644
Epoch: 18, Steps: 65 | Train Loss: 0.2168479 Vali Loss: 0.1185543 Test Loss: 0.1652009
Validation loss decreased (0.118701 --> 0.118554).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.675338506698608
Epoch: 19, Steps: 65 | Train Loss: 0.2164983 Vali Loss: 0.1186034 Test Loss: 0.1649960
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.919902086257935
Epoch: 20, Steps: 65 | Train Loss: 0.2160863 Vali Loss: 0.1182909 Test Loss: 0.1648928
Validation loss decreased (0.118554 --> 0.118291).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 10.994375467300415
Epoch: 21, Steps: 65 | Train Loss: 0.2164681 Vali Loss: 0.1183288 Test Loss: 0.1647680
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.755343437194824
Epoch: 22, Steps: 65 | Train Loss: 0.2154781 Vali Loss: 0.1182004 Test Loss: 0.1647675
Validation loss decreased (0.118291 --> 0.118200).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.731245756149292
Epoch: 23, Steps: 65 | Train Loss: 0.2153293 Vali Loss: 0.1181564 Test Loss: 0.1645684
Validation loss decreased (0.118200 --> 0.118156).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.664891242980957
Epoch: 24, Steps: 65 | Train Loss: 0.2148882 Vali Loss: 0.1180379 Test Loss: 0.1645165
Validation loss decreased (0.118156 --> 0.118038).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 11.064062595367432
Epoch: 25, Steps: 65 | Train Loss: 0.2142030 Vali Loss: 0.1178321 Test Loss: 0.1643517
Validation loss decreased (0.118038 --> 0.117832).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.800033330917358
Epoch: 26, Steps: 65 | Train Loss: 0.2148376 Vali Loss: 0.1180141 Test Loss: 0.1642811
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.89234972000122
Epoch: 27, Steps: 65 | Train Loss: 0.2149199 Vali Loss: 0.1179444 Test Loss: 0.1642620
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.651317834854126
Epoch: 28, Steps: 65 | Train Loss: 0.2142775 Vali Loss: 0.1179068 Test Loss: 0.1641960
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_96_FITS_ETTm2_ftM_sl720_ll48_pl96_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
mse:0.1666620373725891, mae:0.2578325569629669, rse:0.3309897482395172, corr:[0.55752414 0.5651481  0.5698342  0.5700604  0.56791407 0.56591755
 0.5650568  0.5652707  0.5661347  0.567139   0.56773335 0.56755847
 0.56671005 0.5656071  0.564688   0.564104   0.56383294 0.5637064
 0.5635715  0.5632517  0.5626572  0.5617846  0.5608002  0.5599183
 0.5592272  0.5587191  0.5583474  0.5580493  0.557745   0.55734366
 0.556751   0.55603844 0.55527186 0.55446386 0.5536444  0.55287725
 0.55220324 0.5516773  0.55125016 0.5508138  0.5502586  0.5495511
 0.5487103  0.54777575 0.5468173  0.5459025  0.54515696 0.54463893
 0.54418695 0.54361176 0.5427988  0.541777   0.5405755  0.53929263
 0.53810185 0.5372702  0.5368801  0.5368244  0.5368336  0.53668815
 0.5362486  0.53556156 0.53469414 0.53379476 0.533097   0.5328926
 0.5331221  0.5335541  0.5338627  0.5337759  0.5331958  0.53219634
 0.5310028  0.5299874  0.5294633  0.52954066 0.52991015 0.53006953
 0.5296396  0.5285783  0.5268394  0.52476156 0.52286917 0.5217685
 0.5216927  0.52222997 0.52241004 0.52165127 0.5198097  0.5171729
 0.5146036  0.5133617  0.5139524  0.5152522  0.51417726 0.5070052 ]
