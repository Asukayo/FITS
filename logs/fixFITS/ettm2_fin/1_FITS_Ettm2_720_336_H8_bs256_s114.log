Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=74, out_features=108, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  28643328.0
params:  8100.0
Trainable parameters:  8100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 7.8347861766815186
Epoch: 1, Steps: 65 | Train Loss: 0.5366604 Vali Loss: 0.2447696 Test Loss: 0.3339391
Validation loss decreased (inf --> 0.244770).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.653470516204834
Epoch: 2, Steps: 65 | Train Loss: 0.4456285 Vali Loss: 0.2216848 Test Loss: 0.3037540
Validation loss decreased (0.244770 --> 0.221685).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.669282913208008
Epoch: 3, Steps: 65 | Train Loss: 0.4224793 Vali Loss: 0.2134545 Test Loss: 0.2936895
Validation loss decreased (0.221685 --> 0.213455).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.218181133270264
Epoch: 4, Steps: 65 | Train Loss: 0.4120961 Vali Loss: 0.2092228 Test Loss: 0.2884291
Validation loss decreased (0.213455 --> 0.209223).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.415603876113892
Epoch: 5, Steps: 65 | Train Loss: 0.4060078 Vali Loss: 0.2062269 Test Loss: 0.2851096
Validation loss decreased (0.209223 --> 0.206227).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.383146286010742
Epoch: 6, Steps: 65 | Train Loss: 0.4009936 Vali Loss: 0.2046850 Test Loss: 0.2827402
Validation loss decreased (0.206227 --> 0.204685).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.264532327651978
Epoch: 7, Steps: 65 | Train Loss: 0.3980086 Vali Loss: 0.2027515 Test Loss: 0.2809332
Validation loss decreased (0.204685 --> 0.202752).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.845494508743286
Epoch: 8, Steps: 65 | Train Loss: 0.3958077 Vali Loss: 0.2018707 Test Loss: 0.2794186
Validation loss decreased (0.202752 --> 0.201871).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.421518564224243
Epoch: 9, Steps: 65 | Train Loss: 0.3939026 Vali Loss: 0.2013736 Test Loss: 0.2783903
Validation loss decreased (0.201871 --> 0.201374).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.428147554397583
Epoch: 10, Steps: 65 | Train Loss: 0.3918839 Vali Loss: 0.2004886 Test Loss: 0.2774156
Validation loss decreased (0.201374 --> 0.200489).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.541635513305664
Epoch: 11, Steps: 65 | Train Loss: 0.3906195 Vali Loss: 0.2000259 Test Loss: 0.2767414
Validation loss decreased (0.200489 --> 0.200026).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.43489956855774
Epoch: 12, Steps: 65 | Train Loss: 0.3897520 Vali Loss: 0.1992014 Test Loss: 0.2760997
Validation loss decreased (0.200026 --> 0.199201).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.187630891799927
Epoch: 13, Steps: 65 | Train Loss: 0.3883556 Vali Loss: 0.1983286 Test Loss: 0.2755014
Validation loss decreased (0.199201 --> 0.198329).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.10666036605835
Epoch: 14, Steps: 65 | Train Loss: 0.3862556 Vali Loss: 0.1985809 Test Loss: 0.2750614
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.795079231262207
Epoch: 15, Steps: 65 | Train Loss: 0.3869282 Vali Loss: 0.1979492 Test Loss: 0.2746319
Validation loss decreased (0.198329 --> 0.197949).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.5106041431427
Epoch: 16, Steps: 65 | Train Loss: 0.3860476 Vali Loss: 0.1975580 Test Loss: 0.2743283
Validation loss decreased (0.197949 --> 0.197558).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.999619483947754
Epoch: 17, Steps: 65 | Train Loss: 0.3852862 Vali Loss: 0.1973436 Test Loss: 0.2739995
Validation loss decreased (0.197558 --> 0.197344).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.744582891464233
Epoch: 18, Steps: 65 | Train Loss: 0.3849054 Vali Loss: 0.1971292 Test Loss: 0.2737565
Validation loss decreased (0.197344 --> 0.197129).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.08488130569458
Epoch: 19, Steps: 65 | Train Loss: 0.3843599 Vali Loss: 0.1969057 Test Loss: 0.2735471
Validation loss decreased (0.197129 --> 0.196906).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 8.932618379592896
Epoch: 20, Steps: 65 | Train Loss: 0.3843175 Vali Loss: 0.1972566 Test Loss: 0.2732886
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.274647951126099
Epoch: 21, Steps: 65 | Train Loss: 0.3837243 Vali Loss: 0.1966098 Test Loss: 0.2731106
Validation loss decreased (0.196906 --> 0.196610).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.321309566497803
Epoch: 22, Steps: 65 | Train Loss: 0.3824425 Vali Loss: 0.1968537 Test Loss: 0.2729303
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 7.927535533905029
Epoch: 23, Steps: 65 | Train Loss: 0.3830642 Vali Loss: 0.1967807 Test Loss: 0.2728133
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 6.285072088241577
Epoch: 24, Steps: 65 | Train Loss: 0.3824424 Vali Loss: 0.1966512 Test Loss: 0.2726807
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2730694115161896, mae:0.3296273648738861, rse:0.4220823347568512, corr:[0.54190797 0.55068487 0.5569385  0.55802643 0.555871   0.5535728
 0.5525283  0.552789   0.55390024 0.5552545  0.5561968  0.55629295
 0.5555684  0.554453   0.5534319  0.55277014 0.5524573  0.55234426
 0.55224717 0.5519723  0.5514071  0.55058634 0.5496616  0.54883265
 0.548201   0.54774946 0.54740155 0.5470657  0.54667866 0.5461907
 0.5455863  0.54490083 0.5441799  0.5434656  0.542758   0.5420687
 0.54140306 0.5407845  0.5402212  0.5396867  0.5391502  0.53859913
 0.53801954 0.53739095 0.53668904 0.5359324  0.53517014 0.5344409
 0.5337579  0.5331149  0.53251535 0.5319494  0.53137124 0.53076667
 0.53014094 0.52954644 0.5290368  0.5286521  0.52838176 0.5281794
 0.52799064 0.52777    0.5274723  0.52708226 0.5266437  0.5262274
 0.52589667 0.5256644  0.5254837  0.5252831  0.5249945  0.524589
 0.52405834 0.52344096 0.52280855 0.5222547  0.5218114  0.5214391
 0.52105504 0.520577   0.51993126 0.5191055  0.518146   0.51716614
 0.5163183  0.5156943  0.51530254 0.5150547  0.5148162  0.5144557
 0.5138675  0.51298153 0.5118061  0.51043344 0.5090186  0.5076954
 0.50655365 0.5055856  0.50467336 0.5037053  0.50261194 0.501388
 0.50008726 0.49880627 0.4976955  0.4967972  0.4961084  0.4955239
 0.49491695 0.4941644  0.49322638 0.49214423 0.49105158 0.4900464
 0.48919913 0.48852214 0.48794544 0.48732978 0.4865719  0.48560205
 0.48447117 0.4832612  0.48214355 0.481226   0.48052302 0.47997597
 0.47945568 0.47880065 0.4778902  0.4767056  0.47533324 0.47394207
 0.4727608  0.47194058 0.47150382 0.47129476 0.47108635 0.4706882
 0.46995634 0.46889406 0.4675788  0.46620578 0.46501708 0.46413717
 0.46359676 0.4632509  0.46296722 0.46255672 0.46191174 0.46100703
 0.45995378 0.45894513 0.45812097 0.45760226 0.45737162 0.45732093
 0.4572527  0.4569889  0.45643678 0.45564196 0.4547248  0.45388338
 0.45331737 0.4531348  0.45327312 0.4535444  0.45370483 0.45356494
 0.4530831  0.45234835 0.4514715  0.45066127 0.45009887 0.44983307
 0.44976592 0.44970438 0.4494339  0.4488212  0.447832   0.44663334
 0.4454266  0.4444482  0.443891   0.44373193 0.44377702 0.44376284
 0.4435049  0.44287443 0.44184387 0.44048834 0.43899012 0.437548
 0.43633005 0.4353883  0.43453327 0.43353212 0.43228155 0.4307612
 0.42910787 0.42751467 0.4261756  0.42518255 0.42449734 0.42394358
 0.42332017 0.4224605  0.421262   0.4198379  0.41845843 0.4174259
 0.4167978  0.41652995 0.41643655 0.41627452 0.41583198 0.41492614
 0.413568   0.41188633 0.41021293 0.4088424  0.40790892 0.4074033
 0.40713203 0.4068131  0.40622288 0.40514484 0.40361542 0.4019052
 0.4002872  0.39914787 0.39857593 0.39851153 0.39864054 0.39863312
 0.39820582 0.39731783 0.39606223 0.3947599  0.39377177 0.39332095
 0.3934342  0.39383647 0.39422852 0.39438808 0.39407492 0.39332947
 0.39236444 0.39150995 0.39102608 0.3910515  0.39160076 0.39240244
 0.3930788  0.39335123 0.39298785 0.3920581  0.39081305 0.38968173
 0.389056   0.38908568 0.38965198 0.3904269  0.3909963  0.39109555
 0.39060035 0.38958403 0.38840786 0.3874767  0.3870587  0.3871989
 0.38769835 0.3882358  0.3884031  0.38799593 0.38698032 0.38558325
 0.38429382 0.38345954 0.38330895 0.38384405 0.38472995 0.3855421
 0.38586375 0.38548213 0.38441107 0.38287798 0.38127005 0.37987876
 0.3789471  0.37855175 0.37836006 0.37793517 0.37703013 0.37558806
 0.37396175 0.37255302 0.37171602 0.3715808  0.3720419  0.3725244
 0.37263668 0.3720246  0.37073433 0.3689664  0.3673377  0.36648673
 0.36679766 0.367951   0.36936226 0.37033874 0.37033576 0.36938006
 0.36761677 0.36574984 0.3645446  0.36451873 0.36572272 0.36761904
 0.36922324 0.36983702 0.36914858 0.36731708 0.36499777 0.36343813
 0.36372375 0.366122   0.36981297 0.37313792 0.37489673 0.3744177
 0.3722164  0.3695408  0.36884284 0.37237704 0.37906832 0.3849712 ]
