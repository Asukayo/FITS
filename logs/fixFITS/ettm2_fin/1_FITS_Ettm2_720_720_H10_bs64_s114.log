Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14515200.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5168626
	speed: 0.1279s/iter; left time: 1637.1467s
	iters: 200, epoch: 1 | loss: 0.6278936
	speed: 0.1215s/iter; left time: 1542.8985s
Epoch: 1 cost time: 31.888248443603516
Epoch: 1, Steps: 258 | Train Loss: 0.5805778 Vali Loss: 0.2803797 Test Loss: 0.3748262
Validation loss decreased (inf --> 0.280380).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5651718
	speed: 0.5154s/iter; left time: 6464.7699s
	iters: 200, epoch: 2 | loss: 0.7278928
	speed: 0.1254s/iter; left time: 1559.7843s
Epoch: 2 cost time: 31.093366861343384
Epoch: 2, Steps: 258 | Train Loss: 0.5185585 Vali Loss: 0.2699361 Test Loss: 0.3644669
Validation loss decreased (0.280380 --> 0.269936).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6949957
	speed: 0.5150s/iter; left time: 6326.3715s
	iters: 200, epoch: 3 | loss: 0.4218248
	speed: 0.1188s/iter; left time: 1448.1245s
Epoch: 3 cost time: 31.755136251449585
Epoch: 3, Steps: 258 | Train Loss: 0.5088136 Vali Loss: 0.2666877 Test Loss: 0.3605225
Validation loss decreased (0.269936 --> 0.266688).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6174852
	speed: 0.5193s/iter; left time: 6246.2014s
	iters: 200, epoch: 4 | loss: 0.3530059
	speed: 0.1183s/iter; left time: 1410.6488s
Epoch: 4 cost time: 30.938785314559937
Epoch: 4, Steps: 258 | Train Loss: 0.5048054 Vali Loss: 0.2654897 Test Loss: 0.3583662
Validation loss decreased (0.266688 --> 0.265490).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5036401
	speed: 0.5434s/iter; left time: 6394.8704s
	iters: 200, epoch: 5 | loss: 0.5482855
	speed: 0.1094s/iter; left time: 1276.9081s
Epoch: 5 cost time: 30.633990049362183
Epoch: 5, Steps: 258 | Train Loss: 0.5014524 Vali Loss: 0.2644367 Test Loss: 0.3568571
Validation loss decreased (0.265490 --> 0.264437).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4506030
	speed: 0.5088s/iter; left time: 5856.8643s
	iters: 200, epoch: 6 | loss: 0.3311033
	speed: 0.1329s/iter; left time: 1517.0537s
Epoch: 6 cost time: 33.23194146156311
Epoch: 6, Steps: 258 | Train Loss: 0.5001056 Vali Loss: 0.2633840 Test Loss: 0.3560343
Validation loss decreased (0.264437 --> 0.263384).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5697007
	speed: 0.5602s/iter; left time: 6304.1559s
	iters: 200, epoch: 7 | loss: 0.4195233
	speed: 0.1230s/iter; left time: 1371.2944s
Epoch: 7 cost time: 32.304823875427246
Epoch: 7, Steps: 258 | Train Loss: 0.4993179 Vali Loss: 0.2628843 Test Loss: 0.3552372
Validation loss decreased (0.263384 --> 0.262884).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4401585
	speed: 0.5216s/iter; left time: 5735.0205s
	iters: 200, epoch: 8 | loss: 0.5172739
	speed: 0.1151s/iter; left time: 1254.0701s
Epoch: 8 cost time: 31.483516931533813
Epoch: 8, Steps: 258 | Train Loss: 0.4982117 Vali Loss: 0.2628721 Test Loss: 0.3547670
Validation loss decreased (0.262884 --> 0.262872).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5587465
	speed: 0.5237s/iter; left time: 5623.4634s
	iters: 200, epoch: 9 | loss: 0.4417576
	speed: 0.1107s/iter; left time: 1177.0738s
Epoch: 9 cost time: 30.076489686965942
Epoch: 9, Steps: 258 | Train Loss: 0.4976066 Vali Loss: 0.2624674 Test Loss: 0.3541912
Validation loss decreased (0.262872 --> 0.262467).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6075826
	speed: 0.5224s/iter; left time: 5473.8014s
	iters: 200, epoch: 10 | loss: 0.4145564
	speed: 0.1188s/iter; left time: 1233.2219s
Epoch: 10 cost time: 31.9779314994812
Epoch: 10, Steps: 258 | Train Loss: 0.4976082 Vali Loss: 0.2623997 Test Loss: 0.3539131
Validation loss decreased (0.262467 --> 0.262400).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4909028
	speed: 0.4987s/iter; left time: 5096.7872s
	iters: 200, epoch: 11 | loss: 0.6676282
	speed: 0.1085s/iter; left time: 1098.5672s
Epoch: 11 cost time: 25.593124866485596
Epoch: 11, Steps: 258 | Train Loss: 0.4971417 Vali Loss: 0.2617125 Test Loss: 0.3539708
Validation loss decreased (0.262400 --> 0.261713).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5488831
	speed: 0.4497s/iter; left time: 4480.3010s
	iters: 200, epoch: 12 | loss: 0.5252265
	speed: 0.1093s/iter; left time: 1078.0420s
Epoch: 12 cost time: 28.476518154144287
Epoch: 12, Steps: 258 | Train Loss: 0.4963422 Vali Loss: 0.2615878 Test Loss: 0.3535343
Validation loss decreased (0.261713 --> 0.261588).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4193499
	speed: 0.4602s/iter; left time: 4466.2083s
	iters: 200, epoch: 13 | loss: 0.4457580
	speed: 0.1064s/iter; left time: 1021.9522s
Epoch: 13 cost time: 27.383007049560547
Epoch: 13, Steps: 258 | Train Loss: 0.4968191 Vali Loss: 0.2613761 Test Loss: 0.3535480
Validation loss decreased (0.261588 --> 0.261376).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6931291
	speed: 0.4719s/iter; left time: 4457.6433s
	iters: 200, epoch: 14 | loss: 0.3756372
	speed: 0.1077s/iter; left time: 1007.1289s
Epoch: 14 cost time: 30.279046297073364
Epoch: 14, Steps: 258 | Train Loss: 0.4961187 Vali Loss: 0.2618047 Test Loss: 0.3533706
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3448982
	speed: 0.4882s/iter; left time: 4486.0981s
	iters: 200, epoch: 15 | loss: 0.5076945
	speed: 0.1122s/iter; left time: 1019.6304s
Epoch: 15 cost time: 29.977704763412476
Epoch: 15, Steps: 258 | Train Loss: 0.4954036 Vali Loss: 0.2612707 Test Loss: 0.3533026
Validation loss decreased (0.261376 --> 0.261271).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5593000
	speed: 0.4968s/iter; left time: 4436.9093s
	iters: 200, epoch: 16 | loss: 0.4606126
	speed: 0.1093s/iter; left time: 965.0182s
Epoch: 16 cost time: 29.196579694747925
Epoch: 16, Steps: 258 | Train Loss: 0.4952594 Vali Loss: 0.2615788 Test Loss: 0.3529984
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.6162699
	speed: 0.5254s/iter; left time: 4556.9909s
	iters: 200, epoch: 17 | loss: 0.6046748
	speed: 0.1227s/iter; left time: 1051.7212s
Epoch: 17 cost time: 32.04059934616089
Epoch: 17, Steps: 258 | Train Loss: 0.4956662 Vali Loss: 0.2613642 Test Loss: 0.3532149
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4966239
	speed: 0.5264s/iter; left time: 4429.3677s
	iters: 200, epoch: 18 | loss: 0.5236257
	speed: 0.1218s/iter; left time: 1013.1328s
Epoch: 18 cost time: 32.22265601158142
Epoch: 18, Steps: 258 | Train Loss: 0.4953931 Vali Loss: 0.2614258 Test Loss: 0.3531328
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.34984534978866577, mae:0.3783256411552429, rse:0.47542551159858704, corr:[0.530438   0.5404495  0.5402257  0.5368078  0.5348372  0.5351354
 0.5367986  0.538219   0.5383088  0.53731954 0.53619874 0.5356474
 0.5358902  0.5366766  0.5374412  0.53753006 0.5368384  0.5357573
 0.53475255 0.53409624 0.53383636 0.53375316 0.533576   0.53315216
 0.5325063  0.53182644 0.5312714  0.5309035  0.5306452  0.5303479
 0.5298584  0.52921355 0.52851295 0.52783674 0.5272542  0.5267896
 0.5263693  0.5259064  0.52529293 0.52455246 0.52380455 0.5231498
 0.5226475  0.52225304 0.521889   0.521442   0.5208394  0.5200664
 0.5191536  0.5181818  0.51729816 0.5166156  0.5161081  0.5157032
 0.51530606 0.5148414  0.51429665 0.5137295  0.5132413  0.512889
 0.5127163  0.512652   0.51258236 0.51239246 0.512045   0.5116289
 0.5111772  0.5108251  0.51058906 0.5104081  0.5101824  0.5098428
 0.5093884  0.50883853 0.50823724 0.5076473  0.5071036  0.5065644
 0.5060205  0.50541866 0.50474435 0.50406855 0.5034754  0.50298643
 0.50257397 0.50215936 0.50164413 0.5010379  0.5003284  0.49958396
 0.49888995 0.49824247 0.4975645  0.49672922 0.49559984 0.4941404
 0.49244738 0.49073416 0.48917663 0.48792437 0.48699546 0.48624837
 0.48546267 0.48448333 0.48327202 0.48194724 0.4807133  0.47975868
 0.47909218 0.47858894 0.4780584  0.47734034 0.4764323  0.47536677
 0.47426134 0.4732752  0.47248083 0.47179237 0.4710842  0.4702164
 0.46919605 0.4680788  0.4670357  0.46616277 0.46545506 0.46477878
 0.46398723 0.4629522  0.46166667 0.4602709  0.4590343  0.4581762
 0.4577652  0.4576264  0.45749563 0.45710626 0.45632425 0.4552349
 0.4540506  0.4530416  0.45236355 0.4519876  0.45173147 0.45133054
 0.45064667 0.44967973 0.44866678 0.44781077 0.447241   0.44688895
 0.44659185 0.446145   0.44539213 0.44439995 0.4433811  0.44258922
 0.44215214 0.4420022  0.4419235  0.441736   0.4412649  0.44051516
 0.439613   0.43878713 0.43819132 0.43782446 0.4375543  0.4372277
 0.4367578  0.43615958 0.43547046 0.4348404  0.43431285 0.43387735
 0.4334362  0.43292034 0.43231    0.43167773 0.43112975 0.430774
 0.43057707 0.43044207 0.43024573 0.42984718 0.42918262 0.42827418
 0.42728752 0.42629248 0.42532653 0.4242829  0.4230536  0.42155433
 0.4198611  0.41826746 0.41688478 0.41582415 0.4151038  0.41455165
 0.41396368 0.41316703 0.41211218 0.41090208 0.4097251  0.40874168
 0.4079897  0.40733737 0.40654776 0.4054289  0.40399486 0.40243992
 0.40093127 0.3996798  0.39880452 0.39825958 0.39786842 0.3973236
 0.3964172  0.39506248 0.39345327 0.3918341  0.39041546 0.38931072
 0.38849366 0.3878542  0.38728032 0.38657218 0.38567728 0.38469335
 0.38366568 0.38270172 0.3818301  0.38106272 0.38038015 0.37978363
 0.37923312 0.37872946 0.37821993 0.37763724 0.3769672  0.37621927
 0.37548962 0.3748818  0.37453437 0.37457615 0.3748356  0.37511352
 0.37518543 0.3749335  0.3743713  0.37367237 0.37314478 0.37296715
 0.37312934 0.37347943 0.3737022  0.37359285 0.37307093 0.37227535
 0.3714252  0.3707481  0.3703377  0.3701698  0.37007222 0.36994487
 0.36970726 0.36933836 0.36894283 0.36860374 0.3683042  0.36795655
 0.3675367  0.367013   0.36640385 0.3658694  0.36553526 0.36545235
 0.36560908 0.36578947 0.3657783  0.36552456 0.36499813 0.364343
 0.36370054 0.3631901  0.3628132  0.36242738 0.3618765  0.36099544
 0.35986748 0.35874674 0.35781282 0.35715675 0.3567806  0.35653034
 0.3562827  0.35584998 0.35514823 0.354237   0.35329947 0.3524545
 0.3518357  0.35134834 0.35094395 0.35043287 0.34980145 0.34908152
 0.3484218  0.34786403 0.3474413  0.34713602 0.34687382 0.34666297
 0.34639317 0.34610847 0.34580708 0.34549946 0.34522668 0.34500003
 0.34474406 0.34446186 0.34418955 0.3439615  0.34380883 0.34377316
 0.3438157  0.34390393 0.34394068 0.34379154 0.3434939  0.3431081
 0.34281117 0.34265086 0.3426429  0.34274387 0.34280804 0.3428025
 0.34269375 0.34247845 0.34231865 0.3423473  0.3425455  0.34276417
 0.34287056 0.34276995 0.3424699  0.34205863 0.34169877 0.34154612
 0.3415931  0.34174705 0.3418276  0.34168723 0.34129596 0.34071773
 0.34011954 0.33969185 0.3394691  0.33947238 0.33949664 0.3394517
 0.33927256 0.3389739  0.33861998 0.33827916 0.33796832 0.337715
 0.33743304 0.33714342 0.33686036 0.3366766  0.33668613 0.33686447
 0.33714724 0.3374366  0.33758783 0.33751482 0.33723143 0.33689043
 0.33665064 0.33654436 0.33647713 0.33625478 0.33582702 0.3350968
 0.33420658 0.3333255  0.33260837 0.33207265 0.33160755 0.3311651
 0.3305936  0.32986516 0.3290607  0.3283498  0.3278925  0.327702
 0.32767886 0.32766438 0.32737574 0.326652   0.32556087 0.3242182
 0.32290623 0.32187134 0.3212871  0.32116663 0.3212725  0.3214154
 0.32140276 0.32120994 0.320872   0.3204639  0.32011962 0.31986487
 0.3197599  0.3197935  0.31986353 0.31993285 0.3199736  0.32002598
 0.32013938 0.3202941  0.3204001  0.32040364 0.32029694 0.32012478
 0.31997737 0.31988657 0.31989002 0.31993473 0.31996602 0.3199119
 0.31972    0.3193615  0.31896588 0.31868008 0.31860656 0.3186954
 0.31880462 0.3189087  0.31891155 0.3187516  0.31847382 0.31818232
 0.3178409  0.31747434 0.31704834 0.31653178 0.3160083  0.3155625
 0.31532818 0.3153314  0.3155288  0.31570917 0.31572163 0.31549716
 0.31502217 0.31438738 0.31372118 0.31314528 0.31275523 0.312427
 0.31207567 0.3116185  0.31107217 0.3105459  0.31018752 0.31010526
 0.31031293 0.3106708  0.3109529  0.31094974 0.310604   0.30991307
 0.30901548 0.30801365 0.3070632  0.30616856 0.3053165  0.30441692
 0.30341414 0.30239594 0.3013804  0.30043456 0.29960138 0.29885224
 0.29809117 0.2972916  0.29641837 0.2955768  0.29480344 0.29411873
 0.29351637 0.29302523 0.2925583  0.29197264 0.29122522 0.29032543
 0.28938892 0.2884491  0.28757885 0.28689    0.2863552  0.28599328
 0.285663   0.2852764  0.28482988 0.28432107 0.28382808 0.28331044
 0.28276238 0.2821703  0.28152695 0.28088644 0.2803392  0.27994058
 0.27967778 0.27946764 0.2793479  0.2792006  0.27895075 0.27861452
 0.27824605 0.27793747 0.27772078 0.27760845 0.27758667 0.27757412
 0.27746266 0.27725473 0.27698871 0.27675113 0.2766366  0.27663726
 0.27668455 0.27671182 0.27674502 0.27671906 0.27662814 0.2764731
 0.27626735 0.27603957 0.2757622  0.27548325 0.27521363 0.2749904
 0.27485493 0.27486247 0.27496934 0.27509043 0.27509883 0.27500087
 0.27480048 0.2746159  0.27453944 0.27462092 0.27469224 0.27470362
 0.27455804 0.27427295 0.27383652 0.2733115  0.27283633 0.27251032
 0.27236733 0.2723456  0.2723888  0.2724145  0.27238905 0.27230957
 0.27217072 0.27192688 0.27144694 0.27066714 0.2695645  0.2681119
 0.26645106 0.26495102 0.2637912  0.2629882  0.26236275 0.26171628
 0.26089856 0.25987577 0.25873038 0.2576109  0.25667408 0.25606138
 0.25575334 0.25559917 0.2554951  0.25526616 0.25481173 0.25416067
 0.2533786  0.252536   0.2518053  0.25120422 0.25079578 0.25055584
 0.25038618 0.25023916 0.25000346 0.24963129 0.24923278 0.2488859
 0.24857855 0.24835461 0.24828982 0.24829772 0.24838565 0.24852017
 0.24873002 0.24883975 0.24884038 0.2487792  0.24865627 0.24848501
 0.24833886 0.24833775 0.2484808  0.24873301 0.24901432 0.24922113
 0.24921747 0.24894452 0.24849483 0.24813764 0.24804685 0.24825744
 0.24862714 0.24900267 0.24927671 0.24922471 0.24904811 0.24883714
 0.24873517 0.24882782 0.24900325 0.24926095 0.24935587 0.24928324
 0.249025   0.24877109 0.24855043 0.2483432  0.24831645 0.24829568
 0.24830897 0.24829616 0.24826556 0.24819875 0.24814667 0.2479358
 0.24769185 0.24740152 0.24708404 0.24696024 0.24710399 0.24741255
 0.2477266  0.24804671 0.2481899  0.24814515 0.24809341 0.24811904
 0.24823472 0.24845663 0.24863294 0.24848022 0.24782628 0.2467081
 0.2453072  0.24389526 0.24267766 0.24172994 0.24112736 0.24075913
 0.24048162 0.24016921 0.23998494 0.23978221 0.2395206  0.23907815
 0.23843493 0.23764646 0.23677447 0.23586416 0.2350937  0.23439553
 0.23378788 0.2332445  0.23273794 0.23252666 0.23253827 0.23262884
 0.23267488 0.2324672  0.23198146 0.23110843 0.23004116 0.22898997
 0.22827321 0.22774045 0.22737443 0.22709447 0.22678849 0.22647424
 0.22611114 0.22575107 0.22527295 0.22478046 0.22405775 0.22334756
 0.2229832  0.22337392 0.22448117 0.2253343  0.22409609 0.2186046 ]
