Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  29442560.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3390996
	speed: 0.1478s/iter; left time: 946.3067s
Epoch: 1 cost time: 19.234394788742065
Epoch: 1, Steps: 130 | Train Loss: 0.3766303 Vali Loss: 0.2575050 Test Loss: 0.3446038
Validation loss decreased (inf --> 0.257505).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2661459
	speed: 0.4003s/iter; left time: 2510.5537s
Epoch: 2 cost time: 19.981526851654053
Epoch: 2, Steps: 130 | Train Loss: 0.2634285 Vali Loss: 0.2337938 Test Loss: 0.3128980
Validation loss decreased (0.257505 --> 0.233794).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1882659
	speed: 0.3549s/iter; left time: 2179.1750s
Epoch: 3 cost time: 16.83155632019043
Epoch: 3, Steps: 130 | Train Loss: 0.2198001 Vali Loss: 0.2238782 Test Loss: 0.3002491
Validation loss decreased (0.233794 --> 0.223878).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2007829
	speed: 0.4134s/iter; left time: 2485.0182s
Epoch: 4 cost time: 19.861937284469604
Epoch: 4, Steps: 130 | Train Loss: 0.1955094 Vali Loss: 0.2180992 Test Loss: 0.2935830
Validation loss decreased (0.223878 --> 0.218099).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1725901
	speed: 0.4569s/iter; left time: 2687.1677s
Epoch: 5 cost time: 22.733036518096924
Epoch: 5, Steps: 130 | Train Loss: 0.1790912 Vali Loss: 0.2145144 Test Loss: 0.2888579
Validation loss decreased (0.218099 --> 0.214514).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1417594
	speed: 0.4609s/iter; left time: 2650.5791s
Epoch: 6 cost time: 22.287773609161377
Epoch: 6, Steps: 130 | Train Loss: 0.1673049 Vali Loss: 0.2106710 Test Loss: 0.2853532
Validation loss decreased (0.214514 --> 0.210671).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1540637
	speed: 0.4512s/iter; left time: 2536.1311s
Epoch: 7 cost time: 21.158796787261963
Epoch: 7, Steps: 130 | Train Loss: 0.1586866 Vali Loss: 0.2084054 Test Loss: 0.2824765
Validation loss decreased (0.210671 --> 0.208405).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1330906
	speed: 0.4063s/iter; left time: 2231.2165s
Epoch: 8 cost time: 18.150678157806396
Epoch: 8, Steps: 130 | Train Loss: 0.1520945 Vali Loss: 0.2061215 Test Loss: 0.2801181
Validation loss decreased (0.208405 --> 0.206121).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1174529
	speed: 0.3927s/iter; left time: 2105.1551s
Epoch: 9 cost time: 18.262977361679077
Epoch: 9, Steps: 130 | Train Loss: 0.1475003 Vali Loss: 0.2049412 Test Loss: 0.2786103
Validation loss decreased (0.206121 --> 0.204941).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1548050
	speed: 0.3751s/iter; left time: 1962.0187s
Epoch: 10 cost time: 19.216747522354126
Epoch: 10, Steps: 130 | Train Loss: 0.1436896 Vali Loss: 0.2032155 Test Loss: 0.2772363
Validation loss decreased (0.204941 --> 0.203215).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1136987
	speed: 0.3819s/iter; left time: 1948.0981s
Epoch: 11 cost time: 18.79998755455017
Epoch: 11, Steps: 130 | Train Loss: 0.1409151 Vali Loss: 0.2023183 Test Loss: 0.2761984
Validation loss decreased (0.203215 --> 0.202318).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1025453
	speed: 0.3770s/iter; left time: 1874.1975s
Epoch: 12 cost time: 17.943127393722534
Epoch: 12, Steps: 130 | Train Loss: 0.1386104 Vali Loss: 0.2011618 Test Loss: 0.2752855
Validation loss decreased (0.202318 --> 0.201162).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1539373
	speed: 0.3702s/iter; left time: 1792.1150s
Epoch: 13 cost time: 17.758710384368896
Epoch: 13, Steps: 130 | Train Loss: 0.1369626 Vali Loss: 0.2007066 Test Loss: 0.2746418
Validation loss decreased (0.201162 --> 0.200707).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1351815
	speed: 0.3833s/iter; left time: 1805.6619s
Epoch: 14 cost time: 19.00265407562256
Epoch: 14, Steps: 130 | Train Loss: 0.1355335 Vali Loss: 0.1998113 Test Loss: 0.2740884
Validation loss decreased (0.200707 --> 0.199811).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1262575
	speed: 0.3953s/iter; left time: 1810.9773s
Epoch: 15 cost time: 19.087799072265625
Epoch: 15, Steps: 130 | Train Loss: 0.1344288 Vali Loss: 0.1992749 Test Loss: 0.2736347
Validation loss decreased (0.199811 --> 0.199275).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1500761
	speed: 0.3809s/iter; left time: 1695.4257s
Epoch: 16 cost time: 18.752726078033447
Epoch: 16, Steps: 130 | Train Loss: 0.1339210 Vali Loss: 0.1988065 Test Loss: 0.2734259
Validation loss decreased (0.199275 --> 0.198807).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1615105
	speed: 0.3666s/iter; left time: 1584.2013s
Epoch: 17 cost time: 19.561846017837524
Epoch: 17, Steps: 130 | Train Loss: 0.1330660 Vali Loss: 0.1985103 Test Loss: 0.2731130
Validation loss decreased (0.198807 --> 0.198510).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1429449
	speed: 0.4312s/iter; left time: 1807.2851s
Epoch: 18 cost time: 22.390183448791504
Epoch: 18, Steps: 130 | Train Loss: 0.1326813 Vali Loss: 0.1976146 Test Loss: 0.2729326
Validation loss decreased (0.198510 --> 0.197615).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1147041
	speed: 0.4817s/iter; left time: 1956.0971s
Epoch: 19 cost time: 23.35707116127014
Epoch: 19, Steps: 130 | Train Loss: 0.1323138 Vali Loss: 0.1975028 Test Loss: 0.2727843
Validation loss decreased (0.197615 --> 0.197503).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1209659
	speed: 0.4550s/iter; left time: 1788.4732s
Epoch: 20 cost time: 21.16757607460022
Epoch: 20, Steps: 130 | Train Loss: 0.1319645 Vali Loss: 0.1976110 Test Loss: 0.2726495
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1394494
	speed: 0.4388s/iter; left time: 1667.9551s
Epoch: 21 cost time: 22.035428047180176
Epoch: 21, Steps: 130 | Train Loss: 0.1315733 Vali Loss: 0.1973140 Test Loss: 0.2725793
Validation loss decreased (0.197503 --> 0.197314).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1439370
	speed: 0.4361s/iter; left time: 1601.0418s
Epoch: 22 cost time: 20.412065982818604
Epoch: 22, Steps: 130 | Train Loss: 0.1313877 Vali Loss: 0.1969867 Test Loss: 0.2725361
Validation loss decreased (0.197314 --> 0.196987).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1293741
	speed: 0.3833s/iter; left time: 1357.4298s
Epoch: 23 cost time: 18.969996213912964
Epoch: 23, Steps: 130 | Train Loss: 0.1314500 Vali Loss: 0.1969293 Test Loss: 0.2724762
Validation loss decreased (0.196987 --> 0.196929).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1396611
	speed: 0.3782s/iter; left time: 1289.9884s
Epoch: 24 cost time: 18.60641074180603
Epoch: 24, Steps: 130 | Train Loss: 0.1310710 Vali Loss: 0.1970849 Test Loss: 0.2724651
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1240018
	speed: 0.4002s/iter; left time: 1313.0741s
Epoch: 25 cost time: 19.766270399093628
Epoch: 25, Steps: 130 | Train Loss: 0.1310259 Vali Loss: 0.1970279 Test Loss: 0.2724089
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1266861
	speed: 0.3881s/iter; left time: 1223.0220s
Epoch: 26 cost time: 18.299630403518677
Epoch: 26, Steps: 130 | Train Loss: 0.1310001 Vali Loss: 0.1964734 Test Loss: 0.2723857
Validation loss decreased (0.196929 --> 0.196473).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.0967747
	speed: 0.3903s/iter; left time: 1179.1744s
Epoch: 27 cost time: 19.869592905044556
Epoch: 27, Steps: 130 | Train Loss: 0.1306677 Vali Loss: 0.1966349 Test Loss: 0.2723109
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1328339
	speed: 0.4035s/iter; left time: 1166.5463s
Epoch: 28 cost time: 18.94471049308777
Epoch: 28, Steps: 130 | Train Loss: 0.1307425 Vali Loss: 0.1967797 Test Loss: 0.2723763
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1449099
	speed: 0.3896s/iter; left time: 1075.8096s
Epoch: 29 cost time: 19.339089155197144
Epoch: 29, Steps: 130 | Train Loss: 0.1306654 Vali Loss: 0.1964935 Test Loss: 0.2723618
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  29442560.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2942693
	speed: 0.1347s/iter; left time: 862.0429s
Epoch: 1 cost time: 17.763578176498413
Epoch: 1, Steps: 130 | Train Loss: 0.3801679 Vali Loss: 0.1941062 Test Loss: 0.2700606
Validation loss decreased (inf --> 0.194106).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3573904
	speed: 0.3894s/iter; left time: 2441.9798s
Epoch: 2 cost time: 18.481310844421387
Epoch: 2, Steps: 130 | Train Loss: 0.3776516 Vali Loss: 0.1938669 Test Loss: 0.2696229
Validation loss decreased (0.194106 --> 0.193867).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3996980
	speed: 0.4582s/iter; left time: 2813.6578s
Epoch: 3 cost time: 21.994725465774536
Epoch: 3, Steps: 130 | Train Loss: 0.3769022 Vali Loss: 0.1934034 Test Loss: 0.2690340
Validation loss decreased (0.193867 --> 0.193403).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3663934
	speed: 0.4609s/iter; left time: 2770.4636s
Epoch: 4 cost time: 22.091387271881104
Epoch: 4, Steps: 130 | Train Loss: 0.3760043 Vali Loss: 0.1931158 Test Loss: 0.2689823
Validation loss decreased (0.193403 --> 0.193116).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3517985
	speed: 0.4735s/iter; left time: 2784.4744s
Epoch: 5 cost time: 22.747844696044922
Epoch: 5, Steps: 130 | Train Loss: 0.3757253 Vali Loss: 0.1929299 Test Loss: 0.2685528
Validation loss decreased (0.193116 --> 0.192930).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3707981
	speed: 0.4432s/iter; left time: 2548.5686s
Epoch: 6 cost time: 21.21105980873108
Epoch: 6, Steps: 130 | Train Loss: 0.3760636 Vali Loss: 0.1929048 Test Loss: 0.2682155
Validation loss decreased (0.192930 --> 0.192905).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3080378
	speed: 0.4163s/iter; left time: 2339.8470s
Epoch: 7 cost time: 19.568281650543213
Epoch: 7, Steps: 130 | Train Loss: 0.3747713 Vali Loss: 0.1930465 Test Loss: 0.2683753
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3261078
	speed: 0.3728s/iter; left time: 2046.8409s
Epoch: 8 cost time: 17.25360417366028
Epoch: 8, Steps: 130 | Train Loss: 0.3761455 Vali Loss: 0.1927768 Test Loss: 0.2682240
Validation loss decreased (0.192905 --> 0.192777).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4067752
	speed: 0.3814s/iter; left time: 2044.8476s
Epoch: 9 cost time: 18.735278367996216
Epoch: 9, Steps: 130 | Train Loss: 0.3751469 Vali Loss: 0.1923407 Test Loss: 0.2682117
Validation loss decreased (0.192777 --> 0.192341).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3095722
	speed: 0.3766s/iter; left time: 1969.8954s
Epoch: 10 cost time: 17.99722123146057
Epoch: 10, Steps: 130 | Train Loss: 0.3748471 Vali Loss: 0.1925742 Test Loss: 0.2680796
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4128243
	speed: 0.4101s/iter; left time: 2092.1395s
Epoch: 11 cost time: 20.553797721862793
Epoch: 11, Steps: 130 | Train Loss: 0.3751322 Vali Loss: 0.1924675 Test Loss: 0.2681906
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4422191
	speed: 0.3778s/iter; left time: 1878.2140s
Epoch: 12 cost time: 18.58031988143921
Epoch: 12, Steps: 130 | Train Loss: 0.3742562 Vali Loss: 0.1923902 Test Loss: 0.2680051
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.26885756850242615, mae:0.3264704644680023, rse:0.41881459951400757, corr:[0.5549966  0.55996394 0.5589191  0.5562014  0.55499214 0.5554905
 0.5564951  0.5566782  0.55581075 0.55470693 0.55415314 0.55431277
 0.55476105 0.5548397  0.5542853  0.55333453 0.55245304 0.55191463
 0.5516264  0.5512739  0.55063397 0.549753   0.54890805 0.54835695
 0.5481237  0.5480311  0.5477938  0.5472251  0.5463898  0.545531
 0.5448474  0.54441017 0.5440588  0.54358697 0.5429193  0.54215586
 0.54140574 0.54077965 0.5402923  0.5398825  0.5394363  0.5388734
 0.53822386 0.53754926 0.5368961  0.5362684  0.53565365 0.53500193
 0.534247   0.5334267  0.5326486  0.53200644 0.53146726 0.5309696
 0.53043556 0.5298685  0.52928895 0.52877665 0.5283979  0.5281476
 0.5279632  0.5277875  0.52755576 0.52724916 0.52692443 0.5266568
 0.52643514 0.5262626  0.5261097  0.5259489  0.52577156 0.52558225
 0.52535033 0.52503425 0.52458453 0.5240443  0.5234518  0.5228402
 0.522192   0.5214929  0.52065736 0.51974434 0.5188578  0.5181394
 0.5176813  0.5174171  0.5171586  0.51677674 0.51623684 0.5156202
 0.51504964 0.5145608  0.5140702  0.51343805 0.51251596 0.5112235
 0.5096791  0.50814563 0.5067925  0.5057077  0.50483006 0.5040047
 0.5030982  0.5020416  0.500914   0.4998177  0.49887428 0.49809456
 0.49735868 0.49655178 0.4956443  0.4946783  0.49377245 0.49293035
 0.49210203 0.4912742  0.4904519  0.4896186  0.48885468 0.48818088
 0.48766837 0.48724127 0.4868529  0.4863425  0.48554474 0.48439392
 0.48297536 0.48144534 0.48000884 0.47886676 0.47812566 0.47772393
 0.47749266 0.47719854 0.47664508 0.47577113 0.47467798 0.47362575
 0.47281706 0.47233856 0.47203514 0.4716943  0.4711684  0.4704011
 0.4695423  0.46874318 0.468195   0.467797   0.46736065 0.46669868
 0.46584043 0.4649518  0.46413368 0.46346068 0.4629302  0.4624972
 0.46207008 0.46165276 0.4613119  0.46116248 0.46112484 0.46104243
 0.46073005 0.46017244 0.45949543 0.45892778 0.4586161  0.4585916
 0.45871928 0.45877844 0.45849368 0.45785135 0.45703837 0.45630133
 0.45579502 0.45547512 0.45515215 0.45463935 0.4538473  0.4529581
 0.4520947  0.45137638 0.45087644 0.45048583 0.45003024 0.44939294
 0.4486038  0.44777045 0.4469633  0.44616932 0.44531643 0.44430935
 0.44312847 0.44185263 0.4404132  0.43886462 0.43743876 0.43625253
 0.43534148 0.4345739  0.43380272 0.43289387 0.43186706 0.43083414
 0.42992973 0.42916006 0.42836067 0.42740527 0.42633906 0.42530945
 0.42433512 0.42353252 0.4229679  0.42254943 0.4221007  0.42136323
 0.420294   0.41898644 0.41775683 0.41673884 0.41583744 0.41491294
 0.41383013 0.412617   0.41147014 0.41041782 0.40951723 0.40874943
 0.40788242 0.4068769  0.40572315 0.40462512 0.40374938 0.40317756
 0.40277025 0.4023947  0.40189353 0.40131965 0.4008814  0.40071428
 0.40083507 0.40097612 0.40094417 0.40071446 0.40018603 0.3995309
 0.39893633 0.39850196 0.3981249  0.39767212 0.39725402 0.39693356
 0.3967844  0.39689615 0.39705697 0.3971339  0.39696124 0.3966155
 0.39625755 0.39606783 0.39612877 0.39636618 0.39655116 0.39653423
 0.39623627 0.39570704 0.39524084 0.3950014  0.39492252 0.3947938
 0.39445102 0.39393443 0.39331317 0.39286384 0.3927035  0.39279324
 0.39302734 0.3929929  0.39243308 0.39142123 0.39015347 0.3890423
 0.3883756  0.38827273 0.3885825  0.38894632 0.3889895  0.38833952
 0.3870718  0.38567892 0.38446772 0.38363525 0.38325137 0.38314044
 0.38317448 0.38305324 0.3825594  0.38172442 0.38085318 0.37998816
 0.37933078 0.37873596 0.37812376 0.3772548  0.3762856  0.3755007
 0.37516814 0.37513652 0.37529135 0.37545776 0.3754844  0.37556997
 0.37562704 0.37580284 0.37602642 0.37618247 0.37626797 0.3763421
 0.37632412 0.37627587 0.37619975 0.37603304 0.3757131  0.37543613
 0.37528795 0.3752921  0.37540346 0.3754354  0.37561515 0.37593818
 0.3764071  0.37642536 0.37528813 0.3724088  0.3681543  0.36420816]
