Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=106, out_features=134, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50907136.0
params:  14338.0
Trainable parameters:  14338
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.99851369857788
Epoch: 1, Steps: 65 | Train Loss: 0.3803252 Vali Loss: 0.2601677 Test Loss: 0.3428131
Validation loss decreased (inf --> 0.260168).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.306310176849365
Epoch: 2, Steps: 65 | Train Loss: 0.2885408 Vali Loss: 0.2274238 Test Loss: 0.2983645
Validation loss decreased (0.260168 --> 0.227424).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.929840564727783
Epoch: 3, Steps: 65 | Train Loss: 0.2404821 Vali Loss: 0.2118421 Test Loss: 0.2775338
Validation loss decreased (0.227424 --> 0.211842).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.275384426116943
Epoch: 4, Steps: 65 | Train Loss: 0.2121192 Vali Loss: 0.2026968 Test Loss: 0.2661183
Validation loss decreased (0.211842 --> 0.202697).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.554563045501709
Epoch: 5, Steps: 65 | Train Loss: 0.1914125 Vali Loss: 0.1971735 Test Loss: 0.2591567
Validation loss decreased (0.202697 --> 0.197174).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.842350006103516
Epoch: 6, Steps: 65 | Train Loss: 0.1765859 Vali Loss: 0.1930303 Test Loss: 0.2544016
Validation loss decreased (0.197174 --> 0.193030).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.306821584701538
Epoch: 7, Steps: 65 | Train Loss: 0.1647533 Vali Loss: 0.1900397 Test Loss: 0.2511388
Validation loss decreased (0.193030 --> 0.190040).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.729743003845215
Epoch: 8, Steps: 65 | Train Loss: 0.1552149 Vali Loss: 0.1877991 Test Loss: 0.2485384
Validation loss decreased (0.190040 --> 0.187799).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.174174547195435
Epoch: 9, Steps: 65 | Train Loss: 0.1468104 Vali Loss: 0.1856664 Test Loss: 0.2463555
Validation loss decreased (0.187799 --> 0.185666).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.467202186584473
Epoch: 10, Steps: 65 | Train Loss: 0.1399317 Vali Loss: 0.1838699 Test Loss: 0.2445723
Validation loss decreased (0.185666 --> 0.183870).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.549756050109863
Epoch: 11, Steps: 65 | Train Loss: 0.1343306 Vali Loss: 0.1823218 Test Loss: 0.2429951
Validation loss decreased (0.183870 --> 0.182322).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.874000310897827
Epoch: 12, Steps: 65 | Train Loss: 0.1293780 Vali Loss: 0.1810273 Test Loss: 0.2415672
Validation loss decreased (0.182322 --> 0.181027).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.691158294677734
Epoch: 13, Steps: 65 | Train Loss: 0.1249559 Vali Loss: 0.1797414 Test Loss: 0.2402633
Validation loss decreased (0.181027 --> 0.179741).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.953132629394531
Epoch: 14, Steps: 65 | Train Loss: 0.1212371 Vali Loss: 0.1788088 Test Loss: 0.2391670
Validation loss decreased (0.179741 --> 0.178809).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.176671981811523
Epoch: 15, Steps: 65 | Train Loss: 0.1175490 Vali Loss: 0.1773493 Test Loss: 0.2380575
Validation loss decreased (0.178809 --> 0.177349).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.2669038772583
Epoch: 16, Steps: 65 | Train Loss: 0.1144547 Vali Loss: 0.1766378 Test Loss: 0.2370889
Validation loss decreased (0.177349 --> 0.176638).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.731512069702148
Epoch: 17, Steps: 65 | Train Loss: 0.1117800 Vali Loss: 0.1756851 Test Loss: 0.2362117
Validation loss decreased (0.176638 --> 0.175685).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 12.208811521530151
Epoch: 18, Steps: 65 | Train Loss: 0.1092376 Vali Loss: 0.1749274 Test Loss: 0.2354116
Validation loss decreased (0.175685 --> 0.174927).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 12.210217714309692
Epoch: 19, Steps: 65 | Train Loss: 0.1073039 Vali Loss: 0.1738115 Test Loss: 0.2346243
Validation loss decreased (0.174927 --> 0.173812).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.125407934188843
Epoch: 20, Steps: 65 | Train Loss: 0.1049801 Vali Loss: 0.1731686 Test Loss: 0.2338506
Validation loss decreased (0.173812 --> 0.173169).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.961090087890625
Epoch: 21, Steps: 65 | Train Loss: 0.1033986 Vali Loss: 0.1726694 Test Loss: 0.2331969
Validation loss decreased (0.173169 --> 0.172669).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.765332460403442
Epoch: 22, Steps: 65 | Train Loss: 0.1016142 Vali Loss: 0.1718897 Test Loss: 0.2325452
Validation loss decreased (0.172669 --> 0.171890).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.403837442398071
Epoch: 23, Steps: 65 | Train Loss: 0.1000804 Vali Loss: 0.1710999 Test Loss: 0.2319931
Validation loss decreased (0.171890 --> 0.171100).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.664680242538452
Epoch: 24, Steps: 65 | Train Loss: 0.0987878 Vali Loss: 0.1707784 Test Loss: 0.2314649
Validation loss decreased (0.171100 --> 0.170778).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 9.943119287490845
Epoch: 25, Steps: 65 | Train Loss: 0.0973227 Vali Loss: 0.1700997 Test Loss: 0.2309934
Validation loss decreased (0.170778 --> 0.170100).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 9.880235433578491
Epoch: 26, Steps: 65 | Train Loss: 0.0962989 Vali Loss: 0.1697147 Test Loss: 0.2304912
Validation loss decreased (0.170100 --> 0.169715).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.420128107070923
Epoch: 27, Steps: 65 | Train Loss: 0.0952220 Vali Loss: 0.1692215 Test Loss: 0.2300967
Validation loss decreased (0.169715 --> 0.169222).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.232404470443726
Epoch: 28, Steps: 65 | Train Loss: 0.0942442 Vali Loss: 0.1688148 Test Loss: 0.2296944
Validation loss decreased (0.169222 --> 0.168815).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.705780982971191
Epoch: 29, Steps: 65 | Train Loss: 0.0933154 Vali Loss: 0.1684080 Test Loss: 0.2293102
Validation loss decreased (0.168815 --> 0.168408).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.741448879241943
Epoch: 30, Steps: 65 | Train Loss: 0.0923047 Vali Loss: 0.1679930 Test Loss: 0.2289653
Validation loss decreased (0.168408 --> 0.167993).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 9.725246906280518
Epoch: 31, Steps: 65 | Train Loss: 0.0916653 Vali Loss: 0.1677177 Test Loss: 0.2286389
Validation loss decreased (0.167993 --> 0.167718).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.884479761123657
Epoch: 32, Steps: 65 | Train Loss: 0.0908928 Vali Loss: 0.1673603 Test Loss: 0.2283361
Validation loss decreased (0.167718 --> 0.167360).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 10.575264692306519
Epoch: 33, Steps: 65 | Train Loss: 0.0903114 Vali Loss: 0.1669701 Test Loss: 0.2280369
Validation loss decreased (0.167360 --> 0.166970).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 11.007773399353027
Epoch: 34, Steps: 65 | Train Loss: 0.0896990 Vali Loss: 0.1665991 Test Loss: 0.2277653
Validation loss decreased (0.166970 --> 0.166599).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 11.190826416015625
Epoch: 35, Steps: 65 | Train Loss: 0.0889588 Vali Loss: 0.1663318 Test Loss: 0.2275073
Validation loss decreased (0.166599 --> 0.166332).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 10.924521446228027
Epoch: 36, Steps: 65 | Train Loss: 0.0886142 Vali Loss: 0.1661752 Test Loss: 0.2272882
Validation loss decreased (0.166332 --> 0.166175).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 10.006034135818481
Epoch: 37, Steps: 65 | Train Loss: 0.0882218 Vali Loss: 0.1659690 Test Loss: 0.2270439
Validation loss decreased (0.166175 --> 0.165969).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 10.528788089752197
Epoch: 38, Steps: 65 | Train Loss: 0.0877504 Vali Loss: 0.1655253 Test Loss: 0.2268444
Validation loss decreased (0.165969 --> 0.165525).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 10.685895442962646
Epoch: 39, Steps: 65 | Train Loss: 0.0869116 Vali Loss: 0.1652015 Test Loss: 0.2266633
Validation loss decreased (0.165525 --> 0.165202).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 10.103803157806396
Epoch: 40, Steps: 65 | Train Loss: 0.0867144 Vali Loss: 0.1650638 Test Loss: 0.2264992
Validation loss decreased (0.165202 --> 0.165064).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 10.286783695220947
Epoch: 41, Steps: 65 | Train Loss: 0.0863530 Vali Loss: 0.1649292 Test Loss: 0.2263218
Validation loss decreased (0.165064 --> 0.164929).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 9.862735986709595
Epoch: 42, Steps: 65 | Train Loss: 0.0861355 Vali Loss: 0.1647319 Test Loss: 0.2261363
Validation loss decreased (0.164929 --> 0.164732).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 10.281137228012085
Epoch: 43, Steps: 65 | Train Loss: 0.0856545 Vali Loss: 0.1645824 Test Loss: 0.2259869
Validation loss decreased (0.164732 --> 0.164582).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 9.739990234375
Epoch: 44, Steps: 65 | Train Loss: 0.0853442 Vali Loss: 0.1644385 Test Loss: 0.2258308
Validation loss decreased (0.164582 --> 0.164439).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 10.019962072372437
Epoch: 45, Steps: 65 | Train Loss: 0.0849583 Vali Loss: 0.1641503 Test Loss: 0.2256892
Validation loss decreased (0.164439 --> 0.164150).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 10.514986038208008
Epoch: 46, Steps: 65 | Train Loss: 0.0848163 Vali Loss: 0.1641271 Test Loss: 0.2255609
Validation loss decreased (0.164150 --> 0.164127).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 11.092939615249634
Epoch: 47, Steps: 65 | Train Loss: 0.0845831 Vali Loss: 0.1639740 Test Loss: 0.2254472
Validation loss decreased (0.164127 --> 0.163974).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 11.411072969436646
Epoch: 48, Steps: 65 | Train Loss: 0.0840946 Vali Loss: 0.1638644 Test Loss: 0.2253387
Validation loss decreased (0.163974 --> 0.163864).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 11.300588846206665
Epoch: 49, Steps: 65 | Train Loss: 0.0841950 Vali Loss: 0.1636688 Test Loss: 0.2252276
Validation loss decreased (0.163864 --> 0.163669).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 10.323041915893555
Epoch: 50, Steps: 65 | Train Loss: 0.0838746 Vali Loss: 0.1635326 Test Loss: 0.2251345
Validation loss decreased (0.163669 --> 0.163533).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=106, out_features=134, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50907136.0
params:  14338.0
Trainable parameters:  14338
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.446158647537231
Epoch: 1, Steps: 65 | Train Loss: 0.3033200 Vali Loss: 0.1560848 Test Loss: 0.2180847
Validation loss decreased (inf --> 0.156085).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.004270553588867
Epoch: 2, Steps: 65 | Train Loss: 0.2969117 Vali Loss: 0.1539375 Test Loss: 0.2157818
Validation loss decreased (0.156085 --> 0.153937).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.822036027908325
Epoch: 3, Steps: 65 | Train Loss: 0.2938580 Vali Loss: 0.1531412 Test Loss: 0.2151930
Validation loss decreased (0.153937 --> 0.153141).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 7.914785146713257
Epoch: 4, Steps: 65 | Train Loss: 0.2929040 Vali Loss: 0.1525397 Test Loss: 0.2146926
Validation loss decreased (0.153141 --> 0.152540).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.727773904800415
Epoch: 5, Steps: 65 | Train Loss: 0.2919702 Vali Loss: 0.1524578 Test Loss: 0.2143007
Validation loss decreased (0.152540 --> 0.152458).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.75576114654541
Epoch: 6, Steps: 65 | Train Loss: 0.2911498 Vali Loss: 0.1521686 Test Loss: 0.2139602
Validation loss decreased (0.152458 --> 0.152169).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.032917976379395
Epoch: 7, Steps: 65 | Train Loss: 0.2910260 Vali Loss: 0.1519378 Test Loss: 0.2140245
Validation loss decreased (0.152169 --> 0.151938).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.227314949035645
Epoch: 8, Steps: 65 | Train Loss: 0.2902390 Vali Loss: 0.1518701 Test Loss: 0.2137818
Validation loss decreased (0.151938 --> 0.151870).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.301182508468628
Epoch: 9, Steps: 65 | Train Loss: 0.2905952 Vali Loss: 0.1518326 Test Loss: 0.2137031
Validation loss decreased (0.151870 --> 0.151833).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.292571306228638
Epoch: 10, Steps: 65 | Train Loss: 0.2904269 Vali Loss: 0.1516878 Test Loss: 0.2135634
Validation loss decreased (0.151833 --> 0.151688).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.469021558761597
Epoch: 11, Steps: 65 | Train Loss: 0.2903013 Vali Loss: 0.1517390 Test Loss: 0.2135886
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.29489016532898
Epoch: 12, Steps: 65 | Train Loss: 0.2901459 Vali Loss: 0.1516706 Test Loss: 0.2134709
Validation loss decreased (0.151688 --> 0.151671).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.420811891555786
Epoch: 13, Steps: 65 | Train Loss: 0.2887975 Vali Loss: 0.1516349 Test Loss: 0.2134970
Validation loss decreased (0.151671 --> 0.151635).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.493202686309814
Epoch: 14, Steps: 65 | Train Loss: 0.2888669 Vali Loss: 0.1516311 Test Loss: 0.2133661
Validation loss decreased (0.151635 --> 0.151631).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.641180753707886
Epoch: 15, Steps: 65 | Train Loss: 0.2895727 Vali Loss: 0.1516350 Test Loss: 0.2132768
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.446763515472412
Epoch: 16, Steps: 65 | Train Loss: 0.2890326 Vali Loss: 0.1517339 Test Loss: 0.2134479
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.512970447540283
Epoch: 17, Steps: 65 | Train Loss: 0.2890697 Vali Loss: 0.1514722 Test Loss: 0.2132646
Validation loss decreased (0.151631 --> 0.151472).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.959913730621338
Epoch: 18, Steps: 65 | Train Loss: 0.2893414 Vali Loss: 0.1516102 Test Loss: 0.2132030
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.927929162979126
Epoch: 19, Steps: 65 | Train Loss: 0.2888836 Vali Loss: 0.1515163 Test Loss: 0.2132856
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.831531286239624
Epoch: 20, Steps: 65 | Train Loss: 0.2891371 Vali Loss: 0.1515082 Test Loss: 0.2131742
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.21717172861099243, mae:0.2916452884674072, rse:0.3772217631340027, corr:[0.5618342  0.5659348  0.56501275 0.5626789  0.56169015 0.56217873
 0.56302124 0.56303227 0.5620899  0.56102735 0.5605446  0.5607067
 0.5611013  0.56112117 0.56059426 0.5597565  0.55905616 0.55865014
 0.55838066 0.55794215 0.5571988  0.5562541  0.5554221  0.55491483
 0.55468345 0.55450875 0.5541342  0.5534626  0.55263263 0.55188745
 0.55136377 0.551102   0.5508688  0.5503816  0.54960907 0.54874176
 0.54792774 0.5472674  0.5467573  0.5463135  0.54581785 0.54520136
 0.54452366 0.5438587  0.54327023 0.5427345  0.5421917  0.54156923
 0.54077595 0.5398419  0.5388946  0.53810877 0.5374971  0.536955
 0.53636295 0.5357415  0.5351235  0.53458023 0.53419405 0.5339579
 0.5337609  0.5335626  0.53332436 0.5330831  0.53288716 0.5328013
 0.5326708  0.53242636 0.53204376 0.53156614 0.5310961  0.5307561
 0.5305533  0.53037816 0.53005815 0.5295726  0.5289864  0.52842396
 0.5279776  0.52764565 0.5272093  0.52656776 0.52574563 0.52489847
 0.5242168  0.5238322  0.52355593 0.52322966 0.5227241  0.5220684
 0.52145106 0.5210364  0.52072334 0.520367   0.519722   0.5186021
 0.517078   0.5154758  0.5140383  0.51286453 0.51184624 0.5107499
 0.50948274 0.50807256 0.5066859  0.50555986 0.50479025 0.5042607
 0.5036032  0.5026514  0.50148284 0.5003133  0.4994714  0.4989213
 0.49839363 0.4977061  0.4968182  0.49580616 0.49494627 0.49427068
 0.49378285 0.4931898  0.49241    0.49140802 0.49030936 0.4893683
 0.48876345 0.48836577 0.48786172 0.48701027 0.4858575  0.48463932
 0.483721   0.48324993 0.48305446 0.48278987 0.48217937 0.48136556
 0.48056346 0.48001605 0.47969505 0.47944725 0.47908854 0.4784267
 0.47756296 0.47663233 0.4759872  0.47549996 0.47496143 0.474099
 0.47305048 0.47218588 0.47167563 0.47139135 0.47108793 0.47063655
 0.4699665  0.46928272 0.46877784 0.4687427  0.46883643 0.4687342
 0.46811622 0.4671186  0.46616283 0.4657482  0.4658915  0.46623203
 0.46631745 0.46602482 0.4654229  0.46499407 0.46506014 0.46545634
 0.4656692  0.46523437 0.46415806 0.46291247 0.4620655  0.46211395
 0.46237966 0.46199462 0.46085417 0.45943254 0.45862037 0.45899543
 0.45986298 0.46000925 0.4586669  0.45657992 0.45664585 0.459913  ]
