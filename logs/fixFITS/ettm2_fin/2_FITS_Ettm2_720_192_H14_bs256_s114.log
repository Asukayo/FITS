Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=122, out_features=154, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  67336192.0
params:  18942.0
Trainable parameters:  18942
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.842689275741577
Epoch: 1, Steps: 65 | Train Loss: 0.3914789 Vali Loss: 0.2518386 Test Loss: 0.3274517
Validation loss decreased (inf --> 0.251839).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.669808387756348
Epoch: 2, Steps: 65 | Train Loss: 0.2998947 Vali Loss: 0.2228036 Test Loss: 0.2898149
Validation loss decreased (0.251839 --> 0.222804).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.004411697387695
Epoch: 3, Steps: 65 | Train Loss: 0.2523773 Vali Loss: 0.2090492 Test Loss: 0.2726027
Validation loss decreased (0.222804 --> 0.209049).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.960344314575195
Epoch: 4, Steps: 65 | Train Loss: 0.2237844 Vali Loss: 0.2019010 Test Loss: 0.2641216
Validation loss decreased (0.209049 --> 0.201901).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.520806312561035
Epoch: 5, Steps: 65 | Train Loss: 0.2042082 Vali Loss: 0.1972404 Test Loss: 0.2589439
Validation loss decreased (0.201901 --> 0.197240).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.556245803833008
Epoch: 6, Steps: 65 | Train Loss: 0.1886897 Vali Loss: 0.1941115 Test Loss: 0.2555250
Validation loss decreased (0.197240 --> 0.194111).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.545483827590942
Epoch: 7, Steps: 65 | Train Loss: 0.1765968 Vali Loss: 0.1915656 Test Loss: 0.2528926
Validation loss decreased (0.194111 --> 0.191566).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.323563814163208
Epoch: 8, Steps: 65 | Train Loss: 0.1664150 Vali Loss: 0.1896715 Test Loss: 0.2508489
Validation loss decreased (0.191566 --> 0.189672).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.284101963043213
Epoch: 9, Steps: 65 | Train Loss: 0.1578862 Vali Loss: 0.1873662 Test Loss: 0.2488618
Validation loss decreased (0.189672 --> 0.187366).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.463055849075317
Epoch: 10, Steps: 65 | Train Loss: 0.1505273 Vali Loss: 0.1860049 Test Loss: 0.2472803
Validation loss decreased (0.187366 --> 0.186005).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.017457962036133
Epoch: 11, Steps: 65 | Train Loss: 0.1445339 Vali Loss: 0.1842859 Test Loss: 0.2458775
Validation loss decreased (0.186005 --> 0.184286).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.496284008026123
Epoch: 12, Steps: 65 | Train Loss: 0.1389377 Vali Loss: 0.1832263 Test Loss: 0.2445322
Validation loss decreased (0.184286 --> 0.183226).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 12.91877555847168
Epoch: 13, Steps: 65 | Train Loss: 0.1343138 Vali Loss: 0.1821053 Test Loss: 0.2433150
Validation loss decreased (0.183226 --> 0.182105).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.805738687515259
Epoch: 14, Steps: 65 | Train Loss: 0.1298130 Vali Loss: 0.1808351 Test Loss: 0.2421820
Validation loss decreased (0.182105 --> 0.180835).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 12.153169393539429
Epoch: 15, Steps: 65 | Train Loss: 0.1263532 Vali Loss: 0.1798317 Test Loss: 0.2411454
Validation loss decreased (0.180835 --> 0.179832).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 12.58675479888916
Epoch: 16, Steps: 65 | Train Loss: 0.1227666 Vali Loss: 0.1787904 Test Loss: 0.2401913
Validation loss decreased (0.179832 --> 0.178790).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.181735038757324
Epoch: 17, Steps: 65 | Train Loss: 0.1197502 Vali Loss: 0.1779796 Test Loss: 0.2392725
Validation loss decreased (0.178790 --> 0.177980).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.40810775756836
Epoch: 18, Steps: 65 | Train Loss: 0.1169088 Vali Loss: 0.1771505 Test Loss: 0.2384375
Validation loss decreased (0.177980 --> 0.177150).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.241214752197266
Epoch: 19, Steps: 65 | Train Loss: 0.1143712 Vali Loss: 0.1763033 Test Loss: 0.2376274
Validation loss decreased (0.177150 --> 0.176303).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.596441745758057
Epoch: 20, Steps: 65 | Train Loss: 0.1121552 Vali Loss: 0.1753701 Test Loss: 0.2369070
Validation loss decreased (0.176303 --> 0.175370).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 12.001919507980347
Epoch: 21, Steps: 65 | Train Loss: 0.1101169 Vali Loss: 0.1748397 Test Loss: 0.2362225
Validation loss decreased (0.175370 --> 0.174840).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.759905338287354
Epoch: 22, Steps: 65 | Train Loss: 0.1080360 Vali Loss: 0.1740739 Test Loss: 0.2354979
Validation loss decreased (0.174840 --> 0.174074).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.206604957580566
Epoch: 23, Steps: 65 | Train Loss: 0.1065181 Vali Loss: 0.1735777 Test Loss: 0.2349449
Validation loss decreased (0.174074 --> 0.173578).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.748381614685059
Epoch: 24, Steps: 65 | Train Loss: 0.1048758 Vali Loss: 0.1731377 Test Loss: 0.2343980
Validation loss decreased (0.173578 --> 0.173138).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 9.952045917510986
Epoch: 25, Steps: 65 | Train Loss: 0.1032548 Vali Loss: 0.1725398 Test Loss: 0.2338448
Validation loss decreased (0.173138 --> 0.172540).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 11.054647207260132
Epoch: 26, Steps: 65 | Train Loss: 0.1019705 Vali Loss: 0.1718266 Test Loss: 0.2333265
Validation loss decreased (0.172540 --> 0.171827).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.119428157806396
Epoch: 27, Steps: 65 | Train Loss: 0.1006710 Vali Loss: 0.1716293 Test Loss: 0.2328924
Validation loss decreased (0.171827 --> 0.171629).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 11.549992084503174
Epoch: 28, Steps: 65 | Train Loss: 0.0998076 Vali Loss: 0.1710137 Test Loss: 0.2324385
Validation loss decreased (0.171629 --> 0.171014).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 12.271268129348755
Epoch: 29, Steps: 65 | Train Loss: 0.0985558 Vali Loss: 0.1706272 Test Loss: 0.2319984
Validation loss decreased (0.171014 --> 0.170627).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 13.016864538192749
Epoch: 30, Steps: 65 | Train Loss: 0.0977631 Vali Loss: 0.1702598 Test Loss: 0.2316384
Validation loss decreased (0.170627 --> 0.170260).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 11.802338123321533
Epoch: 31, Steps: 65 | Train Loss: 0.0967870 Vali Loss: 0.1698245 Test Loss: 0.2312580
Validation loss decreased (0.170260 --> 0.169825).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 10.249970197677612
Epoch: 32, Steps: 65 | Train Loss: 0.0958519 Vali Loss: 0.1694843 Test Loss: 0.2309381
Validation loss decreased (0.169825 --> 0.169484).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 10.410942554473877
Epoch: 33, Steps: 65 | Train Loss: 0.0951228 Vali Loss: 0.1691104 Test Loss: 0.2306148
Validation loss decreased (0.169484 --> 0.169110).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 9.966988563537598
Epoch: 34, Steps: 65 | Train Loss: 0.0943680 Vali Loss: 0.1688983 Test Loss: 0.2302923
Validation loss decreased (0.169110 --> 0.168898).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 9.991413116455078
Epoch: 35, Steps: 65 | Train Loss: 0.0932989 Vali Loss: 0.1686874 Test Loss: 0.2300345
Validation loss decreased (0.168898 --> 0.168687).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 10.93338394165039
Epoch: 36, Steps: 65 | Train Loss: 0.0931696 Vali Loss: 0.1683319 Test Loss: 0.2297690
Validation loss decreased (0.168687 --> 0.168332).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 8.729106903076172
Epoch: 37, Steps: 65 | Train Loss: 0.0924462 Vali Loss: 0.1681008 Test Loss: 0.2295450
Validation loss decreased (0.168332 --> 0.168101).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 11.859117984771729
Epoch: 38, Steps: 65 | Train Loss: 0.0918084 Vali Loss: 0.1677244 Test Loss: 0.2293002
Validation loss decreased (0.168101 --> 0.167724).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 11.13440465927124
Epoch: 39, Steps: 65 | Train Loss: 0.0914194 Vali Loss: 0.1675361 Test Loss: 0.2290617
Validation loss decreased (0.167724 --> 0.167536).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 11.090659618377686
Epoch: 40, Steps: 65 | Train Loss: 0.0908464 Vali Loss: 0.1671423 Test Loss: 0.2288493
Validation loss decreased (0.167536 --> 0.167142).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 11.175124406814575
Epoch: 41, Steps: 65 | Train Loss: 0.0905314 Vali Loss: 0.1671856 Test Loss: 0.2286614
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 11.662419557571411
Epoch: 42, Steps: 65 | Train Loss: 0.0902266 Vali Loss: 0.1668325 Test Loss: 0.2284677
Validation loss decreased (0.167142 --> 0.166832).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 11.35441541671753
Epoch: 43, Steps: 65 | Train Loss: 0.0892971 Vali Loss: 0.1666325 Test Loss: 0.2282608
Validation loss decreased (0.166832 --> 0.166633).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 11.961512327194214
Epoch: 44, Steps: 65 | Train Loss: 0.0892786 Vali Loss: 0.1664527 Test Loss: 0.2281030
Validation loss decreased (0.166633 --> 0.166453).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 8.53865933418274
Epoch: 45, Steps: 65 | Train Loss: 0.0887335 Vali Loss: 0.1663010 Test Loss: 0.2279507
Validation loss decreased (0.166453 --> 0.166301).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 12.329120874404907
Epoch: 46, Steps: 65 | Train Loss: 0.0886767 Vali Loss: 0.1661741 Test Loss: 0.2278023
Validation loss decreased (0.166301 --> 0.166174).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 12.146790742874146
Epoch: 47, Steps: 65 | Train Loss: 0.0881573 Vali Loss: 0.1658929 Test Loss: 0.2276521
Validation loss decreased (0.166174 --> 0.165893).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 12.067126512527466
Epoch: 48, Steps: 65 | Train Loss: 0.0880242 Vali Loss: 0.1657622 Test Loss: 0.2275063
Validation loss decreased (0.165893 --> 0.165762).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 11.5030837059021
Epoch: 49, Steps: 65 | Train Loss: 0.0874540 Vali Loss: 0.1656457 Test Loss: 0.2273967
Validation loss decreased (0.165762 --> 0.165646).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 11.176984071731567
Epoch: 50, Steps: 65 | Train Loss: 0.0873827 Vali Loss: 0.1656132 Test Loss: 0.2272648
Validation loss decreased (0.165646 --> 0.165613).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=122, out_features=154, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  67336192.0
params:  18942.0
Trainable parameters:  18942
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.371740102767944
Epoch: 1, Steps: 65 | Train Loss: 0.3072010 Vali Loss: 0.1569895 Test Loss: 0.2186197
Validation loss decreased (inf --> 0.156989).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.522024154663086
Epoch: 2, Steps: 65 | Train Loss: 0.2985422 Vali Loss: 0.1544881 Test Loss: 0.2165334
Validation loss decreased (0.156989 --> 0.154488).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.590837717056274
Epoch: 3, Steps: 65 | Train Loss: 0.2953785 Vali Loss: 0.1535245 Test Loss: 0.2154710
Validation loss decreased (0.154488 --> 0.153524).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.183054685592651
Epoch: 4, Steps: 65 | Train Loss: 0.2934341 Vali Loss: 0.1529562 Test Loss: 0.2149634
Validation loss decreased (0.153524 --> 0.152956).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.050156593322754
Epoch: 5, Steps: 65 | Train Loss: 0.2921677 Vali Loss: 0.1526878 Test Loss: 0.2145178
Validation loss decreased (0.152956 --> 0.152688).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.575268030166626
Epoch: 6, Steps: 65 | Train Loss: 0.2917658 Vali Loss: 0.1523178 Test Loss: 0.2143626
Validation loss decreased (0.152688 --> 0.152318).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.317591667175293
Epoch: 7, Steps: 65 | Train Loss: 0.2910876 Vali Loss: 0.1521963 Test Loss: 0.2141723
Validation loss decreased (0.152318 --> 0.152196).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.6531662940979
Epoch: 8, Steps: 65 | Train Loss: 0.2902174 Vali Loss: 0.1520475 Test Loss: 0.2138784
Validation loss decreased (0.152196 --> 0.152047).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.269360065460205
Epoch: 9, Steps: 65 | Train Loss: 0.2900391 Vali Loss: 0.1518180 Test Loss: 0.2139230
Validation loss decreased (0.152047 --> 0.151818).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 12.759305715560913
Epoch: 10, Steps: 65 | Train Loss: 0.2903440 Vali Loss: 0.1517806 Test Loss: 0.2136278
Validation loss decreased (0.151818 --> 0.151781).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.6902334690094
Epoch: 11, Steps: 65 | Train Loss: 0.2899756 Vali Loss: 0.1516717 Test Loss: 0.2134905
Validation loss decreased (0.151781 --> 0.151672).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 13.03081727027893
Epoch: 12, Steps: 65 | Train Loss: 0.2887482 Vali Loss: 0.1514903 Test Loss: 0.2135893
Validation loss decreased (0.151672 --> 0.151490).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.99418020248413
Epoch: 13, Steps: 65 | Train Loss: 0.2891612 Vali Loss: 0.1515713 Test Loss: 0.2133800
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 12.165694236755371
Epoch: 14, Steps: 65 | Train Loss: 0.2895034 Vali Loss: 0.1516093 Test Loss: 0.2135129
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.721277475357056
Epoch: 15, Steps: 65 | Train Loss: 0.2896073 Vali Loss: 0.1515491 Test Loss: 0.2133175
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.21748051047325134, mae:0.29198694229125977, rse:0.37748983502388, corr:[0.56202483 0.56663704 0.56478405 0.56261677 0.5626058  0.56370556
 0.5639993  0.56295747 0.56178415 0.5614342  0.56177336 0.56200814
 0.56154805 0.5606331  0.55996263 0.55974805 0.55974287 0.55946136
 0.55870676 0.5577728  0.55708414 0.5567127  0.5564445  0.5559676
 0.55524325 0.5545448  0.55407196 0.55375844 0.55334145 0.5526236
 0.551713   0.5509598  0.5504693  0.5501311  0.54977787 0.5492435
 0.548469   0.5476156  0.54685605 0.5462879  0.5458454  0.5453543
 0.5447555  0.54407984 0.54344136 0.54289967 0.54237765 0.54173464
 0.54089916 0.5399915  0.5392074  0.5386794  0.5382242  0.5375841
 0.5366852  0.5357448  0.5350001  0.5345899  0.53443134 0.5342823
 0.533981   0.5336114  0.5332911  0.5331046  0.5329661  0.5327814
 0.5324407  0.53207964 0.5318518  0.5317747  0.53170353 0.53148484
 0.531069   0.5305615  0.530059   0.5296351  0.5292126  0.528698
 0.52810246 0.5275625  0.52708554 0.5266463  0.5260943  0.52534515
 0.52445614 0.52365655 0.5230445  0.5226828  0.5224237  0.5221068
 0.5217155  0.5212886  0.52075523 0.5201059  0.5192226  0.51799977
 0.5165354  0.5150904  0.5137676  0.5125716  0.51143134 0.5102677
 0.5091488  0.5081321  0.50719476 0.5062406  0.5052021  0.5041236
 0.50299954 0.5019863  0.50118357 0.50046486 0.49972734 0.49880767
 0.49771857 0.4967281  0.49601164 0.49544036 0.4948584  0.49404997
 0.4931924  0.49237955 0.49178293 0.49125767 0.49060374 0.48979327
 0.48899513 0.48831308 0.48765308 0.48681644 0.48575923 0.48460758
 0.48366004 0.483041   0.4825842  0.48195407 0.48094934 0.47985968
 0.47905093 0.47880763 0.4789057  0.4788937  0.4784426  0.47747523
 0.4762973  0.47517455 0.47446078 0.473938   0.4733811  0.4726311
 0.47198358 0.47170466 0.47152334 0.47096792 0.46992248 0.46876702
 0.4679312  0.46770334 0.4677657  0.46778685 0.46731812 0.46656927
 0.46589163 0.4656655  0.46578616 0.46584043 0.46541908 0.46468815
 0.46416745 0.46422943 0.46447945 0.4644362  0.46389198 0.46323234
 0.46304378 0.46328533 0.4633265  0.4625964  0.46108234 0.4599307
 0.45978603 0.46013913 0.45997906 0.45873797 0.45708147 0.45671275
 0.45828792 0.46018696 0.460286   0.4584002  0.45718098 0.45969385]
