Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=90, out_features=114, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  18385920.0
params:  10374.0
Trainable parameters:  10374
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4536015
	speed: 0.1296s/iter; left time: 835.7904s
Epoch: 1 cost time: 16.6456139087677
Epoch: 1, Steps: 131 | Train Loss: 0.4313490 Vali Loss: 0.1834218 Test Loss: 0.2502823
Validation loss decreased (inf --> 0.183422).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2712057
	speed: 0.3293s/iter; left time: 2081.4143s
Epoch: 2 cost time: 16.091530799865723
Epoch: 2, Steps: 131 | Train Loss: 0.3431122 Vali Loss: 0.1695478 Test Loss: 0.2347813
Validation loss decreased (0.183422 --> 0.169548).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2954346
	speed: 0.3535s/iter; left time: 2187.6916s
Epoch: 3 cost time: 16.404967784881592
Epoch: 3, Steps: 131 | Train Loss: 0.3243907 Vali Loss: 0.1640098 Test Loss: 0.2289607
Validation loss decreased (0.169548 --> 0.164010).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2690055
	speed: 0.3509s/iter; left time: 2125.8753s
Epoch: 4 cost time: 16.54904818534851
Epoch: 4, Steps: 131 | Train Loss: 0.3151338 Vali Loss: 0.1609951 Test Loss: 0.2255940
Validation loss decreased (0.164010 --> 0.160995).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3117573
	speed: 0.3366s/iter; left time: 1995.1949s
Epoch: 5 cost time: 15.948870182037354
Epoch: 5, Steps: 131 | Train Loss: 0.3096051 Vali Loss: 0.1590635 Test Loss: 0.2237962
Validation loss decreased (0.160995 --> 0.159064).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3437100
	speed: 0.3098s/iter; left time: 1795.8262s
Epoch: 6 cost time: 15.284168481826782
Epoch: 6, Steps: 131 | Train Loss: 0.3051032 Vali Loss: 0.1576582 Test Loss: 0.2221319
Validation loss decreased (0.159064 --> 0.157658).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2293980
	speed: 0.3212s/iter; left time: 1819.4292s
Epoch: 7 cost time: 15.311494588851929
Epoch: 7, Steps: 131 | Train Loss: 0.3029830 Vali Loss: 0.1566713 Test Loss: 0.2210755
Validation loss decreased (0.157658 --> 0.156671).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3022947
	speed: 0.3293s/iter; left time: 1822.0811s
Epoch: 8 cost time: 15.874294757843018
Epoch: 8, Steps: 131 | Train Loss: 0.3006452 Vali Loss: 0.1558553 Test Loss: 0.2201355
Validation loss decreased (0.156671 --> 0.155855).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3073367
	speed: 0.3343s/iter; left time: 1806.2428s
Epoch: 9 cost time: 15.622636318206787
Epoch: 9, Steps: 131 | Train Loss: 0.2988831 Vali Loss: 0.1553302 Test Loss: 0.2195031
Validation loss decreased (0.155855 --> 0.155330).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3274321
	speed: 0.3240s/iter; left time: 1708.3316s
Epoch: 10 cost time: 15.484787464141846
Epoch: 10, Steps: 131 | Train Loss: 0.2977766 Vali Loss: 0.1550381 Test Loss: 0.2191402
Validation loss decreased (0.155330 --> 0.155038).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2968929
	speed: 0.3372s/iter; left time: 1733.5093s
Epoch: 11 cost time: 17.013607025146484
Epoch: 11, Steps: 131 | Train Loss: 0.2966875 Vali Loss: 0.1547396 Test Loss: 0.2187834
Validation loss decreased (0.155038 --> 0.154740).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3088146
	speed: 0.3514s/iter; left time: 1760.2715s
Epoch: 12 cost time: 17.084641695022583
Epoch: 12, Steps: 131 | Train Loss: 0.2959588 Vali Loss: 0.1543094 Test Loss: 0.2182789
Validation loss decreased (0.154740 --> 0.154309).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2575597
	speed: 0.3427s/iter; left time: 1672.2237s
Epoch: 13 cost time: 16.654117822647095
Epoch: 13, Steps: 131 | Train Loss: 0.2953224 Vali Loss: 0.1542480 Test Loss: 0.2181736
Validation loss decreased (0.154309 --> 0.154248).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4035671
	speed: 0.3549s/iter; left time: 1685.0387s
Epoch: 14 cost time: 17.395046949386597
Epoch: 14, Steps: 131 | Train Loss: 0.2950665 Vali Loss: 0.1539873 Test Loss: 0.2179349
Validation loss decreased (0.154248 --> 0.153987).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3553649
	speed: 0.3650s/iter; left time: 1685.3595s
Epoch: 15 cost time: 18.557523488998413
Epoch: 15, Steps: 131 | Train Loss: 0.2941824 Vali Loss: 0.1538196 Test Loss: 0.2175825
Validation loss decreased (0.153987 --> 0.153820).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2767848
	speed: 0.3391s/iter; left time: 1521.3366s
Epoch: 16 cost time: 15.980234146118164
Epoch: 16, Steps: 131 | Train Loss: 0.2942665 Vali Loss: 0.1535991 Test Loss: 0.2174019
Validation loss decreased (0.153820 --> 0.153599).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2528856
	speed: 0.3209s/iter; left time: 1397.6532s
Epoch: 17 cost time: 15.933455228805542
Epoch: 17, Steps: 131 | Train Loss: 0.2933928 Vali Loss: 0.1536733 Test Loss: 0.2173941
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3117330
	speed: 0.3407s/iter; left time: 1439.2524s
Epoch: 18 cost time: 16.338950872421265
Epoch: 18, Steps: 131 | Train Loss: 0.2929531 Vali Loss: 0.1534059 Test Loss: 0.2171951
Validation loss decreased (0.153599 --> 0.153406).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2814660
	speed: 0.3569s/iter; left time: 1460.8913s
Epoch: 19 cost time: 16.648391485214233
Epoch: 19, Steps: 131 | Train Loss: 0.2931685 Vali Loss: 0.1534106 Test Loss: 0.2171000
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3698473
	speed: 0.3366s/iter; left time: 1333.6537s
Epoch: 20 cost time: 16.675023555755615
Epoch: 20, Steps: 131 | Train Loss: 0.2927476 Vali Loss: 0.1533195 Test Loss: 0.2170442
Validation loss decreased (0.153406 --> 0.153320).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2806034
	speed: 0.3346s/iter; left time: 1281.7364s
Epoch: 21 cost time: 16.335639238357544
Epoch: 21, Steps: 131 | Train Loss: 0.2927696 Vali Loss: 0.1532405 Test Loss: 0.2169656
Validation loss decreased (0.153320 --> 0.153241).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2866462
	speed: 0.3456s/iter; left time: 1278.6938s
Epoch: 22 cost time: 16.37333035469055
Epoch: 22, Steps: 131 | Train Loss: 0.2925227 Vali Loss: 0.1531775 Test Loss: 0.2168895
Validation loss decreased (0.153241 --> 0.153178).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2837238
	speed: 0.3577s/iter; left time: 1276.5062s
Epoch: 23 cost time: 16.603609561920166
Epoch: 23, Steps: 131 | Train Loss: 0.2923310 Vali Loss: 0.1529885 Test Loss: 0.2168679
Validation loss decreased (0.153178 --> 0.152988).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3643152
	speed: 0.3390s/iter; left time: 1165.5518s
Epoch: 24 cost time: 16.163358211517334
Epoch: 24, Steps: 131 | Train Loss: 0.2915813 Vali Loss: 0.1530781 Test Loss: 0.2167889
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3666135
	speed: 0.3293s/iter; left time: 1089.0209s
Epoch: 25 cost time: 16.02799701690674
Epoch: 25, Steps: 131 | Train Loss: 0.2920806 Vali Loss: 0.1531186 Test Loss: 0.2166910
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2795762
	speed: 0.3153s/iter; left time: 1001.4708s
Epoch: 26 cost time: 16.347723484039307
Epoch: 26, Steps: 131 | Train Loss: 0.2920935 Vali Loss: 0.1530683 Test Loss: 0.2166488
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.21856346726417542, mae:0.2928491234779358, rse:0.37842854857444763, corr:[0.55213875 0.561729   0.56284195 0.56023383 0.55843216 0.55850595
 0.55986726 0.5612742  0.56167245 0.5610471  0.5601425  0.55962455
 0.55971634 0.5601964  0.5606249  0.5604975  0.5597574  0.55871284
 0.5577606  0.5570989  0.5567698  0.55662507 0.5564474  0.55604464
 0.5554062  0.5546937  0.554055   0.55353034 0.5530943  0.5527
 0.5522699  0.5518067  0.55129516 0.5507145  0.55008847 0.5494736
 0.5488803  0.54831076 0.5477218  0.54709476 0.54645044 0.54580015
 0.54515266 0.5445111  0.5439096  0.5433435  0.54277295 0.54215944
 0.5414365  0.54059815 0.5396998  0.5388626  0.5381402  0.5375491
 0.53703463 0.5365595  0.53604984 0.5354833  0.53493196 0.534494
 0.53421515 0.53409153 0.5340067  0.5338479  0.53353375 0.53313047
 0.5327021  0.53240764 0.5323066  0.5322862  0.5321795  0.5318958
 0.53141725 0.5308165  0.5302001  0.5297128  0.5293686  0.52905214
 0.5286173  0.5280112  0.5272016  0.52631706 0.5255409  0.5250091
 0.5246967  0.5244858  0.52413505 0.5235919  0.52290213 0.5222153
 0.521707   0.5214207  0.52113825 0.5206571  0.5197822  0.5184151
 0.5166911  0.51497823 0.51356786 0.5125607  0.5118005  0.5109772
 0.5098756  0.5084     0.5066367  0.5049176  0.50368345 0.50315624
 0.50302166 0.502817   0.50220144 0.5010566  0.49962077 0.4982138
 0.49719462 0.49673784 0.4966358  0.4964193  0.49579683 0.4946056
 0.4931086  0.49168518 0.49084145 0.49063814 0.49070102 0.49053156
 0.48982683 0.48853576 0.4868775  0.48529205 0.48424587 0.48385486
 0.4838718  0.48383722 0.4834171  0.48251402 0.48126727 0.48014796
 0.4794602  0.47924426 0.47918186 0.47892627 0.4782773  0.4771812
 0.47591907 0.4748239  0.47431394 0.47422084 0.47415492 0.47360796
 0.47244263 0.4708999  0.46942782 0.46851015 0.46835265 0.46870172
 0.46895236 0.46868616 0.4676935  0.46635443 0.46511233 0.46458465
 0.46479213 0.46531472 0.46555918 0.4652018  0.4641981  0.46298894
 0.4623067  0.46264803 0.46356383 0.46423954 0.4639975  0.46271715
 0.46091148 0.4595258  0.45940426 0.46041435 0.46131858 0.4611794
 0.45938104 0.4562247  0.4532903  0.45242336 0.45395803 0.45606753
 0.45656034 0.45460552 0.45076835 0.44860283 0.4531599  0.4589984 ]
