Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21288960.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2822684
	speed: 0.2400s/iter; left time: 1536.4529s
Epoch: 1 cost time: 30.050999641418457
Epoch: 1, Steps: 130 | Train Loss: 0.3889321 Vali Loss: 0.2638880 Test Loss: 0.3528505
Validation loss decreased (inf --> 0.263888).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2580971
	speed: 0.5683s/iter; left time: 3563.7117s
Epoch: 2 cost time: 26.0020751953125
Epoch: 2, Steps: 130 | Train Loss: 0.2727834 Vali Loss: 0.2382954 Test Loss: 0.3178762
Validation loss decreased (0.263888 --> 0.238295).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2565561
	speed: 0.5413s/iter; left time: 3324.3597s
Epoch: 3 cost time: 25.160711765289307
Epoch: 3, Steps: 130 | Train Loss: 0.2258309 Vali Loss: 0.2278499 Test Loss: 0.3040038
Validation loss decreased (0.238295 --> 0.227850).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2427140
	speed: 0.5838s/iter; left time: 3509.1263s
Epoch: 4 cost time: 26.095545768737793
Epoch: 4, Steps: 130 | Train Loss: 0.1996748 Vali Loss: 0.2209637 Test Loss: 0.2961075
Validation loss decreased (0.227850 --> 0.220964).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1570414
	speed: 0.5425s/iter; left time: 3190.2421s
Epoch: 5 cost time: 29.422696590423584
Epoch: 5, Steps: 130 | Train Loss: 0.1830408 Vali Loss: 0.2162118 Test Loss: 0.2908919
Validation loss decreased (0.220964 --> 0.216212).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1481338
	speed: 0.6178s/iter; left time: 3553.1879s
Epoch: 6 cost time: 29.555548429489136
Epoch: 6, Steps: 130 | Train Loss: 0.1705647 Vali Loss: 0.2129014 Test Loss: 0.2872439
Validation loss decreased (0.216212 --> 0.212901).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1624729
	speed: 0.6244s/iter; left time: 3509.8689s
Epoch: 7 cost time: 30.184672832489014
Epoch: 7, Steps: 130 | Train Loss: 0.1619046 Vali Loss: 0.2100230 Test Loss: 0.2844156
Validation loss decreased (0.212901 --> 0.210023).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1618569
	speed: 0.5919s/iter; left time: 3250.2537s
Epoch: 8 cost time: 29.45241928100586
Epoch: 8, Steps: 130 | Train Loss: 0.1554889 Vali Loss: 0.2077608 Test Loss: 0.2819424
Validation loss decreased (0.210023 --> 0.207761).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1685504
	speed: 0.6226s/iter; left time: 3337.7251s
Epoch: 9 cost time: 29.756202459335327
Epoch: 9, Steps: 130 | Train Loss: 0.1503506 Vali Loss: 0.2053152 Test Loss: 0.2801225
Validation loss decreased (0.207761 --> 0.205315).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1518748
	speed: 0.6179s/iter; left time: 3232.3801s
Epoch: 10 cost time: 28.59285068511963
Epoch: 10, Steps: 130 | Train Loss: 0.1464747 Vali Loss: 0.2042990 Test Loss: 0.2787146
Validation loss decreased (0.205315 --> 0.204299).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1573065
	speed: 0.5819s/iter; left time: 2968.4803s
Epoch: 11 cost time: 27.240352869033813
Epoch: 11, Steps: 130 | Train Loss: 0.1435439 Vali Loss: 0.2034518 Test Loss: 0.2775406
Validation loss decreased (0.204299 --> 0.203452).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1697280
	speed: 0.6005s/iter; left time: 2985.1744s
Epoch: 12 cost time: 28.899352550506592
Epoch: 12, Steps: 130 | Train Loss: 0.1412409 Vali Loss: 0.2022066 Test Loss: 0.2766607
Validation loss decreased (0.203452 --> 0.202207).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1171674
	speed: 0.6025s/iter; left time: 2916.8384s
Epoch: 13 cost time: 29.521930694580078
Epoch: 13, Steps: 130 | Train Loss: 0.1395908 Vali Loss: 0.2014373 Test Loss: 0.2759421
Validation loss decreased (0.202207 --> 0.201437).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1161737
	speed: 0.6174s/iter; left time: 2908.5828s
Epoch: 14 cost time: 28.846582889556885
Epoch: 14, Steps: 130 | Train Loss: 0.1383559 Vali Loss: 0.2008977 Test Loss: 0.2753337
Validation loss decreased (0.201437 --> 0.200898).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1749544
	speed: 0.5846s/iter; left time: 2677.9300s
Epoch: 15 cost time: 27.491309642791748
Epoch: 15, Steps: 130 | Train Loss: 0.1372412 Vali Loss: 0.2003614 Test Loss: 0.2749661
Validation loss decreased (0.200898 --> 0.200361).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1318355
	speed: 0.5471s/iter; left time: 2435.2178s
Epoch: 16 cost time: 26.714533805847168
Epoch: 16, Steps: 130 | Train Loss: 0.1360035 Vali Loss: 0.1998135 Test Loss: 0.2744785
Validation loss decreased (0.200361 --> 0.199813).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1707418
	speed: 0.5739s/iter; left time: 2480.0258s
Epoch: 17 cost time: 27.810057163238525
Epoch: 17, Steps: 130 | Train Loss: 0.1355372 Vali Loss: 0.1992919 Test Loss: 0.2742544
Validation loss decreased (0.199813 --> 0.199292).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1065916
	speed: 0.5806s/iter; left time: 2433.3821s
Epoch: 18 cost time: 29.514009952545166
Epoch: 18, Steps: 130 | Train Loss: 0.1350189 Vali Loss: 0.1995079 Test Loss: 0.2739680
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1065381
	speed: 0.6004s/iter; left time: 2438.4152s
Epoch: 19 cost time: 29.019391775131226
Epoch: 19, Steps: 130 | Train Loss: 0.1343628 Vali Loss: 0.1987604 Test Loss: 0.2737714
Validation loss decreased (0.199292 --> 0.198760).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1304506
	speed: 0.6381s/iter; left time: 2508.3025s
Epoch: 20 cost time: 30.717915773391724
Epoch: 20, Steps: 130 | Train Loss: 0.1338084 Vali Loss: 0.1982964 Test Loss: 0.2735676
Validation loss decreased (0.198760 --> 0.198296).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1777862
	speed: 0.6033s/iter; left time: 2293.1761s
Epoch: 21 cost time: 28.519217252731323
Epoch: 21, Steps: 130 | Train Loss: 0.1336877 Vali Loss: 0.1981529 Test Loss: 0.2734458
Validation loss decreased (0.198296 --> 0.198153).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1120854
	speed: 0.5764s/iter; left time: 2116.0316s
Epoch: 22 cost time: 27.202960729599
Epoch: 22, Steps: 130 | Train Loss: 0.1334963 Vali Loss: 0.1983131 Test Loss: 0.2733513
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1238422
	speed: 0.5921s/iter; left time: 2096.5616s
Epoch: 23 cost time: 29.563950061798096
Epoch: 23, Steps: 130 | Train Loss: 0.1334536 Vali Loss: 0.1978126 Test Loss: 0.2732911
Validation loss decreased (0.198153 --> 0.197813).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1445009
	speed: 0.5863s/iter; left time: 2000.0179s
Epoch: 24 cost time: 27.971426486968994
Epoch: 24, Steps: 130 | Train Loss: 0.1333242 Vali Loss: 0.1978815 Test Loss: 0.2731668
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1263809
	speed: 0.5750s/iter; left time: 1886.6887s
Epoch: 25 cost time: 28.520652532577515
Epoch: 25, Steps: 130 | Train Loss: 0.1329859 Vali Loss: 0.1976597 Test Loss: 0.2731566
Validation loss decreased (0.197813 --> 0.197660).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1423010
	speed: 0.5786s/iter; left time: 1823.1943s
Epoch: 26 cost time: 27.746692657470703
Epoch: 26, Steps: 130 | Train Loss: 0.1327692 Vali Loss: 0.1977193 Test Loss: 0.2730636
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1039195
	speed: 0.5580s/iter; left time: 1685.7384s
Epoch: 27 cost time: 25.671509742736816
Epoch: 27, Steps: 130 | Train Loss: 0.1324037 Vali Loss: 0.1977705 Test Loss: 0.2729836
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1277509
	speed: 0.5438s/iter; left time: 1572.1375s
Epoch: 28 cost time: 27.505000591278076
Epoch: 28, Steps: 130 | Train Loss: 0.1326716 Vali Loss: 0.1974842 Test Loss: 0.2729063
Validation loss decreased (0.197660 --> 0.197484).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1401625
	speed: 0.5991s/iter; left time: 1654.2407s
Epoch: 29 cost time: 29.248892784118652
Epoch: 29, Steps: 130 | Train Loss: 0.1326046 Vali Loss: 0.1973851 Test Loss: 0.2728756
Validation loss decreased (0.197484 --> 0.197385).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1349763
	speed: 0.6560s/iter; left time: 1725.9725s
Epoch: 30 cost time: 34.65085172653198
Epoch: 30, Steps: 130 | Train Loss: 0.1323235 Vali Loss: 0.1972755 Test Loss: 0.2728447
Validation loss decreased (0.197385 --> 0.197275).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1379496
	speed: 0.6342s/iter; left time: 1586.1238s
Epoch: 31 cost time: 29.397382736206055
Epoch: 31, Steps: 130 | Train Loss: 0.1324793 Vali Loss: 0.1973184 Test Loss: 0.2728785
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1121900
	speed: 0.5970s/iter; left time: 1415.5927s
Epoch: 32 cost time: 27.706379175186157
Epoch: 32, Steps: 130 | Train Loss: 0.1324570 Vali Loss: 0.1976079 Test Loss: 0.2728579
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1320612
	speed: 0.5658s/iter; left time: 1267.9890s
Epoch: 33 cost time: 26.542903661727905
Epoch: 33, Steps: 130 | Train Loss: 0.1323201 Vali Loss: 0.1971903 Test Loss: 0.2728418
Validation loss decreased (0.197275 --> 0.197190).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1354666
	speed: 0.5700s/iter; left time: 1203.2648s
Epoch: 34 cost time: 27.66985774040222
Epoch: 34, Steps: 130 | Train Loss: 0.1324725 Vali Loss: 0.1973433 Test Loss: 0.2727784
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1071348
	speed: 0.5481s/iter; left time: 1085.8691s
Epoch: 35 cost time: 25.7465558052063
Epoch: 35, Steps: 130 | Train Loss: 0.1323666 Vali Loss: 0.1968598 Test Loss: 0.2727771
Validation loss decreased (0.197190 --> 0.196860).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1127446
	speed: 0.4708s/iter; left time: 871.4629s
Epoch: 36 cost time: 20.780524969100952
Epoch: 36, Steps: 130 | Train Loss: 0.1323841 Vali Loss: 0.1971770 Test Loss: 0.2727908
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1334065
	speed: 0.3739s/iter; left time: 643.4150s
Epoch: 37 cost time: 18.392661809921265
Epoch: 37, Steps: 130 | Train Loss: 0.1319358 Vali Loss: 0.1969877 Test Loss: 0.2727517
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1432227
	speed: 0.3673s/iter; left time: 584.3759s
Epoch: 38 cost time: 17.814794063568115
Epoch: 38, Steps: 130 | Train Loss: 0.1320841 Vali Loss: 0.1968822 Test Loss: 0.2727492
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21288960.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4840488
	speed: 0.1513s/iter; left time: 968.7018s
Epoch: 1 cost time: 18.303279399871826
Epoch: 1, Steps: 130 | Train Loss: 0.3809470 Vali Loss: 0.1952698 Test Loss: 0.2704835
Validation loss decreased (inf --> 0.195270).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2894745
	speed: 0.3667s/iter; left time: 2299.3295s
Epoch: 2 cost time: 17.275354623794556
Epoch: 2, Steps: 130 | Train Loss: 0.3786123 Vali Loss: 0.1945820 Test Loss: 0.2699755
Validation loss decreased (0.195270 --> 0.194582).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3598353
	speed: 0.3651s/iter; left time: 2241.8879s
Epoch: 3 cost time: 18.735206365585327
Epoch: 3, Steps: 130 | Train Loss: 0.3785802 Vali Loss: 0.1944675 Test Loss: 0.2693651
Validation loss decreased (0.194582 --> 0.194467).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4189802
	speed: 0.4208s/iter; left time: 2529.6877s
Epoch: 4 cost time: 20.890745878219604
Epoch: 4, Steps: 130 | Train Loss: 0.3775327 Vali Loss: 0.1938188 Test Loss: 0.2691782
Validation loss decreased (0.194467 --> 0.193819).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4826128
	speed: 0.3967s/iter; left time: 2332.9938s
Epoch: 5 cost time: 18.929551124572754
Epoch: 5, Steps: 130 | Train Loss: 0.3770638 Vali Loss: 0.1936448 Test Loss: 0.2687477
Validation loss decreased (0.193819 --> 0.193645).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3787967
	speed: 0.5441s/iter; left time: 3129.2488s
Epoch: 6 cost time: 28.82249402999878
Epoch: 6, Steps: 130 | Train Loss: 0.3759681 Vali Loss: 0.1937063 Test Loss: 0.2687933
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3793978
	speed: 0.5542s/iter; left time: 3114.9055s
Epoch: 7 cost time: 26.386082887649536
Epoch: 7, Steps: 130 | Train Loss: 0.3765184 Vali Loss: 0.1934770 Test Loss: 0.2685617
Validation loss decreased (0.193645 --> 0.193477).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3313604
	speed: 0.5715s/iter; left time: 3138.0131s
Epoch: 8 cost time: 26.685773611068726
Epoch: 8, Steps: 130 | Train Loss: 0.3759803 Vali Loss: 0.1933188 Test Loss: 0.2685235
Validation loss decreased (0.193477 --> 0.193319).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4028618
	speed: 0.5964s/iter; left time: 3197.4190s
Epoch: 9 cost time: 27.746411561965942
Epoch: 9, Steps: 130 | Train Loss: 0.3756660 Vali Loss: 0.1931945 Test Loss: 0.2683196
Validation loss decreased (0.193319 --> 0.193194).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4006922
	speed: 0.5214s/iter; left time: 2727.5372s
Epoch: 10 cost time: 27.931963682174683
Epoch: 10, Steps: 130 | Train Loss: 0.3760192 Vali Loss: 0.1934957 Test Loss: 0.2682559
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4033144
	speed: 0.5635s/iter; left time: 2874.6108s
Epoch: 11 cost time: 27.02916193008423
Epoch: 11, Steps: 130 | Train Loss: 0.3756914 Vali Loss: 0.1929593 Test Loss: 0.2683553
Validation loss decreased (0.193194 --> 0.192959).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3478961
	speed: 0.5553s/iter; left time: 2760.2331s
Epoch: 12 cost time: 26.280351400375366
Epoch: 12, Steps: 130 | Train Loss: 0.3755165 Vali Loss: 0.1929123 Test Loss: 0.2683771
Validation loss decreased (0.192959 --> 0.192912).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3834249
	speed: 0.5421s/iter; left time: 2624.3490s
Epoch: 13 cost time: 26.278377294540405
Epoch: 13, Steps: 130 | Train Loss: 0.3753275 Vali Loss: 0.1932923 Test Loss: 0.2681622
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4322167
	speed: 0.5698s/iter; left time: 2684.1666s
Epoch: 14 cost time: 28.16736102104187
Epoch: 14, Steps: 130 | Train Loss: 0.3747112 Vali Loss: 0.1930896 Test Loss: 0.2681579
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4559270
	speed: 0.5621s/iter; left time: 2574.9507s
Epoch: 15 cost time: 26.6772038936615
Epoch: 15, Steps: 130 | Train Loss: 0.3751195 Vali Loss: 0.1935245 Test Loss: 0.2684022
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.26901698112487793, mae:0.32663020491600037, rse:0.4189387261867523, corr:[0.55467373 0.55898964 0.55902916 0.5567306  0.55477566 0.5541467
 0.5545969  0.55533594 0.5555989  0.555129   0.5542642  0.5535387
 0.5532766  0.553414   0.55365443 0.55355597 0.55291957 0.55189335
 0.55079746 0.54992646 0.549424   0.54920655 0.54906756 0.54878056
 0.5482041  0.5474003  0.54655415 0.54582876 0.54529613 0.54493135
 0.5446118  0.54425544 0.543761   0.543102   0.54235786 0.5416589
 0.5410474  0.5405349  0.54005295 0.5395239  0.5389137  0.53823036
 0.53753227 0.53686017 0.5362404  0.53566474 0.53511316 0.5345401
 0.53385586 0.5330478  0.5321896  0.5314075  0.53073937 0.5301959
 0.52973413 0.52930665 0.52884    0.5283334  0.52785105 0.5274645
 0.52720726 0.527094   0.5270421  0.52693224 0.5267092  0.52641886
 0.5261077  0.52587306 0.52575165 0.52569145 0.52558076 0.5253423
 0.524947   0.5244414  0.5238813  0.5233605  0.5229045  0.52247936
 0.52200776 0.52146816 0.52079713 0.5200642  0.5193542  0.5187676
 0.5183604  0.5180767  0.5177744  0.51736933 0.5168221  0.5161501
 0.5154123  0.51465553 0.5138733  0.51305306 0.5121317  0.5110271
 0.5097612  0.5084762  0.5072274  0.5060708  0.5050318  0.5040718
 0.50312215 0.5020786  0.5009282  0.499684   0.49848673 0.49747062
 0.49665105 0.4959515  0.49526027 0.49446544 0.4935773  0.49259487
 0.4915625  0.49060693 0.48980373 0.48907238 0.48833406 0.48744822
 0.48644388 0.48536715 0.48440158 0.48365417 0.48311657 0.48268858
 0.48223835 0.48160943 0.4807229  0.4796419  0.47853696 0.4775834
 0.47690752 0.47647852 0.47614726 0.47571558 0.47503838 0.4741538
 0.47317553 0.4723083  0.47163567 0.4711419  0.47070628 0.4701203
 0.46930546 0.46825537 0.46722865 0.46639708 0.4658564  0.4654851
 0.46511984 0.46460637 0.46380198 0.4627612  0.46171394 0.4609527
 0.4605761  0.46048254 0.46041504 0.4602021  0.4596995  0.4589738
 0.45821372 0.45770487 0.4575956  0.4578253  0.45810455 0.45817524
 0.45793596 0.45745364 0.45679682 0.4561572  0.4556775  0.45537603
 0.4551493  0.45486322 0.45443064 0.453881   0.45327008 0.45278582
 0.4524186  0.452066   0.45165348 0.45106333 0.4502456  0.44929007
 0.44842502 0.4478454  0.4475305  0.44724724 0.4466913  0.44563884
 0.44408995 0.44232973 0.44055837 0.43902102 0.43793896 0.43722513
 0.436656   0.43592918 0.43488157 0.4334969  0.4319635  0.43054655
 0.4294607  0.4286933  0.42801824 0.4272066  0.42620054 0.42509574
 0.42391172 0.4227993  0.42190734 0.42127284 0.42083263 0.42032775
 0.41957274 0.41849345 0.41727957 0.41611448 0.41507757 0.4142074
 0.41341826 0.41259265 0.4116737  0.41053557 0.40928057 0.40813717
 0.40716517 0.40647346 0.40594319 0.40548542 0.4049575  0.40432248
 0.40357384 0.40283874 0.4021476  0.40152657 0.4009626  0.40037748
 0.39977434 0.39912218 0.39856255 0.39831275 0.3982622  0.39833048
 0.3983932  0.39835948 0.39815345 0.39780268 0.39755106 0.39747688
 0.39752918 0.39764065 0.3975543  0.3972213  0.39667144 0.39611986
 0.39575386 0.3956317  0.39566836 0.39570364 0.39555535 0.39522907
 0.394801   0.39438865 0.3942046  0.3942789  0.39442456 0.394392
 0.39406285 0.39353985 0.39293596 0.39251468 0.39238676 0.39250186
 0.3927615  0.3927908  0.39238402 0.39165106 0.3907952  0.39015147
 0.38985175 0.38983223 0.38981238 0.3894395  0.38851154 0.38698417
 0.38526025 0.38401788 0.38348612 0.38348898 0.38362822 0.3834271
 0.38278595 0.38176122 0.38063258 0.37980995 0.37964985 0.37989756
 0.38026375 0.38023886 0.37963104 0.37837008 0.37688738 0.37577847
 0.37549222 0.37581217 0.3763292  0.3765665  0.37624672 0.3756417
 0.37495112 0.37463766 0.37479022 0.37514654 0.37535074 0.37516844
 0.37453905 0.37385157 0.37361017 0.37405545 0.3749056  0.3756884
 0.37584907 0.37518877 0.37399298 0.37281284 0.3724932  0.37316766
 0.37443337 0.37512386 0.3741648  0.37127346 0.36758697 0.36467355]
