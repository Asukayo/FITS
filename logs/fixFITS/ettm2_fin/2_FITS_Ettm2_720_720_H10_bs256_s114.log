Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  58060800.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 14.261602878570557
Epoch: 1, Steps: 64 | Train Loss: 0.5401497 Vali Loss: 0.3535471 Test Loss: 0.4891238
Validation loss decreased (inf --> 0.353547).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 15.372457027435303
Epoch: 2, Steps: 64 | Train Loss: 0.4318345 Vali Loss: 0.3210282 Test Loss: 0.4463258
Validation loss decreased (0.353547 --> 0.321028).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 15.494408130645752
Epoch: 3, Steps: 64 | Train Loss: 0.3803172 Vali Loss: 0.3052698 Test Loss: 0.4253822
Validation loss decreased (0.321028 --> 0.305270).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 14.898322105407715
Epoch: 4, Steps: 64 | Train Loss: 0.3517477 Vali Loss: 0.2960852 Test Loss: 0.4141432
Validation loss decreased (0.305270 --> 0.296085).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 14.380528688430786
Epoch: 5, Steps: 64 | Train Loss: 0.3342522 Vali Loss: 0.2902977 Test Loss: 0.4073029
Validation loss decreased (0.296085 --> 0.290298).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 14.950231552124023
Epoch: 6, Steps: 64 | Train Loss: 0.3215082 Vali Loss: 0.2864582 Test Loss: 0.4026368
Validation loss decreased (0.290298 --> 0.286458).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 14.349551677703857
Epoch: 7, Steps: 64 | Train Loss: 0.3127204 Vali Loss: 0.2838277 Test Loss: 0.3993196
Validation loss decreased (0.286458 --> 0.283828).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 13.686531782150269
Epoch: 8, Steps: 64 | Train Loss: 0.3055392 Vali Loss: 0.2817847 Test Loss: 0.3965765
Validation loss decreased (0.283828 --> 0.281785).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 14.367575407028198
Epoch: 9, Steps: 64 | Train Loss: 0.2993566 Vali Loss: 0.2799288 Test Loss: 0.3943784
Validation loss decreased (0.281785 --> 0.279929).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 13.898725271224976
Epoch: 10, Steps: 64 | Train Loss: 0.2948418 Vali Loss: 0.2787499 Test Loss: 0.3925219
Validation loss decreased (0.279929 --> 0.278750).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 14.381637811660767
Epoch: 11, Steps: 64 | Train Loss: 0.2909749 Vali Loss: 0.2771829 Test Loss: 0.3908582
Validation loss decreased (0.278750 --> 0.277183).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 13.37173843383789
Epoch: 12, Steps: 64 | Train Loss: 0.2872650 Vali Loss: 0.2761400 Test Loss: 0.3894508
Validation loss decreased (0.277183 --> 0.276140).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 13.594534635543823
Epoch: 13, Steps: 64 | Train Loss: 0.2841961 Vali Loss: 0.2754271 Test Loss: 0.3881786
Validation loss decreased (0.276140 --> 0.275427).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 13.890453815460205
Epoch: 14, Steps: 64 | Train Loss: 0.2811539 Vali Loss: 0.2745502 Test Loss: 0.3870348
Validation loss decreased (0.275427 --> 0.274550).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 13.194649934768677
Epoch: 15, Steps: 64 | Train Loss: 0.2790151 Vali Loss: 0.2735439 Test Loss: 0.3860206
Validation loss decreased (0.274550 --> 0.273544).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 13.98320484161377
Epoch: 16, Steps: 64 | Train Loss: 0.2772297 Vali Loss: 0.2730080 Test Loss: 0.3851210
Validation loss decreased (0.273544 --> 0.273008).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 14.374172925949097
Epoch: 17, Steps: 64 | Train Loss: 0.2755851 Vali Loss: 0.2724075 Test Loss: 0.3842774
Validation loss decreased (0.273008 --> 0.272408).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 15.499666690826416
Epoch: 18, Steps: 64 | Train Loss: 0.2739062 Vali Loss: 0.2718611 Test Loss: 0.3835267
Validation loss decreased (0.272408 --> 0.271861).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 14.593286752700806
Epoch: 19, Steps: 64 | Train Loss: 0.2726992 Vali Loss: 0.2715218 Test Loss: 0.3828552
Validation loss decreased (0.271861 --> 0.271522).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 14.613942861557007
Epoch: 20, Steps: 64 | Train Loss: 0.2717417 Vali Loss: 0.2707646 Test Loss: 0.3822115
Validation loss decreased (0.271522 --> 0.270765).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 14.246237516403198
Epoch: 21, Steps: 64 | Train Loss: 0.2705403 Vali Loss: 0.2706642 Test Loss: 0.3816141
Validation loss decreased (0.270765 --> 0.270664).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 15.16918683052063
Epoch: 22, Steps: 64 | Train Loss: 0.2697756 Vali Loss: 0.2700789 Test Loss: 0.3811201
Validation loss decreased (0.270664 --> 0.270079).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 15.405746698379517
Epoch: 23, Steps: 64 | Train Loss: 0.2687933 Vali Loss: 0.2698719 Test Loss: 0.3806503
Validation loss decreased (0.270079 --> 0.269872).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 14.38258409500122
Epoch: 24, Steps: 64 | Train Loss: 0.2680664 Vali Loss: 0.2694026 Test Loss: 0.3802089
Validation loss decreased (0.269872 --> 0.269403).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 13.836520433425903
Epoch: 25, Steps: 64 | Train Loss: 0.2675353 Vali Loss: 0.2694151 Test Loss: 0.3797571
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 13.993955135345459
Epoch: 26, Steps: 64 | Train Loss: 0.2665704 Vali Loss: 0.2690534 Test Loss: 0.3794018
Validation loss decreased (0.269403 --> 0.269053).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 13.273530006408691
Epoch: 27, Steps: 64 | Train Loss: 0.2664392 Vali Loss: 0.2684442 Test Loss: 0.3790477
Validation loss decreased (0.269053 --> 0.268444).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 13.121991395950317
Epoch: 28, Steps: 64 | Train Loss: 0.2655395 Vali Loss: 0.2685199 Test Loss: 0.3787177
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 12.75355577468872
Epoch: 29, Steps: 64 | Train Loss: 0.2647108 Vali Loss: 0.2679415 Test Loss: 0.3784019
Validation loss decreased (0.268444 --> 0.267942).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 12.755316495895386
Epoch: 30, Steps: 64 | Train Loss: 0.2647614 Vali Loss: 0.2681343 Test Loss: 0.3781109
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 13.46787977218628
Epoch: 31, Steps: 64 | Train Loss: 0.2637254 Vali Loss: 0.2680512 Test Loss: 0.3778858
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 13.158946990966797
Epoch: 32, Steps: 64 | Train Loss: 0.2635075 Vali Loss: 0.2674909 Test Loss: 0.3776322
Validation loss decreased (0.267942 --> 0.267491).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 13.622247695922852
Epoch: 33, Steps: 64 | Train Loss: 0.2635779 Vali Loss: 0.2674610 Test Loss: 0.3773966
Validation loss decreased (0.267491 --> 0.267461).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 13.845954895019531
Epoch: 34, Steps: 64 | Train Loss: 0.2633538 Vali Loss: 0.2674333 Test Loss: 0.3771942
Validation loss decreased (0.267461 --> 0.267433).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 12.757195949554443
Epoch: 35, Steps: 64 | Train Loss: 0.2636091 Vali Loss: 0.2672707 Test Loss: 0.3770008
Validation loss decreased (0.267433 --> 0.267271).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 12.920538187026978
Epoch: 36, Steps: 64 | Train Loss: 0.2630169 Vali Loss: 0.2673236 Test Loss: 0.3768271
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 12.279437065124512
Epoch: 37, Steps: 64 | Train Loss: 0.2620194 Vali Loss: 0.2670924 Test Loss: 0.3766652
Validation loss decreased (0.267271 --> 0.267092).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 11.446405410766602
Epoch: 38, Steps: 64 | Train Loss: 0.2620321 Vali Loss: 0.2670040 Test Loss: 0.3764887
Validation loss decreased (0.267092 --> 0.267004).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 12.920266151428223
Epoch: 39, Steps: 64 | Train Loss: 0.2620424 Vali Loss: 0.2667167 Test Loss: 0.3763563
Validation loss decreased (0.267004 --> 0.266717).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 14.096363067626953
Epoch: 40, Steps: 64 | Train Loss: 0.2620938 Vali Loss: 0.2667508 Test Loss: 0.3762129
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 14.727158784866333
Epoch: 41, Steps: 64 | Train Loss: 0.2617977 Vali Loss: 0.2667683 Test Loss: 0.3760815
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 14.94391417503357
Epoch: 42, Steps: 64 | Train Loss: 0.2613893 Vali Loss: 0.2667758 Test Loss: 0.3759571
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  58060800.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 14.971921920776367
Epoch: 1, Steps: 64 | Train Loss: 0.5029836 Vali Loss: 0.2642849 Test Loss: 0.3731278
Validation loss decreased (inf --> 0.264285).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 14.253273248672485
Epoch: 2, Steps: 64 | Train Loss: 0.4993051 Vali Loss: 0.2632858 Test Loss: 0.3717249
Validation loss decreased (0.264285 --> 0.263286).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 14.304756879806519
Epoch: 3, Steps: 64 | Train Loss: 0.4976219 Vali Loss: 0.2627177 Test Loss: 0.3709589
Validation loss decreased (0.263286 --> 0.262718).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 14.26685881614685
Epoch: 4, Steps: 64 | Train Loss: 0.4971289 Vali Loss: 0.2626852 Test Loss: 0.3704217
Validation loss decreased (0.262718 --> 0.262685).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.903502941131592
Epoch: 5, Steps: 64 | Train Loss: 0.4969306 Vali Loss: 0.2621728 Test Loss: 0.3700552
Validation loss decreased (0.262685 --> 0.262173).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 14.266098022460938
Epoch: 6, Steps: 64 | Train Loss: 0.4964353 Vali Loss: 0.2618291 Test Loss: 0.3698080
Validation loss decreased (0.262173 --> 0.261829).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 13.316765785217285
Epoch: 7, Steps: 64 | Train Loss: 0.4946242 Vali Loss: 0.2617227 Test Loss: 0.3694876
Validation loss decreased (0.261829 --> 0.261723).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 13.34552526473999
Epoch: 8, Steps: 64 | Train Loss: 0.4948197 Vali Loss: 0.2615818 Test Loss: 0.3693664
Validation loss decreased (0.261723 --> 0.261582).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.957428216934204
Epoch: 9, Steps: 64 | Train Loss: 0.4973071 Vali Loss: 0.2617403 Test Loss: 0.3692594
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 13.290862560272217
Epoch: 10, Steps: 64 | Train Loss: 0.4955806 Vali Loss: 0.2613885 Test Loss: 0.3691454
Validation loss decreased (0.261582 --> 0.261389).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.190853595733643
Epoch: 11, Steps: 64 | Train Loss: 0.4956634 Vali Loss: 0.2612920 Test Loss: 0.3690943
Validation loss decreased (0.261389 --> 0.261292).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 12.167484045028687
Epoch: 12, Steps: 64 | Train Loss: 0.4962629 Vali Loss: 0.2614713 Test Loss: 0.3688879
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 12.903204917907715
Epoch: 13, Steps: 64 | Train Loss: 0.4955534 Vali Loss: 0.2612405 Test Loss: 0.3688034
Validation loss decreased (0.261292 --> 0.261241).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 13.434544086456299
Epoch: 14, Steps: 64 | Train Loss: 0.4948998 Vali Loss: 0.2610997 Test Loss: 0.3688212
Validation loss decreased (0.261241 --> 0.261100).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 12.355340957641602
Epoch: 15, Steps: 64 | Train Loss: 0.4938343 Vali Loss: 0.2613887 Test Loss: 0.3687717
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 14.297402143478394
Epoch: 16, Steps: 64 | Train Loss: 0.4952152 Vali Loss: 0.2609204 Test Loss: 0.3686556
Validation loss decreased (0.261100 --> 0.260920).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 13.737448930740356
Epoch: 17, Steps: 64 | Train Loss: 0.4951683 Vali Loss: 0.2610871 Test Loss: 0.3686286
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 14.50888729095459
Epoch: 18, Steps: 64 | Train Loss: 0.4948357 Vali Loss: 0.2609448 Test Loss: 0.3686592
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 13.402607440948486
Epoch: 19, Steps: 64 | Train Loss: 0.4937439 Vali Loss: 0.2609892 Test Loss: 0.3685828
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3492835760116577, mae:0.37813496589660645, rse:0.4750436246395111, corr:[0.5371288  0.5429882  0.54378885 0.541553   0.5395649  0.53901386
 0.5396234  0.54051304 0.54079145 0.5402068  0.5392208  0.53844386
 0.5382031  0.5384246  0.5387794  0.53875893 0.5381295  0.5370442
 0.5358771  0.53494924 0.5344175  0.5341787  0.5340294  0.5337534
 0.53321505 0.53249246 0.531761   0.5311703  0.5307569  0.5304621
 0.5301476  0.52973264 0.52915925 0.5284335  0.5276661  0.52695894
 0.5263301  0.5257821  0.52524054 0.52465737 0.52402323 0.52335745
 0.52272725 0.5221519  0.52162015 0.52107906 0.52049124 0.5198281
 0.5190505  0.51816535 0.51727617 0.5164881  0.51581734 0.5152674
 0.51481473 0.51439905 0.5139601  0.5134871  0.51303285 0.51262593
 0.51232857 0.5121389  0.5120091  0.51185066 0.5116086  0.51132774
 0.5109852  0.51067567 0.5104138  0.51015973 0.50984585 0.5094278
 0.5089234  0.5083607  0.5077791  0.5072275  0.5067281  0.5062321
 0.50571835 0.50510675 0.50434303 0.50350296 0.50270706 0.5020469
 0.50156224 0.50120336 0.5008482  0.50046784 0.49999967 0.4994603
 0.49890044 0.49831828 0.49768054 0.49692118 0.49593362 0.49467444
 0.49320135 0.49166593 0.4901662  0.48881373 0.48768836 0.48676494
 0.48593038 0.4850426  0.48400325 0.48282304 0.48160204 0.48047125
 0.47947845 0.47861212 0.47780353 0.47694772 0.47603598 0.47504023
 0.47400674 0.47304678 0.4722307  0.4715026  0.4707812  0.469945
 0.46899638 0.46795982 0.46699145 0.4661962  0.46560416 0.46512318
 0.46461445 0.46390864 0.46291298 0.46164083 0.4602569  0.4589815
 0.45803228 0.4574456  0.4571012  0.4567682  0.45624325 0.45548704
 0.45456785 0.45364746 0.45287126 0.4522987  0.4518592  0.4513552
 0.45065176 0.44968712 0.44864702 0.44770148 0.4470103  0.44656458
 0.44625476 0.44588187 0.44522956 0.44426975 0.44315097 0.44212517
 0.44139656 0.44100863 0.44082236 0.4406782  0.44035345 0.4397813
 0.4390064  0.43822566 0.4376241  0.43727404 0.4370887  0.43692505
 0.4366537  0.43622306 0.43560153 0.43492213 0.43427584 0.4337338
 0.43325573 0.4327735  0.43221122 0.4315548  0.43085933 0.43027717
 0.42986774 0.4296339  0.42951435 0.4293514  0.429005   0.42840183
 0.4276189  0.42670923 0.42577106 0.42480218 0.42376456 0.42257568
 0.4212272  0.41988087 0.41851673 0.41720802 0.41607746 0.41513255
 0.41433054 0.4135444  0.4126407  0.41154116 0.41023743 0.4087978
 0.40732825 0.40591395 0.40458456 0.40336052 0.4023344  0.4015882
 0.40101177 0.40050662 0.39997488 0.39932647 0.39851192 0.39744234
 0.39613247 0.39464012 0.3932012  0.39196524 0.39097708 0.390179
 0.3894327  0.388617   0.38770097 0.3865969  0.38538662 0.3842669
 0.38330844 0.38257602 0.38198763 0.38143256 0.380797   0.38007504
 0.37929556 0.37859657 0.3780548  0.3776687  0.37739432 0.37711874
 0.37678406 0.3763522  0.37590632 0.3756574  0.37557343 0.37562168
 0.37566313 0.37556383 0.37522835 0.3746759  0.37410176 0.3736503
 0.37339586 0.3733475  0.3733192  0.37317356 0.37280402 0.37227577
 0.37168944 0.37117094 0.37077656 0.3705227  0.37031397 0.370124
 0.36990577 0.36962718 0.36936888 0.3691898  0.36906818 0.36893076
 0.3687789  0.36857128 0.36826682 0.36793822 0.3676211  0.36731994
 0.36705607 0.36669153 0.36613718 0.36547747 0.36478016 0.364226
 0.36390364 0.36382088 0.36382955 0.36366373 0.3631024  0.36197975
 0.36045718 0.35892567 0.35767516 0.35685423 0.3564492  0.35621375
 0.35594574 0.35540307 0.35450187 0.35336795 0.35229662 0.3514851
 0.35108036 0.35089964 0.35076153 0.35035786 0.34964398 0.34871092
 0.34783396 0.34717718 0.34684053 0.3467704  0.34677738 0.34674153
 0.34645993 0.3459937  0.34543273 0.3449176  0.34458703 0.34446582
 0.3443963  0.34425938 0.34399733 0.34362662 0.34325838 0.34307137
 0.34314212 0.3434643  0.34386832 0.3440751  0.34399593 0.34361652
 0.34316292 0.3428091  0.34271964 0.34294215 0.34329304 0.34362888
 0.34375757 0.34354454 0.34312758 0.34273708 0.3425176  0.34246394
 0.34251472 0.34254184 0.3424378  0.34215945 0.34178346 0.3414724
 0.34130934 0.3413153  0.34137514 0.34133127 0.3411045  0.34068805
 0.34018818 0.339769   0.3394766  0.339378   0.33929524 0.3391505
 0.3388842  0.33852834 0.33819258 0.3380017  0.33799937 0.33819988
 0.33842868 0.3385744  0.33850458 0.3382197  0.33783787 0.33746955
 0.33725604 0.33728316 0.33749595 0.3377676  0.33797488 0.33810908
 0.3382137  0.33830234 0.33833796 0.338219   0.33796698 0.3374779
 0.3368176  0.33604088 0.3352209  0.33436748 0.33345646 0.33257395
 0.33169848 0.33086082 0.33011353 0.3295302  0.3291633  0.32896212
 0.3288375  0.32869136 0.32830545 0.3275857  0.32662773 0.32551518
 0.32445532 0.3236161  0.32312557 0.32299054 0.32299206 0.3229824
 0.32280296 0.32248142 0.32209587 0.32174045 0.3215481  0.32150194
 0.32159418 0.3217411  0.32178843 0.32169172 0.32146493 0.3212355
 0.32114926 0.32124463 0.32143486 0.3216244  0.32173046 0.32172507
 0.32167244 0.32162783 0.32168415 0.32183114 0.3220133  0.32212666
 0.32207447 0.32177746 0.3213479  0.3209617  0.32077092 0.32074833
 0.32072234 0.32062504 0.3203028  0.3196897  0.3189248  0.31828967
 0.3179067  0.31787932 0.31808352 0.318276   0.31827706 0.3179597
 0.31739572 0.31674603 0.3162309  0.31591007 0.3157948  0.31580782
 0.31577945 0.31558478 0.31517643 0.3146388  0.31418234 0.3138609
 0.31373343 0.31372872 0.31371307 0.31354555 0.31314558 0.3125474
 0.31189167 0.31131697 0.31090233 0.31065688 0.31057924 0.31053784
 0.31042674 0.31008923 0.30951723 0.30866396 0.30758366 0.30631575
 0.3049302  0.303608   0.30240396 0.3013862  0.3005876  0.29995656
 0.29937348 0.29879424 0.29814744 0.29750255 0.2968526  0.29619658
 0.29553562 0.29492304 0.2943043  0.29354924 0.2926349  0.29159078
 0.29055357 0.28959414 0.28880927 0.28830832 0.2879999  0.2878255
 0.28755248 0.28705782 0.28637394 0.28559566 0.2849377  0.28447172
 0.28420654 0.28404582 0.28382888 0.28345132 0.28292057 0.28230968
 0.28171068 0.28118074 0.28088155 0.28072757 0.28059086 0.28038365
 0.2800599  0.27968323 0.2793169  0.279055   0.27896425 0.27898356
 0.2789781  0.27887928 0.2786517  0.2783545  0.27810657 0.27796245
 0.27789202 0.27785146 0.27785543 0.2778109  0.27768156 0.27747786
 0.27724823 0.27707073 0.2769402  0.27687925 0.27681792 0.2766704
 0.27639925 0.27605677 0.27568012 0.27531412 0.27494678 0.27464837
 0.27440548 0.27425623 0.27419043 0.2741981  0.27411884 0.27399415
 0.2738404  0.2737632  0.2737454  0.27375206 0.27375793 0.27371284
 0.27358073 0.2733266  0.2730018  0.272666   0.2724071  0.2722828
 0.27226248 0.27221775 0.27193755 0.27130604 0.27029377 0.26889536
 0.26727948 0.26584306 0.26476136 0.26401976 0.26341334 0.26275223
 0.26192722 0.26094037 0.25989327 0.25890934 0.25808004 0.25746256
 0.25696808 0.25643587 0.25583407 0.25511885 0.25432414 0.25358364
 0.25298616 0.25253588 0.25227353 0.25206408 0.25186357 0.2516128
 0.25125188 0.2508376  0.25037205 0.24989218 0.24953969 0.24935147
 0.2492178  0.24907911 0.24892965 0.24866237 0.24833754 0.24802211
 0.24785537 0.24772598 0.24766524 0.24771884 0.24783796 0.24796686
 0.24811319 0.24838087 0.24877362 0.24929735 0.2498979  0.2504797
 0.2508689  0.25094622 0.25074595 0.2505013  0.2503963  0.25051787
 0.2507886  0.25111642 0.25140837 0.2514018  0.25126144 0.25103465
 0.25084445 0.2507912  0.25080153 0.25091752 0.25091165 0.25076607
 0.25047225 0.25024134 0.25010598 0.25000376 0.2500488  0.24999207
 0.24981074 0.24942735 0.24890833 0.24836819 0.24803294 0.24785402
 0.24796526 0.2482292  0.24843292 0.24856168 0.24856451 0.24838023
 0.24803829 0.24779923 0.24768952 0.24776644 0.2480937  0.24852039
 0.24883905 0.24897319 0.24883574 0.24831699 0.2474289  0.2463072
 0.24508487 0.24387571 0.24272183 0.24160866 0.24062586 0.2397615
 0.23899688 0.23829208 0.23782499 0.23742001 0.23703021 0.23656832
 0.23606841 0.23561826 0.2352631  0.23494928 0.23468912 0.23426199
 0.23362958 0.23282047 0.2319289  0.23132217 0.23100403 0.23085514
 0.23072545 0.23039272 0.22987308 0.22914918 0.22846311 0.22797513
 0.22784574 0.2277327  0.22746639 0.22696577 0.22628249 0.22566484
 0.22524881 0.22514129 0.22511674 0.22501476 0.22437412 0.2233199
 0.22226118 0.22178909 0.2220991  0.22249494 0.22131343 0.216513  ]
