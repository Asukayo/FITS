Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  40269824.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4727296
	speed: 0.1598s/iter; left time: 1014.9921s
Epoch: 1 cost time: 20.67848825454712
Epoch: 1, Steps: 129 | Train Loss: 0.6081098 Vali Loss: 0.2918267 Test Loss: 0.3949105
Validation loss decreased (inf --> 0.291827).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4837103
	speed: 0.4750s/iter; left time: 2955.3869s
Epoch: 2 cost time: 23.422877550125122
Epoch: 2, Steps: 129 | Train Loss: 0.5314796 Vali Loss: 0.2766080 Test Loss: 0.3787863
Validation loss decreased (0.291827 --> 0.276608).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4588016
	speed: 0.4503s/iter; left time: 2743.3942s
Epoch: 3 cost time: 20.99906301498413
Epoch: 3, Steps: 129 | Train Loss: 0.5181193 Vali Loss: 0.2710493 Test Loss: 0.3724254
Validation loss decreased (0.276608 --> 0.271049).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4957875
	speed: 0.3938s/iter; left time: 2348.3893s
Epoch: 4 cost time: 21.85644221305847
Epoch: 4, Steps: 129 | Train Loss: 0.5118926 Vali Loss: 0.2680806 Test Loss: 0.3690142
Validation loss decreased (0.271049 --> 0.268081).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5291378
	speed: 0.4173s/iter; left time: 2434.9200s
Epoch: 5 cost time: 18.924606561660767
Epoch: 5, Steps: 129 | Train Loss: 0.5077991 Vali Loss: 0.2666661 Test Loss: 0.3667341
Validation loss decreased (0.268081 --> 0.266666).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6039218
	speed: 0.3890s/iter; left time: 2219.4220s
Epoch: 6 cost time: 17.466899394989014
Epoch: 6, Steps: 129 | Train Loss: 0.5050303 Vali Loss: 0.2654236 Test Loss: 0.3651002
Validation loss decreased (0.266666 --> 0.265424).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4550671
	speed: 0.3800s/iter; left time: 2119.2533s
Epoch: 7 cost time: 18.40553307533264
Epoch: 7, Steps: 129 | Train Loss: 0.5030201 Vali Loss: 0.2646188 Test Loss: 0.3639579
Validation loss decreased (0.265424 --> 0.264619).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4862224
	speed: 0.3571s/iter; left time: 1945.4129s
Epoch: 8 cost time: 17.416054725646973
Epoch: 8, Steps: 129 | Train Loss: 0.5021342 Vali Loss: 0.2641918 Test Loss: 0.3630672
Validation loss decreased (0.264619 --> 0.264192).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4265642
	speed: 0.4102s/iter; left time: 2181.6841s
Epoch: 9 cost time: 21.243274450302124
Epoch: 9, Steps: 129 | Train Loss: 0.5007425 Vali Loss: 0.2633552 Test Loss: 0.3623664
Validation loss decreased (0.264192 --> 0.263355).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5084483
	speed: 0.4497s/iter; left time: 2333.8276s
Epoch: 10 cost time: 20.744930744171143
Epoch: 10, Steps: 129 | Train Loss: 0.4999540 Vali Loss: 0.2634454 Test Loss: 0.3616626
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5148272
	speed: 0.3494s/iter; left time: 1768.3857s
Epoch: 11 cost time: 17.572051763534546
Epoch: 11, Steps: 129 | Train Loss: 0.4990139 Vali Loss: 0.2627220 Test Loss: 0.3613896
Validation loss decreased (0.263355 --> 0.262722).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4794918
	speed: 0.3767s/iter; left time: 1857.7331s
Epoch: 12 cost time: 18.968286752700806
Epoch: 12, Steps: 129 | Train Loss: 0.4986516 Vali Loss: 0.2624047 Test Loss: 0.3609633
Validation loss decreased (0.262722 --> 0.262405).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6199121
	speed: 0.4058s/iter; left time: 1949.1377s
Epoch: 13 cost time: 19.213417530059814
Epoch: 13, Steps: 129 | Train Loss: 0.4985101 Vali Loss: 0.2624141 Test Loss: 0.3605300
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4698872
	speed: 0.4038s/iter; left time: 1887.5516s
Epoch: 14 cost time: 19.421401500701904
Epoch: 14, Steps: 129 | Train Loss: 0.4983238 Vali Loss: 0.2621174 Test Loss: 0.3603604
Validation loss decreased (0.262405 --> 0.262117).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4975682
	speed: 0.4029s/iter; left time: 1831.0752s
Epoch: 15 cost time: 19.285768508911133
Epoch: 15, Steps: 129 | Train Loss: 0.4974698 Vali Loss: 0.2620321 Test Loss: 0.3600550
Validation loss decreased (0.262117 --> 0.262032).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4833357
	speed: 0.3747s/iter; left time: 1654.7398s
Epoch: 16 cost time: 18.38952374458313
Epoch: 16, Steps: 129 | Train Loss: 0.4972824 Vali Loss: 0.2619477 Test Loss: 0.3598813
Validation loss decreased (0.262032 --> 0.261948).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4191526
	speed: 0.3899s/iter; left time: 1671.3138s
Epoch: 17 cost time: 18.20900869369507
Epoch: 17, Steps: 129 | Train Loss: 0.4968473 Vali Loss: 0.2617156 Test Loss: 0.3596811
Validation loss decreased (0.261948 --> 0.261716).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4865013
	speed: 0.3888s/iter; left time: 1616.5399s
Epoch: 18 cost time: 19.51772904396057
Epoch: 18, Steps: 129 | Train Loss: 0.4968208 Vali Loss: 0.2617479 Test Loss: 0.3595265
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3843438
	speed: 0.4299s/iter; left time: 1731.8950s
Epoch: 19 cost time: 22.382641553878784
Epoch: 19, Steps: 129 | Train Loss: 0.4965637 Vali Loss: 0.2615134 Test Loss: 0.3594065
Validation loss decreased (0.261716 --> 0.261513).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5770898
	speed: 0.4499s/iter; left time: 1754.5361s
Epoch: 20 cost time: 21.495336055755615
Epoch: 20, Steps: 129 | Train Loss: 0.4966254 Vali Loss: 0.2613236 Test Loss: 0.3593150
Validation loss decreased (0.261513 --> 0.261324).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5128178
	speed: 0.4481s/iter; left time: 1689.7966s
Epoch: 21 cost time: 22.023778915405273
Epoch: 21, Steps: 129 | Train Loss: 0.4965193 Vali Loss: 0.2613524 Test Loss: 0.3591846
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4423396
	speed: 0.4119s/iter; left time: 1500.2519s
Epoch: 22 cost time: 19.262553453445435
Epoch: 22, Steps: 129 | Train Loss: 0.4964101 Vali Loss: 0.2614694 Test Loss: 0.3591009
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4996840
	speed: 0.3869s/iter; left time: 1359.2029s
Epoch: 23 cost time: 18.8797504901886
Epoch: 23, Steps: 129 | Train Loss: 0.4961250 Vali Loss: 0.2611880 Test Loss: 0.3590498
Validation loss decreased (0.261324 --> 0.261188).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4133876
	speed: 0.4144s/iter; left time: 1402.4135s
Epoch: 24 cost time: 20.5358464717865
Epoch: 24, Steps: 129 | Train Loss: 0.4956458 Vali Loss: 0.2611676 Test Loss: 0.3588670
Validation loss decreased (0.261188 --> 0.261168).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4612392
	speed: 0.4299s/iter; left time: 1399.4501s
Epoch: 25 cost time: 22.157848834991455
Epoch: 25, Steps: 129 | Train Loss: 0.4956880 Vali Loss: 0.2610181 Test Loss: 0.3588510
Validation loss decreased (0.261168 --> 0.261018).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4858603
	speed: 0.4652s/iter; left time: 1454.3284s
Epoch: 26 cost time: 22.170243740081787
Epoch: 26, Steps: 129 | Train Loss: 0.4957779 Vali Loss: 0.2613170 Test Loss: 0.3587616
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4821440
	speed: 0.3969s/iter; left time: 1189.3667s
Epoch: 27 cost time: 18.2810537815094
Epoch: 27, Steps: 129 | Train Loss: 0.4956656 Vali Loss: 0.2608491 Test Loss: 0.3587109
Validation loss decreased (0.261018 --> 0.260849).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5006331
	speed: 0.3935s/iter; left time: 1128.4323s
Epoch: 28 cost time: 19.075237274169922
Epoch: 28, Steps: 129 | Train Loss: 0.4955822 Vali Loss: 0.2610366 Test Loss: 0.3586528
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4263841
	speed: 0.3757s/iter; left time: 1029.0933s
Epoch: 29 cost time: 17.424044370651245
Epoch: 29, Steps: 129 | Train Loss: 0.4945740 Vali Loss: 0.2609476 Test Loss: 0.3586534
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4680882
	speed: 0.3935s/iter; left time: 1027.0056s
Epoch: 30 cost time: 20.40794849395752
Epoch: 30, Steps: 129 | Train Loss: 0.4956392 Vali Loss: 0.2610946 Test Loss: 0.3585545
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.34975507855415344, mae:0.37834569811820984, rse:0.47536417841911316, corr:[0.5311266  0.54038775 0.5371019  0.5337886  0.5339388  0.53631645
 0.5381524  0.5378278  0.5363867  0.53556234 0.53601795 0.53724664
 0.5381287  0.5378769  0.5368462  0.535872   0.5355188  0.53571814
 0.535922   0.5355639  0.5346374  0.53357995 0.5328727  0.5326676
 0.53270805 0.5326004  0.53210807 0.5313555  0.53064835 0.5302057
 0.5300034  0.52985936 0.5294831  0.5287629  0.5278863  0.52710575
 0.5265205  0.5261001  0.52567357 0.5251174  0.52444184 0.52374494
 0.5231537  0.5226931  0.5223026  0.5218294  0.5211617  0.5203198
 0.51940614 0.5185419  0.51786804 0.5174107  0.5170112  0.5165234
 0.5158965  0.515188   0.5145225  0.5140347  0.51376736 0.5136087
 0.51345253 0.51320016 0.5128508  0.5124745  0.5121601  0.5119578
 0.5117335  0.51144624 0.5110529  0.51058245 0.5101284  0.5097626
 0.5094818  0.5091772  0.50875324 0.50818956 0.50755864 0.50693226
 0.5063931  0.5058542  0.5052197  0.5044758  0.5036859  0.50295
 0.5023654  0.5019195  0.50149333 0.5010208  0.5004266  0.49975482
 0.4991104  0.49852416 0.49794757 0.49722323 0.4961783  0.49478617
 0.49318928 0.4916499  0.49033687 0.48928487 0.48835796 0.4873754
 0.4862246  0.4849552  0.4837343  0.4827344  0.48196492 0.4812838
 0.48049712 0.47952455 0.4784389  0.47739697 0.47654292 0.475809
 0.4750474  0.47416046 0.4731478  0.47208887 0.47113055 0.47031558
 0.46959972 0.4688235  0.46792033 0.46688393 0.46584162 0.46494776
 0.46425432 0.46362242 0.46287456 0.46192786 0.4608829  0.45992988
 0.45922887 0.45873657 0.45826653 0.45760897 0.45668474 0.45563453
 0.4546609  0.4538971  0.4532831  0.45263743 0.4518143  0.45078498
 0.44974086 0.44890848 0.44843388 0.44812047 0.44768584 0.44696006
 0.44603342 0.44515535 0.4445165  0.44417575 0.44395515 0.4435886
 0.4429203  0.44201103 0.44110668 0.44050345 0.44021526 0.44004908
 0.43973735 0.43918613 0.43849766 0.43789458 0.4375435  0.43742767
 0.4373404  0.43705252 0.4364332  0.4356635  0.43498147 0.4345511
 0.4342838  0.43396273 0.433386   0.4325426  0.4316349  0.43093258
 0.4304968  0.4302012  0.42982718 0.42921284 0.4284193  0.4276615
 0.42719218 0.42694682 0.42666686 0.42598894 0.4247382  0.4229831
 0.4210625  0.41941527 0.4180885  0.41697565 0.4159212  0.41480935
 0.41368902 0.41267142 0.41177824 0.41090047 0.4098639  0.40855506
 0.4070498  0.40559688 0.40445057 0.40369758 0.40320888 0.40269184
 0.4017969  0.40048808 0.3990195  0.397745   0.39685372 0.39617887
 0.39544255 0.3944004  0.39313832 0.39184543 0.39071962 0.3898105
 0.38899413 0.38810188 0.38708925 0.38595226 0.38489136 0.38411584
 0.383547   0.3830007  0.3822379  0.3811826  0.3799668  0.37889025
 0.37815428 0.37779728 0.37761256 0.3773522  0.3769198  0.37634712
 0.375789   0.37533665 0.37501886 0.37483734 0.3746367  0.37442687
 0.37424722 0.37414727 0.3740665  0.37389117 0.37359935 0.373204
 0.37284797 0.37273532 0.37288913 0.37318477 0.373338   0.3731607
 0.37259877 0.37181965 0.37109977 0.37068596 0.37056285 0.37057984
 0.37046993 0.37006155 0.36945823 0.36886516 0.3684209  0.36810645
 0.3678495  0.367513   0.36703765 0.3665881  0.3663208  0.366301
 0.36646655 0.36654937 0.36634636 0.36590052 0.3653054  0.36478117
 0.36439672 0.3641101  0.36376116 0.3631781  0.36232057 0.3612061
 0.36003944 0.35902324 0.35813114 0.35727218 0.35644707 0.35570252
 0.3551804  0.35480803 0.35439736 0.3537832  0.35296673 0.35204372
 0.35129848 0.3508108  0.35057393 0.35026035 0.34966898 0.34872988
 0.3476861  0.3468169  0.3463808  0.34638903 0.3465893  0.34672704
 0.34651837 0.346023   0.3454134  0.3448907  0.34456465 0.3443661
 0.3441028  0.34373638 0.34336555 0.34312642 0.34308857 0.34320086
 0.3432681  0.34315103 0.34281272 0.34233916 0.341994   0.3418966
 0.34206647 0.342293   0.34241676 0.3424068  0.34228644 0.34221378
 0.34219503 0.3420898  0.34184033 0.34145495 0.34101847 0.34066698
 0.3405496  0.3406476  0.34081545 0.34085357 0.34070703 0.34050316
 0.34039012 0.340461   0.34060058 0.3405944  0.34030312 0.3397421
 0.33913168 0.3387604  0.33871287 0.33892733 0.339043   0.33885333
 0.33832556 0.3376525  0.33712867 0.33695248 0.33707783 0.33732843
 0.33741018 0.33727318 0.33700687 0.3368307  0.33690643 0.33715537
 0.33741218 0.33752263 0.33741772 0.3371836  0.33697578 0.33693606
 0.33703986 0.33707935 0.3368363  0.33623177 0.33548054 0.3347253
 0.33413914 0.33366892 0.33313736 0.33236286 0.33133677 0.33034748
 0.32960346 0.32919738 0.32899958 0.32875812 0.32829732 0.3275803
 0.32677826 0.32613808 0.32567352 0.32529944 0.3249125  0.32433054
 0.32357326 0.32276893 0.32212108 0.3217309  0.32143334 0.3211101
 0.32066098 0.3201861  0.31981406 0.3196025  0.31955364 0.3195076
 0.3194031  0.31924963 0.31909078 0.31904387 0.31912538 0.3192728
 0.31936625 0.3192856  0.319023   0.31874177 0.31864882 0.3188287
 0.31921458 0.31958196 0.3197817  0.3197644  0.31963718 0.31952906
 0.31947976 0.31937757 0.31918484 0.31893864 0.31874323 0.3186413
 0.31859273 0.3185984  0.3184963  0.31814697 0.31760284 0.31708464
 0.31670803 0.31656933 0.3165626  0.31649658 0.31627622 0.3158832
 0.315462   0.3151349  0.31496316 0.31481934 0.31461138 0.3143218
 0.31397828 0.31366947 0.31344002 0.3132739  0.31312922 0.31284848
 0.31245527 0.31205052 0.31177744 0.31168732 0.31169754 0.31165117
 0.31143454 0.3110111  0.31045437 0.30990478 0.30950743 0.3092159
 0.30891535 0.30841088 0.3076761  0.30672732 0.30570078 0.3046628
 0.30362272 0.3026311  0.30161455 0.30060342 0.2996765  0.29887852
 0.29816094 0.29747903 0.29671663 0.29590094 0.29505202 0.29425615
 0.29358736 0.29311246 0.29271546 0.29220158 0.29151142 0.2906944
 0.28991967 0.2892375  0.28863615 0.28807977 0.28739473 0.28658763
 0.28566328 0.28479418 0.28418505 0.2838687  0.28378057 0.2836488
 0.28329763 0.28271145 0.28203067 0.2814753  0.28119305 0.28113514
 0.28110123 0.28088152 0.28054    0.28011128 0.2797128  0.27946082
 0.2793565  0.27932712 0.27924284 0.2790552  0.27881515 0.27856416
 0.27829334 0.2780221  0.27775356 0.27753732 0.27743474 0.27741173
 0.27739394 0.2773207  0.27724704 0.2771474  0.27706143 0.27702665
 0.27704462 0.27707624 0.27700266 0.27679953 0.27644864 0.27600452
 0.27557325 0.2752675  0.27507246 0.2749209  0.2747178  0.2745203
 0.27434844 0.27426055 0.27424863 0.27425188 0.27409166 0.2738463
 0.27360582 0.27352652 0.27357516 0.27363917 0.2736206  0.27345642
 0.27317858 0.2728415  0.2725409  0.2722887  0.27206102 0.27181622
 0.27152282 0.2711743  0.27073348 0.270192   0.2694847  0.26843432
 0.2670104  0.26548472 0.2640515  0.2628709  0.2619835  0.26135728
 0.2608687  0.26033497 0.25961787 0.25866255 0.25753954 0.25645372
 0.2555375  0.25482142 0.2543325  0.25393403 0.25349742 0.2529942
 0.2524241  0.2518092  0.2512823  0.2508267  0.25049844 0.2503063
 0.25021076 0.25022143 0.25022158 0.25010344 0.24989137 0.24959315
 0.24918093 0.24875836 0.24849002 0.248333   0.24828137 0.24822263
 0.24813554 0.2478812  0.24762265 0.247598   0.24787365 0.24834062
 0.24881315 0.24920022 0.24942245 0.24957879 0.2498231  0.2502612
 0.2507742  0.25114745 0.25125232 0.2511856  0.25106    0.2509874
 0.25097895 0.25104752 0.2511815  0.25114483 0.25109977 0.2510192
 0.25093186 0.25087664 0.25079656 0.2508241  0.25081843 0.2507986
 0.2507033  0.25061113 0.25041613 0.25003645 0.24972259 0.24942684
 0.24928579 0.24921526 0.24913573 0.24894594 0.24871235 0.24836984
 0.24818416 0.24818495 0.24827646 0.24850099 0.24875802 0.24888852
 0.24885157 0.24886502 0.24890356 0.24899493 0.24922037 0.24947496
 0.24962552 0.24968642 0.24960627 0.2492369  0.24851876 0.24752444
 0.24635558 0.24513777 0.2439546  0.24285273 0.2419914  0.24136548
 0.24090555 0.24050032 0.24029659 0.24004954 0.23971866 0.23924892
 0.2386955  0.23812822 0.23756412 0.23694962 0.23639102 0.23579587
 0.23526232 0.23479424 0.23428757 0.23391262 0.23350982 0.23301233
 0.2325391  0.23213921 0.23190798 0.2315811  0.23104337 0.23023179
 0.22944379 0.22872114 0.22830442 0.22818294 0.22808017 0.22774099
 0.22698832 0.22618036 0.22566941 0.22571634 0.22571348 0.22530897
 0.22434428 0.22344317 0.2235182  0.22434667 0.22389822 0.21834363]
