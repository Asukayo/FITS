Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10067456.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3948099
	speed: 0.1246s/iter; left time: 3209.6612s
	iters: 200, epoch: 1 | loss: 0.4957308
	speed: 0.1201s/iter; left time: 3079.8003s
	iters: 300, epoch: 1 | loss: 0.4258193
	speed: 0.1224s/iter; left time: 3127.9446s
	iters: 400, epoch: 1 | loss: 0.5360079
	speed: 0.1214s/iter; left time: 3090.7746s
	iters: 500, epoch: 1 | loss: 0.5559853
	speed: 0.1209s/iter; left time: 3065.0860s
Epoch: 1 cost time: 63.157843828201294
Epoch: 1, Steps: 517 | Train Loss: 0.5517350 Vali Loss: 0.2701445 Test Loss: 0.3628234
Validation loss decreased (inf --> 0.270144).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4909369
	speed: 0.8523s/iter; left time: 21507.8525s
	iters: 200, epoch: 2 | loss: 0.8204080
	speed: 0.1256s/iter; left time: 3156.2518s
	iters: 300, epoch: 2 | loss: 0.6213151
	speed: 0.1340s/iter; left time: 3354.6806s
	iters: 400, epoch: 2 | loss: 0.4606245
	speed: 0.1281s/iter; left time: 3193.9361s
	iters: 500, epoch: 2 | loss: 0.5327563
	speed: 0.1225s/iter; left time: 3041.5480s
Epoch: 2 cost time: 67.39106941223145
Epoch: 2, Steps: 517 | Train Loss: 0.5090104 Vali Loss: 0.2658021 Test Loss: 0.3561708
Validation loss decreased (0.270144 --> 0.265802).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5665633
	speed: 0.8458s/iter; left time: 20905.0013s
	iters: 200, epoch: 3 | loss: 0.4810358
	speed: 0.1363s/iter; left time: 3355.9365s
	iters: 300, epoch: 3 | loss: 0.5945600
	speed: 0.1227s/iter; left time: 3009.0962s
	iters: 400, epoch: 3 | loss: 0.5142869
	speed: 0.1226s/iter; left time: 2993.6042s
	iters: 500, epoch: 3 | loss: 0.5985848
	speed: 0.1281s/iter; left time: 3114.4093s
Epoch: 3 cost time: 65.14448809623718
Epoch: 3, Steps: 517 | Train Loss: 0.5028323 Vali Loss: 0.2635390 Test Loss: 0.3538401
Validation loss decreased (0.265802 --> 0.263539).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 1.0145417
	speed: 0.7680s/iter; left time: 18585.1105s
	iters: 200, epoch: 4 | loss: 0.3963850
	speed: 0.1093s/iter; left time: 2633.9166s
	iters: 300, epoch: 4 | loss: 0.8201433
	speed: 0.0903s/iter; left time: 2167.5976s
	iters: 400, epoch: 4 | loss: 0.6918218
	speed: 0.0779s/iter; left time: 1861.2437s
	iters: 500, epoch: 4 | loss: 0.6158366
	speed: 0.0917s/iter; left time: 2182.6592s
Epoch: 4 cost time: 50.5132372379303
Epoch: 4, Steps: 517 | Train Loss: 0.5002000 Vali Loss: 0.2626285 Test Loss: 0.3526682
Validation loss decreased (0.263539 --> 0.262628).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6054377
	speed: 0.6003s/iter; left time: 14216.1393s
	iters: 200, epoch: 5 | loss: 0.5151552
	speed: 0.1073s/iter; left time: 2529.3307s
	iters: 300, epoch: 5 | loss: 0.3846889
	speed: 0.0960s/iter; left time: 2254.6775s
	iters: 400, epoch: 5 | loss: 0.6100461
	speed: 0.0945s/iter; left time: 2209.2461s
	iters: 500, epoch: 5 | loss: 0.6106778
	speed: 0.0975s/iter; left time: 2268.9319s
Epoch: 5 cost time: 52.98062324523926
Epoch: 5, Steps: 517 | Train Loss: 0.4984878 Vali Loss: 0.2619379 Test Loss: 0.3514797
Validation loss decreased (0.262628 --> 0.261938).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3581495
	speed: 0.6497s/iter; left time: 15050.2584s
	iters: 200, epoch: 6 | loss: 0.5891891
	speed: 0.1036s/iter; left time: 2388.7977s
	iters: 300, epoch: 6 | loss: 0.7119816
	speed: 0.0948s/iter; left time: 2177.2757s
	iters: 400, epoch: 6 | loss: 0.5480735
	speed: 0.1008s/iter; left time: 2304.9402s
	iters: 500, epoch: 6 | loss: 0.2872995
	speed: 0.1045s/iter; left time: 2379.0470s
Epoch: 6 cost time: 52.73376798629761
Epoch: 6, Steps: 517 | Train Loss: 0.4974620 Vali Loss: 0.2616629 Test Loss: 0.3507846
Validation loss decreased (0.261938 --> 0.261663).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5069284
	speed: 0.7101s/iter; left time: 16083.9537s
	iters: 200, epoch: 7 | loss: 0.3547132
	speed: 0.1162s/iter; left time: 2619.9103s
	iters: 300, epoch: 7 | loss: 0.3325432
	speed: 0.1088s/iter; left time: 2441.8198s
	iters: 400, epoch: 7 | loss: 0.5050849
	speed: 0.1096s/iter; left time: 2449.1868s
	iters: 500, epoch: 7 | loss: 0.3427950
	speed: 0.1097s/iter; left time: 2440.5944s
Epoch: 7 cost time: 60.22989559173584
Epoch: 7, Steps: 517 | Train Loss: 0.4964938 Vali Loss: 0.2612240 Test Loss: 0.3507499
Validation loss decreased (0.261663 --> 0.261224).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2939424
	speed: 0.6825s/iter; left time: 15106.0161s
	iters: 200, epoch: 8 | loss: 0.3625818
	speed: 0.1118s/iter; left time: 2463.7878s
	iters: 300, epoch: 8 | loss: 0.5387550
	speed: 0.1144s/iter; left time: 2508.6781s
	iters: 400, epoch: 8 | loss: 0.4877445
	speed: 0.1140s/iter; left time: 2489.6921s
	iters: 500, epoch: 8 | loss: 0.3877302
	speed: 0.1135s/iter; left time: 2467.0267s
Epoch: 8 cost time: 58.86954689025879
Epoch: 8, Steps: 517 | Train Loss: 0.4962558 Vali Loss: 0.2610419 Test Loss: 0.3507473
Validation loss decreased (0.261224 --> 0.261042).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 1.0927132
	speed: 0.7040s/iter; left time: 15217.3180s
	iters: 200, epoch: 9 | loss: 0.5306810
	speed: 0.0999s/iter; left time: 2148.5881s
	iters: 300, epoch: 9 | loss: 0.4112889
	speed: 0.0979s/iter; left time: 2096.0039s
	iters: 400, epoch: 9 | loss: 0.5852984
	speed: 0.0922s/iter; left time: 1964.3664s
	iters: 500, epoch: 9 | loss: 0.4566378
	speed: 0.1208s/iter; left time: 2563.0637s
Epoch: 9 cost time: 54.601073265075684
Epoch: 9, Steps: 517 | Train Loss: 0.4958755 Vali Loss: 0.2605810 Test Loss: 0.3505141
Validation loss decreased (0.261042 --> 0.260581).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3806545
	speed: 0.8772s/iter; left time: 18506.9045s
	iters: 200, epoch: 10 | loss: 0.3212793
	speed: 0.1273s/iter; left time: 2674.0250s
	iters: 300, epoch: 10 | loss: 0.7218514
	speed: 0.1316s/iter; left time: 2749.5322s
	iters: 400, epoch: 10 | loss: 0.7080737
	speed: 0.1183s/iter; left time: 2460.9862s
	iters: 500, epoch: 10 | loss: 0.5027266
	speed: 0.1231s/iter; left time: 2548.3175s
Epoch: 10 cost time: 65.66776037216187
Epoch: 10, Steps: 517 | Train Loss: 0.4954350 Vali Loss: 0.2606572 Test Loss: 0.3501102
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5076514
	speed: 0.7702s/iter; left time: 15851.1665s
	iters: 200, epoch: 11 | loss: 0.5033131
	speed: 0.0907s/iter; left time: 1857.4529s
	iters: 300, epoch: 11 | loss: 0.4103476
	speed: 0.0848s/iter; left time: 1728.1711s
	iters: 400, epoch: 11 | loss: 0.4907868
	speed: 0.0805s/iter; left time: 1632.2808s
	iters: 500, epoch: 11 | loss: 0.3480129
	speed: 0.0874s/iter; left time: 1764.3005s
Epoch: 11 cost time: 48.351632833480835
Epoch: 11, Steps: 517 | Train Loss: 0.4951960 Vali Loss: 0.2610978 Test Loss: 0.3498396
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6574038
	speed: 0.4995s/iter; left time: 10022.3330s
	iters: 200, epoch: 12 | loss: 0.3684739
	speed: 0.0912s/iter; left time: 1820.4840s
	iters: 300, epoch: 12 | loss: 0.6441725
	speed: 0.0920s/iter; left time: 1827.3309s
	iters: 400, epoch: 12 | loss: 0.6018083
	speed: 0.1043s/iter; left time: 2060.4754s
	iters: 500, epoch: 12 | loss: 0.5010039
	speed: 0.1172s/iter; left time: 2304.7803s
Epoch: 12 cost time: 52.401655435562134
Epoch: 12, Steps: 517 | Train Loss: 0.4947437 Vali Loss: 0.2607832 Test Loss: 0.3500614
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.34985458850860596, mae:0.37824106216430664, rse:0.47543177008628845, corr:[0.53352314 0.54224557 0.5379843  0.53451896 0.53471124 0.5369972
 0.5385317  0.5379554  0.5364772  0.5357096  0.53614676 0.53719413
 0.5378279  0.53744483 0.5364141  0.5354235  0.53493327 0.5348728
 0.5348033  0.5343659  0.53365266 0.5329794  0.53262454 0.5325885
 0.5325486  0.53218234 0.53143823 0.53059167 0.52995706 0.5296806
 0.5296324  0.52955437 0.5291628  0.5283914  0.5274786  0.526715
 0.5262072  0.52587914 0.5254934  0.52491486 0.5241827  0.5234274
 0.52282256 0.52241576 0.52210754 0.52169526 0.5210507  0.5202062
 0.5192828  0.5184409  0.5178373  0.51746947 0.5171268  0.5166345
 0.51595426 0.5151737  0.51443464 0.5138574  0.5134642  0.5131461
 0.5128371  0.51250184 0.51218414 0.51195043 0.51181924 0.5117555
 0.5115964  0.51133615 0.5109813  0.5105823  0.51019996 0.5098588
 0.5095279  0.50912595 0.5085983  0.50800645 0.50746113 0.50699884
 0.50659627 0.5061134  0.50544316 0.5046234  0.5037862  0.5030627
 0.50251234 0.5020676  0.5016189  0.50115067 0.50065553 0.50021076
 0.49986622 0.49951845 0.49900872 0.49818543 0.4969288  0.4952469
 0.493351   0.49160647 0.4902083  0.48912895 0.48816326 0.4871084
 0.48587757 0.48456493 0.48337108 0.48247337 0.48185667 0.48134574
 0.48071438 0.479854   0.4787854  0.47763485 0.4765906  0.47568515
 0.4748596  0.47404563 0.4731924  0.47227833 0.47136524 0.47049552
 0.46973714 0.46903604 0.46835428 0.46761614 0.466813   0.46598956
 0.4652127  0.46447352 0.4637131  0.46284252 0.46182364 0.46069267
 0.45958963 0.45865446 0.4579865  0.4575708  0.45729044 0.45698878
 0.45645335 0.45555285 0.454305   0.45290118 0.45162964 0.4507113
 0.4502602  0.4501551  0.45018804 0.45001033 0.449405   0.44835857
 0.4471233  0.44602206 0.44520435 0.44469455 0.4443887  0.44417086
 0.44395146 0.44368112 0.44334996 0.44299603 0.4425653  0.442051
 0.4414829  0.44094795 0.44050878 0.44017467 0.4398792  0.43950891
 0.43895027 0.4381641  0.43714687 0.43606696 0.43511602 0.4345007
 0.43424907 0.4341863  0.43404546 0.4336576  0.43303043 0.43229288
 0.43154883 0.43091223 0.43043765 0.43004376 0.4296329  0.42914385
 0.42863458 0.428096   0.42752758 0.42681926 0.4258417  0.42444053
 0.42259398 0.42053038 0.41844982 0.41670078 0.41555324 0.41489902
 0.4143894  0.41361782 0.4123524  0.41065392 0.4088703  0.40743494
 0.4065623  0.40606052 0.40548286 0.4044628  0.40295714 0.40123007
 0.39961487 0.39849138 0.3979779  0.3978243  0.39762512 0.3970161
 0.39592034 0.39444903 0.39295992 0.3916726  0.39060184 0.38963065
 0.3886486  0.38765642 0.3867512  0.38586852 0.38496745 0.3840383
 0.38301244 0.3820029  0.3811654  0.38066456 0.3805259  0.38061225
 0.38061962 0.38033226 0.37967768 0.37878144 0.37791198 0.3772546
 0.3768701  0.3766123  0.37633574 0.37604555 0.37565857 0.37523806
 0.37479338 0.37432778 0.3738122  0.37329102 0.37294757 0.37286934
 0.37302482 0.37328818 0.37340122 0.3732427  0.3728262  0.37237224
 0.37205574 0.3719063  0.37175658 0.37140468 0.37070307 0.36979526
 0.36892924 0.36830166 0.3679886  0.36779472 0.36740002 0.3666345
 0.3657232  0.36506447 0.36495322 0.36539844 0.36597654 0.36615252
 0.36566627 0.36458123 0.36340693 0.36287758 0.36329678 0.3644535
 0.3656435  0.366199   0.3657741  0.36443377 0.36262316 0.36081234
 0.35940728 0.35849297 0.35772192 0.3567451  0.35552257 0.3542588
 0.35334545 0.352903   0.3527867  0.3526986  0.35241076 0.3517838
 0.3509658  0.35004506 0.34917715 0.3482898  0.3474261  0.3466339
 0.34609047 0.34586614 0.3459602  0.34620592 0.34631974 0.34616596
 0.34563425 0.3449583  0.34440196 0.34418094 0.34432805 0.34462845
 0.3446943  0.3443108  0.34349158 0.3424657  0.34159762 0.34124485
 0.34149715 0.34221333 0.34303963 0.34359443 0.3437717  0.34361723
 0.34340045 0.3432973  0.34341335 0.34370655 0.34391853 0.34388745
 0.3434748  0.34268656 0.34180826 0.34114775 0.34081176 0.34067822
 0.3405609  0.34029606 0.33989373 0.33950287 0.33933276 0.3394956
 0.33987647 0.34025806 0.3403826  0.3401555  0.3397688  0.33953163
 0.33964592 0.34005192 0.34045276 0.34066707 0.34055358 0.34031108
 0.34021083 0.34038183 0.34066126 0.340695   0.34015086 0.33904788
 0.33771026 0.33683792 0.3368774  0.33774537 0.33891058 0.3397263
 0.33984727 0.33932838 0.33855277 0.33803362 0.33804962 0.3384959
 0.3389767  0.33907184 0.33861056 0.33773226 0.3368964  0.33634588
 0.33613634 0.33596942 0.33544773 0.33431855 0.3326599  0.33095285
 0.32961124 0.32885754 0.32857677 0.32842276 0.32805774 0.3272807
 0.3261724  0.32502106 0.32403618 0.32339385 0.32319072 0.32325897
 0.32344598 0.3236199  0.3237675  0.32392094 0.32398528 0.32395324
 0.3237662  0.32346377 0.32305434 0.32254097 0.32199687 0.32144886
 0.3210197  0.32078573 0.32070738 0.32075185 0.32084545 0.32099253
 0.3212364  0.32157645 0.3219474  0.322276   0.32248095 0.32249957
 0.32233927 0.32204548 0.32178417 0.32166037 0.3216785  0.32169145
 0.3215054  0.32096294 0.32020053 0.31954065 0.3193191  0.3195999
 0.32013023 0.3205945  0.3205725  0.3198426  0.31859595 0.31734025
 0.31648374 0.31629002 0.31667224 0.31735113 0.31807512 0.3186063
 0.3188628  0.31882688 0.31857103 0.31810167 0.3175006  0.3168695
 0.31624714 0.3156482  0.31504652 0.31446362 0.31402653 0.31374353
 0.31366563 0.31372    0.313782   0.31370735 0.31341517 0.31294283
 0.31247383 0.31217527 0.31208766 0.31211352 0.31212127 0.31192458
 0.31145927 0.31071487 0.3098666  0.30899516 0.30813324 0.30716404
 0.30599087 0.30470774 0.30340135 0.3022755  0.3014743  0.3009693
 0.30057567 0.30015117 0.29953232 0.29873553 0.29776075 0.29667714
 0.29558367 0.29464036 0.2938822  0.29326534 0.29277322 0.29234025
 0.29191676 0.29136834 0.29065436 0.28986555 0.28902352 0.28826457
 0.28752658 0.28676042 0.28595668 0.28510633 0.2843442  0.28372517
 0.28332675 0.2831356  0.2830275  0.28286982 0.28259543 0.28219694
 0.2817442  0.2813547  0.2812628  0.2814101  0.28162348 0.28171888
 0.28154013 0.28107834 0.28041354 0.27977344 0.27940577 0.27939788
 0.27965224 0.280013   0.28028363 0.28034323 0.2801813  0.27982664
 0.27935708 0.27889875 0.27863586 0.2785472  0.278571   0.27860212
 0.2785685  0.27846238 0.2782648  0.27804133 0.27778566 0.2774899
 0.27716777 0.2768953  0.27670938 0.27665082 0.27670282 0.27690768
 0.27715316 0.27734557 0.27736866 0.27715892 0.27661464 0.275925
 0.27525002 0.27474976 0.27437255 0.27403995 0.2737454  0.27351883
 0.2734288  0.2734458  0.27349243 0.27343944 0.2732726  0.27307603
 0.27295643 0.27294615 0.27290145 0.27262896 0.27191442 0.2705943
 0.26882192 0.26711836 0.26581854 0.26499116 0.2643876  0.26370838
 0.26277652 0.2616209  0.2604243  0.2593664  0.2585062  0.2577737
 0.25692996 0.25581285 0.25458822 0.25352508 0.2529172  0.25291678
 0.25332347 0.25367448 0.25361893 0.25291243 0.25174066 0.25049126
 0.24954163 0.24916036 0.24922593 0.24944653 0.24964997 0.24973887
 0.24968126 0.24962412 0.24973516 0.24991949 0.25007266 0.25000405
 0.24968533 0.24904388 0.24835478 0.24797237 0.24804798 0.2484902
 0.24905254 0.24952772 0.2497291  0.24972726 0.24974407 0.2500183
 0.25056416 0.25118187 0.25157413 0.25157794 0.25113887 0.25043
 0.24975573 0.24951237 0.2499197  0.25067422 0.25158978 0.25224668
 0.2524532  0.25229788 0.25197184 0.25185436 0.25188762 0.2519739
 0.2518504  0.25146273 0.25073603 0.24973628 0.24886596 0.24813989
 0.24767973 0.24736507 0.24716265 0.24710482 0.2473625  0.24781412
 0.2484783  0.24906659 0.24926257 0.24911173 0.24876499 0.24841067
 0.24823403 0.24844374 0.24886271 0.24933536 0.24984507 0.25031915
 0.25072953 0.2511478  0.25146374 0.2514083  0.2508068  0.24967624
 0.24817447 0.24658792 0.24515574 0.24394822 0.24302538 0.24227346
 0.241601   0.2409831  0.24070017 0.24061538 0.24063557 0.24052575
 0.24012873 0.23939058 0.23837233 0.23725337 0.23642154 0.2359654
 0.23595364 0.23618321 0.23624599 0.23603773 0.23532419 0.23419298
 0.23306161 0.23227122 0.23204511 0.23202743 0.23186752 0.23127934
 0.23044942 0.22946706 0.22873504 0.22836621 0.22807193 0.22748233
 0.2262937  0.22484462 0.22366343 0.22321163 0.22289003 0.22215122
 0.22072199 0.21931657 0.21931124 0.22104676 0.22222598 0.21817273]
