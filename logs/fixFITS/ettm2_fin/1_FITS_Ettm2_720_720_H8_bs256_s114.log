Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=74, out_features=148, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  39251968.0
params:  11100.0
Trainable parameters:  11100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.922219038009644
Epoch: 1, Steps: 64 | Train Loss: 0.6605695 Vali Loss: 0.3207620 Test Loss: 0.4415317
Validation loss decreased (inf --> 0.320762).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.756673812866211
Epoch: 2, Steps: 64 | Train Loss: 0.5653535 Vali Loss: 0.2935759 Test Loss: 0.4089932
Validation loss decreased (0.320762 --> 0.293576).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.004917621612549
Epoch: 3, Steps: 64 | Train Loss: 0.5383939 Vali Loss: 0.2830372 Test Loss: 0.3976186
Validation loss decreased (0.293576 --> 0.283037).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.398375034332275
Epoch: 4, Steps: 64 | Train Loss: 0.5276649 Vali Loss: 0.2777815 Test Loss: 0.3917270
Validation loss decreased (0.283037 --> 0.277781).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.470894575119019
Epoch: 5, Steps: 64 | Train Loss: 0.5208041 Vali Loss: 0.2744898 Test Loss: 0.3880791
Validation loss decreased (0.277781 --> 0.274490).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.131580591201782
Epoch: 6, Steps: 64 | Train Loss: 0.5175598 Vali Loss: 0.2724774 Test Loss: 0.3852868
Validation loss decreased (0.274490 --> 0.272477).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.498600006103516
Epoch: 7, Steps: 64 | Train Loss: 0.5150536 Vali Loss: 0.2707943 Test Loss: 0.3832923
Validation loss decreased (0.272477 --> 0.270794).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.083626985549927
Epoch: 8, Steps: 64 | Train Loss: 0.5123204 Vali Loss: 0.2697727 Test Loss: 0.3816150
Validation loss decreased (0.270794 --> 0.269773).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.742278337478638
Epoch: 9, Steps: 64 | Train Loss: 0.5082108 Vali Loss: 0.2685392 Test Loss: 0.3803166
Validation loss decreased (0.269773 --> 0.268539).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 7.965443134307861
Epoch: 10, Steps: 64 | Train Loss: 0.5080942 Vali Loss: 0.2677520 Test Loss: 0.3792218
Validation loss decreased (0.268539 --> 0.267752).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.297322034835815
Epoch: 11, Steps: 64 | Train Loss: 0.5078956 Vali Loss: 0.2669442 Test Loss: 0.3782730
Validation loss decreased (0.267752 --> 0.266944).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.021488189697266
Epoch: 12, Steps: 64 | Train Loss: 0.5072447 Vali Loss: 0.2670078 Test Loss: 0.3775286
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.493601560592651
Epoch: 13, Steps: 64 | Train Loss: 0.5071567 Vali Loss: 0.2661442 Test Loss: 0.3768247
Validation loss decreased (0.266944 --> 0.266144).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.579042196273804
Epoch: 14, Steps: 64 | Train Loss: 0.5045827 Vali Loss: 0.2659553 Test Loss: 0.3762316
Validation loss decreased (0.266144 --> 0.265955).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.89947247505188
Epoch: 15, Steps: 64 | Train Loss: 0.5038632 Vali Loss: 0.2658383 Test Loss: 0.3757820
Validation loss decreased (0.265955 --> 0.265838).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.51294469833374
Epoch: 16, Steps: 64 | Train Loss: 0.5044874 Vali Loss: 0.2655199 Test Loss: 0.3753268
Validation loss decreased (0.265838 --> 0.265520).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.582037448883057
Epoch: 17, Steps: 64 | Train Loss: 0.5033711 Vali Loss: 0.2654241 Test Loss: 0.3749287
Validation loss decreased (0.265520 --> 0.265424).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.462365865707397
Epoch: 18, Steps: 64 | Train Loss: 0.5039262 Vali Loss: 0.2650478 Test Loss: 0.3746324
Validation loss decreased (0.265424 --> 0.265048).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.084471464157104
Epoch: 19, Steps: 64 | Train Loss: 0.5018070 Vali Loss: 0.2649170 Test Loss: 0.3742766
Validation loss decreased (0.265048 --> 0.264917).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 8.768022537231445
Epoch: 20, Steps: 64 | Train Loss: 0.5001610 Vali Loss: 0.2646282 Test Loss: 0.3740450
Validation loss decreased (0.264917 --> 0.264628).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.432968139648438
Epoch: 21, Steps: 64 | Train Loss: 0.5027070 Vali Loss: 0.2647757 Test Loss: 0.3737690
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.189929723739624
Epoch: 22, Steps: 64 | Train Loss: 0.5011524 Vali Loss: 0.2642294 Test Loss: 0.3734994
Validation loss decreased (0.264628 --> 0.264229).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 7.709070682525635
Epoch: 23, Steps: 64 | Train Loss: 0.5015929 Vali Loss: 0.2644935 Test Loss: 0.3733391
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 7.35724401473999
Epoch: 24, Steps: 64 | Train Loss: 0.5014045 Vali Loss: 0.2641709 Test Loss: 0.3731390
Validation loss decreased (0.264229 --> 0.264171).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 7.412627458572388
Epoch: 25, Steps: 64 | Train Loss: 0.5009129 Vali Loss: 0.2642175 Test Loss: 0.3729987
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 7.613522052764893
Epoch: 26, Steps: 64 | Train Loss: 0.5000832 Vali Loss: 0.2640937 Test Loss: 0.3728090
Validation loss decreased (0.264171 --> 0.264094).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.670825958251953
Epoch: 27, Steps: 64 | Train Loss: 0.5006750 Vali Loss: 0.2641971 Test Loss: 0.3726994
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 8.773955583572388
Epoch: 28, Steps: 64 | Train Loss: 0.5001419 Vali Loss: 0.2640782 Test Loss: 0.3725846
Validation loss decreased (0.264094 --> 0.264078).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.019814014434814
Epoch: 29, Steps: 64 | Train Loss: 0.4996237 Vali Loss: 0.2639691 Test Loss: 0.3724662
Validation loss decreased (0.264078 --> 0.263969).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 8.47956132888794
Epoch: 30, Steps: 64 | Train Loss: 0.4991565 Vali Loss: 0.2638679 Test Loss: 0.3723486
Validation loss decreased (0.263969 --> 0.263868).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 9.26260232925415
Epoch: 31, Steps: 64 | Train Loss: 0.5000545 Vali Loss: 0.2635919 Test Loss: 0.3722698
Validation loss decreased (0.263868 --> 0.263592).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 8.266697883605957
Epoch: 32, Steps: 64 | Train Loss: 0.5003046 Vali Loss: 0.2635291 Test Loss: 0.3721677
Validation loss decreased (0.263592 --> 0.263529).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 5.2819907665252686
Epoch: 33, Steps: 64 | Train Loss: 0.4999735 Vali Loss: 0.2635084 Test Loss: 0.3720405
Validation loss decreased (0.263529 --> 0.263508).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 8.279177904129028
Epoch: 34, Steps: 64 | Train Loss: 0.4998920 Vali Loss: 0.2638462 Test Loss: 0.3719824
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 8.02749752998352
Epoch: 35, Steps: 64 | Train Loss: 0.5009665 Vali Loss: 0.2633076 Test Loss: 0.3719191
Validation loss decreased (0.263508 --> 0.263308).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.545341491699219
Epoch: 36, Steps: 64 | Train Loss: 0.4995220 Vali Loss: 0.2635931 Test Loss: 0.3718346
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 8.242986917495728
Epoch: 37, Steps: 64 | Train Loss: 0.4995993 Vali Loss: 0.2634775 Test Loss: 0.3717905
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 7.902920246124268
Epoch: 38, Steps: 64 | Train Loss: 0.4993934 Vali Loss: 0.2634627 Test Loss: 0.3717201
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3520495295524597, mae:0.3800472617149353, rse:0.4769207835197449, corr:[0.5244613  0.5340461  0.5399509  0.54009837 0.5375393  0.53537756
 0.5346866  0.5353597  0.53686017 0.53846014 0.5394108  0.5393153
 0.53838485 0.5371908  0.5362546  0.5357731  0.5357166  0.53589725
 0.53604794 0.5358959  0.53533524 0.5344461  0.5334755  0.5326844
 0.5321769  0.5319363  0.53185326 0.53179103 0.53160304 0.5311892
 0.53053826 0.5297605  0.52896756 0.52822256 0.52756554 0.5270084
 0.52652067 0.526072   0.52560025 0.5250676  0.5244714  0.52382237
 0.523166   0.52250916 0.5218525  0.52118856 0.5205269  0.51986855
 0.51919776 0.5184797  0.5177471  0.5170393  0.51635325 0.5157116
 0.5151446  0.5146615  0.5142639  0.51394266 0.51366395 0.5133755
 0.5130859  0.51276773 0.512412   0.512014   0.5115904  0.5112085
 0.51083636 0.51050484 0.5101814  0.5098342  0.50943166 0.50895846
 0.50844246 0.5078869  0.5073176  0.5067596  0.5062339  0.50570554
 0.50518376 0.5045912  0.5038923  0.50309855 0.50227684 0.5014924
 0.5008155  0.50026643 0.4998196  0.49945322 0.49906397 0.49858415
 0.4979526  0.49710864 0.4960614  0.49485815 0.49355793 0.49225992
 0.49103588 0.48991305 0.4888445  0.4877766  0.4866693  0.48551214
 0.48431078 0.4831144  0.4819752  0.4809396  0.48001274 0.47916564
 0.47832865 0.47744414 0.47649008 0.475473   0.4744733  0.47351745
 0.47262847 0.4718098  0.47103575 0.47024238 0.46938598 0.46842533
 0.46739587 0.46632543 0.46531114 0.46440274 0.4636172  0.46291903
 0.46225563 0.46153867 0.46070638 0.45972544 0.45864257 0.45752448
 0.4564907  0.4556087  0.45492274 0.4543707  0.4538433  0.45325932
 0.45253912 0.4516624  0.45065936 0.44961092 0.44863373 0.4477771
 0.44707012 0.4464616  0.44593227 0.44538897 0.44477317 0.44404888
 0.44325936 0.44246012 0.4416819  0.44101116 0.44046974 0.44003758
 0.43965045 0.4392228  0.4386816  0.43804023 0.43731388 0.43658665
 0.43594813 0.43546948 0.43514255 0.4348908  0.43460846 0.43421343
 0.4336754  0.43303302 0.43232262 0.4316757  0.43114147 0.43074635
 0.43043002 0.43010303 0.42966086 0.42903343 0.42822233 0.4273208
 0.42643073 0.4256852  0.42516682 0.42483583 0.42458773 0.42425895
 0.42376217 0.42295277 0.42183802 0.42045933 0.41894907 0.417423
 0.41599718 0.41479796 0.41370177 0.41259727 0.41141552 0.41011143
 0.40873042 0.40735468 0.40605927 0.40491337 0.40393123 0.40306526
 0.40224677 0.40139472 0.40042174 0.39930815 0.39814302 0.39706177
 0.39607123 0.3951786  0.39436054 0.39357826 0.39279002 0.39189783
 0.3908611  0.389649   0.38836905 0.3871174  0.38597706 0.38498846
 0.38414612 0.38340172 0.38271666 0.3819506  0.38104117 0.38005438
 0.37904078 0.37814125 0.37742022 0.3768999  0.376515   0.37618437
 0.37578738 0.3752886  0.3746822  0.37401894 0.3733947  0.37287563
 0.37249509 0.37220794 0.37195867 0.37175593 0.37149897 0.37118647
 0.3708228  0.3704568  0.37011927 0.36984587 0.36967877 0.36958346
 0.36949286 0.36934927 0.3690679  0.3686447  0.36811617 0.36760044
 0.3671939  0.36695212 0.36685747 0.36685264 0.3668117  0.36668426
 0.36640236 0.36592782 0.36533716 0.36473718 0.36421648 0.36383018
 0.36363104 0.3635501  0.36345828 0.3632803  0.36294556 0.36245063
 0.36189327 0.36134556 0.36088592 0.36062288 0.36051127 0.3604833
 0.3603995  0.36014864 0.35964808 0.3588644  0.35786235 0.35670108
 0.35555765 0.35460562 0.35387668 0.35329494 0.35277516 0.352206
 0.3515961  0.35092163 0.3502019  0.34947833 0.3488172  0.34820706
 0.3476713  0.34710476 0.34650868 0.34580573 0.34504604 0.34426376
 0.34357533 0.34298903 0.34252048 0.34216806 0.3418883  0.34169045
 0.34145755 0.34118304 0.34082213 0.340375   0.33991545 0.3395133
 0.3391818  0.3389477  0.33880362 0.33868933 0.33854258 0.33834526
 0.33809733 0.33787882 0.33773544 0.33765155 0.33765268 0.3376962
 0.33776733 0.33778778 0.33772403 0.33758232 0.3373454  0.33708698
 0.3368473  0.33661705 0.33645314 0.33637896 0.33636668 0.33633646
 0.33624855 0.33607933 0.3358664  0.33563182 0.33543408 0.33533716
 0.33532986 0.3353758  0.33538485 0.3352705  0.3350025  0.33458817
 0.33410466 0.33367583 0.3333636  0.33324942 0.33324805 0.33329856
 0.33329913 0.33316168 0.33285296 0.33240786 0.33191034 0.3315118
 0.33126628 0.33122736 0.33133212 0.33150008 0.33164367 0.3316589
 0.3315285  0.33132648 0.33112818 0.33101386 0.3310272  0.33119094
 0.3314557  0.33167866 0.33169562 0.33135188 0.33068115 0.3296945
 0.32857844 0.32750094 0.326599   0.32589978 0.325322   0.32484314
 0.32432732 0.32370138 0.32294697 0.32210132 0.32126024 0.32048565
 0.3198441  0.31939754 0.31905425 0.31872705 0.31839296 0.31794915
 0.31737682 0.31667635 0.31595892 0.31536633 0.31490225 0.31460863
 0.31444222 0.3143719  0.3143223  0.31419656 0.31397    0.31361058
 0.31322402 0.31291363 0.31269434 0.312588   0.31257045 0.3126303
 0.312758   0.31291103 0.31302062 0.31305718 0.31303114 0.31296366
 0.31289536 0.31283268 0.31280375 0.3127886  0.3127789  0.31275052
 0.3126714  0.31249952 0.31228337 0.31208283 0.31196743 0.31193018
 0.311907   0.31190327 0.31184167 0.31165498 0.3113473  0.31099996
 0.3106208  0.31028843 0.31002516 0.3098035  0.3096216  0.30942696
 0.30921894 0.30897158 0.30871838 0.30843675 0.30816588 0.3079539
 0.307796   0.30767384 0.3075467  0.3073856  0.3072042  0.30695495
 0.30667558 0.30637467 0.30608845 0.3058244  0.30557102 0.30531615
 0.30506468 0.30480316 0.30451536 0.3041792  0.30382618 0.3034303
 0.3029984  0.30245245 0.30181122 0.301033   0.30014554 0.2991305
 0.29800558 0.2968762  0.2957597  0.29470682 0.2937612  0.29293975
 0.29221177 0.29158086 0.29097906 0.29040858 0.28979123 0.28907073
 0.2882207  0.28734165 0.28648525 0.28567252 0.28494695 0.28430012
 0.28374067 0.28315276 0.28247765 0.28176492 0.28102526 0.28037047
 0.27979127 0.27929047 0.27888647 0.2785369  0.2782439  0.27790433
 0.27749225 0.27699748 0.27643925 0.27588364 0.27542344 0.2751117
 0.27493548 0.27481538 0.27476895 0.27468792 0.27449635 0.2741836
 0.27378005 0.2733579  0.27298144 0.27271786 0.27260512 0.27260348
 0.27259526 0.27253526 0.27238157 0.2721504  0.2719064  0.27168274
 0.2714927  0.27134532 0.2713232  0.27137598 0.27147904 0.27157804
 0.27163488 0.27163592 0.27152687 0.27133572 0.2710561  0.2707245
 0.27038205 0.2701213  0.26998055 0.26995274 0.26996428 0.2699853
 0.26992777 0.26977894 0.26955673 0.26932535 0.26904783 0.26882097
 0.268691   0.26870242 0.26878846 0.26885208 0.26884067 0.26872793
 0.26852325 0.26825687 0.26799318 0.2677778  0.26763254 0.267536
 0.26743054 0.26720473 0.26672038 0.2659315  0.26486096 0.26352292
 0.262041   0.2606788  0.25953084 0.25861427 0.2578338  0.25710505
 0.2563691  0.25561842 0.25486755 0.25411585 0.2533632  0.2526205
 0.251852   0.25100687 0.25015384 0.24932352 0.2485683  0.2479681
 0.24754146 0.24722774 0.24702443 0.2468192  0.24659738 0.24634862
 0.24606128 0.24578145 0.24549818 0.24520385 0.24497469 0.2448106
 0.24463685 0.24442148 0.2441955  0.2438999  0.24360578 0.24337015
 0.24331483 0.24334344 0.2434756  0.24372251 0.24400742 0.24424015
 0.24438506 0.24449809 0.24461333 0.24478397 0.24506931 0.24547033
 0.24588054 0.24616925 0.24627058 0.24627529 0.24625482 0.24627179
 0.24631374 0.24638872 0.24652863 0.24657774 0.24664132 0.24668078
 0.24671201 0.24676381 0.24677281 0.24680895 0.24672079 0.24654096
 0.2462489  0.2460071  0.2458357  0.24570227 0.24573791 0.2457907
 0.24586038 0.24584852 0.24572417 0.24549717 0.24528402 0.24503402
 0.24492039 0.24492997 0.24499185 0.24517064 0.24542955 0.24564396
 0.24573405 0.24581037 0.24584778 0.24591489 0.24614136 0.2464943
 0.24686998 0.24720173 0.2473277  0.24702212 0.24619839 0.24495189
 0.24346447 0.24195555 0.24058855 0.2394245  0.2385387  0.23784855
 0.23725866 0.23667741 0.23625919 0.23589651 0.23558787 0.23524648
 0.23484094 0.23437041 0.23385009 0.23327626 0.23277391 0.23226398
 0.23177674 0.23129594 0.23077802 0.23041421 0.23014684 0.22992066
 0.22971618 0.22943716 0.22912689 0.22866586 0.22811961 0.22752585
 0.22707401 0.22662155 0.2262115  0.22587365 0.22558135 0.22532429
 0.22498843 0.22462451 0.22421739 0.22395952 0.22369659 0.22354905
 0.22348006 0.22343343 0.22333905 0.2229752  0.2217265  0.21913604]
