Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  42577920.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 14.11102843284607
Epoch: 1, Steps: 65 | Train Loss: 0.4342606 Vali Loss: 0.2917366 Test Loss: 0.3939748
Validation loss decreased (inf --> 0.291737).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 14.580797672271729
Epoch: 2, Steps: 65 | Train Loss: 0.3378807 Vali Loss: 0.2631203 Test Loss: 0.3527730
Validation loss decreased (0.291737 --> 0.263120).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 14.440762996673584
Epoch: 3, Steps: 65 | Train Loss: 0.2874975 Vali Loss: 0.2476367 Test Loss: 0.3317505
Validation loss decreased (0.263120 --> 0.247637).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 13.671638250350952
Epoch: 4, Steps: 65 | Train Loss: 0.2565126 Vali Loss: 0.2389655 Test Loss: 0.3197450
Validation loss decreased (0.247637 --> 0.238966).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.92343544960022
Epoch: 5, Steps: 65 | Train Loss: 0.2362152 Vali Loss: 0.2325122 Test Loss: 0.3119075
Validation loss decreased (0.238966 --> 0.232512).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 12.785053253173828
Epoch: 6, Steps: 65 | Train Loss: 0.2208407 Vali Loss: 0.2289925 Test Loss: 0.3064297
Validation loss decreased (0.232512 --> 0.228993).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.533907413482666
Epoch: 7, Steps: 65 | Train Loss: 0.2095369 Vali Loss: 0.2258339 Test Loss: 0.3023579
Validation loss decreased (0.228993 --> 0.225834).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.66905927658081
Epoch: 8, Steps: 65 | Train Loss: 0.2000573 Vali Loss: 0.2224606 Test Loss: 0.2992087
Validation loss decreased (0.225834 --> 0.222461).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.966304063796997
Epoch: 9, Steps: 65 | Train Loss: 0.1919136 Vali Loss: 0.2195536 Test Loss: 0.2964986
Validation loss decreased (0.222461 --> 0.219554).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 13.523642301559448
Epoch: 10, Steps: 65 | Train Loss: 0.1853671 Vali Loss: 0.2183006 Test Loss: 0.2943033
Validation loss decreased (0.219554 --> 0.218301).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 13.749306917190552
Epoch: 11, Steps: 65 | Train Loss: 0.1801572 Vali Loss: 0.2173115 Test Loss: 0.2924813
Validation loss decreased (0.218301 --> 0.217312).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 13.04067087173462
Epoch: 12, Steps: 65 | Train Loss: 0.1754430 Vali Loss: 0.2153482 Test Loss: 0.2908635
Validation loss decreased (0.217312 --> 0.215348).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 12.860923290252686
Epoch: 13, Steps: 65 | Train Loss: 0.1709325 Vali Loss: 0.2136585 Test Loss: 0.2893963
Validation loss decreased (0.215348 --> 0.213658).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 12.834572792053223
Epoch: 14, Steps: 65 | Train Loss: 0.1680079 Vali Loss: 0.2121342 Test Loss: 0.2881774
Validation loss decreased (0.213658 --> 0.212134).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 12.093133211135864
Epoch: 15, Steps: 65 | Train Loss: 0.1647882 Vali Loss: 0.2121243 Test Loss: 0.2870660
Validation loss decreased (0.212134 --> 0.212124).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 12.351771593093872
Epoch: 16, Steps: 65 | Train Loss: 0.1621976 Vali Loss: 0.2112249 Test Loss: 0.2861020
Validation loss decreased (0.212124 --> 0.211225).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 12.873971462249756
Epoch: 17, Steps: 65 | Train Loss: 0.1597421 Vali Loss: 0.2097086 Test Loss: 0.2852166
Validation loss decreased (0.211225 --> 0.209709).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 13.62993311882019
Epoch: 18, Steps: 65 | Train Loss: 0.1574936 Vali Loss: 0.2084272 Test Loss: 0.2843957
Validation loss decreased (0.209709 --> 0.208427).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 12.700317621231079
Epoch: 19, Steps: 65 | Train Loss: 0.1556953 Vali Loss: 0.2082340 Test Loss: 0.2837174
Validation loss decreased (0.208427 --> 0.208234).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 12.526665687561035
Epoch: 20, Steps: 65 | Train Loss: 0.1539973 Vali Loss: 0.2078503 Test Loss: 0.2830414
Validation loss decreased (0.208234 --> 0.207850).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 13.075959920883179
Epoch: 21, Steps: 65 | Train Loss: 0.1525350 Vali Loss: 0.2076264 Test Loss: 0.2824675
Validation loss decreased (0.207850 --> 0.207626).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 13.637744426727295
Epoch: 22, Steps: 65 | Train Loss: 0.1510883 Vali Loss: 0.2065142 Test Loss: 0.2819658
Validation loss decreased (0.207626 --> 0.206514).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 14.096055746078491
Epoch: 23, Steps: 65 | Train Loss: 0.1497874 Vali Loss: 0.2067771 Test Loss: 0.2814544
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 14.244455337524414
Epoch: 24, Steps: 65 | Train Loss: 0.1487802 Vali Loss: 0.2057944 Test Loss: 0.2810146
Validation loss decreased (0.206514 --> 0.205794).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 13.762209177017212
Epoch: 25, Steps: 65 | Train Loss: 0.1478470 Vali Loss: 0.2050635 Test Loss: 0.2806219
Validation loss decreased (0.205794 --> 0.205064).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 13.64553689956665
Epoch: 26, Steps: 65 | Train Loss: 0.1469049 Vali Loss: 0.2047407 Test Loss: 0.2802487
Validation loss decreased (0.205064 --> 0.204741).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 13.78355360031128
Epoch: 27, Steps: 65 | Train Loss: 0.1457626 Vali Loss: 0.2049710 Test Loss: 0.2799146
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 13.6238694190979
Epoch: 28, Steps: 65 | Train Loss: 0.1451981 Vali Loss: 0.2044411 Test Loss: 0.2795782
Validation loss decreased (0.204741 --> 0.204441).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 13.833987712860107
Epoch: 29, Steps: 65 | Train Loss: 0.1447810 Vali Loss: 0.2038824 Test Loss: 0.2792908
Validation loss decreased (0.204441 --> 0.203882).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 13.68362283706665
Epoch: 30, Steps: 65 | Train Loss: 0.1441120 Vali Loss: 0.2040193 Test Loss: 0.2790473
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 13.372389554977417
Epoch: 31, Steps: 65 | Train Loss: 0.1433505 Vali Loss: 0.2037471 Test Loss: 0.2787914
Validation loss decreased (0.203882 --> 0.203747).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 13.492079019546509
Epoch: 32, Steps: 65 | Train Loss: 0.1428817 Vali Loss: 0.2036906 Test Loss: 0.2785674
Validation loss decreased (0.203747 --> 0.203691).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 12.806813955307007
Epoch: 33, Steps: 65 | Train Loss: 0.1421254 Vali Loss: 0.2031084 Test Loss: 0.2783536
Validation loss decreased (0.203691 --> 0.203108).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 12.834458589553833
Epoch: 34, Steps: 65 | Train Loss: 0.1419406 Vali Loss: 0.2028890 Test Loss: 0.2781563
Validation loss decreased (0.203108 --> 0.202889).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 13.295753479003906
Epoch: 35, Steps: 65 | Train Loss: 0.1416278 Vali Loss: 0.2027897 Test Loss: 0.2779836
Validation loss decreased (0.202889 --> 0.202790).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 12.812516450881958
Epoch: 36, Steps: 65 | Train Loss: 0.1409068 Vali Loss: 0.2026045 Test Loss: 0.2778178
Validation loss decreased (0.202790 --> 0.202605).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 13.290341854095459
Epoch: 37, Steps: 65 | Train Loss: 0.1409844 Vali Loss: 0.2026254 Test Loss: 0.2776822
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 12.518320798873901
Epoch: 38, Steps: 65 | Train Loss: 0.1404700 Vali Loss: 0.2023613 Test Loss: 0.2774974
Validation loss decreased (0.202605 --> 0.202361).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 12.667133808135986
Epoch: 39, Steps: 65 | Train Loss: 0.1403083 Vali Loss: 0.2025020 Test Loss: 0.2773716
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 12.304155111312866
Epoch: 40, Steps: 65 | Train Loss: 0.1398577 Vali Loss: 0.2020592 Test Loss: 0.2772442
Validation loss decreased (0.202361 --> 0.202059).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 13.020967483520508
Epoch: 41, Steps: 65 | Train Loss: 0.1399025 Vali Loss: 0.2020773 Test Loss: 0.2771298
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 12.931004285812378
Epoch: 42, Steps: 65 | Train Loss: 0.1395203 Vali Loss: 0.2017355 Test Loss: 0.2770247
Validation loss decreased (0.202059 --> 0.201736).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 12.431215047836304
Epoch: 43, Steps: 65 | Train Loss: 0.1393094 Vali Loss: 0.2014650 Test Loss: 0.2769444
Validation loss decreased (0.201736 --> 0.201465).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 13.032479047775269
Epoch: 44, Steps: 65 | Train Loss: 0.1388419 Vali Loss: 0.2016285 Test Loss: 0.2768236
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 13.33229374885559
Epoch: 45, Steps: 65 | Train Loss: 0.1389215 Vali Loss: 0.2016112 Test Loss: 0.2767364
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 13.286566019058228
Epoch: 46, Steps: 65 | Train Loss: 0.1386683 Vali Loss: 0.2013055 Test Loss: 0.2766552
Validation loss decreased (0.201465 --> 0.201306).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 13.26115894317627
Epoch: 47, Steps: 65 | Train Loss: 0.1384619 Vali Loss: 0.2013116 Test Loss: 0.2765854
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 14.007774114608765
Epoch: 48, Steps: 65 | Train Loss: 0.1384696 Vali Loss: 0.2017366 Test Loss: 0.2765052
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 13.750353336334229
Epoch: 49, Steps: 65 | Train Loss: 0.1382340 Vali Loss: 0.2005899 Test Loss: 0.2764267
Validation loss decreased (0.201306 --> 0.200590).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 14.089021444320679
Epoch: 50, Steps: 65 | Train Loss: 0.1380746 Vali Loss: 0.2009087 Test Loss: 0.2763694
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.0497355408796396e-05
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  42577920.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 14.696261644363403
Epoch: 1, Steps: 65 | Train Loss: 0.3854530 Vali Loss: 0.1968115 Test Loss: 0.2722014
Validation loss decreased (inf --> 0.196812).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 13.86217188835144
Epoch: 2, Steps: 65 | Train Loss: 0.3808184 Vali Loss: 0.1952564 Test Loss: 0.2708926
Validation loss decreased (0.196812 --> 0.195256).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 14.340065717697144
Epoch: 3, Steps: 65 | Train Loss: 0.3788021 Vali Loss: 0.1948120 Test Loss: 0.2704623
Validation loss decreased (0.195256 --> 0.194812).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 14.11481523513794
Epoch: 4, Steps: 65 | Train Loss: 0.3786439 Vali Loss: 0.1940281 Test Loss: 0.2700692
Validation loss decreased (0.194812 --> 0.194028).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 13.504531383514404
Epoch: 5, Steps: 65 | Train Loss: 0.3775759 Vali Loss: 0.1938443 Test Loss: 0.2698691
Validation loss decreased (0.194028 --> 0.193844).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 13.616789817810059
Epoch: 6, Steps: 65 | Train Loss: 0.3778742 Vali Loss: 0.1934026 Test Loss: 0.2696388
Validation loss decreased (0.193844 --> 0.193403).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 14.099854946136475
Epoch: 7, Steps: 65 | Train Loss: 0.3773807 Vali Loss: 0.1940411 Test Loss: 0.2696321
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 14.51485276222229
Epoch: 8, Steps: 65 | Train Loss: 0.3770798 Vali Loss: 0.1936809 Test Loss: 0.2695334
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 14.049464702606201
Epoch: 9, Steps: 65 | Train Loss: 0.3769292 Vali Loss: 0.1935629 Test Loss: 0.2694489
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2696630656719208, mae:0.32707205414772034, rse:0.41944146156311035, corr:[0.5542983  0.55947316 0.56119573 0.5597445  0.5576791  0.55660146
 0.5566376  0.5572294  0.5576859  0.55752504 0.55677426 0.5558794
 0.55525684 0.5550335  0.5550726  0.5550641  0.5547138  0.55394185
 0.5529197  0.5519094  0.5511252  0.55062354 0.55033565 0.55010796
 0.5497539  0.549184   0.5484528  0.54767853 0.54697526 0.5464088
 0.545965   0.54557425 0.5451005  0.5444741  0.5437115  0.54289246
 0.542067   0.5413027  0.540639   0.540075   0.5395679  0.5390581
 0.5385087  0.5378937  0.53721803 0.53651655 0.5358401  0.5351997
 0.5345498  0.53386635 0.533173   0.5325031  0.53184354 0.5312159
 0.5306402  0.5301243  0.5296298  0.52914333 0.5286854  0.52829164
 0.52800316 0.5278579  0.5278052  0.52773434 0.52756405 0.52726865
 0.5268432  0.5263605  0.52591103 0.5255673  0.52532464 0.525118
 0.52484304 0.5244337  0.5238736  0.52324104 0.52262783 0.52211475
 0.5217074  0.52135175 0.5209055  0.5203125  0.51955956 0.51870775
 0.51788276 0.51719856 0.5166828  0.5162984  0.51594585 0.51553285
 0.5150105  0.514348   0.51355356 0.5126836  0.51176625 0.5107574
 0.5096371  0.5084282  0.50710034 0.5056759  0.50423175 0.50286615
 0.50165904 0.50059    0.49962097 0.49865818 0.49771655 0.49683496
 0.4960296  0.495289   0.49458247 0.4938452  0.4930509  0.49214026
 0.49111262 0.49008387 0.4891759  0.48839435 0.48770568 0.48696524
 0.48610994 0.48508304 0.48399952 0.48297706 0.4821094  0.48143658
 0.4809135  0.48038545 0.4796834  0.47874415 0.4776425  0.47655737
 0.47572112 0.47524628 0.47506255 0.4749441  0.47464347 0.47404167
 0.47311825 0.47202724 0.47094825 0.47006834 0.46945137 0.4689664
 0.46846983 0.46780813 0.4670417  0.46622622 0.46548304 0.46488044
 0.46445283 0.46414837 0.46379125 0.4632968  0.46268213 0.46205986
 0.4614765  0.46093425 0.4603713  0.4597747  0.45910063 0.45840675
 0.4578028  0.45745298 0.45741996 0.4576245  0.4578301  0.4578247
 0.45752123 0.4569696  0.45623115 0.45549563 0.45492297 0.45454532
 0.4542628  0.4539177  0.4533745  0.45262384 0.45174196 0.4509748
 0.45042247 0.45007655 0.44987    0.44961813 0.44915274 0.44843295
 0.4475906  0.4468085  0.4461704  0.44561026 0.44496143 0.44404173
 0.44281366 0.44142553 0.43994567 0.4385041  0.4372824  0.43626282
 0.43533656 0.43430218 0.43303394 0.43153033 0.42996123 0.42856258
 0.42751873 0.42681012 0.426208   0.42548618 0.4245712  0.42354518
 0.42244655 0.42145872 0.4207549  0.42035368 0.42011943 0.41971043
 0.4188831  0.4175493  0.41594136 0.41436306 0.41304007 0.41211304
 0.41150203 0.411002   0.41042635 0.4095403  0.40837863 0.40715724
 0.40599918 0.40509087 0.40436158 0.40372413 0.40301695 0.402207
 0.40133575 0.40059444 0.40007606 0.39980608 0.39969328 0.39954367
 0.3992422  0.39869943 0.39807823 0.3976851  0.3975382  0.39762518
 0.39779297 0.39783928 0.39757687 0.39697814 0.39631602 0.3957953
 0.3955249  0.39555115 0.39564323 0.39567065 0.39553308 0.39532012
 0.39516795 0.39516944 0.39533114 0.39557108 0.3957223  0.39571834
 0.39553416 0.3952103  0.39494058 0.39480743 0.39472198 0.3945161
 0.39407524 0.39344037 0.39265713 0.39197344 0.39157167 0.39153567
 0.39186198 0.39220026 0.3922585  0.3919846  0.39142457 0.39083195
 0.39041126 0.39029208 0.39038745 0.39042082 0.39008933 0.389117
 0.38762003 0.3860821  0.38479388 0.38389656 0.38337865 0.3830147
 0.38269103 0.38222083 0.38151988 0.3806679  0.37992492 0.379249
 0.37876898 0.37834564 0.3779435  0.37736246 0.37669033 0.37608582
 0.3757293  0.37548485 0.37532103 0.37520072 0.3751082  0.37530223
 0.37567616 0.37625462 0.37678927 0.37695026 0.37660792 0.3758747
 0.37494078 0.3742556  0.37413073 0.3745248  0.37500444 0.37519935
 0.37483954 0.3740855  0.37349144 0.37345892 0.37433508 0.37573376
 0.37718025 0.37775695 0.37704578 0.37563303 0.3744913  0.37426397]
