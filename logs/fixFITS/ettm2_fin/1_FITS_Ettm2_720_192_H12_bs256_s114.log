Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=106, out_features=134, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50907136.0
params:  14338.0
Trainable parameters:  14338
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.197770118713379
Epoch: 1, Steps: 65 | Train Loss: 0.4812731 Vali Loss: 0.2070842 Test Loss: 0.2755058
Validation loss decreased (inf --> 0.207084).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.571883201599121
Epoch: 2, Steps: 65 | Train Loss: 0.3737928 Vali Loss: 0.1819756 Test Loss: 0.2453684
Validation loss decreased (0.207084 --> 0.181976).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 12.245800018310547
Epoch: 3, Steps: 65 | Train Loss: 0.3449039 Vali Loss: 0.1729077 Test Loss: 0.2355097
Validation loss decreased (0.181976 --> 0.172908).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.539708137512207
Epoch: 4, Steps: 65 | Train Loss: 0.3326847 Vali Loss: 0.1679167 Test Loss: 0.2305385
Validation loss decreased (0.172908 --> 0.167917).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.427832126617432
Epoch: 5, Steps: 65 | Train Loss: 0.3231547 Vali Loss: 0.1649846 Test Loss: 0.2274734
Validation loss decreased (0.167917 --> 0.164985).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.733908414840698
Epoch: 6, Steps: 65 | Train Loss: 0.3190463 Vali Loss: 0.1626527 Test Loss: 0.2252546
Validation loss decreased (0.164985 --> 0.162653).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.988379716873169
Epoch: 7, Steps: 65 | Train Loss: 0.3143873 Vali Loss: 0.1609909 Test Loss: 0.2237462
Validation loss decreased (0.162653 --> 0.160991).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.882269144058228
Epoch: 8, Steps: 65 | Train Loss: 0.3113899 Vali Loss: 0.1598814 Test Loss: 0.2224057
Validation loss decreased (0.160991 --> 0.159881).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.387062311172485
Epoch: 9, Steps: 65 | Train Loss: 0.3077711 Vali Loss: 0.1587865 Test Loss: 0.2213712
Validation loss decreased (0.159881 --> 0.158786).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.514790773391724
Epoch: 10, Steps: 65 | Train Loss: 0.3054707 Vali Loss: 0.1579832 Test Loss: 0.2205502
Validation loss decreased (0.158786 --> 0.157983).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.354762554168701
Epoch: 11, Steps: 65 | Train Loss: 0.3044590 Vali Loss: 0.1573718 Test Loss: 0.2198781
Validation loss decreased (0.157983 --> 0.157372).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.998750686645508
Epoch: 12, Steps: 65 | Train Loss: 0.3031215 Vali Loss: 0.1568399 Test Loss: 0.2192288
Validation loss decreased (0.157372 --> 0.156840).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.961458683013916
Epoch: 13, Steps: 65 | Train Loss: 0.3019563 Vali Loss: 0.1563753 Test Loss: 0.2187577
Validation loss decreased (0.156840 --> 0.156375).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.12255859375
Epoch: 14, Steps: 65 | Train Loss: 0.3017203 Vali Loss: 0.1561995 Test Loss: 0.2183900
Validation loss decreased (0.156375 --> 0.156199).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.059152364730835
Epoch: 15, Steps: 65 | Train Loss: 0.3000742 Vali Loss: 0.1555573 Test Loss: 0.2180120
Validation loss decreased (0.156199 --> 0.155557).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 10.363004446029663
Epoch: 16, Steps: 65 | Train Loss: 0.2992427 Vali Loss: 0.1555353 Test Loss: 0.2177405
Validation loss decreased (0.155557 --> 0.155535).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.334527254104614
Epoch: 17, Steps: 65 | Train Loss: 0.2987145 Vali Loss: 0.1552887 Test Loss: 0.2174805
Validation loss decreased (0.155535 --> 0.155289).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.311210632324219
Epoch: 18, Steps: 65 | Train Loss: 0.2980771 Vali Loss: 0.1550765 Test Loss: 0.2172539
Validation loss decreased (0.155289 --> 0.155076).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.274672508239746
Epoch: 19, Steps: 65 | Train Loss: 0.2983322 Vali Loss: 0.1546196 Test Loss: 0.2170194
Validation loss decreased (0.155076 --> 0.154620).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.339401960372925
Epoch: 20, Steps: 65 | Train Loss: 0.2968747 Vali Loss: 0.1545613 Test Loss: 0.2168240
Validation loss decreased (0.154620 --> 0.154561).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.71185302734375
Epoch: 21, Steps: 65 | Train Loss: 0.2973276 Vali Loss: 0.1546091 Test Loss: 0.2166791
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.722921371459961
Epoch: 22, Steps: 65 | Train Loss: 0.2966094 Vali Loss: 0.1543725 Test Loss: 0.2164916
Validation loss decreased (0.154561 --> 0.154372).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 11.081528663635254
Epoch: 23, Steps: 65 | Train Loss: 0.2961125 Vali Loss: 0.1540850 Test Loss: 0.2164078
Validation loss decreased (0.154372 --> 0.154085).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 11.663991451263428
Epoch: 24, Steps: 65 | Train Loss: 0.2961143 Vali Loss: 0.1541898 Test Loss: 0.2162867
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 11.192399263381958
Epoch: 25, Steps: 65 | Train Loss: 0.2952522 Vali Loss: 0.1539714 Test Loss: 0.2161912
Validation loss decreased (0.154085 --> 0.153971).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 12.03333044052124
Epoch: 26, Steps: 65 | Train Loss: 0.2953085 Vali Loss: 0.1539839 Test Loss: 0.2160559
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 11.642920732498169
Epoch: 27, Steps: 65 | Train Loss: 0.2952927 Vali Loss: 0.1538276 Test Loss: 0.2159290
Validation loss decreased (0.153971 --> 0.153828).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.786662578582764
Epoch: 28, Steps: 65 | Train Loss: 0.2950947 Vali Loss: 0.1538416 Test Loss: 0.2159197
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 10.391731977462769
Epoch: 29, Steps: 65 | Train Loss: 0.2948422 Vali Loss: 0.1537614 Test Loss: 0.2157713
Validation loss decreased (0.153828 --> 0.153761).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 10.632753133773804
Epoch: 30, Steps: 65 | Train Loss: 0.2939443 Vali Loss: 0.1536463 Test Loss: 0.2157125
Validation loss decreased (0.153761 --> 0.153646).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 11.191354751586914
Epoch: 31, Steps: 65 | Train Loss: 0.2941838 Vali Loss: 0.1536674 Test Loss: 0.2156490
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 10.099445343017578
Epoch: 32, Steps: 65 | Train Loss: 0.2938988 Vali Loss: 0.1536247 Test Loss: 0.2156052
Validation loss decreased (0.153646 --> 0.153625).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 10.34226393699646
Epoch: 33, Steps: 65 | Train Loss: 0.2940163 Vali Loss: 0.1534899 Test Loss: 0.2155242
Validation loss decreased (0.153625 --> 0.153490).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 10.267761945724487
Epoch: 34, Steps: 65 | Train Loss: 0.2940647 Vali Loss: 0.1533905 Test Loss: 0.2154703
Validation loss decreased (0.153490 --> 0.153390).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 11.422868967056274
Epoch: 35, Steps: 65 | Train Loss: 0.2932161 Vali Loss: 0.1533605 Test Loss: 0.2153753
Validation loss decreased (0.153390 --> 0.153361).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 10.24432897567749
Epoch: 36, Steps: 65 | Train Loss: 0.2938839 Vali Loss: 0.1534290 Test Loss: 0.2153823
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 10.509130477905273
Epoch: 37, Steps: 65 | Train Loss: 0.2941081 Vali Loss: 0.1534472 Test Loss: 0.2153089
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 10.088430881500244
Epoch: 38, Steps: 65 | Train Loss: 0.2941084 Vali Loss: 0.1532498 Test Loss: 0.2153143
Validation loss decreased (0.153361 --> 0.153250).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 10.881015300750732
Epoch: 39, Steps: 65 | Train Loss: 0.2924981 Vali Loss: 0.1531023 Test Loss: 0.2152776
Validation loss decreased (0.153250 --> 0.153102).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 11.3694429397583
Epoch: 40, Steps: 65 | Train Loss: 0.2932361 Vali Loss: 0.1531430 Test Loss: 0.2152429
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 10.949875354766846
Epoch: 41, Steps: 65 | Train Loss: 0.2931003 Vali Loss: 0.1531736 Test Loss: 0.2152240
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 11.23517918586731
Epoch: 42, Steps: 65 | Train Loss: 0.2935519 Vali Loss: 0.1531202 Test Loss: 0.2151669
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.2191513031721115, mae:0.2934144139289856, rse:0.37893709540367126, corr:[0.5513399  0.5627986  0.5632305  0.56006837 0.5591063  0.56059724
 0.5627721  0.56351256 0.5624909  0.56119245 0.56076986 0.56129324
 0.5621226  0.5623634  0.5617076  0.56064403 0.55984885 0.55950403
 0.559415   0.5591867  0.5585732  0.5576905  0.5569076  0.55643505
 0.55620843 0.5560182  0.555615   0.5549041  0.55407166 0.5533529
 0.55281395 0.5524304  0.552043   0.5514752  0.55071956 0.54992276
 0.5491595  0.5484451  0.5478133  0.5472751  0.54676056 0.54619366
 0.54556763 0.5448778  0.54417294 0.54351777 0.5429126  0.54229164
 0.5415606  0.5406995  0.53979224 0.5390423  0.5385211  0.5381199
 0.53764766 0.53705174 0.53634405 0.5356548  0.5352016  0.5350403
 0.5349972  0.5348796  0.5345515  0.5340382  0.5335144  0.533243
 0.533183   0.53314745 0.53294605 0.5324987  0.531893   0.5313817
 0.53114325 0.53108114 0.53091174 0.53047246 0.52974904 0.5289113
 0.5282313  0.52782434 0.5274743  0.52693284 0.5261033  0.52510816
 0.5242438  0.5237992  0.5236435  0.5234846  0.52305055 0.5222984
 0.5214258  0.52073085 0.5202965  0.5199563  0.519313   0.51807207
 0.51628256 0.51437306 0.5128425  0.5119262  0.5113555  0.510619
 0.5094543  0.50791067 0.5063328  0.50517136 0.50455606 0.5041753
 0.50349677 0.5023345  0.50093096 0.49976724 0.49925494 0.4991382
 0.49882683 0.4979441  0.49652612 0.4949911  0.49398804 0.49366343
 0.49367937 0.4933416  0.49237183 0.4908572  0.48930448 0.48833597
 0.4880866  0.48802334 0.4874649  0.48612112 0.48433244 0.48283893
 0.48226553 0.48243856 0.48258707 0.48203045 0.48060444 0.478952
 0.47793737 0.47798485 0.47854105 0.4787225  0.4779838  0.47632238
 0.47448948 0.47337738 0.47344026 0.47391394 0.47383994 0.47267002
 0.47080272 0.46930844 0.46893546 0.46938342 0.4696834  0.46913448
 0.46762156 0.46583578 0.46477684 0.46510968 0.46601692 0.4663765
 0.46546954 0.4636291  0.46202087 0.4618472  0.46296722 0.46414226
 0.46424556 0.46310538 0.4613712  0.46046612 0.46110636 0.46240053
 0.46283984 0.461633   0.45926633 0.45736536 0.457357   0.4590168
 0.46003726 0.45876685 0.4554923  0.45222393 0.4518406  0.45450258
 0.45672554 0.4558848  0.45177177 0.44787753 0.4514202  0.4571261 ]
