Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  80539648.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.9379723072052
Epoch: 1, Steps: 64 | Train Loss: 0.6552748 Vali Loss: 0.3159299 Test Loss: 0.4364192
Validation loss decreased (inf --> 0.315930).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.730141162872314
Epoch: 2, Steps: 64 | Train Loss: 0.5580951 Vali Loss: 0.2902688 Test Loss: 0.4061787
Validation loss decreased (0.315930 --> 0.290269).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.373692750930786
Epoch: 3, Steps: 64 | Train Loss: 0.5350653 Vali Loss: 0.2812280 Test Loss: 0.3960381
Validation loss decreased (0.290269 --> 0.281228).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.026369571685791
Epoch: 4, Steps: 64 | Train Loss: 0.5246968 Vali Loss: 0.2760890 Test Loss: 0.3906311
Validation loss decreased (0.281228 --> 0.276089).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.14303970336914
Epoch: 5, Steps: 64 | Train Loss: 0.5194560 Vali Loss: 0.2730352 Test Loss: 0.3869788
Validation loss decreased (0.276089 --> 0.273035).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.347107172012329
Epoch: 6, Steps: 64 | Train Loss: 0.5145746 Vali Loss: 0.2708844 Test Loss: 0.3842828
Validation loss decreased (0.273035 --> 0.270884).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.31959581375122
Epoch: 7, Steps: 64 | Train Loss: 0.5115093 Vali Loss: 0.2696714 Test Loss: 0.3823012
Validation loss decreased (0.270884 --> 0.269671).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.767745733261108
Epoch: 8, Steps: 64 | Train Loss: 0.5106554 Vali Loss: 0.2682790 Test Loss: 0.3807198
Validation loss decreased (0.269671 --> 0.268279).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.16107988357544
Epoch: 9, Steps: 64 | Train Loss: 0.5077261 Vali Loss: 0.2674163 Test Loss: 0.3794062
Validation loss decreased (0.268279 --> 0.267416).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.479743719100952
Epoch: 10, Steps: 64 | Train Loss: 0.5056301 Vali Loss: 0.2668453 Test Loss: 0.3783259
Validation loss decreased (0.267416 --> 0.266845).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.085578680038452
Epoch: 11, Steps: 64 | Train Loss: 0.5059868 Vali Loss: 0.2656933 Test Loss: 0.3775204
Validation loss decreased (0.266845 --> 0.265693).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.229613304138184
Epoch: 12, Steps: 64 | Train Loss: 0.5043182 Vali Loss: 0.2656291 Test Loss: 0.3766969
Validation loss decreased (0.265693 --> 0.265629).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.605555534362793
Epoch: 13, Steps: 64 | Train Loss: 0.5039440 Vali Loss: 0.2648695 Test Loss: 0.3760186
Validation loss decreased (0.265629 --> 0.264870).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.777673959732056
Epoch: 14, Steps: 64 | Train Loss: 0.5021702 Vali Loss: 0.2645840 Test Loss: 0.3754474
Validation loss decreased (0.264870 --> 0.264584).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.08223843574524
Epoch: 15, Steps: 64 | Train Loss: 0.5016676 Vali Loss: 0.2644216 Test Loss: 0.3749759
Validation loss decreased (0.264584 --> 0.264422).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 10.4956955909729
Epoch: 16, Steps: 64 | Train Loss: 0.5024276 Vali Loss: 0.2642152 Test Loss: 0.3746037
Validation loss decreased (0.264422 --> 0.264215).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.54910683631897
Epoch: 17, Steps: 64 | Train Loss: 0.5009272 Vali Loss: 0.2638754 Test Loss: 0.3742111
Validation loss decreased (0.264215 --> 0.263875).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.258469820022583
Epoch: 18, Steps: 64 | Train Loss: 0.5004374 Vali Loss: 0.2636873 Test Loss: 0.3739218
Validation loss decreased (0.263875 --> 0.263687).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.533538579940796
Epoch: 19, Steps: 64 | Train Loss: 0.5018502 Vali Loss: 0.2636341 Test Loss: 0.3736172
Validation loss decreased (0.263687 --> 0.263634).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.866822481155396
Epoch: 20, Steps: 64 | Train Loss: 0.5003341 Vali Loss: 0.2634388 Test Loss: 0.3733259
Validation loss decreased (0.263634 --> 0.263439).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.567644119262695
Epoch: 21, Steps: 64 | Train Loss: 0.4999220 Vali Loss: 0.2632077 Test Loss: 0.3730932
Validation loss decreased (0.263439 --> 0.263208).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.969491243362427
Epoch: 22, Steps: 64 | Train Loss: 0.4994946 Vali Loss: 0.2630785 Test Loss: 0.3728587
Validation loss decreased (0.263208 --> 0.263079).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.449315786361694
Epoch: 23, Steps: 64 | Train Loss: 0.4993056 Vali Loss: 0.2628821 Test Loss: 0.3726550
Validation loss decreased (0.263079 --> 0.262882).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 10.974791049957275
Epoch: 24, Steps: 64 | Train Loss: 0.4985529 Vali Loss: 0.2630245 Test Loss: 0.3724542
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.382340431213379
Epoch: 25, Steps: 64 | Train Loss: 0.4997815 Vali Loss: 0.2627191 Test Loss: 0.3722623
Validation loss decreased (0.262882 --> 0.262719).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.207053661346436
Epoch: 26, Steps: 64 | Train Loss: 0.4983547 Vali Loss: 0.2625647 Test Loss: 0.3721860
Validation loss decreased (0.262719 --> 0.262565).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 9.899890899658203
Epoch: 27, Steps: 64 | Train Loss: 0.4984588 Vali Loss: 0.2626714 Test Loss: 0.3720421
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.289072751998901
Epoch: 28, Steps: 64 | Train Loss: 0.4990242 Vali Loss: 0.2624248 Test Loss: 0.3719070
Validation loss decreased (0.262565 --> 0.262425).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 10.863180875778198
Epoch: 29, Steps: 64 | Train Loss: 0.4968549 Vali Loss: 0.2623417 Test Loss: 0.3717732
Validation loss decreased (0.262425 --> 0.262342).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 10.849163293838501
Epoch: 30, Steps: 64 | Train Loss: 0.4984165 Vali Loss: 0.2624394 Test Loss: 0.3716361
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 11.314608573913574
Epoch: 31, Steps: 64 | Train Loss: 0.4986793 Vali Loss: 0.2623878 Test Loss: 0.3715385
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 11.065426588058472
Epoch: 32, Steps: 64 | Train Loss: 0.4982310 Vali Loss: 0.2625655 Test Loss: 0.3714734
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3516318202018738, mae:0.37954550981521606, rse:0.4766378104686737, corr:[0.5251705  0.53838646 0.5387604  0.53502625 0.53419733 0.53622437
 0.53890604 0.5398558  0.5386997  0.53727305 0.53703636 0.5379809
 0.5391937  0.53958    0.538762   0.53745484 0.53661066 0.53650516
 0.5367689  0.5367807  0.5361501  0.5350501  0.53407174 0.53362566
 0.5336156  0.5336622  0.5333689  0.53261584 0.5316826  0.53095585
 0.53057057 0.5303767  0.5300936  0.5295166  0.5286722  0.5277824
 0.5270261  0.5264286  0.5259081  0.52539945 0.5248465  0.524249
 0.52367604 0.5231263  0.52253485 0.52188015 0.52116746 0.5203958
 0.51961696 0.5188733  0.518215   0.5176383  0.5171136  0.5166297
 0.5161283  0.51556325 0.5149763  0.5144329  0.51399845 0.51369137
 0.51350987 0.5132993  0.51295584 0.5124827  0.511983   0.5116141
 0.51138246 0.5112281  0.5109919  0.5105793  0.51000714 0.50940454
 0.50893915 0.5086103  0.5082955  0.5078322  0.5071673  0.50633067
 0.5055286  0.50484616 0.5042986  0.5037775  0.5031793  0.50243795
 0.5016164  0.5008548  0.50028205 0.4999252  0.49961773 0.49922624
 0.49865866 0.49785852 0.49692667 0.49592537 0.494798   0.4934971
 0.49204716 0.49056143 0.48919913 0.48809582 0.48721477 0.48638448
 0.48539343 0.4841744  0.4828335  0.4816216  0.48071292 0.4800507
 0.47941503 0.47861484 0.47760028 0.47648892 0.47548828 0.47466394
 0.4739296  0.47311655 0.4721023  0.47090802 0.46970758 0.4687207
 0.46801737 0.4674257  0.46673474 0.4657999  0.46468493 0.46361074
 0.46276745 0.4621194  0.46144542 0.46052012 0.45931754 0.45803753
 0.4570005  0.45635623 0.45597607 0.4555299  0.4547481  0.45361492
 0.4523546  0.4513086  0.45066357 0.45027822 0.44982764 0.44904348
 0.44793886 0.44679627 0.44602054 0.44570324 0.44559908 0.4453193
 0.44463348 0.44358677 0.4424615  0.44165564 0.44127968 0.44110653
 0.44077656 0.44007102 0.43909746 0.4382195  0.43771684 0.43758604
 0.43753183 0.43722716 0.4365406  0.43562892 0.43486962 0.434542
 0.43458533 0.43464595 0.43434566 0.4336402  0.4327605  0.4320878
 0.43178913 0.4317129  0.43148983 0.43082353 0.42972338 0.4284978
 0.42752817 0.4270386  0.42689764 0.426747   0.42632923 0.42560023
 0.4247989  0.42406994 0.42343646 0.42267534 0.4215693  0.42002937
 0.41826084 0.41666138 0.41535437 0.41429198 0.41325194 0.4119976
 0.41049677 0.40894568 0.4075897  0.40655312 0.40572837 0.40486512
 0.40379238 0.4025443  0.40133002 0.40039325 0.39980933 0.39937937
 0.3987142  0.39762777 0.39624134 0.39489713 0.3938937  0.3931933
 0.39255086 0.3916421  0.3904203  0.3890395  0.38779932 0.3869027
 0.38631538 0.38579974 0.38512763 0.3841143  0.38287532 0.38173255
 0.38085118 0.38023773 0.37969133 0.37903693 0.37825108 0.3774839
 0.37687713 0.37649754 0.3762106  0.375822   0.37525263 0.37457132
 0.3739809  0.3736233  0.37350225 0.37347987 0.3732604  0.37277582
 0.372169   0.37173924 0.37165844 0.37185565 0.3720863  0.37205723
 0.37167197 0.37111843 0.37067172 0.3705279  0.3705912  0.3706096
 0.3703308  0.36971903 0.36899024 0.36847347 0.36829135 0.36834735
 0.36832812 0.36797962 0.36737102 0.36677375 0.3664356  0.36637428
 0.36641112 0.36623272 0.36567935 0.3649469  0.36437857 0.36422813
 0.36446524 0.36471358 0.36459363 0.36402556 0.36316034 0.36240855
 0.36203343 0.3620005  0.3619694  0.3615306  0.36051735 0.35904056
 0.35755026 0.3564524  0.35576117 0.35521868 0.3545706  0.35371256
 0.3528237  0.35205492 0.35145316 0.35092157 0.35034114 0.3496172
 0.3488803  0.34822673 0.34779486 0.3474405  0.34701118 0.3463314
 0.34545937 0.3445448  0.3438454  0.34350988 0.34344077 0.34345618
 0.343269   0.3428605  0.34232852 0.34186587 0.34161943 0.34154946
 0.34144926 0.34121165 0.34085727 0.34050408 0.34027442 0.3402013
 0.34014443 0.33997753 0.33963782 0.3392044  0.33893237 0.33895314
 0.33924142 0.33952624 0.3395721  0.33931717 0.33887154 0.33854914
 0.3385085  0.33864063 0.33873928 0.33857673 0.33809173 0.3374648
 0.3370503  0.3370582  0.33741477 0.33776787 0.3378037  0.33745533
 0.33691263 0.33653355 0.33651084 0.33673984 0.33691204 0.3367316
 0.33616477 0.3354713  0.33498266 0.33492413 0.33511132 0.33524376
 0.335048   0.33450276 0.33386546 0.33347926 0.3334884  0.3337691
 0.33392513 0.3337091  0.33310556 0.3324232  0.3320539  0.33212674
 0.3325009  0.33285376 0.3328907  0.3325841  0.33217642 0.33201545
 0.3322449  0.33265787 0.3328705  0.3325832  0.33183956 0.3308175
 0.32985666 0.3291147  0.32853034 0.32790577 0.3270993  0.3262535
 0.3254955  0.3249219  0.32446855 0.32395014 0.32322645 0.32227325
 0.32128826 0.32056916 0.3201733  0.31999928 0.31986699 0.31948775
 0.31878704 0.317874   0.3170229  0.31646675 0.31615347 0.3159985
 0.31584814 0.31567818 0.31551662 0.31538913 0.31530848 0.31517056
 0.3149626  0.31471083 0.314454   0.31431234 0.31432432 0.314447
 0.31455705 0.31450835 0.31424665 0.31390375 0.31371596 0.31380352
 0.31411424 0.31442225 0.31455183 0.31442845 0.31417572 0.3139865
 0.3139749  0.31405774 0.31411976 0.31404674 0.31382805 0.3135262
 0.31324038 0.3130897  0.31299636 0.31283307 0.312579   0.31235522
 0.31223002 0.31227064 0.3123521  0.31224284 0.31181005 0.31107503
 0.310312   0.3098109  0.3097389  0.30992845 0.31010878 0.31005812
 0.3097099  0.30923396 0.30888876 0.30883503 0.30901143 0.30910578
 0.3089131  0.30839244 0.30775747 0.30728814 0.30712622 0.30716616
 0.30716535 0.3068921  0.30630636 0.30559507 0.30506548 0.30483717
 0.30484542 0.30479187 0.30444917 0.30367517 0.30257216 0.3012938
 0.3000027  0.29886788 0.29785264 0.29693264 0.2960909  0.2953157
 0.2945741  0.29388234 0.29319212 0.29253277 0.2918622  0.29116657
 0.29045868 0.28981304 0.2891805  0.2884785  0.2876899  0.2868306
 0.28601018 0.28523493 0.2845261  0.28393453 0.28338897 0.2828927
 0.28234115 0.28173578 0.28115764 0.28065968 0.28032938 0.28007752
 0.27983934 0.27954215 0.27914903 0.2786932  0.27824435 0.27784136
 0.27747256 0.27709052 0.2767891  0.27652806 0.2762847  0.27606675
 0.27587003 0.27568263 0.2754718  0.27525088 0.2750611  0.274915
 0.27476624 0.27461517 0.27445817 0.2743343  0.2742905  0.27428326
 0.27422777 0.27405003 0.27382183 0.27357933 0.27345395 0.2735212
 0.27375636 0.27401453 0.2740636  0.27383    0.2733639  0.27289265
 0.27264756 0.2727229  0.27293345 0.27298838 0.27266428 0.2720656
 0.27145642 0.2711734  0.27132174 0.2717188  0.27193448 0.27179167
 0.27132207 0.2708438  0.270603   0.27067536 0.2709354  0.27112365
 0.27105772 0.27071255 0.27028424 0.26998875 0.26995316 0.27011305
 0.2702711  0.2702018  0.26973227 0.2688782  0.2677188  0.26627314
 0.2646459  0.26312697 0.26184896 0.2608652  0.2601123  0.25949904
 0.25890967 0.25823355 0.25740063 0.25639    0.2552672  0.25418824
 0.25324294 0.25245005 0.25186935 0.25141627 0.25098643 0.25053596
 0.25003693 0.24949573 0.24904662 0.24868372 0.24843448 0.24824895
 0.24803013 0.2477723  0.2474296  0.24701548 0.24664167 0.24632572
 0.24598144 0.24562246 0.24536075 0.24519712 0.24521562 0.2453728
 0.2456178  0.2456995  0.24564144 0.24559696 0.24566671 0.24587007
 0.24615683 0.24646932 0.24669449 0.24684125 0.24701735 0.24733602
 0.24775064 0.24810632 0.2482904  0.24837773 0.24843113 0.24849492
 0.24852508 0.24851896 0.24852368 0.24843042 0.24848558 0.24868155
 0.2489562  0.24918406 0.24917652 0.24902132 0.24869342 0.24839544
 0.24818817 0.24813628 0.24804537 0.24774401 0.2474497  0.24721286
 0.24726309 0.2475351  0.24784097 0.24793853 0.2478042  0.24742557
 0.24719377 0.24725072 0.24749994 0.2478443  0.2480475  0.24794358
 0.2476378  0.24753286 0.24772808 0.24818559 0.24877498 0.2491766
 0.24919988 0.24898005 0.24868007 0.2483228  0.2478479  0.24717426
 0.24621646 0.24498983 0.24360952 0.24225773 0.24122249 0.24056101
 0.24017136 0.23984842 0.23963638 0.23927297 0.23876505 0.23816091
 0.23759978 0.2371602  0.23678249 0.23630624 0.23577027 0.23512618
 0.23458102 0.2342401  0.23398225 0.23382968 0.23346359 0.23278347
 0.23201388 0.23139517 0.23116665 0.23103689 0.23069431 0.2298724
 0.22882496 0.22774331 0.22712824 0.22718732 0.22760202 0.2278174
 0.22735843 0.2264685  0.22561663 0.22545362 0.2257155  0.22601731
 0.22582424 0.2252596  0.22496255 0.2254318  0.2261907  0.22504698]
