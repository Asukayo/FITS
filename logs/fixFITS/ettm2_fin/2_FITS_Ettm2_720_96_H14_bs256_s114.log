Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_96_FITS_ETTm2_ftM_sl720_ll48_pl96_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 11425
test 11425
Model(
  (freq_upsampler): Linear(in_features=122, out_features=138, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  60340224.0
params:  16974.0
Trainable parameters:  16974
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.253778219223022
Epoch: 1, Steps: 65 | Train Loss: 0.3647562 Vali Loss: 0.2143244 Test Loss: 0.2763246
Validation loss decreased (inf --> 0.214324).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.486297130584717
Epoch: 2, Steps: 65 | Train Loss: 0.2784806 Vali Loss: 0.1926255 Test Loss: 0.2447510
Validation loss decreased (0.214324 --> 0.192626).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.376732349395752
Epoch: 3, Steps: 65 | Train Loss: 0.2338411 Vali Loss: 0.1840844 Test Loss: 0.2334306
Validation loss decreased (0.192626 --> 0.184084).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.999604940414429
Epoch: 4, Steps: 65 | Train Loss: 0.2064935 Vali Loss: 0.1802751 Test Loss: 0.2281455
Validation loss decreased (0.184084 --> 0.180275).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.402713060379028
Epoch: 5, Steps: 65 | Train Loss: 0.1871292 Vali Loss: 0.1777175 Test Loss: 0.2249175
Validation loss decreased (0.180275 --> 0.177718).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.884655952453613
Epoch: 6, Steps: 65 | Train Loss: 0.1722204 Vali Loss: 0.1754806 Test Loss: 0.2223809
Validation loss decreased (0.177718 --> 0.175481).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.339603662490845
Epoch: 7, Steps: 65 | Train Loss: 0.1606943 Vali Loss: 0.1735868 Test Loss: 0.2203893
Validation loss decreased (0.175481 --> 0.173587).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.654596328735352
Epoch: 8, Steps: 65 | Train Loss: 0.1510654 Vali Loss: 0.1715879 Test Loss: 0.2182262
Validation loss decreased (0.173587 --> 0.171588).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.460476875305176
Epoch: 9, Steps: 65 | Train Loss: 0.1425155 Vali Loss: 0.1696785 Test Loss: 0.2161334
Validation loss decreased (0.171588 --> 0.169679).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.677595853805542
Epoch: 10, Steps: 65 | Train Loss: 0.1351354 Vali Loss: 0.1681633 Test Loss: 0.2142027
Validation loss decreased (0.169679 --> 0.168163).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.899250030517578
Epoch: 11, Steps: 65 | Train Loss: 0.1286955 Vali Loss: 0.1663617 Test Loss: 0.2124736
Validation loss decreased (0.168163 --> 0.166362).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.38696813583374
Epoch: 12, Steps: 65 | Train Loss: 0.1228779 Vali Loss: 0.1646582 Test Loss: 0.2107650
Validation loss decreased (0.166362 --> 0.164658).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.627750635147095
Epoch: 13, Steps: 65 | Train Loss: 0.1182519 Vali Loss: 0.1630925 Test Loss: 0.2090842
Validation loss decreased (0.164658 --> 0.163092).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.10485577583313
Epoch: 14, Steps: 65 | Train Loss: 0.1134689 Vali Loss: 0.1616421 Test Loss: 0.2074652
Validation loss decreased (0.163092 --> 0.161642).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.359646797180176
Epoch: 15, Steps: 65 | Train Loss: 0.1096108 Vali Loss: 0.1603432 Test Loss: 0.2060060
Validation loss decreased (0.161642 --> 0.160343).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.32923412322998
Epoch: 16, Steps: 65 | Train Loss: 0.1058472 Vali Loss: 0.1588683 Test Loss: 0.2045343
Validation loss decreased (0.160343 --> 0.158868).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.49448561668396
Epoch: 17, Steps: 65 | Train Loss: 0.1024815 Vali Loss: 0.1572128 Test Loss: 0.2031544
Validation loss decreased (0.158868 --> 0.157213).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.776550054550171
Epoch: 18, Steps: 65 | Train Loss: 0.0994573 Vali Loss: 0.1566882 Test Loss: 0.2019904
Validation loss decreased (0.157213 --> 0.156688).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.547837495803833
Epoch: 19, Steps: 65 | Train Loss: 0.0965381 Vali Loss: 0.1549470 Test Loss: 0.2008079
Validation loss decreased (0.156688 --> 0.154947).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.231848001480103
Epoch: 20, Steps: 65 | Train Loss: 0.0940109 Vali Loss: 0.1540845 Test Loss: 0.1995943
Validation loss decreased (0.154947 --> 0.154085).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.792547702789307
Epoch: 21, Steps: 65 | Train Loss: 0.0914585 Vali Loss: 0.1528887 Test Loss: 0.1985432
Validation loss decreased (0.154085 --> 0.152889).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.289611339569092
Epoch: 22, Steps: 65 | Train Loss: 0.0894153 Vali Loss: 0.1518570 Test Loss: 0.1975486
Validation loss decreased (0.152889 --> 0.151857).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.692927122116089
Epoch: 23, Steps: 65 | Train Loss: 0.0875238 Vali Loss: 0.1511992 Test Loss: 0.1965997
Validation loss decreased (0.151857 --> 0.151199).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 10.610586166381836
Epoch: 24, Steps: 65 | Train Loss: 0.0856375 Vali Loss: 0.1505989 Test Loss: 0.1957519
Validation loss decreased (0.151199 --> 0.150599).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.317453384399414
Epoch: 25, Steps: 65 | Train Loss: 0.0837144 Vali Loss: 0.1496940 Test Loss: 0.1949597
Validation loss decreased (0.150599 --> 0.149694).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.455332517623901
Epoch: 26, Steps: 65 | Train Loss: 0.0822084 Vali Loss: 0.1487119 Test Loss: 0.1941575
Validation loss decreased (0.149694 --> 0.148712).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.213348865509033
Epoch: 27, Steps: 65 | Train Loss: 0.0806984 Vali Loss: 0.1480032 Test Loss: 0.1933815
Validation loss decreased (0.148712 --> 0.148003).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 8.8499276638031
Epoch: 28, Steps: 65 | Train Loss: 0.0791296 Vali Loss: 0.1477414 Test Loss: 0.1927644
Validation loss decreased (0.148003 --> 0.147741).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 7.013319730758667
Epoch: 29, Steps: 65 | Train Loss: 0.0780161 Vali Loss: 0.1467847 Test Loss: 0.1920608
Validation loss decreased (0.147741 --> 0.146785).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 10.13492202758789
Epoch: 30, Steps: 65 | Train Loss: 0.0766868 Vali Loss: 0.1460166 Test Loss: 0.1914952
Validation loss decreased (0.146785 --> 0.146017).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 10.150326490402222
Epoch: 31, Steps: 65 | Train Loss: 0.0756076 Vali Loss: 0.1457279 Test Loss: 0.1909617
Validation loss decreased (0.146017 --> 0.145728).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.397791862487793
Epoch: 32, Steps: 65 | Train Loss: 0.0744972 Vali Loss: 0.1451004 Test Loss: 0.1904207
Validation loss decreased (0.145728 --> 0.145100).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 11.545191287994385
Epoch: 33, Steps: 65 | Train Loss: 0.0734855 Vali Loss: 0.1447500 Test Loss: 0.1898516
Validation loss decreased (0.145100 --> 0.144750).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 11.956993579864502
Epoch: 34, Steps: 65 | Train Loss: 0.0724899 Vali Loss: 0.1443277 Test Loss: 0.1893591
Validation loss decreased (0.144750 --> 0.144328).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 9.651864290237427
Epoch: 35, Steps: 65 | Train Loss: 0.0717369 Vali Loss: 0.1436451 Test Loss: 0.1888438
Validation loss decreased (0.144328 --> 0.143645).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 9.43854808807373
Epoch: 36, Steps: 65 | Train Loss: 0.0709607 Vali Loss: 0.1434855 Test Loss: 0.1885576
Validation loss decreased (0.143645 --> 0.143486).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 9.275797843933105
Epoch: 37, Steps: 65 | Train Loss: 0.0699001 Vali Loss: 0.1430703 Test Loss: 0.1881458
Validation loss decreased (0.143486 --> 0.143070).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 9.318817853927612
Epoch: 38, Steps: 65 | Train Loss: 0.0695086 Vali Loss: 0.1424138 Test Loss: 0.1876636
Validation loss decreased (0.143070 --> 0.142414).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 9.456471681594849
Epoch: 39, Steps: 65 | Train Loss: 0.0687753 Vali Loss: 0.1424793 Test Loss: 0.1873379
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 9.2335786819458
Epoch: 40, Steps: 65 | Train Loss: 0.0680580 Vali Loss: 0.1419805 Test Loss: 0.1869949
Validation loss decreased (0.142414 --> 0.141980).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 9.482119798660278
Epoch: 41, Steps: 65 | Train Loss: 0.0676344 Vali Loss: 0.1417842 Test Loss: 0.1866374
Validation loss decreased (0.141980 --> 0.141784).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 9.831265211105347
Epoch: 42, Steps: 65 | Train Loss: 0.0670758 Vali Loss: 0.1413243 Test Loss: 0.1863221
Validation loss decreased (0.141784 --> 0.141324).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 10.257984399795532
Epoch: 43, Steps: 65 | Train Loss: 0.0664208 Vali Loss: 0.1410899 Test Loss: 0.1859992
Validation loss decreased (0.141324 --> 0.141090).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 9.709065198898315
Epoch: 44, Steps: 65 | Train Loss: 0.0657561 Vali Loss: 0.1408166 Test Loss: 0.1857205
Validation loss decreased (0.141090 --> 0.140817).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 9.522521734237671
Epoch: 45, Steps: 65 | Train Loss: 0.0653292 Vali Loss: 0.1403459 Test Loss: 0.1854319
Validation loss decreased (0.140817 --> 0.140346).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.497843503952026
Epoch: 46, Steps: 65 | Train Loss: 0.0650136 Vali Loss: 0.1401165 Test Loss: 0.1852369
Validation loss decreased (0.140346 --> 0.140116).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 7.6273956298828125
Epoch: 47, Steps: 65 | Train Loss: 0.0644981 Vali Loss: 0.1399178 Test Loss: 0.1849862
Validation loss decreased (0.140116 --> 0.139918).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 8.344912052154541
Epoch: 48, Steps: 65 | Train Loss: 0.0640857 Vali Loss: 0.1396743 Test Loss: 0.1847218
Validation loss decreased (0.139918 --> 0.139674).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 9.215446949005127
Epoch: 49, Steps: 65 | Train Loss: 0.0637218 Vali Loss: 0.1396061 Test Loss: 0.1844991
Validation loss decreased (0.139674 --> 0.139606).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 9.188483476638794
Epoch: 50, Steps: 65 | Train Loss: 0.0633802 Vali Loss: 0.1393487 Test Loss: 0.1842960
Validation loss decreased (0.139606 --> 0.139349).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 33745
val 11425
test 11425
Model(
  (freq_upsampler): Linear(in_features=122, out_features=138, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  60340224.0
params:  16974.0
Trainable parameters:  16974
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.970473766326904
Epoch: 1, Steps: 65 | Train Loss: 0.2329814 Vali Loss: 0.1225674 Test Loss: 0.1676349
Validation loss decreased (inf --> 0.122567).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.504781723022461
Epoch: 2, Steps: 65 | Train Loss: 0.2180684 Vali Loss: 0.1185150 Test Loss: 0.1642082
Validation loss decreased (0.122567 --> 0.118515).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.905369758605957
Epoch: 3, Steps: 65 | Train Loss: 0.2136056 Vali Loss: 0.1167090 Test Loss: 0.1627440
Validation loss decreased (0.118515 --> 0.116709).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.844397068023682
Epoch: 4, Steps: 65 | Train Loss: 0.2114480 Vali Loss: 0.1159799 Test Loss: 0.1620996
Validation loss decreased (0.116709 --> 0.115980).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.833787202835083
Epoch: 5, Steps: 65 | Train Loss: 0.2103408 Vali Loss: 0.1155178 Test Loss: 0.1617826
Validation loss decreased (0.115980 --> 0.115518).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.378558874130249
Epoch: 6, Steps: 65 | Train Loss: 0.2101254 Vali Loss: 0.1153236 Test Loss: 0.1615108
Validation loss decreased (0.115518 --> 0.115324).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.665231466293335
Epoch: 7, Steps: 65 | Train Loss: 0.2093256 Vali Loss: 0.1152574 Test Loss: 0.1615295
Validation loss decreased (0.115324 --> 0.115257).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.264600276947021
Epoch: 8, Steps: 65 | Train Loss: 0.2087260 Vali Loss: 0.1149030 Test Loss: 0.1613744
Validation loss decreased (0.115257 --> 0.114903).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.812264680862427
Epoch: 9, Steps: 65 | Train Loss: 0.2083772 Vali Loss: 0.1146391 Test Loss: 0.1610575
Validation loss decreased (0.114903 --> 0.114639).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.004817008972168
Epoch: 10, Steps: 65 | Train Loss: 0.2077213 Vali Loss: 0.1149364 Test Loss: 0.1611647
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.651194334030151
Epoch: 11, Steps: 65 | Train Loss: 0.2068699 Vali Loss: 0.1148750 Test Loss: 0.1609672
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.383033990859985
Epoch: 12, Steps: 65 | Train Loss: 0.2075864 Vali Loss: 0.1145548 Test Loss: 0.1607778
Validation loss decreased (0.114639 --> 0.114555).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.638837814331055
Epoch: 13, Steps: 65 | Train Loss: 0.2075365 Vali Loss: 0.1144300 Test Loss: 0.1609382
Validation loss decreased (0.114555 --> 0.114430).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.66752314567566
Epoch: 14, Steps: 65 | Train Loss: 0.2077429 Vali Loss: 0.1146170 Test Loss: 0.1607993
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.112630128860474
Epoch: 15, Steps: 65 | Train Loss: 0.2068391 Vali Loss: 0.1145491 Test Loss: 0.1608146
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.797425985336304
Epoch: 16, Steps: 65 | Train Loss: 0.2063934 Vali Loss: 0.1142364 Test Loss: 0.1607063
Validation loss decreased (0.114430 --> 0.114236).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.843545913696289
Epoch: 17, Steps: 65 | Train Loss: 0.2074066 Vali Loss: 0.1143783 Test Loss: 0.1606323
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.77638030052185
Epoch: 18, Steps: 65 | Train Loss: 0.2062892 Vali Loss: 0.1144050 Test Loss: 0.1606482
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.824065208435059
Epoch: 19, Steps: 65 | Train Loss: 0.2063890 Vali Loss: 0.1146557 Test Loss: 0.1606286
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_96_FITS_ETTm2_ftM_sl720_ll48_pl96_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
mse:0.1629606932401657, mae:0.2540571689605713, rse:0.3272936940193176, corr:[0.5678662  0.57137704 0.56928796 0.56728905 0.5673025  0.5682125
 0.5682268  0.56704366 0.5658735  0.56553584 0.5658559  0.5660537
 0.5656284  0.5648631  0.56436616 0.564216   0.56411994 0.5635866
 0.56254935 0.56146234 0.5607601  0.5603985  0.5600753  0.5594666
 0.5585892  0.5578034  0.55736536 0.5571759  0.55685884 0.55618733
 0.5552877  0.55460423 0.5542522  0.5540223  0.5536477  0.5530023
 0.55215657 0.55134714 0.55071175 0.5502495  0.54973847 0.54899555
 0.54812586 0.54733074 0.5467728  0.5463574  0.5458465  0.54506534
 0.5439808  0.54285794 0.54199165 0.54144704 0.54090524 0.54009575
 0.5390825  0.5382515  0.53771645 0.5372842  0.5366873  0.5358866
 0.53509694 0.53480107 0.53499585 0.53517723 0.5349588  0.53445745
 0.53391397 0.53374666 0.5338985  0.5338631  0.53334785 0.532522
 0.5319027  0.53194016 0.53217363 0.5319297  0.5310029  0.52980006
 0.52901834 0.52907777 0.5290543  0.52820915 0.526569   0.5250082
 0.52455395 0.52502906 0.5248935  0.52351415 0.52150166 0.5204591
 0.5214402  0.5226688  0.5219207  0.51969296 0.519172   0.52317506]
