Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  29030400.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5462341
	speed: 0.1340s/iter; left time: 850.8283s
Epoch: 1 cost time: 17.349676609039307
Epoch: 1, Steps: 129 | Train Loss: 0.6214404 Vali Loss: 0.2966426 Test Loss: 0.3985910
Validation loss decreased (inf --> 0.296643).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4656284
	speed: 0.3472s/iter; left time: 2160.3773s
Epoch: 2 cost time: 17.822519302368164
Epoch: 2, Steps: 129 | Train Loss: 0.5349929 Vali Loss: 0.2786798 Test Loss: 0.3798753
Validation loss decreased (0.296643 --> 0.278680).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5265902
	speed: 0.3748s/iter; left time: 2283.4645s
Epoch: 3 cost time: 18.991694927215576
Epoch: 3, Steps: 129 | Train Loss: 0.5194957 Vali Loss: 0.2725445 Test Loss: 0.3735363
Validation loss decreased (0.278680 --> 0.272545).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4345028
	speed: 0.3769s/iter; left time: 2247.6710s
Epoch: 4 cost time: 17.950681924819946
Epoch: 4, Steps: 129 | Train Loss: 0.5128507 Vali Loss: 0.2693824 Test Loss: 0.3698307
Validation loss decreased (0.272545 --> 0.269382).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4683345
	speed: 0.3659s/iter; left time: 2135.0706s
Epoch: 5 cost time: 18.561719179153442
Epoch: 5, Steps: 129 | Train Loss: 0.5086220 Vali Loss: 0.2673810 Test Loss: 0.3673878
Validation loss decreased (0.269382 --> 0.267381).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4915534
	speed: 0.3657s/iter; left time: 2086.9230s
Epoch: 6 cost time: 17.66629958152771
Epoch: 6, Steps: 129 | Train Loss: 0.5060184 Vali Loss: 0.2663469 Test Loss: 0.3658200
Validation loss decreased (0.267381 --> 0.266347).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5536667
	speed: 0.3611s/iter; left time: 2013.9902s
Epoch: 7 cost time: 17.484609127044678
Epoch: 7, Steps: 129 | Train Loss: 0.5041269 Vali Loss: 0.2652588 Test Loss: 0.3645020
Validation loss decreased (0.266347 --> 0.265259).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5500425
	speed: 0.3536s/iter; left time: 1926.1479s
Epoch: 8 cost time: 17.023685693740845
Epoch: 8, Steps: 129 | Train Loss: 0.5020257 Vali Loss: 0.2647344 Test Loss: 0.3636676
Validation loss decreased (0.265259 --> 0.264734).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5532597
	speed: 0.4081s/iter; left time: 2170.8079s
Epoch: 9 cost time: 19.590079307556152
Epoch: 9, Steps: 129 | Train Loss: 0.5012619 Vali Loss: 0.2640497 Test Loss: 0.3629147
Validation loss decreased (0.264734 --> 0.264050).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5051801
	speed: 0.3971s/iter; left time: 2061.1151s
Epoch: 10 cost time: 18.43683695793152
Epoch: 10, Steps: 129 | Train Loss: 0.5007562 Vali Loss: 0.2638915 Test Loss: 0.3620529
Validation loss decreased (0.264050 --> 0.263892).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6185868
	speed: 0.3751s/iter; left time: 1898.3875s
Epoch: 11 cost time: 19.14128875732422
Epoch: 11, Steps: 129 | Train Loss: 0.5003079 Vali Loss: 0.2632736 Test Loss: 0.3618263
Validation loss decreased (0.263892 --> 0.263274).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6027366
	speed: 0.3749s/iter; left time: 1848.9171s
Epoch: 12 cost time: 17.919941663742065
Epoch: 12, Steps: 129 | Train Loss: 0.4994909 Vali Loss: 0.2633533 Test Loss: 0.3613014
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6220955
	speed: 0.3620s/iter; left time: 1738.9089s
Epoch: 13 cost time: 20.729311227798462
Epoch: 13, Steps: 129 | Train Loss: 0.4986367 Vali Loss: 0.2630522 Test Loss: 0.3609440
Validation loss decreased (0.263274 --> 0.263052).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4419887
	speed: 0.4416s/iter; left time: 2063.9608s
Epoch: 14 cost time: 21.633363485336304
Epoch: 14, Steps: 129 | Train Loss: 0.4983075 Vali Loss: 0.2629052 Test Loss: 0.3605920
Validation loss decreased (0.263052 --> 0.262905).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5216931
	speed: 0.4093s/iter; left time: 1860.3181s
Epoch: 15 cost time: 19.82959294319153
Epoch: 15, Steps: 129 | Train Loss: 0.4980352 Vali Loss: 0.2627527 Test Loss: 0.3603245
Validation loss decreased (0.262905 --> 0.262753).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4563097
	speed: 0.3884s/iter; left time: 1715.1468s
Epoch: 16 cost time: 19.41986322402954
Epoch: 16, Steps: 129 | Train Loss: 0.4972933 Vali Loss: 0.2624552 Test Loss: 0.3601331
Validation loss decreased (0.262753 --> 0.262455).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4688514
	speed: 0.3788s/iter; left time: 1624.0507s
Epoch: 17 cost time: 17.938165187835693
Epoch: 17, Steps: 129 | Train Loss: 0.4974181 Vali Loss: 0.2621409 Test Loss: 0.3599972
Validation loss decreased (0.262455 --> 0.262141).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5122979
	speed: 0.3789s/iter; left time: 1575.3911s
Epoch: 18 cost time: 18.214502334594727
Epoch: 18, Steps: 129 | Train Loss: 0.4971399 Vali Loss: 0.2624975 Test Loss: 0.3597781
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3993979
	speed: 0.3836s/iter; left time: 1545.6091s
Epoch: 19 cost time: 18.949209213256836
Epoch: 19, Steps: 129 | Train Loss: 0.4975307 Vali Loss: 0.2621441 Test Loss: 0.3596585
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4267817
	speed: 0.3707s/iter; left time: 1445.7469s
Epoch: 20 cost time: 17.667357921600342
Epoch: 20, Steps: 129 | Train Loss: 0.4959053 Vali Loss: 0.2621642 Test Loss: 0.3595197
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.35081544518470764, mae:0.37900641560554504, rse:0.4760841429233551, corr:[0.5255429  0.5377798  0.5405163  0.5374956  0.53504586 0.53495705
 0.5366685  0.53877264 0.5397288  0.53908354 0.5377306  0.5367681
 0.5366922  0.5373803  0.5382928  0.5386812  0.538154   0.53696406
 0.53571135 0.53484654 0.5345099  0.5345302  0.5345772  0.53432214
 0.53365064 0.53276867 0.5319703  0.53142524 0.5311257  0.5309606
 0.53074926 0.53034395 0.5296952  0.5288904  0.5280912  0.5274028
 0.52683216 0.52635926 0.5258943  0.52536595 0.5247555  0.5240875
 0.52342385 0.52278197 0.52216595 0.5215755  0.52098256 0.52033806
 0.5196041  0.5187687  0.5179123  0.5171175  0.51642567 0.51589257
 0.5155124  0.51519936 0.5148647  0.5144657  0.5140151  0.51354223
 0.51316303 0.5128872  0.51266515 0.51241595 0.5120739  0.5116549
 0.5111385  0.51065093 0.5102735  0.5100193  0.50981593 0.5095639
 0.5092071  0.50869024 0.5080205  0.5072848  0.5066185  0.5060571
 0.5056     0.5050954  0.5044554  0.5036755  0.50284624 0.5020822
 0.5014964  0.50108755 0.5007572  0.5004361  0.5000213  0.49949422
 0.4988582  0.49810946 0.49730137 0.49644098 0.4954377  0.4942266
 0.4928085  0.4912595  0.48967516 0.48821723 0.48703215 0.48616028
 0.4854703  0.4847774  0.48392305 0.48286083 0.4816441  0.48039967
 0.47924778 0.47826067 0.47740844 0.47658423 0.47572765 0.47479093
 0.47382265 0.47293395 0.47220388 0.47158962 0.47097614 0.47022673
 0.46929866 0.4681891  0.46702257 0.46592224 0.4649947  0.4642443
 0.46356788 0.46278843 0.46178317 0.46055448 0.45926782 0.45813537
 0.4573397  0.45687017 0.45658445 0.45624176 0.45566458 0.45484516
 0.45388708 0.45296863 0.45222256 0.45165566 0.45114017 0.4504688
 0.44952026 0.44830376 0.44706234 0.4460462  0.44544452 0.44521555
 0.44513735 0.44489855 0.44426093 0.4432541  0.44208    0.4410345
 0.44034278 0.44003162 0.4399349  0.43983215 0.439506   0.43891883
 0.43818176 0.43750247 0.43701982 0.43671542 0.43645468 0.43608215
 0.43551064 0.43479422 0.43406132 0.4335428  0.43330443 0.43326604
 0.4331991  0.43288317 0.4321941  0.43118015 0.43006167 0.429128
 0.42854434 0.42831275 0.42825985 0.42812446 0.42773405 0.42704344
 0.4262061  0.42531517 0.42446566 0.42358834 0.4225584  0.42122406
 0.4195875  0.4178812  0.41624615 0.41485804 0.41378775 0.41290596
 0.41200054 0.41087005 0.40943348 0.40782928 0.4063431  0.40524164
 0.4046043  0.4042586  0.4038709  0.40315568 0.4020351  0.4006518
 0.3992042  0.3979761  0.3971499  0.39668712 0.3963648  0.39583984
 0.394905   0.3935125  0.39190608 0.3903772  0.38914248 0.38824162
 0.38753235 0.38680997 0.38595134 0.38486442 0.3836584  0.38258693
 0.38176844 0.3812467  0.38087285 0.38046774 0.37989688 0.37917218
 0.37837908 0.3777036  0.37724066 0.3769547  0.37671366 0.3763369
 0.37573802 0.37493944 0.37412924 0.37359446 0.3733781  0.37342715
 0.37354183 0.37352142 0.37324414 0.37276384 0.37230068 0.37203002
 0.37200522 0.37214747 0.37222227 0.3720597  0.37159595 0.37096253
 0.37035838 0.36994404 0.36974612 0.36966255 0.36948875 0.36913592
 0.36859554 0.3679668  0.36747164 0.3672488  0.36723682 0.3672298
 0.36704716 0.36654627 0.36573124 0.364851   0.364183   0.36391658
 0.36406893 0.36438343 0.364561   0.3644694  0.36407188 0.36356333
 0.3631619  0.36301884 0.36306873 0.3630382  0.36262518 0.36158234
 0.36000884 0.358284   0.35679838 0.35581288 0.35536724 0.3552082
 0.3550432  0.35455355 0.3536139  0.35235476 0.3511024  0.3501064
 0.34952408 0.3491805  0.34888902 0.34837395 0.34761423 0.34672904
 0.3460126  0.3456135  0.34554234 0.3456477  0.34567723 0.34548774
 0.34493312 0.3441447  0.3433155  0.34266075 0.34231293 0.34221965
 0.34216616 0.3420008  0.34167427 0.34125656 0.3409113  0.34082678
 0.34103572 0.34144622 0.34181663 0.34188163 0.34159073 0.34101647
 0.34043154 0.34004784 0.33999223 0.34021232 0.34045994 0.3405424
 0.34030125 0.33970806 0.33900318 0.33848637 0.33831745 0.33843863
 0.3386827  0.3388343  0.33877918 0.33850628 0.33816016 0.33793843
 0.3379039  0.3379983  0.33802304 0.33780402 0.3373081  0.33664393
 0.3360545  0.33577386 0.335834   0.33614874 0.3364118  0.33641833
 0.33608422 0.33551747 0.33495897 0.3346423  0.33464056 0.33488736
 0.33510727 0.33510548 0.3347665  0.33422077 0.33375543 0.3336132
 0.3339171  0.3345764  0.33530372 0.33579734 0.33587527 0.33559266
 0.33516338 0.33479393 0.33457682 0.33441722 0.33421072 0.33371562
 0.33288905 0.33180568 0.33066744 0.32966694 0.3288891  0.32838196
 0.32796377 0.327461   0.3267681  0.32591528 0.32506034 0.3243554
 0.32390013 0.3236973  0.323543   0.32324627 0.32275218 0.32201138
 0.3211474  0.32031086 0.31967157 0.31931946 0.31910983 0.31894276
 0.3186992  0.31840363 0.3181256  0.3179347  0.31789467 0.31793302
 0.3179968  0.31798735 0.31777322 0.31737432 0.316906   0.3165722
 0.3165243  0.31675923 0.31713364 0.31747127 0.31762812 0.31754708
 0.3173087  0.31703845 0.3168859  0.31688115 0.31698465 0.3170834
 0.31705105 0.3167856  0.31637678 0.31598693 0.31575972 0.31568286
 0.3156245  0.3155213  0.31525663 0.31479692 0.31426117 0.31385756
 0.31363255 0.3136078  0.31365585 0.31360286 0.31338552 0.31300634
 0.31261027 0.3123221  0.3122305  0.31223854 0.31222808 0.31209
 0.31173795 0.31120226 0.3106059  0.31011397 0.30985683 0.30974668
 0.30967835 0.30950373 0.30917063 0.30872744 0.30830657 0.30803812
 0.30798513 0.30807152 0.30814022 0.3080446  0.30777028 0.30733898
 0.30686742 0.30637568 0.30588704 0.30527294 0.30443054 0.30325815
 0.30179304 0.30029398 0.29895815 0.29794762 0.29727054 0.2967743
 0.29623944 0.29556    0.2946911  0.29379743 0.29300198 0.2923796
 0.29188955 0.29147005 0.29094836 0.2901715  0.28914326 0.28798634
 0.28694394 0.2861152  0.2855257  0.28514326 0.2847824  0.2843585
 0.28374237 0.28295562 0.28216428 0.28151444 0.28114864 0.28097615
 0.28087765 0.28069943 0.2803259  0.2797653  0.2791538  0.27863643
 0.27828664 0.2780597  0.27796862 0.27786463 0.27764034 0.27728984
 0.27687773 0.27652207 0.27628878 0.2762005  0.27622083 0.27622458
 0.27605376 0.27567607 0.2751555  0.27464816 0.27430895 0.27415296
 0.27410352 0.27407125 0.27407077 0.27404785 0.27403608 0.2740681
 0.274159   0.27428082 0.274313   0.2742062  0.27391535 0.2735074
 0.2731184  0.27292344 0.2729478  0.2730829  0.27311626 0.27293885
 0.2724798  0.27186754 0.2713069  0.27101627 0.27097046 0.2711519
 0.27140415 0.27161652 0.27165338 0.27149907 0.27127817 0.27112812
 0.27112198 0.2711971  0.27125767 0.27119002 0.27096975 0.27065232
 0.27033037 0.27004    0.26968953 0.26917088 0.26835635 0.26710862
 0.26549268 0.2638647  0.2624608  0.26139295 0.26057175 0.2598491
 0.25907347 0.2581719  0.25716206 0.25613233 0.25520205 0.25447735
 0.25392804 0.25343594 0.252975   0.25246084 0.25186545 0.25124848
 0.25065807 0.2500979  0.24963774 0.24921264 0.24885152 0.24856474
 0.2483206  0.24813329 0.24793339 0.24767695 0.24743289 0.24721292
 0.24695882 0.24667886 0.24644567 0.2462025  0.24600631 0.24587785
 0.24588178 0.24588525 0.24591476 0.24602069 0.24616633 0.24629699
 0.24640687 0.24656953 0.24679922 0.2471202  0.24754131 0.24802195
 0.24841963 0.2486154  0.24861693 0.24861468 0.2487203  0.24894156
 0.24915206 0.24925655 0.24923722 0.24897538 0.24876118 0.24871598
 0.24893096 0.24936172 0.24976794 0.25003347 0.24988604 0.24939069
 0.24870309 0.24820055 0.24801016 0.24805877 0.24834432 0.24853227
 0.24854401 0.24833219 0.2480322  0.24781433 0.24785282 0.24798076
 0.2481839  0.24824636 0.24802251 0.24768417 0.24741289 0.2473021
 0.24737927 0.24772121 0.24811885 0.248444   0.24873444 0.24897882
 0.24919412 0.2494604  0.24969377 0.24964443 0.24910893 0.24804688
 0.24655932 0.24489312 0.24331836 0.24201147 0.24112208 0.24055889
 0.24015163 0.23971833 0.2393574  0.23890214 0.23834637 0.23765458
 0.23690343 0.23620716 0.23565821 0.23526548 0.235088   0.2349033
 0.23459941 0.23408063 0.23330069 0.2325544  0.23189688 0.23138979
 0.23108186 0.23083559 0.23060241 0.23018102 0.22961476 0.2289634
 0.22848797 0.22807568 0.22780426 0.22767514 0.22759461 0.22748992
 0.22716345 0.22659463 0.22578652 0.22511804 0.22460222 0.22458561
 0.22516602 0.22611512 0.22677334 0.22623545 0.22344343 0.21775308]
