Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=74, out_features=148, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4906496.0
params:  11100.0
Trainable parameters:  11100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5545471
	speed: 0.0796s/iter; left time: 2049.5865s
	iters: 200, epoch: 1 | loss: 0.3702533
	speed: 0.0774s/iter; left time: 1986.2120s
	iters: 300, epoch: 1 | loss: 0.2674796
	speed: 0.0825s/iter; left time: 2107.7066s
	iters: 400, epoch: 1 | loss: 0.2601131
	speed: 0.0776s/iter; left time: 1975.2877s
	iters: 500, epoch: 1 | loss: 0.3832476
	speed: 0.0835s/iter; left time: 2117.9908s
Epoch: 1 cost time: 41.624961376190186
Epoch: 1, Steps: 517 | Train Loss: 0.3917093 Vali Loss: 0.2853885 Test Loss: 0.3764090
Validation loss decreased (inf --> 0.285389).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3024368
	speed: 0.5527s/iter; left time: 13945.7986s
	iters: 200, epoch: 2 | loss: 0.1581405
	speed: 0.0901s/iter; left time: 2263.9759s
	iters: 300, epoch: 2 | loss: 0.3771021
	speed: 0.0978s/iter; left time: 2447.1066s
	iters: 400, epoch: 2 | loss: 0.3012973
	speed: 0.0748s/iter; left time: 1865.3964s
	iters: 500, epoch: 2 | loss: 0.2495495
	speed: 0.0886s/iter; left time: 2199.1622s
Epoch: 2 cost time: 46.30123782157898
Epoch: 2, Steps: 517 | Train Loss: 0.2892110 Vali Loss: 0.2737013 Test Loss: 0.3625808
Validation loss decreased (0.285389 --> 0.273701).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2436032
	speed: 0.5876s/iter; left time: 14524.7930s
	iters: 200, epoch: 3 | loss: 0.1678556
	speed: 0.0809s/iter; left time: 1992.2139s
	iters: 300, epoch: 3 | loss: 0.2412877
	speed: 0.0821s/iter; left time: 2012.0903s
	iters: 400, epoch: 3 | loss: 0.1786295
	speed: 0.0793s/iter; left time: 1936.3894s
	iters: 500, epoch: 3 | loss: 0.2251459
	speed: 0.0773s/iter; left time: 1879.5760s
Epoch: 3 cost time: 42.07127022743225
Epoch: 3, Steps: 517 | Train Loss: 0.2692351 Vali Loss: 0.2685314 Test Loss: 0.3570609
Validation loss decreased (0.273701 --> 0.268531).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2508778
	speed: 0.5386s/iter; left time: 13033.8355s
	iters: 200, epoch: 4 | loss: 0.1970703
	speed: 0.0822s/iter; left time: 1979.9763s
	iters: 300, epoch: 4 | loss: 0.2307859
	speed: 0.0829s/iter; left time: 1988.4969s
	iters: 400, epoch: 4 | loss: 0.2409387
	speed: 0.0799s/iter; left time: 1909.9147s
	iters: 500, epoch: 4 | loss: 0.3514539
	speed: 0.0825s/iter; left time: 1964.0332s
Epoch: 4 cost time: 42.5492262840271
Epoch: 4, Steps: 517 | Train Loss: 0.2630116 Vali Loss: 0.2662016 Test Loss: 0.3544044
Validation loss decreased (0.268531 --> 0.266202).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3486227
	speed: 0.5196s/iter; left time: 12306.5276s
	iters: 200, epoch: 5 | loss: 0.3293290
	speed: 0.0827s/iter; left time: 1950.6090s
	iters: 300, epoch: 5 | loss: 0.3819537
	speed: 0.0777s/iter; left time: 1825.3045s
	iters: 400, epoch: 5 | loss: 0.2936219
	speed: 0.0786s/iter; left time: 1838.2908s
	iters: 500, epoch: 5 | loss: 0.3277477
	speed: 0.0806s/iter; left time: 1876.3008s
Epoch: 5 cost time: 42.09841322898865
Epoch: 5, Steps: 517 | Train Loss: 0.2607982 Vali Loss: 0.2657928 Test Loss: 0.3534194
Validation loss decreased (0.266202 --> 0.265793).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2823720
	speed: 0.4988s/iter; left time: 11554.7026s
	iters: 200, epoch: 6 | loss: 0.2478780
	speed: 0.0837s/iter; left time: 1931.2706s
	iters: 300, epoch: 6 | loss: 0.2636569
	speed: 0.0894s/iter; left time: 2052.2046s
	iters: 400, epoch: 6 | loss: 0.2666878
	speed: 0.0771s/iter; left time: 1762.7548s
	iters: 500, epoch: 6 | loss: 0.3202352
	speed: 0.0760s/iter; left time: 1729.4102s
Epoch: 6 cost time: 43.32116341590881
Epoch: 6, Steps: 517 | Train Loss: 0.2603947 Vali Loss: 0.2648833 Test Loss: 0.3527771
Validation loss decreased (0.265793 --> 0.264883).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3248164
	speed: 0.4924s/iter; left time: 11152.9236s
	iters: 200, epoch: 7 | loss: 0.1975096
	speed: 0.0833s/iter; left time: 1878.4615s
	iters: 300, epoch: 7 | loss: 0.3119433
	speed: 0.0415s/iter; left time: 932.1026s
	iters: 400, epoch: 7 | loss: 0.2513504
	speed: 0.0455s/iter; left time: 1016.1815s
	iters: 500, epoch: 7 | loss: 0.1941431
	speed: 0.0425s/iter; left time: 944.9885s
Epoch: 7 cost time: 29.482141256332397
Epoch: 7, Steps: 517 | Train Loss: 0.2600856 Vali Loss: 0.2648533 Test Loss: 0.3528714
Validation loss decreased (0.264883 --> 0.264853).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3414850
	speed: 0.4930s/iter; left time: 10911.7585s
	iters: 200, epoch: 8 | loss: 0.4131304
	speed: 0.0802s/iter; left time: 1767.8065s
	iters: 300, epoch: 8 | loss: 0.1881114
	speed: 0.0742s/iter; left time: 1626.5147s
	iters: 400, epoch: 8 | loss: 0.3099588
	speed: 0.0891s/iter; left time: 1944.7539s
	iters: 500, epoch: 8 | loss: 0.3387505
	speed: 0.0889s/iter; left time: 1931.6123s
Epoch: 8 cost time: 44.46268844604492
Epoch: 8, Steps: 517 | Train Loss: 0.2598982 Vali Loss: 0.2649679 Test Loss: 0.3522455
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2318567
	speed: 0.5640s/iter; left time: 12190.9817s
	iters: 200, epoch: 9 | loss: 0.3318234
	speed: 0.0873s/iter; left time: 1877.3387s
	iters: 300, epoch: 9 | loss: 0.3756454
	speed: 0.0798s/iter; left time: 1708.5746s
	iters: 400, epoch: 9 | loss: 0.2560698
	speed: 0.0892s/iter; left time: 1900.9814s
	iters: 500, epoch: 9 | loss: 0.1950358
	speed: 0.0855s/iter; left time: 1813.8727s
Epoch: 9 cost time: 45.809961795806885
Epoch: 9, Steps: 517 | Train Loss: 0.2597963 Vali Loss: 0.2648299 Test Loss: 0.3523778
Validation loss decreased (0.264853 --> 0.264830).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3522862
	speed: 0.4862s/iter; left time: 10258.7658s
	iters: 200, epoch: 10 | loss: 0.2807061
	speed: 0.0835s/iter; left time: 1753.7021s
	iters: 300, epoch: 10 | loss: 0.2758271
	speed: 0.0777s/iter; left time: 1624.5472s
	iters: 400, epoch: 10 | loss: 0.3144808
	speed: 0.0714s/iter; left time: 1485.6904s
	iters: 500, epoch: 10 | loss: 0.2914574
	speed: 0.0806s/iter; left time: 1667.4080s
Epoch: 10 cost time: 41.02577519416809
Epoch: 10, Steps: 517 | Train Loss: 0.2597794 Vali Loss: 0.2645144 Test Loss: 0.3520562
Validation loss decreased (0.264830 --> 0.264514).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1721952
	speed: 0.4972s/iter; left time: 10233.8972s
	iters: 200, epoch: 11 | loss: 0.1894444
	speed: 0.0756s/iter; left time: 1548.2402s
	iters: 300, epoch: 11 | loss: 0.1611897
	speed: 0.0754s/iter; left time: 1537.0940s
	iters: 400, epoch: 11 | loss: 0.2460034
	speed: 0.0810s/iter; left time: 1643.6407s
	iters: 500, epoch: 11 | loss: 0.2888461
	speed: 0.0747s/iter; left time: 1507.5038s
Epoch: 11 cost time: 40.14147925376892
Epoch: 11, Steps: 517 | Train Loss: 0.2597162 Vali Loss: 0.2644255 Test Loss: 0.3522589
Validation loss decreased (0.264514 --> 0.264426).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3053828
	speed: 0.4773s/iter; left time: 9575.7548s
	iters: 200, epoch: 12 | loss: 0.2709244
	speed: 0.0704s/iter; left time: 1405.0489s
	iters: 300, epoch: 12 | loss: 0.1962948
	speed: 0.0694s/iter; left time: 1377.6736s
	iters: 400, epoch: 12 | loss: 0.2769928
	speed: 0.0685s/iter; left time: 1354.0848s
	iters: 500, epoch: 12 | loss: 0.2865973
	speed: 0.0714s/iter; left time: 1404.1458s
Epoch: 12 cost time: 37.62238883972168
Epoch: 12, Steps: 517 | Train Loss: 0.2595772 Vali Loss: 0.2641374 Test Loss: 0.3522563
Validation loss decreased (0.264426 --> 0.264137).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2701677
	speed: 0.4313s/iter; left time: 8430.5399s
	iters: 200, epoch: 13 | loss: 0.2096116
	speed: 0.0835s/iter; left time: 1624.7269s
	iters: 300, epoch: 13 | loss: 0.2828937
	speed: 0.0771s/iter; left time: 1491.8842s
	iters: 400, epoch: 13 | loss: 0.2689081
	speed: 0.0738s/iter; left time: 1420.7669s
	iters: 500, epoch: 13 | loss: 0.1612993
	speed: 0.0819s/iter; left time: 1567.3961s
Epoch: 13 cost time: 42.813289403915405
Epoch: 13, Steps: 517 | Train Loss: 0.2595969 Vali Loss: 0.2642720 Test Loss: 0.3520724
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3238828
	speed: 0.4866s/iter; left time: 9260.3116s
	iters: 200, epoch: 14 | loss: 0.2763121
	speed: 0.0742s/iter; left time: 1403.7521s
	iters: 300, epoch: 14 | loss: 0.3704019
	speed: 0.0713s/iter; left time: 1341.7959s
	iters: 400, epoch: 14 | loss: 0.4032558
	speed: 0.0845s/iter; left time: 1583.2850s
	iters: 500, epoch: 14 | loss: 0.4235496
	speed: 0.0833s/iter; left time: 1552.6327s
Epoch: 14 cost time: 40.386714935302734
Epoch: 14, Steps: 517 | Train Loss: 0.2595605 Vali Loss: 0.2639865 Test Loss: 0.3520818
Validation loss decreased (0.264137 --> 0.263986).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2448025
	speed: 0.4718s/iter; left time: 8735.0809s
	iters: 200, epoch: 15 | loss: 0.2635579
	speed: 0.0914s/iter; left time: 1682.0711s
	iters: 300, epoch: 15 | loss: 0.2426774
	speed: 0.0829s/iter; left time: 1517.2549s
	iters: 400, epoch: 15 | loss: 0.1671325
	speed: 0.0856s/iter; left time: 1559.4594s
	iters: 500, epoch: 15 | loss: 0.3414589
	speed: 0.0846s/iter; left time: 1532.3605s
Epoch: 15 cost time: 45.23703384399414
Epoch: 15, Steps: 517 | Train Loss: 0.2595090 Vali Loss: 0.2644813 Test Loss: 0.3521298
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2796988
	speed: 0.5263s/iter; left time: 9471.8054s
	iters: 200, epoch: 16 | loss: 0.3551656
	speed: 0.0773s/iter; left time: 1384.1358s
	iters: 300, epoch: 16 | loss: 0.2682919
	speed: 0.0778s/iter; left time: 1384.6062s
	iters: 400, epoch: 16 | loss: 0.2103194
	speed: 0.0634s/iter; left time: 1121.9930s
	iters: 500, epoch: 16 | loss: 0.2691594
	speed: 0.0779s/iter; left time: 1370.1001s
Epoch: 16 cost time: 39.47676134109497
Epoch: 16, Steps: 517 | Train Loss: 0.2595062 Vali Loss: 0.2642033 Test Loss: 0.3518995
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2353766
	speed: 0.3987s/iter; left time: 6968.2303s
	iters: 200, epoch: 17 | loss: 0.2537850
	speed: 0.0684s/iter; left time: 1188.9785s
	iters: 300, epoch: 17 | loss: 0.3074130
	speed: 0.0715s/iter; left time: 1236.0934s
	iters: 400, epoch: 17 | loss: 0.1879446
	speed: 0.0558s/iter; left time: 958.7425s
	iters: 500, epoch: 17 | loss: 0.3451796
	speed: 0.0428s/iter; left time: 730.4231s
Epoch: 17 cost time: 32.02416706085205
Epoch: 17, Steps: 517 | Train Loss: 0.2594378 Vali Loss: 0.2641358 Test Loss: 0.3518240
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=74, out_features=148, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4906496.0
params:  11100.0
Trainable parameters:  11100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4073661
	speed: 0.0725s/iter; left time: 1867.7205s
	iters: 200, epoch: 1 | loss: 0.5986646
	speed: 0.0718s/iter; left time: 1840.9063s
	iters: 300, epoch: 1 | loss: 0.5204831
	speed: 0.0574s/iter; left time: 1465.4557s
	iters: 400, epoch: 1 | loss: 0.3522955
	speed: 0.0603s/iter; left time: 1535.6877s
	iters: 500, epoch: 1 | loss: 0.3491168
	speed: 0.0588s/iter; left time: 1490.7108s
Epoch: 1 cost time: 33.21226739883423
Epoch: 1, Steps: 517 | Train Loss: 0.4990897 Vali Loss: 0.2630427 Test Loss: 0.3513753
Validation loss decreased (inf --> 0.263043).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4681964
	speed: 0.3938s/iter; left time: 9937.2183s
	iters: 200, epoch: 2 | loss: 0.5785249
	speed: 0.0619s/iter; left time: 1555.5027s
	iters: 300, epoch: 2 | loss: 0.5966238
	speed: 0.0547s/iter; left time: 1368.8037s
	iters: 400, epoch: 2 | loss: 0.4494525
	speed: 0.0577s/iter; left time: 1438.9148s
	iters: 500, epoch: 2 | loss: 0.4023929
	speed: 0.0585s/iter; left time: 1451.6597s
Epoch: 2 cost time: 31.157500743865967
Epoch: 2, Steps: 517 | Train Loss: 0.4976224 Vali Loss: 0.2620807 Test Loss: 0.3512492
Validation loss decreased (0.263043 --> 0.262081).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5334567
	speed: 0.4530s/iter; left time: 11196.1948s
	iters: 200, epoch: 3 | loss: 0.4367268
	speed: 0.0601s/iter; left time: 1478.7521s
	iters: 300, epoch: 3 | loss: 0.3023382
	speed: 0.0568s/iter; left time: 1392.7032s
	iters: 400, epoch: 3 | loss: 0.4122500
	speed: 0.0698s/iter; left time: 1704.4141s
	iters: 500, epoch: 3 | loss: 0.5982593
	speed: 0.0604s/iter; left time: 1468.6222s
Epoch: 3 cost time: 33.646193742752075
Epoch: 3, Steps: 517 | Train Loss: 0.4971639 Vali Loss: 0.2619580 Test Loss: 0.3507420
Validation loss decreased (0.262081 --> 0.261958).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6416848
	speed: 0.4269s/iter; left time: 10331.8118s
	iters: 200, epoch: 4 | loss: 0.4770898
	speed: 0.0661s/iter; left time: 1592.4596s
	iters: 300, epoch: 4 | loss: 0.4893349
	speed: 0.0642s/iter; left time: 1540.1525s
	iters: 400, epoch: 4 | loss: 0.4732291
	speed: 0.0651s/iter; left time: 1555.3625s
	iters: 500, epoch: 4 | loss: 0.7119179
	speed: 0.0673s/iter; left time: 1601.2910s
Epoch: 4 cost time: 34.74769616127014
Epoch: 4, Steps: 517 | Train Loss: 0.4963934 Vali Loss: 0.2619269 Test Loss: 0.3502706
Validation loss decreased (0.261958 --> 0.261927).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5987237
	speed: 0.4184s/iter; left time: 9909.9585s
	iters: 200, epoch: 5 | loss: 0.4695608
	speed: 0.0666s/iter; left time: 1571.2610s
	iters: 300, epoch: 5 | loss: 0.6068926
	speed: 0.0612s/iter; left time: 1438.2369s
	iters: 400, epoch: 5 | loss: 0.3891229
	speed: 0.0607s/iter; left time: 1420.4549s
	iters: 500, epoch: 5 | loss: 0.3387270
	speed: 0.0671s/iter; left time: 1561.3997s
Epoch: 5 cost time: 33.81035780906677
Epoch: 5, Steps: 517 | Train Loss: 0.4964532 Vali Loss: 0.2623068 Test Loss: 0.3501148
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4202583
	speed: 0.4342s/iter; left time: 10058.4789s
	iters: 200, epoch: 6 | loss: 0.4712767
	speed: 0.0697s/iter; left time: 1607.9102s
	iters: 300, epoch: 6 | loss: 0.7489645
	speed: 0.0753s/iter; left time: 1730.1904s
	iters: 400, epoch: 6 | loss: 0.5578402
	speed: 0.0664s/iter; left time: 1517.9868s
	iters: 500, epoch: 6 | loss: 0.5427904
	speed: 0.0669s/iter; left time: 1523.1456s
Epoch: 6 cost time: 37.10645246505737
Epoch: 6, Steps: 517 | Train Loss: 0.4960669 Vali Loss: 0.2616629 Test Loss: 0.3503432
Validation loss decreased (0.261927 --> 0.261663).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5534486
	speed: 0.4689s/iter; left time: 10619.2171s
	iters: 200, epoch: 7 | loss: 0.4474834
	speed: 0.0664s/iter; left time: 1496.4501s
	iters: 300, epoch: 7 | loss: 0.5404707
	speed: 0.0594s/iter; left time: 1334.0070s
	iters: 400, epoch: 7 | loss: 0.4514650
	speed: 0.0702s/iter; left time: 1569.4694s
	iters: 500, epoch: 7 | loss: 0.4401667
	speed: 0.0592s/iter; left time: 1317.8554s
Epoch: 7 cost time: 33.838377714157104
Epoch: 7, Steps: 517 | Train Loss: 0.4961942 Vali Loss: 0.2615127 Test Loss: 0.3503509
Validation loss decreased (0.261663 --> 0.261513).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5655602
	speed: 0.1724s/iter; left time: 3815.3297s
	iters: 200, epoch: 8 | loss: 0.5534728
	speed: 0.0635s/iter; left time: 1398.4830s
	iters: 300, epoch: 8 | loss: 0.7422683
	speed: 0.0667s/iter; left time: 1463.6044s
	iters: 400, epoch: 8 | loss: 0.3454244
	speed: 0.0663s/iter; left time: 1447.1669s
	iters: 500, epoch: 8 | loss: 0.3708711
	speed: 0.0615s/iter; left time: 1337.2210s
Epoch: 8 cost time: 34.24949502944946
Epoch: 8, Steps: 517 | Train Loss: 0.4958587 Vali Loss: 0.2616432 Test Loss: 0.3501959
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6764752
	speed: 0.4084s/iter; left time: 8826.5268s
	iters: 200, epoch: 9 | loss: 0.4823153
	speed: 0.0624s/iter; left time: 1342.2099s
	iters: 300, epoch: 9 | loss: 0.5025931
	speed: 0.0709s/iter; left time: 1517.7842s
	iters: 400, epoch: 9 | loss: 0.5014774
	speed: 0.0613s/iter; left time: 1306.7702s
	iters: 500, epoch: 9 | loss: 0.6241890
	speed: 0.0642s/iter; left time: 1362.7691s
Epoch: 9 cost time: 33.85499334335327
Epoch: 9, Steps: 517 | Train Loss: 0.4958062 Vali Loss: 0.2612565 Test Loss: 0.3502032
Validation loss decreased (0.261513 --> 0.261256).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4635983
	speed: 0.4183s/iter; left time: 8826.1018s
	iters: 200, epoch: 10 | loss: 0.4275809
	speed: 0.0655s/iter; left time: 1375.2988s
	iters: 300, epoch: 10 | loss: 0.5455523
	speed: 0.0640s/iter; left time: 1338.3519s
	iters: 400, epoch: 10 | loss: 0.5235560
	speed: 0.0655s/iter; left time: 1361.6978s
	iters: 500, epoch: 10 | loss: 0.5256569
	speed: 0.0657s/iter; left time: 1360.3339s
Epoch: 10 cost time: 33.900442361831665
Epoch: 10, Steps: 517 | Train Loss: 0.4957324 Vali Loss: 0.2612252 Test Loss: 0.3501878
Validation loss decreased (0.261256 --> 0.261225).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5675263
	speed: 0.4365s/iter; left time: 8983.7337s
	iters: 200, epoch: 11 | loss: 0.6175500
	speed: 0.0672s/iter; left time: 1377.2422s
	iters: 300, epoch: 11 | loss: 0.4308115
	speed: 0.0672s/iter; left time: 1368.6504s
	iters: 400, epoch: 11 | loss: 0.5153471
	speed: 0.0663s/iter; left time: 1345.1292s
	iters: 500, epoch: 11 | loss: 0.3639364
	speed: 0.0624s/iter; left time: 1259.8241s
Epoch: 11 cost time: 34.8795006275177
Epoch: 11, Steps: 517 | Train Loss: 0.4956315 Vali Loss: 0.2613554 Test Loss: 0.3503008
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4953098
	speed: 0.2388s/iter; left time: 4790.5360s
	iters: 200, epoch: 12 | loss: 0.5309477
	speed: 0.0315s/iter; left time: 628.9974s
	iters: 300, epoch: 12 | loss: 0.6239380
	speed: 0.0577s/iter; left time: 1146.6891s
	iters: 400, epoch: 12 | loss: 0.5179099
	speed: 0.0516s/iter; left time: 1020.4502s
	iters: 500, epoch: 12 | loss: 0.5008997
	speed: 0.0542s/iter; left time: 1065.9383s
Epoch: 12 cost time: 23.496522426605225
Epoch: 12, Steps: 517 | Train Loss: 0.4955384 Vali Loss: 0.2615059 Test Loss: 0.3500519
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3216360
	speed: 0.4117s/iter; left time: 8047.0697s
	iters: 200, epoch: 13 | loss: 0.5304064
	speed: 0.0649s/iter; left time: 1262.6454s
	iters: 300, epoch: 13 | loss: 0.6028255
	speed: 0.0623s/iter; left time: 1206.2127s
	iters: 400, epoch: 13 | loss: 0.5909593
	speed: 0.0632s/iter; left time: 1216.8285s
	iters: 500, epoch: 13 | loss: 0.4486865
	speed: 0.0658s/iter; left time: 1259.3411s
Epoch: 13 cost time: 34.23310995101929
Epoch: 13, Steps: 517 | Train Loss: 0.4956562 Vali Loss: 0.2612476 Test Loss: 0.3503167
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3495502769947052, mae:0.3787270188331604, rse:0.47522494196891785, corr:[0.5399468  0.54279065 0.54274076 0.5408737  0.53883564 0.5375077
 0.5370449  0.53721976 0.537657   0.5379803  0.53795284 0.5375609
 0.5370005  0.53646713 0.53611857 0.5358781  0.5356628  0.535364
 0.53486073 0.5341305  0.5332941  0.53245693 0.53175807 0.5312887
 0.53099513 0.5307936  0.53055745 0.53020054 0.52967966 0.5290334
 0.5283277  0.5277077  0.52720255 0.5267333  0.52625966 0.525759
 0.5251797  0.5245539  0.5238813  0.5232144  0.5225948  0.5220205
 0.5215186  0.5210616  0.52061844 0.5200997  0.5194846  0.5187886
 0.5179815  0.5170595  0.5161044  0.5152431  0.5145161  0.5139416
 0.5135066  0.51316774 0.5128473  0.51248807 0.5120923  0.51166433
 0.51125884 0.51093173 0.510703   0.51053804 0.51039153 0.5102698
 0.5100606  0.5097975  0.5094799  0.5091021  0.508668   0.50819916
 0.5077352  0.507279   0.5067919  0.5062644  0.5056992  0.50508726
 0.50447065 0.50385857 0.50320286 0.5025407  0.5019068  0.50132227
 0.5008111  0.5003778  0.49996594 0.4995968  0.49922618 0.49883822
 0.49843863 0.4979962  0.49741617 0.49663043 0.49558097 0.49426195
 0.49275106 0.49121577 0.48969644 0.48822296 0.48683023 0.4855199
 0.48427624 0.48305878 0.48181826 0.48055542 0.47931966 0.47820917
 0.47723788 0.4764319  0.47578466 0.4752408  0.47479036 0.4743134
 0.4736947  0.47290355 0.4719711  0.47091144 0.46982458 0.46874768
 0.46780822 0.46701896 0.46640223 0.46586773 0.46529597 0.46458706
 0.46374688 0.46280283 0.46183035 0.46090978 0.46014228 0.45954412
 0.4591146  0.45877406 0.45843956 0.45799804 0.45736074 0.45655945
 0.45565483 0.45476592 0.4539877  0.45339015 0.45297876 0.45259967
 0.45211923 0.45138407 0.45045736 0.4493673  0.44822386 0.4471084
 0.44616973 0.44551066 0.44505382 0.4446937  0.44433963 0.44394168
 0.44347355 0.44291797 0.4422448  0.44155863 0.44087198 0.44026327
 0.43976206 0.4394131  0.43919456 0.43905148 0.4388696  0.43857357
 0.43814582 0.4376229  0.43697488 0.43628418 0.43559498 0.43497947
 0.4344566  0.43400967 0.43357474 0.43306613 0.43240634 0.43165973
 0.4308446  0.43002817 0.4293304  0.42879176 0.42838928 0.4280611
 0.42775318 0.42734337 0.42675263 0.4258829  0.42470104 0.42320406
 0.42149645 0.41984442 0.4182343  0.41666207 0.41519958 0.41385284
 0.41268134 0.41167453 0.4107929  0.40995848 0.40908507 0.4081187
 0.40703624 0.40582907 0.4045009  0.40308803 0.40175274 0.40067303
 0.39978722 0.399017   0.39829582 0.39757007 0.3968385  0.3960088
 0.395052   0.3939318  0.392767   0.3916196  0.39051577 0.3894755
 0.38848665 0.38753295 0.38662985 0.38562873 0.38448545 0.38327545
 0.3820373  0.3809328  0.38006046 0.37947327 0.37910578 0.3788621
 0.37858152 0.37819386 0.3776292  0.37686706 0.37597513 0.37500393
 0.37404987 0.37314326 0.37235484 0.3718738  0.3716348  0.37164763
 0.371839   0.3721197  0.37235028 0.37241453 0.37235412 0.37218454
 0.37193748 0.37170723 0.37145996 0.37122148 0.3709568  0.37070343
 0.3704528  0.3702048  0.36993    0.36965582 0.36936358 0.3691273
 0.36894503 0.3687516  0.3685461  0.3683108  0.3680016  0.36757678
 0.3671057  0.3666114  0.36606938 0.3655186  0.3649408  0.36433032
 0.36377284 0.36324722 0.3627682  0.36246216 0.36232197 0.3623582
 0.36246738 0.36255345 0.36249995 0.36219287 0.36159235 0.36062998
 0.3594444  0.35829747 0.35727924 0.35639274 0.3556291  0.3548949
 0.35422203 0.35355744 0.35284904 0.35210583 0.3514153  0.35078204
 0.35030472 0.3499232  0.34967864 0.349433   0.3491946  0.3489292
 0.3486779  0.34834784 0.34793276 0.3474526  0.34693137 0.3464933
 0.34604704 0.34561336 0.34510761 0.34447673 0.34377253 0.34309718
 0.34250423 0.34212676 0.3420585  0.3422721  0.34265673 0.3430993
 0.34345332 0.34365627 0.34365657 0.34336022 0.3428589  0.34221607
 0.3416364  0.34117487 0.34089345 0.34081873 0.3408254  0.34088415
 0.3408746  0.34063837 0.34025586 0.3399252  0.33978304 0.33986503
 0.34020123 0.3407135  0.34125873 0.3416643  0.34181342 0.34171057
 0.34138176 0.34095377 0.34054083 0.34022248 0.34005928 0.34001315
 0.34002262 0.34004605 0.33995563 0.33980435 0.33949214 0.339109
 0.33873302 0.33844185 0.3382844  0.33826637 0.33831602 0.33841515
 0.33841938 0.33830178 0.33800521 0.33756867 0.33709267 0.3366328
 0.33628124 0.3361439  0.3362389  0.3365166  0.3368933  0.33734554
 0.33784798 0.33831584 0.33860472 0.33854097 0.33816117 0.33739623
 0.3363844  0.33525905 0.33415955 0.33314493 0.33221844 0.33149353
 0.3309312  0.3305061  0.33016095 0.32984    0.32949427 0.3290369
 0.3284585  0.32784107 0.32717302 0.32653216 0.3260847  0.32581016
 0.32568604 0.3256205  0.32556304 0.325474   0.32520312 0.32474428
 0.32408908 0.32338145 0.32273346 0.3222047  0.32189018 0.32176733
 0.32186228 0.32213855 0.32245785 0.32272792 0.32285511 0.32286006
 0.32284158 0.3228643  0.32290867 0.32296115 0.32298025 0.3229193
 0.32277632 0.32252735 0.32224947 0.32200465 0.32187864 0.32191554
 0.32209373 0.32227656 0.32241106 0.32246554 0.32245353 0.32232544
 0.3219945  0.32156944 0.32102734 0.32033882 0.31956378 0.31883648
 0.31815138 0.31761366 0.31723967 0.31699264 0.31688267 0.31683868
 0.31684735 0.3168552  0.3168457  0.31671658 0.31647557 0.3161822
 0.3158684  0.3155817  0.31532925 0.3151102  0.31498915 0.31485426
 0.31469032 0.31446812 0.31417146 0.3137979  0.31334332 0.31285244
 0.31238756 0.3119731  0.31159252 0.31120747 0.3108547  0.31047887
 0.31006527 0.30951712 0.3088692  0.30805868 0.3071153  0.30603397
 0.3048232  0.30359945 0.3023437  0.3010955  0.29992214 0.29886445
 0.29791778 0.29714045 0.29649392 0.29600438 0.29555798 0.29502746
 0.29432446 0.29350606 0.29261866 0.29167745 0.29080382 0.29008865
 0.2895939  0.28922257 0.2888915  0.28861222 0.28831014 0.2880399
 0.28769138 0.28721133 0.2866211  0.2859296  0.28524518 0.28458846
 0.28399476 0.28347042 0.28298342 0.2825252  0.28212145 0.28176925
 0.2814288  0.28104115 0.2807442  0.2805061  0.28029707 0.2801222
 0.2799587  0.27983448 0.2797024  0.27955428 0.27940905 0.2792185
 0.2789161  0.27852398 0.27805695 0.27759108 0.2772357  0.27703202
 0.2769375  0.2769072  0.27697352 0.2770445  0.277043   0.27692577
 0.2766734  0.27633563 0.27592996 0.2755596  0.27525577 0.27501494
 0.27484217 0.27476507 0.274738   0.27468598 0.27448907 0.27415848
 0.27368176 0.27317598 0.27277714 0.27260882 0.27258012 0.27271712
 0.27292907 0.27315804 0.2732482  0.27309787 0.27271846 0.27220145
 0.271686   0.27129117 0.2711158  0.2711495  0.27130955 0.2714714
 0.2714736  0.27116808 0.27040848 0.26921603 0.26771906 0.26602072
 0.264312   0.2629333  0.2619693  0.26135805 0.26087862 0.2603526
 0.25968033 0.25885984 0.25797564 0.25711763 0.25634703 0.25572142
 0.25518614 0.2546219  0.25401312 0.25329828 0.2524739  0.2516336
 0.25084677 0.25014678 0.2496486  0.24932437 0.24920751 0.24925645
 0.24936199 0.2494623  0.24940678 0.24910763 0.24863689 0.24806227
 0.2473937  0.24672072 0.24617538 0.24572682 0.24544063 0.24531716
 0.24541871 0.24557267 0.24577655 0.24607025 0.24642    0.2467633
 0.24707131 0.24738926 0.2476869  0.2479803  0.24829328 0.24863198
 0.2489117  0.24904405 0.24900949 0.2489307  0.2488873  0.24892461
 0.24898553 0.24904872 0.24911864 0.24898836 0.24880505 0.24856
 0.24830836 0.24813478 0.24800336 0.24803789 0.24807681 0.24812357
 0.24811196 0.248156   0.24817719 0.24804057 0.24785505 0.24745703
 0.24692816 0.24628668 0.24564457 0.24511494 0.24488825 0.24487874
 0.24516916 0.24559982 0.24595079 0.24620937 0.24633682 0.2462704
 0.24602437 0.24582279 0.2456933  0.24571024 0.24595718 0.24633266
 0.24666643 0.24687271 0.24681316 0.24630992 0.2453445  0.2440623
 0.24266455 0.24135375 0.24024278 0.23932569 0.23861916 0.2380086
 0.23739499 0.23672332 0.23620647 0.23579568 0.23553221 0.23534681
 0.23518398 0.23499319 0.23471631 0.23428376 0.23375699 0.23304158
 0.23217991 0.2312121  0.23018187 0.22936098 0.22877046 0.22838426
 0.2281514  0.22790714 0.22759542 0.22703832 0.22625409 0.22528116
 0.22433847 0.2234248  0.22270791 0.22231415 0.22224939 0.222423
 0.22256818 0.22251354 0.22207244 0.22132455 0.22024097 0.2191795
 0.21852702 0.2186297  0.21951406 0.22067094 0.2209103  0.21853694]
