Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10644480.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3905188
	speed: 0.1283s/iter; left time: 1661.9082s
	iters: 200, epoch: 1 | loss: 0.4317648
	speed: 0.1141s/iter; left time: 1466.1775s
Epoch: 1 cost time: 31.744680404663086
Epoch: 1, Steps: 261 | Train Loss: 0.4677133 Vali Loss: 0.2116005 Test Loss: 0.2889774
Validation loss decreased (inf --> 0.211601).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3628764
	speed: 0.5228s/iter; left time: 6633.7504s
	iters: 200, epoch: 2 | loss: 0.4210671
	speed: 0.1240s/iter; left time: 1561.3007s
Epoch: 2 cost time: 32.721070528030396
Epoch: 2, Steps: 261 | Train Loss: 0.4044724 Vali Loss: 0.2030035 Test Loss: 0.2793831
Validation loss decreased (0.211601 --> 0.203003).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3000817
	speed: 0.5257s/iter; left time: 6533.9387s
	iters: 200, epoch: 3 | loss: 0.4903250
	speed: 0.1033s/iter; left time: 1273.0060s
Epoch: 3 cost time: 28.52717423439026
Epoch: 3, Steps: 261 | Train Loss: 0.3933555 Vali Loss: 0.1992747 Test Loss: 0.2751046
Validation loss decreased (0.203003 --> 0.199275).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3769745
	speed: 0.4680s/iter; left time: 5694.6171s
	iters: 200, epoch: 4 | loss: 0.4321515
	speed: 0.1150s/iter; left time: 1387.9187s
Epoch: 4 cost time: 28.728966236114502
Epoch: 4, Steps: 261 | Train Loss: 0.3880823 Vali Loss: 0.1973626 Test Loss: 0.2728716
Validation loss decreased (0.199275 --> 0.197363).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5290220
	speed: 0.4878s/iter; left time: 5808.4342s
	iters: 200, epoch: 5 | loss: 0.3642936
	speed: 0.0986s/iter; left time: 1164.6301s
Epoch: 5 cost time: 28.149256229400635
Epoch: 5, Steps: 261 | Train Loss: 0.3850144 Vali Loss: 0.1964049 Test Loss: 0.2717744
Validation loss decreased (0.197363 --> 0.196405).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3518465
	speed: 0.4621s/iter; left time: 5381.6702s
	iters: 200, epoch: 6 | loss: 0.4701680
	speed: 0.1012s/iter; left time: 1168.2451s
Epoch: 6 cost time: 28.418813228607178
Epoch: 6, Steps: 261 | Train Loss: 0.3827564 Vali Loss: 0.1958746 Test Loss: 0.2709225
Validation loss decreased (0.196405 --> 0.195875).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3774920
	speed: 0.5197s/iter; left time: 5916.2900s
	iters: 200, epoch: 7 | loss: 0.4167481
	speed: 0.1045s/iter; left time: 1179.3980s
Epoch: 7 cost time: 30.581425666809082
Epoch: 7, Steps: 261 | Train Loss: 0.3811087 Vali Loss: 0.1951527 Test Loss: 0.2700362
Validation loss decreased (0.195875 --> 0.195153).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4226385
	speed: 0.4891s/iter; left time: 5441.2143s
	iters: 200, epoch: 8 | loss: 0.4759077
	speed: 0.1152s/iter; left time: 1269.8146s
Epoch: 8 cost time: 29.543209552764893
Epoch: 8, Steps: 261 | Train Loss: 0.3800183 Vali Loss: 0.1946341 Test Loss: 0.2697934
Validation loss decreased (0.195153 --> 0.194634).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3693011
	speed: 0.5102s/iter; left time: 5542.2125s
	iters: 200, epoch: 9 | loss: 0.4317890
	speed: 0.0818s/iter; left time: 880.2933s
Epoch: 9 cost time: 22.54309630393982
Epoch: 9, Steps: 261 | Train Loss: 0.3788137 Vali Loss: 0.1946117 Test Loss: 0.2693233
Validation loss decreased (0.194634 --> 0.194612).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3602844
	speed: 0.3112s/iter; left time: 3299.3380s
	iters: 200, epoch: 10 | loss: 0.2799991
	speed: 0.1051s/iter; left time: 1103.8702s
Epoch: 10 cost time: 24.650518655776978
Epoch: 10, Steps: 261 | Train Loss: 0.3785788 Vali Loss: 0.1944502 Test Loss: 0.2692540
Validation loss decreased (0.194612 --> 0.194450).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4437194
	speed: 0.5102s/iter; left time: 5276.3882s
	iters: 200, epoch: 11 | loss: 0.4663024
	speed: 0.1110s/iter; left time: 1136.2786s
Epoch: 11 cost time: 29.42066240310669
Epoch: 11, Steps: 261 | Train Loss: 0.3784391 Vali Loss: 0.1940797 Test Loss: 0.2688363
Validation loss decreased (0.194450 --> 0.194080).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3964263
	speed: 0.5324s/iter; left time: 5366.9981s
	iters: 200, epoch: 12 | loss: 0.3381942
	speed: 0.1206s/iter; left time: 1203.6817s
Epoch: 12 cost time: 31.390938997268677
Epoch: 12, Steps: 261 | Train Loss: 0.3780473 Vali Loss: 0.1939309 Test Loss: 0.2689555
Validation loss decreased (0.194080 --> 0.193931).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2399769
	speed: 0.5137s/iter; left time: 5043.7282s
	iters: 200, epoch: 13 | loss: 0.3698869
	speed: 0.1099s/iter; left time: 1068.1459s
Epoch: 13 cost time: 30.16030240058899
Epoch: 13, Steps: 261 | Train Loss: 0.3775708 Vali Loss: 0.1939349 Test Loss: 0.2688232
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4368623
	speed: 0.4634s/iter; left time: 4429.2715s
	iters: 200, epoch: 14 | loss: 0.5555330
	speed: 0.1116s/iter; left time: 1055.6473s
Epoch: 14 cost time: 29.432026386260986
Epoch: 14, Steps: 261 | Train Loss: 0.3773255 Vali Loss: 0.1939186 Test Loss: 0.2686442
Validation loss decreased (0.193931 --> 0.193919).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3355992
	speed: 0.4928s/iter; left time: 4581.6459s
	iters: 200, epoch: 15 | loss: 0.3900049
	speed: 0.1043s/iter; left time: 959.0278s
Epoch: 15 cost time: 29.53022003173828
Epoch: 15, Steps: 261 | Train Loss: 0.3771131 Vali Loss: 0.1936977 Test Loss: 0.2684948
Validation loss decreased (0.193919 --> 0.193698).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3668847
	speed: 0.5233s/iter; left time: 4728.2598s
	iters: 200, epoch: 16 | loss: 0.3033461
	speed: 0.1074s/iter; left time: 960.0267s
Epoch: 16 cost time: 29.754425287246704
Epoch: 16, Steps: 261 | Train Loss: 0.3765422 Vali Loss: 0.1936406 Test Loss: 0.2685215
Validation loss decreased (0.193698 --> 0.193641).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3956203
	speed: 0.4793s/iter; left time: 4206.2389s
	iters: 200, epoch: 17 | loss: 0.2512920
	speed: 0.1117s/iter; left time: 969.1455s
Epoch: 17 cost time: 29.459602117538452
Epoch: 17, Steps: 261 | Train Loss: 0.3763557 Vali Loss: 0.1937061 Test Loss: 0.2682215
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3112090
	speed: 0.4975s/iter; left time: 4235.3759s
	iters: 200, epoch: 18 | loss: 0.2766194
	speed: 0.1098s/iter; left time: 924.1964s
Epoch: 18 cost time: 29.361300706863403
Epoch: 18, Steps: 261 | Train Loss: 0.3764286 Vali Loss: 0.1935727 Test Loss: 0.2683292
Validation loss decreased (0.193641 --> 0.193573).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5033927
	speed: 0.4749s/iter; left time: 3919.3069s
	iters: 200, epoch: 19 | loss: 0.4786796
	speed: 0.1033s/iter; left time: 842.4302s
Epoch: 19 cost time: 28.010459184646606
Epoch: 19, Steps: 261 | Train Loss: 0.3762018 Vali Loss: 0.1934667 Test Loss: 0.2683792
Validation loss decreased (0.193573 --> 0.193467).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4257652
	speed: 0.4906s/iter; left time: 3920.6914s
	iters: 200, epoch: 20 | loss: 0.3448886
	speed: 0.1137s/iter; left time: 897.1903s
Epoch: 20 cost time: 29.917803049087524
Epoch: 20, Steps: 261 | Train Loss: 0.3759334 Vali Loss: 0.1933141 Test Loss: 0.2680109
Validation loss decreased (0.193467 --> 0.193314).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2861972
	speed: 0.4700s/iter; left time: 3633.8991s
	iters: 200, epoch: 21 | loss: 0.5244645
	speed: 0.1136s/iter; left time: 867.1517s
Epoch: 21 cost time: 29.20341730117798
Epoch: 21, Steps: 261 | Train Loss: 0.3756727 Vali Loss: 0.1934821 Test Loss: 0.2680975
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4271061
	speed: 0.5003s/iter; left time: 3737.5146s
	iters: 200, epoch: 22 | loss: 0.3486760
	speed: 0.1129s/iter; left time: 832.1572s
Epoch: 22 cost time: 30.267372846603394
Epoch: 22, Steps: 261 | Train Loss: 0.3756151 Vali Loss: 0.1935551 Test Loss: 0.2681239
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3859768
	speed: 0.5009s/iter; left time: 3610.8666s
	iters: 200, epoch: 23 | loss: 0.4439905
	speed: 0.1219s/iter; left time: 866.3087s
Epoch: 23 cost time: 33.07641959190369
Epoch: 23, Steps: 261 | Train Loss: 0.3757488 Vali Loss: 0.1933134 Test Loss: 0.2680454
Validation loss decreased (0.193314 --> 0.193313).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3817163
	speed: 0.5812s/iter; left time: 4038.4948s
	iters: 200, epoch: 24 | loss: 0.4372077
	speed: 0.1329s/iter; left time: 910.2851s
Epoch: 24 cost time: 35.311694860458374
Epoch: 24, Steps: 261 | Train Loss: 0.3756285 Vali Loss: 0.1932689 Test Loss: 0.2679906
Validation loss decreased (0.193313 --> 0.193269).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5093567
	speed: 0.5514s/iter; left time: 3687.3863s
	iters: 200, epoch: 25 | loss: 0.3865750
	speed: 0.1008s/iter; left time: 663.8587s
Epoch: 25 cost time: 28.525082111358643
Epoch: 25, Steps: 261 | Train Loss: 0.3755471 Vali Loss: 0.1933499 Test Loss: 0.2679580
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3958955
	speed: 0.4704s/iter; left time: 3022.9310s
	iters: 200, epoch: 26 | loss: 0.3256424
	speed: 0.1032s/iter; left time: 652.5408s
Epoch: 26 cost time: 27.586891412734985
Epoch: 26, Steps: 261 | Train Loss: 0.3754371 Vali Loss: 0.1933245 Test Loss: 0.2679369
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.6398550
	speed: 0.4937s/iter; left time: 3043.3890s
	iters: 200, epoch: 27 | loss: 0.5216437
	speed: 0.1121s/iter; left time: 679.7324s
Epoch: 27 cost time: 30.5536847114563
Epoch: 27, Steps: 261 | Train Loss: 0.3753686 Vali Loss: 0.1933734 Test Loss: 0.2680352
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.26900535821914673, mae:0.3264499008655548, rse:0.41892966628074646, corr:[0.54984    0.55622494 0.5550283  0.5522947  0.5510103  0.5515044
 0.5528538  0.55373883 0.55355126 0.55268526 0.55188894 0.55164754
 0.5520307  0.55265474 0.5530187  0.5527077  0.5518177  0.550743
 0.54983556 0.5492792  0.5490619  0.5489517  0.54871327 0.54825413
 0.5476098  0.54694754 0.5464198  0.5460731  0.54583764 0.5455776
 0.54515725 0.54460996 0.54397637 0.5432943  0.5426384  0.54208416
 0.5415873  0.5410972  0.54054344 0.5399133  0.53924996 0.5386019
 0.5380378  0.5375677  0.5371508  0.536694   0.5361343  0.53545624
 0.5346336  0.5337188  0.53282696 0.5320885  0.5314957  0.530985
 0.5304771  0.5299503  0.52938575 0.5288281  0.52838045 0.52810556
 0.52797645 0.52793366 0.5278576  0.5276444  0.5272858  0.5268745
 0.52648836 0.5262361  0.5261315  0.52608716 0.5259667  0.5256981
 0.52526736 0.52474093 0.52417994 0.5236732  0.5232205  0.522769
 0.5222398  0.5216409  0.5209357  0.520217   0.51956326 0.51903254
 0.51863176 0.5182924  0.5178944  0.51740384 0.5168363  0.51625025
 0.51572406 0.51526135 0.5147596  0.5141083  0.5132027  0.5119796
 0.5105187  0.5090171  0.50758237 0.50630623 0.505223   0.5042786
 0.50338465 0.50243104 0.5014052  0.5003178  0.49928156 0.49838752
 0.49761984 0.49690008 0.49614987 0.49531153 0.49443853 0.4935468
 0.4926615  0.49185932 0.49116367 0.49048218 0.4897604  0.48889
 0.48792028 0.48688275 0.4859167  0.48507446 0.48432547 0.48359975
 0.48284137 0.4819808  0.48098868 0.4799236  0.4789001  0.4780121
 0.47732943 0.47682634 0.47641906 0.47598833 0.47544903 0.47482702
 0.47414917 0.4734803  0.47280005 0.4720807  0.4712909  0.47035372
 0.46931723 0.46824408 0.4673791  0.46679133 0.4664411  0.46613628
 0.4657389  0.46518457 0.46443245 0.46359405 0.46286753 0.46242195
 0.4622103  0.46206182 0.4617518  0.46119934 0.460364   0.4593924
 0.45851344 0.4579961  0.45793295 0.45820516 0.45849445 0.45854926
 0.45829394 0.45780364 0.45713583 0.45644858 0.45585606 0.45536852
 0.45490226 0.45433813 0.4535821  0.4526542  0.45161882 0.45072234
 0.4500594  0.44963542 0.4494408  0.44934458 0.44918272 0.44884607
 0.4483379  0.44771332 0.4469887  0.44614288 0.44515285 0.44400698
 0.44277933 0.44162798 0.44048133 0.4392816  0.4380918  0.43690887
 0.43578315 0.4347089  0.43369946 0.43273008 0.4317849  0.4308349
 0.42985472 0.4288014  0.42762905 0.42638567 0.4252489  0.42438316
 0.4236855  0.423049   0.4223709  0.42156798 0.42065364 0.4195844
 0.41840267 0.41717577 0.4160755  0.4151362  0.41423893 0.4132909
 0.41220298 0.41097713 0.4097228  0.40844148 0.4072717  0.40637022
 0.4056495  0.40507698 0.4044705  0.4037768  0.40296733 0.402158
 0.40146488 0.40105236 0.4008878  0.40085793 0.40078348 0.40045828
 0.39983112 0.39890137 0.3979224  0.39727715 0.3970055  0.3971014
 0.397408   0.39771488 0.3977939  0.39755398 0.3972012  0.39687887
 0.39667183 0.39664242 0.39659277 0.396434   0.39609078 0.39566478
 0.3952905  0.3950644  0.39501184 0.39508477 0.39514545 0.39513397
 0.39497986 0.3946291  0.3942078  0.39379555 0.39337027 0.39288318
 0.39233622 0.39184257 0.39142406 0.391211   0.39122576 0.39141694
 0.3917431  0.391898   0.39167634 0.39110568 0.39026806 0.38942486
 0.38877848 0.38847232 0.3884579  0.3885037  0.3883441  0.38768268
 0.38656405 0.3853622  0.38425377 0.38332918 0.38262886 0.38204917
 0.38163364 0.3812978  0.3809655  0.38062674 0.3803921  0.38007686
 0.37972513 0.37920135 0.37856945 0.3777708  0.37702677 0.37656042
 0.37649867 0.37655368 0.37651965 0.37623942 0.3756861  0.37525138
 0.37506327 0.3753852  0.3760675  0.3766861  0.37688068 0.37650725
 0.37559754 0.37463093 0.37418002 0.37452415 0.3754007  0.37636736
 0.37690774 0.3768532  0.37647098 0.37619144 0.37664977 0.37770087
 0.3788117  0.3789324  0.37720293 0.37372795 0.37098858 0.37183905]
