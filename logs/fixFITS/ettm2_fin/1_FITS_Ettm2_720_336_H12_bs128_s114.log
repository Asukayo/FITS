Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  29442560.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3920956
	speed: 0.1511s/iter; left time: 967.0774s
Epoch: 1 cost time: 20.054698944091797
Epoch: 1, Steps: 130 | Train Loss: 0.4909856 Vali Loss: 0.2206273 Test Loss: 0.3007173
Validation loss decreased (inf --> 0.220627).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5118687
	speed: 0.4775s/iter; left time: 2994.4490s
Epoch: 2 cost time: 24.891212701797485
Epoch: 2, Steps: 130 | Train Loss: 0.4165765 Vali Loss: 0.2079664 Test Loss: 0.2860553
Validation loss decreased (0.220627 --> 0.207966).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3135810
	speed: 0.5789s/iter; left time: 3554.8833s
Epoch: 3 cost time: 28.47530460357666
Epoch: 3, Steps: 130 | Train Loss: 0.4025693 Vali Loss: 0.2031591 Test Loss: 0.2802323
Validation loss decreased (0.207966 --> 0.203159).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4436206
	speed: 0.4776s/iter; left time: 2870.9673s
Epoch: 4 cost time: 19.53552794456482
Epoch: 4, Steps: 130 | Train Loss: 0.3960495 Vali Loss: 0.2004791 Test Loss: 0.2773950
Validation loss decreased (0.203159 --> 0.200479).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3873392
	speed: 0.4012s/iter; left time: 2359.5497s
Epoch: 5 cost time: 19.86370611190796
Epoch: 5, Steps: 130 | Train Loss: 0.3916843 Vali Loss: 0.1992125 Test Loss: 0.2753987
Validation loss decreased (0.200479 --> 0.199213).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3262461
	speed: 0.4447s/iter; left time: 2557.4099s
Epoch: 6 cost time: 21.476463794708252
Epoch: 6, Steps: 130 | Train Loss: 0.3883171 Vali Loss: 0.1974181 Test Loss: 0.2740673
Validation loss decreased (0.199213 --> 0.197418).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3796864
	speed: 0.4004s/iter; left time: 2250.7193s
Epoch: 7 cost time: 19.42811894416809
Epoch: 7, Steps: 130 | Train Loss: 0.3859709 Vali Loss: 0.1968184 Test Loss: 0.2729521
Validation loss decreased (0.197418 --> 0.196818).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3363294
	speed: 0.4303s/iter; left time: 2362.7499s
Epoch: 8 cost time: 22.769586324691772
Epoch: 8, Steps: 130 | Train Loss: 0.3837908 Vali Loss: 0.1959231 Test Loss: 0.2718369
Validation loss decreased (0.196818 --> 0.195923).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2940724
	speed: 0.4279s/iter; left time: 2294.0294s
Epoch: 9 cost time: 18.849653482437134
Epoch: 9, Steps: 130 | Train Loss: 0.3832654 Vali Loss: 0.1959750 Test Loss: 0.2716323
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4128159
	speed: 0.4017s/iter; left time: 2101.5033s
Epoch: 10 cost time: 21.44948935508728
Epoch: 10, Steps: 130 | Train Loss: 0.3818736 Vali Loss: 0.1952162 Test Loss: 0.2711040
Validation loss decreased (0.195923 --> 0.195216).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2999025
	speed: 0.4454s/iter; left time: 2271.9016s
Epoch: 11 cost time: 21.495849132537842
Epoch: 11, Steps: 130 | Train Loss: 0.3812033 Vali Loss: 0.1951917 Test Loss: 0.2708868
Validation loss decreased (0.195216 --> 0.195192).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2706755
	speed: 0.4936s/iter; left time: 2453.8985s
Epoch: 12 cost time: 23.519679069519043
Epoch: 12, Steps: 130 | Train Loss: 0.3802197 Vali Loss: 0.1946264 Test Loss: 0.2703463
Validation loss decreased (0.195192 --> 0.194626).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4276854
	speed: 0.4530s/iter; left time: 2192.7423s
Epoch: 13 cost time: 21.3658185005188
Epoch: 13, Steps: 130 | Train Loss: 0.3796976 Vali Loss: 0.1948358 Test Loss: 0.2702165
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3845871
	speed: 0.4660s/iter; left time: 2195.1950s
Epoch: 14 cost time: 22.18150806427002
Epoch: 14, Steps: 130 | Train Loss: 0.3788913 Vali Loss: 0.1943761 Test Loss: 0.2698989
Validation loss decreased (0.194626 --> 0.194376).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3549257
	speed: 0.4956s/iter; left time: 2270.2116s
Epoch: 15 cost time: 23.31740164756775
Epoch: 15, Steps: 130 | Train Loss: 0.3782457 Vali Loss: 0.1942687 Test Loss: 0.2696679
Validation loss decreased (0.194376 --> 0.194269).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4261437
	speed: 0.4612s/iter; left time: 2052.6108s
Epoch: 16 cost time: 22.304892778396606
Epoch: 16, Steps: 130 | Train Loss: 0.3787289 Vali Loss: 0.1941034 Test Loss: 0.2696888
Validation loss decreased (0.194269 --> 0.194103).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4647554
	speed: 0.4506s/iter; left time: 1946.9055s
Epoch: 17 cost time: 21.403168439865112
Epoch: 17, Steps: 130 | Train Loss: 0.3777704 Vali Loss: 0.1940637 Test Loss: 0.2694054
Validation loss decreased (0.194103 --> 0.194064).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4116541
	speed: 0.4210s/iter; left time: 1764.4168s
Epoch: 18 cost time: 15.084022760391235
Epoch: 18, Steps: 130 | Train Loss: 0.3778531 Vali Loss: 0.1933752 Test Loss: 0.2693175
Validation loss decreased (0.194064 --> 0.193375).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3227770
	speed: 0.4040s/iter; left time: 1640.6462s
Epoch: 19 cost time: 18.705504417419434
Epoch: 19, Steps: 130 | Train Loss: 0.3778266 Vali Loss: 0.1934495 Test Loss: 0.2691887
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3414760
	speed: 0.3684s/iter; left time: 1448.3273s
Epoch: 20 cost time: 18.00157332420349
Epoch: 20, Steps: 130 | Train Loss: 0.3774875 Vali Loss: 0.1937299 Test Loss: 0.2690956
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4070918
	speed: 0.3654s/iter; left time: 1388.9107s
Epoch: 21 cost time: 17.391653537750244
Epoch: 21, Steps: 130 | Train Loss: 0.3769256 Vali Loss: 0.1935147 Test Loss: 0.2690204
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.27001628279685974, mae:0.32716140151023865, rse:0.41971608996391296, corr:[0.5438958  0.5558743  0.5550491  0.5517584  0.5512106  0.55309194
 0.55520964 0.5555034  0.55423313 0.5530529  0.55293256 0.5537529
 0.55467397 0.5547618  0.55392843 0.5528502  0.5521915  0.5520964
 0.55223334 0.5520838  0.5514067  0.5504363  0.54960954 0.5491778
 0.5490414  0.5488828  0.54841435 0.5476212  0.5467618  0.5460998
 0.545705   0.5454662  0.5451201  0.5444877  0.5436148  0.542722
 0.5419665  0.5413915  0.5409272  0.5404738  0.53996927 0.5394033
 0.5388169  0.53824276 0.53768486 0.53710896 0.5364743  0.5357643
 0.5349932  0.53423184 0.5335808  0.53308004 0.5326294  0.5321343
 0.5315421  0.53088564 0.5302271  0.52967817 0.52932423 0.52914417
 0.5290357  0.52889717 0.52865314 0.5283044  0.52794695 0.52766967
 0.5274576  0.5272587  0.52700686 0.52667165 0.5262686  0.5258685
 0.52550167 0.5251414  0.52470756 0.5241809  0.5235714  0.5229441
 0.52236676 0.5218722  0.5213815  0.5208567  0.5202703  0.51964265
 0.5190256  0.5184484  0.5179191  0.5174287  0.5169485  0.5164731
 0.51600564 0.51549524 0.514877   0.51406926 0.5130017  0.51164895
 0.5101178  0.50861377 0.50727016 0.50615114 0.5051857  0.50422573
 0.503152   0.5019205  0.5006546  0.49950013 0.49858797 0.4978703
 0.49718383 0.49637973 0.49544415 0.49446112 0.4935847  0.4928145
 0.4920585  0.49124792 0.49036902 0.48945078 0.48861453 0.48791033
 0.48732325 0.48666713 0.48580775 0.48464274 0.4832354  0.48185694
 0.48080862 0.48014727 0.47967407 0.47913817 0.47840318 0.4774966
 0.47659263 0.4758545  0.47531086 0.4748313  0.4742519  0.47353008
 0.4727231  0.47198802 0.47139776 0.47092566 0.47043842 0.46975166
 0.4688349  0.46777493 0.46685493 0.46618897 0.4657384  0.46532646
 0.46483102 0.46422476 0.4635084  0.46278033 0.4621244  0.4615641
 0.46101505 0.46042287 0.45981756 0.45933193 0.45899406 0.45874044
 0.45841417 0.45791784 0.45723808 0.456488   0.4558291  0.45541844
 0.45528919 0.45532176 0.45525268 0.45497447 0.45453775 0.4540701
 0.45368052 0.4533761  0.4530579  0.4526245  0.4520166  0.4513302
 0.45059112 0.44984707 0.44918182 0.4485872  0.44802845 0.44748923
 0.44699463 0.4464986  0.44585896 0.44492683 0.4436486  0.4421033
 0.44051424 0.4391294  0.43791503 0.43677157 0.43566445 0.43454987
 0.4334891  0.43254015 0.43173847 0.43102032 0.4302945  0.42946556
 0.42851937 0.42749625 0.426431   0.42541605 0.4245331  0.42377076
 0.4229113  0.4218928  0.42080325 0.41980094 0.4189957  0.41826153
 0.41745186 0.4164358  0.41530612 0.41414854 0.41301677 0.41197893
 0.41100344 0.4100265  0.40905407 0.40802464 0.40705287 0.40629447
 0.4056905  0.40520272 0.40464163 0.4039597  0.40318292 0.40245184
 0.40184093 0.40139872 0.40098163 0.40047404 0.39984038 0.39911753
 0.39845794 0.39791617 0.39757353 0.39746454 0.39735344 0.3971814
 0.3969623  0.3967765  0.39661735 0.3964283  0.3962531  0.39601997
 0.3957083  0.39544258 0.39521465 0.3951017  0.39502066 0.39491627
 0.39469603 0.3943288  0.3938736  0.39344147 0.39306086 0.3927531
 0.39244044 0.3920499  0.39173114 0.39160118 0.39164495 0.39172843
 0.39167872 0.39144078 0.39098084 0.39052576 0.39024687 0.39021912
 0.39041382 0.39044872 0.3900667  0.38933522 0.3884588  0.38779792
 0.38748965 0.38743937 0.38734522 0.38691375 0.38607973 0.38491955
 0.3838275  0.38321814 0.38291687 0.3825217  0.38181153 0.38083014
 0.38006216 0.37978798 0.3799124  0.38003725 0.37981987 0.37892127
 0.37770548 0.37665978 0.3763105  0.3764632  0.37667963 0.3764641
 0.37573215 0.37467343 0.37396187 0.37404266 0.37465182 0.3752444
 0.37494543 0.37370944 0.37209478 0.37106663 0.3713111  0.37252507
 0.37352836 0.37353876 0.37257066 0.37150845 0.371505   0.3730506
 0.37514514 0.3763656  0.37605792 0.3746454  0.37410656 0.37579226
 0.3791193  0.38153625 0.3812009  0.3782149  0.37628382 0.37957156]
