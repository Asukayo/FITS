Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  53344256.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3834239
	speed: 0.1671s/iter; left time: 1061.2639s
Epoch: 1 cost time: 21.673707008361816
Epoch: 1, Steps: 129 | Train Loss: 0.4852144 Vali Loss: 0.3166166 Test Loss: 0.4265838
Validation loss decreased (inf --> 0.316617).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3326975
	speed: 0.5247s/iter; left time: 3264.9916s
Epoch: 2 cost time: 26.25679636001587
Epoch: 2, Steps: 129 | Train Loss: 0.3669774 Vali Loss: 0.2938460 Test Loss: 0.3982409
Validation loss decreased (0.316617 --> 0.293846).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3067071
	speed: 0.5576s/iter; left time: 3397.1581s
Epoch: 3 cost time: 26.955743312835693
Epoch: 3, Steps: 129 | Train Loss: 0.3296911 Vali Loss: 0.2855382 Test Loss: 0.3880599
Validation loss decreased (0.293846 --> 0.285538).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3195807
	speed: 0.5427s/iter; left time: 3236.5476s
Epoch: 4 cost time: 26.359182596206665
Epoch: 4, Steps: 129 | Train Loss: 0.3092387 Vali Loss: 0.2805910 Test Loss: 0.3820911
Validation loss decreased (0.285538 --> 0.280591).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2626007
	speed: 0.5445s/iter; left time: 3176.9213s
Epoch: 5 cost time: 26.376389741897583
Epoch: 5, Steps: 129 | Train Loss: 0.2959064 Vali Loss: 0.2773458 Test Loss: 0.3778792
Validation loss decreased (0.280591 --> 0.277346).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3056609
	speed: 0.5640s/iter; left time: 3218.4665s
Epoch: 6 cost time: 29.519487857818604
Epoch: 6, Steps: 129 | Train Loss: 0.2860329 Vali Loss: 0.2749117 Test Loss: 0.3747108
Validation loss decreased (0.277346 --> 0.274912).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2126927
	speed: 0.6020s/iter; left time: 3357.4113s
Epoch: 7 cost time: 28.77743911743164
Epoch: 7, Steps: 129 | Train Loss: 0.2784066 Vali Loss: 0.2729486 Test Loss: 0.3719781
Validation loss decreased (0.274912 --> 0.272949).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2978131
	speed: 0.5870s/iter; left time: 3197.7749s
Epoch: 8 cost time: 26.014840364456177
Epoch: 8, Steps: 129 | Train Loss: 0.2734162 Vali Loss: 0.2709493 Test Loss: 0.3698469
Validation loss decreased (0.272949 --> 0.270949).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2591555
	speed: 0.5191s/iter; left time: 2761.2519s
Epoch: 9 cost time: 25.67009949684143
Epoch: 9, Steps: 129 | Train Loss: 0.2692656 Vali Loss: 0.2698562 Test Loss: 0.3681279
Validation loss decreased (0.270949 --> 0.269856).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3001117
	speed: 0.5181s/iter; left time: 2688.7867s
Epoch: 10 cost time: 24.468871593475342
Epoch: 10, Steps: 129 | Train Loss: 0.2659487 Vali Loss: 0.2687751 Test Loss: 0.3668044
Validation loss decreased (0.269856 --> 0.268775).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2070836
	speed: 0.5317s/iter; left time: 2690.9635s
Epoch: 11 cost time: 26.012201070785522
Epoch: 11, Steps: 129 | Train Loss: 0.2635999 Vali Loss: 0.2675884 Test Loss: 0.3656973
Validation loss decreased (0.268775 --> 0.267588).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2684616
	speed: 0.5178s/iter; left time: 2553.9032s
Epoch: 12 cost time: 23.634949684143066
Epoch: 12, Steps: 129 | Train Loss: 0.2618243 Vali Loss: 0.2669666 Test Loss: 0.3646415
Validation loss decreased (0.267588 --> 0.266967).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2521137
	speed: 0.5023s/iter; left time: 2412.6590s
Epoch: 13 cost time: 25.45615577697754
Epoch: 13, Steps: 129 | Train Loss: 0.2602658 Vali Loss: 0.2663772 Test Loss: 0.3638622
Validation loss decreased (0.266967 --> 0.266377).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2957435
	speed: 0.4728s/iter; left time: 2209.7894s
Epoch: 14 cost time: 22.895029544830322
Epoch: 14, Steps: 129 | Train Loss: 0.2594588 Vali Loss: 0.2659266 Test Loss: 0.3632676
Validation loss decreased (0.266377 --> 0.265927).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2417321
	speed: 0.5285s/iter; left time: 2401.9411s
Epoch: 15 cost time: 22.790629625320435
Epoch: 15, Steps: 129 | Train Loss: 0.2588527 Vali Loss: 0.2653282 Test Loss: 0.3627927
Validation loss decreased (0.265927 --> 0.265328).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2261787
	speed: 0.5124s/iter; left time: 2262.9232s
Epoch: 16 cost time: 25.841542959213257
Epoch: 16, Steps: 129 | Train Loss: 0.2580324 Vali Loss: 0.2649654 Test Loss: 0.3622823
Validation loss decreased (0.265328 --> 0.264965).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3184080
	speed: 0.4805s/iter; left time: 2059.9787s
Epoch: 17 cost time: 20.67483115196228
Epoch: 17, Steps: 129 | Train Loss: 0.2576003 Vali Loss: 0.2646475 Test Loss: 0.3619276
Validation loss decreased (0.264965 --> 0.264647).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2141653
	speed: 0.5044s/iter; left time: 2097.4441s
Epoch: 18 cost time: 24.5564968585968
Epoch: 18, Steps: 129 | Train Loss: 0.2573272 Vali Loss: 0.2643652 Test Loss: 0.3617115
Validation loss decreased (0.264647 --> 0.264365).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2767296
	speed: 0.5127s/iter; left time: 2065.5605s
Epoch: 19 cost time: 24.131414651870728
Epoch: 19, Steps: 129 | Train Loss: 0.2566572 Vali Loss: 0.2642619 Test Loss: 0.3614236
Validation loss decreased (0.264365 --> 0.264262).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2471465
	speed: 0.5028s/iter; left time: 1961.0201s
Epoch: 20 cost time: 24.242437601089478
Epoch: 20, Steps: 129 | Train Loss: 0.2564722 Vali Loss: 0.2640108 Test Loss: 0.3612355
Validation loss decreased (0.264262 --> 0.264011).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2661780
	speed: 0.5357s/iter; left time: 2020.3096s
Epoch: 21 cost time: 26.83642601966858
Epoch: 21, Steps: 129 | Train Loss: 0.2559566 Vali Loss: 0.2636247 Test Loss: 0.3610875
Validation loss decreased (0.264011 --> 0.263625).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2918990
	speed: 0.5537s/iter; left time: 2016.4100s
Epoch: 22 cost time: 26.398311138153076
Epoch: 22, Steps: 129 | Train Loss: 0.2559024 Vali Loss: 0.2636963 Test Loss: 0.3609155
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2296430
	speed: 0.4799s/iter; left time: 1685.7734s
Epoch: 23 cost time: 20.1589298248291
Epoch: 23, Steps: 129 | Train Loss: 0.2559910 Vali Loss: 0.2636053 Test Loss: 0.3608134
Validation loss decreased (0.263625 --> 0.263605).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3121048
	speed: 0.5012s/iter; left time: 1696.0809s
Epoch: 24 cost time: 24.457064151763916
Epoch: 24, Steps: 129 | Train Loss: 0.2558143 Vali Loss: 0.2636449 Test Loss: 0.3606472
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2067033
	speed: 0.5095s/iter; left time: 1658.4469s
Epoch: 25 cost time: 24.801626443862915
Epoch: 25, Steps: 129 | Train Loss: 0.2554608 Vali Loss: 0.2631894 Test Loss: 0.3605546
Validation loss decreased (0.263605 --> 0.263189).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3405588
	speed: 0.5178s/iter; left time: 1618.7572s
Epoch: 26 cost time: 25.145768404006958
Epoch: 26, Steps: 129 | Train Loss: 0.2551714 Vali Loss: 0.2632801 Test Loss: 0.3605385
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2624226
	speed: 0.5157s/iter; left time: 1545.4251s
Epoch: 27 cost time: 23.90949034690857
Epoch: 27, Steps: 129 | Train Loss: 0.2554069 Vali Loss: 0.2631995 Test Loss: 0.3604940
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2542776
	speed: 0.5053s/iter; left time: 1449.2215s
Epoch: 28 cost time: 26.024338722229004
Epoch: 28, Steps: 129 | Train Loss: 0.2553497 Vali Loss: 0.2632132 Test Loss: 0.3603923
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  53344256.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4674960
	speed: 0.2022s/iter; left time: 1284.3296s
Epoch: 1 cost time: 26.42526149749756
Epoch: 1, Steps: 129 | Train Loss: 0.4977627 Vali Loss: 0.2619527 Test Loss: 0.3595652
Validation loss decreased (inf --> 0.261953).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4029670
	speed: 0.5644s/iter; left time: 3511.5057s
Epoch: 2 cost time: 25.5801420211792
Epoch: 2, Steps: 129 | Train Loss: 0.4965493 Vali Loss: 0.2616427 Test Loss: 0.3590555
Validation loss decreased (0.261953 --> 0.261643).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6180454
	speed: 0.5134s/iter; left time: 3128.2007s
Epoch: 3 cost time: 24.967706203460693
Epoch: 3, Steps: 129 | Train Loss: 0.4960410 Vali Loss: 0.2611815 Test Loss: 0.3587239
Validation loss decreased (0.261643 --> 0.261182).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4542242
	speed: 0.5004s/iter; left time: 2984.4598s
Epoch: 4 cost time: 23.46557354927063
Epoch: 4, Steps: 129 | Train Loss: 0.4954304 Vali Loss: 0.2610991 Test Loss: 0.3583196
Validation loss decreased (0.261182 --> 0.261099).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5431855
	speed: 0.5063s/iter; left time: 2954.1012s
Epoch: 5 cost time: 23.680084705352783
Epoch: 5, Steps: 129 | Train Loss: 0.4951079 Vali Loss: 0.2606332 Test Loss: 0.3582529
Validation loss decreased (0.261099 --> 0.260633).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4891143
	speed: 0.4799s/iter; left time: 2738.5042s
Epoch: 6 cost time: 21.901082515716553
Epoch: 6, Steps: 129 | Train Loss: 0.4953393 Vali Loss: 0.2606349 Test Loss: 0.3581719
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5086465
	speed: 0.4207s/iter; left time: 2346.0831s
Epoch: 7 cost time: 16.424021005630493
Epoch: 7, Steps: 129 | Train Loss: 0.4951329 Vali Loss: 0.2606679 Test Loss: 0.3580017
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4631063
	speed: 0.2898s/iter; left time: 1578.7057s
Epoch: 8 cost time: 15.738506078720093
Epoch: 8, Steps: 129 | Train Loss: 0.4945798 Vali Loss: 0.2606765 Test Loss: 0.3579911
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.34928274154663086, mae:0.37825918197631836, rse:0.47504302859306335, corr:[0.5403647  0.5460871  0.54387116 0.5408197  0.540497   0.54193664
 0.54281086 0.5419854  0.5405884  0.5399914  0.5403314  0.5408089
 0.5406083  0.5398023  0.53907603 0.53882545 0.5388116  0.53844726
 0.5374999  0.53636765 0.5355768  0.5352383  0.53505266 0.5346084
 0.5337986  0.53296024 0.5323938  0.53204674 0.53163236 0.53095376
 0.5301188  0.5294171  0.5289677  0.5286489  0.5282384  0.5275602
 0.5266692  0.5258096  0.5251331  0.5246477  0.52421993 0.5237097
 0.52314186 0.522574   0.5219959  0.52135646 0.5206003  0.51970977
 0.5187661  0.5178751  0.51714444 0.51659125 0.5160759  0.5154776
 0.51483697 0.514246   0.5137437  0.51337236 0.5131436  0.5129297
 0.5126917  0.5124431  0.5122127  0.5119656  0.51165646 0.5113068
 0.51092213 0.5106198  0.51040304 0.51019293 0.5098946  0.50946534
 0.50894225 0.5083909  0.5078721  0.50741124 0.5069936  0.50656056
 0.50614476 0.5056862  0.5051215  0.5044049  0.50353664 0.5025833
 0.50167936 0.50094295 0.500426   0.50010324 0.49978423 0.49935588
 0.49880028 0.49810126 0.49727282 0.4962838  0.49505526 0.49363333
 0.4922024  0.49093565 0.48983568 0.488799   0.4876821  0.48643193
 0.48512    0.48389927 0.4828699  0.48200434 0.4811815  0.4803023
 0.4793471  0.47841653 0.4775793  0.4767886  0.47597954 0.4750811
 0.474148   0.47326723 0.47241497 0.47146806 0.47040018 0.46936896
 0.4686597  0.46828124 0.4679889  0.4673913  0.4662885  0.46484867
 0.4635502  0.46269763 0.46214107 0.4614813  0.46047312 0.45923808
 0.4582456  0.4578159  0.457779   0.45761657 0.4569094  0.4556497
 0.45421302 0.45309776 0.4524337  0.451929   0.4512534  0.45033607
 0.44950297 0.4491117  0.44921526 0.44932196 0.44894293 0.44795218
 0.44669548 0.44561613 0.44479167 0.44401592 0.44305775 0.44198418
 0.441142   0.44081768 0.44090182 0.44096488 0.44051242 0.43949673
 0.43834528 0.43766597 0.43764865 0.4379382  0.43802416 0.43769062
 0.43710822 0.43660027 0.4362573  0.43598258 0.43552786 0.43481037
 0.4339435  0.43315187 0.4325258  0.43198878 0.43145138 0.43094152
 0.43049505 0.43013766 0.4297983  0.4293082  0.42864412 0.42794985
 0.4274667  0.4271271  0.42662457 0.4255376  0.4237215  0.4214626
 0.4194121  0.41812024 0.41735694 0.41662028 0.41554704 0.4141079
 0.4126553  0.4115687  0.41092476 0.41044986 0.40978312 0.4087384
 0.40738705 0.40595356 0.40464774 0.40355682 0.4026735  0.4018996
 0.40106058 0.40021044 0.39951605 0.39907858 0.39880615 0.39835092
 0.39742216 0.39591065 0.39414304 0.3925584  0.39149344 0.39094564
 0.39058906 0.39005706 0.38919604 0.38801453 0.38684443 0.38598713
 0.3853241  0.3845884  0.38355368 0.3823109  0.38117942 0.3804522
 0.38007072 0.3797867  0.37931648 0.37863505 0.3780192  0.377676
 0.37752783 0.3772113  0.3765213  0.3756677  0.37499264 0.37490422
 0.37536418 0.375957   0.37616053 0.37572578 0.3749047  0.3741158
 0.37369028 0.3736755  0.37379646 0.37386566 0.37379974 0.37367615
 0.37349883 0.37321433 0.37276262 0.3721821  0.37150112 0.37081957
 0.3701578  0.36953083 0.36913428 0.36909452 0.36929882 0.36935964
 0.36891863 0.36781368 0.36631855 0.36515197 0.3648166  0.36524042
 0.36592838 0.36625865 0.36606383 0.3657084  0.36550787 0.36550823
 0.36534157 0.36469093 0.3635691  0.3623666  0.36155626 0.36110622
 0.36063647 0.3597768  0.35841426 0.35695666 0.35603312 0.3557717
 0.35576132 0.3553154  0.35418236 0.35278055 0.3518665  0.35171434
 0.35202083 0.35203025 0.35139644 0.3502004  0.3490928  0.3485287
 0.34846106 0.348349   0.3478394  0.34700054 0.3462202  0.3459128
 0.34595412 0.34608528 0.34600106 0.34566173 0.34529224 0.34507838
 0.34492722 0.34472737 0.34443545 0.34410042 0.34380725 0.34357944
 0.34329155 0.34291872 0.34259912 0.34253842 0.34293494 0.34356773
 0.34405467 0.3439864  0.343384   0.34267506 0.34228215 0.34241435
 0.3427156  0.34263802 0.34202874 0.34114477 0.3404148  0.34012157
 0.34025142 0.34050643 0.3406237  0.34056652 0.34051955 0.3406863
 0.3410303  0.34139147 0.3415758  0.34153834 0.34141567 0.34131798
 0.34125108 0.3411386  0.34084907 0.34046572 0.34000713 0.33964458
 0.3394359  0.33935246 0.3393069  0.3392333  0.3390717  0.33885568
 0.33852    0.33813733 0.33776218 0.3374964  0.3374223  0.3374763
 0.33760914 0.33778504 0.33795443 0.3380806  0.33812836 0.33810902
 0.3380376  0.33791015 0.33772838 0.33747843 0.3371902  0.33667183
 0.33584893 0.33474666 0.33359623 0.3327126  0.33229855 0.33235344
 0.33246925 0.33224708 0.33151013 0.3303948  0.32926753 0.32845786
 0.3280959  0.32805324 0.32796046 0.32761812 0.32707804 0.32639042
 0.32568568 0.32501316 0.32440203 0.32388183 0.32342643 0.3231782
 0.32319558 0.32347727 0.32378757 0.32379767 0.32336378 0.3225471
 0.321755   0.321383   0.32149914 0.32187292 0.32205832 0.3217738
 0.32109654 0.32039526 0.32006595 0.3202951  0.32090595 0.3214813
 0.32169887 0.3214841  0.3211101  0.32088777 0.3209798  0.32126316
 0.32146823 0.32137707 0.32112536 0.32098258 0.3211148  0.32140326
 0.32155615 0.32147387 0.3210785  0.32046148 0.31985956 0.31949043
 0.31931442 0.31930473 0.31933132 0.31927252 0.31911734 0.3188363
 0.31848606 0.3180956  0.31771964 0.31732914 0.3169862  0.31675357
 0.31662092 0.31654736 0.31644297 0.316235   0.31593642 0.31545946
 0.31488535 0.31432733 0.31390214 0.3136741  0.31362507 0.31368837
 0.31377053 0.31373647 0.31347564 0.31296635 0.31233448 0.31165665
 0.31100497 0.31029505 0.3094723  0.30843413 0.30726773 0.30611968
 0.30512482 0.30436206 0.3036258  0.30273822 0.30164704 0.30046406
 0.2993897  0.29862547 0.29808572 0.29761776 0.2970265  0.29632726
 0.2957137  0.2954108  0.29530212 0.29499286 0.29415837 0.29272446
 0.29103908 0.28960642 0.2888067  0.28864786 0.2886858  0.28856134
 0.28794226 0.2868752  0.28574404 0.28492755 0.2845953  0.28450125
 0.28438526 0.2841158  0.28369057 0.28324726 0.2829733  0.2828816
 0.28281322 0.28258213 0.2822432  0.2817545  0.28119442 0.2807304
 0.2804573  0.28039902 0.2804428  0.2804582  0.28037608 0.28018448
 0.27992874 0.27969557 0.27948573 0.2792541  0.27898386 0.27868944
 0.27844265 0.27833706 0.2784718  0.27870706 0.27888057 0.27892637
 0.27889863 0.27890378 0.27893105 0.2789297  0.2787187  0.27819535
 0.27751395 0.27697322 0.27675462 0.2768774  0.27714953 0.27740803
 0.27746674 0.27732408 0.27707478 0.27686977 0.2767015  0.27666432
 0.27665025 0.27655977 0.27621138 0.27562213 0.27503046 0.2746731
 0.2746196  0.27464014 0.27445474 0.27390066 0.27313763 0.2725214
 0.27227855 0.27231678 0.27221915 0.27166814 0.27060956 0.26921067
 0.26784915 0.26693064 0.2663472  0.26574415 0.264829   0.26366252
 0.2625117  0.26157275 0.26082337 0.2600642  0.25913435 0.25809997
 0.2570869  0.25619575 0.25550473 0.25491795 0.2543552  0.25388527
 0.25355774 0.25332925 0.2531313  0.2527288  0.25212353 0.2514573
 0.25092044 0.2507301  0.25079355 0.25086495 0.25076497 0.25035837
 0.24963914 0.24891733 0.2485733  0.24865542 0.24908474 0.24955575
 0.2498265  0.24964704 0.24925388 0.2490685  0.24929963 0.2498467
 0.25043038 0.25085184 0.2510164  0.2511271  0.25142133 0.251952
 0.25242338 0.2524988  0.2521356  0.2517348  0.25173762 0.25233737
 0.25324717 0.25397855 0.254132   0.2534836  0.2526194  0.2520115
 0.2519159  0.25219533 0.2523805  0.2523035  0.2518343  0.25128862
 0.2509424  0.2510203  0.25128794 0.25140855 0.25144932 0.25132242
 0.2512595  0.2512636  0.25124604 0.25102916 0.2506671  0.2501715
 0.24989566 0.24984792 0.2497851  0.24957927 0.24914895 0.24861413
 0.24831656 0.24869598 0.24954204 0.2503728  0.2507843  0.25058383
 0.24997666 0.2494606  0.24925645 0.24913067 0.24873172 0.24788931
 0.24668485 0.24543455 0.24439147 0.24354288 0.24280241 0.24196306
 0.24097088 0.23993628 0.2392389  0.23871735 0.23830536 0.23788773
 0.23748884 0.23717305 0.23689924 0.23649862 0.23597643 0.23519696
 0.23429824 0.23344187 0.23267992 0.23217747 0.2317068  0.2311326
 0.2305571  0.23011565 0.23001371 0.23004037 0.22998844 0.22958378
 0.22892764 0.22806306 0.22740483 0.22715974 0.22708574 0.22673556
 0.2257807  0.2245875  0.22376595 0.22388966 0.22433227 0.2243678
 0.22363363 0.22272897 0.22268036 0.22292608 0.21983685 0.20903437]
