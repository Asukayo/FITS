Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=74, out_features=108, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7160832.0
params:  8100.0
Trainable parameters:  8100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3601262
	speed: 0.0704s/iter; left time: 911.8838s
	iters: 200, epoch: 1 | loss: 0.2632196
	speed: 0.0727s/iter; left time: 934.2231s
Epoch: 1 cost time: 18.64630103111267
Epoch: 1, Steps: 261 | Train Loss: 0.3381004 Vali Loss: 0.2364889 Test Loss: 0.3177634
Validation loss decreased (inf --> 0.236489).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1757077
	speed: 0.3224s/iter; left time: 4090.6815s
	iters: 200, epoch: 2 | loss: 0.2061195
	speed: 0.0710s/iter; left time: 894.4480s
Epoch: 2 cost time: 18.825166702270508
Epoch: 2, Steps: 261 | Train Loss: 0.2215628 Vali Loss: 0.2196711 Test Loss: 0.2971269
Validation loss decreased (0.236489 --> 0.219671).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1715344
	speed: 0.2995s/iter; left time: 3722.5263s
	iters: 200, epoch: 3 | loss: 0.1909464
	speed: 0.0736s/iter; left time: 907.9699s
Epoch: 3 cost time: 19.511119604110718
Epoch: 3, Steps: 261 | Train Loss: 0.1818529 Vali Loss: 0.2123103 Test Loss: 0.2882828
Validation loss decreased (0.219671 --> 0.212310).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2113437
	speed: 0.2881s/iter; left time: 3505.8393s
	iters: 200, epoch: 4 | loss: 0.1214118
	speed: 0.0661s/iter; left time: 797.4338s
Epoch: 4 cost time: 18.523249864578247
Epoch: 4, Steps: 261 | Train Loss: 0.1614935 Vali Loss: 0.2074494 Test Loss: 0.2825059
Validation loss decreased (0.212310 --> 0.207449).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1497323
	speed: 0.2947s/iter; left time: 3509.3532s
	iters: 200, epoch: 5 | loss: 0.1233359
	speed: 0.0654s/iter; left time: 771.8935s
Epoch: 5 cost time: 18.750205516815186
Epoch: 5, Steps: 261 | Train Loss: 0.1497997 Vali Loss: 0.2043451 Test Loss: 0.2787592
Validation loss decreased (0.207449 --> 0.204345).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1515300
	speed: 0.4669s/iter; left time: 5437.3052s
	iters: 200, epoch: 6 | loss: 0.1634459
	speed: 0.0809s/iter; left time: 934.2279s
Epoch: 6 cost time: 20.98678684234619
Epoch: 6, Steps: 261 | Train Loss: 0.1435915 Vali Loss: 0.2022364 Test Loss: 0.2767002
Validation loss decreased (0.204345 --> 0.202236).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1434952
	speed: 0.4192s/iter; left time: 4772.6513s
	iters: 200, epoch: 7 | loss: 0.1114283
	speed: 0.1148s/iter; left time: 1295.2138s
Epoch: 7 cost time: 29.358327627182007
Epoch: 7, Steps: 261 | Train Loss: 0.1398105 Vali Loss: 0.2001626 Test Loss: 0.2750110
Validation loss decreased (0.202236 --> 0.200163).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1472471
	speed: 0.3257s/iter; left time: 3623.0383s
	iters: 200, epoch: 8 | loss: 0.1424619
	speed: 0.0676s/iter; left time: 745.6871s
Epoch: 8 cost time: 17.39656925201416
Epoch: 8, Steps: 261 | Train Loss: 0.1378164 Vali Loss: 0.1994806 Test Loss: 0.2743373
Validation loss decreased (0.200163 --> 0.199481).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1310943
	speed: 0.1853s/iter; left time: 2013.0936s
	iters: 200, epoch: 9 | loss: 0.1226038
	speed: 0.0608s/iter; left time: 654.2474s
Epoch: 9 cost time: 15.720236539840698
Epoch: 9, Steps: 261 | Train Loss: 0.1364241 Vali Loss: 0.1990193 Test Loss: 0.2739206
Validation loss decreased (0.199481 --> 0.199019).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1171994
	speed: 0.2671s/iter; left time: 2831.8233s
	iters: 200, epoch: 10 | loss: 0.1608966
	speed: 0.0650s/iter; left time: 682.7641s
Epoch: 10 cost time: 17.517231941223145
Epoch: 10, Steps: 261 | Train Loss: 0.1359909 Vali Loss: 0.1983091 Test Loss: 0.2735241
Validation loss decreased (0.199019 --> 0.198309).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1623322
	speed: 0.2730s/iter; left time: 2822.8536s
	iters: 200, epoch: 11 | loss: 0.1233978
	speed: 0.0615s/iter; left time: 630.2520s
Epoch: 11 cost time: 17.251577377319336
Epoch: 11, Steps: 261 | Train Loss: 0.1355749 Vali Loss: 0.1980242 Test Loss: 0.2733909
Validation loss decreased (0.198309 --> 0.198024).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1494214
	speed: 0.2344s/iter; left time: 2363.2473s
	iters: 200, epoch: 12 | loss: 0.1210822
	speed: 0.0607s/iter; left time: 605.3052s
Epoch: 12 cost time: 16.697329998016357
Epoch: 12, Steps: 261 | Train Loss: 0.1351926 Vali Loss: 0.1979365 Test Loss: 0.2732468
Validation loss decreased (0.198024 --> 0.197937).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1382360
	speed: 0.2786s/iter; left time: 2735.8506s
	iters: 200, epoch: 13 | loss: 0.1274515
	speed: 0.0652s/iter; left time: 633.1993s
Epoch: 13 cost time: 18.483848571777344
Epoch: 13, Steps: 261 | Train Loss: 0.1349182 Vali Loss: 0.1977813 Test Loss: 0.2730000
Validation loss decreased (0.197937 --> 0.197781).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1058787
	speed: 0.2619s/iter; left time: 2502.8916s
	iters: 200, epoch: 14 | loss: 0.0957959
	speed: 0.0693s/iter; left time: 655.4456s
Epoch: 14 cost time: 18.448543310165405
Epoch: 14, Steps: 261 | Train Loss: 0.1348705 Vali Loss: 0.1977354 Test Loss: 0.2731433
Validation loss decreased (0.197781 --> 0.197735).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1655235
	speed: 0.2441s/iter; left time: 2269.7086s
	iters: 200, epoch: 15 | loss: 0.1770822
	speed: 0.0592s/iter; left time: 544.8093s
Epoch: 15 cost time: 15.932281255722046
Epoch: 15, Steps: 261 | Train Loss: 0.1348258 Vali Loss: 0.1980383 Test Loss: 0.2731292
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1090620
	speed: 0.2008s/iter; left time: 1814.4823s
	iters: 200, epoch: 16 | loss: 0.1600201
	speed: 0.0464s/iter; left time: 414.9404s
Epoch: 16 cost time: 13.422980546951294
Epoch: 16, Steps: 261 | Train Loss: 0.1349200 Vali Loss: 0.1977382 Test Loss: 0.2729045
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1259787
	speed: 0.1943s/iter; left time: 1705.3002s
	iters: 200, epoch: 17 | loss: 0.1188594
	speed: 0.0619s/iter; left time: 536.7662s
Epoch: 17 cost time: 15.387541055679321
Epoch: 17, Steps: 261 | Train Loss: 0.1349514 Vali Loss: 0.1976517 Test Loss: 0.2729308
Validation loss decreased (0.197735 --> 0.197652).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1039780
	speed: 0.2447s/iter; left time: 2083.6941s
	iters: 200, epoch: 18 | loss: 0.1608183
	speed: 0.0643s/iter; left time: 541.1537s
Epoch: 18 cost time: 16.69243359565735
Epoch: 18, Steps: 261 | Train Loss: 0.1347280 Vali Loss: 0.1979827 Test Loss: 0.2732752
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1180434
	speed: 0.2415s/iter; left time: 1993.3761s
	iters: 200, epoch: 19 | loss: 0.0936169
	speed: 0.0581s/iter; left time: 473.9256s
Epoch: 19 cost time: 16.274661540985107
Epoch: 19, Steps: 261 | Train Loss: 0.1348742 Vali Loss: 0.1977436 Test Loss: 0.2730342
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1694443
	speed: 0.2485s/iter; left time: 1986.0465s
	iters: 200, epoch: 20 | loss: 0.1684407
	speed: 0.0619s/iter; left time: 488.5554s
Epoch: 20 cost time: 16.49802827835083
Epoch: 20, Steps: 261 | Train Loss: 0.1349081 Vali Loss: 0.1977996 Test Loss: 0.2730639
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=74, out_features=108, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7160832.0
params:  8100.0
Trainable parameters:  8100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3517284
	speed: 0.0662s/iter; left time: 858.0029s
	iters: 200, epoch: 1 | loss: 0.3616848
	speed: 0.0644s/iter; left time: 828.2011s
Epoch: 1 cost time: 16.779395580291748
Epoch: 1, Steps: 261 | Train Loss: 0.3817010 Vali Loss: 0.1956655 Test Loss: 0.2708693
Validation loss decreased (inf --> 0.195666).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4224123
	speed: 0.2497s/iter; left time: 3168.5893s
	iters: 200, epoch: 2 | loss: 0.4508137
	speed: 0.0615s/iter; left time: 774.2038s
Epoch: 2 cost time: 17.4723482131958
Epoch: 2, Steps: 261 | Train Loss: 0.3801147 Vali Loss: 0.1954226 Test Loss: 0.2701248
Validation loss decreased (0.195666 --> 0.195423).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3630956
	speed: 0.2251s/iter; left time: 2797.1887s
	iters: 200, epoch: 3 | loss: 0.5832925
	speed: 0.0599s/iter; left time: 738.6849s
Epoch: 3 cost time: 15.99642825126648
Epoch: 3, Steps: 261 | Train Loss: 0.3788194 Vali Loss: 0.1947822 Test Loss: 0.2695638
Validation loss decreased (0.195423 --> 0.194782).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3436989
	speed: 0.2323s/iter; left time: 2826.0502s
	iters: 200, epoch: 4 | loss: 0.3433111
	speed: 0.0664s/iter; left time: 801.5752s
Epoch: 4 cost time: 16.316230297088623
Epoch: 4, Steps: 261 | Train Loss: 0.3785782 Vali Loss: 0.1944376 Test Loss: 0.2691681
Validation loss decreased (0.194782 --> 0.194438).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3532349
	speed: 0.2571s/iter; left time: 3060.7651s
	iters: 200, epoch: 5 | loss: 0.4723274
	speed: 0.0594s/iter; left time: 701.3220s
Epoch: 5 cost time: 16.164292335510254
Epoch: 5, Steps: 261 | Train Loss: 0.3775949 Vali Loss: 0.1944162 Test Loss: 0.2693110
Validation loss decreased (0.194438 --> 0.194416).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3847030
	speed: 0.2522s/iter; left time: 2936.6567s
	iters: 200, epoch: 6 | loss: 0.4445336
	speed: 0.0690s/iter; left time: 796.7753s
Epoch: 6 cost time: 17.64316987991333
Epoch: 6, Steps: 261 | Train Loss: 0.3776395 Vali Loss: 0.1942193 Test Loss: 0.2690788
Validation loss decreased (0.194416 --> 0.194219).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3837445
	speed: 0.2305s/iter; left time: 2624.4393s
	iters: 200, epoch: 7 | loss: 0.3248416
	speed: 0.0587s/iter; left time: 662.9234s
Epoch: 7 cost time: 15.386473655700684
Epoch: 7, Steps: 261 | Train Loss: 0.3769365 Vali Loss: 0.1943253 Test Loss: 0.2688604
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3984226
	speed: 0.2308s/iter; left time: 2567.3177s
	iters: 200, epoch: 8 | loss: 0.3548462
	speed: 0.0637s/iter; left time: 701.8027s
Epoch: 8 cost time: 16.795963048934937
Epoch: 8, Steps: 261 | Train Loss: 0.3771027 Vali Loss: 0.1942953 Test Loss: 0.2688392
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4027509
	speed: 0.2507s/iter; left time: 2723.0102s
	iters: 200, epoch: 9 | loss: 0.3869572
	speed: 0.0615s/iter; left time: 662.2047s
Epoch: 9 cost time: 17.310327768325806
Epoch: 9, Steps: 261 | Train Loss: 0.3764718 Vali Loss: 0.1940281 Test Loss: 0.2686627
Validation loss decreased (0.194219 --> 0.194028).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3678475
	speed: 0.2450s/iter; left time: 2597.9982s
	iters: 200, epoch: 10 | loss: 0.2990869
	speed: 0.0653s/iter; left time: 685.8973s
Epoch: 10 cost time: 19.503959894180298
Epoch: 10, Steps: 261 | Train Loss: 0.3762338 Vali Loss: 0.1939255 Test Loss: 0.2686191
Validation loss decreased (0.194028 --> 0.193926).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4719285
	speed: 0.2829s/iter; left time: 2925.4275s
	iters: 200, epoch: 11 | loss: 0.3282478
	speed: 0.0665s/iter; left time: 680.8727s
Epoch: 11 cost time: 18.075229167938232
Epoch: 11, Steps: 261 | Train Loss: 0.3765832 Vali Loss: 0.1940280 Test Loss: 0.2686691
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3110693
	speed: 0.2999s/iter; left time: 3022.9434s
	iters: 200, epoch: 12 | loss: 0.4246790
	speed: 0.0632s/iter; left time: 631.0594s
Epoch: 12 cost time: 18.594862461090088
Epoch: 12, Steps: 261 | Train Loss: 0.3764557 Vali Loss: 0.1939036 Test Loss: 0.2684194
Validation loss decreased (0.193926 --> 0.193904).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4591500
	speed: 0.2725s/iter; left time: 2675.3304s
	iters: 200, epoch: 13 | loss: 0.3244827
	speed: 0.0552s/iter; left time: 536.8594s
Epoch: 13 cost time: 15.874024391174316
Epoch: 13, Steps: 261 | Train Loss: 0.3756300 Vali Loss: 0.1939063 Test Loss: 0.2683928
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2497601
	speed: 0.2282s/iter; left time: 2181.1192s
	iters: 200, epoch: 14 | loss: 0.4774914
	speed: 0.0573s/iter; left time: 541.9834s
Epoch: 14 cost time: 15.492419719696045
Epoch: 14, Steps: 261 | Train Loss: 0.3758389 Vali Loss: 0.1938997 Test Loss: 0.2684864
Validation loss decreased (0.193904 --> 0.193900).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3242716
	speed: 0.2572s/iter; left time: 2391.0060s
	iters: 200, epoch: 15 | loss: 0.3626460
	speed: 0.0735s/iter; left time: 675.6108s
Epoch: 15 cost time: 18.26805090904236
Epoch: 15, Steps: 261 | Train Loss: 0.3756748 Vali Loss: 0.1937977 Test Loss: 0.2683274
Validation loss decreased (0.193900 --> 0.193798).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3188751
	speed: 0.2439s/iter; left time: 2203.9185s
	iters: 200, epoch: 16 | loss: 0.5228622
	speed: 0.0611s/iter; left time: 545.7818s
Epoch: 16 cost time: 15.942368507385254
Epoch: 16, Steps: 261 | Train Loss: 0.3763644 Vali Loss: 0.1937804 Test Loss: 0.2682419
Validation loss decreased (0.193798 --> 0.193780).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3259745
	speed: 0.1523s/iter; left time: 1336.0434s
	iters: 200, epoch: 17 | loss: 0.3585569
	speed: 0.0486s/iter; left time: 421.4147s
Epoch: 17 cost time: 12.90266466140747
Epoch: 17, Steps: 261 | Train Loss: 0.3760048 Vali Loss: 0.1937871 Test Loss: 0.2684362
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3977392
	speed: 0.1761s/iter; left time: 1499.3135s
	iters: 200, epoch: 18 | loss: 0.4162463
	speed: 0.0465s/iter; left time: 391.4869s
Epoch: 18 cost time: 13.978265762329102
Epoch: 18, Steps: 261 | Train Loss: 0.3752231 Vali Loss: 0.1936981 Test Loss: 0.2683001
Validation loss decreased (0.193780 --> 0.193698).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4899519
	speed: 0.2284s/iter; left time: 1885.3103s
	iters: 200, epoch: 19 | loss: 0.3025487
	speed: 0.0630s/iter; left time: 513.3300s
Epoch: 19 cost time: 17.292449951171875
Epoch: 19, Steps: 261 | Train Loss: 0.3759019 Vali Loss: 0.1938692 Test Loss: 0.2683975
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4798333
	speed: 0.2565s/iter; left time: 2050.2863s
	iters: 200, epoch: 20 | loss: 0.3533358
	speed: 0.0566s/iter; left time: 446.4958s
Epoch: 20 cost time: 15.830978631973267
Epoch: 20, Steps: 261 | Train Loss: 0.3758398 Vali Loss: 0.1937461 Test Loss: 0.2683572
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3525234
	speed: 0.2322s/iter; left time: 1794.9052s
	iters: 200, epoch: 21 | loss: 0.3279520
	speed: 0.0569s/iter; left time: 434.0703s
Epoch: 21 cost time: 16.139768600463867
Epoch: 21, Steps: 261 | Train Loss: 0.3756940 Vali Loss: 0.1938768 Test Loss: 0.2683612
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2693209946155548, mae:0.3268699049949646, rse:0.419175386428833, corr:[0.55486035 0.55735844 0.55739003 0.55586636 0.55420023 0.5531793
 0.552936   0.55321586 0.5536414  0.5538513  0.55365825 0.55313104
 0.55250967 0.5519804  0.55169743 0.55157757 0.5514988  0.55131185
 0.5508649  0.550124   0.54920524 0.5482394  0.5473873  0.54676265
 0.54634035 0.5460412  0.54576105 0.54540116 0.54491156 0.54431385
 0.5436499  0.5430534  0.5425491  0.54208374 0.5416265  0.5411771
 0.5406738  0.54012    0.53950065 0.5388433  0.53818566 0.5375403
 0.53693604 0.5363736  0.5358453  0.53530306 0.5347254  0.5341031
 0.5333713  0.53251284 0.531576   0.53068936 0.5299186  0.52930814
 0.5288572  0.5285627  0.5283177  0.528035   0.5276861  0.5272779
 0.5268305  0.52643365 0.5261318  0.5259173  0.5257661  0.52566457
 0.5255179  0.5253136  0.5250454  0.5247083  0.5243     0.5238673
 0.5234497  0.5231103  0.5228315  0.5226113  0.52238715 0.52209973
 0.5217016  0.5212233  0.52061176 0.5199207  0.51918423 0.5184595
 0.51781464 0.51728135 0.5168181  0.51642114 0.51606244 0.5157133
 0.51535505 0.51494783 0.5143776  0.5135874  0.51255745 0.51127714
 0.5098391  0.5084144  0.5070523  0.5057789  0.504622   0.5035713
 0.5026178  0.5016921  0.50077057 0.49980438 0.49883842 0.4979291
 0.49706215 0.49621513 0.49537596 0.4945243  0.4937168  0.49292064
 0.49208355 0.4912379  0.49044192 0.48967737 0.48898658 0.48829153
 0.48761505 0.4868787  0.486094   0.4852167  0.48420388 0.48306736
 0.48191896 0.48085105 0.47992373 0.47916633 0.47855744 0.47800165
 0.47743136 0.47679132 0.47605857 0.4752206  0.4742953  0.47338995
 0.4725728  0.47193664 0.4714652  0.47111735 0.47082478 0.47042754
 0.4698556  0.4690395  0.46815318 0.46730036 0.46659538 0.46602884
 0.4655955  0.46524653 0.4648129  0.46417108 0.46331656 0.46238673
 0.46150547 0.46078712 0.46025538 0.4599585  0.45976683 0.45955902
 0.45920336 0.45867133 0.457995   0.45729518 0.4566476  0.45616516
 0.45595893 0.45608062 0.45634833 0.456605   0.4567061  0.45657024
 0.45619664 0.45564228 0.45499122 0.4543299  0.453672   0.45312506
 0.45261413 0.45206606 0.45149845 0.45089716 0.45023763 0.44953558
 0.44884926 0.44824466 0.44769073 0.44707406 0.44627893 0.44520968
 0.44388857 0.44248807 0.4409893  0.43939403 0.43783545 0.43637523
 0.43510404 0.4340044  0.43302798 0.4320484  0.43097386 0.42977205
 0.42849725 0.4272286  0.42601946 0.42495075 0.42415535 0.4236964
 0.42334276 0.42289925 0.4222516  0.42137927 0.42039013 0.41931328
 0.4182122  0.4171026  0.41607684 0.41513065 0.41416577 0.41315925
 0.412091   0.41099048 0.4099427  0.40885958 0.40775934 0.40671566
 0.40567264 0.40474096 0.40389472 0.40318838 0.40257773 0.4020605
 0.4015867  0.40119922 0.40085807 0.40055767 0.40028185 0.39994225
 0.39949504 0.3988318  0.3980396  0.39738593 0.39692596 0.39679924
 0.39703307 0.39754525 0.39809662 0.39844847 0.39857364 0.3984249
 0.39802042 0.3975223  0.39694774 0.3964145  0.39594474 0.3955814
 0.39531058 0.39506736 0.3947987  0.39450407 0.39417702 0.39390448
 0.39369515 0.39348903 0.39331073 0.393138   0.39290687 0.3925982
 0.39226696 0.39206305 0.3919921  0.3920807  0.39221838 0.39227667
 0.39223713 0.3919475  0.39139345 0.39075673 0.3901897  0.38987827
 0.38984495 0.3900246  0.39023322 0.39022356 0.38979316 0.3887373
 0.38715065 0.3854392  0.38387492 0.38268036 0.38201046 0.38179538
 0.38196623 0.38225117 0.38236162 0.38212252 0.38159564 0.38074827
 0.37986413 0.3790797  0.3785728  0.37822223 0.3779785  0.37776554
 0.37756926 0.377239   0.376848   0.37653354 0.3763867  0.37662068
 0.377028   0.37749407 0.37775898 0.37761867 0.3770865  0.37636116
 0.37564594 0.37525913 0.3753641  0.37584755 0.37635857 0.37664062
 0.37645453 0.37581638 0.37497064 0.3741645  0.37380442 0.3739458
 0.3745368  0.375031   0.37479755 0.37329447 0.37012187 0.36544734]
