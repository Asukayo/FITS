Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  38915072.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5316055
	speed: 0.1489s/iter; left time: 952.9179s
Epoch: 1 cost time: 19.50283455848694
Epoch: 1, Steps: 130 | Train Loss: 0.4943673 Vali Loss: 0.2235186 Test Loss: 0.3042572
Validation loss decreased (inf --> 0.223519).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4209992
	speed: 0.4353s/iter; left time: 2730.0584s
Epoch: 2 cost time: 20.120225191116333
Epoch: 2, Steps: 130 | Train Loss: 0.4197620 Vali Loss: 0.2092738 Test Loss: 0.2878408
Validation loss decreased (0.223519 --> 0.209274).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3137162
	speed: 0.3714s/iter; left time: 2280.7990s
Epoch: 3 cost time: 16.383362531661987
Epoch: 3, Steps: 130 | Train Loss: 0.4036062 Vali Loss: 0.2036942 Test Loss: 0.2816727
Validation loss decreased (0.209274 --> 0.203694).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2303488
	speed: 0.4220s/iter; left time: 2536.7873s
Epoch: 4 cost time: 21.835673570632935
Epoch: 4, Steps: 130 | Train Loss: 0.3971576 Vali Loss: 0.2010520 Test Loss: 0.2780078
Validation loss decreased (0.203694 --> 0.201052).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3246649
	speed: 0.4383s/iter; left time: 2577.4519s
Epoch: 5 cost time: 21.142237186431885
Epoch: 5, Steps: 130 | Train Loss: 0.3916715 Vali Loss: 0.1991862 Test Loss: 0.2760102
Validation loss decreased (0.201052 --> 0.199186).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4011585
	speed: 0.4419s/iter; left time: 2541.5966s
Epoch: 6 cost time: 20.586451530456543
Epoch: 6, Steps: 130 | Train Loss: 0.3888556 Vali Loss: 0.1980106 Test Loss: 0.2744418
Validation loss decreased (0.199186 --> 0.198011).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3928096
	speed: 0.3174s/iter; left time: 1784.2558s
Epoch: 7 cost time: 14.25942611694336
Epoch: 7, Steps: 130 | Train Loss: 0.3859073 Vali Loss: 0.1968034 Test Loss: 0.2734209
Validation loss decreased (0.198011 --> 0.196803).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3543973
	speed: 0.3708s/iter; left time: 2036.2833s
Epoch: 8 cost time: 19.75431776046753
Epoch: 8, Steps: 130 | Train Loss: 0.3844518 Vali Loss: 0.1964486 Test Loss: 0.2724471
Validation loss decreased (0.196803 --> 0.196449).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3482282
	speed: 0.3742s/iter; left time: 2006.3354s
Epoch: 9 cost time: 16.151227951049805
Epoch: 9, Steps: 130 | Train Loss: 0.3834333 Vali Loss: 0.1957549 Test Loss: 0.2717725
Validation loss decreased (0.196449 --> 0.195755).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3391529
	speed: 0.3074s/iter; left time: 1607.9882s
Epoch: 10 cost time: 16.353646516799927
Epoch: 10, Steps: 130 | Train Loss: 0.3821863 Vali Loss: 0.1954704 Test Loss: 0.2713313
Validation loss decreased (0.195755 --> 0.195470).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3552944
	speed: 0.3953s/iter; left time: 2016.6727s
Epoch: 11 cost time: 18.52715802192688
Epoch: 11, Steps: 130 | Train Loss: 0.3801558 Vali Loss: 0.1950224 Test Loss: 0.2709850
Validation loss decreased (0.195470 --> 0.195022).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2932927
	speed: 0.3839s/iter; left time: 1908.2009s
Epoch: 12 cost time: 19.412018060684204
Epoch: 12, Steps: 130 | Train Loss: 0.3803082 Vali Loss: 0.1944557 Test Loss: 0.2704057
Validation loss decreased (0.195022 --> 0.194456).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4109770
	speed: 0.4001s/iter; left time: 1936.6459s
Epoch: 13 cost time: 18.870936155319214
Epoch: 13, Steps: 130 | Train Loss: 0.3801452 Vali Loss: 0.1946762 Test Loss: 0.2703262
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5040134
	speed: 0.3525s/iter; left time: 1660.4950s
Epoch: 14 cost time: 15.385955333709717
Epoch: 14, Steps: 130 | Train Loss: 0.3795767 Vali Loss: 0.1943009 Test Loss: 0.2699782
Validation loss decreased (0.194456 --> 0.194301).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5190887
	speed: 0.3462s/iter; left time: 1585.9261s
Epoch: 15 cost time: 18.548421621322632
Epoch: 15, Steps: 130 | Train Loss: 0.3782106 Vali Loss: 0.1941867 Test Loss: 0.2699509
Validation loss decreased (0.194301 --> 0.194187).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3181386
	speed: 0.3830s/iter; left time: 1704.7838s
Epoch: 16 cost time: 18.6852810382843
Epoch: 16, Steps: 130 | Train Loss: 0.3787981 Vali Loss: 0.1940540 Test Loss: 0.2696758
Validation loss decreased (0.194187 --> 0.194054).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3440093
	speed: 0.4075s/iter; left time: 1760.6508s
Epoch: 17 cost time: 19.702494382858276
Epoch: 17, Steps: 130 | Train Loss: 0.3783846 Vali Loss: 0.1938214 Test Loss: 0.2694940
Validation loss decreased (0.194054 --> 0.193821).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3384835
	speed: 0.3938s/iter; left time: 1650.5464s
Epoch: 18 cost time: 18.665260076522827
Epoch: 18, Steps: 130 | Train Loss: 0.3766395 Vali Loss: 0.1935410 Test Loss: 0.2693864
Validation loss decreased (0.193821 --> 0.193541).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4194931
	speed: 0.4007s/iter; left time: 1627.2731s
Epoch: 19 cost time: 19.619770526885986
Epoch: 19, Steps: 130 | Train Loss: 0.3774667 Vali Loss: 0.1936585 Test Loss: 0.2692031
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3342846
	speed: 0.3987s/iter; left time: 1567.1918s
Epoch: 20 cost time: 18.67265796661377
Epoch: 20, Steps: 130 | Train Loss: 0.3777562 Vali Loss: 0.1933700 Test Loss: 0.2690990
Validation loss decreased (0.193541 --> 0.193370).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5084584
	speed: 0.3704s/iter; left time: 1407.7160s
Epoch: 21 cost time: 16.181950569152832
Epoch: 21, Steps: 130 | Train Loss: 0.3773584 Vali Loss: 0.1933588 Test Loss: 0.2690941
Validation loss decreased (0.193370 --> 0.193359).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3541601
	speed: 0.3204s/iter; left time: 1176.2936s
Epoch: 22 cost time: 15.798598766326904
Epoch: 22, Steps: 130 | Train Loss: 0.3760614 Vali Loss: 0.1933393 Test Loss: 0.2689236
Validation loss decreased (0.193359 --> 0.193339).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3635907
	speed: 0.3478s/iter; left time: 1231.7220s
Epoch: 23 cost time: 19.758544921875
Epoch: 23, Steps: 130 | Train Loss: 0.3761880 Vali Loss: 0.1933611 Test Loss: 0.2689323
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4126250
	speed: 0.4340s/iter; left time: 1480.3613s
Epoch: 24 cost time: 19.888218879699707
Epoch: 24, Steps: 130 | Train Loss: 0.3760438 Vali Loss: 0.1932032 Test Loss: 0.2688552
Validation loss decreased (0.193339 --> 0.193203).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3643016
	speed: 0.4100s/iter; left time: 1345.1608s
Epoch: 25 cost time: 21.00411343574524
Epoch: 25, Steps: 130 | Train Loss: 0.3766915 Vali Loss: 0.1931764 Test Loss: 0.2687292
Validation loss decreased (0.193203 --> 0.193176).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4066836
	speed: 0.4359s/iter; left time: 1373.5905s
Epoch: 26 cost time: 20.217532873153687
Epoch: 26, Steps: 130 | Train Loss: 0.3763162 Vali Loss: 0.1931892 Test Loss: 0.2687584
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4259336
	speed: 0.4286s/iter; left time: 1294.6702s
Epoch: 27 cost time: 20.776681184768677
Epoch: 27, Steps: 130 | Train Loss: 0.3755509 Vali Loss: 0.1932272 Test Loss: 0.2686593
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3533345
	speed: 0.3665s/iter; left time: 1059.6629s
Epoch: 28 cost time: 13.968368768692017
Epoch: 28, Steps: 130 | Train Loss: 0.3768983 Vali Loss: 0.1930946 Test Loss: 0.2686775
Validation loss decreased (0.193176 --> 0.193095).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3432932
	speed: 0.3686s/iter; left time: 1017.7360s
Epoch: 29 cost time: 19.481922149658203
Epoch: 29, Steps: 130 | Train Loss: 0.3755546 Vali Loss: 0.1930190 Test Loss: 0.2686874
Validation loss decreased (0.193095 --> 0.193019).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4978657
	speed: 0.3341s/iter; left time: 878.9992s
Epoch: 30 cost time: 11.850016593933105
Epoch: 30, Steps: 130 | Train Loss: 0.3759724 Vali Loss: 0.1931989 Test Loss: 0.2685987
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3327352
	speed: 0.3814s/iter; left time: 953.9414s
Epoch: 31 cost time: 20.712700366973877
Epoch: 31, Steps: 130 | Train Loss: 0.3752671 Vali Loss: 0.1927782 Test Loss: 0.2685189
Validation loss decreased (0.193019 --> 0.192778).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3538546
	speed: 0.4218s/iter; left time: 999.9744s
Epoch: 32 cost time: 18.873095750808716
Epoch: 32, Steps: 130 | Train Loss: 0.3757130 Vali Loss: 0.1928600 Test Loss: 0.2685633
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3097426
	speed: 0.3962s/iter; left time: 887.9288s
Epoch: 33 cost time: 20.04212474822998
Epoch: 33, Steps: 130 | Train Loss: 0.3752629 Vali Loss: 0.1929621 Test Loss: 0.2684324
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2742245
	speed: 0.4125s/iter; left time: 870.8823s
Epoch: 34 cost time: 19.13172435760498
Epoch: 34, Steps: 130 | Train Loss: 0.3752010 Vali Loss: 0.1929238 Test Loss: 0.2684006
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.26920321583747864, mae:0.3265690505504608, rse:0.41908368468284607, corr:[0.54969096 0.55708104 0.5523541  0.55032784 0.55212384 0.55424947
 0.5539129  0.5523525  0.5517344  0.5525204  0.553621   0.5537121
 0.5528786  0.55217403 0.55225444 0.55279356 0.55293775 0.55224264
 0.5511491  0.5503574  0.55012167 0.55008787 0.54973227 0.5489787
 0.54819816 0.5477466  0.54760563 0.5474075  0.54681754 0.5459381
 0.5451383  0.54467845 0.54444593 0.54413515 0.54353875 0.54275244
 0.5419817  0.5413679  0.5408749  0.54038346 0.5398057  0.53916675
 0.53857845 0.53809047 0.53766423 0.53716403 0.5364877  0.5356523
 0.5347552  0.5339617  0.5333837  0.5329575  0.53246784 0.5318302
 0.5311229  0.53053325 0.53011835 0.529798   0.5294689  0.52909535
 0.5287493  0.52854836 0.52846146 0.5283333  0.5280673  0.5276859
 0.5272773  0.52699757 0.5268398  0.52663666 0.5262625  0.5257554
 0.5252505  0.5248839  0.52459294 0.52421945 0.52364266 0.522924
 0.5222558  0.5218017  0.5214361  0.5209746  0.52030474 0.5195281
 0.5188848  0.5184804  0.518176   0.5177741  0.5171887  0.5165258
 0.5159982  0.5156362  0.5152225  0.5144961  0.51333374 0.51184285
 0.51036775 0.50916904 0.5081019  0.5069183  0.50552666 0.5040905
 0.5029223  0.5021103  0.50142765 0.50051916 0.499322   0.49804544
 0.49700236 0.4963408  0.49587408 0.49525696 0.4943615  0.4932741
 0.4922426  0.49143404 0.4907363  0.48990405 0.48892963 0.48797458
 0.48728138 0.486755   0.48615503 0.48523322 0.48404014 0.4829208
 0.48218608 0.48168325 0.4809798  0.479847   0.47849596 0.47740954
 0.47690707 0.4767463  0.47639102 0.47552726 0.47434393 0.47342154
 0.47306782 0.4730166  0.47264278 0.4716228  0.4701989  0.46898124
 0.46845484 0.46836656 0.46816093 0.46738565 0.46617517 0.46507993
 0.46455526 0.46437255 0.46385106 0.4627171  0.46137604 0.46060514
 0.4607127  0.46119902 0.46124026 0.46049476 0.45922446 0.4581918
 0.45791888 0.45822117 0.4584197  0.45808786 0.45734808 0.45674053
 0.45663384 0.4568134  0.45665804 0.4559316  0.4549828  0.45441273
 0.45443365 0.45461905 0.45436084 0.45346168 0.45228788 0.45152396
 0.45131114 0.45124274 0.45088804 0.45015094 0.44938633 0.4490206
 0.44906452 0.44901118 0.4482868  0.44678798 0.4450019  0.44353
 0.44254783 0.44170478 0.44043693 0.43873632 0.43719885 0.43625408
 0.435763   0.4351133  0.4339031  0.4322868  0.4308657  0.4300424
 0.4295844  0.4288589  0.42749828 0.42588016 0.4248109  0.42460704
 0.42459652 0.42397764 0.42248985 0.42063743 0.4193172  0.41882575
 0.41868132 0.41811746 0.41696373 0.4155659  0.41441566 0.41364384
 0.41278717 0.4113669  0.40948862 0.40772375 0.40684164 0.40692362
 0.40711546 0.40671146 0.4055747  0.40432215 0.40355211 0.40330696
 0.40299678 0.4021828  0.40095457 0.39998865 0.39981297 0.40012965
 0.40016505 0.39933878 0.3980318  0.39723453 0.3972933  0.3978055
 0.3979178  0.39719555 0.39601946 0.3953662  0.39587626 0.39689967
 0.3973414  0.39678293 0.39562038 0.3949822  0.39538604 0.39628386
 0.396583   0.39580038 0.39450964 0.39381438 0.39416164 0.39496893
 0.39518863 0.3943703  0.39324462 0.3928427  0.39338946 0.39406788
 0.3939448  0.39289862 0.39160576 0.3910953  0.39146614 0.39192402
 0.39177394 0.39083943 0.3898471  0.38963187 0.3899945  0.3901114
 0.38934866 0.3880759  0.3873654  0.3877906  0.38865915 0.3885606
 0.3869578  0.3847346  0.38318172 0.38291353 0.38322386 0.38283935
 0.3814701  0.37983432 0.37904248 0.37932825 0.37975556 0.37901822
 0.37728035 0.37575552 0.3758514  0.3771267  0.37802196 0.37735385
 0.3755417  0.3740492  0.37425905 0.37568086 0.37650764 0.3758741
 0.37427977 0.37370688 0.3749998  0.3768146  0.37724316 0.37569612
 0.37348548 0.373099   0.37505975 0.377211   0.3772854  0.3752761
 0.37355384 0.37461948 0.37761128 0.37930042 0.37817097 0.37555522
 0.3757284  0.37924996 0.3818497  0.37945056 0.371263   0.3686329 ]
