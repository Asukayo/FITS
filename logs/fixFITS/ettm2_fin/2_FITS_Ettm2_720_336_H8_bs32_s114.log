Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=74, out_features=108, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3580416.0
params:  8100.0
Trainable parameters:  8100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3765506
	speed: 0.0907s/iter; left time: 2362.7592s
	iters: 200, epoch: 1 | loss: 0.2000820
	speed: 0.0842s/iter; left time: 2185.8709s
	iters: 300, epoch: 1 | loss: 0.2019818
	speed: 0.0945s/iter; left time: 2443.5925s
	iters: 400, epoch: 1 | loss: 0.1827962
	speed: 0.0934s/iter; left time: 2406.3879s
	iters: 500, epoch: 1 | loss: 0.2262627
	speed: 0.0919s/iter; left time: 2358.1384s
Epoch: 1 cost time: 47.55370497703552
Epoch: 1, Steps: 523 | Train Loss: 0.2857178 Vali Loss: 0.2200289 Test Loss: 0.2986740
Validation loss decreased (inf --> 0.220029).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1665621
	speed: 0.5560s/iter; left time: 14193.4534s
	iters: 200, epoch: 2 | loss: 0.1578455
	speed: 0.0837s/iter; left time: 2129.0327s
	iters: 300, epoch: 2 | loss: 0.1598747
	speed: 0.0886s/iter; left time: 2243.9009s
	iters: 400, epoch: 2 | loss: 0.2045857
	speed: 0.1006s/iter; left time: 2536.8181s
	iters: 500, epoch: 2 | loss: 0.1397732
	speed: 0.0984s/iter; left time: 2472.5422s
Epoch: 2 cost time: 49.679436445236206
Epoch: 2, Steps: 523 | Train Loss: 0.1732720 Vali Loss: 0.2077406 Test Loss: 0.2837963
Validation loss decreased (0.220029 --> 0.207741).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1894724
	speed: 0.6444s/iter; left time: 16112.9654s
	iters: 200, epoch: 3 | loss: 0.1431480
	speed: 0.0464s/iter; left time: 1156.3089s
	iters: 300, epoch: 3 | loss: 0.0939224
	speed: 0.0581s/iter; left time: 1441.5685s
	iters: 400, epoch: 3 | loss: 0.1037430
	speed: 0.0525s/iter; left time: 1297.5924s
	iters: 500, epoch: 3 | loss: 0.1388731
	speed: 0.0944s/iter; left time: 2322.7388s
Epoch: 3 cost time: 35.92231559753418
Epoch: 3, Steps: 523 | Train Loss: 0.1466197 Vali Loss: 0.2020351 Test Loss: 0.2771718
Validation loss decreased (0.207741 --> 0.202035).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1167815
	speed: 0.6019s/iter; left time: 14736.5588s
	iters: 200, epoch: 4 | loss: 0.0996455
	speed: 0.0926s/iter; left time: 2257.7494s
	iters: 300, epoch: 4 | loss: 0.0877198
	speed: 0.0929s/iter; left time: 2254.6928s
	iters: 400, epoch: 4 | loss: 0.1163221
	speed: 0.0951s/iter; left time: 2300.0378s
	iters: 500, epoch: 4 | loss: 0.1127714
	speed: 0.0951s/iter; left time: 2290.3765s
Epoch: 4 cost time: 48.7278196811676
Epoch: 4, Steps: 523 | Train Loss: 0.1384712 Vali Loss: 0.1995636 Test Loss: 0.2750266
Validation loss decreased (0.202035 --> 0.199564).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1079795
	speed: 0.6361s/iter; left time: 15239.2496s
	iters: 200, epoch: 5 | loss: 0.1397399
	speed: 0.0949s/iter; left time: 2264.8197s
	iters: 300, epoch: 5 | loss: 0.1122716
	speed: 0.0852s/iter; left time: 2024.7029s
	iters: 400, epoch: 5 | loss: 0.1513311
	speed: 0.0839s/iter; left time: 1984.4932s
	iters: 500, epoch: 5 | loss: 0.1189773
	speed: 0.0768s/iter; left time: 1809.7379s
Epoch: 5 cost time: 47.15296745300293
Epoch: 5, Steps: 523 | Train Loss: 0.1361015 Vali Loss: 0.1982197 Test Loss: 0.2743105
Validation loss decreased (0.199564 --> 0.198220).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1687002
	speed: 0.5610s/iter; left time: 13147.2036s
	iters: 200, epoch: 6 | loss: 0.1058873
	speed: 0.0861s/iter; left time: 2008.3170s
	iters: 300, epoch: 6 | loss: 0.1911550
	speed: 0.0828s/iter; left time: 1924.8020s
	iters: 400, epoch: 6 | loss: 0.0937722
	speed: 0.0887s/iter; left time: 2052.0885s
	iters: 500, epoch: 6 | loss: 0.1109556
	speed: 0.0767s/iter; left time: 1765.7253s
Epoch: 6 cost time: 44.04295349121094
Epoch: 6, Steps: 523 | Train Loss: 0.1353402 Vali Loss: 0.1980816 Test Loss: 0.2742381
Validation loss decreased (0.198220 --> 0.198082).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1698851
	speed: 0.5338s/iter; left time: 12230.4051s
	iters: 200, epoch: 7 | loss: 0.1318401
	speed: 0.0881s/iter; left time: 2010.4060s
	iters: 300, epoch: 7 | loss: 0.1323470
	speed: 0.0846s/iter; left time: 1922.0671s
	iters: 400, epoch: 7 | loss: 0.1907640
	speed: 0.0855s/iter; left time: 1933.1615s
	iters: 500, epoch: 7 | loss: 0.0791247
	speed: 0.0726s/iter; left time: 1634.0434s
Epoch: 7 cost time: 44.42056393623352
Epoch: 7, Steps: 523 | Train Loss: 0.1351180 Vali Loss: 0.1978037 Test Loss: 0.2740663
Validation loss decreased (0.198082 --> 0.197804).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1085439
	speed: 0.5772s/iter; left time: 12923.4430s
	iters: 200, epoch: 8 | loss: 0.1066671
	speed: 0.0816s/iter; left time: 1819.5314s
	iters: 300, epoch: 8 | loss: 0.1140350
	speed: 0.0895s/iter; left time: 1986.0779s
	iters: 400, epoch: 8 | loss: 0.2132439
	speed: 0.0834s/iter; left time: 1842.3634s
	iters: 500, epoch: 8 | loss: 0.1778047
	speed: 0.0886s/iter; left time: 1947.2258s
Epoch: 8 cost time: 45.03503727912903
Epoch: 8, Steps: 523 | Train Loss: 0.1350569 Vali Loss: 0.1979564 Test Loss: 0.2743118
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.0806343
	speed: 0.5417s/iter; left time: 11844.5402s
	iters: 200, epoch: 9 | loss: 0.0954706
	speed: 0.0839s/iter; left time: 1826.3417s
	iters: 300, epoch: 9 | loss: 0.1091422
	speed: 0.0790s/iter; left time: 1711.4282s
	iters: 400, epoch: 9 | loss: 0.1349843
	speed: 0.0786s/iter; left time: 1695.7543s
	iters: 500, epoch: 9 | loss: 0.1782560
	speed: 0.0858s/iter; left time: 1842.9317s
Epoch: 9 cost time: 44.31431770324707
Epoch: 9, Steps: 523 | Train Loss: 0.1349773 Vali Loss: 0.1979677 Test Loss: 0.2742377
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.0966742
	speed: 0.5472s/iter; left time: 11679.1721s
	iters: 200, epoch: 10 | loss: 0.1292354
	speed: 0.0879s/iter; left time: 1867.3366s
	iters: 300, epoch: 10 | loss: 0.1954267
	speed: 0.0943s/iter; left time: 1994.4461s
	iters: 400, epoch: 10 | loss: 0.0967111
	speed: 0.0971s/iter; left time: 2042.7792s
	iters: 500, epoch: 10 | loss: 0.1743115
	speed: 0.0939s/iter; left time: 1966.1573s
Epoch: 10 cost time: 48.15461754798889
Epoch: 10, Steps: 523 | Train Loss: 0.1348397 Vali Loss: 0.1980227 Test Loss: 0.2741126
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=74, out_features=108, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3580416.0
params:  8100.0
Trainable parameters:  8100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2979405
	speed: 0.0946s/iter; left time: 2463.5412s
	iters: 200, epoch: 1 | loss: 0.5222235
	speed: 0.0830s/iter; left time: 2154.9150s
	iters: 300, epoch: 1 | loss: 0.2858081
	speed: 0.0804s/iter; left time: 2079.6895s
	iters: 400, epoch: 1 | loss: 0.3936597
	speed: 0.0824s/iter; left time: 2121.8926s
	iters: 500, epoch: 1 | loss: 0.4093473
	speed: 0.0842s/iter; left time: 2159.8873s
Epoch: 1 cost time: 44.45653176307678
Epoch: 1, Steps: 523 | Train Loss: 0.3819439 Vali Loss: 0.1949197 Test Loss: 0.2708206
Validation loss decreased (inf --> 0.194920).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2961236
	speed: 0.4737s/iter; left time: 12091.7091s
	iters: 200, epoch: 2 | loss: 0.3898509
	speed: 0.0789s/iter; left time: 2006.8387s
	iters: 300, epoch: 2 | loss: 0.5955183
	speed: 0.0763s/iter; left time: 1931.3709s
	iters: 400, epoch: 2 | loss: 0.3999926
	speed: 0.0828s/iter; left time: 2089.7954s
	iters: 500, epoch: 2 | loss: 0.2894994
	speed: 0.0877s/iter; left time: 2203.2537s
Epoch: 2 cost time: 42.70043611526489
Epoch: 2, Steps: 523 | Train Loss: 0.3797603 Vali Loss: 0.1948348 Test Loss: 0.2705660
Validation loss decreased (0.194920 --> 0.194835).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2534804
	speed: 0.5581s/iter; left time: 13954.6367s
	iters: 200, epoch: 3 | loss: 0.4921487
	speed: 0.0847s/iter; left time: 2110.1457s
	iters: 300, epoch: 3 | loss: 0.3209999
	speed: 0.0874s/iter; left time: 2166.8532s
	iters: 400, epoch: 3 | loss: 0.4289271
	speed: 0.0844s/iter; left time: 2085.5096s
	iters: 500, epoch: 3 | loss: 0.2287794
	speed: 0.0778s/iter; left time: 1913.4749s
Epoch: 3 cost time: 43.933425188064575
Epoch: 3, Steps: 523 | Train Loss: 0.3784645 Vali Loss: 0.1942609 Test Loss: 0.2696526
Validation loss decreased (0.194835 --> 0.194261).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4070469
	speed: 0.5450s/iter; left time: 13342.0078s
	iters: 200, epoch: 4 | loss: 0.2036601
	speed: 0.0779s/iter; left time: 1900.5022s
	iters: 300, epoch: 4 | loss: 0.3721222
	speed: 0.0747s/iter; left time: 1813.6165s
	iters: 400, epoch: 4 | loss: 0.2235659
	speed: 0.0895s/iter; left time: 2163.1347s
	iters: 500, epoch: 4 | loss: 0.2566950
	speed: 0.0817s/iter; left time: 1966.6249s
Epoch: 4 cost time: 43.44821786880493
Epoch: 4, Steps: 523 | Train Loss: 0.3776414 Vali Loss: 0.1943726 Test Loss: 0.2700727
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4272572
	speed: 0.5473s/iter; left time: 13112.0628s
	iters: 200, epoch: 5 | loss: 0.5713089
	speed: 0.0844s/iter; left time: 2013.8146s
	iters: 300, epoch: 5 | loss: 0.5310646
	speed: 0.0788s/iter; left time: 1872.0946s
	iters: 400, epoch: 5 | loss: 0.6335114
	speed: 0.0879s/iter; left time: 2080.1247s
	iters: 500, epoch: 5 | loss: 0.2821888
	speed: 0.0852s/iter; left time: 2006.1359s
Epoch: 5 cost time: 45.14040684700012
Epoch: 5, Steps: 523 | Train Loss: 0.3769780 Vali Loss: 0.1946819 Test Loss: 0.2698624
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3141025
	speed: 0.5529s/iter; left time: 12956.9606s
	iters: 200, epoch: 6 | loss: 0.1998823
	speed: 0.0807s/iter; left time: 1883.7782s
	iters: 300, epoch: 6 | loss: 0.2259328
	speed: 0.0899s/iter; left time: 2089.7759s
	iters: 400, epoch: 6 | loss: 0.4020405
	speed: 0.0944s/iter; left time: 2183.8750s
	iters: 500, epoch: 6 | loss: 0.4676916
	speed: 0.0967s/iter; left time: 2226.6841s
Epoch: 6 cost time: 46.80004525184631
Epoch: 6, Steps: 523 | Train Loss: 0.3768915 Vali Loss: 0.1938712 Test Loss: 0.2691360
Validation loss decreased (0.194261 --> 0.193871).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3419148
	speed: 0.6150s/iter; left time: 14091.8657s
	iters: 200, epoch: 7 | loss: 0.2863887
	speed: 0.0925s/iter; left time: 2110.7434s
	iters: 300, epoch: 7 | loss: 0.2267800
	speed: 0.0816s/iter; left time: 1853.8548s
	iters: 400, epoch: 7 | loss: 0.2166287
	speed: 0.0872s/iter; left time: 1972.5387s
	iters: 500, epoch: 7 | loss: 0.2910268
	speed: 0.0852s/iter; left time: 1918.9641s
Epoch: 7 cost time: 46.8128707408905
Epoch: 7, Steps: 523 | Train Loss: 0.3769799 Vali Loss: 0.1937320 Test Loss: 0.2689182
Validation loss decreased (0.193871 --> 0.193732).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2690655
	speed: 0.5610s/iter; left time: 12561.8949s
	iters: 200, epoch: 8 | loss: 0.2626534
	speed: 0.0782s/iter; left time: 1742.4111s
	iters: 300, epoch: 8 | loss: 0.3936212
	speed: 0.0877s/iter; left time: 1946.0926s
	iters: 400, epoch: 8 | loss: 0.6063342
	speed: 0.0879s/iter; left time: 1941.1560s
	iters: 500, epoch: 8 | loss: 0.6966566
	speed: 0.0784s/iter; left time: 1723.7244s
Epoch: 8 cost time: 43.66235303878784
Epoch: 8, Steps: 523 | Train Loss: 0.3765133 Vali Loss: 0.1946689 Test Loss: 0.2701960
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3016385
	speed: 0.5740s/iter; left time: 12551.5809s
	iters: 200, epoch: 9 | loss: 0.2943009
	speed: 0.0870s/iter; left time: 1894.3145s
	iters: 300, epoch: 9 | loss: 0.4823841
	speed: 0.0840s/iter; left time: 1820.0639s
	iters: 400, epoch: 9 | loss: 0.3030362
	speed: 0.0855s/iter; left time: 1843.6918s
	iters: 500, epoch: 9 | loss: 0.2525732
	speed: 0.0871s/iter; left time: 1868.9282s
Epoch: 9 cost time: 45.93933629989624
Epoch: 9, Steps: 523 | Train Loss: 0.3765326 Vali Loss: 0.1941523 Test Loss: 0.2695385
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4255224
	speed: 0.5373s/iter; left time: 11468.4257s
	iters: 200, epoch: 10 | loss: 0.3224155
	speed: 0.0890s/iter; left time: 1891.3500s
	iters: 300, epoch: 10 | loss: 0.3619185
	speed: 0.0870s/iter; left time: 1838.6009s
	iters: 400, epoch: 10 | loss: 0.3906207
	speed: 0.0874s/iter; left time: 1838.2959s
	iters: 500, epoch: 10 | loss: 0.3672140
	speed: 0.0838s/iter; left time: 1754.1960s
Epoch: 10 cost time: 46.38240933418274
Epoch: 10, Steps: 523 | Train Loss: 0.3764147 Vali Loss: 0.1940850 Test Loss: 0.2693426
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2691143751144409, mae:0.3266279697418213, rse:0.4190145432949066, corr:[0.5528084  0.5565388  0.5577311  0.5566839  0.5549533  0.55365795
 0.5531591  0.553335   0.5538608  0.5543241  0.5543909  0.5540064
 0.55336076 0.55267876 0.55216634 0.55181974 0.5515806  0.551338
 0.55094486 0.5503126  0.5494772  0.5485351  0.54764634 0.54695994
 0.54649776 0.5462146  0.54601276 0.5457807  0.5454319  0.54494417
 0.54433244 0.54371375 0.5431296  0.5425714  0.54204446 0.5415642
 0.54107636 0.54055655 0.53996575 0.53930044 0.5385879  0.53785133
 0.53714025 0.5364902  0.53592354 0.53541964 0.5349552  0.5344871
 0.53391975 0.53320557 0.53236705 0.5315021  0.53066254 0.529899
 0.5292444  0.5287386  0.5283323  0.52797216 0.5276325  0.52730596
 0.5270017  0.5267886  0.5266921  0.5266856  0.5267282  0.52678
 0.52674454 0.52659976 0.52634305 0.52598333 0.5255442  0.525087
 0.5246613  0.52430695 0.52400136 0.52372354 0.5234131  0.52301174
 0.5224807  0.52184796 0.5210865  0.52027726 0.5194884  0.51879376
 0.518253   0.5178626  0.517539   0.51723045 0.5168759  0.5164311
 0.5158901  0.515241   0.5144251  0.5134363  0.51228505 0.51097536
 0.50958246 0.50824124 0.5069682  0.5057708  0.5046658  0.5036401
 0.5026801  0.5017136  0.5007149  0.49963123 0.49851134 0.49742144
 0.49638966 0.49544    0.49460697 0.49389353 0.49333522 0.49284056
 0.49228    0.4916022  0.49080813 0.48987406 0.4888747  0.48782504
 0.48684883 0.485969   0.4852412  0.48460382 0.48393705 0.483149
 0.4822419  0.48123553 0.48019314 0.47921023 0.4783791  0.4777253
 0.47724932 0.47688842 0.4765516  0.47611597 0.47549197 0.47470018
 0.4737891  0.47289255 0.47209004 0.47145012 0.47098073 0.47054693
 0.47003424 0.46927732 0.4683371  0.46724743 0.4661382  0.4651066
 0.46429747 0.46379435 0.46350127 0.46329466 0.463089   0.46288288
 0.46264854 0.46238333 0.462071   0.4617824  0.46147707 0.46112922
 0.4606788  0.4601099  0.45941332 0.45861644 0.45770225 0.45673215
 0.45586953 0.4553176  0.4550937  0.4551761  0.45543632 0.4557094
 0.45585775 0.45579687 0.45549223 0.4549683  0.4542519  0.4535263
 0.45285287 0.45226866 0.45182732 0.4514666  0.45106882 0.45053485
 0.44983774 0.44899717 0.44802147 0.44692096 0.4457406  0.44451985
 0.44332254 0.4422474  0.44112265 0.43978697 0.43824047 0.4364969
 0.43471086 0.43303397 0.4316403  0.43059385 0.42987007 0.42935112
 0.42888245 0.4283014  0.42745563 0.4263222  0.42504153 0.4238154
 0.42263705 0.42154315 0.42057085 0.41974235 0.4190903  0.41849232
 0.41785252 0.41707176 0.41619104 0.4152273  0.41416374 0.4130605
 0.41197285 0.41096005 0.41009983 0.40927795 0.40847906 0.40774578
 0.4070147  0.40636936 0.40574646 0.4051423  0.40446675 0.40368715
 0.4027675  0.4018175  0.40091792 0.4002178  0.39984575 0.39979485
 0.39999628 0.40021265 0.4003118  0.40030208 0.40005472 0.3996352
 0.39914724 0.39871585 0.39838096 0.39815888 0.39816943 0.3983693
 0.39863577 0.39888865 0.3988973  0.3985969  0.39795104 0.39707804
 0.39615986 0.39538237 0.39490718 0.39484355 0.39514965 0.39574394
 0.39641792 0.3969217  0.39715704 0.3970626  0.39663061 0.39594138
 0.3951697  0.3945579  0.394163   0.3940221  0.39399874 0.39393046
 0.39376655 0.3933346  0.39261925 0.3918094  0.39106587 0.39057642
 0.3903686  0.39038393 0.39046538 0.39039835 0.39001587 0.38913432
 0.38782707 0.3864196  0.3850619  0.3838471  0.38286522 0.38208857
 0.38159513 0.38131243 0.3811309  0.3809906  0.38097265 0.3809586
 0.3810491  0.38116637 0.38130313 0.38126025 0.38102213 0.38062873
 0.3802093  0.3797163  0.37924534 0.37887415 0.37859333 0.3785282
 0.37845722 0.37832695 0.37801486 0.37748542 0.37690067 0.37652293
 0.37648356 0.37690857 0.377728   0.3786665  0.3793435  0.37958914
 0.37931418 0.37868747 0.3780379  0.3775913  0.37765902 0.37818095
 0.37908232 0.37982672 0.37985107 0.37872702 0.3760589  0.37181023]
