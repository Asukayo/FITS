Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  58885120.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.541699886322021
Epoch: 1, Steps: 65 | Train Loss: 0.4228029 Vali Loss: 0.2847846 Test Loss: 0.3835154
Validation loss decreased (inf --> 0.284785).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.73844861984253
Epoch: 2, Steps: 65 | Train Loss: 0.3253048 Vali Loss: 0.2562891 Test Loss: 0.3443036
Validation loss decreased (0.284785 --> 0.256289).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.776162147521973
Epoch: 3, Steps: 65 | Train Loss: 0.2773811 Vali Loss: 0.2422070 Test Loss: 0.3251415
Validation loss decreased (0.256289 --> 0.242207).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 12.945247173309326
Epoch: 4, Steps: 65 | Train Loss: 0.2490123 Vali Loss: 0.2345744 Test Loss: 0.3142338
Validation loss decreased (0.242207 --> 0.234574).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 13.294303178787231
Epoch: 5, Steps: 65 | Train Loss: 0.2293788 Vali Loss: 0.2285155 Test Loss: 0.3072766
Validation loss decreased (0.234574 --> 0.228516).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 12.742782354354858
Epoch: 6, Steps: 65 | Train Loss: 0.2149816 Vali Loss: 0.2248027 Test Loss: 0.3026198
Validation loss decreased (0.228516 --> 0.224803).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.587623119354248
Epoch: 7, Steps: 65 | Train Loss: 0.2041564 Vali Loss: 0.2219626 Test Loss: 0.2990388
Validation loss decreased (0.224803 --> 0.221963).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.202460289001465
Epoch: 8, Steps: 65 | Train Loss: 0.1950075 Vali Loss: 0.2194989 Test Loss: 0.2962194
Validation loss decreased (0.221963 --> 0.219499).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.22390604019165
Epoch: 9, Steps: 65 | Train Loss: 0.1877754 Vali Loss: 0.2181635 Test Loss: 0.2938319
Validation loss decreased (0.219499 --> 0.218164).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.110973834991455
Epoch: 10, Steps: 65 | Train Loss: 0.1815794 Vali Loss: 0.2156487 Test Loss: 0.2918422
Validation loss decreased (0.218164 --> 0.215649).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.845476388931274
Epoch: 11, Steps: 65 | Train Loss: 0.1759449 Vali Loss: 0.2143259 Test Loss: 0.2901245
Validation loss decreased (0.215649 --> 0.214326).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.799850702285767
Epoch: 12, Steps: 65 | Train Loss: 0.1717040 Vali Loss: 0.2130084 Test Loss: 0.2886406
Validation loss decreased (0.214326 --> 0.213008).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.054943561553955
Epoch: 13, Steps: 65 | Train Loss: 0.1677383 Vali Loss: 0.2123741 Test Loss: 0.2872915
Validation loss decreased (0.213008 --> 0.212374).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.644679307937622
Epoch: 14, Steps: 65 | Train Loss: 0.1643967 Vali Loss: 0.2112061 Test Loss: 0.2861437
Validation loss decreased (0.212374 --> 0.211206).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.746212720870972
Epoch: 15, Steps: 65 | Train Loss: 0.1614194 Vali Loss: 0.2101851 Test Loss: 0.2850875
Validation loss decreased (0.211206 --> 0.210185).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.054846286773682
Epoch: 16, Steps: 65 | Train Loss: 0.1584019 Vali Loss: 0.2086222 Test Loss: 0.2841811
Validation loss decreased (0.210185 --> 0.208622).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.736822605133057
Epoch: 17, Steps: 65 | Train Loss: 0.1565761 Vali Loss: 0.2085482 Test Loss: 0.2833842
Validation loss decreased (0.208622 --> 0.208548).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.602999448776245
Epoch: 18, Steps: 65 | Train Loss: 0.1542343 Vali Loss: 0.2076032 Test Loss: 0.2826127
Validation loss decreased (0.208548 --> 0.207603).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.723362922668457
Epoch: 19, Steps: 65 | Train Loss: 0.1522174 Vali Loss: 0.2069131 Test Loss: 0.2819050
Validation loss decreased (0.207603 --> 0.206913).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 12.359164953231812
Epoch: 20, Steps: 65 | Train Loss: 0.1510244 Vali Loss: 0.2061590 Test Loss: 0.2813041
Validation loss decreased (0.206913 --> 0.206159).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 10.243191719055176
Epoch: 21, Steps: 65 | Train Loss: 0.1493916 Vali Loss: 0.2054740 Test Loss: 0.2807125
Validation loss decreased (0.206159 --> 0.205474).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.555235624313354
Epoch: 22, Steps: 65 | Train Loss: 0.1478874 Vali Loss: 0.2058083 Test Loss: 0.2801976
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.305752038955688
Epoch: 23, Steps: 65 | Train Loss: 0.1469287 Vali Loss: 0.2052363 Test Loss: 0.2797492
Validation loss decreased (0.205474 --> 0.205236).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 10.84082317352295
Epoch: 24, Steps: 65 | Train Loss: 0.1457854 Vali Loss: 0.2046402 Test Loss: 0.2793154
Validation loss decreased (0.205236 --> 0.204640).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.509201049804688
Epoch: 25, Steps: 65 | Train Loss: 0.1449204 Vali Loss: 0.2043359 Test Loss: 0.2789613
Validation loss decreased (0.204640 --> 0.204336).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 12.003460168838501
Epoch: 26, Steps: 65 | Train Loss: 0.1439996 Vali Loss: 0.2035588 Test Loss: 0.2785974
Validation loss decreased (0.204336 --> 0.203559).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 11.268506526947021
Epoch: 27, Steps: 65 | Train Loss: 0.1426677 Vali Loss: 0.2034494 Test Loss: 0.2782420
Validation loss decreased (0.203559 --> 0.203449).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 11.95368242263794
Epoch: 28, Steps: 65 | Train Loss: 0.1424713 Vali Loss: 0.2032989 Test Loss: 0.2780025
Validation loss decreased (0.203449 --> 0.203299).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 12.149731397628784
Epoch: 29, Steps: 65 | Train Loss: 0.1419336 Vali Loss: 0.2026443 Test Loss: 0.2777085
Validation loss decreased (0.203299 --> 0.202644).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 10.593222618103027
Epoch: 30, Steps: 65 | Train Loss: 0.1411390 Vali Loss: 0.2027577 Test Loss: 0.2774684
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 12.747223138809204
Epoch: 31, Steps: 65 | Train Loss: 0.1404075 Vali Loss: 0.2024827 Test Loss: 0.2772357
Validation loss decreased (0.202644 --> 0.202483).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 13.124603033065796
Epoch: 32, Steps: 65 | Train Loss: 0.1399649 Vali Loss: 0.2020500 Test Loss: 0.2770352
Validation loss decreased (0.202483 --> 0.202050).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 11.904284000396729
Epoch: 33, Steps: 65 | Train Loss: 0.1394369 Vali Loss: 0.2017870 Test Loss: 0.2768263
Validation loss decreased (0.202050 --> 0.201787).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 11.249187707901001
Epoch: 34, Steps: 65 | Train Loss: 0.1390630 Vali Loss: 0.2018282 Test Loss: 0.2766534
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 10.895794868469238
Epoch: 35, Steps: 65 | Train Loss: 0.1389220 Vali Loss: 0.2016097 Test Loss: 0.2764790
Validation loss decreased (0.201787 --> 0.201610).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 10.641581296920776
Epoch: 36, Steps: 65 | Train Loss: 0.1382461 Vali Loss: 0.2020379 Test Loss: 0.2763166
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 11.708493947982788
Epoch: 37, Steps: 65 | Train Loss: 0.1384382 Vali Loss: 0.2013742 Test Loss: 0.2761906
Validation loss decreased (0.201610 --> 0.201374).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 12.067170858383179
Epoch: 38, Steps: 65 | Train Loss: 0.1378536 Vali Loss: 0.2007833 Test Loss: 0.2760383
Validation loss decreased (0.201374 --> 0.200783).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 12.359666109085083
Epoch: 39, Steps: 65 | Train Loss: 0.1375452 Vali Loss: 0.2011828 Test Loss: 0.2759253
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 12.104584217071533
Epoch: 40, Steps: 65 | Train Loss: 0.1374058 Vali Loss: 0.2006296 Test Loss: 0.2758137
Validation loss decreased (0.200783 --> 0.200630).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 11.12378740310669
Epoch: 41, Steps: 65 | Train Loss: 0.1369153 Vali Loss: 0.2007344 Test Loss: 0.2756993
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 10.73668384552002
Epoch: 42, Steps: 65 | Train Loss: 0.1369199 Vali Loss: 0.2007271 Test Loss: 0.2756224
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 9.858889102935791
Epoch: 43, Steps: 65 | Train Loss: 0.1363602 Vali Loss: 0.2000608 Test Loss: 0.2755068
Validation loss decreased (0.200630 --> 0.200061).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 9.746266603469849
Epoch: 44, Steps: 65 | Train Loss: 0.1364108 Vali Loss: 0.2000507 Test Loss: 0.2754185
Validation loss decreased (0.200061 --> 0.200051).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 11.137925386428833
Epoch: 45, Steps: 65 | Train Loss: 0.1361736 Vali Loss: 0.2004922 Test Loss: 0.2753367
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 11.73401141166687
Epoch: 46, Steps: 65 | Train Loss: 0.1362357 Vali Loss: 0.2005393 Test Loss: 0.2752617
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 12.126410245895386
Epoch: 47, Steps: 65 | Train Loss: 0.1359048 Vali Loss: 0.1998979 Test Loss: 0.2752072
Validation loss decreased (0.200051 --> 0.199898).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 10.625643253326416
Epoch: 48, Steps: 65 | Train Loss: 0.1356965 Vali Loss: 0.1999914 Test Loss: 0.2751309
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 10.548311948776245
Epoch: 49, Steps: 65 | Train Loss: 0.1355006 Vali Loss: 0.2000313 Test Loss: 0.2750574
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 10.88949966430664
Epoch: 50, Steps: 65 | Train Loss: 0.1357307 Vali Loss: 0.2002731 Test Loss: 0.2750114
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  58885120.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.574366092681885
Epoch: 1, Steps: 65 | Train Loss: 0.3833765 Vali Loss: 0.1957404 Test Loss: 0.2714584
Validation loss decreased (inf --> 0.195740).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.42443060874939
Epoch: 2, Steps: 65 | Train Loss: 0.3793714 Vali Loss: 0.1944939 Test Loss: 0.2704645
Validation loss decreased (0.195740 --> 0.194494).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.556899309158325
Epoch: 3, Steps: 65 | Train Loss: 0.3784518 Vali Loss: 0.1940347 Test Loss: 0.2701590
Validation loss decreased (0.194494 --> 0.194035).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.74995470046997
Epoch: 4, Steps: 65 | Train Loss: 0.3774778 Vali Loss: 0.1935052 Test Loss: 0.2698624
Validation loss decreased (0.194035 --> 0.193505).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.761934518814087
Epoch: 5, Steps: 65 | Train Loss: 0.3760230 Vali Loss: 0.1935866 Test Loss: 0.2695971
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.287972211837769
Epoch: 6, Steps: 65 | Train Loss: 0.3765700 Vali Loss: 0.1934262 Test Loss: 0.2695937
Validation loss decreased (0.193505 --> 0.193426).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.892807245254517
Epoch: 7, Steps: 65 | Train Loss: 0.3768007 Vali Loss: 0.1934277 Test Loss: 0.2692947
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.675429821014404
Epoch: 8, Steps: 65 | Train Loss: 0.3763120 Vali Loss: 0.1928940 Test Loss: 0.2692623
Validation loss decreased (0.193426 --> 0.192894).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.551920175552368
Epoch: 9, Steps: 65 | Train Loss: 0.3756181 Vali Loss: 0.1922636 Test Loss: 0.2690619
Validation loss decreased (0.192894 --> 0.192264).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 12.074126720428467
Epoch: 10, Steps: 65 | Train Loss: 0.3754759 Vali Loss: 0.1930500 Test Loss: 0.2689591
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.388205766677856
Epoch: 11, Steps: 65 | Train Loss: 0.3753986 Vali Loss: 0.1924484 Test Loss: 0.2689890
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.017964839935303
Epoch: 12, Steps: 65 | Train Loss: 0.3750921 Vali Loss: 0.1931491 Test Loss: 0.2689151
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.26908737421035767, mae:0.3266439735889435, rse:0.4189935326576233, corr:[0.55370206 0.5597214  0.55979097 0.5571657  0.5557056  0.5560376
 0.55713475 0.55762637 0.55695736 0.55577725 0.55499715 0.5549625
 0.5553812  0.55561554 0.55525017 0.5543854  0.553468   0.55282384
 0.55245584 0.5521148  0.55155486 0.5507552  0.5499479  0.5493747
 0.5490629  0.5488584  0.5484972  0.54779    0.5468374  0.5459286
 0.5452736  0.54492164 0.5447025  0.5443653  0.54375345 0.54292226
 0.5420105  0.5411821  0.5405309  0.540066   0.5396738  0.53920084
 0.53859925 0.53789115 0.537131   0.5364003  0.5357642  0.5351905
 0.53456736 0.53387064 0.53315645 0.53248477 0.5318433  0.5312359
 0.53064066 0.5300446  0.529441   0.528897   0.52850556 0.528295
 0.5282023  0.52812266 0.52794164 0.52762073 0.52724046 0.5269228
 0.5267023  0.52655846 0.52641    0.52617985 0.5258455  0.5254606
 0.5250738  0.52468854 0.5242316  0.5236789  0.52303594 0.52237236
 0.5217767  0.52131444 0.5208906  0.5204199  0.51983666 0.51917213
 0.5185426  0.51805055 0.517712   0.517456   0.5171472  0.516698
 0.51610875 0.5154     0.514618   0.5137925  0.51285356 0.5116777
 0.51026285 0.50874513 0.50725377 0.50595665 0.50492185 0.50403625
 0.5031005  0.5019504  0.5006334  0.49932718 0.49828243 0.49757993
 0.49705413 0.49645212 0.49562964 0.49461234 0.49361795 0.49279246
 0.49214324 0.49157628 0.49092975 0.4900642  0.48903033 0.48793992
 0.48699054 0.48616892 0.48539755 0.48449245 0.48336524 0.4821415
 0.48108825 0.4803522  0.47985208 0.47936878 0.47870857 0.47780263
 0.47678274 0.47583947 0.47506756 0.47440913 0.47372112 0.47295097
 0.47212383 0.4713912  0.47084332 0.47047573 0.4701431  0.4696186
 0.46886098 0.46796358 0.46723667 0.46675906 0.46641827 0.4659341
 0.46513683 0.46405303 0.4628583  0.46191382 0.46149546 0.46157116
 0.4617734  0.46172354 0.4612318  0.46041924 0.45951495 0.45879668
 0.45836017 0.45815882 0.4580392  0.4578878  0.4576444  0.45735535
 0.45706913 0.45676172 0.45625982 0.455543   0.4547562  0.4541238
 0.4537918  0.45369437 0.4535911  0.4532561  0.45259503 0.45180747
 0.45106292 0.4505051  0.45017856 0.44990528 0.44947618 0.4488132
 0.44805026 0.4473828  0.4468812  0.44643292 0.44584262 0.44495767
 0.4438087  0.44256854 0.44121197 0.43969885 0.43806812 0.4363159
 0.43458584 0.43307808 0.4319921  0.43130392 0.4307804  0.43009272
 0.42904818 0.42768776 0.42627117 0.42520964 0.42477974 0.42481688
 0.4246963  0.4239829  0.42262444 0.4209493  0.41951114 0.41861635
 0.41822726 0.41795594 0.41751102 0.41672686 0.41565803 0.41458374
 0.41370323 0.41301784 0.41237712 0.41144592 0.41017565 0.4087887
 0.40746874 0.40644556 0.40564182 0.40493965 0.40420496 0.4034912
 0.40290838 0.40261665 0.40253276 0.40247104 0.40224242 0.40173113
 0.4010503  0.4003359  0.39979717 0.3995475  0.39928496 0.39884838
 0.3982194  0.39758778 0.3971494  0.39699057 0.39712504 0.39726633
 0.39714384 0.39678004 0.39627352 0.39596736 0.39598784 0.39627257
 0.3965206  0.39645448 0.39601687 0.39539695 0.39484286 0.39457035
 0.3945185  0.39446202 0.39437622 0.39428905 0.39425087 0.39426318
 0.39424318 0.39409572 0.393655   0.3930554  0.3925011  0.39221966
 0.39235586 0.392579   0.39253932 0.39215612 0.391524   0.3910024
 0.3908084  0.3909054  0.3909598  0.39054197 0.38943666 0.3876755
 0.38582575 0.3846466  0.3841985  0.38406432 0.3837854  0.38310137
 0.38228947 0.381638   0.38126054 0.38102224 0.38071924 0.37995625
 0.37892902 0.37792218 0.3773862  0.37724304 0.37728533 0.37719867
 0.37690434 0.37641338 0.37611458 0.37626165 0.37668753 0.37714276
 0.37704307 0.376448   0.37572438 0.3753818  0.37569454 0.37637767
 0.37666777 0.3761904  0.37509653 0.3740401  0.37374333 0.3745514
 0.37583992 0.37668175 0.3765957  0.37571648 0.37517786 0.37572044
 0.37734264 0.37864918 0.3782534  0.3758792  0.37299654 0.37158743]
