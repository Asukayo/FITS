Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=74, out_features=148, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9812992.0
params:  11100.0
Trainable parameters:  11100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4396072
	speed: 0.0793s/iter; left time: 1015.0342s
	iters: 200, epoch: 1 | loss: 0.2957340
	speed: 0.0604s/iter; left time: 767.3795s
Epoch: 1 cost time: 17.827330827713013
Epoch: 1, Steps: 258 | Train Loss: 0.4444199 Vali Loss: 0.2998880 Test Loss: 0.3977737
Validation loss decreased (inf --> 0.299888).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3525011
	speed: 0.2468s/iter; left time: 3095.4397s
	iters: 200, epoch: 2 | loss: 0.3449480
	speed: 0.0584s/iter; left time: 726.9285s
Epoch: 2 cost time: 16.74480891227722
Epoch: 2, Steps: 258 | Train Loss: 0.3275234 Vali Loss: 0.2835427 Test Loss: 0.3785647
Validation loss decreased (0.299888 --> 0.283543).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3144425
	speed: 0.2464s/iter; left time: 3026.5731s
	iters: 200, epoch: 3 | loss: 0.2994215
	speed: 0.0614s/iter; left time: 748.4735s
Epoch: 3 cost time: 15.538713693618774
Epoch: 3, Steps: 258 | Train Loss: 0.2959515 Vali Loss: 0.2764859 Test Loss: 0.3700947
Validation loss decreased (0.283543 --> 0.276486).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3300203
	speed: 0.2619s/iter; left time: 3150.4597s
	iters: 200, epoch: 4 | loss: 0.2252587
	speed: 0.0601s/iter; left time: 716.6075s
Epoch: 4 cost time: 16.27324891090393
Epoch: 4, Steps: 258 | Train Loss: 0.2798348 Vali Loss: 0.2728237 Test Loss: 0.3651125
Validation loss decreased (0.276486 --> 0.272824).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3631716
	speed: 0.2503s/iter; left time: 2945.4308s
	iters: 200, epoch: 5 | loss: 0.2989188
	speed: 0.0592s/iter; left time: 691.1338s
Epoch: 5 cost time: 16.02380108833313
Epoch: 5, Steps: 258 | Train Loss: 0.2710314 Vali Loss: 0.2703173 Test Loss: 0.3616149
Validation loss decreased (0.272824 --> 0.270317).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2666706
	speed: 0.2765s/iter; left time: 3182.9079s
	iters: 200, epoch: 6 | loss: 0.3026553
	speed: 0.0687s/iter; left time: 783.9656s
Epoch: 6 cost time: 19.0770263671875
Epoch: 6, Steps: 258 | Train Loss: 0.2664924 Vali Loss: 0.2681180 Test Loss: 0.3594380
Validation loss decreased (0.270317 --> 0.268118).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2042280
	speed: 0.3020s/iter; left time: 3398.4572s
	iters: 200, epoch: 7 | loss: 0.2738600
	speed: 0.0758s/iter; left time: 845.9149s
Epoch: 7 cost time: 19.861292600631714
Epoch: 7, Steps: 258 | Train Loss: 0.2636206 Vali Loss: 0.2671340 Test Loss: 0.3581095
Validation loss decreased (0.268118 --> 0.267134).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2380278
	speed: 0.3257s/iter; left time: 3581.0296s
	iters: 200, epoch: 8 | loss: 0.2426305
	speed: 0.0790s/iter; left time: 860.3830s
Epoch: 8 cost time: 21.05106019973755
Epoch: 8, Steps: 258 | Train Loss: 0.2616058 Vali Loss: 0.2661495 Test Loss: 0.3570120
Validation loss decreased (0.267134 --> 0.266149).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2385889
	speed: 0.2931s/iter; left time: 3147.4917s
	iters: 200, epoch: 9 | loss: 0.3166419
	speed: 0.0695s/iter; left time: 738.9257s
Epoch: 9 cost time: 18.399379014968872
Epoch: 9, Steps: 258 | Train Loss: 0.2612270 Vali Loss: 0.2657191 Test Loss: 0.3565175
Validation loss decreased (0.266149 --> 0.265719).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2362209
	speed: 0.2891s/iter; left time: 3029.3455s
	iters: 200, epoch: 10 | loss: 0.2599612
	speed: 0.0575s/iter; left time: 597.0154s
Epoch: 10 cost time: 16.87008500099182
Epoch: 10, Steps: 258 | Train Loss: 0.2607828 Vali Loss: 0.2649798 Test Loss: 0.3562025
Validation loss decreased (0.265719 --> 0.264980).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1868761
	speed: 0.2762s/iter; left time: 2822.6215s
	iters: 200, epoch: 11 | loss: 0.2889309
	speed: 0.0695s/iter; left time: 703.2567s
Epoch: 11 cost time: 19.164493083953857
Epoch: 11, Steps: 258 | Train Loss: 0.2601894 Vali Loss: 0.2654213 Test Loss: 0.3557214
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2427062
	speed: 0.2879s/iter; left time: 2868.7317s
	iters: 200, epoch: 12 | loss: 0.2191669
	speed: 0.0785s/iter; left time: 774.6836s
Epoch: 12 cost time: 19.041727781295776
Epoch: 12, Steps: 258 | Train Loss: 0.2599726 Vali Loss: 0.2648243 Test Loss: 0.3554571
Validation loss decreased (0.264980 --> 0.264824).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1941884
	speed: 0.2529s/iter; left time: 2454.4171s
	iters: 200, epoch: 13 | loss: 0.2258432
	speed: 0.0635s/iter; left time: 609.6703s
Epoch: 13 cost time: 16.987902879714966
Epoch: 13, Steps: 258 | Train Loss: 0.2599214 Vali Loss: 0.2648307 Test Loss: 0.3555287
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2385326
	speed: 0.2570s/iter; left time: 2428.0803s
	iters: 200, epoch: 14 | loss: 0.3113628
	speed: 0.0628s/iter; left time: 587.3191s
Epoch: 14 cost time: 16.660544872283936
Epoch: 14, Steps: 258 | Train Loss: 0.2598431 Vali Loss: 0.2644201 Test Loss: 0.3554824
Validation loss decreased (0.264824 --> 0.264420).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2487242
	speed: 0.2659s/iter; left time: 2443.4787s
	iters: 200, epoch: 15 | loss: 0.2906695
	speed: 0.0654s/iter; left time: 594.2786s
Epoch: 15 cost time: 16.944647073745728
Epoch: 15, Steps: 258 | Train Loss: 0.2595523 Vali Loss: 0.2644797 Test Loss: 0.3551488
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2366942
	speed: 0.2544s/iter; left time: 2272.4030s
	iters: 200, epoch: 16 | loss: 0.2290751
	speed: 0.0624s/iter; left time: 550.7663s
Epoch: 16 cost time: 16.596656560897827
Epoch: 16, Steps: 258 | Train Loss: 0.2597348 Vali Loss: 0.2644439 Test Loss: 0.3551053
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2338224
	speed: 0.2591s/iter; left time: 2247.3916s
	iters: 200, epoch: 17 | loss: 0.1831010
	speed: 0.0619s/iter; left time: 530.4688s
Epoch: 17 cost time: 16.53854203224182
Epoch: 17, Steps: 258 | Train Loss: 0.2594883 Vali Loss: 0.2644808 Test Loss: 0.3551452
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=74, out_features=148, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9812992.0
params:  11100.0
Trainable parameters:  11100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4092551
	speed: 0.0802s/iter; left time: 1026.9953s
	iters: 200, epoch: 1 | loss: 0.4943137
	speed: 0.0726s/iter; left time: 921.6483s
Epoch: 1 cost time: 19.053229331970215
Epoch: 1, Steps: 258 | Train Loss: 0.4997652 Vali Loss: 0.2633815 Test Loss: 0.3543475
Validation loss decreased (inf --> 0.263381).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5060931
	speed: 0.2756s/iter; left time: 3457.2164s
	iters: 200, epoch: 2 | loss: 0.6088409
	speed: 0.0633s/iter; left time: 787.6386s
Epoch: 2 cost time: 16.634042978286743
Epoch: 2, Steps: 258 | Train Loss: 0.4985140 Vali Loss: 0.2625126 Test Loss: 0.3538563
Validation loss decreased (0.263381 --> 0.262513).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3296104
	speed: 0.2715s/iter; left time: 3335.2882s
	iters: 200, epoch: 3 | loss: 0.4890495
	speed: 0.0613s/iter; left time: 747.4939s
Epoch: 3 cost time: 17.041025161743164
Epoch: 3, Steps: 258 | Train Loss: 0.4977993 Vali Loss: 0.2624598 Test Loss: 0.3535823
Validation loss decreased (0.262513 --> 0.262460).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4730299
	speed: 0.2781s/iter; left time: 3345.2590s
	iters: 200, epoch: 4 | loss: 0.6750495
	speed: 0.0684s/iter; left time: 816.3755s
Epoch: 4 cost time: 18.452714920043945
Epoch: 4, Steps: 258 | Train Loss: 0.4973924 Vali Loss: 0.2623104 Test Loss: 0.3534254
Validation loss decreased (0.262460 --> 0.262310).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4825628
	speed: 0.2840s/iter; left time: 3342.1517s
	iters: 200, epoch: 5 | loss: 0.4151550
	speed: 0.0374s/iter; left time: 436.3992s
Epoch: 5 cost time: 11.606996536254883
Epoch: 5, Steps: 258 | Train Loss: 0.4971950 Vali Loss: 0.2622760 Test Loss: 0.3533947
Validation loss decreased (0.262310 --> 0.262276).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.7529858
	speed: 0.2663s/iter; left time: 3065.2171s
	iters: 200, epoch: 6 | loss: 0.3826271
	speed: 0.0740s/iter; left time: 844.8436s
Epoch: 6 cost time: 19.287741899490356
Epoch: 6, Steps: 258 | Train Loss: 0.4966439 Vali Loss: 0.2620870 Test Loss: 0.3531390
Validation loss decreased (0.262276 --> 0.262087).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4181723
	speed: 0.2883s/iter; left time: 3244.6346s
	iters: 200, epoch: 7 | loss: 0.6518456
	speed: 0.0724s/iter; left time: 807.5273s
Epoch: 7 cost time: 18.565627098083496
Epoch: 7, Steps: 258 | Train Loss: 0.4959596 Vali Loss: 0.2617979 Test Loss: 0.3529957
Validation loss decreased (0.262087 --> 0.261798).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5446169
	speed: 0.2768s/iter; left time: 3043.7791s
	iters: 200, epoch: 8 | loss: 0.4798167
	speed: 0.0581s/iter; left time: 632.9706s
Epoch: 8 cost time: 16.4139244556427
Epoch: 8, Steps: 258 | Train Loss: 0.4960712 Vali Loss: 0.2616986 Test Loss: 0.3528500
Validation loss decreased (0.261798 --> 0.261699).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4497523
	speed: 0.2750s/iter; left time: 2952.3483s
	iters: 200, epoch: 9 | loss: 0.4758063
	speed: 0.0584s/iter; left time: 621.1040s
Epoch: 9 cost time: 15.735091209411621
Epoch: 9, Steps: 258 | Train Loss: 0.4963226 Vali Loss: 0.2614955 Test Loss: 0.3530915
Validation loss decreased (0.261699 --> 0.261496).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5412317
	speed: 0.2596s/iter; left time: 2719.8656s
	iters: 200, epoch: 10 | loss: 0.4097893
	speed: 0.0620s/iter; left time: 643.6632s
Epoch: 10 cost time: 17.16991877555847
Epoch: 10, Steps: 258 | Train Loss: 0.4959500 Vali Loss: 0.2614864 Test Loss: 0.3532574
Validation loss decreased (0.261496 --> 0.261486).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6336311
	speed: 0.2690s/iter; left time: 2749.4175s
	iters: 200, epoch: 11 | loss: 0.4904002
	speed: 0.0694s/iter; left time: 702.2056s
Epoch: 11 cost time: 17.140775442123413
Epoch: 11, Steps: 258 | Train Loss: 0.4950755 Vali Loss: 0.2613454 Test Loss: 0.3533211
Validation loss decreased (0.261486 --> 0.261345).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4191834
	speed: 0.2770s/iter; left time: 2759.2663s
	iters: 200, epoch: 12 | loss: 0.5824162
	speed: 0.0615s/iter; left time: 606.9533s
Epoch: 12 cost time: 16.3563871383667
Epoch: 12, Steps: 258 | Train Loss: 0.4957808 Vali Loss: 0.2612814 Test Loss: 0.3529974
Validation loss decreased (0.261345 --> 0.261281).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4573519
	speed: 0.2908s/iter; left time: 2821.9626s
	iters: 200, epoch: 13 | loss: 0.4368933
	speed: 0.0521s/iter; left time: 500.2246s
Epoch: 13 cost time: 14.190304040908813
Epoch: 13, Steps: 258 | Train Loss: 0.4958037 Vali Loss: 0.2618006 Test Loss: 0.3528750
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5638034
	speed: 0.1481s/iter; left time: 1399.4071s
	iters: 200, epoch: 14 | loss: 0.4684892
	speed: 0.0604s/iter; left time: 564.8598s
Epoch: 14 cost time: 16.727592706680298
Epoch: 14, Steps: 258 | Train Loss: 0.4959250 Vali Loss: 0.2614951 Test Loss: 0.3530941
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4856810
	speed: 0.3047s/iter; left time: 2800.0840s
	iters: 200, epoch: 15 | loss: 0.3974519
	speed: 0.0629s/iter; left time: 571.9419s
Epoch: 15 cost time: 17.904375791549683
Epoch: 15, Steps: 258 | Train Loss: 0.4954124 Vali Loss: 0.2616258 Test Loss: 0.3529167
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3496323525905609, mae:0.3785673975944519, rse:0.4752807021141052, corr:[0.53881955 0.5425814  0.5432833  0.54171044 0.53969395 0.5383389
 0.5378937  0.53814733 0.5386801  0.53904736 0.53895605 0.53838754
 0.53758097 0.53678566 0.5362091  0.53582215 0.53554654 0.53525114
 0.5347835  0.5340802  0.5332363  0.53235626 0.53158957 0.5310407
 0.53068215 0.5304515  0.53024113 0.5299624  0.5295542  0.5290302
 0.5284297  0.5278695  0.5273684  0.52686197 0.52632934 0.52576417
 0.5251297  0.5244534  0.52373797 0.5230365  0.5223873  0.5217835
 0.52124125 0.52074    0.52025515 0.51971203 0.5190845  0.5183737
 0.5175506  0.51661587 0.5156457  0.5147611  0.51401275 0.51344407
 0.51307565 0.51286906 0.5127133  0.51250416 0.51220006 0.5117851
 0.5113189  0.5108702  0.5104889  0.51018703 0.50996125 0.5098382
 0.50969845 0.50954205 0.509329   0.50903237 0.5086463  0.50820273
 0.5077618  0.50735605 0.506975   0.5066074  0.50623584 0.5058119
 0.50533664 0.50477254 0.5040655  0.5032525  0.5024136  0.5016256
 0.50097144 0.50048405 0.5001054  0.49982673 0.49955818 0.49924377
 0.49884817 0.49831703 0.49757335 0.49658856 0.49535564 0.49391985
 0.492391   0.49093112 0.48957524 0.48833713 0.48722506 0.48622137
 0.4852838  0.48434985 0.4833523  0.48226875 0.48111925 0.47997203
 0.47883227 0.47774553 0.47676256 0.4759025  0.47522607 0.4746523
 0.47406593 0.4733963  0.4726139  0.4716748  0.47062725 0.4694777
 0.46835363 0.46730956 0.4664358  0.46570852 0.4650471  0.46435085
 0.46358562 0.46272105 0.4617673  0.4607561  0.45976692 0.4588465
 0.4580521  0.4573761  0.45680237 0.45625567 0.45565462 0.4549987
 0.45429862 0.45360556 0.452955   0.45238367 0.45189872 0.45139778
 0.45080853 0.45004213 0.449193   0.44830438 0.44746616 0.44670245
 0.44606444 0.44554582 0.44503298 0.44445455 0.44379494 0.4431064
 0.4424521  0.44188058 0.44139412 0.44106472 0.44082376 0.44061962
 0.4403567  0.43999317 0.43951014 0.43892962 0.4382774  0.43762764
 0.4370786  0.43672445 0.43652424 0.43645984 0.4364052  0.43625438
 0.43590075 0.43529975 0.43445575 0.43343666 0.4323528  0.43141392
 0.43071932 0.43031424 0.43018186 0.43016973 0.43008235 0.42973727
 0.4290582  0.4279696  0.42651644 0.42477608 0.42291278 0.42109
 0.41949165 0.41833133 0.417446   0.4166376  0.41578355 0.41474888
 0.4135169  0.41208392 0.4104902  0.40879607 0.40708372 0.40545824
 0.40402067 0.40282893 0.40188017 0.40114036 0.4006409  0.40039054
 0.40018925 0.39986387 0.3993058  0.39847675 0.3974397  0.39620456
 0.39485052 0.39342287 0.39208397 0.3909045  0.38988957 0.38902494
 0.38826165 0.3875659  0.3869451  0.38626432 0.38547516 0.38462257
 0.38368103 0.3827209  0.38175982 0.3808173  0.37986508 0.37891936
 0.37798807 0.3771799  0.37657782 0.37623563 0.37617296 0.3762739
 0.37639672 0.37633044 0.37596315 0.3753965  0.37462088 0.3738052
 0.37309414 0.37262076 0.37240335 0.37238574 0.37253955 0.37274542
 0.3728818  0.37291023 0.37274197 0.37240696 0.37193817 0.37145862
 0.37103367 0.37069342 0.37039527 0.3701176  0.36978877 0.36944643
 0.36909342 0.36871147 0.3683672  0.36811486 0.36795127 0.36783352
 0.36777395 0.36771595 0.36755306 0.36726248 0.36679623 0.36614814
 0.3654216  0.36463103 0.36382872 0.36316058 0.36263698 0.36228547
 0.3620382  0.36183003 0.36158046 0.36119223 0.36063105 0.3598192
 0.35886213 0.35796085 0.35716644 0.35646605 0.3558572  0.3552757
 0.35477045 0.354291   0.35376468 0.35314825 0.35246012 0.35166407
 0.35086116 0.35004577 0.34935978 0.34878403 0.34839574 0.3481679
 0.3480886  0.34798244 0.34773675 0.34729293 0.34665874 0.34600627
 0.34537753 0.34493035 0.34468326 0.34459934 0.3446337  0.34471378
 0.34469473 0.34454113 0.34426945 0.34390584 0.3435022  0.34317628
 0.34300578 0.34307593 0.34335846 0.3436727  0.34392986 0.3440017
 0.34392947 0.3437138  0.34345034 0.3432751  0.34319967 0.34328645
 0.34344643 0.3434879  0.34337112 0.34312987 0.34278515 0.34235394
 0.34194475 0.34165144 0.34152788 0.34155738 0.34169245 0.34189555
 0.34207112 0.34217158 0.3421469  0.34197655 0.3417071  0.3413883
 0.3410887  0.3408991  0.34078053 0.3407791  0.34073222 0.34060735
 0.3403622  0.340002   0.3395748  0.33916602 0.33883616 0.33869076
 0.33867225 0.33875835 0.33883628 0.33883706 0.33874512 0.33853355
 0.33825797 0.3380398  0.3379465  0.3379964  0.33814457 0.33836395
 0.33860302 0.33875275 0.33868265 0.33827624 0.3376512  0.33684438
 0.336067   0.33546337 0.33509943 0.33489022 0.33465552 0.33433616
 0.33379725 0.3330178  0.33204576 0.3309926  0.32999623 0.3291222
 0.3284188  0.3279159  0.32747257 0.3270073  0.32654938 0.32603502
 0.32548365 0.324918   0.32443213 0.32411224 0.3238882  0.3237464
 0.3235962  0.32344234 0.3232609  0.32301906 0.32277656 0.3225378
 0.32239288 0.3223786  0.3224162  0.32244903 0.32239676 0.3222673
 0.32213372 0.32205036 0.32201576 0.3220518  0.32215858 0.3223184
 0.32252532 0.3227181  0.3229053  0.32306862 0.32322794 0.32338467
 0.3235163  0.32352296 0.3233945  0.32315543 0.32286888 0.32253727
 0.32211494 0.32172215 0.32133314 0.32089972 0.32044047 0.32002783
 0.3195996  0.31920022 0.31881014 0.31838936 0.3179723  0.31753978
 0.31714192 0.31678376 0.31649265 0.316198   0.3159058  0.31564823
 0.3154226  0.31524044 0.31508732 0.3149538  0.31489506 0.3148175
 0.31470978 0.31454098 0.31428674 0.3139285  0.3134472  0.31288266
 0.31231397 0.31179765 0.31135735 0.3109862  0.31072    0.3104999
 0.31028652 0.30995747 0.30950838 0.30885395 0.3080036  0.3069483
 0.30570537 0.30439737 0.30302092 0.30162194 0.30026868 0.29900786
 0.29785082 0.29688624 0.29611832 0.29560938 0.29527086 0.29497752
 0.29460722 0.2941591  0.29360142 0.29286715 0.2920021  0.29105833
 0.2901271  0.2891956  0.28829363 0.28752849 0.28689203 0.28645527
 0.286092   0.2857059  0.28526157 0.28471527 0.284136   0.28353515
 0.28297457 0.28250542 0.282139   0.28187922 0.28174025 0.28168398
 0.28162247 0.28145382 0.28126752 0.28101674 0.28068754 0.2803161
 0.27992934 0.27959165 0.2792976  0.27906248 0.27891308 0.2788012
 0.2786433  0.27843767 0.2781705  0.27788204 0.27763978 0.27745298
 0.27726236 0.27703196 0.27682048 0.27658626 0.27632004 0.27602473
 0.27570596 0.2754041  0.27509862 0.27484518 0.27463457 0.27445546
 0.27431303 0.2742615  0.2742822  0.2743262  0.27428523 0.27414638
 0.27385187 0.27345547 0.2730431  0.27272692 0.27245757 0.27232388
 0.27231762 0.27244294 0.27258572 0.2726414  0.27257785 0.2724199
 0.27223128 0.27208117 0.27204913 0.272144   0.27232295 0.27250722
 0.2725728  0.27238858 0.2718041  0.270809   0.26949254 0.26791883
 0.2662653  0.2648575  0.26379004 0.26302275 0.2623645  0.26165769
 0.26080504 0.2597926  0.2586856  0.2575681  0.25650716 0.25558352
 0.25477704 0.25400463 0.2532822  0.25256044 0.25182906 0.25114942
 0.25054443 0.24999498 0.24957845 0.24924444 0.2490321  0.24893263
 0.24889156 0.24890405 0.24887481 0.2487394  0.24855894 0.2483614
 0.24809171 0.24776584 0.24744694 0.24706464 0.2466742  0.24630992
 0.24609083 0.24592413 0.24587926 0.2460446  0.24639697 0.24684988
 0.24732862 0.24782589 0.24825954 0.24860127 0.24884868 0.24900481
 0.2490117  0.24882664 0.24847516 0.24813436 0.24792217 0.24789977
 0.24800412 0.24818529 0.24840303 0.24842021 0.24834387 0.24814804
 0.24788956 0.24766372 0.24746181 0.2474214  0.24738756 0.24736102
 0.24726336 0.24720947 0.24715044 0.2469948  0.24688824 0.24668558
 0.24645863 0.24618356 0.24590106 0.24564011 0.24551778 0.24541473
 0.24544331 0.24552032 0.24552804 0.24555098 0.24560632 0.2456439
 0.24563292 0.24571745 0.24583766 0.24600215 0.24627058 0.24656595
 0.24676707 0.24684313 0.24669771 0.24617237 0.24523775 0.2440141
 0.24267396 0.24140467 0.24032474 0.2394566  0.238847   0.23840547
 0.23802337 0.237599   0.23726343 0.23689047 0.23647617 0.23595738
 0.23533708 0.23465855 0.2339699  0.23329069 0.2327334  0.23219614
 0.23166822 0.23110346 0.23044798 0.22989434 0.22943783 0.22906993
 0.22880022 0.22853245 0.22824663 0.22776163 0.22705793 0.2261376
 0.22520925 0.2242681  0.22349945 0.22305004 0.22294311 0.22311452
 0.22331132 0.22336155 0.22306643 0.22249882 0.2216163  0.22076814
 0.22028814 0.22040227 0.22100803 0.2214799  0.22056043 0.21667068]
