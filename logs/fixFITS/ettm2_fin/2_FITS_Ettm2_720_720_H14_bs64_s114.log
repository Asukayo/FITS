Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3730170
	speed: 0.1314s/iter; left time: 1681.7707s
	iters: 200, epoch: 1 | loss: 0.4586482
	speed: 0.1225s/iter; left time: 1556.1113s
Epoch: 1 cost time: 32.786003828048706
Epoch: 1, Steps: 258 | Train Loss: 0.4335389 Vali Loss: 0.2959006 Test Loss: 0.3944496
Validation loss decreased (inf --> 0.295901).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2645182
	speed: 0.5485s/iter; left time: 6879.4769s
	iters: 200, epoch: 2 | loss: 0.2988102
	speed: 0.1235s/iter; left time: 1536.8369s
Epoch: 2 cost time: 32.82612228393555
Epoch: 2, Steps: 258 | Train Loss: 0.3216932 Vali Loss: 0.2809815 Test Loss: 0.3765641
Validation loss decreased (0.295901 --> 0.280981).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3317439
	speed: 0.5467s/iter; left time: 6716.0061s
	iters: 200, epoch: 3 | loss: 0.2035412
	speed: 0.1233s/iter; left time: 1502.3287s
Epoch: 3 cost time: 32.20888090133667
Epoch: 3, Steps: 258 | Train Loss: 0.2903330 Vali Loss: 0.2751558 Test Loss: 0.3683769
Validation loss decreased (0.280981 --> 0.275156).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2301612
	speed: 0.5474s/iter; left time: 6583.8835s
	iters: 200, epoch: 4 | loss: 0.3256615
	speed: 0.1343s/iter; left time: 1601.8450s
Epoch: 4 cost time: 34.77531170845032
Epoch: 4, Steps: 258 | Train Loss: 0.2746679 Vali Loss: 0.2709505 Test Loss: 0.3635639
Validation loss decreased (0.275156 --> 0.270950).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2262934
	speed: 0.5517s/iter; left time: 6493.4300s
	iters: 200, epoch: 5 | loss: 0.3379228
	speed: 0.1228s/iter; left time: 1432.8563s
Epoch: 5 cost time: 32.599151849746704
Epoch: 5, Steps: 258 | Train Loss: 0.2662879 Vali Loss: 0.2678682 Test Loss: 0.3602269
Validation loss decreased (0.270950 --> 0.267868).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2460114
	speed: 0.5401s/iter; left time: 6217.2002s
	iters: 200, epoch: 6 | loss: 0.2529966
	speed: 0.1262s/iter; left time: 1439.6731s
Epoch: 6 cost time: 33.58910894393921
Epoch: 6, Steps: 258 | Train Loss: 0.2612914 Vali Loss: 0.2661007 Test Loss: 0.3581317
Validation loss decreased (0.267868 --> 0.266101).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2749721
	speed: 0.5390s/iter; left time: 6065.8509s
	iters: 200, epoch: 7 | loss: 0.2342370
	speed: 0.1036s/iter; left time: 1155.6011s
Epoch: 7 cost time: 28.986125230789185
Epoch: 7, Steps: 258 | Train Loss: 0.2588457 Vali Loss: 0.2652811 Test Loss: 0.3567462
Validation loss decreased (0.266101 --> 0.265281).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3057777
	speed: 0.4035s/iter; left time: 4436.5770s
	iters: 200, epoch: 8 | loss: 0.2564284
	speed: 0.1171s/iter; left time: 1275.5417s
Epoch: 8 cost time: 28.931182384490967
Epoch: 8, Steps: 258 | Train Loss: 0.2573758 Vali Loss: 0.2647232 Test Loss: 0.3558214
Validation loss decreased (0.265281 --> 0.264723).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2752057
	speed: 0.6710s/iter; left time: 7204.9871s
	iters: 200, epoch: 9 | loss: 0.2416268
	speed: 0.1567s/iter; left time: 1666.2895s
Epoch: 9 cost time: 42.633341789245605
Epoch: 9, Steps: 258 | Train Loss: 0.2563807 Vali Loss: 0.2638050 Test Loss: 0.3555689
Validation loss decreased (0.264723 --> 0.263805).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3036138
	speed: 0.7697s/iter; left time: 8065.5983s
	iters: 200, epoch: 10 | loss: 0.3010431
	speed: 0.1740s/iter; left time: 1806.3758s
Epoch: 10 cost time: 46.0143346786499
Epoch: 10, Steps: 258 | Train Loss: 0.2560379 Vali Loss: 0.2635105 Test Loss: 0.3551272
Validation loss decreased (0.263805 --> 0.263510).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2263809
	speed: 0.6737s/iter; left time: 6885.4697s
	iters: 200, epoch: 11 | loss: 0.3149883
	speed: 0.1234s/iter; left time: 1248.8501s
Epoch: 11 cost time: 32.59819197654724
Epoch: 11, Steps: 258 | Train Loss: 0.2552850 Vali Loss: 0.2632782 Test Loss: 0.3549829
Validation loss decreased (0.263510 --> 0.263278).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3233831
	speed: 0.6904s/iter; left time: 6878.7506s
	iters: 200, epoch: 12 | loss: 0.2491063
	speed: 0.1583s/iter; left time: 1561.3268s
Epoch: 12 cost time: 41.9521050453186
Epoch: 12, Steps: 258 | Train Loss: 0.2556122 Vali Loss: 0.2629014 Test Loss: 0.3548568
Validation loss decreased (0.263278 --> 0.262901).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2305654
	speed: 0.7118s/iter; left time: 6908.0076s
	iters: 200, epoch: 13 | loss: 0.2618488
	speed: 0.1291s/iter; left time: 1239.5379s
Epoch: 13 cost time: 37.9526743888855
Epoch: 13, Steps: 258 | Train Loss: 0.2554212 Vali Loss: 0.2632873 Test Loss: 0.3546778
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3281725
	speed: 0.7045s/iter; left time: 6655.1281s
	iters: 200, epoch: 14 | loss: 0.2461372
	speed: 0.1767s/iter; left time: 1651.2189s
Epoch: 14 cost time: 46.47044658660889
Epoch: 14, Steps: 258 | Train Loss: 0.2554555 Vali Loss: 0.2627331 Test Loss: 0.3543943
Validation loss decreased (0.262901 --> 0.262733).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2086113
	speed: 0.7466s/iter; left time: 6860.5582s
	iters: 200, epoch: 15 | loss: 0.3251408
	speed: 0.1254s/iter; left time: 1139.6305s
Epoch: 15 cost time: 35.809107303619385
Epoch: 15, Steps: 258 | Train Loss: 0.2553645 Vali Loss: 0.2630064 Test Loss: 0.3543761
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2769078
	speed: 0.7053s/iter; left time: 6298.6855s
	iters: 200, epoch: 16 | loss: 0.2346590
	speed: 0.1269s/iter; left time: 1120.2999s
Epoch: 16 cost time: 37.57980442047119
Epoch: 16, Steps: 258 | Train Loss: 0.2551161 Vali Loss: 0.2627674 Test Loss: 0.3544936
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2808945
	speed: 0.6681s/iter; left time: 5794.3608s
	iters: 200, epoch: 17 | loss: 0.2621742
	speed: 0.1637s/iter; left time: 1403.6548s
Epoch: 17 cost time: 42.9236958026886
Epoch: 17, Steps: 258 | Train Loss: 0.2551890 Vali Loss: 0.2629700 Test Loss: 0.3543020
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4665325
	speed: 0.1706s/iter; left time: 2183.3907s
	iters: 200, epoch: 1 | loss: 0.4289265
	speed: 0.1907s/iter; left time: 2421.8386s
Epoch: 1 cost time: 47.311307191848755
Epoch: 1, Steps: 258 | Train Loss: 0.4981108 Vali Loss: 0.2615634 Test Loss: 0.3537484
Validation loss decreased (inf --> 0.261563).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5047772
	speed: 0.8153s/iter; left time: 10226.4593s
	iters: 200, epoch: 2 | loss: 0.4385608
	speed: 0.1212s/iter; left time: 1507.7199s
Epoch: 2 cost time: 37.39575719833374
Epoch: 2, Steps: 258 | Train Loss: 0.4962177 Vali Loss: 0.2611329 Test Loss: 0.3532441
Validation loss decreased (0.261563 --> 0.261133).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5410763
	speed: 0.7024s/iter; left time: 8629.3185s
	iters: 200, epoch: 3 | loss: 0.4873258
	speed: 0.1586s/iter; left time: 1932.3744s
Epoch: 3 cost time: 42.459930181503296
Epoch: 3, Steps: 258 | Train Loss: 0.4957294 Vali Loss: 0.2608703 Test Loss: 0.3528140
Validation loss decreased (0.261133 --> 0.260870).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4024670
	speed: 0.7187s/iter; left time: 8643.2159s
	iters: 200, epoch: 4 | loss: 0.6117232
	speed: 0.1494s/iter; left time: 1782.4827s
Epoch: 4 cost time: 39.925312995910645
Epoch: 4, Steps: 258 | Train Loss: 0.4947007 Vali Loss: 0.2602829 Test Loss: 0.3529371
Validation loss decreased (0.260870 --> 0.260283).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6286260
	speed: 0.6837s/iter; left time: 8046.2497s
	iters: 200, epoch: 5 | loss: 0.4159233
	speed: 0.1622s/iter; left time: 1892.2324s
Epoch: 5 cost time: 43.01478433609009
Epoch: 5, Steps: 258 | Train Loss: 0.4946381 Vali Loss: 0.2608457 Test Loss: 0.3524488
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4156894
	speed: 0.7486s/iter; left time: 8617.6696s
	iters: 200, epoch: 6 | loss: 0.4650931
	speed: 0.1703s/iter; left time: 1943.7273s
Epoch: 6 cost time: 45.588592290878296
Epoch: 6, Steps: 258 | Train Loss: 0.4949084 Vali Loss: 0.2605880 Test Loss: 0.3523362
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6784293
	speed: 0.7332s/iter; left time: 8250.8262s
	iters: 200, epoch: 7 | loss: 0.7282176
	speed: 0.1575s/iter; left time: 1756.3387s
Epoch: 7 cost time: 42.29924964904785
Epoch: 7, Steps: 258 | Train Loss: 0.4945661 Vali Loss: 0.2608290 Test Loss: 0.3521996
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.34948965907096863, mae:0.3781927227973938, rse:0.47518372535705566, corr:[0.54028577 0.54609776 0.54397243 0.54113036 0.54090935 0.542284
 0.54296464 0.54200345 0.54059    0.5400043  0.54033905 0.5407862
 0.5405758  0.539807   0.5391053  0.5387935  0.5386336  0.5381582
 0.5372528  0.53626645 0.5355973  0.5352548  0.5349084  0.53426516
 0.5333976  0.53264195 0.5321664  0.5318149  0.5312952  0.5304901
 0.5295985  0.5289374  0.5285985  0.5284153  0.5281338  0.5275911
 0.5268226  0.52600133 0.5252582  0.52464664 0.5240959  0.5235214
 0.52292687 0.52230835 0.5216393  0.5208941  0.5200675  0.51923364
 0.51849127 0.51785606 0.5173112  0.5167483  0.51600796 0.5151486
 0.51437986 0.5138206  0.51345396 0.5132012  0.51292974 0.51253533
 0.5121473  0.5119041  0.51184106 0.5118521  0.51175755 0.51149714
 0.51108634 0.5106979  0.5103973  0.5101396  0.50983065 0.5094471
 0.50905883 0.50869715 0.5083246  0.50787205 0.5073128  0.50668865
 0.5061671  0.5057613  0.5053812  0.50487447 0.50414246 0.5032159
 0.5022813  0.50153655 0.50105566 0.50078183 0.50049174 0.5000799
 0.4995889  0.4990577  0.49850458 0.49782476 0.4968269  0.49543127
 0.49376643 0.49211556 0.4906856  0.48953602 0.48854586 0.4875584
 0.4864989  0.48540163 0.4843508  0.4834049  0.48253742 0.48170435
 0.48088408 0.4801029  0.4793163  0.47838792 0.47724617 0.4758924
 0.47455278 0.47354987 0.47296783 0.47254235 0.4719367  0.47097564
 0.46980712 0.4686451  0.46766752 0.46680132 0.46589655 0.4648868
 0.46389118 0.46306187 0.46245113 0.46194437 0.46135923 0.46056244
 0.45956933 0.45847416 0.45741403 0.4564798  0.45573214 0.45525455
 0.455013   0.45485514 0.45456046 0.4539645  0.4530717  0.45206124
 0.451246   0.45073128 0.45039782 0.4499001  0.4490673  0.4479801
 0.44691893 0.4460501  0.4452407  0.44433284 0.4433481  0.44251898
 0.44209942 0.44211558 0.44231352 0.44237477 0.44204742 0.4413811
 0.44057223 0.43983492 0.43920693 0.438598   0.43792805 0.43721676
 0.4364994  0.43574822 0.43485636 0.43394172 0.4332291  0.43294254
 0.43303436 0.43323755 0.4332394  0.4329055  0.4323096  0.43160546
 0.43078935 0.42982394 0.42876056 0.42780903 0.4273038  0.42733446
 0.42761433 0.42755878 0.42680854 0.42538917 0.42379454 0.42254874
 0.42174876 0.42107067 0.42000157 0.4185562  0.4172365  0.4164624
 0.41613033 0.41565314 0.41450384 0.41266558 0.41067803 0.40917727
 0.40825602 0.40740192 0.40608904 0.40435463 0.40291575 0.40248394
 0.4028274  0.4030476  0.40233752 0.4005453  0.39827922 0.396442
 0.39557415 0.3952648  0.3947482  0.3934798  0.39161777 0.38982907
 0.38862845 0.38795215 0.38734517 0.38633576 0.38501045 0.383827
 0.383026   0.382533   0.38197955 0.38110963 0.37997988 0.37889752
 0.37803164 0.37738627 0.3768589  0.37644604 0.37629965 0.37641564
 0.37651667 0.37615803 0.37525412 0.37422457 0.37354082 0.37360808
 0.37424523 0.3748668  0.3749646  0.37456334 0.37418917 0.37422758
 0.37461174 0.3749225  0.37468627 0.3739399  0.37310308 0.37267974
 0.3726638  0.37259457 0.37198573 0.37080187 0.36944437 0.36855114
 0.36833805 0.36848179 0.3685967  0.3684945  0.36829272 0.36820623
 0.36831978 0.36837706 0.36805317 0.3673371  0.36643836 0.36568716
 0.36532518 0.3651984  0.3651118  0.3650672  0.36507335 0.36520907
 0.3653578  0.36536306 0.36509478 0.3644937  0.3635706  0.3622593
 0.360673   0.35908365 0.35770094 0.35673317 0.35625315 0.35598812
 0.3556083  0.35478595 0.3535636  0.3523791  0.3516997  0.351495
 0.35145938 0.35110384 0.35041508 0.3495744  0.34898308 0.34870306
 0.3485264  0.34809494 0.34741864 0.3468041  0.3464993  0.3464759
 0.3462326  0.34552142 0.34444094 0.34348595 0.34309268 0.34315366
 0.3430936  0.34257424 0.34177417 0.34128147 0.34157208 0.34251705
 0.3433742  0.3435     0.34279236 0.34175727 0.34126133 0.3416021
 0.3424361  0.34300068 0.34288055 0.34227076 0.3417134  0.34175646
 0.34233353 0.3429401  0.34322724 0.34315    0.34289917 0.34272215
 0.34275275 0.3428589  0.3427915  0.342391   0.34170952 0.34095684
 0.34030935 0.3399421  0.33990887 0.3401712  0.3405946  0.3408847
 0.34079733 0.34033665 0.33967268 0.33918706 0.33901477 0.3391931
 0.3394701  0.33960155 0.33953714 0.33943468 0.33946154 0.3396834
 0.33978984 0.339534   0.33884594 0.3379958  0.33742264 0.33739707
 0.337955   0.33884633 0.33969244 0.3402506  0.34046122 0.34042728
 0.34027168 0.34001058 0.33956778 0.3388564  0.33794567 0.33681557
 0.335637   0.334603   0.33391193 0.33361897 0.33358246 0.33360276
 0.33333233 0.33260766 0.3314876  0.33025622 0.32928303 0.32875198
 0.32861444 0.32866567 0.32858002 0.32823247 0.3276995  0.3270158
 0.3262823  0.32553515 0.32481414 0.32414111 0.32346058 0.32290804
 0.3225986  0.3226595  0.3229734  0.3232441  0.32321453 0.32268864
 0.32181102 0.32091606 0.32029432 0.32008034 0.3201263  0.3202146
 0.32021308 0.32011312 0.320037   0.32016212 0.32050747 0.32092535
 0.3212843  0.3214914  0.32160914 0.32171643 0.32187995 0.32206902
 0.322217   0.3222722  0.3223256  0.3224128  0.32247642 0.3223672
 0.321988   0.32151815 0.32103813 0.32058147 0.32018077 0.31987688
 0.31957462 0.3192686  0.31887373 0.31831902 0.31766984 0.31705427
 0.31673452 0.31677845 0.31701028 0.31707937 0.31683117 0.31636232
 0.31591496 0.3157278  0.31577015 0.3158256  0.31573856 0.31540605
 0.31507275 0.31496388 0.31504112 0.3150548  0.31480664 0.31437153
 0.3140617  0.31410035 0.3143672  0.31447405 0.3140874  0.31307322
 0.3117702  0.31065255 0.31000635 0.30956528 0.30891868 0.3077629
 0.30616435 0.30458155 0.3032879  0.30231658 0.30147135 0.30054218
 0.29947922 0.2984863  0.29769993 0.2971942  0.29682282 0.2964386
 0.29599217 0.29555345 0.29507822 0.29446164 0.29372156 0.29292342
 0.29220048 0.29155102 0.29092458 0.29028592 0.2895314  0.28877828
 0.28804776 0.2874195  0.28692624 0.2864988  0.2861181  0.28567976
 0.2851555  0.28453934 0.28382513 0.28309965 0.28253803 0.282282
 0.28231797 0.28242198 0.28242832 0.28205386 0.28128988 0.28045192
 0.27992532 0.27991423 0.28022042 0.2805138  0.28056198 0.2803226
 0.27994058 0.27963206 0.27943987 0.27929342 0.2791524  0.2790639
 0.2791487  0.27943167 0.279772   0.27983388 0.27950624 0.27901223
 0.27877378 0.2790671  0.27967545 0.2801123  0.27989188 0.27895904
 0.27779597 0.27705652 0.27698752 0.27732596 0.27752554 0.27735117
 0.27690625 0.2766052  0.2767119  0.27714503 0.27745673 0.2774413
 0.2770583  0.27656144 0.27610275 0.2757618  0.27557698 0.27555883
 0.27573082 0.2760048  0.27623114 0.27619293 0.27581805 0.27523577
 0.27466202 0.2742184  0.27373874 0.27297074 0.27173162 0.27001032
 0.26813945 0.26664743 0.26560605 0.2648138  0.2640389  0.26327956
 0.26263422 0.26206902 0.26136178 0.2602508  0.2587318  0.2572396
 0.25627857 0.25604343 0.25630888 0.25643778 0.25592434 0.25474104
 0.25323287 0.25188965 0.25110796 0.2507518  0.25054243 0.2502181
 0.24972616 0.24929295 0.24903941 0.24898706 0.24912111 0.24925205
 0.24909566 0.2486217  0.24799779 0.247321   0.24680376 0.24651067
 0.24644561 0.24632442 0.24617569 0.24619731 0.24646086 0.2468728
 0.24724333 0.24742876 0.24733219 0.24720745 0.24746408 0.24835098
 0.24959084 0.25056136 0.25073865 0.2501298  0.24918106 0.24855481
 0.24863967 0.24940713 0.25041917 0.2509914  0.25112888 0.25098056
 0.2508591  0.2509429  0.251078   0.25121945 0.25112277 0.25084814
 0.250437   0.2500003  0.24939455 0.24858059 0.24802327 0.24790691
 0.24845102 0.24937429 0.25023603 0.25063777 0.2506133  0.25029528
 0.25017777 0.25028756 0.25031492 0.25014225 0.24976288 0.24932575
 0.2490518  0.24913839 0.24926487 0.24923226 0.24922763 0.2494936
 0.25012377 0.25096565 0.2514889  0.25112933 0.24984755 0.2482046
 0.24689351 0.24621902 0.24587905 0.24530229 0.24427332 0.24291727
 0.24162225 0.2406352  0.24012277 0.23960885 0.23886442 0.23788552
 0.23697437 0.23638925 0.23611833 0.23595533 0.23583524 0.2355505
 0.23511142 0.23446965 0.23353729 0.2325922  0.23171902 0.23106945
 0.23078847 0.23073289 0.23077515 0.230611   0.23021606 0.22962941
 0.22911544 0.22849955 0.22780405 0.22709706 0.22638458 0.22562805
 0.22472146 0.22390215 0.22341317 0.22355069 0.22375089 0.22354825
 0.22275567 0.2220085  0.22230652 0.22321048 0.22137822 0.21178715]
