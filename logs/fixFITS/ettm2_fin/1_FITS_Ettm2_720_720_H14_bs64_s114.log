Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4544778
	speed: 0.1369s/iter; left time: 1752.6850s
	iters: 200, epoch: 1 | loss: 0.7022394
	speed: 0.1315s/iter; left time: 1670.5536s
Epoch: 1 cost time: 34.90746808052063
Epoch: 1, Steps: 258 | Train Loss: 0.5723936 Vali Loss: 0.2777416 Test Loss: 0.3739824
Validation loss decreased (inf --> 0.277742).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4349810
	speed: 0.6362s/iter; left time: 7979.8362s
	iters: 200, epoch: 2 | loss: 0.4985230
	speed: 0.1410s/iter; left time: 1754.3826s
Epoch: 2 cost time: 34.829601764678955
Epoch: 2, Steps: 258 | Train Loss: 0.5172120 Vali Loss: 0.2686980 Test Loss: 0.3641858
Validation loss decreased (0.277742 --> 0.268698).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5926580
	speed: 0.5254s/iter; left time: 6454.0287s
	iters: 200, epoch: 3 | loss: 0.3564458
	speed: 0.1392s/iter; left time: 1696.6541s
Epoch: 3 cost time: 34.19542574882507
Epoch: 3, Steps: 258 | Train Loss: 0.5083822 Vali Loss: 0.2664196 Test Loss: 0.3601144
Validation loss decreased (0.268698 --> 0.266420).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4143704
	speed: 0.6035s/iter; left time: 7257.8932s
	iters: 200, epoch: 4 | loss: 0.6128854
	speed: 0.1373s/iter; left time: 1637.7100s
Epoch: 4 cost time: 36.367716550827026
Epoch: 4, Steps: 258 | Train Loss: 0.5042916 Vali Loss: 0.2644932 Test Loss: 0.3580889
Validation loss decreased (0.266420 --> 0.264493).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4208990
	speed: 0.5757s/iter; left time: 6775.5859s
	iters: 200, epoch: 5 | loss: 0.6471041
	speed: 0.1393s/iter; left time: 1624.9296s
Epoch: 5 cost time: 35.59007120132446
Epoch: 5, Steps: 258 | Train Loss: 0.5019744 Vali Loss: 0.2632241 Test Loss: 0.3566158
Validation loss decreased (0.264493 --> 0.263224).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4641482
	speed: 0.5262s/iter; left time: 6057.1483s
	iters: 200, epoch: 6 | loss: 0.4832663
	speed: 0.1372s/iter; left time: 1565.0705s
Epoch: 6 cost time: 35.6318895816803
Epoch: 6, Steps: 258 | Train Loss: 0.4997227 Vali Loss: 0.2625380 Test Loss: 0.3556511
Validation loss decreased (0.263224 --> 0.262538).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5310655
	speed: 0.5839s/iter; left time: 6570.1373s
	iters: 200, epoch: 7 | loss: 0.4512278
	speed: 0.1305s/iter; left time: 1455.4277s
Epoch: 7 cost time: 34.37916564941406
Epoch: 7, Steps: 258 | Train Loss: 0.4988393 Vali Loss: 0.2624554 Test Loss: 0.3548595
Validation loss decreased (0.262538 --> 0.262455).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5921586
	speed: 0.5922s/iter; left time: 6511.4366s
	iters: 200, epoch: 8 | loss: 0.4941877
	speed: 0.0922s/iter; left time: 1004.1979s
Epoch: 8 cost time: 28.648026704788208
Epoch: 8, Steps: 258 | Train Loss: 0.4979616 Vali Loss: 0.2622595 Test Loss: 0.3542668
Validation loss decreased (0.262455 --> 0.262259).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5366186
	speed: 0.4527s/iter; left time: 4860.2685s
	iters: 200, epoch: 9 | loss: 0.4637832
	speed: 0.0621s/iter; left time: 660.0654s
Epoch: 9 cost time: 18.063197374343872
Epoch: 9, Steps: 258 | Train Loss: 0.4969715 Vali Loss: 0.2615247 Test Loss: 0.3542207
Validation loss decreased (0.262259 --> 0.261525).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5925203
	speed: 0.2185s/iter; left time: 2290.0196s
	iters: 200, epoch: 10 | loss: 0.5905334
	speed: 0.0715s/iter; left time: 741.8166s
Epoch: 10 cost time: 20.726801872253418
Epoch: 10, Steps: 258 | Train Loss: 0.4967525 Vali Loss: 0.2614141 Test Loss: 0.3537720
Validation loss decreased (0.261525 --> 0.261414).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4411637
	speed: 0.5924s/iter; left time: 6054.8207s
	iters: 200, epoch: 11 | loss: 0.6168321
	speed: 0.1317s/iter; left time: 1332.4385s
Epoch: 11 cost time: 35.27008891105652
Epoch: 11, Steps: 258 | Train Loss: 0.4954586 Vali Loss: 0.2612070 Test Loss: 0.3536832
Validation loss decreased (0.261414 --> 0.261207).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6347083
	speed: 0.5838s/iter; left time: 5815.9348s
	iters: 200, epoch: 12 | loss: 0.4849268
	speed: 0.1182s/iter; left time: 1165.4295s
Epoch: 12 cost time: 30.589518070220947
Epoch: 12, Steps: 258 | Train Loss: 0.4961481 Vali Loss: 0.2607695 Test Loss: 0.3535275
Validation loss decreased (0.261207 --> 0.260769).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4413612
	speed: 0.5083s/iter; left time: 4933.2104s
	iters: 200, epoch: 13 | loss: 0.5066220
	speed: 0.0092s/iter; left time: 88.6356s
Epoch: 13 cost time: 15.578251838684082
Epoch: 13, Steps: 258 | Train Loss: 0.4957576 Vali Loss: 0.2611679 Test Loss: 0.3533006
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6367861
	speed: 0.4656s/iter; left time: 4398.2485s
	iters: 200, epoch: 14 | loss: 0.4812858
	speed: 0.1348s/iter; left time: 1260.2883s
Epoch: 14 cost time: 36.03236651420593
Epoch: 14, Steps: 258 | Train Loss: 0.4957763 Vali Loss: 0.2606065 Test Loss: 0.3529517
Validation loss decreased (0.260769 --> 0.260607).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4012551
	speed: 0.5886s/iter; left time: 5408.5307s
	iters: 200, epoch: 15 | loss: 0.6354129
	speed: 0.1379s/iter; left time: 1253.7798s
Epoch: 15 cost time: 36.02952790260315
Epoch: 15, Steps: 258 | Train Loss: 0.4955311 Vali Loss: 0.2608686 Test Loss: 0.3529368
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5374254
	speed: 0.5154s/iter; left time: 4603.4024s
	iters: 200, epoch: 16 | loss: 0.4532540
	speed: 0.0553s/iter; left time: 488.4838s
Epoch: 16 cost time: 18.379088640213013
Epoch: 16, Steps: 258 | Train Loss: 0.4949697 Vali Loss: 0.2605938 Test Loss: 0.3530515
Validation loss decreased (0.260607 --> 0.260594).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5397234
	speed: 0.2782s/iter; left time: 2413.0113s
	iters: 200, epoch: 17 | loss: 0.5074313
	speed: 0.0532s/iter; left time: 455.8844s
Epoch: 17 cost time: 14.63182783126831
Epoch: 17, Steps: 258 | Train Loss: 0.4950247 Vali Loss: 0.2607827 Test Loss: 0.3528188
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.6026906
	speed: 0.4221s/iter; left time: 3551.8324s
	iters: 200, epoch: 18 | loss: 0.3719104
	speed: 0.1220s/iter; left time: 1014.2099s
Epoch: 18 cost time: 30.809502363204956
Epoch: 18, Steps: 258 | Train Loss: 0.4949632 Vali Loss: 0.2606691 Test Loss: 0.3528820
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3757881
	speed: 0.5439s/iter; left time: 4436.5426s
	iters: 200, epoch: 19 | loss: 0.4356644
	speed: 0.1491s/iter; left time: 1201.4048s
Epoch: 19 cost time: 39.11026334762573
Epoch: 19, Steps: 258 | Train Loss: 0.4951749 Vali Loss: 0.2606961 Test Loss: 0.3527223
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.34960371255874634, mae:0.3782476782798767, rse:0.47526130080223083, corr:[0.5352968  0.5410446  0.53555423 0.5339025  0.53636765 0.5386132
 0.53798056 0.53627825 0.53571117 0.53650135 0.5373616  0.5371509
 0.5362777  0.53580487 0.53617287 0.53677446 0.53670746 0.5358058
 0.53471875 0.53407776 0.53395957 0.53384644 0.53329897 0.5324742
 0.53181964 0.53159046 0.5316009  0.53138894 0.5307079  0.5298354
 0.52918035 0.5289362  0.52885586 0.5285177  0.52778465 0.5268835
 0.52607936 0.5255149  0.5250964  0.52465314 0.5241075  0.52352536
 0.52304554 0.5226809  0.5223115  0.52172625 0.52087957 0.51993436
 0.5190496  0.5183113  0.5177534  0.5172728  0.51665497 0.51587063
 0.51508355 0.51445466 0.5140176  0.5136985  0.51338583 0.5130058
 0.51267046 0.5124659  0.5123606  0.5122317  0.5119895  0.5116623
 0.51126    0.51088566 0.5105222  0.5101407  0.5097427  0.50935304
 0.5089871  0.5085926  0.5080784  0.50745076 0.50682205 0.5062669
 0.5058146  0.5053089  0.50460947 0.5037643  0.5029562  0.5023523
 0.50198007 0.5016613  0.50119495 0.50060326 0.5000032  0.49956378
 0.4993011  0.49899212 0.4984066  0.49742535 0.49608615 0.4945773
 0.49309012 0.49168518 0.49029085 0.4889224  0.48770264 0.48675382
 0.48602712 0.48530638 0.48439905 0.48329338 0.4821161  0.48103353
 0.48007408 0.47917366 0.47826385 0.4773421  0.4764993  0.47572726
 0.47496086 0.47410968 0.47313488 0.47210124 0.47116446 0.47039375
 0.4697608  0.46907017 0.46823016 0.46726844 0.4663733  0.46567816
 0.46512187 0.46447092 0.4635879  0.46253806 0.46155798 0.46081206
 0.4602246  0.4595328  0.45858088 0.4574149  0.4562768  0.45546606
 0.45499468 0.454622   0.45411873 0.45347297 0.45282003 0.45222107
 0.45162618 0.4508726  0.44999376 0.44909614 0.44835806 0.44778287
 0.44719535 0.4463659  0.44522488 0.44412014 0.4435401  0.44360355
 0.44384634 0.44367144 0.4428414  0.4416068  0.44040638 0.43965846
 0.43936655 0.43921047 0.4388741  0.43830633 0.43770167 0.43729836
 0.43713346 0.43703106 0.43677035 0.43638843 0.43596494 0.43555072
 0.4350648  0.4344578  0.43380588 0.43330383 0.43305975 0.4329262
 0.43250945 0.4315766  0.4302847  0.42908505 0.4284347  0.4283674
 0.42852578 0.42835528 0.4276237  0.42647064 0.42528936 0.42430216
 0.4234436  0.42254955 0.42135075 0.41991413 0.41852632 0.41735098
 0.4163176  0.4151668  0.41369757 0.41192484 0.4100636  0.4083742
 0.4069893  0.40587318 0.4049081  0.40399632 0.40309596 0.40212196
 0.40090212 0.39951754 0.39828372 0.3975016  0.3971683  0.39685795
 0.3961498  0.3948307  0.39314055 0.39142773 0.3900004  0.3889523
 0.38815224 0.3874644  0.38686988 0.386263   0.38556615 0.3846806
 0.38344565 0.38199022 0.38063994 0.37976018 0.37945047 0.37950775
 0.3795158  0.3792347  0.37866414 0.3780393  0.37762126 0.37741402
 0.37722602 0.37680164 0.37612212 0.37543708 0.37491283 0.37474713
 0.37493503 0.3753075  0.37556607 0.37546802 0.3749998  0.37427643
 0.3735964  0.3732979  0.37338737 0.37363064 0.373599   0.37308574
 0.37222296 0.37146038 0.3711861  0.37142017 0.37170452 0.37160116
 0.37094393 0.3700169  0.36940914 0.36935034 0.3694802  0.36920512
 0.36831188 0.36705455 0.36602232 0.36575034 0.36611024 0.36650887
 0.3664746  0.36582923 0.36486128 0.36409464 0.36369345 0.3635378
 0.36332625 0.36295727 0.36252034 0.36211112 0.361691   0.3610016
 0.35997376 0.3588223  0.35777822 0.35701862 0.35652488 0.356012
 0.35528472 0.35423777 0.35302058 0.35195518 0.35129997 0.350969
 0.3508198  0.3506027  0.3503538  0.35005558 0.34980056 0.34950766
 0.34908098 0.34835684 0.34741586 0.34652522 0.34595844 0.34589252
 0.34608847 0.34628838 0.34617114 0.34560782 0.344768   0.3439811
 0.3434894  0.34343758 0.34370688 0.34398472 0.34399134 0.34368488
 0.34321246 0.34287226 0.3428255  0.34297755 0.3431897  0.34324855
 0.34316337 0.34300467 0.34298217 0.34325266 0.34370983 0.34416148
 0.34427804 0.3438224  0.34295633 0.34206334 0.34150055 0.34141517
 0.34171137 0.3420628  0.34215975 0.34190905 0.34151924 0.3413221
 0.34143284 0.34170172 0.3417725  0.34140247 0.34069863 0.34001178
 0.33968872 0.33977902 0.3399475  0.3398991  0.33943072 0.33879787
 0.338348   0.338254   0.33836997 0.3383724  0.33801433 0.3374149
 0.33681256 0.33652878 0.336562   0.33671385 0.33677748 0.3366289
 0.33637822 0.33621046 0.33617228 0.33616206 0.3360543  0.33588213
 0.33578858 0.33585855 0.33603203 0.336125   0.33605996 0.3357017
 0.33513603 0.33445522 0.3337483  0.33304557 0.3323629  0.33180714
 0.3313016  0.33075768 0.33010715 0.32938617 0.3287407  0.32827023
 0.32795298 0.32764012 0.32705364 0.32612327 0.32507676 0.32415378
 0.32356566 0.32326388 0.32305512 0.3227549  0.32224947 0.32172278
 0.32132167 0.32112706 0.32099643 0.32075527 0.32044995 0.32021153
 0.3202647  0.32060492 0.32094532 0.32101983 0.32072112 0.3202321
 0.31985143 0.31973794 0.3198284  0.32000622 0.3202053  0.32041687
 0.32065517 0.32084042 0.3209215  0.32087442 0.32081133 0.32088625
 0.32114822 0.32144076 0.3216315  0.32164004 0.32152012 0.32135576
 0.32116345 0.32096076 0.32053378 0.3197347  0.3187002  0.3178038
 0.31727976 0.31724548 0.31749356 0.31768328 0.31761426 0.3172231
 0.31668606 0.31620076 0.31591374 0.31576508 0.31568274 0.3155895
 0.3153943  0.31508705 0.31472856 0.31445256 0.3143885  0.31436047
 0.31417707 0.3136459  0.3127723  0.31180477 0.3111233  0.3109884
 0.31132016 0.3116895  0.31160143 0.31082845 0.30963352 0.3085027
 0.30789143 0.3077916  0.30788714 0.3076958  0.30701852 0.30593607
 0.30474195 0.3037599  0.30294758 0.30214503 0.3012295  0.30022517
 0.29925898 0.2984897  0.29782802 0.297155   0.29630613 0.29530627
 0.29432735 0.29356995 0.2929601  0.2922759  0.29144484 0.2905588
 0.2898725  0.28945574 0.28920168 0.28891632 0.28835127 0.28754744
 0.28657782 0.28567234 0.28505427 0.28475115 0.28472075 0.28477457
 0.2847723  0.28456995 0.2840422  0.28322336 0.28232813 0.28161728
 0.2812677  0.28125218 0.28149214 0.281631   0.28142703 0.28090516
 0.28026193 0.27973735 0.27939066 0.27917308 0.2790138  0.27885523
 0.2786946  0.2785986  0.27856016 0.27852216 0.27842364 0.27820745
 0.2779198  0.27769804 0.27770492 0.27783966 0.27792555 0.27783495
 0.27758664 0.27733752 0.27716097 0.27704346 0.27677777 0.2762149
 0.2754634  0.27485508 0.27463177 0.27479497 0.27504975 0.27513948
 0.27489415 0.27446672 0.27412486 0.27404648 0.27406204 0.27403268
 0.27376854 0.2733374  0.272859   0.2725401  0.27253106 0.27277774
 0.27305943 0.27309415 0.27279097 0.2722372  0.27171007 0.2714742
 0.271572   0.27180195 0.27182552 0.27144    0.27060556 0.26935884
 0.26789936 0.26656786 0.26541537 0.26439783 0.26346684 0.26268446
 0.262075   0.26150355 0.26076186 0.25968474 0.25827482 0.25678217
 0.25545305 0.25443226 0.25380972 0.2534586  0.25321835 0.25298613
 0.252634   0.2520882  0.25148743 0.2508875  0.25041744 0.25006962
 0.24973026 0.24939942 0.24904755 0.24875234 0.24865893 0.2487088
 0.24863999 0.24836978 0.24803466 0.24772952 0.24763475 0.2477146
 0.24785022 0.24774444 0.24751891 0.24747373 0.24773695 0.24817953
 0.24852993 0.24863696 0.24848704 0.24838485 0.24865738 0.24939409
 0.25025496 0.250766   0.2506831  0.25019988 0.24967103 0.24944063
 0.24958621 0.24998762 0.2503896  0.25042793 0.250348   0.2503061
 0.25043917 0.2507148  0.25084215 0.25077182 0.25037095 0.24989419
 0.24957484 0.24961598 0.2497723  0.24970147 0.24950513 0.24914178
 0.24888736 0.24874681 0.24864344 0.24847643 0.24839021 0.24839687
 0.24874803 0.24923217 0.24940014 0.2491     0.24840532 0.24762209
 0.24716565 0.24739623 0.24799058 0.24855152 0.24894601 0.24917972
 0.24939023 0.24976167 0.25015563 0.2501926  0.24966131 0.248674
 0.24750091 0.24640435 0.24545802 0.24455637 0.24369739 0.24281885
 0.24192762 0.24102324 0.24033672 0.23960859 0.2388023  0.2379361
 0.23722026 0.23684312 0.23677145 0.23673208 0.23658736 0.23608907
 0.23538251 0.23467304 0.23405603 0.23375736 0.23349233 0.23300555
 0.23232974 0.23160568 0.2311505  0.2308375  0.23049425 0.22980343
 0.22878689 0.22736986 0.22608826 0.22550252 0.22562282 0.22587027
 0.22544962 0.22427143 0.2228417  0.22231214 0.22269927 0.22323924
 0.22277738 0.22115177 0.21971922 0.219837   0.21960095 0.2131191 ]
