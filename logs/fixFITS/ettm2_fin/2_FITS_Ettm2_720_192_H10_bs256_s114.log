Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=90, out_features=114, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  36771840.0
params:  10374.0
Trainable parameters:  10374
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 13.295286655426025
Epoch: 1, Steps: 65 | Train Loss: 0.3929873 Vali Loss: 0.2584507 Test Loss: 0.3421270
Validation loss decreased (inf --> 0.258451).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 12.985856294631958
Epoch: 2, Steps: 65 | Train Loss: 0.3022135 Vali Loss: 0.2262544 Test Loss: 0.2980342
Validation loss decreased (0.258451 --> 0.226254).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 12.493951797485352
Epoch: 3, Steps: 65 | Train Loss: 0.2529289 Vali Loss: 0.2107742 Test Loss: 0.2767284
Validation loss decreased (0.226254 --> 0.210774).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.67099118232727
Epoch: 4, Steps: 65 | Train Loss: 0.2224883 Vali Loss: 0.2019046 Test Loss: 0.2652921
Validation loss decreased (0.210774 --> 0.201905).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.125185489654541
Epoch: 5, Steps: 65 | Train Loss: 0.2005675 Vali Loss: 0.1964557 Test Loss: 0.2584763
Validation loss decreased (0.201905 --> 0.196456).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.60060977935791
Epoch: 6, Steps: 65 | Train Loss: 0.1850177 Vali Loss: 0.1924997 Test Loss: 0.2539446
Validation loss decreased (0.196456 --> 0.192500).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.093920230865479
Epoch: 7, Steps: 65 | Train Loss: 0.1725116 Vali Loss: 0.1896841 Test Loss: 0.2506877
Validation loss decreased (0.192500 --> 0.189684).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.595269680023193
Epoch: 8, Steps: 65 | Train Loss: 0.1621619 Vali Loss: 0.1873844 Test Loss: 0.2481673
Validation loss decreased (0.189684 --> 0.187384).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.786344289779663
Epoch: 9, Steps: 65 | Train Loss: 0.1537085 Vali Loss: 0.1851967 Test Loss: 0.2459806
Validation loss decreased (0.187384 --> 0.185197).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 14.048493385314941
Epoch: 10, Steps: 65 | Train Loss: 0.1467344 Vali Loss: 0.1837550 Test Loss: 0.2443247
Validation loss decreased (0.185197 --> 0.183755).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 14.171513080596924
Epoch: 11, Steps: 65 | Train Loss: 0.1406111 Vali Loss: 0.1821816 Test Loss: 0.2427710
Validation loss decreased (0.183755 --> 0.182182).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 14.214165925979614
Epoch: 12, Steps: 65 | Train Loss: 0.1353622 Vali Loss: 0.1808805 Test Loss: 0.2414706
Validation loss decreased (0.182182 --> 0.180881).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 14.42320990562439
Epoch: 13, Steps: 65 | Train Loss: 0.1308594 Vali Loss: 0.1797512 Test Loss: 0.2403060
Validation loss decreased (0.180881 --> 0.179751).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 13.019981145858765
Epoch: 14, Steps: 65 | Train Loss: 0.1270258 Vali Loss: 0.1784548 Test Loss: 0.2392119
Validation loss decreased (0.179751 --> 0.178455).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 12.534403800964355
Epoch: 15, Steps: 65 | Train Loss: 0.1232434 Vali Loss: 0.1773362 Test Loss: 0.2382619
Validation loss decreased (0.178455 --> 0.177336).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 13.216475009918213
Epoch: 16, Steps: 65 | Train Loss: 0.1201389 Vali Loss: 0.1766648 Test Loss: 0.2373541
Validation loss decreased (0.177336 --> 0.176665).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 12.854762554168701
Epoch: 17, Steps: 65 | Train Loss: 0.1170705 Vali Loss: 0.1757847 Test Loss: 0.2364825
Validation loss decreased (0.176665 --> 0.175785).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 12.746541500091553
Epoch: 18, Steps: 65 | Train Loss: 0.1147427 Vali Loss: 0.1748807 Test Loss: 0.2357475
Validation loss decreased (0.175785 --> 0.174881).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 12.531737565994263
Epoch: 19, Steps: 65 | Train Loss: 0.1121793 Vali Loss: 0.1743028 Test Loss: 0.2350098
Validation loss decreased (0.174881 --> 0.174303).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 13.039147853851318
Epoch: 20, Steps: 65 | Train Loss: 0.1102751 Vali Loss: 0.1735593 Test Loss: 0.2342775
Validation loss decreased (0.174303 --> 0.173559).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 12.78981614112854
Epoch: 21, Steps: 65 | Train Loss: 0.1083082 Vali Loss: 0.1728675 Test Loss: 0.2336975
Validation loss decreased (0.173559 --> 0.172867).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 13.282061576843262
Epoch: 22, Steps: 65 | Train Loss: 0.1065964 Vali Loss: 0.1722190 Test Loss: 0.2331147
Validation loss decreased (0.172867 --> 0.172219).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 12.964683055877686
Epoch: 23, Steps: 65 | Train Loss: 0.1050234 Vali Loss: 0.1717308 Test Loss: 0.2325726
Validation loss decreased (0.172219 --> 0.171731).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 12.503039836883545
Epoch: 24, Steps: 65 | Train Loss: 0.1036809 Vali Loss: 0.1709822 Test Loss: 0.2320576
Validation loss decreased (0.171731 --> 0.170982).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 13.260156631469727
Epoch: 25, Steps: 65 | Train Loss: 0.1023037 Vali Loss: 0.1707344 Test Loss: 0.2316228
Validation loss decreased (0.170982 --> 0.170734).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 12.50806450843811
Epoch: 26, Steps: 65 | Train Loss: 0.1010111 Vali Loss: 0.1700991 Test Loss: 0.2311517
Validation loss decreased (0.170734 --> 0.170099).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 11.714868783950806
Epoch: 27, Steps: 65 | Train Loss: 0.0999780 Vali Loss: 0.1697183 Test Loss: 0.2306766
Validation loss decreased (0.170099 --> 0.169718).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 11.307847738265991
Epoch: 28, Steps: 65 | Train Loss: 0.0990127 Vali Loss: 0.1692395 Test Loss: 0.2303281
Validation loss decreased (0.169718 --> 0.169240).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 14.314530372619629
Epoch: 29, Steps: 65 | Train Loss: 0.0980101 Vali Loss: 0.1687602 Test Loss: 0.2299865
Validation loss decreased (0.169240 --> 0.168760).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 14.860829830169678
Epoch: 30, Steps: 65 | Train Loss: 0.0971497 Vali Loss: 0.1685505 Test Loss: 0.2296408
Validation loss decreased (0.168760 --> 0.168551).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 15.73783278465271
Epoch: 31, Steps: 65 | Train Loss: 0.0961741 Vali Loss: 0.1681800 Test Loss: 0.2293222
Validation loss decreased (0.168551 --> 0.168180).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 15.067939043045044
Epoch: 32, Steps: 65 | Train Loss: 0.0955589 Vali Loss: 0.1678728 Test Loss: 0.2290350
Validation loss decreased (0.168180 --> 0.167873).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 15.530699014663696
Epoch: 33, Steps: 65 | Train Loss: 0.0948540 Vali Loss: 0.1675237 Test Loss: 0.2287795
Validation loss decreased (0.167873 --> 0.167524).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 14.327861309051514
Epoch: 34, Steps: 65 | Train Loss: 0.0939895 Vali Loss: 0.1674034 Test Loss: 0.2285158
Validation loss decreased (0.167524 --> 0.167403).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 12.243828296661377
Epoch: 35, Steps: 65 | Train Loss: 0.0933535 Vali Loss: 0.1668726 Test Loss: 0.2282389
Validation loss decreased (0.167403 --> 0.166873).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 12.851778507232666
Epoch: 36, Steps: 65 | Train Loss: 0.0928408 Vali Loss: 0.1668560 Test Loss: 0.2280184
Validation loss decreased (0.166873 --> 0.166856).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 12.453342914581299
Epoch: 37, Steps: 65 | Train Loss: 0.0922929 Vali Loss: 0.1665990 Test Loss: 0.2278033
Validation loss decreased (0.166856 --> 0.166599).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 12.690442085266113
Epoch: 38, Steps: 65 | Train Loss: 0.0918540 Vali Loss: 0.1661843 Test Loss: 0.2276367
Validation loss decreased (0.166599 --> 0.166184).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 13.065840721130371
Epoch: 39, Steps: 65 | Train Loss: 0.0913015 Vali Loss: 0.1660643 Test Loss: 0.2274285
Validation loss decreased (0.166184 --> 0.166064).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 12.581392288208008
Epoch: 40, Steps: 65 | Train Loss: 0.0909210 Vali Loss: 0.1660080 Test Loss: 0.2272641
Validation loss decreased (0.166064 --> 0.166008).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 12.87673282623291
Epoch: 41, Steps: 65 | Train Loss: 0.0907178 Vali Loss: 0.1657483 Test Loss: 0.2270901
Validation loss decreased (0.166008 --> 0.165748).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 12.324183940887451
Epoch: 42, Steps: 65 | Train Loss: 0.0900666 Vali Loss: 0.1654215 Test Loss: 0.2269064
Validation loss decreased (0.165748 --> 0.165421).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 13.660785913467407
Epoch: 43, Steps: 65 | Train Loss: 0.0899244 Vali Loss: 0.1652462 Test Loss: 0.2267571
Validation loss decreased (0.165421 --> 0.165246).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 12.645777940750122
Epoch: 44, Steps: 65 | Train Loss: 0.0895141 Vali Loss: 0.1652541 Test Loss: 0.2266258
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 12.544429779052734
Epoch: 45, Steps: 65 | Train Loss: 0.0892972 Vali Loss: 0.1648929 Test Loss: 0.2264788
Validation loss decreased (0.165246 --> 0.164893).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 12.39177942276001
Epoch: 46, Steps: 65 | Train Loss: 0.0889344 Vali Loss: 0.1647639 Test Loss: 0.2263377
Validation loss decreased (0.164893 --> 0.164764).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 13.086578845977783
Epoch: 47, Steps: 65 | Train Loss: 0.0888125 Vali Loss: 0.1646225 Test Loss: 0.2262423
Validation loss decreased (0.164764 --> 0.164622).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 12.930501461029053
Epoch: 48, Steps: 65 | Train Loss: 0.0883334 Vali Loss: 0.1645346 Test Loss: 0.2261284
Validation loss decreased (0.164622 --> 0.164535).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 12.837290048599243
Epoch: 49, Steps: 65 | Train Loss: 0.0879910 Vali Loss: 0.1644735 Test Loss: 0.2260130
Validation loss decreased (0.164535 --> 0.164474).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 13.467267036437988
Epoch: 50, Steps: 65 | Train Loss: 0.0877777 Vali Loss: 0.1643459 Test Loss: 0.2259076
Validation loss decreased (0.164474 --> 0.164346).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=90, out_features=114, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  36771840.0
params:  10374.0
Trainable parameters:  10374
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 14.374506950378418
Epoch: 1, Steps: 65 | Train Loss: 0.3070101 Vali Loss: 0.1568688 Test Loss: 0.2183245
Validation loss decreased (inf --> 0.156869).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 14.8959379196167
Epoch: 2, Steps: 65 | Train Loss: 0.2984548 Vali Loss: 0.1547655 Test Loss: 0.2164160
Validation loss decreased (0.156869 --> 0.154765).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 13.649481773376465
Epoch: 3, Steps: 65 | Train Loss: 0.2957173 Vali Loss: 0.1539034 Test Loss: 0.2155964
Validation loss decreased (0.154765 --> 0.153903).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 14.323958158493042
Epoch: 4, Steps: 65 | Train Loss: 0.2934492 Vali Loss: 0.1535081 Test Loss: 0.2151303
Validation loss decreased (0.153903 --> 0.153508).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 13.437604904174805
Epoch: 5, Steps: 65 | Train Loss: 0.2927287 Vali Loss: 0.1531111 Test Loss: 0.2147124
Validation loss decreased (0.153508 --> 0.153111).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 12.720776796340942
Epoch: 6, Steps: 65 | Train Loss: 0.2921908 Vali Loss: 0.1529348 Test Loss: 0.2145182
Validation loss decreased (0.153111 --> 0.152935).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 13.58211874961853
Epoch: 7, Steps: 65 | Train Loss: 0.2914091 Vali Loss: 0.1528859 Test Loss: 0.2145435
Validation loss decreased (0.152935 --> 0.152886).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 13.54914116859436
Epoch: 8, Steps: 65 | Train Loss: 0.2913030 Vali Loss: 0.1526107 Test Loss: 0.2141694
Validation loss decreased (0.152886 --> 0.152611).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 13.5930016040802
Epoch: 9, Steps: 65 | Train Loss: 0.2904799 Vali Loss: 0.1524728 Test Loss: 0.2141633
Validation loss decreased (0.152611 --> 0.152473).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 13.194575786590576
Epoch: 10, Steps: 65 | Train Loss: 0.2908088 Vali Loss: 0.1526223 Test Loss: 0.2139976
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.784093618392944
Epoch: 11, Steps: 65 | Train Loss: 0.2911614 Vali Loss: 0.1525018 Test Loss: 0.2138997
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 12.831365585327148
Epoch: 12, Steps: 65 | Train Loss: 0.2911772 Vali Loss: 0.1524189 Test Loss: 0.2138895
Validation loss decreased (0.152473 --> 0.152419).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 13.132996559143066
Epoch: 13, Steps: 65 | Train Loss: 0.2898014 Vali Loss: 0.1524100 Test Loss: 0.2138388
Validation loss decreased (0.152419 --> 0.152410).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 13.0212242603302
Epoch: 14, Steps: 65 | Train Loss: 0.2909873 Vali Loss: 0.1523558 Test Loss: 0.2137365
Validation loss decreased (0.152410 --> 0.152356).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 13.623732089996338
Epoch: 15, Steps: 65 | Train Loss: 0.2903361 Vali Loss: 0.1523656 Test Loss: 0.2137226
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 13.949512720108032
Epoch: 16, Steps: 65 | Train Loss: 0.2905904 Vali Loss: 0.1522188 Test Loss: 0.2138121
Validation loss decreased (0.152356 --> 0.152219).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 13.296571016311646
Epoch: 17, Steps: 65 | Train Loss: 0.2896601 Vali Loss: 0.1520769 Test Loss: 0.2135950
Validation loss decreased (0.152219 --> 0.152077).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 13.20826244354248
Epoch: 18, Steps: 65 | Train Loss: 0.2897389 Vali Loss: 0.1522858 Test Loss: 0.2136586
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 13.03635311126709
Epoch: 19, Steps: 65 | Train Loss: 0.2894843 Vali Loss: 0.1521591 Test Loss: 0.2136468
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 13.712772130966187
Epoch: 20, Steps: 65 | Train Loss: 0.2890521 Vali Loss: 0.1524114 Test Loss: 0.2136423
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.21752165257930756, mae:0.2918764352798462, rse:0.37752556800842285, corr:[0.5609547  0.5651014  0.5655732  0.56373537 0.5620551  0.5615053
 0.5618713  0.5624362  0.5624945  0.56187165 0.5609512  0.560232
 0.5599754  0.560082   0.5603001  0.56021076 0.5596464  0.55870926
 0.5576815  0.5568018  0.556211   0.555855   0.5556051  0.55528206
 0.5547652  0.5541012  0.55341935 0.5528203  0.5523412  0.5519677
 0.5516057  0.5512192  0.5507157  0.55003107 0.54923517 0.5484747
 0.547814   0.5472823  0.54682523 0.54636544 0.545855   0.5452573
 0.5445971  0.5439112  0.5432588  0.54263467 0.5420254  0.54140544
 0.54066443 0.5397637  0.5387782  0.5378994  0.5372171  0.5367109
 0.5362705  0.53584176 0.5353541  0.5347835  0.5341982  0.53371865
 0.5334302  0.53337103 0.53340334 0.5333712  0.533164   0.53284
 0.5324246  0.5320768  0.5319027  0.53185254 0.5317766  0.53155464
 0.53113407 0.5305579  0.5299045  0.52932465 0.52888376 0.5285245
 0.52812326 0.5276113  0.5268916  0.5260504  0.5252387  0.52461076
 0.52421284 0.5239947  0.5237041  0.52325195 0.52262205 0.521895
 0.52124304 0.5207888  0.5203941  0.5199138  0.5191412  0.5179157
 0.5162963  0.5145789  0.5130161  0.5118006  0.510947   0.51023877
 0.5094565  0.50841665 0.5070628  0.5055356  0.50413626 0.50317234
 0.5025999  0.5022052  0.5017184  0.5009313  0.49992126 0.49877253
 0.4976486  0.49675792 0.49614674 0.4956165  0.49500322 0.4940804
 0.4929568  0.49177578 0.49089205 0.49042538 0.4902367  0.49005228
 0.4896587  0.48894718 0.4879561  0.4868436  0.4858459  0.4850735
 0.4845411  0.48410365 0.48359764 0.48291197 0.4820162  0.48116538
 0.4805142  0.48015463 0.47994936 0.47972402 0.4793283  0.4786048
 0.47763523 0.47654155 0.4757203  0.47520548 0.4748986  0.47447988
 0.4738526  0.4730918  0.47227818 0.4715368  0.47100714 0.4707439
 0.47053167 0.4701904  0.46953508 0.46878573 0.46804965 0.4676317
 0.46752307 0.4675806  0.4675278  0.46720305 0.4664975  0.46557245
 0.46481127 0.4646182  0.46486476 0.4651728  0.4650821  0.46442813
 0.4634332  0.46253753 0.46219173 0.46245116 0.46273497 0.46261913
 0.46165216 0.45992893 0.4582952  0.45766318 0.45829752 0.45948157
 0.46003512 0.45934138 0.4574758  0.45577526 0.45666534 0.45987445]
