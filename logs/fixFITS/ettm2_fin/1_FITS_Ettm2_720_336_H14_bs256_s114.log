Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  77830144.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.593013525009155
Epoch: 1, Steps: 65 | Train Loss: 0.5348567 Vali Loss: 0.2428021 Test Loss: 0.3293977
Validation loss decreased (inf --> 0.242802).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.272003412246704
Epoch: 2, Steps: 65 | Train Loss: 0.4450310 Vali Loss: 0.2209204 Test Loss: 0.3030044
Validation loss decreased (0.242802 --> 0.220920).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.745284795761108
Epoch: 3, Steps: 65 | Train Loss: 0.4229786 Vali Loss: 0.2133911 Test Loss: 0.2933738
Validation loss decreased (0.220920 --> 0.213391).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.651123285293579
Epoch: 4, Steps: 65 | Train Loss: 0.4122045 Vali Loss: 0.2083429 Test Loss: 0.2880734
Validation loss decreased (0.213391 --> 0.208343).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.219436168670654
Epoch: 5, Steps: 65 | Train Loss: 0.4043287 Vali Loss: 0.2055690 Test Loss: 0.2845780
Validation loss decreased (0.208343 --> 0.205569).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.413724899291992
Epoch: 6, Steps: 65 | Train Loss: 0.4000745 Vali Loss: 0.2037537 Test Loss: 0.2820882
Validation loss decreased (0.205569 --> 0.203754).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.381534576416016
Epoch: 7, Steps: 65 | Train Loss: 0.3969832 Vali Loss: 0.2019894 Test Loss: 0.2802081
Validation loss decreased (0.203754 --> 0.201989).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.339896202087402
Epoch: 8, Steps: 65 | Train Loss: 0.3938993 Vali Loss: 0.2006674 Test Loss: 0.2787432
Validation loss decreased (0.201989 --> 0.200667).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.529876947402954
Epoch: 9, Steps: 65 | Train Loss: 0.3923366 Vali Loss: 0.2004283 Test Loss: 0.2775976
Validation loss decreased (0.200667 --> 0.200428).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.518754720687866
Epoch: 10, Steps: 65 | Train Loss: 0.3897829 Vali Loss: 0.1991085 Test Loss: 0.2766654
Validation loss decreased (0.200428 --> 0.199108).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.366574764251709
Epoch: 11, Steps: 65 | Train Loss: 0.3886801 Vali Loss: 0.1988052 Test Loss: 0.2759210
Validation loss decreased (0.199108 --> 0.198805).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.271894693374634
Epoch: 12, Steps: 65 | Train Loss: 0.3866930 Vali Loss: 0.1977593 Test Loss: 0.2753328
Validation loss decreased (0.198805 --> 0.197759).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.237584352493286
Epoch: 13, Steps: 65 | Train Loss: 0.3865217 Vali Loss: 0.1975942 Test Loss: 0.2747487
Validation loss decreased (0.197759 --> 0.197594).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.210984706878662
Epoch: 14, Steps: 65 | Train Loss: 0.3859497 Vali Loss: 0.1971321 Test Loss: 0.2742896
Validation loss decreased (0.197594 --> 0.197132).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.328472137451172
Epoch: 15, Steps: 65 | Train Loss: 0.3847232 Vali Loss: 0.1968668 Test Loss: 0.2738656
Validation loss decreased (0.197132 --> 0.196867).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 10.86805009841919
Epoch: 16, Steps: 65 | Train Loss: 0.3844128 Vali Loss: 0.1964718 Test Loss: 0.2734931
Validation loss decreased (0.196867 --> 0.196472).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.280079126358032
Epoch: 17, Steps: 65 | Train Loss: 0.3836859 Vali Loss: 0.1962937 Test Loss: 0.2731858
Validation loss decreased (0.196472 --> 0.196294).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.97399616241455
Epoch: 18, Steps: 65 | Train Loss: 0.3828859 Vali Loss: 0.1961659 Test Loss: 0.2729618
Validation loss decreased (0.196294 --> 0.196166).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.984227895736694
Epoch: 19, Steps: 65 | Train Loss: 0.3818501 Vali Loss: 0.1953310 Test Loss: 0.2726810
Validation loss decreased (0.196166 --> 0.195331).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 8.99977421760559
Epoch: 20, Steps: 65 | Train Loss: 0.3819379 Vali Loss: 0.1959358 Test Loss: 0.2724802
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.172053813934326
Epoch: 21, Steps: 65 | Train Loss: 0.3813414 Vali Loss: 0.1960065 Test Loss: 0.2722540
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 11.229500770568848
Epoch: 22, Steps: 65 | Train Loss: 0.3814907 Vali Loss: 0.1947534 Test Loss: 0.2720495
Validation loss decreased (0.195331 --> 0.194753).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 11.32019305229187
Epoch: 23, Steps: 65 | Train Loss: 0.3807973 Vali Loss: 0.1956164 Test Loss: 0.2719440
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.564663887023926
Epoch: 24, Steps: 65 | Train Loss: 0.3799946 Vali Loss: 0.1949580 Test Loss: 0.2718041
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 9.278927326202393
Epoch: 25, Steps: 65 | Train Loss: 0.3800725 Vali Loss: 0.1947917 Test Loss: 0.2716497
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2720077633857727, mae:0.32873430848121643, rse:0.4212610721588135, corr:[0.5422045  0.55591744 0.55563694 0.55213046 0.55229247 0.5550857
 0.5572064  0.55654997 0.5548196  0.5542343  0.555048   0.5561928
 0.5563207  0.55531925 0.5542936  0.5540082  0.5542436  0.5542782
 0.55358815 0.5524558  0.5515694  0.55120724 0.55113584 0.5509039
 0.55022866 0.5493366  0.54861665 0.54813606 0.5477385  0.5472251
 0.5465425  0.5458124  0.54514307 0.5445706  0.5440629  0.5435167
 0.5428292  0.5420145  0.5411689  0.54041815 0.5398615  0.5394704
 0.5390946  0.53857744 0.53781766 0.53691536 0.53607184 0.5353679
 0.5347481  0.5340873  0.5333439  0.53260946 0.5320019  0.53154296
 0.53111565 0.5306086  0.52998716 0.5293884  0.5289809  0.52876884
 0.5286061  0.52830756 0.52781785 0.52732027 0.5270361  0.526934
 0.5268284  0.5265575  0.5260935  0.5255888  0.52520984 0.52495885
 0.5246808  0.5242242  0.52360314 0.52299714 0.5225376  0.52217156
 0.5217517  0.5211673  0.52045465 0.51979345 0.5192804  0.5188221
 0.5182575  0.5174883  0.51664484 0.51598454 0.51561886 0.51539785
 0.5150345  0.5143058  0.5132804  0.5121963  0.51116794 0.5100914
 0.5088055  0.50728244 0.50572664 0.50448954 0.5036734  0.5030263
 0.5021591  0.50087106 0.49938235 0.49807674 0.49720818 0.49662292
 0.4959965  0.49514976 0.4941969  0.49335796 0.49271053 0.49205288
 0.4911728  0.49008125 0.48898035 0.4880308  0.4872186  0.4863404
 0.48527068 0.484046   0.48296058 0.48217437 0.4815725  0.48089087
 0.47995585 0.47883922 0.47782263 0.4771133  0.4765893  0.4759162
 0.4748784  0.47360405 0.4724958  0.4718291  0.4714764  0.47105995
 0.4702564  0.46914637 0.4680833  0.467352   0.46685007 0.46618462
 0.4651611  0.46398646 0.46319303 0.46295616 0.4629544  0.46267596
 0.46191612 0.46091178 0.46005338 0.45958278 0.4592824  0.4587377
 0.4577354  0.4565598  0.45578074 0.4556911  0.45596468 0.4560242
 0.4555675  0.45481208 0.45419055 0.45392117 0.4538042  0.45349056
 0.4528879  0.45227078 0.45192736 0.45193094 0.45199767 0.45176703
 0.45115417 0.45042387 0.44990015 0.4496351  0.4493508  0.4487948
 0.4479263  0.44707385 0.44659144 0.44638598 0.44604138 0.44522762
 0.44410408 0.4431733  0.4427663  0.4426536  0.44222075 0.4409702
 0.43902072 0.43707034 0.43570375 0.43499035 0.4344698  0.4335232
 0.4319873  0.43024218 0.42885602 0.42800578 0.42736816 0.42648473
 0.42525187 0.42399803 0.42309928 0.42262655 0.42224413 0.42151436
 0.42020237 0.41867816 0.41749632 0.41684443 0.41643727 0.41580176
 0.41487333 0.41394153 0.4133865  0.41307682 0.41250187 0.4113316
 0.40971485 0.40821376 0.4072964  0.40675688 0.40611842 0.40506652
 0.40365505 0.40243447 0.4016934  0.4013037  0.4007667  0.3998193
 0.39866394 0.3978611  0.39760876 0.39759955 0.3972774  0.39635158
 0.39516422 0.39427897 0.39406577 0.39433417 0.39441618 0.39408854
 0.39366463 0.39364886 0.39407915 0.39449024 0.39446667 0.3938811
 0.39314857 0.3928889  0.39312395 0.39344424 0.393292   0.39263597
 0.39192784 0.3916583  0.39186278 0.39207175 0.39174733 0.39090168
 0.39001942 0.38959602 0.38977423 0.39010283 0.39007902 0.38967174
 0.38929838 0.3893109  0.38944188 0.38925758 0.38851494 0.3875949
 0.3872595  0.387614   0.3881121  0.38805595 0.38712788 0.38588077
 0.38516185 0.38535404 0.38590232 0.38582924 0.38466796 0.38280615
 0.38131735 0.3808984  0.3809335  0.3803952  0.3789755  0.37736142
 0.37674662 0.37725642 0.37780184 0.37717858 0.3753397  0.3732659
 0.37248924 0.3731203  0.3739179  0.37324023 0.37105274 0.3689154
 0.36857903 0.3698475  0.3710419  0.37056938 0.3685381  0.36706573
 0.36765984 0.36991242 0.3715314  0.37074122 0.3681385  0.36632514
 0.367066   0.36946663 0.37080628 0.36943844 0.36658013 0.36568263
 0.36823767 0.3717837  0.3728432  0.3702844  0.3675159  0.36905968
 0.37483826 0.3790598  0.37741107 0.37182766 0.37252012 0.38252398]
