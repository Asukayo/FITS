Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  42577920.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.084303617477417
Epoch: 1, Steps: 65 | Train Loss: 0.5524911 Vali Loss: 0.2472039 Test Loss: 0.3355412
Validation loss decreased (inf --> 0.247204).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.864428997039795
Epoch: 2, Steps: 65 | Train Loss: 0.4505906 Vali Loss: 0.2231527 Test Loss: 0.3039948
Validation loss decreased (0.247204 --> 0.223153).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.658604621887207
Epoch: 3, Steps: 65 | Train Loss: 0.4257307 Vali Loss: 0.2139750 Test Loss: 0.2934980
Validation loss decreased (0.223153 --> 0.213975).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.322046756744385
Epoch: 4, Steps: 65 | Train Loss: 0.4134661 Vali Loss: 0.2095506 Test Loss: 0.2880748
Validation loss decreased (0.213975 --> 0.209551).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.384828567504883
Epoch: 5, Steps: 65 | Train Loss: 0.4067568 Vali Loss: 0.2062640 Test Loss: 0.2846698
Validation loss decreased (0.209551 --> 0.206264).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.763433694839478
Epoch: 6, Steps: 65 | Train Loss: 0.4018746 Vali Loss: 0.2048357 Test Loss: 0.2821468
Validation loss decreased (0.206264 --> 0.204836).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.213520288467407
Epoch: 7, Steps: 65 | Train Loss: 0.3990622 Vali Loss: 0.2035667 Test Loss: 0.2803811
Validation loss decreased (0.204836 --> 0.203567).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.077709197998047
Epoch: 8, Steps: 65 | Train Loss: 0.3961155 Vali Loss: 0.2017678 Test Loss: 0.2790645
Validation loss decreased (0.203567 --> 0.201768).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.50756549835205
Epoch: 9, Steps: 65 | Train Loss: 0.3930481 Vali Loss: 0.2002361 Test Loss: 0.2779067
Validation loss decreased (0.201768 --> 0.200236).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.543219804763794
Epoch: 10, Steps: 65 | Train Loss: 0.3909871 Vali Loss: 0.2001491 Test Loss: 0.2769726
Validation loss decreased (0.200236 --> 0.200149).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.86902403831482
Epoch: 11, Steps: 65 | Train Loss: 0.3902811 Vali Loss: 0.2001415 Test Loss: 0.2762963
Validation loss decreased (0.200149 --> 0.200141).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.999922513961792
Epoch: 12, Steps: 65 | Train Loss: 0.3889563 Vali Loss: 0.1991860 Test Loss: 0.2756132
Validation loss decreased (0.200141 --> 0.199186).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.1213960647583
Epoch: 13, Steps: 65 | Train Loss: 0.3866307 Vali Loss: 0.1983458 Test Loss: 0.2750329
Validation loss decreased (0.199186 --> 0.198346).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.791270017623901
Epoch: 14, Steps: 65 | Train Loss: 0.3873955 Vali Loss: 0.1975733 Test Loss: 0.2745392
Validation loss decreased (0.198346 --> 0.197573).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.872342109680176
Epoch: 15, Steps: 65 | Train Loss: 0.3862121 Vali Loss: 0.1981847 Test Loss: 0.2741553
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.103893995285034
Epoch: 16, Steps: 65 | Train Loss: 0.3857990 Vali Loss: 0.1979401 Test Loss: 0.2738565
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.53275465965271
Epoch: 17, Steps: 65 | Train Loss: 0.3850284 Vali Loss: 0.1969789 Test Loss: 0.2735212
Validation loss decreased (0.197573 --> 0.196979).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.47230315208435
Epoch: 18, Steps: 65 | Train Loss: 0.3841381 Vali Loss: 0.1962751 Test Loss: 0.2731867
Validation loss decreased (0.196979 --> 0.196275).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 8.933310270309448
Epoch: 19, Steps: 65 | Train Loss: 0.3838121 Vali Loss: 0.1965755 Test Loss: 0.2729623
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.085850954055786
Epoch: 20, Steps: 65 | Train Loss: 0.3833667 Vali Loss: 0.1965387 Test Loss: 0.2726988
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.604720115661621
Epoch: 21, Steps: 65 | Train Loss: 0.3830907 Vali Loss: 0.1967277 Test Loss: 0.2725103
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.27311909198760986, mae:0.32966411113739014, rse:0.42212074995040894, corr:[0.54106355 0.55251735 0.5590278  0.55781513 0.5545935  0.5532379
 0.55395544 0.5556897  0.55717087 0.55738586 0.5563117  0.5549127
 0.55408853 0.55405694 0.5544711  0.5548461  0.5547308  0.55396366
 0.55282235 0.5517785  0.5511305  0.5508728  0.5507839  0.5506208
 0.55020344 0.54948467 0.54861003 0.5477878  0.5471417  0.5466419
 0.5462078  0.5457372  0.5451321  0.5443939  0.5435885  0.54279536
 0.5420236  0.54125303 0.54051495 0.5398659  0.53931326 0.5388164
 0.53830963 0.5377316  0.537018   0.53617203 0.535279   0.53444034
 0.53372484 0.53312624 0.532607   0.5320774  0.53143895 0.53071976
 0.5300222  0.52944475 0.5290096  0.52868646 0.52840894 0.5281084
 0.527747   0.52733654 0.5269217  0.5265582  0.5262681  0.52600646
 0.5257276  0.52539754 0.525015   0.5246162  0.52425164 0.5239494
 0.5236749  0.5233459  0.5229016  0.52232414 0.5216495  0.52096933
 0.5203774  0.5198931  0.5194511  0.5189676  0.51838034 0.51768667
 0.51695293 0.5162688  0.5157244  0.5153309  0.5150089  0.5146317
 0.5140724  0.5132162  0.5120867  0.51080316 0.5095149  0.50830615
 0.5071941  0.5061187  0.50496995 0.50370735 0.5023864  0.5011459
 0.5000917  0.49920925 0.49842754 0.49759424 0.49665827 0.49563262
 0.49461004 0.49370226 0.49298725 0.492425   0.49188387 0.4911613
 0.49014345 0.48888174 0.48755845 0.48637396 0.4854785  0.48484564
 0.484338   0.48373318 0.4829296  0.4818994  0.48073962 0.47965738
 0.47880834 0.478171   0.47756603 0.47679815 0.47574854 0.47444066
 0.47307086 0.47188297 0.4710551  0.4705616  0.47021294 0.46978995
 0.46911308 0.4681792  0.46708518 0.46602908 0.46517476 0.4645215
 0.46398315 0.4634013  0.46274596 0.46201664 0.4613106  0.46070412
 0.46022955 0.45980856 0.4592498  0.4584596  0.45746526 0.45646083
 0.4556549  0.45518243 0.45502788 0.45503417 0.45495182 0.45458958
 0.4539148  0.45309123 0.4523729  0.4519587  0.45185146 0.45188907
 0.4518475  0.45154956 0.450873   0.44995052 0.4490802  0.4485395
 0.44842017 0.44858226 0.44872043 0.4485586  0.44793966 0.44698337
 0.44591135 0.4450109  0.44449794 0.44430834 0.44418284 0.44382495
 0.44308776 0.44196627 0.44059265 0.43917137 0.4378921  0.4368119
 0.4358981  0.43505248 0.4340725  0.4328717  0.43157074 0.43031496
 0.42923245 0.4283224  0.42746398 0.42650965 0.4253753  0.42409578
 0.42283985 0.4217916  0.4210088  0.42048648 0.42013484 0.41980246
 0.41922274 0.41831657 0.4171755  0.41600794 0.4150255  0.4142379
 0.41355315 0.41277465 0.41182268 0.41066423 0.40933135 0.4079825
 0.40677375 0.40579128 0.40504393 0.40432894 0.40353027 0.40264487
 0.40164936 0.40070853 0.39986795 0.39919776 0.39864787 0.39815482
 0.39762858 0.39708793 0.39653617 0.39604092 0.39563677 0.39528623
 0.39494577 0.39449972 0.39397806 0.39353454 0.39316988 0.39295635
 0.39289305 0.39293182 0.3929606  0.39290252 0.39284548 0.39281616
 0.39281547 0.39285645 0.3928014  0.39261448 0.39227873 0.39188576
 0.39154014 0.39128825 0.3911423  0.39105052 0.39089572 0.39063543
 0.39024395 0.38974762 0.38932735 0.38910162 0.38907167 0.38915417
 0.3892286  0.38922217 0.38901538 0.38865682 0.3882126  0.3877901
 0.3875524  0.38740596 0.38725036 0.3870554  0.3867537  0.38637945
 0.3859509  0.38553524 0.38514537 0.38472646 0.38418496 0.38334736
 0.38224292 0.38110217 0.3799875  0.37893245 0.37801477 0.37723053
 0.37667602 0.3762395  0.3757478  0.37506762 0.37427202 0.3733156
 0.37249592 0.37196746 0.37188965 0.37204444 0.37223017 0.37218127
 0.37175518 0.37079132 0.36954382 0.36844906 0.3678795  0.3681596
 0.36888096 0.3695896  0.36973062 0.36901557 0.36771128 0.3664909
 0.36590657 0.36628765 0.3673073  0.36816058 0.3680529  0.36691287
 0.3653707  0.3645817  0.36551288 0.36785728 0.370525   0.3718972
 0.37131312 0.36904866 0.3673739  0.3696701  0.3762368  0.38288516]
