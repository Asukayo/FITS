Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10067456.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3348993
	speed: 0.1409s/iter; left time: 3627.7836s
	iters: 200, epoch: 1 | loss: 0.3630628
	speed: 0.1475s/iter; left time: 3782.5278s
	iters: 300, epoch: 1 | loss: 0.2778495
	speed: 0.1346s/iter; left time: 3438.0176s
	iters: 400, epoch: 1 | loss: 0.3197910
	speed: 0.1346s/iter; left time: 3425.4831s
	iters: 500, epoch: 1 | loss: 0.3282438
	speed: 0.1349s/iter; left time: 3419.9043s
Epoch: 1 cost time: 71.06306910514832
Epoch: 1, Steps: 517 | Train Loss: 0.3885668 Vali Loss: 0.2829018 Test Loss: 0.3762052
Validation loss decreased (inf --> 0.282902).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2779906
	speed: 0.8789s/iter; left time: 22177.9432s
	iters: 200, epoch: 2 | loss: 0.4470721
	speed: 0.1241s/iter; left time: 3119.7135s
	iters: 300, epoch: 2 | loss: 0.3523576
	speed: 0.1282s/iter; left time: 3209.0320s
	iters: 400, epoch: 2 | loss: 0.2502417
	speed: 0.1315s/iter; left time: 3278.6489s
	iters: 500, epoch: 2 | loss: 0.2894787
	speed: 0.1281s/iter; left time: 3180.9352s
Epoch: 2 cost time: 65.38366055488586
Epoch: 2, Steps: 517 | Train Loss: 0.2860839 Vali Loss: 0.2717144 Test Loss: 0.3619419
Validation loss decreased (0.282902 --> 0.271714).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2986332
	speed: 0.9135s/iter; left time: 22579.6717s
	iters: 200, epoch: 3 | loss: 0.2553003
	speed: 0.1491s/iter; left time: 3669.7233s
	iters: 300, epoch: 3 | loss: 0.3105544
	speed: 0.1516s/iter; left time: 3717.9505s
	iters: 400, epoch: 3 | loss: 0.2685573
	speed: 0.1525s/iter; left time: 3724.6852s
	iters: 500, epoch: 3 | loss: 0.3097520
	speed: 0.1500s/iter; left time: 3648.3648s
Epoch: 3 cost time: 77.86921191215515
Epoch: 3, Steps: 517 | Train Loss: 0.2654953 Vali Loss: 0.2666644 Test Loss: 0.3561809
Validation loss decreased (0.271714 --> 0.266664).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5174723
	speed: 0.9936s/iter; left time: 24044.8939s
	iters: 200, epoch: 4 | loss: 0.2064184
	speed: 0.1369s/iter; left time: 3299.7179s
	iters: 300, epoch: 4 | loss: 0.4184319
	speed: 0.1326s/iter; left time: 3182.1360s
	iters: 400, epoch: 4 | loss: 0.3545181
	speed: 0.1424s/iter; left time: 3403.0729s
	iters: 500, epoch: 4 | loss: 0.3165760
	speed: 0.1407s/iter; left time: 3348.5287s
Epoch: 4 cost time: 72.72126007080078
Epoch: 4, Steps: 517 | Train Loss: 0.2593983 Vali Loss: 0.2647269 Test Loss: 0.3538425
Validation loss decreased (0.266664 --> 0.264727).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3100393
	speed: 0.9291s/iter; left time: 22003.6890s
	iters: 200, epoch: 5 | loss: 0.2645724
	speed: 0.1446s/iter; left time: 3409.6926s
	iters: 300, epoch: 5 | loss: 0.2012346
	speed: 0.1349s/iter; left time: 3166.9357s
	iters: 400, epoch: 5 | loss: 0.3144557
	speed: 0.1408s/iter; left time: 3293.0364s
	iters: 500, epoch: 5 | loss: 0.3165630
	speed: 0.1305s/iter; left time: 3038.7576s
Epoch: 5 cost time: 72.50558590888977
Epoch: 5, Steps: 517 | Train Loss: 0.2574051 Vali Loss: 0.2638037 Test Loss: 0.3524600
Validation loss decreased (0.264727 --> 0.263804).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1847854
	speed: 0.9132s/iter; left time: 21155.6440s
	iters: 200, epoch: 6 | loss: 0.2994813
	speed: 0.1424s/iter; left time: 3285.3076s
	iters: 300, epoch: 6 | loss: 0.3643703
	speed: 0.1470s/iter; left time: 3376.9765s
	iters: 400, epoch: 6 | loss: 0.2805746
	speed: 0.1353s/iter; left time: 3093.6240s
	iters: 500, epoch: 6 | loss: 0.1532581
	speed: 0.1310s/iter; left time: 2981.4297s
Epoch: 6 cost time: 72.3172607421875
Epoch: 6, Steps: 517 | Train Loss: 0.2567067 Vali Loss: 0.2634302 Test Loss: 0.3518425
Validation loss decreased (0.263804 --> 0.263430).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2628303
	speed: 0.8847s/iter; left time: 20038.6230s
	iters: 200, epoch: 7 | loss: 0.1839524
	speed: 0.1366s/iter; left time: 3079.5500s
	iters: 300, epoch: 7 | loss: 0.1751726
	speed: 0.1505s/iter; left time: 3377.9306s
	iters: 400, epoch: 7 | loss: 0.2611648
	speed: 0.1663s/iter; left time: 3717.1957s
	iters: 500, epoch: 7 | loss: 0.1799357
	speed: 0.1649s/iter; left time: 3668.4462s
Epoch: 7 cost time: 79.90386414527893
Epoch: 7, Steps: 517 | Train Loss: 0.2562838 Vali Loss: 0.2631682 Test Loss: 0.3518792
Validation loss decreased (0.263430 --> 0.263168).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1537080
	speed: 0.9176s/iter; left time: 20308.3794s
	iters: 200, epoch: 8 | loss: 0.1874492
	speed: 0.1577s/iter; left time: 3475.2286s
	iters: 300, epoch: 8 | loss: 0.2767957
	speed: 0.1418s/iter; left time: 3110.3170s
	iters: 400, epoch: 8 | loss: 0.2529839
	speed: 0.1524s/iter; left time: 3326.7772s
	iters: 500, epoch: 8 | loss: 0.2009674
	speed: 0.1449s/iter; left time: 3149.7841s
Epoch: 8 cost time: 78.41267371177673
Epoch: 8, Steps: 517 | Train Loss: 0.2562790 Vali Loss: 0.2630981 Test Loss: 0.3519050
Validation loss decreased (0.263168 --> 0.263098).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5528018
	speed: 0.8575s/iter; left time: 18533.9511s
	iters: 200, epoch: 9 | loss: 0.2755095
	speed: 0.1228s/iter; left time: 2642.7949s
	iters: 300, epoch: 9 | loss: 0.2145274
	speed: 0.1226s/iter; left time: 2626.4767s
	iters: 400, epoch: 9 | loss: 0.3003282
	speed: 0.1419s/iter; left time: 3024.1112s
	iters: 500, epoch: 9 | loss: 0.2383272
	speed: 0.1422s/iter; left time: 3016.9763s
Epoch: 9 cost time: 69.36912560462952
Epoch: 9, Steps: 517 | Train Loss: 0.2561959 Vali Loss: 0.2627224 Test Loss: 0.3518152
Validation loss decreased (0.263098 --> 0.262722).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1974275
	speed: 1.0302s/iter; left time: 21736.0322s
	iters: 200, epoch: 10 | loss: 0.1709186
	speed: 0.1122s/iter; left time: 2356.3218s
	iters: 300, epoch: 10 | loss: 0.3684158
	speed: 0.1348s/iter; left time: 2816.6347s
	iters: 400, epoch: 10 | loss: 0.3611449
	speed: 0.1355s/iter; left time: 2817.5744s
	iters: 500, epoch: 10 | loss: 0.2596163
	speed: 0.1417s/iter; left time: 2933.8952s
Epoch: 10 cost time: 70.6872386932373
Epoch: 10, Steps: 517 | Train Loss: 0.2560640 Vali Loss: 0.2627829 Test Loss: 0.3514782
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2620143
	speed: 0.9190s/iter; left time: 18914.1439s
	iters: 200, epoch: 11 | loss: 0.2591250
	speed: 0.1342s/iter; left time: 2748.1934s
	iters: 300, epoch: 11 | loss: 0.2143038
	speed: 0.1222s/iter; left time: 2491.3387s
	iters: 400, epoch: 11 | loss: 0.2563577
	speed: 0.1463s/iter; left time: 2966.3100s
	iters: 500, epoch: 11 | loss: 0.1803692
	speed: 0.1551s/iter; left time: 3129.7761s
Epoch: 11 cost time: 73.41955351829529
Epoch: 11, Steps: 517 | Train Loss: 0.2560134 Vali Loss: 0.2633381 Test Loss: 0.3512978
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3405356
	speed: 1.0275s/iter; left time: 20615.3146s
	iters: 200, epoch: 12 | loss: 0.1909817
	speed: 0.1617s/iter; left time: 3227.5270s
	iters: 300, epoch: 12 | loss: 0.3276922
	speed: 0.1644s/iter; left time: 3264.7848s
	iters: 400, epoch: 12 | loss: 0.3109088
	speed: 0.1540s/iter; left time: 3042.6921s
	iters: 500, epoch: 12 | loss: 0.2571283
	speed: 0.1630s/iter; left time: 3205.4166s
Epoch: 12 cost time: 83.35772681236267
Epoch: 12, Steps: 517 | Train Loss: 0.2558552 Vali Loss: 0.2630672 Test Loss: 0.3514743
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10067456.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4664876
	speed: 0.1766s/iter; left time: 4547.1255s
	iters: 200, epoch: 1 | loss: 0.3969132
	speed: 0.1691s/iter; left time: 4337.0788s
	iters: 300, epoch: 1 | loss: 0.6154352
	speed: 0.1722s/iter; left time: 4398.8983s
	iters: 400, epoch: 1 | loss: 0.7567745
	speed: 0.1619s/iter; left time: 4120.0378s
	iters: 500, epoch: 1 | loss: 0.2424228
	speed: 0.1459s/iter; left time: 3698.9396s
Epoch: 1 cost time: 84.89227104187012
Epoch: 1, Steps: 517 | Train Loss: 0.4975293 Vali Loss: 0.2615048 Test Loss: 0.3506637
Validation loss decreased (inf --> 0.261505).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3809645
	speed: 0.8231s/iter; left time: 20769.7978s
	iters: 200, epoch: 2 | loss: 0.4039953
	speed: 0.0986s/iter; left time: 2478.2390s
	iters: 300, epoch: 2 | loss: 0.6138470
	speed: 0.0952s/iter; left time: 2382.5030s
	iters: 400, epoch: 2 | loss: 0.4998139
	speed: 0.1114s/iter; left time: 2778.5860s
	iters: 500, epoch: 2 | loss: 0.4734975
	speed: 0.1040s/iter; left time: 2583.6616s
Epoch: 2 cost time: 53.521137952804565
Epoch: 2, Steps: 517 | Train Loss: 0.4961528 Vali Loss: 0.2606479 Test Loss: 0.3506794
Validation loss decreased (0.261505 --> 0.260648).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3184713
	speed: 0.6626s/iter; left time: 16377.7438s
	iters: 200, epoch: 3 | loss: 0.3908963
	speed: 0.1027s/iter; left time: 2528.5151s
	iters: 300, epoch: 3 | loss: 0.5717517
	speed: 0.1018s/iter; left time: 2495.6864s
	iters: 400, epoch: 3 | loss: 0.5829496
	speed: 0.0982s/iter; left time: 2397.3479s
	iters: 500, epoch: 3 | loss: 0.6669863
	speed: 0.1011s/iter; left time: 2458.8293s
Epoch: 3 cost time: 53.923455238342285
Epoch: 3, Steps: 517 | Train Loss: 0.4954781 Vali Loss: 0.2609707 Test Loss: 0.3498151
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4651998
	speed: 0.6332s/iter; left time: 15324.0037s
	iters: 200, epoch: 4 | loss: 0.4992350
	speed: 0.0962s/iter; left time: 2317.5614s
	iters: 300, epoch: 4 | loss: 0.7718037
	speed: 0.1025s/iter; left time: 2459.7877s
	iters: 400, epoch: 4 | loss: 0.5671934
	speed: 0.0863s/iter; left time: 2063.4574s
	iters: 500, epoch: 4 | loss: 0.3759921
	speed: 0.0970s/iter; left time: 2307.5648s
Epoch: 4 cost time: 51.53559422492981
Epoch: 4, Steps: 517 | Train Loss: 0.4947742 Vali Loss: 0.2598013 Test Loss: 0.3505607
Validation loss decreased (0.260648 --> 0.259801).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4233631
	speed: 0.6814s/iter; left time: 16138.2879s
	iters: 200, epoch: 5 | loss: 0.3205813
	speed: 0.1033s/iter; left time: 2436.3901s
	iters: 300, epoch: 5 | loss: 0.4887598
	speed: 0.1125s/iter; left time: 2642.0587s
	iters: 400, epoch: 5 | loss: 0.3376600
	speed: 0.0995s/iter; left time: 2326.2629s
	iters: 500, epoch: 5 | loss: 0.8823565
	speed: 0.1104s/iter; left time: 2570.9158s
Epoch: 5 cost time: 56.58390212059021
Epoch: 5, Steps: 517 | Train Loss: 0.4944851 Vali Loss: 0.2611442 Test Loss: 0.3494928
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5597786
	speed: 0.6554s/iter; left time: 15182.3355s
	iters: 200, epoch: 6 | loss: 0.5787376
	speed: 0.0737s/iter; left time: 1700.5426s
	iters: 300, epoch: 6 | loss: 0.6631560
	speed: 0.0961s/iter; left time: 2206.1836s
	iters: 400, epoch: 6 | loss: 0.4698788
	speed: 0.1077s/iter; left time: 2462.7307s
	iters: 500, epoch: 6 | loss: 0.5718191
	speed: 0.1254s/iter; left time: 2853.9433s
Epoch: 6 cost time: 53.62537503242493
Epoch: 6, Steps: 517 | Train Loss: 0.4946022 Vali Loss: 0.2603525 Test Loss: 0.3495207
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.7659826
	speed: 0.9535s/iter; left time: 21596.7854s
	iters: 200, epoch: 7 | loss: 0.5978902
	speed: 0.1422s/iter; left time: 3207.1504s
	iters: 300, epoch: 7 | loss: 0.2852798
	speed: 0.1489s/iter; left time: 3342.0865s
	iters: 400, epoch: 7 | loss: 0.5977208
	speed: 0.1586s/iter; left time: 3545.4431s
	iters: 500, epoch: 7 | loss: 0.7239526
	speed: 0.1529s/iter; left time: 3402.1923s
Epoch: 7 cost time: 79.24830603599548
Epoch: 7, Steps: 517 | Train Loss: 0.4945201 Vali Loss: 0.2604940 Test Loss: 0.3497714
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3499101996421814, mae:0.3781724274158478, rse:0.47546952962875366, corr:[0.5413724  0.5462533  0.54438204 0.5411307  0.5398224  0.54042345
 0.5414927  0.5416042  0.5405534  0.5392643  0.5386095  0.5387439
 0.53920114 0.5393133  0.5387972  0.53785795 0.5369657  0.536408
 0.5361274  0.535844   0.5353753  0.5347323  0.53410816 0.5336626
 0.53336024 0.5330453  0.5325347  0.53180474 0.53101397 0.53036225
 0.52991515 0.52959025 0.52916795 0.528497   0.52765006 0.5268185
 0.526116   0.5255474  0.52499163 0.5243674  0.5236695  0.522957
 0.5223415  0.52186036 0.5214665  0.52101403 0.5203684  0.51950383
 0.5185161  0.5175775  0.5168517  0.51637566 0.5160202  0.515645
 0.5151631  0.5145616  0.51390743 0.51329404 0.51278025 0.51233274
 0.5119503  0.5116006  0.51128274 0.5110194  0.51083565 0.5107407
 0.5106077  0.5104274  0.51016486 0.50982016 0.50943196 0.5090329
 0.50861055 0.50810987 0.5075418  0.5070159  0.5066403  0.50640297
 0.5062377  0.50593483 0.5053283  0.50445133 0.503475   0.5025888
 0.5019216  0.5014605  0.50108784 0.5007367  0.5003186  0.49985942
 0.49940613 0.4989224  0.4983291  0.49751154 0.49635017 0.49484214
 0.49312812 0.49144045 0.48992833 0.48867512 0.48763877 0.4867013
 0.48575354 0.48475626 0.48371792 0.48268512 0.48166317 0.4806137
 0.4794538  0.4782319  0.4771042  0.4762379  0.47572115 0.47541156
 0.47511378 0.4746976  0.47410974 0.47334227 0.47247013 0.47152156
 0.47052303 0.46939012 0.46809503 0.466625   0.4651315  0.4638975
 0.4631482  0.46278995 0.46251982 0.46204534 0.46126005 0.46019113
 0.4590001  0.457894   0.4570945  0.4566986  0.4566145  0.45665535
 0.4565786  0.45624736 0.45563853 0.45481864 0.45389238 0.45294344
 0.45207205 0.4512556  0.45048895 0.4496554  0.4487075  0.44764745
 0.4466193  0.4458118  0.445235   0.44478232 0.44427943 0.44363818
 0.44289637 0.44218042 0.44159758 0.44121218 0.4408654  0.44038662
 0.43967688 0.43887234 0.43822166 0.43788874 0.43780708 0.4377491
 0.4374378  0.4367188  0.43561444 0.4345543  0.43397075 0.4340159
 0.43439433 0.4346807  0.43459624 0.43410987 0.43338427 0.4326543
 0.43196067 0.43121135 0.43028972 0.4291341  0.42787558 0.42678222
 0.42611727 0.42577475 0.42540026 0.42455134 0.4230417  0.4210415
 0.41908586 0.41779816 0.41715539 0.41679344 0.41628355 0.41531003
 0.41389397 0.41231677 0.41098362 0.41013697 0.40972465 0.4094926
 0.40916374 0.4085374  0.40755275 0.40632197 0.4050875  0.40403435
 0.40313005 0.4023788  0.40176997 0.4012173  0.4005586  0.39950687
 0.39791286 0.3958047  0.39356816 0.39159575 0.39017263 0.3893565
 0.38894236 0.38862547 0.3882355  0.38764673 0.3869512  0.38628903
 0.38560697 0.38486195 0.38398826 0.38301983 0.38206744 0.38126108
 0.38056946 0.3799108  0.3791486  0.3782748  0.37749457 0.37701717
 0.37692696 0.37697306 0.37679535 0.3762055  0.37507838 0.37374598
 0.3726917  0.3723056  0.37253666 0.3729568  0.37315792 0.37289673
 0.3723026  0.37182432 0.37175944 0.37215686 0.37266064 0.3728789
 0.37254542 0.37175232 0.37087834 0.37039298 0.37040493 0.37068048
 0.37070972 0.37006518 0.36879733 0.3673626  0.3663808  0.36624885
 0.36696637 0.36805624 0.36889553 0.36913034 0.3686627  0.36765388
 0.36643684 0.3651603  0.3639044  0.36282325 0.36199206 0.36161876
 0.36174598 0.36225107 0.3627893  0.36297014 0.362604   0.36170116
 0.3606481  0.35991192 0.35954878 0.35926563 0.35866505 0.35744682
 0.35572833 0.3538375  0.35227284 0.35146263 0.35151085 0.35203424
 0.35258156 0.3526454  0.35207257 0.35084477 0.34938803 0.34818953
 0.34762087 0.34761965 0.347936   0.34821418 0.34813216 0.34766367
 0.34685674 0.3460578  0.3454805  0.3451645  0.3450158  0.34485987
 0.34451556 0.34407854 0.34382105 0.343991   0.34464827 0.3456228
 0.34652403 0.34699732 0.34682867 0.3460363  0.34503964 0.34427658
 0.3441184  0.34450904 0.34518725 0.3458307  0.34609663 0.34594265
 0.34540385 0.34458095 0.34376976 0.34325656 0.34316626 0.34344822
 0.3439596  0.34445727 0.34469458 0.34451637 0.34394252 0.34317237
 0.34241232 0.3418507  0.3415006  0.341268   0.34109586 0.34095758
 0.3408935  0.3409273  0.34090698 0.34074363 0.3402488  0.33953002
 0.33881244 0.33834553 0.33822837 0.33832446 0.33831683 0.3380231
 0.33738413 0.33674505 0.33650514 0.33690935 0.3378409  0.33882922
 0.3394353  0.33945078 0.33896628 0.33834895 0.33800533 0.33814234
 0.33860892 0.33898142 0.3388588  0.3381059  0.3370798  0.33614737
 0.33565247 0.33554173 0.33549303 0.3351343  0.33432436 0.33332768
 0.33242863 0.33182654 0.33145756 0.3310948  0.33056232 0.32986012
 0.3292416  0.32900485 0.32914555 0.32950306 0.32984462 0.32978588
 0.32917234 0.32813632 0.3270524  0.3262176  0.32557982 0.325044
 0.32446933 0.3239365  0.323599   0.3235709  0.32384512 0.32411495
 0.3241141  0.32371378 0.32298282 0.32223633 0.32177174 0.3217301
 0.3219937  0.32222766 0.32214713 0.32177836 0.32141924 0.32139683
 0.32185814 0.3225948  0.3232779  0.32357645 0.32341474 0.32297245
 0.32255328 0.3223283  0.32238334 0.32264248 0.32296365 0.3231613
 0.32310736 0.32289737 0.32247958 0.32175326 0.3207568  0.31971774
 0.3188226  0.31835198 0.31835574 0.31867483 0.31910506 0.31937236
 0.31934166 0.3189766  0.31840506 0.3177343  0.31713802 0.31674528
 0.31655213 0.31651714 0.31656918 0.31667736 0.31688446 0.31704915
 0.31712812 0.31706417 0.316842   0.31647632 0.31598294 0.3153621
 0.314607   0.31369868 0.31268    0.31171778 0.31110525 0.31097433
 0.31130177 0.31177825 0.31205946 0.31176177 0.31077042 0.30922806
 0.3075032  0.30608964 0.30512494 0.3044991  0.3039125  0.3030511
 0.30178532 0.30032805 0.29899234 0.2981079  0.29766488 0.29742754
 0.2971182  0.29664692 0.29600877 0.29527113 0.29460508 0.29409087
 0.29370224 0.29323843 0.29256105 0.29169464 0.29063225 0.28954682
 0.28847045 0.28751332 0.28680903 0.2863657  0.28616482 0.28598678
 0.28561628 0.28491518 0.28390214 0.28282106 0.28203335 0.28177503
 0.28200042 0.28239775 0.28272933 0.28267163 0.28214017 0.28130844
 0.28046355 0.27990675 0.27972004 0.2798209  0.2800254  0.2801034
 0.27989233 0.27943683 0.27889508 0.27850306 0.27842432 0.27859974
 0.27882826 0.27892154 0.27885023 0.27858913 0.27825165 0.2779971
 0.27790654 0.27795282 0.27797335 0.277885   0.27762926 0.27725524
 0.27691028 0.2767598  0.276815   0.27696645 0.27700907 0.27687678
 0.27651498 0.2760289  0.27555537 0.275217   0.27494946 0.2748389
 0.27482992 0.2748778  0.27481538 0.2745725  0.2742395  0.27398995
 0.27400953 0.274311   0.27476576 0.27508566 0.2750516  0.27460513
 0.27384028 0.27293217 0.2719757  0.27103463 0.27007467 0.26896265
 0.26773974 0.2667397  0.2660782  0.26567402 0.2652718  0.26465762
 0.26372737 0.26252183 0.26124194 0.26011547 0.25927758 0.2587683
 0.2584286  0.25802332 0.2574216  0.2564969  0.25528586 0.25402823
 0.2529928  0.25240386 0.2524413  0.2529048  0.2534732  0.25372782
 0.25334078 0.25233185 0.2509288  0.24955511 0.24867013 0.24841554
 0.24856766 0.24885288 0.24907745 0.249039   0.24877457 0.24832231
 0.24779834 0.24712873 0.24649075 0.24615848 0.24629375 0.24685283
 0.2475963  0.24827562 0.24864888 0.24877992 0.24891344 0.24933101
 0.25003004 0.25074372 0.2511502  0.25116777 0.25089952 0.25064775
 0.25069782 0.25120756 0.2520538  0.2527021  0.2530546  0.25293523
 0.25240633 0.25171873 0.2510762  0.2507818  0.25072145 0.25085375
 0.25104704 0.25129935 0.2514053  0.25115386 0.25069493 0.24998876
 0.24935077 0.2489919  0.24908906 0.24958713 0.25032756 0.25084254
 0.25103346 0.2508085  0.25025967 0.24979717 0.24968286 0.24989736
 0.25022838 0.25051793 0.2504284  0.24994296 0.24935794 0.2488979
 0.24869649 0.24881883 0.24902742 0.24888925 0.24816339 0.24692437
 0.24542093 0.24397524 0.24278763 0.2418454  0.24110076 0.24039096
 0.23964398 0.2388936  0.23845561 0.23819637 0.23799504 0.23762949
 0.23701458 0.23620693 0.23536071 0.23463419 0.23419498 0.23384097
 0.23347504 0.23296182 0.23216611 0.2313313  0.230536   0.22985782
 0.22934474 0.2289031  0.22850524 0.22790278 0.22709249 0.2262151
 0.22566205 0.22548015 0.22575577 0.22620195 0.22627333 0.2256529
 0.22437686 0.22309217 0.22249252 0.22293258 0.22349761 0.22313379
 0.22124216 0.21859314 0.21712725 0.21826398 0.22090742 0.22121066]
