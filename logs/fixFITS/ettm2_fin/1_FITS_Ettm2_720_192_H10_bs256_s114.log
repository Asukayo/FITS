Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=90, out_features=114, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  36771840.0
params:  10374.0
Trainable parameters:  10374
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.987435579299927
Epoch: 1, Steps: 65 | Train Loss: 0.4856968 Vali Loss: 0.2075671 Test Loss: 0.2768280
Validation loss decreased (inf --> 0.207567).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.74736762046814
Epoch: 2, Steps: 65 | Train Loss: 0.3769277 Vali Loss: 0.1830241 Test Loss: 0.2468655
Validation loss decreased (0.207567 --> 0.183024).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.261824369430542
Epoch: 3, Steps: 65 | Train Loss: 0.3492363 Vali Loss: 0.1744078 Test Loss: 0.2370739
Validation loss decreased (0.183024 --> 0.174408).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.25796127319336
Epoch: 4, Steps: 65 | Train Loss: 0.3356613 Vali Loss: 0.1695454 Test Loss: 0.2321382
Validation loss decreased (0.174408 --> 0.169545).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.841237306594849
Epoch: 5, Steps: 65 | Train Loss: 0.3258316 Vali Loss: 0.1663795 Test Loss: 0.2289483
Validation loss decreased (0.169545 --> 0.166379).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.274721622467041
Epoch: 6, Steps: 65 | Train Loss: 0.3208418 Vali Loss: 0.1640516 Test Loss: 0.2265535
Validation loss decreased (0.166379 --> 0.164052).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.024009943008423
Epoch: 7, Steps: 65 | Train Loss: 0.3165270 Vali Loss: 0.1624429 Test Loss: 0.2247306
Validation loss decreased (0.164052 --> 0.162443).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.629887580871582
Epoch: 8, Steps: 65 | Train Loss: 0.3124363 Vali Loss: 0.1612322 Test Loss: 0.2235729
Validation loss decreased (0.162443 --> 0.161232).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.12096118927002
Epoch: 9, Steps: 65 | Train Loss: 0.3096942 Vali Loss: 0.1599862 Test Loss: 0.2223145
Validation loss decreased (0.161232 --> 0.159986).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.846668004989624
Epoch: 10, Steps: 65 | Train Loss: 0.3082793 Vali Loss: 0.1594118 Test Loss: 0.2214395
Validation loss decreased (0.159986 --> 0.159412).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.592478275299072
Epoch: 11, Steps: 65 | Train Loss: 0.3065089 Vali Loss: 0.1586103 Test Loss: 0.2205835
Validation loss decreased (0.159412 --> 0.158610).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.746411800384521
Epoch: 12, Steps: 65 | Train Loss: 0.3047717 Vali Loss: 0.1580530 Test Loss: 0.2200269
Validation loss decreased (0.158610 --> 0.158053).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.57007908821106
Epoch: 13, Steps: 65 | Train Loss: 0.3042460 Vali Loss: 0.1576219 Test Loss: 0.2195661
Validation loss decreased (0.158053 --> 0.157622).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.651160478591919
Epoch: 14, Steps: 65 | Train Loss: 0.3034974 Vali Loss: 0.1570977 Test Loss: 0.2191388
Validation loss decreased (0.157622 --> 0.157098).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.776870727539062
Epoch: 15, Steps: 65 | Train Loss: 0.3017465 Vali Loss: 0.1565843 Test Loss: 0.2187506
Validation loss decreased (0.157098 --> 0.156584).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.937689065933228
Epoch: 16, Steps: 65 | Train Loss: 0.3015409 Vali Loss: 0.1565616 Test Loss: 0.2184705
Validation loss decreased (0.156584 --> 0.156562).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.214983224868774
Epoch: 17, Steps: 65 | Train Loss: 0.3000420 Vali Loss: 0.1562707 Test Loss: 0.2180917
Validation loss decreased (0.156562 --> 0.156271).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.110650062561035
Epoch: 18, Steps: 65 | Train Loss: 0.3002416 Vali Loss: 0.1558930 Test Loss: 0.2177579
Validation loss decreased (0.156271 --> 0.155893).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 8.43555736541748
Epoch: 19, Steps: 65 | Train Loss: 0.2988885 Vali Loss: 0.1558375 Test Loss: 0.2175788
Validation loss decreased (0.155893 --> 0.155838).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.718260288238525
Epoch: 20, Steps: 65 | Train Loss: 0.2988541 Vali Loss: 0.1555950 Test Loss: 0.2172561
Validation loss decreased (0.155838 --> 0.155595).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 7.125026702880859
Epoch: 21, Steps: 65 | Train Loss: 0.2980106 Vali Loss: 0.1554012 Test Loss: 0.2172036
Validation loss decreased (0.155595 --> 0.155401).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.246405124664307
Epoch: 22, Steps: 65 | Train Loss: 0.2976761 Vali Loss: 0.1552522 Test Loss: 0.2170123
Validation loss decreased (0.155401 --> 0.155252).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.541544914245605
Epoch: 23, Steps: 65 | Train Loss: 0.2971238 Vali Loss: 0.1551186 Test Loss: 0.2167828
Validation loss decreased (0.155252 --> 0.155119).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.914055109024048
Epoch: 24, Steps: 65 | Train Loss: 0.2972426 Vali Loss: 0.1548812 Test Loss: 0.2166594
Validation loss decreased (0.155119 --> 0.154881).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.742429256439209
Epoch: 25, Steps: 65 | Train Loss: 0.2967682 Vali Loss: 0.1549223 Test Loss: 0.2165846
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 9.600912094116211
Epoch: 26, Steps: 65 | Train Loss: 0.2964669 Vali Loss: 0.1547251 Test Loss: 0.2164232
Validation loss decreased (0.154881 --> 0.154725).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.981935024261475
Epoch: 27, Steps: 65 | Train Loss: 0.2962795 Vali Loss: 0.1546774 Test Loss: 0.2162739
Validation loss decreased (0.154725 --> 0.154677).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.217579364776611
Epoch: 28, Steps: 65 | Train Loss: 0.2964142 Vali Loss: 0.1545405 Test Loss: 0.2161922
Validation loss decreased (0.154677 --> 0.154540).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.998433113098145
Epoch: 29, Steps: 65 | Train Loss: 0.2958777 Vali Loss: 0.1543411 Test Loss: 0.2161599
Validation loss decreased (0.154540 --> 0.154341).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.61876630783081
Epoch: 30, Steps: 65 | Train Loss: 0.2960054 Vali Loss: 0.1544399 Test Loss: 0.2160440
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 9.276700496673584
Epoch: 31, Steps: 65 | Train Loss: 0.2950151 Vali Loss: 0.1543734 Test Loss: 0.2160189
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.472149848937988
Epoch: 32, Steps: 65 | Train Loss: 0.2952775 Vali Loss: 0.1542895 Test Loss: 0.2159143
Validation loss decreased (0.154341 --> 0.154290).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.693538188934326
Epoch: 33, Steps: 65 | Train Loss: 0.2952265 Vali Loss: 0.1542255 Test Loss: 0.2159030
Validation loss decreased (0.154290 --> 0.154225).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 9.096616268157959
Epoch: 34, Steps: 65 | Train Loss: 0.2941819 Vali Loss: 0.1543178 Test Loss: 0.2157932
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 9.147788763046265
Epoch: 35, Steps: 65 | Train Loss: 0.2939606 Vali Loss: 0.1540562 Test Loss: 0.2157509
Validation loss decreased (0.154225 --> 0.154056).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.490508794784546
Epoch: 36, Steps: 65 | Train Loss: 0.2940090 Vali Loss: 0.1541981 Test Loss: 0.2156850
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 8.089528322219849
Epoch: 37, Steps: 65 | Train Loss: 0.2938438 Vali Loss: 0.1541425 Test Loss: 0.2156354
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 8.507459878921509
Epoch: 38, Steps: 65 | Train Loss: 0.2939875 Vali Loss: 0.1539259 Test Loss: 0.2156262
Validation loss decreased (0.154056 --> 0.153926).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 8.400484561920166
Epoch: 39, Steps: 65 | Train Loss: 0.2935963 Vali Loss: 0.1539589 Test Loss: 0.2155706
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 7.98052716255188
Epoch: 40, Steps: 65 | Train Loss: 0.2937705 Vali Loss: 0.1540636 Test Loss: 0.2155485
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 9.078679323196411
Epoch: 41, Steps: 65 | Train Loss: 0.2943995 Vali Loss: 0.1540118 Test Loss: 0.2155058
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.21950632333755493, mae:0.29369688034057617, rse:0.3792439103126526, corr:[0.5506242  0.5610693  0.5641366  0.5620454  0.55993265 0.55958295
 0.56067544 0.562189   0.5629842  0.5626293  0.5616503  0.5608327
 0.5605791  0.56081635 0.5612313  0.561366   0.5609639  0.5600981
 0.559107   0.5582692  0.55771744 0.5574125  0.5572222  0.5569578
 0.5564947  0.5558594  0.55515736 0.55447125 0.55385405 0.5533342
 0.55288756 0.55247486 0.5520037  0.55140626 0.55068177 0.54988503
 0.5490722  0.54833287 0.5477139  0.5472003  0.5467335  0.54622585
 0.5456104  0.54487026 0.54406637 0.54329246 0.5426169  0.5420383
 0.5414545  0.5407793  0.5399859  0.5391411  0.5383154  0.5376092
 0.53705686 0.5366522  0.5363008  0.53590226 0.5354318  0.53493965
 0.53451556 0.5342446  0.5340937  0.53398615 0.53380686 0.53353024
 0.53312826 0.53270996 0.53238195 0.53215253 0.53194916 0.53169096
 0.5313132  0.5307972  0.5301857  0.52960944 0.52915055 0.528781
 0.5284118  0.52796024 0.52733666 0.52656764 0.5257589  0.5250399
 0.524476   0.5240733  0.52367544 0.5232184  0.5226688  0.52205956
 0.5214759  0.52095544 0.52039266 0.519706   0.5187717  0.5174966
 0.5159449  0.514349   0.51290035 0.511712   0.51076216 0.50988954
 0.50894386 0.5078146  0.5064744  0.5050653  0.5038377  0.5029948
 0.5024356  0.5019511  0.5013358  0.50045925 0.4994285  0.49834716
 0.4973688  0.49662283 0.49607196 0.49551114 0.49480197 0.49380085
 0.4926432  0.49148867 0.49062625 0.49007973 0.48965472 0.48910487
 0.4882813  0.4871613  0.48586398 0.4845966  0.48358038 0.48285747
 0.4823522  0.48187345 0.48128104 0.48052803 0.47965962 0.4789331
 0.47844395 0.47817013 0.47791958 0.47753692 0.47693613 0.47605082
 0.47499904 0.47390687 0.4730901  0.4725175  0.47206488 0.4714702
 0.47069514 0.46986133 0.46906704 0.46836814 0.4677891  0.4673014
 0.46673983 0.46603793 0.4651859  0.4645373  0.4642133  0.46429852
 0.46447456 0.46443564 0.46398294 0.46316394 0.46216244 0.46141243
 0.4613367  0.46197703 0.46269286 0.46286222 0.4621517  0.46072683
 0.45923296 0.45846644 0.4588153  0.4597638  0.46015543 0.45944324
 0.45741734 0.45471948 0.45293322 0.4531927  0.45492613 0.45625314
 0.45570686 0.4532377  0.45009893 0.4493891  0.45290935 0.45461506]
