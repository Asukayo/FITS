Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  29030400.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4158495
	speed: 0.2309s/iter; left time: 1466.7052s
Epoch: 1 cost time: 29.001749992370605
Epoch: 1, Steps: 129 | Train Loss: 0.4901083 Vali Loss: 0.3230755 Test Loss: 0.4347974
Validation loss decreased (inf --> 0.323075).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3128904
	speed: 0.5790s/iter; left time: 3602.7531s
Epoch: 2 cost time: 29.466614723205566
Epoch: 2, Steps: 129 | Train Loss: 0.3675811 Vali Loss: 0.2959750 Test Loss: 0.4014006
Validation loss decreased (0.323075 --> 0.295975).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3304682
	speed: 0.6193s/iter; left time: 3773.4548s
Epoch: 3 cost time: 30.481602668762207
Epoch: 3, Steps: 129 | Train Loss: 0.3261856 Vali Loss: 0.2859764 Test Loss: 0.3895682
Validation loss decreased (0.295975 --> 0.285976).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2596138
	speed: 0.5781s/iter; left time: 3447.6116s
Epoch: 4 cost time: 26.825565576553345
Epoch: 4, Steps: 129 | Train Loss: 0.3058895 Vali Loss: 0.2806561 Test Loss: 0.3832099
Validation loss decreased (0.285976 --> 0.280656).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2689652
	speed: 0.5795s/iter; left time: 3381.5394s
Epoch: 5 cost time: 29.0841383934021
Epoch: 5, Steps: 129 | Train Loss: 0.2928278 Vali Loss: 0.2768878 Test Loss: 0.3787839
Validation loss decreased (0.280656 --> 0.276888).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2767445
	speed: 0.5417s/iter; left time: 3090.9172s
Epoch: 6 cost time: 26.385956525802612
Epoch: 6, Steps: 129 | Train Loss: 0.2838783 Vali Loss: 0.2746000 Test Loss: 0.3756315
Validation loss decreased (0.276888 --> 0.274600).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3017431
	speed: 0.5376s/iter; left time: 2998.2098s
Epoch: 7 cost time: 26.307406425476074
Epoch: 7, Steps: 129 | Train Loss: 0.2774173 Vali Loss: 0.2723407 Test Loss: 0.3729097
Validation loss decreased (0.274600 --> 0.272341).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2960561
	speed: 0.5448s/iter; left time: 2968.0175s
Epoch: 8 cost time: 26.732199668884277
Epoch: 8, Steps: 129 | Train Loss: 0.2723605 Vali Loss: 0.2708848 Test Loss: 0.3708910
Validation loss decreased (0.272341 --> 0.270885).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2945039
	speed: 0.5278s/iter; left time: 2807.1559s
Epoch: 9 cost time: 27.462337017059326
Epoch: 9, Steps: 129 | Train Loss: 0.2690329 Vali Loss: 0.2694495 Test Loss: 0.3691001
Validation loss decreased (0.270885 --> 0.269450).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2673039
	speed: 0.5953s/iter; left time: 3089.4537s
Epoch: 10 cost time: 29.39844822883606
Epoch: 10, Steps: 129 | Train Loss: 0.2666132 Vali Loss: 0.2686137 Test Loss: 0.3675233
Validation loss decreased (0.269450 --> 0.268614).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3222643
	speed: 0.5725s/iter; left time: 2897.4256s
Epoch: 11 cost time: 26.909452199935913
Epoch: 11, Steps: 129 | Train Loss: 0.2647869 Vali Loss: 0.2675211 Test Loss: 0.3665114
Validation loss decreased (0.268614 --> 0.267521).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3174774
	speed: 0.5593s/iter; left time: 2758.3144s
Epoch: 12 cost time: 26.55675220489502
Epoch: 12, Steps: 129 | Train Loss: 0.2631669 Vali Loss: 0.2671779 Test Loss: 0.3654937
Validation loss decreased (0.267521 --> 0.267178).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3240172
	speed: 0.5703s/iter; left time: 2739.3006s
Epoch: 13 cost time: 30.14742350578308
Epoch: 13, Steps: 129 | Train Loss: 0.2618453 Vali Loss: 0.2665284 Test Loss: 0.3646705
Validation loss decreased (0.267178 --> 0.266528).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2326996
	speed: 0.6106s/iter; left time: 2854.0388s
Epoch: 14 cost time: 29.231812715530396
Epoch: 14, Steps: 129 | Train Loss: 0.2610098 Vali Loss: 0.2661099 Test Loss: 0.3639683
Validation loss decreased (0.266528 --> 0.266110).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2721320
	speed: 0.6124s/iter; left time: 2783.3153s
Epoch: 15 cost time: 30.588937282562256
Epoch: 15, Steps: 129 | Train Loss: 0.2603531 Vali Loss: 0.2657708 Test Loss: 0.3633847
Validation loss decreased (0.266110 --> 0.265771).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2387329
	speed: 0.5910s/iter; left time: 2609.6700s
Epoch: 16 cost time: 27.134215116500854
Epoch: 16, Steps: 129 | Train Loss: 0.2595905 Vali Loss: 0.2653117 Test Loss: 0.3629270
Validation loss decreased (0.265771 --> 0.265312).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2454086
	speed: 0.5621s/iter; left time: 2409.6549s
Epoch: 17 cost time: 27.335864067077637
Epoch: 17, Steps: 129 | Train Loss: 0.2593511 Vali Loss: 0.2648709 Test Loss: 0.3625974
Validation loss decreased (0.265312 --> 0.264871).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2677147
	speed: 0.5362s/iter; left time: 2229.6657s
Epoch: 18 cost time: 26.033400297164917
Epoch: 18, Steps: 129 | Train Loss: 0.2589797 Vali Loss: 0.2650881 Test Loss: 0.3622405
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2098199
	speed: 0.5477s/iter; left time: 2206.5043s
Epoch: 19 cost time: 26.976195335388184
Epoch: 19, Steps: 129 | Train Loss: 0.2589920 Vali Loss: 0.2646616 Test Loss: 0.3619632
Validation loss decreased (0.264871 --> 0.264662).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2236751
	speed: 0.5386s/iter; left time: 2100.7223s
Epoch: 20 cost time: 26.739275455474854
Epoch: 20, Steps: 129 | Train Loss: 0.2580345 Vali Loss: 0.2646118 Test Loss: 0.3617203
Validation loss decreased (0.264662 --> 0.264612).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3047385
	speed: 0.5556s/iter; left time: 2094.9848s
Epoch: 21 cost time: 26.532009840011597
Epoch: 21, Steps: 129 | Train Loss: 0.2583413 Vali Loss: 0.2643385 Test Loss: 0.3615845
Validation loss decreased (0.264612 --> 0.264338).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2130719
	speed: 0.5578s/iter; left time: 2031.4676s
Epoch: 22 cost time: 26.3763427734375
Epoch: 22, Steps: 129 | Train Loss: 0.2579884 Vali Loss: 0.2643921 Test Loss: 0.3613431
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2634158
	speed: 0.5396s/iter; left time: 1895.6724s
Epoch: 23 cost time: 26.73816704750061
Epoch: 23, Steps: 129 | Train Loss: 0.2581040 Vali Loss: 0.2640738 Test Loss: 0.3612367
Validation loss decreased (0.264338 --> 0.264074).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2425526
	speed: 0.5565s/iter; left time: 1883.2179s
Epoch: 24 cost time: 27.87161660194397
Epoch: 24, Steps: 129 | Train Loss: 0.2580015 Vali Loss: 0.2638908 Test Loss: 0.3611608
Validation loss decreased (0.264074 --> 0.263891).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2629294
	speed: 0.5744s/iter; left time: 1869.6589s
Epoch: 25 cost time: 28.157410383224487
Epoch: 25, Steps: 129 | Train Loss: 0.2578359 Vali Loss: 0.2640281 Test Loss: 0.3610006
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2425877
	speed: 0.6059s/iter; left time: 1893.9337s
Epoch: 26 cost time: 28.756349563598633
Epoch: 26, Steps: 129 | Train Loss: 0.2579372 Vali Loss: 0.2639359 Test Loss: 0.3608855
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2655774
	speed: 0.5713s/iter; left time: 1712.0459s
Epoch: 27 cost time: 27.623759031295776
Epoch: 27, Steps: 129 | Train Loss: 0.2576503 Vali Loss: 0.2640694 Test Loss: 0.3607941
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  29030400.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5786231
	speed: 0.2265s/iter; left time: 1438.6248s
Epoch: 1 cost time: 28.684770822525024
Epoch: 1, Steps: 129 | Train Loss: 0.4986662 Vali Loss: 0.2629839 Test Loss: 0.3597067
Validation loss decreased (inf --> 0.262984).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4988841
	speed: 0.5985s/iter; left time: 3723.9378s
Epoch: 2 cost time: 29.558741807937622
Epoch: 2, Steps: 129 | Train Loss: 0.4969363 Vali Loss: 0.2618409 Test Loss: 0.3593674
Validation loss decreased (0.262984 --> 0.261841).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5828362
	speed: 0.6004s/iter; left time: 3658.2448s
Epoch: 3 cost time: 28.29574179649353
Epoch: 3, Steps: 129 | Train Loss: 0.4966872 Vali Loss: 0.2620709 Test Loss: 0.3590439
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4333519
	speed: 0.5907s/iter; left time: 3523.2238s
Epoch: 4 cost time: 28.312675952911377
Epoch: 4, Steps: 129 | Train Loss: 0.4960697 Vali Loss: 0.2620777 Test Loss: 0.3586440
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5851705
	speed: 0.5165s/iter; left time: 3013.9513s
Epoch: 5 cost time: 25.819308519363403
Epoch: 5, Steps: 129 | Train Loss: 0.4961034 Vali Loss: 0.2613438 Test Loss: 0.3585349
Validation loss decreased (0.261841 --> 0.261344).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4594656
	speed: 0.5379s/iter; left time: 3069.2879s
Epoch: 6 cost time: 25.855711936950684
Epoch: 6, Steps: 129 | Train Loss: 0.4954110 Vali Loss: 0.2612066 Test Loss: 0.3583073
Validation loss decreased (0.261344 --> 0.261207).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4872301
	speed: 0.5982s/iter; left time: 3335.9417s
Epoch: 7 cost time: 28.943552017211914
Epoch: 7, Steps: 129 | Train Loss: 0.4955722 Vali Loss: 0.2613235 Test Loss: 0.3581623
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4797018
	speed: 0.5872s/iter; left time: 3199.3151s
Epoch: 8 cost time: 27.76595425605774
Epoch: 8, Steps: 129 | Train Loss: 0.4952455 Vali Loss: 0.2610474 Test Loss: 0.3582956
Validation loss decreased (0.261207 --> 0.261047).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4535198
	speed: 0.6158s/iter; left time: 3275.5872s
Epoch: 9 cost time: 30.07597255706787
Epoch: 9, Steps: 129 | Train Loss: 0.4944526 Vali Loss: 0.2611837 Test Loss: 0.3581798
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5620336
	speed: 0.6070s/iter; left time: 3150.5843s
Epoch: 10 cost time: 28.84550976753235
Epoch: 10, Steps: 129 | Train Loss: 0.4948468 Vali Loss: 0.2609854 Test Loss: 0.3580765
Validation loss decreased (0.261047 --> 0.260985).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5334969
	speed: 0.5080s/iter; left time: 2570.8982s
Epoch: 11 cost time: 26.502575874328613
Epoch: 11, Steps: 129 | Train Loss: 0.4945128 Vali Loss: 0.2611196 Test Loss: 0.3579976
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5768710
	speed: 0.5478s/iter; left time: 2701.7875s
Epoch: 12 cost time: 26.359652280807495
Epoch: 12, Steps: 129 | Train Loss: 0.4949472 Vali Loss: 0.2608157 Test Loss: 0.3579655
Validation loss decreased (0.260985 --> 0.260816).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5062620
	speed: 0.5437s/iter; left time: 2611.3938s
Epoch: 13 cost time: 24.86193013191223
Epoch: 13, Steps: 129 | Train Loss: 0.4947258 Vali Loss: 0.2609816 Test Loss: 0.3578600
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3931318
	speed: 0.5131s/iter; left time: 2398.2921s
Epoch: 14 cost time: 24.864575624465942
Epoch: 14, Steps: 129 | Train Loss: 0.4943827 Vali Loss: 0.2609777 Test Loss: 0.3577537
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5635638
	speed: 0.4961s/iter; left time: 2254.9687s
Epoch: 15 cost time: 23.634273290634155
Epoch: 15, Steps: 129 | Train Loss: 0.4946846 Vali Loss: 0.2610938 Test Loss: 0.3576998
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.34914878010749817, mae:0.3780912756919861, rse:0.4749518930912018, corr:[0.5392196  0.5438864  0.5437141  0.5412374  0.53932047 0.53886604
 0.5395073  0.5403268  0.5404833  0.53981316 0.53879327 0.53799134
 0.53771204 0.53788495 0.5381954  0.5381645  0.53759205 0.53662544
 0.53558695 0.5347493  0.5342527  0.5339968  0.5337959  0.53346264
 0.5328873  0.5321564  0.53143364 0.5308542  0.5304327  0.53009915
 0.52972317 0.52925473 0.5286582  0.5279399  0.5272051  0.52655554
 0.52599674 0.5254971  0.5249601  0.5243405  0.52364933 0.5229184
 0.5222305  0.5216226  0.5210894  0.5205642  0.5199852  0.5193211
 0.51853245 0.517631   0.51672894 0.51595974 0.5153476  0.51487446
 0.51447433 0.51406765 0.51360536 0.51310176 0.5126252  0.5122167
 0.5119266  0.51172745 0.51154244 0.5112844  0.51091635 0.51050985
 0.510068   0.50973034 0.50953865 0.5094366  0.5092964  0.5090172
 0.5085976  0.50807965 0.50752157 0.50699574 0.5065423  0.5061168
 0.50567913 0.50512725 0.50439006 0.503528   0.5026569  0.50188404
 0.5012981  0.50089574 0.5005667  0.50025994 0.4998859  0.49944744
 0.49897623 0.49845588 0.49783576 0.4970514  0.49600807 0.49467987
 0.49314567 0.4915873  0.49011478 0.4888294  0.48777166 0.4868851
 0.48604426 0.4851129  0.48401222 0.4827695  0.48151112 0.48039648
 0.4794687  0.47869587 0.4779665  0.47713774 0.47617102 0.47502035
 0.47373962 0.47249782 0.47145957 0.4706519  0.4700284  0.4694369
 0.46882167 0.46812743 0.4674166  0.46672046 0.46605334 0.4653833
 0.4646744  0.46385732 0.46288368 0.46176592 0.46061224 0.45954475
 0.45868415 0.45802227 0.45747963 0.4569163  0.456228   0.45545197
 0.45467746 0.4540198  0.45353034 0.45317203 0.45283234 0.4523287
 0.451588   0.45060763 0.4495981  0.44870722 0.448049   0.4475721
 0.44716248 0.44665226 0.4458744  0.4448408  0.44370127 0.4426511
 0.44181123 0.44116706 0.44057852 0.43997183 0.4392487  0.43848196
 0.43781307 0.43742338 0.437353   0.4374776  0.43755084 0.43736625
 0.43683037 0.4360208  0.43508428 0.4343197  0.43388262 0.4337813
 0.43381935 0.43376228 0.43341595 0.43272746 0.43178615 0.43083575
 0.43004435 0.42949077 0.429143   0.42884052 0.42843455 0.42786062
 0.4272161  0.426572   0.42600933 0.4254668  0.42482463 0.42391205
 0.42266944 0.42125034 0.4196767  0.4181023  0.41674164 0.41565204
 0.41478327 0.41396227 0.4130135  0.41184512 0.41048118 0.409047
 0.40769756 0.4065217  0.40549034 0.40450895 0.40355128 0.40263617
 0.40167332 0.4006773  0.39972746 0.39889598 0.39821067 0.39753512
 0.39674133 0.39570916 0.3945166  0.39324436 0.39198115 0.3908065
 0.38975677 0.3888322  0.38803    0.3871877  0.3862507  0.38527277
 0.38423678 0.38321033 0.38221133 0.3812589  0.38035774 0.37955365
 0.37886468 0.3783626  0.37803453 0.37780538 0.37760794 0.3773516
 0.37702757 0.37663004 0.37624788 0.37606087 0.3759705  0.37589267
 0.37567037 0.3752017  0.37446588 0.37357658 0.37283605 0.37244758
 0.37246013 0.37277928 0.37307402 0.37309545 0.37269813 0.37198535
 0.37116873 0.3705146  0.3701889  0.37022248 0.37042597 0.37061188
 0.3705768  0.37018806 0.36952525 0.36874652 0.36800283 0.36739594
 0.36703846 0.3668912  0.36681038 0.3667231  0.36654472 0.3662387
 0.3658698  0.36539188 0.36480623 0.36425224 0.3637728  0.36348465
 0.36339352 0.36346832 0.36357433 0.36349684 0.36306998 0.36214668
 0.36085415 0.35950485 0.35828793 0.35730442 0.3565817  0.35600364
 0.3555279  0.35504022 0.3544937  0.3539434  0.35351765 0.3532061
 0.35299885 0.35265964 0.35207924 0.3510975  0.34985775 0.34861687
 0.34774402 0.34738392 0.34753075 0.3479769  0.3483939  0.34858233
 0.3483143  0.3476747  0.34681514 0.34596515 0.3453717  0.34513903
 0.3451233  0.3451653  0.3451255  0.34491774 0.34456652 0.34421226
 0.34396553 0.34392172 0.3440408  0.34414425 0.3441958  0.34414825
 0.34412464 0.34414634 0.34423876 0.34438434 0.34442428 0.34432772
 0.34404883 0.34358996 0.34315747 0.34297183 0.34306315 0.34327537
 0.3434256  0.34332737 0.3429006  0.34220403 0.34145877 0.34095752
 0.34083727 0.34107888 0.34145254 0.34167424 0.3415709  0.3411184
 0.34048992 0.33998203 0.33977026 0.3400008  0.3404619  0.34096313
 0.34127215 0.34125453 0.34090003 0.34032866 0.33969173 0.33919615
 0.3388528  0.33868152 0.33858365 0.33850902 0.3384521  0.33837786
 0.33831584 0.338305   0.3383109  0.33826962 0.3381333  0.33796805
 0.33786243 0.33784324 0.33785838 0.33776754 0.33755603 0.33708858
 0.33642566 0.3356413  0.33484492 0.33408725 0.33337116 0.3327788
 0.33223698 0.33169594 0.33111954 0.33051428 0.329928   0.32936803
 0.32885045 0.32837787 0.3278085  0.32707652 0.326276   0.32543972
 0.3247117  0.32420367 0.32401538 0.3241439  0.32435963 0.32449028
 0.32434723 0.32394615 0.32338646 0.32282725 0.3224877  0.32241276
 0.32260165 0.32290787 0.3230722  0.3229572  0.3225381  0.32200262
 0.32163244 0.32160786 0.32192236 0.32245815 0.3230133  0.32338828
 0.32349825 0.3233294  0.32302827 0.32273632 0.32258812 0.32262778
 0.32280192 0.32295412 0.32304493 0.32309517 0.32316214 0.32320666
 0.32310647 0.32287806 0.32242832 0.3217163  0.32086006 0.32009536
 0.31951863 0.31925166 0.31922993 0.31927532 0.31924975 0.31901658
 0.31860167 0.31810293 0.31769046 0.31740075 0.31726238 0.31724352
 0.3172082  0.31703812 0.31665486 0.3160943  0.31554246 0.31504115
 0.314682   0.3144553  0.31430194 0.3141386  0.3138948  0.31355911
 0.3131899  0.31282458 0.31246063 0.31206402 0.31165692 0.3111851
 0.31065768 0.31003672 0.30939427 0.30870637 0.30798832 0.30718502
 0.3062393  0.30521184 0.30406922 0.30286685 0.30170086 0.30063203
 0.2996586  0.29882437 0.2980818  0.2974711  0.29691136 0.29632074
 0.29565373 0.2949742  0.29428142 0.29351634 0.2927084  0.291895
 0.29116088 0.29047838 0.2898526  0.28934625 0.28888783 0.28849486
 0.28802767 0.28742284 0.28672865 0.2860021  0.28538695 0.28488854
 0.28448784 0.2841121  0.28366363 0.28310394 0.2824856  0.2818967
 0.28141376 0.28105626 0.28096026 0.28102517 0.28112888 0.28119242
 0.28115422 0.28103545 0.28084397 0.28063786 0.2804822  0.2803512
 0.28016672 0.2799046  0.27954996 0.2791673  0.27886885 0.27869242
 0.27859172 0.27851298 0.2784737  0.27837664 0.2781737  0.27786145
 0.27748865 0.27714705 0.2768539  0.27664655 0.27646017 0.27620625
 0.27584943 0.2754494  0.27505893 0.2747406  0.2744872  0.27435124
 0.27426913 0.27422363 0.27416402 0.27407914 0.27384013 0.27355412
 0.27329546 0.2731993  0.27322632 0.27328897 0.27329844 0.2731789
 0.27291194 0.2725095  0.27207443 0.2716997  0.27146626 0.27141333
 0.27148482 0.2715501  0.2714083  0.27096626 0.27019897 0.2690653
 0.26765734 0.26627162 0.26502794 0.2639476  0.2629347  0.26193672
 0.26095292 0.2600208  0.25919828 0.25850663 0.2579227  0.2574368
 0.25696155 0.25639847 0.25579426 0.25516206 0.2545378  0.25399542
 0.25353515 0.25309056 0.2526878  0.25224003 0.25179842 0.2514003
 0.25104848 0.25079942 0.25058624 0.25034943 0.2501355  0.24993634
 0.24964812 0.24926493 0.2488571  0.24839956 0.24802026 0.24781899
 0.24792638 0.24818568 0.24855275 0.2489819  0.24932994 0.24946745
 0.24937357 0.24919796 0.2490546  0.24910039 0.24940927 0.24994735
 0.25050756 0.2508591  0.25087565 0.25066274 0.25036332 0.2501318
 0.25002882 0.25010285 0.2503374  0.2504492  0.25051558 0.25047547
 0.2503811  0.25033233 0.25031328 0.25044155 0.2505201  0.25048888
 0.25024426 0.24991955 0.24953297 0.24909575 0.24886282 0.24873479
 0.24876665 0.24885385 0.24892613 0.24891919 0.24891222 0.24879299
 0.24873693 0.24870601 0.24860425 0.24849293 0.24833675 0.2480422
 0.24759218 0.24721827 0.24695139 0.24687734 0.24707982 0.24741931
 0.24768141 0.24776854 0.24758595 0.24704507 0.24621348 0.2452846
 0.24441992 0.24370271 0.24309339 0.24245986 0.24177817 0.24097764
 0.24006225 0.23908162 0.23832247 0.23771833 0.23727281 0.23687503
 0.23646998 0.23603968 0.23556888 0.23500943 0.23442875 0.23370317
 0.23287593 0.23200727 0.23116489 0.23065919 0.23045923 0.23043087
 0.23041995 0.23019882 0.22974412 0.22898197 0.22809468 0.22725406
 0.22670583 0.226264   0.22588441 0.22553554 0.22522226 0.22503492
 0.22493795 0.22495389 0.22491428 0.22475246 0.22412336 0.22318377
 0.22228377 0.22187631 0.2220506  0.22210674 0.22033508 0.21426016]
