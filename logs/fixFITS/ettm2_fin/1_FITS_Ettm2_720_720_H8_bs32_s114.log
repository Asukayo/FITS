Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=74, out_features=148, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4906496.0
params:  11100.0
Trainable parameters:  11100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7229565
	speed: 0.1694s/iter; left time: 4361.1078s
	iters: 200, epoch: 1 | loss: 0.5066293
	speed: 0.1611s/iter; left time: 4132.7963s
	iters: 300, epoch: 1 | loss: 0.3726349
	speed: 0.1574s/iter; left time: 4021.6369s
	iters: 400, epoch: 1 | loss: 0.4141400
	speed: 0.1238s/iter; left time: 3150.2298s
	iters: 500, epoch: 1 | loss: 0.6747878
	speed: 0.1121s/iter; left time: 2842.3494s
Epoch: 1 cost time: 74.13182735443115
Epoch: 1, Steps: 517 | Train Loss: 0.5542314 Vali Loss: 0.2727458 Test Loss: 0.3635499
Validation loss decreased (inf --> 0.272746).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4888103
	speed: 0.8457s/iter; left time: 21340.1341s
	iters: 200, epoch: 2 | loss: 0.2596771
	speed: 0.1252s/iter; left time: 3146.1340s
	iters: 300, epoch: 2 | loss: 0.6932756
	speed: 0.1190s/iter; left time: 2979.1230s
	iters: 400, epoch: 2 | loss: 0.5519056
	speed: 0.1216s/iter; left time: 3033.2049s
	iters: 500, epoch: 2 | loss: 0.4617919
	speed: 0.1199s/iter; left time: 2978.7727s
Epoch: 2 cost time: 64.03250527381897
Epoch: 2, Steps: 517 | Train Loss: 0.5111666 Vali Loss: 0.2673112 Test Loss: 0.3573564
Validation loss decreased (0.272746 --> 0.267311).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4442624
	speed: 0.7619s/iter; left time: 18832.4817s
	iters: 200, epoch: 3 | loss: 0.3066241
	speed: 0.1165s/iter; left time: 2867.7338s
	iters: 300, epoch: 3 | loss: 0.4508256
	speed: 0.1034s/iter; left time: 2534.0986s
	iters: 400, epoch: 3 | loss: 0.3294014
	speed: 0.0991s/iter; left time: 2419.0820s
	iters: 500, epoch: 3 | loss: 0.4240572
	speed: 0.1102s/iter; left time: 2680.7901s
Epoch: 3 cost time: 56.505221128463745
Epoch: 3, Steps: 517 | Train Loss: 0.5050297 Vali Loss: 0.2650982 Test Loss: 0.3550000
Validation loss decreased (0.267311 --> 0.265098).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4771148
	speed: 0.6715s/iter; left time: 16250.0972s
	iters: 200, epoch: 4 | loss: 0.3728643
	speed: 0.0993s/iter; left time: 2393.4258s
	iters: 300, epoch: 4 | loss: 0.4381531
	speed: 0.0976s/iter; left time: 2341.7776s
	iters: 400, epoch: 4 | loss: 0.4599876
	speed: 0.0968s/iter; left time: 2313.2801s
	iters: 500, epoch: 4 | loss: 0.6744211
	speed: 0.1021s/iter; left time: 2428.9228s
Epoch: 4 cost time: 52.09422707557678
Epoch: 4, Steps: 517 | Train Loss: 0.5018676 Vali Loss: 0.2638906 Test Loss: 0.3532740
Validation loss decreased (0.265098 --> 0.263891).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6786460
	speed: 0.7113s/iter; left time: 16844.9260s
	iters: 200, epoch: 5 | loss: 0.6427565
	speed: 0.1337s/iter; left time: 3152.6820s
	iters: 300, epoch: 5 | loss: 0.7466167
	speed: 0.1313s/iter; left time: 3082.4471s
	iters: 400, epoch: 5 | loss: 0.5665726
	speed: 0.1288s/iter; left time: 3012.0991s
	iters: 500, epoch: 5 | loss: 0.6352937
	speed: 0.1280s/iter; left time: 2979.5964s
Epoch: 5 cost time: 68.20461845397949
Epoch: 5, Steps: 517 | Train Loss: 0.4996458 Vali Loss: 0.2638373 Test Loss: 0.3524730
Validation loss decreased (0.263891 --> 0.263837).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5422385
	speed: 0.8782s/iter; left time: 20343.4700s
	iters: 200, epoch: 6 | loss: 0.4717656
	speed: 0.1353s/iter; left time: 3121.7839s
	iters: 300, epoch: 6 | loss: 0.5048623
	speed: 0.1296s/iter; left time: 2977.4585s
	iters: 400, epoch: 6 | loss: 0.5035266
	speed: 0.1334s/iter; left time: 3051.3802s
	iters: 500, epoch: 6 | loss: 0.6241409
	speed: 0.1311s/iter; left time: 2985.0796s
Epoch: 6 cost time: 69.26181435585022
Epoch: 6, Steps: 517 | Train Loss: 0.4991656 Vali Loss: 0.2629167 Test Loss: 0.3516888
Validation loss decreased (0.263837 --> 0.262917).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6277377
	speed: 0.8816s/iter; left time: 19968.2000s
	iters: 200, epoch: 7 | loss: 0.3752083
	speed: 0.1364s/iter; left time: 3075.7670s
	iters: 300, epoch: 7 | loss: 0.6050598
	speed: 0.1336s/iter; left time: 3000.2413s
	iters: 400, epoch: 7 | loss: 0.4754576
	speed: 0.1330s/iter; left time: 2972.1404s
	iters: 500, epoch: 7 | loss: 0.3675028
	speed: 0.1297s/iter; left time: 2886.2536s
Epoch: 7 cost time: 69.50928378105164
Epoch: 7, Steps: 517 | Train Loss: 0.4984078 Vali Loss: 0.2628294 Test Loss: 0.3516908
Validation loss decreased (0.262917 --> 0.262829).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6505771
	speed: 0.8501s/iter; left time: 18813.5059s
	iters: 200, epoch: 8 | loss: 0.8061393
	speed: 0.1237s/iter; left time: 2725.5892s
	iters: 300, epoch: 8 | loss: 0.3534364
	speed: 0.1266s/iter; left time: 2776.2030s
	iters: 400, epoch: 8 | loss: 0.5981498
	speed: 0.1247s/iter; left time: 2722.3811s
	iters: 500, epoch: 8 | loss: 0.6574473
	speed: 0.1298s/iter; left time: 2821.3827s
Epoch: 8 cost time: 65.93822169303894
Epoch: 8, Steps: 517 | Train Loss: 0.4978199 Vali Loss: 0.2628319 Test Loss: 0.3510199
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4321478
	speed: 0.9406s/iter; left time: 20331.7711s
	iters: 200, epoch: 9 | loss: 0.6389095
	speed: 0.1372s/iter; left time: 2952.1934s
	iters: 300, epoch: 9 | loss: 0.7278481
	speed: 0.1354s/iter; left time: 2898.7232s
	iters: 400, epoch: 9 | loss: 0.4918821
	speed: 0.1427s/iter; left time: 3041.9891s
	iters: 500, epoch: 9 | loss: 0.3687290
	speed: 0.1340s/iter; left time: 2842.4411s
Epoch: 9 cost time: 72.97384715080261
Epoch: 9, Steps: 517 | Train Loss: 0.4973940 Vali Loss: 0.2625916 Test Loss: 0.3510787
Validation loss decreased (0.262829 --> 0.262592).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6796653
	speed: 0.8875s/iter; left time: 18724.1318s
	iters: 200, epoch: 10 | loss: 0.5384228
	speed: 0.1356s/iter; left time: 2846.3261s
	iters: 300, epoch: 10 | loss: 0.5233834
	speed: 0.1370s/iter; left time: 2862.1399s
	iters: 400, epoch: 10 | loss: 0.6044140
	speed: 0.1356s/iter; left time: 2820.1548s
	iters: 500, epoch: 10 | loss: 0.5599902
	speed: 0.1383s/iter; left time: 2862.9357s
Epoch: 10 cost time: 71.79204654693604
Epoch: 10, Steps: 517 | Train Loss: 0.4971833 Vali Loss: 0.2622702 Test Loss: 0.3506149
Validation loss decreased (0.262592 --> 0.262270).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3207671
	speed: 0.8814s/iter; left time: 18139.5062s
	iters: 200, epoch: 11 | loss: 0.3613833
	speed: 0.1313s/iter; left time: 2689.2688s
	iters: 300, epoch: 11 | loss: 0.3025515
	speed: 0.1353s/iter; left time: 2757.0323s
	iters: 400, epoch: 11 | loss: 0.4697593
	speed: 0.1336s/iter; left time: 2709.0515s
	iters: 500, epoch: 11 | loss: 0.5613716
	speed: 0.1323s/iter; left time: 2670.6984s
Epoch: 11 cost time: 69.79765629768372
Epoch: 11, Steps: 517 | Train Loss: 0.4969120 Vali Loss: 0.2620602 Test Loss: 0.3508917
Validation loss decreased (0.262270 --> 0.262060).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5820351
	speed: 0.8570s/iter; left time: 17194.3634s
	iters: 200, epoch: 12 | loss: 0.5221630
	speed: 0.1271s/iter; left time: 2538.2318s
	iters: 300, epoch: 12 | loss: 0.3711688
	speed: 0.1200s/iter; left time: 2383.9569s
	iters: 400, epoch: 12 | loss: 0.5295364
	speed: 0.1164s/iter; left time: 2300.8735s
	iters: 500, epoch: 12 | loss: 0.5477990
	speed: 0.0999s/iter; left time: 1964.0358s
Epoch: 12 cost time: 61.94065856933594
Epoch: 12, Steps: 517 | Train Loss: 0.4965045 Vali Loss: 0.2617409 Test Loss: 0.3508523
Validation loss decreased (0.262060 --> 0.261741).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5216848
	speed: 0.8719s/iter; left time: 17043.3194s
	iters: 200, epoch: 13 | loss: 0.3906957
	speed: 0.1345s/iter; left time: 2616.3236s
	iters: 300, epoch: 13 | loss: 0.5384850
	speed: 0.1326s/iter; left time: 2565.0990s
	iters: 400, epoch: 13 | loss: 0.5092708
	speed: 0.1363s/iter; left time: 2622.5672s
	iters: 500, epoch: 13 | loss: 0.3043312
	speed: 0.1277s/iter; left time: 2445.3828s
Epoch: 13 cost time: 69.3400411605835
Epoch: 13, Steps: 517 | Train Loss: 0.4964551 Vali Loss: 0.2618353 Test Loss: 0.3506126
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6271602
	speed: 0.8407s/iter; left time: 15998.6670s
	iters: 200, epoch: 14 | loss: 0.5320354
	speed: 0.1262s/iter; left time: 2388.1823s
	iters: 300, epoch: 14 | loss: 0.7196023
	speed: 0.1319s/iter; left time: 2483.0741s
	iters: 400, epoch: 14 | loss: 0.7845336
	speed: 0.1245s/iter; left time: 2331.2252s
	iters: 500, epoch: 14 | loss: 0.8293943
	speed: 0.1322s/iter; left time: 2462.1019s
Epoch: 14 cost time: 67.59163641929626
Epoch: 14, Steps: 517 | Train Loss: 0.4962968 Vali Loss: 0.2615417 Test Loss: 0.3505834
Validation loss decreased (0.261741 --> 0.261542).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4622822
	speed: 0.8836s/iter; left time: 16358.6910s
	iters: 200, epoch: 15 | loss: 0.5010409
	speed: 0.1508s/iter; left time: 2777.3810s
	iters: 300, epoch: 15 | loss: 0.4620283
	speed: 0.1489s/iter; left time: 2726.1639s
	iters: 400, epoch: 15 | loss: 0.3153298
	speed: 0.1510s/iter; left time: 2750.5249s
	iters: 500, epoch: 15 | loss: 0.6520491
	speed: 0.1601s/iter; left time: 2900.5616s
Epoch: 15 cost time: 79.33673477172852
Epoch: 15, Steps: 517 | Train Loss: 0.4961136 Vali Loss: 0.2619774 Test Loss: 0.3506089
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5443361
	speed: 0.9854s/iter; left time: 17733.4261s
	iters: 200, epoch: 16 | loss: 0.6950611
	speed: 0.1386s/iter; left time: 2480.2208s
	iters: 300, epoch: 16 | loss: 0.5115579
	speed: 0.1140s/iter; left time: 2028.4709s
	iters: 400, epoch: 16 | loss: 0.4017010
	speed: 0.0990s/iter; left time: 1751.8186s
	iters: 500, epoch: 16 | loss: 0.5114759
	speed: 0.1198s/iter; left time: 2108.1382s
Epoch: 16 cost time: 64.70711517333984
Epoch: 16, Steps: 517 | Train Loss: 0.4960467 Vali Loss: 0.2616961 Test Loss: 0.3503599
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4442863
	speed: 0.8017s/iter; left time: 14012.5651s
	iters: 200, epoch: 17 | loss: 0.4842986
	speed: 0.1252s/iter; left time: 2175.3787s
	iters: 300, epoch: 17 | loss: 0.5893982
	speed: 0.1251s/iter; left time: 2161.3415s
	iters: 400, epoch: 17 | loss: 0.3479564
	speed: 0.1214s/iter; left time: 2085.6126s
	iters: 500, epoch: 17 | loss: 0.6673051
	speed: 0.1232s/iter; left time: 2103.3385s
Epoch: 17 cost time: 64.91165018081665
Epoch: 17, Steps: 517 | Train Loss: 0.4958594 Vali Loss: 0.2616016 Test Loss: 0.3502737
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3499349355697632, mae:0.3788362741470337, rse:0.4754863381385803, corr:[0.53241426 0.53877586 0.538917   0.53652185 0.5343994  0.5335916
 0.5340943  0.53533864 0.53652287 0.5370458  0.53679657 0.5360756
 0.53532016 0.5348141  0.53468436 0.5347591  0.5348266  0.53467286
 0.53415805 0.53332424 0.53239805 0.53156066 0.53098035 0.5307272
 0.53069526 0.53072816 0.53064    0.53032595 0.5297714  0.529068
 0.5283095  0.5276535  0.52712226 0.52664423 0.5261845  0.5257269
 0.52522373 0.5246967  0.52413964 0.52359164 0.5230765  0.52257144
 0.52208954 0.52159196 0.5210444  0.5203777  0.51959646 0.5187499
 0.5178418  0.51687866 0.5159269  0.51509523 0.514415   0.5138976
 0.5135261  0.51324296 0.5129694  0.512645   0.51227146 0.51185405
 0.51145786 0.51115334 0.5109693  0.5108715  0.51079404 0.5107209
 0.51052773 0.51024866 0.5098915  0.50947434 0.5090227  0.50856954
 0.50815034 0.5077552  0.50734097 0.50689    0.5063972  0.5058384
 0.50523716 0.50458586 0.50384736 0.5030678  0.5023008  0.5015771
 0.50092334 0.50035065 0.499813   0.49933195 0.49886203 0.4983798
 0.49787852 0.49732026 0.4966348  0.49577838 0.49471256 0.49345428
 0.4920869  0.49075744 0.48949793 0.48832643 0.487243   0.48621836
 0.48519748 0.48412174 0.48293695 0.48164576 0.4802998  0.4789974
 0.47778    0.47669432 0.47576067 0.4749474  0.4742559  0.4735888
 0.4728546  0.4720412  0.47118267 0.4702936  0.46944758 0.46863616
 0.46791294 0.46721804 0.46653143 0.46576816 0.4648644  0.4637911
 0.46262446 0.46145526 0.4603893  0.45950323 0.4588431  0.45835352
 0.45796114 0.45754474 0.45703772 0.4563799  0.455554   0.4546506
 0.45376214 0.4529951  0.4523913  0.45194465 0.4515937  0.4511839
 0.45061603 0.44979805 0.44883573 0.44780603 0.44683373 0.44598487
 0.44532806 0.44485363 0.4444416  0.44401312 0.4435332  0.44303077
 0.44255614 0.4421487  0.44180807 0.44158664 0.44141123 0.44122356
 0.44094473 0.44055    0.4400202  0.439372   0.43861687 0.43780884
 0.43703207 0.4363708  0.43579546 0.43532005 0.43487537 0.43443456
 0.43395954 0.43344694 0.4328975  0.43231842 0.43171597 0.43115363
 0.43061927 0.4301164  0.4296641  0.42923552 0.42878994 0.42828497
 0.42773306 0.4270713  0.4262881  0.4253237  0.42414522 0.42270914
 0.42105135 0.419359   0.41761023 0.41583824 0.41415995 0.41264307
 0.4113753  0.4103515  0.40950447 0.4087222  0.40788522 0.40691334
 0.40578678 0.40453815 0.40322173 0.40192285 0.4008003  0.39997026
 0.39932054 0.3987208  0.39805874 0.39726892 0.39636633 0.39532593
 0.3941903  0.39298624 0.39185324 0.39085117 0.38998035 0.3892089
 0.38847667 0.38773417 0.38697547 0.3860708  0.38498446 0.38379508
 0.3825399  0.38135967 0.38033363 0.37952262 0.37890765 0.3784647
 0.3781063  0.37780273 0.3775015  0.37715665 0.3767681  0.3763037
 0.37577143 0.37515372 0.3745017  0.37399405 0.3736266  0.3734633
 0.37347847 0.3736127  0.37374058 0.3737479  0.3736381  0.373408
 0.37309211 0.37278774 0.37250048 0.37227052 0.37207377 0.37192038
 0.37176317 0.37155157 0.37123135 0.3708243  0.37033966 0.3698795
 0.3694829  0.36911938 0.3687969  0.36848655 0.3681159  0.36761925
 0.3670472  0.3664351  0.3658095  0.36527184 0.3648652  0.36460787
 0.36454558 0.36458743 0.36464134 0.3646938  0.36463848 0.36444616
 0.36406708 0.36350808 0.362789   0.36191943 0.36095086 0.35986376
 0.35877857 0.3578728  0.35714698 0.35651678 0.35590854 0.35520518
 0.35444316 0.35359994 0.35267618 0.3517172  0.35082695 0.35002914
 0.34943506 0.34899637 0.34876406 0.34861454 0.3485284  0.34842372
 0.34828612 0.3479966  0.3475213  0.3468816  0.3461368  0.3454645
 0.3448778  0.34446687 0.3441886  0.34396812 0.34375757 0.34352636
 0.34318534 0.34275123 0.34227607 0.34179637 0.34135506 0.34102997
 0.34084564 0.34085304 0.34101662 0.3411991  0.34136802 0.34144863
 0.34149238 0.34147602 0.3414318  0.34140816 0.3413525  0.34129006
 0.3411611  0.34085354 0.34042084 0.3399829  0.33962718 0.3393924
 0.3393383  0.33946025 0.33970898 0.33997372 0.34016508 0.340256
 0.34021914 0.34009147 0.33990687 0.33969864 0.33951938 0.3393777
 0.3392759  0.33921942 0.3391326  0.3390438  0.3388462  0.33856586
 0.33822802 0.3378727  0.33754978 0.33729893 0.3371142  0.33702907
 0.3369492  0.3368526  0.33666992 0.33640414 0.33610594 0.33580273
 0.33557278 0.33551177 0.3356364  0.33590472 0.3362297  0.33656663
 0.33687416 0.33707336 0.33707485 0.33678707 0.3363054  0.33562413
 0.33488002 0.3341558  0.33349678 0.33286372 0.33216846 0.33145663
 0.3306953  0.3299155  0.32916316 0.32848352 0.3279143  0.32740635
 0.32691672 0.32643607 0.32584602 0.32511562 0.32433647 0.3235017
 0.32266593 0.32184958 0.3211271  0.32056352 0.32007542 0.31966433
 0.31928536 0.31899905 0.31883    0.3187771  0.31888837 0.31912515
 0.319507   0.31999967 0.32046294 0.3208068  0.3209522  0.32092705
 0.32082775 0.32073748 0.32067218 0.32065704 0.32068503 0.32072598
 0.32076058 0.32073694 0.32068074 0.3205995  0.32053745 0.3205154
 0.32051596 0.32044032 0.32026535 0.3199851  0.319633   0.3191961
 0.31864575 0.31810763 0.31760243 0.3171326  0.31674773 0.31653002
 0.3164228  0.31644607 0.31654802 0.31664565 0.3167264  0.3167405
 0.31670782 0.3166241  0.31651732 0.31632826 0.31606334 0.31574482
 0.3153625  0.3149207  0.31441754 0.31387532 0.31338927 0.31292802
 0.31253284 0.31219313 0.3118933  0.31159067 0.3112224  0.31075978
 0.31021452 0.3096033  0.30894977 0.3082857  0.30771187 0.3072286
 0.30684015 0.30644184 0.3060216  0.30548355 0.30481276 0.30396283
 0.3029258  0.30181012 0.30063075 0.29945323 0.29835677 0.29738104
 0.29651335 0.29579344 0.295172   0.29467377 0.29420462 0.29366788
 0.29298875 0.2922212  0.2913836  0.2904631  0.2895251  0.28861734
 0.28780308 0.28702378 0.2862579  0.28558156 0.284982   0.28454027
 0.28417593 0.28383264 0.28350517 0.28316116 0.28284788 0.2825217
 0.2821929  0.2818614  0.2815099  0.2811421  0.28078347 0.2804353
 0.28006455 0.2796193  0.27922106 0.27884385 0.27847338 0.27813107
 0.27782166 0.27757186 0.27735183 0.2771536  0.2769883  0.27680966
 0.27655038 0.27623412 0.2758775  0.275543   0.27531436 0.27520734
 0.27516466 0.27513498 0.27515393 0.27515674 0.27511677 0.27501956
 0.2748637  0.27467462 0.27443194 0.27418292 0.27392238 0.2736504
 0.2733884  0.27321008 0.27313435 0.27314916 0.2731761  0.27320448
 0.27315846 0.27304918 0.2728998  0.27276248 0.27255854 0.27236098
 0.27218732 0.2720885  0.27201277 0.27189884 0.27173346 0.27152252
 0.27129272 0.2710635  0.27088115 0.2707495  0.2706528  0.27055392
 0.27038527 0.2700597  0.26945755 0.2685687  0.26744577 0.266112
 0.2646939  0.2634729  0.26253515 0.26185632 0.2612759  0.26065153
 0.25988644 0.25895494 0.25789613 0.25676435 0.2556241  0.25457394
 0.25364068 0.25279507 0.25209534 0.25149983 0.25097167 0.2505132
 0.25007436 0.24957502 0.24903993 0.24841234 0.24776092 0.24714677
 0.24660583 0.24621804 0.24595621 0.24579003 0.24577999 0.24591687
 0.24609597 0.24627161 0.24644914 0.24651515 0.2464974  0.24641034
 0.24635983 0.24624817 0.24613343 0.24610242 0.24614505 0.24620937
 0.24627031 0.24638034 0.24652767 0.24673858 0.2470431  0.24743417
 0.24780835 0.24804212 0.24807891 0.24801858 0.24793355 0.24788731
 0.2478568  0.2478563  0.2479108  0.24783552 0.2477492  0.24761257
 0.24744476 0.24729745 0.24713148 0.24706395 0.24695733 0.24683619
 0.24666159 0.24656446 0.24650553 0.24639499 0.2463666  0.24626641
 0.24615817 0.2460113  0.24586031 0.24572274 0.24569164 0.24562784
 0.24561659 0.24556579 0.24536681 0.24513732 0.24493313 0.24472639
 0.24449985 0.24441022 0.24438538 0.24442546 0.24459766 0.24483646
 0.24504533 0.24522884 0.24532062 0.24516971 0.24472445 0.24404718
 0.24321686 0.24231952 0.24139883 0.24045531 0.2395799  0.23876221
 0.23799482 0.23727877 0.23684382 0.23658879 0.2364899  0.23641416
 0.23627731 0.23604871 0.23572078 0.23527895 0.23482509 0.23424657
 0.2335438  0.23269483 0.23167776 0.23075438 0.22997563 0.22940353
 0.22910817 0.22901614 0.22910939 0.22914957 0.22905207 0.22872867
 0.22829188 0.22761801 0.2268392  0.22611755 0.22553161 0.22509094
 0.22461765 0.22404034 0.22323208 0.22233045 0.22131474 0.22057383
 0.22041011 0.2209052  0.22172311 0.2220219  0.22025636 0.21408492]
