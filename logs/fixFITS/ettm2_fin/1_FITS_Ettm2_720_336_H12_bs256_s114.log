Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  58885120.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 12.76011610031128
Epoch: 1, Steps: 65 | Train Loss: 0.5325765 Vali Loss: 0.2404444 Test Loss: 0.3263773
Validation loss decreased (inf --> 0.240444).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.986455202102661
Epoch: 2, Steps: 65 | Train Loss: 0.4410986 Vali Loss: 0.2190810 Test Loss: 0.3000093
Validation loss decreased (0.240444 --> 0.219081).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 13.254661798477173
Epoch: 3, Steps: 65 | Train Loss: 0.4200801 Vali Loss: 0.2113838 Test Loss: 0.2910555
Validation loss decreased (0.219081 --> 0.211384).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 12.61214542388916
Epoch: 4, Steps: 65 | Train Loss: 0.4105161 Vali Loss: 0.2079105 Test Loss: 0.2861838
Validation loss decreased (0.211384 --> 0.207910).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.986013412475586
Epoch: 5, Steps: 65 | Train Loss: 0.4032496 Vali Loss: 0.2046592 Test Loss: 0.2831659
Validation loss decreased (0.207910 --> 0.204659).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.957855939865112
Epoch: 6, Steps: 65 | Train Loss: 0.3989473 Vali Loss: 0.2028272 Test Loss: 0.2809159
Validation loss decreased (0.204659 --> 0.202827).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.403598308563232
Epoch: 7, Steps: 65 | Train Loss: 0.3959894 Vali Loss: 0.2014640 Test Loss: 0.2793239
Validation loss decreased (0.202827 --> 0.201464).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.92947793006897
Epoch: 8, Steps: 65 | Train Loss: 0.3933479 Vali Loss: 0.2003187 Test Loss: 0.2780095
Validation loss decreased (0.201464 --> 0.200319).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.917853355407715
Epoch: 9, Steps: 65 | Train Loss: 0.3914850 Vali Loss: 0.2001140 Test Loss: 0.2769112
Validation loss decreased (0.200319 --> 0.200114).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.516875267028809
Epoch: 10, Steps: 65 | Train Loss: 0.3899749 Vali Loss: 0.1985934 Test Loss: 0.2760659
Validation loss decreased (0.200114 --> 0.198593).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.2603600025177
Epoch: 11, Steps: 65 | Train Loss: 0.3875761 Vali Loss: 0.1982226 Test Loss: 0.2753836
Validation loss decreased (0.198593 --> 0.198223).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.088387966156006
Epoch: 12, Steps: 65 | Train Loss: 0.3873860 Vali Loss: 0.1976680 Test Loss: 0.2747938
Validation loss decreased (0.198223 --> 0.197668).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.627004146575928
Epoch: 13, Steps: 65 | Train Loss: 0.3861520 Vali Loss: 0.1977010 Test Loss: 0.2742921
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.384194374084473
Epoch: 14, Steps: 65 | Train Loss: 0.3855602 Vali Loss: 0.1973150 Test Loss: 0.2738518
Validation loss decreased (0.197668 --> 0.197315).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.234152555465698
Epoch: 15, Steps: 65 | Train Loss: 0.3848468 Vali Loss: 0.1969223 Test Loss: 0.2735045
Validation loss decreased (0.197315 --> 0.196922).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 7.559703826904297
Epoch: 16, Steps: 65 | Train Loss: 0.3831531 Vali Loss: 0.1959556 Test Loss: 0.2732308
Validation loss decreased (0.196922 --> 0.195956).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.718120098114014
Epoch: 17, Steps: 65 | Train Loss: 0.3839911 Vali Loss: 0.1963341 Test Loss: 0.2729466
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.801783084869385
Epoch: 18, Steps: 65 | Train Loss: 0.3827002 Vali Loss: 0.1959054 Test Loss: 0.2726591
Validation loss decreased (0.195956 --> 0.195905).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.101993560791016
Epoch: 19, Steps: 65 | Train Loss: 0.3817465 Vali Loss: 0.1956909 Test Loss: 0.2724252
Validation loss decreased (0.195905 --> 0.195691).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 11.461377382278442
Epoch: 20, Steps: 65 | Train Loss: 0.3826179 Vali Loss: 0.1953240 Test Loss: 0.2722416
Validation loss decreased (0.195691 --> 0.195324).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.135692596435547
Epoch: 21, Steps: 65 | Train Loss: 0.3818810 Vali Loss: 0.1950913 Test Loss: 0.2720371
Validation loss decreased (0.195324 --> 0.195091).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 11.508857727050781
Epoch: 22, Steps: 65 | Train Loss: 0.3809527 Vali Loss: 0.1957685 Test Loss: 0.2718685
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.978402614593506
Epoch: 23, Steps: 65 | Train Loss: 0.3813051 Vali Loss: 0.1954784 Test Loss: 0.2717668
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 12.238755226135254
Epoch: 24, Steps: 65 | Train Loss: 0.3807500 Vali Loss: 0.1952210 Test Loss: 0.2715923
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2720048129558563, mae:0.32878440618515015, rse:0.42125874757766724, corr:[0.5425064  0.55517507 0.55714357 0.5539478  0.5527969  0.5542337
 0.556462   0.5573754  0.55634284 0.55484647 0.55432194 0.55489177
 0.55579996 0.5561234  0.5554142  0.5541744  0.5532125  0.55284435
 0.55279773 0.5526027  0.5519718  0.5510263  0.5501724  0.5497164
 0.5495575  0.5493553  0.5488364  0.54795885 0.5469708  0.54620355
 0.54577476 0.54549354 0.54507333 0.5444046  0.5435413  0.54265535
 0.54188836 0.54123616 0.540613   0.53999966 0.53942525 0.53890795
 0.538431   0.5379354  0.5373203  0.53656834 0.535773   0.5350308
 0.53437203 0.5337763  0.5331965  0.53257203 0.5318984  0.5312815
 0.530758   0.53028935 0.5297959  0.5292398  0.52867997 0.52824825
 0.52802867 0.5279407  0.52782327 0.52754736 0.52711344 0.52664405
 0.52630275 0.5261282  0.5260071  0.52579755 0.5254125  0.5248885
 0.5243582  0.5239237  0.5235568  0.52314687 0.52258813 0.52185917
 0.5210824  0.5204441  0.5199955  0.5196234  0.51914823 0.5184677
 0.5176181  0.5167697  0.51615405 0.51582664 0.51560736 0.5152642
 0.5146356  0.51366985 0.5125472  0.5114975  0.5105621  0.5095686
 0.5083587  0.50690967 0.50536335 0.50403607 0.5031305  0.50254
 0.50193083 0.5010166  0.4997811  0.49842703 0.49731898 0.4966115
 0.49614778 0.49559864 0.49476248 0.49368042 0.49262    0.49179995
 0.4912273  0.49069414 0.48993835 0.48881933 0.48746204 0.4861741
 0.48525676 0.4846456  0.48405266 0.48314294 0.4818241  0.48037228
 0.47924456 0.47865447 0.47836557 0.47793    0.47702974 0.47567716
 0.47424263 0.47318935 0.47268036 0.47242948 0.4719704  0.4710244
 0.46964252 0.4682645  0.46734598 0.46698877 0.46683684 0.4663885
 0.46542305 0.4641103  0.46304145 0.46264434 0.46283907 0.46308437
 0.46285212 0.4619424  0.4605411  0.45923552 0.4585382  0.45846248
 0.45851195 0.45816264 0.4572412  0.45605856 0.45516655 0.45494083
 0.45523676 0.4555777  0.45553032 0.45498684 0.45420432 0.45363837
 0.4535552  0.4537778  0.45380008 0.45331082 0.4523815  0.4514053
 0.45082098 0.4507294  0.45080036 0.45058876 0.4498697  0.44884825
 0.4478955  0.44732448 0.4471281  0.44692996 0.44636187 0.4453455
 0.44420978 0.44338936 0.44302112 0.44278625 0.44217068 0.44084078
 0.43892372 0.436951   0.4353833  0.43436617 0.4336417  0.4327193
 0.4313701  0.42976916 0.42838234 0.42755836 0.42721188 0.4268418
 0.4259731  0.42448413 0.4226978  0.4212897  0.4207352  0.4208355
 0.42079476 0.42003408 0.41853014 0.4168109  0.4156103  0.41515896
 0.4150697  0.41460502 0.41340414 0.41161114 0.4098245  0.4087499
 0.40854895 0.40869698 0.40846896 0.40735102 0.40554768 0.4038107
 0.40276012 0.402564   0.4026295  0.40230796 0.40131167 0.39996263
 0.39888144 0.39857757 0.39886597 0.39910823 0.3986901  0.39745334
 0.3958852  0.39470404 0.39446497 0.39508185 0.39574164 0.39580742
 0.395147   0.39421457 0.39361778 0.3936629  0.39418396 0.39457485
 0.39439547 0.3937682  0.39311522 0.39296988 0.39333072 0.39374572
 0.39362642 0.39275596 0.3915474  0.3907727  0.39089844 0.3917265
 0.3924682  0.39237556 0.39139012 0.39006525 0.38918883 0.389162
 0.38967162 0.390019   0.38960448 0.38858864 0.38763863 0.3874594
 0.388202   0.389103   0.3893099  0.3885331  0.3871469  0.38608894
 0.38600162 0.3867303  0.3874115  0.38714924 0.38564605 0.38331595
 0.38127807 0.3804488  0.38048667 0.38038534 0.37944597 0.3777296
 0.3762086  0.37571287 0.3761902  0.3767385  0.376487   0.37500757
 0.37309423 0.37192288 0.37227997 0.3734408  0.3741032  0.37331322
 0.37124804 0.36896166 0.36802822 0.36897704 0.3706708  0.37161404
 0.37070677 0.36848196 0.36653462 0.36640432 0.36811772 0.37007326
 0.37045512 0.3689289  0.36673892 0.36599484 0.36771432 0.3707019
 0.3725081  0.37172276 0.36903837 0.3670272  0.36872065 0.37341827
 0.37766293 0.37814343 0.37428257 0.37042782 0.37462184 0.3853986 ]
