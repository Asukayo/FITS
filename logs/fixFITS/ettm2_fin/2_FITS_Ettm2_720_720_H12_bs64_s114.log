Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20134912.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3921543
	speed: 0.1393s/iter; left time: 1782.7334s
	iters: 200, epoch: 1 | loss: 0.3915864
	speed: 0.1376s/iter; left time: 1747.2466s
Epoch: 1 cost time: 35.43592548370361
Epoch: 1, Steps: 258 | Train Loss: 0.4407683 Vali Loss: 0.2980641 Test Loss: 0.3959436
Validation loss decreased (inf --> 0.298064).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3730833
	speed: 0.5357s/iter; left time: 6719.6453s
	iters: 200, epoch: 2 | loss: 0.3025674
	speed: 0.1157s/iter; left time: 1439.7735s
Epoch: 2 cost time: 30.401151657104492
Epoch: 2, Steps: 258 | Train Loss: 0.3246943 Vali Loss: 0.2821398 Test Loss: 0.3779528
Validation loss decreased (0.298064 --> 0.282140).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2732590
	speed: 0.5924s/iter; left time: 7277.5803s
	iters: 200, epoch: 3 | loss: 0.3530373
	speed: 0.1413s/iter; left time: 1721.3281s
Epoch: 3 cost time: 38.17597508430481
Epoch: 3, Steps: 258 | Train Loss: 0.2925542 Vali Loss: 0.2754716 Test Loss: 0.3696584
Validation loss decreased (0.282140 --> 0.275472).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2068734
	speed: 0.5177s/iter; left time: 6226.2257s
	iters: 200, epoch: 4 | loss: 0.2820471
	speed: 0.1347s/iter; left time: 1606.5707s
Epoch: 4 cost time: 31.23155665397644
Epoch: 4, Steps: 258 | Train Loss: 0.2762980 Vali Loss: 0.2711940 Test Loss: 0.3646491
Validation loss decreased (0.275472 --> 0.271194).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2492316
	speed: 0.6102s/iter; left time: 7181.6202s
	iters: 200, epoch: 5 | loss: 0.3061903
	speed: 0.1403s/iter; left time: 1637.6790s
Epoch: 5 cost time: 36.22380256652832
Epoch: 5, Steps: 258 | Train Loss: 0.2676995 Vali Loss: 0.2685368 Test Loss: 0.3610025
Validation loss decreased (0.271194 --> 0.268537).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2821819
	speed: 0.5294s/iter; left time: 6093.8309s
	iters: 200, epoch: 6 | loss: 0.2365275
	speed: 0.1271s/iter; left time: 1450.0060s
Epoch: 6 cost time: 33.78122854232788
Epoch: 6, Steps: 258 | Train Loss: 0.2629355 Vali Loss: 0.2667826 Test Loss: 0.3586138
Validation loss decreased (0.268537 --> 0.266783).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3414450
	speed: 0.5370s/iter; left time: 6043.4044s
	iters: 200, epoch: 7 | loss: 0.2785385
	speed: 0.1287s/iter; left time: 1435.2016s
Epoch: 7 cost time: 32.169556856155396
Epoch: 7, Steps: 258 | Train Loss: 0.2600395 Vali Loss: 0.2653373 Test Loss: 0.3573277
Validation loss decreased (0.266783 --> 0.265337).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2673735
	speed: 0.5073s/iter; left time: 5578.2967s
	iters: 200, epoch: 8 | loss: 0.2522047
	speed: 0.1183s/iter; left time: 1288.7635s
Epoch: 8 cost time: 32.345993518829346
Epoch: 8, Steps: 258 | Train Loss: 0.2585558 Vali Loss: 0.2645434 Test Loss: 0.3562971
Validation loss decreased (0.265337 --> 0.264543).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2060364
	speed: 0.5714s/iter; left time: 6134.6721s
	iters: 200, epoch: 9 | loss: 0.2208962
	speed: 0.1292s/iter; left time: 1374.2661s
Epoch: 9 cost time: 34.87190270423889
Epoch: 9, Steps: 258 | Train Loss: 0.2574895 Vali Loss: 0.2639816 Test Loss: 0.3558054
Validation loss decreased (0.264543 --> 0.263982).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2843706
	speed: 0.5205s/iter; left time: 5454.6763s
	iters: 200, epoch: 10 | loss: 0.2297556
	speed: 0.1121s/iter; left time: 1163.1100s
Epoch: 10 cost time: 30.45998978614807
Epoch: 10, Steps: 258 | Train Loss: 0.2570240 Vali Loss: 0.2634017 Test Loss: 0.3554800
Validation loss decreased (0.263982 --> 0.263402).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2511830
	speed: 0.5138s/iter; left time: 5251.6993s
	iters: 200, epoch: 11 | loss: 0.1916731
	speed: 0.1054s/iter; left time: 1067.0793s
Epoch: 11 cost time: 30.093227863311768
Epoch: 11, Steps: 258 | Train Loss: 0.2565084 Vali Loss: 0.2634842 Test Loss: 0.3550071
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3002301
	speed: 0.5114s/iter; left time: 5095.2303s
	iters: 200, epoch: 12 | loss: 0.3813866
	speed: 0.1173s/iter; left time: 1157.0478s
Epoch: 12 cost time: 31.554458141326904
Epoch: 12, Steps: 258 | Train Loss: 0.2563727 Vali Loss: 0.2634016 Test Loss: 0.3548543
Validation loss decreased (0.263402 --> 0.263402).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2190208
	speed: 0.5976s/iter; left time: 5800.1210s
	iters: 200, epoch: 13 | loss: 0.3111260
	speed: 0.1386s/iter; left time: 1330.8900s
Epoch: 13 cost time: 37.03555417060852
Epoch: 13, Steps: 258 | Train Loss: 0.2563190 Vali Loss: 0.2632317 Test Loss: 0.3547258
Validation loss decreased (0.263402 --> 0.263232).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1932932
	speed: 0.5273s/iter; left time: 4981.8479s
	iters: 200, epoch: 14 | loss: 0.2369614
	speed: 0.1091s/iter; left time: 1019.7815s
Epoch: 14 cost time: 29.17174220085144
Epoch: 14, Steps: 258 | Train Loss: 0.2558964 Vali Loss: 0.2629077 Test Loss: 0.3546888
Validation loss decreased (0.263232 --> 0.262908).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2293981
	speed: 0.4906s/iter; left time: 4507.8441s
	iters: 200, epoch: 15 | loss: 0.2705281
	speed: 0.1126s/iter; left time: 1023.4023s
Epoch: 15 cost time: 30.00188112258911
Epoch: 15, Steps: 258 | Train Loss: 0.2560655 Vali Loss: 0.2630895 Test Loss: 0.3544040
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2385410
	speed: 0.4789s/iter; left time: 4277.0959s
	iters: 200, epoch: 16 | loss: 0.2277959
	speed: 0.1120s/iter; left time: 988.7031s
Epoch: 16 cost time: 29.107346057891846
Epoch: 16, Steps: 258 | Train Loss: 0.2556536 Vali Loss: 0.2626509 Test Loss: 0.3545426
Validation loss decreased (0.262908 --> 0.262651).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1800974
	speed: 0.6285s/iter; left time: 5450.9317s
	iters: 200, epoch: 17 | loss: 0.2384268
	speed: 0.1516s/iter; left time: 1299.9107s
Epoch: 17 cost time: 40.64154124259949
Epoch: 17, Steps: 258 | Train Loss: 0.2557578 Vali Loss: 0.2630401 Test Loss: 0.3541859
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2504621
	speed: 0.5765s/iter; left time: 4851.2183s
	iters: 200, epoch: 18 | loss: 0.2217940
	speed: 0.1332s/iter; left time: 1107.4617s
Epoch: 18 cost time: 34.82021689414978
Epoch: 18, Steps: 258 | Train Loss: 0.2557482 Vali Loss: 0.2628942 Test Loss: 0.3543081
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2606554
	speed: 0.5581s/iter; left time: 4552.4809s
	iters: 200, epoch: 19 | loss: 0.2772282
	speed: 0.1361s/iter; left time: 1096.5158s
Epoch: 19 cost time: 36.64808392524719
Epoch: 19, Steps: 258 | Train Loss: 0.2557846 Vali Loss: 0.2629282 Test Loss: 0.3543111
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20134912.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5082617
	speed: 0.1584s/iter; left time: 2027.2563s
	iters: 200, epoch: 1 | loss: 0.6925407
	speed: 0.1517s/iter; left time: 1926.8561s
Epoch: 1 cost time: 40.22462868690491
Epoch: 1, Steps: 258 | Train Loss: 0.4978257 Vali Loss: 0.2618927 Test Loss: 0.3535042
Validation loss decreased (inf --> 0.261893).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3512419
	speed: 0.6294s/iter; left time: 7894.9055s
	iters: 200, epoch: 2 | loss: 0.4708975
	speed: 0.1516s/iter; left time: 1886.3132s
Epoch: 2 cost time: 39.04640316963196
Epoch: 2, Steps: 258 | Train Loss: 0.4966888 Vali Loss: 0.2614143 Test Loss: 0.3531475
Validation loss decreased (0.261893 --> 0.261414).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5878066
	speed: 0.6310s/iter; left time: 7751.3262s
	iters: 200, epoch: 3 | loss: 0.2848004
	speed: 0.1368s/iter; left time: 1666.9481s
Epoch: 3 cost time: 38.37084698677063
Epoch: 3, Steps: 258 | Train Loss: 0.4960886 Vali Loss: 0.2609022 Test Loss: 0.3531485
Validation loss decreased (0.261414 --> 0.260902).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4220022
	speed: 0.5898s/iter; left time: 7093.2850s
	iters: 200, epoch: 4 | loss: 0.4512690
	speed: 0.1313s/iter; left time: 1565.5143s
Epoch: 4 cost time: 34.04516124725342
Epoch: 4, Steps: 258 | Train Loss: 0.4954786 Vali Loss: 0.2605546 Test Loss: 0.3526892
Validation loss decreased (0.260902 --> 0.260555).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3758281
	speed: 0.5908s/iter; left time: 6953.3920s
	iters: 200, epoch: 5 | loss: 0.4273502
	speed: 0.1361s/iter; left time: 1588.6750s
Epoch: 5 cost time: 37.1412615776062
Epoch: 5, Steps: 258 | Train Loss: 0.4948340 Vali Loss: 0.2606896 Test Loss: 0.3522432
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5807987
	speed: 0.6354s/iter; left time: 7314.6302s
	iters: 200, epoch: 6 | loss: 0.5240217
	speed: 0.1253s/iter; left time: 1430.3245s
Epoch: 6 cost time: 34.721497774124146
Epoch: 6, Steps: 258 | Train Loss: 0.4947521 Vali Loss: 0.2604330 Test Loss: 0.3523905
Validation loss decreased (0.260555 --> 0.260433).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4767069
	speed: 0.5733s/iter; left time: 6451.8254s
	iters: 200, epoch: 7 | loss: 0.5071837
	speed: 0.1282s/iter; left time: 1430.1412s
Epoch: 7 cost time: 35.28373050689697
Epoch: 7, Steps: 258 | Train Loss: 0.4947176 Vali Loss: 0.2603996 Test Loss: 0.3524675
Validation loss decreased (0.260433 --> 0.260400).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4027173
	speed: 0.5919s/iter; left time: 6508.3876s
	iters: 200, epoch: 8 | loss: 0.4526183
	speed: 0.1265s/iter; left time: 1378.5203s
Epoch: 8 cost time: 34.22779440879822
Epoch: 8, Steps: 258 | Train Loss: 0.4945425 Vali Loss: 0.2603760 Test Loss: 0.3522659
Validation loss decreased (0.260400 --> 0.260376).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5713989
	speed: 0.6067s/iter; left time: 6513.7651s
	iters: 200, epoch: 9 | loss: 0.4900648
	speed: 0.1423s/iter; left time: 1514.0373s
Epoch: 9 cost time: 39.6817262172699
Epoch: 9, Steps: 258 | Train Loss: 0.4944422 Vali Loss: 0.2604331 Test Loss: 0.3522440
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5033641
	speed: 0.6274s/iter; left time: 6574.2548s
	iters: 200, epoch: 10 | loss: 0.4257719
	speed: 0.1229s/iter; left time: 1275.6374s
Epoch: 10 cost time: 35.73711967468262
Epoch: 10, Steps: 258 | Train Loss: 0.4933693 Vali Loss: 0.2601751 Test Loss: 0.3521734
Validation loss decreased (0.260376 --> 0.260175).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6416537
	speed: 0.5851s/iter; left time: 5980.3168s
	iters: 200, epoch: 11 | loss: 0.4611283
	speed: 0.1337s/iter; left time: 1353.3481s
Epoch: 11 cost time: 33.365490436553955
Epoch: 11, Steps: 258 | Train Loss: 0.4944069 Vali Loss: 0.2606000 Test Loss: 0.3521425
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4798543
	speed: 0.5966s/iter; left time: 5944.3056s
	iters: 200, epoch: 12 | loss: 0.5228971
	speed: 0.1431s/iter; left time: 1411.3232s
Epoch: 12 cost time: 37.767831802368164
Epoch: 12, Steps: 258 | Train Loss: 0.4938041 Vali Loss: 0.2601462 Test Loss: 0.3522794
Validation loss decreased (0.260175 --> 0.260146).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.7009491
	speed: 0.6224s/iter; left time: 6040.1226s
	iters: 200, epoch: 13 | loss: 0.4284990
	speed: 0.1312s/iter; left time: 1260.0880s
Epoch: 13 cost time: 36.88266611099243
Epoch: 13, Steps: 258 | Train Loss: 0.4937700 Vali Loss: 0.2604405 Test Loss: 0.3520450
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.7515920
	speed: 0.6950s/iter; left time: 6565.2223s
	iters: 200, epoch: 14 | loss: 0.2876634
	speed: 0.1521s/iter; left time: 1421.7265s
Epoch: 14 cost time: 41.274736166000366
Epoch: 14, Steps: 258 | Train Loss: 0.4942798 Vali Loss: 0.2603353 Test Loss: 0.3520464
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3786956
	speed: 0.6087s/iter; left time: 5593.1926s
	iters: 200, epoch: 15 | loss: 0.6435884
	speed: 0.1380s/iter; left time: 1254.1944s
Epoch: 15 cost time: 35.82681727409363
Epoch: 15, Steps: 258 | Train Loss: 0.4935440 Vali Loss: 0.2604179 Test Loss: 0.3520396
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3489157259464264, mae:0.3780011236667633, rse:0.47479337453842163, corr:[0.54302025 0.54563534 0.54265904 0.5400118  0.5394709  0.54029614
 0.5408837  0.5403576  0.5392465  0.5384995  0.53853196 0.53905004
 0.53936344 0.5389577  0.5380384  0.5370895  0.5364885  0.536214
 0.5359244  0.5353107  0.5344198  0.5335023  0.5328579  0.53256875
 0.5324116  0.5321017  0.5314802  0.5306496  0.5298372  0.52924085
 0.52885216 0.5285596  0.5281427  0.52747315 0.5266626  0.5258961
 0.5252462  0.52470773 0.52416027 0.52355963 0.52294624 0.52238303
 0.5219677  0.5216743  0.5213704  0.52086526 0.52011174 0.51921445
 0.51829064 0.51745075 0.5167865  0.5162804  0.5157576  0.51511055
 0.51436013 0.51361483 0.5129807  0.51251215 0.51218545 0.5118946
 0.51159936 0.5113081  0.5110749  0.51096654 0.510993   0.5110639
 0.5109411  0.5106344  0.51019734 0.50969446 0.50920504 0.5088169
 0.508547   0.5083008  0.5079717  0.50756055 0.5071066  0.5066137
 0.5061106  0.50555676 0.5048803  0.50413144 0.50338095 0.5026887
 0.5021091  0.501639   0.50118893 0.5007527  0.5002959  0.49985984
 0.49948385 0.49910867 0.49861625 0.49789447 0.4968352  0.49542293
 0.4937682  0.49208486 0.49049094 0.48910102 0.48796377 0.48701972
 0.4861471  0.48522612 0.4841965  0.48309258 0.482017   0.4810785
 0.48027208 0.47957012 0.478899   0.47816402 0.47736147 0.47645506
 0.4754691  0.47450596 0.4736322  0.47278908 0.47192553 0.47094378
 0.46990657 0.46886295 0.46794757 0.4671709  0.466449   0.4656375
 0.4646537  0.46346903 0.46217382 0.4609402  0.4599587  0.45925707
 0.45872924 0.4581747  0.45747852 0.45665273 0.45580727 0.45511898
 0.45460567 0.4541584  0.45360094 0.4528794  0.45210293 0.45140988
 0.45094988 0.4506504  0.45039997 0.4498984  0.44899645 0.4477475
 0.44650176 0.445624   0.44515833 0.44491023 0.44459873 0.44401848
 0.44310752 0.4419729  0.44085196 0.44009706 0.43978083 0.4398018
 0.4399191  0.43997866 0.4398746  0.4395398  0.4389215  0.43810382
 0.43727684 0.43665928 0.4362874  0.43615532 0.43607616 0.43585813
 0.43536744 0.43465668 0.4339099  0.43331712 0.4329153  0.43260586
 0.4320951  0.43121    0.43005458 0.42891347 0.42813477 0.4278869
 0.42804697 0.42816347 0.42782137 0.42681116 0.42529655 0.42365754
 0.4223248  0.4215613  0.4210241  0.4203136  0.4192409  0.41779143
 0.41615734 0.41451386 0.41294238 0.41139323 0.40980712 0.408248
 0.40690428 0.40595    0.40539217 0.40504128 0.40467855 0.40410408
 0.40311638 0.40182194 0.40050638 0.399422   0.3986475  0.39797258
 0.3971908  0.3961591  0.39502087 0.39388397 0.392793   0.3917205
 0.39059347 0.3894168  0.38831875 0.38727328 0.3863173  0.3854671
 0.38454837 0.3834744  0.3822215  0.38093057 0.37982112 0.37907767
 0.37864062 0.37836185 0.3780236  0.37751988 0.37695003 0.37644294
 0.37612468 0.37594402 0.3758297  0.37580234 0.37569243 0.37552008
 0.37528595 0.3750027  0.37461334 0.37408912 0.37359458 0.373238
 0.3730746  0.37310675 0.373113   0.37294197 0.37249336 0.37191856
 0.37142372 0.3711877  0.37118    0.37122062 0.37099785 0.37042305
 0.36959267 0.36875942 0.36830416 0.36836    0.3686859  0.3688139
 0.36844817 0.36754283 0.3663428  0.36537382 0.36496013 0.3650766
 0.36544943 0.3655735  0.36517298 0.364426   0.3636578  0.36329722
 0.36343494 0.36387423 0.36422735 0.36412668 0.36343634 0.3621788
 0.3607047  0.3594333  0.35840532 0.35747927 0.35658073 0.35571682
 0.3551124  0.35480422 0.35462177 0.3543084  0.3537084  0.35275543
 0.35173225 0.35087416 0.35045496 0.3503008  0.35018474 0.34981662
 0.34917426 0.3483867  0.3478329  0.34780636 0.3482974  0.34906226
 0.34953648 0.34945533 0.34877604 0.3477442  0.34679344 0.34626094
 0.34613094 0.3462451  0.3463725  0.3463383  0.34612933 0.34589878
 0.34576163 0.34578285 0.34587127 0.34580326 0.3455585  0.3451267
 0.34470314 0.3443617  0.34420624 0.3443166  0.34459317 0.34503806
 0.3455198  0.3458495  0.34601706 0.34605628 0.3459281  0.34559086
 0.3451369  0.34468377 0.34431547 0.34406298 0.34392357 0.3438824
 0.34383258 0.34370726 0.34344047 0.3430375  0.34261218 0.34226337
 0.34205076 0.34197676 0.3418807  0.34172797 0.34134734 0.3408325
 0.3403141  0.339946   0.33981282 0.33991417 0.34013125 0.3404
 0.34054232 0.34055334 0.3404227  0.34022224 0.34006956 0.33999056
 0.34003913 0.3402258  0.34045935 0.34062332 0.34063447 0.34055108
 0.3404908  0.34048912 0.34047064 0.34027943 0.33992672 0.339321
 0.33860976 0.33793342 0.33735645 0.33676586 0.33598453 0.33502606
 0.33389777 0.3327685  0.33184835 0.33130068 0.33110604 0.33102912
 0.33084613 0.330469   0.32980376 0.3289634  0.32822153 0.3276337
 0.32716772 0.32668006 0.32611686 0.3255308  0.32493034 0.32448298
 0.3242511  0.32429066 0.32443944 0.32445166 0.32424095 0.32376453
 0.32321322 0.32281432 0.3226601  0.3227269  0.32282558 0.3228107
 0.32265064 0.322388   0.32213926 0.3221042  0.32238665 0.32290593
 0.32346895 0.32380992 0.323865   0.32372704 0.32364947 0.3238392
 0.32430935 0.32479313 0.325044   0.32488024 0.3242987  0.32343346
 0.3225116  0.32192943 0.32172236 0.32168826 0.3216172  0.3213915
 0.32088727 0.3202644  0.31969085 0.31927457 0.31908482 0.31901127
 0.3189575  0.31881833 0.31860492 0.31830683 0.31801823 0.31780228
 0.31757933 0.31724396 0.31672183 0.3160909  0.3156166  0.31537014
 0.31540918 0.3156096  0.31575352 0.31564713 0.31520852 0.3145466
 0.31390375 0.3134791  0.3133117  0.31327367 0.31322527 0.31296846
 0.31245822 0.31169897 0.3108671  0.31004122 0.3092862  0.30853182
 0.3076736  0.306725   0.3055913  0.304311   0.30303106 0.30191156
 0.30100816 0.30034372 0.29976538 0.2991948  0.2984709  0.29751816
 0.2964086  0.29540652 0.29468304 0.29421213 0.29392007 0.2936519
 0.29330003 0.2927346  0.291957   0.2911122  0.29023552 0.28946283
 0.2887212  0.28801167 0.28740573 0.2869124  0.28657597 0.28627804
 0.28592274 0.28547904 0.28499222 0.28459653 0.28441313 0.28440836
 0.28438398 0.28408742 0.283554   0.2827688  0.28187454 0.28110808
 0.28060886 0.28040788 0.2803566  0.28034353 0.28036425 0.28042316
 0.28051654 0.28065524 0.28075412 0.28071293 0.2804478  0.2799034
 0.2791434  0.27838084 0.2779128  0.27779797 0.27793977 0.27816084
 0.27829275 0.27828413 0.27812132 0.2779067  0.27765515 0.27731532
 0.27689037 0.27647927 0.27617487 0.27606723 0.27612948 0.27634287
 0.27652377 0.27654514 0.27633467 0.27593222 0.27533427 0.27478743
 0.27442217 0.27437133 0.27454516 0.2748257  0.27513123 0.27538326
 0.275526   0.27546498 0.27518463 0.27470905 0.27418643 0.27382198
 0.273705   0.2737412  0.27362943 0.27313444 0.272155   0.27068332
 0.2689682  0.26750687 0.26646852 0.26577684 0.2652042  0.2646013
 0.26391694 0.263141   0.2622767  0.26128265 0.26010916 0.25885135
 0.25762993 0.25658634 0.2559046  0.25551438 0.25522196 0.2548689
 0.254329   0.25358993 0.25286877 0.25227043 0.2519371  0.2518554
 0.25189304 0.25198686 0.25198853 0.25182858 0.2515993  0.25132746
 0.25092074 0.25042072 0.24996199 0.24953414 0.24923049 0.2490439
 0.2489883  0.24885349 0.24869947 0.24870025 0.24890628 0.24924056
 0.24957149 0.24983484 0.24994232 0.25000215 0.25020182 0.25069287
 0.25137562 0.251996   0.25230312 0.25224677 0.2518728  0.25136533
 0.25092098 0.25074726 0.25088644 0.25097424 0.25098133 0.25077325
 0.25043902 0.25019747 0.2501092  0.25026518 0.2502982  0.25000814
 0.24931867 0.24857911 0.24807511 0.24797168 0.24843518 0.24899106
 0.24930423 0.2490782  0.24841765 0.24767374 0.2473308  0.24739324
 0.2478366  0.24824731 0.24825713 0.24799365 0.24771881 0.24762191
 0.24773572 0.24805433 0.24822251 0.24809617 0.2478655  0.24773914
 0.24784867 0.24822147 0.2485764  0.24847972 0.24774301 0.24655622
 0.24526437 0.24417123 0.24334455 0.24263498 0.24195938 0.24119394
 0.24033825 0.23945928 0.23883876 0.23827948 0.23767276 0.23689197
 0.23602442 0.23529986 0.23490585 0.23484571 0.23507221 0.2352106
 0.23506199 0.23454516 0.23370168 0.23287877 0.23213455 0.23149186
 0.23099954 0.23058085 0.23025051 0.22980337 0.229196   0.22843702
 0.22782704 0.22739165 0.22728297 0.2274121  0.22744507 0.22708894
 0.22624353 0.22539102 0.22509441 0.22571161 0.2265498  0.22674747
 0.22562379 0.22359532 0.22227305 0.22310206 0.22526366 0.224722  ]
