Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7257600.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4725684
	speed: 0.1388s/iter; left time: 3575.3855s
	iters: 200, epoch: 1 | loss: 0.3663595
	speed: 0.1179s/iter; left time: 3024.7441s
	iters: 300, epoch: 1 | loss: 0.4017004
	speed: 0.1193s/iter; left time: 3048.8196s
	iters: 400, epoch: 1 | loss: 0.2680527
	speed: 0.1190s/iter; left time: 3029.7730s
	iters: 500, epoch: 1 | loss: 0.2172414
	speed: 0.1180s/iter; left time: 2992.1621s
Epoch: 1 cost time: 63.436118841171265
Epoch: 1, Steps: 517 | Train Loss: 0.3801253 Vali Loss: 0.2823307 Test Loss: 0.3754304
Validation loss decreased (inf --> 0.282331).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2327203
	speed: 0.7946s/iter; left time: 20049.7658s
	iters: 200, epoch: 2 | loss: 0.2759828
	speed: 0.1350s/iter; left time: 3391.9165s
	iters: 300, epoch: 2 | loss: 0.2982658
	speed: 0.1173s/iter; left time: 2937.0460s
	iters: 400, epoch: 2 | loss: 0.2639144
	speed: 0.1156s/iter; left time: 2881.7737s
	iters: 500, epoch: 2 | loss: 0.3495015
	speed: 0.1325s/iter; left time: 3289.7821s
Epoch: 2 cost time: 65.87208127975464
Epoch: 2, Steps: 517 | Train Loss: 0.2817560 Vali Loss: 0.2709241 Test Loss: 0.3616825
Validation loss decreased (0.282331 --> 0.270924).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1627458
	speed: 0.8769s/iter; left time: 21675.4037s
	iters: 200, epoch: 3 | loss: 0.1882614
	speed: 0.1545s/iter; left time: 3803.2102s
	iters: 300, epoch: 3 | loss: 0.1853756
	speed: 0.1449s/iter; left time: 3553.0530s
	iters: 400, epoch: 3 | loss: 0.1891788
	speed: 0.1391s/iter; left time: 3396.2472s
	iters: 500, epoch: 3 | loss: 0.2537297
	speed: 0.1423s/iter; left time: 3460.8183s
Epoch: 3 cost time: 76.94165015220642
Epoch: 3, Steps: 517 | Train Loss: 0.2647722 Vali Loss: 0.2667667 Test Loss: 0.3563466
Validation loss decreased (0.270924 --> 0.266767).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4351010
	speed: 0.8624s/iter; left time: 20870.8757s
	iters: 200, epoch: 4 | loss: 0.3370442
	speed: 0.1514s/iter; left time: 3648.5936s
	iters: 300, epoch: 4 | loss: 0.3428314
	speed: 0.1429s/iter; left time: 3429.1002s
	iters: 400, epoch: 4 | loss: 0.2794973
	speed: 0.1441s/iter; left time: 3444.9754s
	iters: 500, epoch: 4 | loss: 0.2514344
	speed: 0.1310s/iter; left time: 3116.8095s
Epoch: 4 cost time: 73.34977793693542
Epoch: 4, Steps: 517 | Train Loss: 0.2601687 Vali Loss: 0.2653629 Test Loss: 0.3539423
Validation loss decreased (0.266767 --> 0.265363).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1958792
	speed: 0.7205s/iter; left time: 17063.0389s
	iters: 200, epoch: 5 | loss: 0.2338321
	speed: 0.1179s/iter; left time: 2780.5960s
	iters: 300, epoch: 5 | loss: 0.2629459
	speed: 0.1218s/iter; left time: 2859.2327s
	iters: 400, epoch: 5 | loss: 0.1785180
	speed: 0.1248s/iter; left time: 2917.1477s
	iters: 500, epoch: 5 | loss: 0.3061374
	speed: 0.1154s/iter; left time: 2686.9580s
Epoch: 5 cost time: 63.33809447288513
Epoch: 5, Steps: 517 | Train Loss: 0.2586481 Vali Loss: 0.2642531 Test Loss: 0.3526554
Validation loss decreased (0.265363 --> 0.264253).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1644167
	speed: 0.8433s/iter; left time: 19537.0455s
	iters: 200, epoch: 6 | loss: 0.2512845
	speed: 0.1180s/iter; left time: 2721.5642s
	iters: 300, epoch: 6 | loss: 0.2712401
	speed: 0.1188s/iter; left time: 2727.8564s
	iters: 400, epoch: 6 | loss: 0.2780124
	speed: 0.1289s/iter; left time: 2948.2326s
	iters: 500, epoch: 6 | loss: 0.3332061
	speed: 0.1323s/iter; left time: 3011.1314s
Epoch: 6 cost time: 64.54863667488098
Epoch: 6, Steps: 517 | Train Loss: 0.2578319 Vali Loss: 0.2642037 Test Loss: 0.3520524
Validation loss decreased (0.264253 --> 0.264204).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2022764
	speed: 0.7525s/iter; left time: 17043.4053s
	iters: 200, epoch: 7 | loss: 0.1847098
	speed: 0.1404s/iter; left time: 3166.4807s
	iters: 300, epoch: 7 | loss: 0.2771868
	speed: 0.1257s/iter; left time: 2821.8516s
	iters: 400, epoch: 7 | loss: 0.1564602
	speed: 0.1427s/iter; left time: 3189.6014s
	iters: 500, epoch: 7 | loss: 0.2116804
	speed: 0.1307s/iter; left time: 2907.6162s
Epoch: 7 cost time: 69.88804244995117
Epoch: 7, Steps: 517 | Train Loss: 0.2577609 Vali Loss: 0.2638960 Test Loss: 0.3522213
Validation loss decreased (0.264204 --> 0.263896).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1955270
	speed: 0.7790s/iter; left time: 17239.7355s
	iters: 200, epoch: 8 | loss: 0.2302207
	speed: 0.1247s/iter; left time: 2747.8476s
	iters: 300, epoch: 8 | loss: 0.1729795
	speed: 0.1180s/iter; left time: 2587.0886s
	iters: 400, epoch: 8 | loss: 0.2020456
	speed: 0.1113s/iter; left time: 2429.6831s
	iters: 500, epoch: 8 | loss: 0.1505764
	speed: 0.1041s/iter; left time: 2262.5335s
Epoch: 8 cost time: 61.45849943161011
Epoch: 8, Steps: 517 | Train Loss: 0.2575172 Vali Loss: 0.2633735 Test Loss: 0.3519922
Validation loss decreased (0.263896 --> 0.263374).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1921313
	speed: 0.7927s/iter; left time: 17133.7231s
	iters: 200, epoch: 9 | loss: 0.2215478
	speed: 0.1167s/iter; left time: 2511.3318s
	iters: 300, epoch: 9 | loss: 0.1626942
	speed: 0.1498s/iter; left time: 3207.6980s
	iters: 400, epoch: 9 | loss: 0.2926617
	speed: 0.1430s/iter; left time: 3047.2244s
	iters: 500, epoch: 9 | loss: 0.2813452
	speed: 0.1428s/iter; left time: 3029.5044s
Epoch: 9 cost time: 71.22586870193481
Epoch: 9, Steps: 517 | Train Loss: 0.2574441 Vali Loss: 0.2637817 Test Loss: 0.3519388
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3097174
	speed: 0.8829s/iter; left time: 18627.4380s
	iters: 200, epoch: 10 | loss: 0.2942052
	speed: 0.1494s/iter; left time: 3137.0732s
	iters: 300, epoch: 10 | loss: 0.2958669
	speed: 0.1615s/iter; left time: 3375.4946s
	iters: 400, epoch: 10 | loss: 0.2618145
	speed: 0.1631s/iter; left time: 3392.5293s
	iters: 500, epoch: 10 | loss: 0.2966673
	speed: 0.1581s/iter; left time: 3273.2034s
Epoch: 10 cost time: 81.11557030677795
Epoch: 10, Steps: 517 | Train Loss: 0.2573333 Vali Loss: 0.2637250 Test Loss: 0.3518348
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2220395
	speed: 1.0551s/iter; left time: 21715.3513s
	iters: 200, epoch: 11 | loss: 0.3551167
	speed: 0.1646s/iter; left time: 3370.5566s
	iters: 300, epoch: 11 | loss: 0.1618737
	speed: 0.1693s/iter; left time: 3451.1354s
	iters: 400, epoch: 11 | loss: 0.1980383
	speed: 0.1843s/iter; left time: 3737.9412s
	iters: 500, epoch: 11 | loss: 0.2061040
	speed: 0.1791s/iter; left time: 3615.4040s
Epoch: 11 cost time: 89.62251853942871
Epoch: 11, Steps: 517 | Train Loss: 0.2573230 Vali Loss: 0.2640015 Test Loss: 0.3515858
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7257600.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6679025
	speed: 0.1581s/iter; left time: 4070.8100s
	iters: 200, epoch: 1 | loss: 0.7261059
	speed: 0.1548s/iter; left time: 3969.5207s
	iters: 300, epoch: 1 | loss: 0.8581312
	speed: 0.1444s/iter; left time: 3689.0576s
	iters: 400, epoch: 1 | loss: 0.5960643
	speed: 0.1588s/iter; left time: 4042.3682s
	iters: 500, epoch: 1 | loss: 0.5516657
	speed: 0.1577s/iter; left time: 3997.1472s
Epoch: 1 cost time: 80.16392660140991
Epoch: 1, Steps: 517 | Train Loss: 0.4978324 Vali Loss: 0.2622605 Test Loss: 0.3506277
Validation loss decreased (inf --> 0.262261).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6002509
	speed: 1.1041s/iter; left time: 27860.9560s
	iters: 200, epoch: 2 | loss: 0.5339414
	speed: 0.1561s/iter; left time: 3922.6501s
	iters: 300, epoch: 2 | loss: 0.5635599
	speed: 0.1630s/iter; left time: 4080.0653s
	iters: 400, epoch: 2 | loss: 0.3958895
	speed: 0.1624s/iter; left time: 4049.3364s
	iters: 500, epoch: 2 | loss: 0.9081907
	speed: 0.1920s/iter; left time: 4766.9778s
Epoch: 2 cost time: 87.99282670021057
Epoch: 2, Steps: 517 | Train Loss: 0.4970629 Vali Loss: 0.2610369 Test Loss: 0.3508385
Validation loss decreased (0.262261 --> 0.261037).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3862581
	speed: 1.4850s/iter; left time: 36705.3778s
	iters: 200, epoch: 3 | loss: 0.6946442
	speed: 0.2165s/iter; left time: 5328.4495s
	iters: 300, epoch: 3 | loss: 0.4978974
	speed: 0.1812s/iter; left time: 4442.9381s
	iters: 400, epoch: 3 | loss: 0.3860092
	speed: 0.1719s/iter; left time: 4198.1520s
	iters: 500, epoch: 3 | loss: 0.5385964
	speed: 0.1537s/iter; left time: 3737.6300s
Epoch: 3 cost time: 100.14724898338318
Epoch: 3, Steps: 517 | Train Loss: 0.4959057 Vali Loss: 0.2617138 Test Loss: 0.3502476
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3943445
	speed: 1.1065s/iter; left time: 26777.0972s
	iters: 200, epoch: 4 | loss: 0.2943259
	speed: 0.1805s/iter; left time: 4349.9331s
	iters: 300, epoch: 4 | loss: 0.6929519
	speed: 0.1751s/iter; left time: 4201.9189s
	iters: 400, epoch: 4 | loss: 0.5448408
	speed: 0.1619s/iter; left time: 3868.9572s
	iters: 500, epoch: 4 | loss: 0.6165677
	speed: 0.1609s/iter; left time: 3829.4055s
Epoch: 4 cost time: 90.36315703392029
Epoch: 4, Steps: 517 | Train Loss: 0.4952654 Vali Loss: 0.2607208 Test Loss: 0.3502634
Validation loss decreased (0.261037 --> 0.260721).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3397101
	speed: 1.0621s/iter; left time: 25153.4150s
	iters: 200, epoch: 5 | loss: 0.3938914
	speed: 0.1655s/iter; left time: 3902.5935s
	iters: 300, epoch: 5 | loss: 0.4369011
	speed: 0.1603s/iter; left time: 3765.1322s
	iters: 400, epoch: 5 | loss: 0.4998554
	speed: 0.1594s/iter; left time: 3728.3757s
	iters: 500, epoch: 5 | loss: 0.5290288
	speed: 0.1739s/iter; left time: 4049.9286s
Epoch: 5 cost time: 86.69597697257996
Epoch: 5, Steps: 517 | Train Loss: 0.4949106 Vali Loss: 0.2614790 Test Loss: 0.3499140
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5331577
	speed: 1.0631s/iter; left time: 24628.6209s
	iters: 200, epoch: 6 | loss: 0.6365779
	speed: 0.1918s/iter; left time: 4424.6413s
	iters: 300, epoch: 6 | loss: 0.5935756
	speed: 0.1843s/iter; left time: 4232.9309s
	iters: 400, epoch: 6 | loss: 0.5634150
	speed: 0.1812s/iter; left time: 4144.3361s
	iters: 500, epoch: 6 | loss: 0.5624094
	speed: 0.2047s/iter; left time: 4660.5499s
Epoch: 6 cost time: 99.38651895523071
Epoch: 6, Steps: 517 | Train Loss: 0.4952392 Vali Loss: 0.2609103 Test Loss: 0.3504679
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4550712
	speed: 1.2310s/iter; left time: 27880.4438s
	iters: 200, epoch: 7 | loss: 0.4156631
	speed: 0.1842s/iter; left time: 4153.5611s
	iters: 300, epoch: 7 | loss: 0.3946722
	speed: 0.1679s/iter; left time: 3770.1068s
	iters: 400, epoch: 7 | loss: 0.5473349
	speed: 0.1734s/iter; left time: 3875.4397s
	iters: 500, epoch: 7 | loss: 0.3898383
	speed: 0.1752s/iter; left time: 3898.8826s
Epoch: 7 cost time: 91.90173482894897
Epoch: 7, Steps: 517 | Train Loss: 0.4950227 Vali Loss: 0.2611466 Test Loss: 0.3500571
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.34961214661598206, mae:0.3781079947948456, rse:0.4752669930458069, corr:[0.53968346 0.5447312  0.5450033  0.5425039  0.5403291  0.5395998
 0.53999233 0.5406164  0.54064816 0.5398901  0.53879166 0.5379415
 0.53767204 0.5379385  0.53840077 0.53851813 0.53799015 0.53692746
 0.5357125  0.534714   0.53414184 0.5339357  0.533879   0.53370273
 0.5332132  0.53244585 0.5315737  0.53078246 0.5301711  0.5297413
 0.5293927  0.5290256  0.5285452  0.5279298  0.5272608  0.52662593
 0.5260382  0.52549124 0.524906   0.52423877 0.5235026  0.5227554
 0.5220877  0.5215206  0.521026   0.5205439  0.52002096 0.51940495
 0.5186413  0.51773876 0.5168205  0.51600194 0.5153069  0.5147352
 0.5142487  0.51377624 0.5132748  0.5127725  0.51234    0.5120021
 0.51178867 0.5116534  0.5115165  0.5112799  0.51090145 0.5104533
 0.50996494 0.50958556 0.5093673  0.5092759  0.5092088  0.5090604
 0.50878686 0.5083732  0.5078415  0.5072463  0.506654   0.5060839
 0.50556237 0.5050008  0.5043076  0.5034999  0.50265735 0.50187725
 0.5012599  0.5008335  0.5005173  0.5002412  0.49985656 0.49929804
 0.4985514  0.4976095  0.4965149  0.49534583 0.49414766 0.4929534
 0.4917788  0.49063376 0.48944667 0.4881919  0.4869032  0.48564273
 0.4844566  0.4833718  0.48237547 0.48145506 0.4805924  0.4797581
 0.47887093 0.47788763 0.4768079  0.47567308 0.47462836 0.47374353
 0.47306007 0.47258353 0.47222805 0.4718149  0.47120917 0.47031736
 0.46922114 0.4680454  0.46702048 0.46627194 0.46578583 0.46538633
 0.46485034 0.46397066 0.46267077 0.46103245 0.45933035 0.45791727
 0.45711884 0.4569703  0.45722353 0.45744807 0.4572881  0.45664194
 0.45559755 0.45439592 0.45330128 0.45248988 0.45195308 0.4514557
 0.45075926 0.44969615 0.4483775  0.44699255 0.44582155 0.44502798
 0.44464114 0.44450304 0.44432974 0.44396785 0.44339848 0.44271666
 0.44205528 0.44150475 0.44105682 0.44069964 0.44030422 0.43983427
 0.43931118 0.438851   0.43853596 0.4383615  0.4382224  0.4379772
 0.43750784 0.4367856  0.43583977 0.4348923  0.43412387 0.4336396
 0.4333637  0.43313998 0.43279812 0.43226162 0.4315851  0.43098763
 0.43063316 0.4305902  0.43077388 0.43092826 0.43077213 0.43007755
 0.4288208  0.42707938 0.4251572  0.42337698 0.4219862  0.42099884
 0.4202914  0.41977507 0.41915166 0.4182906  0.4172586  0.41617277
 0.4151954  0.41438007 0.41367126 0.412961   0.4121401  0.4111532
 0.41000858 0.40875217 0.4074361  0.406138   0.40499115 0.40408793
 0.4033173  0.4025951  0.40187296 0.40113196 0.4003683  0.39947605
 0.39837894 0.39700952 0.39550087 0.3939914  0.39263096 0.3915406
 0.39074484 0.39018428 0.38974878 0.38915506 0.3882443  0.3870287
 0.38555244 0.38402197 0.3826211  0.38148832 0.3806471  0.38006794
 0.37962565 0.37925205 0.37886387 0.3784177  0.3779444  0.37747204
 0.3770615  0.3767086  0.376426   0.3762836  0.3761169  0.3758386
 0.37536302 0.37470222 0.37395468 0.37330985 0.3730344  0.37322003
 0.37379128 0.37458473 0.37527153 0.37561396 0.37546867 0.37491393
 0.3741196  0.3733073  0.3726232  0.37214336 0.37180063 0.37156004
 0.3713387  0.3710612  0.37077725 0.37054107 0.37035045 0.37016264
 0.37000126 0.3698357  0.36961323 0.36938158 0.36914513 0.36888817
 0.36860418 0.36813387 0.3673651  0.36636013 0.36517105 0.3640192
 0.3631108  0.3626345  0.36260936 0.36283603 0.36301968 0.362817
 0.36216074 0.3612185  0.36014107 0.35909572 0.35824722 0.35761875
 0.3572279  0.35692176 0.3565467  0.35604307 0.35548294 0.3549114
 0.35445255 0.3540227  0.3535914  0.35295635 0.35208824 0.35103014
 0.34997845 0.3490766  0.34849265 0.3482896  0.34837344 0.34864917
 0.34883267 0.34881717 0.3484966  0.347876   0.34710464 0.34635958
 0.3457055  0.34521502 0.34490108 0.34471568 0.34461492 0.344627
 0.34476036 0.34505162 0.34545067 0.34579143 0.3460202  0.34605092
 0.34594542 0.34570292 0.34539694 0.34512654 0.34488055 0.34474468
 0.3446958  0.34465387 0.34468082 0.34483492 0.3450579  0.34519848
 0.34515005 0.34483922 0.34425822 0.34347847 0.3426604  0.34201854
 0.3416518  0.34158435 0.34169608 0.34181994 0.34185427 0.34175652
 0.3415833  0.34146294 0.3414288  0.3415661  0.34172675 0.3418464
 0.341831   0.34163433 0.34126174 0.3407666  0.3402003  0.3396953
 0.339253   0.3389506  0.33876294 0.3386861  0.33871716 0.3387783
 0.3388413  0.33891317 0.33896652 0.3389718  0.3389036  0.33880347
 0.33868843 0.3384992  0.33813116 0.33747733 0.33665943 0.3357486
 0.33500138 0.33457518 0.33448517 0.334561   0.33454874 0.33433062
 0.33376232 0.33288428 0.33187723 0.33100978 0.3304971  0.3303437
 0.33040524 0.33047408 0.3302133  0.3294405  0.3282288  0.32671973
 0.32524574 0.32409495 0.32344368 0.32327518 0.32332653 0.32340774
 0.32332596 0.32306355 0.32264704 0.32213777 0.3216801  0.3212906
 0.32102823 0.32088378 0.32078227 0.32072583 0.3207252  0.32085288
 0.32116693 0.32160613 0.32200968 0.32225502 0.3222989  0.32218954
 0.32206684 0.3220231  0.32214072 0.32235843 0.3225609  0.32260188
 0.32238495 0.32189104 0.32132846 0.32096156 0.32097453 0.32131785
 0.3217516  0.32211757 0.32216448 0.3217513  0.32098255 0.3201589
 0.3194686  0.3191155  0.31907523 0.31917006 0.3192355  0.3191068
 0.31878683 0.3183502  0.31795508 0.31765434 0.3175121  0.31754214
 0.31765333 0.31775343 0.31775513 0.31760976 0.31734818 0.31685823
 0.3161452  0.31522864 0.31418207 0.31315234 0.31234232 0.31194577
 0.3120136  0.31235927 0.3126703  0.31267965 0.3123122  0.31157216
 0.31063145 0.30967256 0.30893254 0.30841944 0.30802298 0.3074951
 0.3066254  0.3054306  0.3039731  0.30250028 0.30129397 0.3004814
 0.29996088 0.29955372 0.29899928 0.29821426 0.29715675 0.29592377
 0.29471585 0.29378495 0.29318598 0.29276964 0.2923795  0.2918626
 0.29119706 0.29038966 0.28957382 0.288956   0.2885319  0.28828946
 0.2880084  0.28753817 0.28685847 0.2860302  0.28528273 0.28474224
 0.28446755 0.2843673  0.28423893 0.28391358 0.28334117 0.28257707
 0.28176102 0.28103945 0.28066564 0.28059837 0.28070012 0.28082335
 0.28083342 0.28071216 0.28048685 0.28026646 0.28016087 0.28014374
 0.28007144 0.2798337  0.279341   0.27862406 0.27784598 0.27718478
 0.27677807 0.2766928  0.27695078 0.2773401  0.27763113 0.27764538
 0.27733713 0.27683467 0.2763102  0.27600253 0.27599314 0.27622536
 0.27655593 0.27684817 0.27695617 0.27681193 0.27640635 0.27591264
 0.27546126 0.2752046  0.27518237 0.2753549  0.27548292 0.27553982
 0.2754924  0.2754242  0.27531016 0.27512544 0.2748791  0.27457058
 0.27421752 0.27383116 0.2735024  0.27330655 0.2733063  0.27348146
 0.27370122 0.2737559  0.27339157 0.27249953 0.27110827 0.26930305
 0.26736027 0.26575068 0.26465166 0.26400277 0.26353467 0.26299545
 0.26222962 0.26122043 0.2600851  0.258983   0.25804657 0.2573718
 0.2568986  0.25648105 0.25606167 0.25554198 0.25490007 0.25423452
 0.25363433 0.25313607 0.25283518 0.25265202 0.25257638 0.25254008
 0.25243446 0.25223148 0.25186282 0.25134215 0.25085026 0.2505257
 0.2503885  0.25047985 0.2508189  0.2512181  0.25155795 0.25168467
 0.25157902 0.25108936 0.2503367  0.24955644 0.2489195  0.24854665
 0.24851568 0.248874   0.24947524 0.25015214 0.25072482 0.25106743
 0.2510689  0.25072253 0.25017184 0.24971269 0.24953046 0.24966626
 0.24998991 0.2503694  0.25069672 0.25072172 0.25059316 0.25033614
 0.25005177 0.24982862 0.24961214 0.24949169 0.24931413 0.24915008
 0.24903086 0.24914627 0.24943574 0.24973771 0.25007084 0.2501513
 0.24995649 0.24945962 0.2488077  0.2481802  0.2478157  0.24762055
 0.24766047 0.24774519 0.24767046 0.24750775 0.24734981 0.24728613
 0.24742614 0.24797559 0.24877794 0.24963504 0.2503683  0.2507162
 0.25053424 0.24993494 0.24906543 0.24801354 0.24688122 0.24575387
 0.24462368 0.24345903 0.24223584 0.24098796 0.23992069 0.23914468
 0.2386929  0.238479   0.2385207  0.23849575 0.23823306 0.23759183
 0.23662274 0.2355073  0.23444003 0.23352194 0.23284923 0.2322386
 0.23164399 0.2310702  0.2305749  0.23043749 0.23059832 0.2308698
 0.23107244 0.23096523 0.23052506 0.22971235 0.22877763 0.22793308
 0.22741567 0.2270094  0.22664012 0.22627056 0.22592092 0.22573273
 0.22572973 0.22591512 0.2260081  0.22582306 0.22497506 0.22366613
 0.22240429 0.22193971 0.22269566 0.2240557  0.22405307 0.22013469]
