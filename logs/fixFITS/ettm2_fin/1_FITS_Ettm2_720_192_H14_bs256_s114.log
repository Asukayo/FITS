Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=122, out_features=154, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  67336192.0
params:  18942.0
Trainable parameters:  18942
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.754932880401611
Epoch: 1, Steps: 65 | Train Loss: 0.4602811 Vali Loss: 0.2029548 Test Loss: 0.2676693
Validation loss decreased (inf --> 0.202955).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.472902297973633
Epoch: 2, Steps: 65 | Train Loss: 0.3697054 Vali Loss: 0.1817582 Test Loss: 0.2442096
Validation loss decreased (0.202955 --> 0.181758).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.841838836669922
Epoch: 3, Steps: 65 | Train Loss: 0.3435870 Vali Loss: 0.1732102 Test Loss: 0.2355766
Validation loss decreased (0.181758 --> 0.173210).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.518380165100098
Epoch: 4, Steps: 65 | Train Loss: 0.3313545 Vali Loss: 0.1683296 Test Loss: 0.2307331
Validation loss decreased (0.173210 --> 0.168330).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.374590158462524
Epoch: 5, Steps: 65 | Train Loss: 0.3239491 Vali Loss: 0.1652122 Test Loss: 0.2276292
Validation loss decreased (0.168330 --> 0.165212).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.312849998474121
Epoch: 6, Steps: 65 | Train Loss: 0.3182952 Vali Loss: 0.1630718 Test Loss: 0.2254554
Validation loss decreased (0.165212 --> 0.163072).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.430757761001587
Epoch: 7, Steps: 65 | Train Loss: 0.3145174 Vali Loss: 0.1613864 Test Loss: 0.2237509
Validation loss decreased (0.163072 --> 0.161386).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.30444860458374
Epoch: 8, Steps: 65 | Train Loss: 0.3102297 Vali Loss: 0.1602284 Test Loss: 0.2224231
Validation loss decreased (0.161386 --> 0.160228).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.176192045211792
Epoch: 9, Steps: 65 | Train Loss: 0.3080911 Vali Loss: 0.1589580 Test Loss: 0.2214230
Validation loss decreased (0.160228 --> 0.158958).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.820338487625122
Epoch: 10, Steps: 65 | Train Loss: 0.3052744 Vali Loss: 0.1582695 Test Loss: 0.2204612
Validation loss decreased (0.158958 --> 0.158270).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.963244915008545
Epoch: 11, Steps: 65 | Train Loss: 0.3043786 Vali Loss: 0.1573559 Test Loss: 0.2197872
Validation loss decreased (0.158270 --> 0.157356).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.319801330566406
Epoch: 12, Steps: 65 | Train Loss: 0.3031146 Vali Loss: 0.1570812 Test Loss: 0.2192353
Validation loss decreased (0.157356 --> 0.157081).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.888633966445923
Epoch: 13, Steps: 65 | Train Loss: 0.3023885 Vali Loss: 0.1567058 Test Loss: 0.2188275
Validation loss decreased (0.157081 --> 0.156706).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.303927898406982
Epoch: 14, Steps: 65 | Train Loss: 0.3006606 Vali Loss: 0.1561590 Test Loss: 0.2184500
Validation loss decreased (0.156706 --> 0.156159).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.425319910049438
Epoch: 15, Steps: 65 | Train Loss: 0.3006972 Vali Loss: 0.1558260 Test Loss: 0.2179772
Validation loss decreased (0.156159 --> 0.155826).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.064256191253662
Epoch: 16, Steps: 65 | Train Loss: 0.2995415 Vali Loss: 0.1554824 Test Loss: 0.2177549
Validation loss decreased (0.155826 --> 0.155482).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.159984111785889
Epoch: 17, Steps: 65 | Train Loss: 0.2989361 Vali Loss: 0.1554135 Test Loss: 0.2175182
Validation loss decreased (0.155482 --> 0.155413).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.703863859176636
Epoch: 18, Steps: 65 | Train Loss: 0.2981085 Vali Loss: 0.1551374 Test Loss: 0.2172787
Validation loss decreased (0.155413 --> 0.155137).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.739566564559937
Epoch: 19, Steps: 65 | Train Loss: 0.2979595 Vali Loss: 0.1548801 Test Loss: 0.2170160
Validation loss decreased (0.155137 --> 0.154880).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 11.050022602081299
Epoch: 20, Steps: 65 | Train Loss: 0.2972288 Vali Loss: 0.1545683 Test Loss: 0.2168911
Validation loss decreased (0.154880 --> 0.154568).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.050263404846191
Epoch: 21, Steps: 65 | Train Loss: 0.2969405 Vali Loss: 0.1545476 Test Loss: 0.2167363
Validation loss decreased (0.154568 --> 0.154548).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.762867450714111
Epoch: 22, Steps: 65 | Train Loss: 0.2958435 Vali Loss: 0.1543241 Test Loss: 0.2164900
Validation loss decreased (0.154548 --> 0.154324).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 11.03305459022522
Epoch: 23, Steps: 65 | Train Loss: 0.2961224 Vali Loss: 0.1542357 Test Loss: 0.2163524
Validation loss decreased (0.154324 --> 0.154236).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 11.132744312286377
Epoch: 24, Steps: 65 | Train Loss: 0.2957265 Vali Loss: 0.1542955 Test Loss: 0.2163151
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.860894918441772
Epoch: 25, Steps: 65 | Train Loss: 0.2947758 Vali Loss: 0.1540903 Test Loss: 0.2160941
Validation loss decreased (0.154236 --> 0.154090).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.956846714019775
Epoch: 26, Steps: 65 | Train Loss: 0.2946054 Vali Loss: 0.1538018 Test Loss: 0.2160073
Validation loss decreased (0.154090 --> 0.153802).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.745755195617676
Epoch: 27, Steps: 65 | Train Loss: 0.2943406 Vali Loss: 0.1539329 Test Loss: 0.2159020
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.859179019927979
Epoch: 28, Steps: 65 | Train Loss: 0.2948701 Vali Loss: 0.1537580 Test Loss: 0.2158165
Validation loss decreased (0.153802 --> 0.153758).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.510561227798462
Epoch: 29, Steps: 65 | Train Loss: 0.2943065 Vali Loss: 0.1537171 Test Loss: 0.2157007
Validation loss decreased (0.153758 --> 0.153717).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 11.073637008666992
Epoch: 30, Steps: 65 | Train Loss: 0.2947382 Vali Loss: 0.1536312 Test Loss: 0.2156632
Validation loss decreased (0.153717 --> 0.153631).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 11.448354721069336
Epoch: 31, Steps: 65 | Train Loss: 0.2941723 Vali Loss: 0.1535489 Test Loss: 0.2155884
Validation loss decreased (0.153631 --> 0.153549).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 11.013866901397705
Epoch: 32, Steps: 65 | Train Loss: 0.2935130 Vali Loss: 0.1534997 Test Loss: 0.2154853
Validation loss decreased (0.153549 --> 0.153500).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.291203498840332
Epoch: 33, Steps: 65 | Train Loss: 0.2936872 Vali Loss: 0.1533736 Test Loss: 0.2154265
Validation loss decreased (0.153500 --> 0.153374).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 8.889577150344849
Epoch: 34, Steps: 65 | Train Loss: 0.2935461 Vali Loss: 0.1534393 Test Loss: 0.2153252
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 9.513216972351074
Epoch: 35, Steps: 65 | Train Loss: 0.2921050 Vali Loss: 0.1535049 Test Loss: 0.2152938
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 9.898160457611084
Epoch: 36, Steps: 65 | Train Loss: 0.2936755 Vali Loss: 0.1533749 Test Loss: 0.2152651
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.21937768161296844, mae:0.29365575313568115, rse:0.37913277745246887, corr:[0.5509017  0.56354517 0.5610403  0.5581942  0.5593563  0.56235176
 0.5634882  0.5620647  0.56058747 0.56062263 0.56179655 0.5626628
 0.56222564 0.5610952  0.56042874 0.56054467 0.5609148  0.5606941
 0.55969715 0.5585466  0.5578641  0.5576931  0.5576282  0.55720407
 0.556407   0.5556245  0.55512685 0.5548368  0.5544657  0.55380285
 0.55296373 0.55223966 0.5517251  0.551346   0.55092585 0.5502996
 0.5494647  0.5485799  0.5478071  0.5472395  0.5468212  0.5463648
 0.54576516 0.5450525  0.544335   0.5437062  0.54313636 0.5424877
 0.541646   0.5407007  0.53987825 0.5393378  0.53894466 0.5384531
 0.537738   0.5369299  0.5362431  0.5358338  0.5356436  0.53543305
 0.5350467  0.5345576  0.5341407  0.5339354  0.53385675 0.5337266
 0.5333826  0.5329272  0.5325599  0.53236353 0.532227   0.5319783
 0.5315264  0.5309516  0.53039694 0.5299708  0.5295945  0.529101
 0.52843297 0.5276969  0.52702105 0.52648944 0.5260251  0.5254877
 0.5248191  0.52413505 0.5235046  0.5229971  0.52251714 0.5219617
 0.5213498  0.5207721  0.5202339  0.51967365 0.5188749  0.51767826
 0.5162143  0.5148176  0.51366997 0.51271015 0.5117028  0.5104446
 0.5090247  0.507707   0.50666654 0.5058215  0.5049288  0.50385666
 0.5026399  0.501622   0.50104207 0.50069726 0.5002313  0.49930462
 0.4980019  0.49681145 0.49607414 0.49560246 0.49499407 0.4939035
 0.49258617 0.4914608  0.49089774 0.49063346 0.49014792 0.48919886
 0.4880367  0.48708972 0.48648638 0.48590964 0.4849799  0.48363194
 0.48230547 0.48149455 0.48125708 0.481102   0.48046002 0.47938877
 0.47836155 0.4779202  0.4779665  0.47791502 0.47724283 0.47589388
 0.4744868  0.4735905  0.47338653 0.473174   0.47239375 0.47104788
 0.46993652 0.46971995 0.47000018 0.46981853 0.46870917 0.46714354
 0.46602976 0.46595913 0.46637198 0.46642855 0.46549797 0.46409166
 0.4631472  0.46327087 0.46385813 0.46385992 0.46283123 0.46152395
 0.46115932 0.46207    0.4629651  0.46260795 0.46094516 0.45934072
 0.45936617 0.46071124 0.46146342 0.4602927  0.45761287 0.45589453
 0.4566407  0.45829925 0.45828226 0.4557848  0.45267966 0.45241085
 0.45526552 0.45704493 0.45483527 0.45056242 0.45157322 0.45640033]
