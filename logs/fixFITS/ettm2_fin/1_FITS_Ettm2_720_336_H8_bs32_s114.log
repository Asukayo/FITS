Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=74, out_features=108, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3580416.0
params:  8100.0
Trainable parameters:  8100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4058618
	speed: 0.1407s/iter; left time: 3664.6286s
	iters: 200, epoch: 1 | loss: 0.2891650
	speed: 0.1355s/iter; left time: 3515.4414s
	iters: 300, epoch: 1 | loss: 0.2822945
	speed: 0.1386s/iter; left time: 3584.1280s
	iters: 400, epoch: 1 | loss: 0.3349083
	speed: 0.1411s/iter; left time: 3633.0835s
	iters: 500, epoch: 1 | loss: 0.4933135
	speed: 0.1343s/iter; left time: 3444.8661s
Epoch: 1 cost time: 72.5606575012207
Epoch: 1, Steps: 523 | Train Loss: 0.4363482 Vali Loss: 0.2043386 Test Loss: 0.2821538
Validation loss decreased (inf --> 0.204339).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3799637
	speed: 0.8538s/iter; left time: 21796.9553s
	iters: 200, epoch: 2 | loss: 0.3581458
	speed: 0.1223s/iter; left time: 3109.9132s
	iters: 300, epoch: 2 | loss: 0.3397896
	speed: 0.1205s/iter; left time: 3050.9698s
	iters: 400, epoch: 2 | loss: 0.5020598
	speed: 0.1138s/iter; left time: 2869.7509s
	iters: 500, epoch: 2 | loss: 0.3474444
	speed: 0.1201s/iter; left time: 3017.9484s
Epoch: 2 cost time: 63.884520053863525
Epoch: 2, Steps: 523 | Train Loss: 0.3937363 Vali Loss: 0.1992607 Test Loss: 0.2760101
Validation loss decreased (0.204339 --> 0.199261).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4986432
	speed: 1.0103s/iter; left time: 25263.5737s
	iters: 200, epoch: 3 | loss: 0.3663023
	speed: 0.1592s/iter; left time: 3963.7190s
	iters: 300, epoch: 3 | loss: 0.2263201
	speed: 0.1630s/iter; left time: 4044.3080s
	iters: 400, epoch: 3 | loss: 0.2699861
	speed: 0.1408s/iter; left time: 3479.3087s
	iters: 500, epoch: 3 | loss: 0.3800867
	speed: 0.1505s/iter; left time: 3702.7392s
Epoch: 3 cost time: 82.15357947349548
Epoch: 3, Steps: 523 | Train Loss: 0.3867351 Vali Loss: 0.1970987 Test Loss: 0.2730087
Validation loss decreased (0.199261 --> 0.197099).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3138037
	speed: 0.8796s/iter; left time: 21534.0782s
	iters: 200, epoch: 4 | loss: 0.2716384
	speed: 0.1206s/iter; left time: 2940.2288s
	iters: 300, epoch: 4 | loss: 0.2316304
	speed: 0.1209s/iter; left time: 2936.8422s
	iters: 400, epoch: 4 | loss: 0.3228221
	speed: 0.1324s/iter; left time: 3201.4563s
	iters: 500, epoch: 4 | loss: 0.3050091
	speed: 0.1271s/iter; left time: 3060.7835s
Epoch: 4 cost time: 65.87308168411255
Epoch: 4, Steps: 523 | Train Loss: 0.3833399 Vali Loss: 0.1961192 Test Loss: 0.2718700
Validation loss decreased (0.197099 --> 0.196119).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2855229
	speed: 0.9209s/iter; left time: 22064.3600s
	iters: 200, epoch: 5 | loss: 0.3943962
	speed: 0.1312s/iter; left time: 3129.2261s
	iters: 300, epoch: 5 | loss: 0.3003116
	speed: 0.1299s/iter; left time: 3085.5501s
	iters: 400, epoch: 5 | loss: 0.4370820
	speed: 0.1263s/iter; left time: 2987.4257s
	iters: 500, epoch: 5 | loss: 0.3291377
	speed: 0.1215s/iter; left time: 2862.4655s
Epoch: 5 cost time: 68.91405415534973
Epoch: 5, Steps: 523 | Train Loss: 0.3816677 Vali Loss: 0.1951904 Test Loss: 0.2711539
Validation loss decreased (0.196119 --> 0.195190).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4848908
	speed: 0.7641s/iter; left time: 17906.5945s
	iters: 200, epoch: 6 | loss: 0.2758611
	speed: 0.1332s/iter; left time: 3108.0924s
	iters: 300, epoch: 6 | loss: 0.5552097
	speed: 0.1296s/iter; left time: 3011.2436s
	iters: 400, epoch: 6 | loss: 0.2495818
	speed: 0.1324s/iter; left time: 3063.9063s
	iters: 500, epoch: 6 | loss: 0.3103768
	speed: 0.1293s/iter; left time: 2978.1242s
Epoch: 6 cost time: 67.36462378501892
Epoch: 6, Steps: 523 | Train Loss: 0.3803394 Vali Loss: 0.1951209 Test Loss: 0.2708584
Validation loss decreased (0.195190 --> 0.195121).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4924383
	speed: 0.9675s/iter; left time: 22168.6034s
	iters: 200, epoch: 7 | loss: 0.3746841
	speed: 0.1362s/iter; left time: 3106.8928s
	iters: 300, epoch: 7 | loss: 0.3654137
	speed: 0.1478s/iter; left time: 3356.7154s
	iters: 400, epoch: 7 | loss: 0.5570145
	speed: 0.1306s/iter; left time: 2953.0310s
	iters: 500, epoch: 7 | loss: 0.2073741
	speed: 0.1387s/iter; left time: 3122.7387s
Epoch: 7 cost time: 72.37828874588013
Epoch: 7, Steps: 523 | Train Loss: 0.3794990 Vali Loss: 0.1947229 Test Loss: 0.2703852
Validation loss decreased (0.195121 --> 0.194723).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3018616
	speed: 0.9300s/iter; left time: 20821.9899s
	iters: 200, epoch: 8 | loss: 0.2919804
	speed: 0.1408s/iter; left time: 3138.4886s
	iters: 300, epoch: 8 | loss: 0.3067209
	speed: 0.1361s/iter; left time: 3019.5881s
	iters: 400, epoch: 8 | loss: 0.6248519
	speed: 0.1260s/iter; left time: 2783.0158s
	iters: 500, epoch: 8 | loss: 0.5148283
	speed: 0.1351s/iter; left time: 2969.8334s
Epoch: 8 cost time: 71.00711464881897
Epoch: 8, Steps: 523 | Train Loss: 0.3789334 Vali Loss: 0.1947689 Test Loss: 0.2705230
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2140845
	speed: 0.9416s/iter; left time: 20589.9150s
	iters: 200, epoch: 9 | loss: 0.2614395
	speed: 0.1365s/iter; left time: 2971.2628s
	iters: 300, epoch: 9 | loss: 0.3009703
	speed: 0.1208s/iter; left time: 2617.3128s
	iters: 400, epoch: 9 | loss: 0.3780475
	speed: 0.1280s/iter; left time: 2761.0328s
	iters: 500, epoch: 9 | loss: 0.5140824
	speed: 0.1195s/iter; left time: 2566.1284s
Epoch: 9 cost time: 68.57976078987122
Epoch: 9, Steps: 523 | Train Loss: 0.3783338 Vali Loss: 0.1946586 Test Loss: 0.2703117
Validation loss decreased (0.194723 --> 0.194659).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2552715
	speed: 0.8614s/iter; left time: 18385.8702s
	iters: 200, epoch: 10 | loss: 0.3564005
	speed: 0.1287s/iter; left time: 2735.0249s
	iters: 300, epoch: 10 | loss: 0.5688321
	speed: 0.1283s/iter; left time: 2712.8846s
	iters: 400, epoch: 10 | loss: 0.2585027
	speed: 0.1290s/iter; left time: 2713.9849s
	iters: 500, epoch: 10 | loss: 0.4813462
	speed: 0.1449s/iter; left time: 3034.9111s
Epoch: 10 cost time: 69.70528841018677
Epoch: 10, Steps: 523 | Train Loss: 0.3776219 Vali Loss: 0.1946061 Test Loss: 0.2700240
Validation loss decreased (0.194659 --> 0.194606).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2758785
	speed: 0.9407s/iter; left time: 19586.8286s
	iters: 200, epoch: 11 | loss: 0.3997616
	speed: 0.1370s/iter; left time: 2837.9440s
	iters: 300, epoch: 11 | loss: 0.3263932
	speed: 0.1238s/iter; left time: 2553.4208s
	iters: 400, epoch: 11 | loss: 0.1856098
	speed: 0.1173s/iter; left time: 2408.0416s
	iters: 500, epoch: 11 | loss: 0.6103514
	speed: 0.0989s/iter; left time: 2020.4198s
Epoch: 11 cost time: 64.45926690101624
Epoch: 11, Steps: 523 | Train Loss: 0.3775274 Vali Loss: 0.1943455 Test Loss: 0.2698181
Validation loss decreased (0.194606 --> 0.194345).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5479196
	speed: 0.8052s/iter; left time: 16343.8331s
	iters: 200, epoch: 12 | loss: 0.5616088
	speed: 0.1329s/iter; left time: 2683.7170s
	iters: 300, epoch: 12 | loss: 0.3695532
	speed: 0.1326s/iter; left time: 2665.6748s
	iters: 400, epoch: 12 | loss: 0.6488525
	speed: 0.1270s/iter; left time: 2540.3786s
	iters: 500, epoch: 12 | loss: 0.4371644
	speed: 0.1423s/iter; left time: 2830.6396s
Epoch: 12 cost time: 70.74246382713318
Epoch: 12, Steps: 523 | Train Loss: 0.3772449 Vali Loss: 0.1944416 Test Loss: 0.2696936
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4000214
	speed: 0.9050s/iter; left time: 17896.4372s
	iters: 200, epoch: 13 | loss: 0.2557464
	speed: 0.1242s/iter; left time: 2442.6842s
	iters: 300, epoch: 13 | loss: 0.4148818
	speed: 0.1096s/iter; left time: 2145.8755s
	iters: 400, epoch: 13 | loss: 0.2929695
	speed: 0.1081s/iter; left time: 2105.9689s
	iters: 500, epoch: 13 | loss: 0.4281496
	speed: 0.1067s/iter; left time: 2066.6182s
Epoch: 13 cost time: 60.953059673309326
Epoch: 13, Steps: 523 | Train Loss: 0.3772503 Vali Loss: 0.1940632 Test Loss: 0.2693302
Validation loss decreased (0.194345 --> 0.194063).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4711538
	speed: 0.7799s/iter; left time: 15015.2029s
	iters: 200, epoch: 14 | loss: 0.2990616
	speed: 0.1222s/iter; left time: 2340.9847s
	iters: 300, epoch: 14 | loss: 0.3663002
	speed: 0.1170s/iter; left time: 2229.6398s
	iters: 400, epoch: 14 | loss: 0.3378209
	speed: 0.1226s/iter; left time: 2324.0824s
	iters: 500, epoch: 14 | loss: 0.3837584
	speed: 0.1276s/iter; left time: 2406.4354s
Epoch: 14 cost time: 64.1685779094696
Epoch: 14, Steps: 523 | Train Loss: 0.3769650 Vali Loss: 0.1941578 Test Loss: 0.2695390
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4824257
	speed: 0.8797s/iter; left time: 16475.5264s
	iters: 200, epoch: 15 | loss: 0.5762073
	speed: 0.1371s/iter; left time: 2553.3532s
	iters: 300, epoch: 15 | loss: 0.3074389
	speed: 0.1411s/iter; left time: 2614.5020s
	iters: 400, epoch: 15 | loss: 0.2350316
	speed: 0.1345s/iter; left time: 2478.5563s
	iters: 500, epoch: 15 | loss: 0.2544543
	speed: 0.1223s/iter; left time: 2240.7817s
Epoch: 15 cost time: 71.18953466415405
Epoch: 15, Steps: 523 | Train Loss: 0.3769827 Vali Loss: 0.1941061 Test Loss: 0.2696322
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3397883
	speed: 0.7622s/iter; left time: 13875.7648s
	iters: 200, epoch: 16 | loss: 0.2716659
	speed: 0.1248s/iter; left time: 2259.6933s
	iters: 300, epoch: 16 | loss: 0.3699282
	speed: 0.1175s/iter; left time: 2115.1435s
	iters: 400, epoch: 16 | loss: 0.3995824
	speed: 0.1146s/iter; left time: 2051.5274s
	iters: 500, epoch: 16 | loss: 0.3749316
	speed: 0.1217s/iter; left time: 2166.5806s
Epoch: 16 cost time: 63.88456320762634
Epoch: 16, Steps: 523 | Train Loss: 0.3767235 Vali Loss: 0.1940680 Test Loss: 0.2693962
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.269525408744812, mae:0.3270148038864136, rse:0.41933441162109375, corr:[0.54773885 0.55430174 0.5552692  0.55339164 0.55138093 0.55040497
 0.5505831  0.55145806 0.5523811  0.552825   0.55263275 0.5520248
 0.5513709  0.550921   0.55082494 0.5509582  0.55112123 0.5510856
 0.55068845 0.5499462  0.54904145 0.5481482  0.5474243  0.5469544
 0.54666764 0.5464364  0.5461202  0.5456277  0.54495376 0.5441893
 0.54342264 0.5427938  0.5423149  0.5419157  0.5415333  0.5411359
 0.5406516  0.54008627 0.5394539  0.538803   0.5381913  0.5376371
 0.5371679  0.5367672  0.5363968  0.5359725  0.5354423  0.5347823
 0.5339508  0.5329637  0.53189915 0.5308926  0.5300142  0.5293158
 0.52880704 0.52849156 0.5282751  0.5280681  0.5278271  0.5275448
 0.52723855 0.5269879  0.52682215 0.5267098  0.526607   0.52648526
 0.5262766  0.5259912  0.5256553  0.5252929  0.5249262  0.52459306
 0.5242982  0.5240365  0.5237403  0.5233838  0.5229334  0.5223791
 0.52173036 0.5210619  0.52037346 0.5197377  0.51918125 0.5187133
 0.5183234  0.517976   0.51759523 0.51716363 0.5166609  0.5160815
 0.5154397  0.5147308  0.51389045 0.5128979  0.5117563  0.5104617
 0.50909454 0.5078002  0.5066043  0.50551724 0.5045344  0.5036276
 0.50275695 0.5018438  0.50087214 0.4998126  0.49872485 0.49768353
 0.49670961 0.4957986  0.494948   0.4941276  0.49335834 0.4925731
 0.49169776 0.49074543 0.48977253 0.48878562 0.48785686 0.48696044
 0.48615336 0.48539236 0.48469523 0.4840108  0.4832763  0.4824699
 0.48164183 0.48082492 0.48003966 0.47931555 0.4786593  0.47803566
 0.47743234 0.47683722 0.47625628 0.47567523 0.47507372 0.47448024
 0.4738773  0.47328693 0.47266838 0.47200075 0.47127402 0.47041678
 0.46944216 0.46835423 0.46734843 0.46652198 0.46594056 0.46552858
 0.46520647 0.46487948 0.46438345 0.46365392 0.46274436 0.4618319
 0.46106    0.46053386 0.46025237 0.46020707 0.46024784 0.4602365
 0.4600626  0.45971414 0.45923814 0.4587406  0.45826823 0.4578693
 0.45759243 0.4574509  0.45729285 0.4570241  0.45658922 0.45599535
 0.45532206 0.45467943 0.4541491  0.4537592  0.4534256  0.4531309
 0.45273215 0.4521351  0.4513853  0.45053622 0.4496508  0.44882324
 0.4481618  0.44771308 0.4473969  0.44704002 0.4464723  0.44557974
 0.44438145 0.44305548 0.44162259 0.44011226 0.43864504 0.43725505
 0.43599436 0.4348277  0.43371174 0.432572   0.43138158 0.4301603
 0.42897817 0.42788452 0.4268494  0.4258567  0.42494568 0.42413703
 0.42324632 0.42218783 0.42098662 0.4197558  0.4186806  0.41778728
 0.41705492 0.4163635  0.41566017 0.414853   0.41384065 0.412656
 0.4113888  0.4101809  0.40919104 0.40835288 0.40761924 0.406948
 0.40617    0.40530506 0.40430817 0.40327916 0.40227923 0.40142047
 0.40074098 0.40032318 0.40011534 0.40006542 0.40008754 0.4000457
 0.39987627 0.39947498 0.39892367 0.39844534 0.39805686 0.39784738
 0.39781928 0.39791614 0.39796954 0.39785215 0.39763895 0.39736888
 0.39709833 0.39693972 0.39680758 0.39667383 0.39643213 0.39605564
 0.39554623 0.394945   0.39435726 0.39393467 0.39376184 0.39390162
 0.39425007 0.39459136 0.39479274 0.39471897 0.39428276 0.39352623
 0.3926419  0.39193815 0.3915505  0.39154968 0.39178106 0.39200866
 0.39209023 0.3917666  0.39097145 0.3898876  0.388734   0.38779947
 0.38721657 0.38699603 0.38699237 0.38696527 0.38670716 0.38601184
 0.38496876 0.38393423 0.38308108 0.38247356 0.38210762 0.3818211
 0.38155967 0.38116828 0.3805323  0.3796691  0.37881628 0.37802947
 0.37756598 0.37740496 0.37749884 0.37752238 0.37734294 0.37692404
 0.37639537 0.37577888 0.375297   0.37512755 0.37527412 0.37577575
 0.37625435 0.37651938 0.37635642 0.37571105 0.3748212  0.37410653
 0.3738363  0.37420538 0.37508452 0.37607074 0.37665483 0.3766274
 0.37598845 0.37512988 0.37466225 0.37488213 0.37584895 0.3770185
 0.37791705 0.37778506 0.37613922 0.37337643 0.3709949  0.37031788]
