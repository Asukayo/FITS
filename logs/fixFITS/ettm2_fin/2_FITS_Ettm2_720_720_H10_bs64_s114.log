Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14515200.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3908640
	speed: 0.1410s/iter; left time: 1804.6559s
	iters: 200, epoch: 1 | loss: 0.4259118
	speed: 0.1209s/iter; left time: 1535.5405s
Epoch: 1 cost time: 33.489346742630005
Epoch: 1, Steps: 258 | Train Loss: 0.4300936 Vali Loss: 0.2975508 Test Loss: 0.3964931
Validation loss decreased (inf --> 0.297551).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3476790
	speed: 0.5546s/iter; left time: 6956.1435s
	iters: 200, epoch: 2 | loss: 0.4167973
	speed: 0.1273s/iter; left time: 1584.2640s
Epoch: 2 cost time: 33.391064405441284
Epoch: 2, Steps: 258 | Train Loss: 0.3167438 Vali Loss: 0.2806151 Test Loss: 0.3769953
Validation loss decreased (0.297551 --> 0.280615).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3785537
	speed: 0.5520s/iter; left time: 6781.8059s
	iters: 200, epoch: 3 | loss: 0.2444618
	speed: 0.1104s/iter; left time: 1345.2424s
Epoch: 3 cost time: 30.810984134674072
Epoch: 3, Steps: 258 | Train Loss: 0.2870529 Vali Loss: 0.2741272 Test Loss: 0.3690991
Validation loss decreased (0.280615 --> 0.274127).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3295376
	speed: 0.6054s/iter; left time: 7280.6875s
	iters: 200, epoch: 4 | loss: 0.1960724
	speed: 0.1385s/iter; left time: 1652.2058s
Epoch: 4 cost time: 37.4716420173645
Epoch: 4, Steps: 258 | Train Loss: 0.2734500 Vali Loss: 0.2706562 Test Loss: 0.3641788
Validation loss decreased (0.274127 --> 0.270656).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2667653
	speed: 0.6096s/iter; left time: 7174.2947s
	iters: 200, epoch: 5 | loss: 0.2892694
	speed: 0.1347s/iter; left time: 1571.8836s
Epoch: 5 cost time: 35.06777238845825
Epoch: 5, Steps: 258 | Train Loss: 0.2659258 Vali Loss: 0.2681783 Test Loss: 0.3608945
Validation loss decreased (0.270656 --> 0.268178).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2375931
	speed: 0.6013s/iter; left time: 6921.8991s
	iters: 200, epoch: 6 | loss: 0.1769053
	speed: 0.1381s/iter; left time: 1575.8981s
Epoch: 6 cost time: 35.925872802734375
Epoch: 6, Steps: 258 | Train Loss: 0.2623042 Vali Loss: 0.2663728 Test Loss: 0.3589295
Validation loss decreased (0.268178 --> 0.266373).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2944026
	speed: 0.5674s/iter; left time: 6385.1081s
	iters: 200, epoch: 7 | loss: 0.2204926
	speed: 0.1106s/iter; left time: 1233.1140s
Epoch: 7 cost time: 30.92848563194275
Epoch: 7, Steps: 258 | Train Loss: 0.2604234 Vali Loss: 0.2653767 Test Loss: 0.3574087
Validation loss decreased (0.266373 --> 0.265377).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2297919
	speed: 0.5454s/iter; left time: 5996.4131s
	iters: 200, epoch: 8 | loss: 0.2678335
	speed: 0.1249s/iter; left time: 1360.8930s
Epoch: 8 cost time: 35.870556116104126
Epoch: 8, Steps: 258 | Train Loss: 0.2591330 Vali Loss: 0.2651080 Test Loss: 0.3565494
Validation loss decreased (0.265377 --> 0.265108).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2889558
	speed: 0.6608s/iter; left time: 7094.6618s
	iters: 200, epoch: 9 | loss: 0.2314289
	speed: 0.1341s/iter; left time: 1426.5670s
Epoch: 9 cost time: 35.51732063293457
Epoch: 9, Steps: 258 | Train Loss: 0.2584607 Vali Loss: 0.2645524 Test Loss: 0.3557974
Validation loss decreased (0.265108 --> 0.264552).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3162968
	speed: 0.5628s/iter; left time: 5897.9951s
	iters: 200, epoch: 10 | loss: 0.2156158
	speed: 0.1320s/iter; left time: 1370.0312s
Epoch: 10 cost time: 35.341118812561035
Epoch: 10, Steps: 258 | Train Loss: 0.2582884 Vali Loss: 0.2644215 Test Loss: 0.3554032
Validation loss decreased (0.264552 --> 0.264422).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2554174
	speed: 0.5892s/iter; left time: 6022.4724s
	iters: 200, epoch: 11 | loss: 0.3480309
	speed: 0.1397s/iter; left time: 1414.4036s
Epoch: 11 cost time: 35.728450298309326
Epoch: 11, Steps: 258 | Train Loss: 0.2579799 Vali Loss: 0.2638462 Test Loss: 0.3553220
Validation loss decreased (0.264422 --> 0.263846).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2840939
	speed: 0.5586s/iter; left time: 5565.2594s
	iters: 200, epoch: 12 | loss: 0.2710284
	speed: 0.1293s/iter; left time: 1275.7126s
Epoch: 12 cost time: 34.13307595252991
Epoch: 12, Steps: 258 | Train Loss: 0.2575489 Vali Loss: 0.2636906 Test Loss: 0.3549408
Validation loss decreased (0.263846 --> 0.263691).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2200260
	speed: 0.5319s/iter; left time: 5162.3330s
	iters: 200, epoch: 13 | loss: 0.2311122
	speed: 0.1294s/iter; left time: 1242.7607s
Epoch: 13 cost time: 34.3332462310791
Epoch: 13, Steps: 258 | Train Loss: 0.2578008 Vali Loss: 0.2635354 Test Loss: 0.3549698
Validation loss decreased (0.263691 --> 0.263535).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3543167
	speed: 0.5613s/iter; left time: 5302.1593s
	iters: 200, epoch: 14 | loss: 0.1973803
	speed: 0.1251s/iter; left time: 1168.8888s
Epoch: 14 cost time: 33.76769471168518
Epoch: 14, Steps: 258 | Train Loss: 0.2574735 Vali Loss: 0.2639680 Test Loss: 0.3548070
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1810275
	speed: 0.5822s/iter; left time: 5350.2394s
	iters: 200, epoch: 15 | loss: 0.2616073
	speed: 0.1358s/iter; left time: 1234.5554s
Epoch: 15 cost time: 35.73079872131348
Epoch: 15, Steps: 258 | Train Loss: 0.2571458 Vali Loss: 0.2634616 Test Loss: 0.3547497
Validation loss decreased (0.263535 --> 0.263462).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2855759
	speed: 0.6065s/iter; left time: 5416.2893s
	iters: 200, epoch: 16 | loss: 0.2393759
	speed: 0.1730s/iter; left time: 1527.3306s
Epoch: 16 cost time: 40.150654792785645
Epoch: 16, Steps: 258 | Train Loss: 0.2571060 Vali Loss: 0.2637630 Test Loss: 0.3545107
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3189641
	speed: 0.7268s/iter; left time: 6303.7227s
	iters: 200, epoch: 17 | loss: 0.3121479
	speed: 0.1501s/iter; left time: 1286.4989s
Epoch: 17 cost time: 40.792826890945435
Epoch: 17, Steps: 258 | Train Loss: 0.2573404 Vali Loss: 0.2636309 Test Loss: 0.3546965
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2576507
	speed: 0.6474s/iter; left time: 5448.1319s
	iters: 200, epoch: 18 | loss: 0.2732523
	speed: 0.1406s/iter; left time: 1169.4068s
Epoch: 18 cost time: 35.80275821685791
Epoch: 18, Steps: 258 | Train Loss: 0.2572420 Vali Loss: 0.2637156 Test Loss: 0.3546321
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14515200.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4133732
	speed: 0.1861s/iter; left time: 2382.7380s
	iters: 200, epoch: 1 | loss: 0.3962764
	speed: 0.1729s/iter; left time: 2196.6242s
Epoch: 1 cost time: 46.18365526199341
Epoch: 1, Steps: 258 | Train Loss: 0.4986201 Vali Loss: 0.2625172 Test Loss: 0.3537554
Validation loss decreased (inf --> 0.262517).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4807010
	speed: 0.8014s/iter; left time: 10052.5510s
	iters: 200, epoch: 2 | loss: 0.3807984
	speed: 0.1824s/iter; left time: 2270.1922s
Epoch: 2 cost time: 49.57007312774658
Epoch: 2, Steps: 258 | Train Loss: 0.4959267 Vali Loss: 0.2621979 Test Loss: 0.3531971
Validation loss decreased (0.262517 --> 0.262198).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5105349
	speed: 0.8373s/iter; left time: 10286.6073s
	iters: 200, epoch: 3 | loss: 0.7365716
	speed: 0.1803s/iter; left time: 2196.3984s
Epoch: 3 cost time: 49.1661319732666
Epoch: 3, Steps: 258 | Train Loss: 0.4962777 Vali Loss: 0.2613916 Test Loss: 0.3532054
Validation loss decreased (0.262198 --> 0.261392).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4231858
	speed: 0.8601s/iter; left time: 10344.8990s
	iters: 200, epoch: 4 | loss: 0.4746290
	speed: 0.1976s/iter; left time: 2356.5672s
Epoch: 4 cost time: 53.58164930343628
Epoch: 4, Steps: 258 | Train Loss: 0.4954427 Vali Loss: 0.2616130 Test Loss: 0.3528459
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4176270
	speed: 0.9397s/iter; left time: 11058.7919s
	iters: 200, epoch: 5 | loss: 0.5828448
	speed: 0.1977s/iter; left time: 2306.4774s
Epoch: 5 cost time: 55.005687952041626
Epoch: 5, Steps: 258 | Train Loss: 0.4956682 Vali Loss: 0.2611464 Test Loss: 0.3528972
Validation loss decreased (0.261392 --> 0.261146).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5347789
	speed: 0.9209s/iter; left time: 10600.1386s
	iters: 200, epoch: 6 | loss: 0.4153432
	speed: 0.1694s/iter; left time: 1932.8633s
Epoch: 6 cost time: 47.30739068984985
Epoch: 6, Steps: 258 | Train Loss: 0.4954094 Vali Loss: 0.2608015 Test Loss: 0.3526676
Validation loss decreased (0.261146 --> 0.260801).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5857368
	speed: 0.7426s/iter; left time: 8356.0235s
	iters: 200, epoch: 7 | loss: 0.5865644
	speed: 0.1602s/iter; left time: 1787.1056s
Epoch: 7 cost time: 45.508164405822754
Epoch: 7, Steps: 258 | Train Loss: 0.4950996 Vali Loss: 0.2609568 Test Loss: 0.3525469
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.7253919
	speed: 0.8372s/iter; left time: 9205.3668s
	iters: 200, epoch: 8 | loss: 0.4304614
	speed: 0.1713s/iter; left time: 1866.3028s
Epoch: 8 cost time: 46.62282395362854
Epoch: 8, Steps: 258 | Train Loss: 0.4953150 Vali Loss: 0.2608803 Test Loss: 0.3524387
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4215173
	speed: 0.7762s/iter; left time: 8334.2934s
	iters: 200, epoch: 9 | loss: 0.4341187
	speed: 0.1709s/iter; left time: 1818.3091s
Epoch: 9 cost time: 46.58078598976135
Epoch: 9, Steps: 258 | Train Loss: 0.4947949 Vali Loss: 0.2611439 Test Loss: 0.3524719
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3492577075958252, mae:0.37812739610671997, rse:0.47502601146698, corr:[0.5395325  0.5444644  0.5445904  0.54191726 0.53958386 0.5387754
 0.5392465  0.540169   0.54064447 0.54026437 0.539307   0.53834957
 0.53781366 0.53774935 0.53791994 0.53790796 0.5374686  0.53664094
 0.5356595  0.53478885 0.5342166  0.53390414 0.5336997  0.53342664
 0.5329639  0.5323554  0.5317268  0.53120655 0.5308367  0.5305678
 0.53025687 0.52980834 0.5291641  0.5283444  0.5274792  0.52670264
 0.52606636 0.5255778  0.5251436  0.52468175 0.5241461  0.52352566
 0.52288574 0.5222684  0.52170354 0.5211502  0.52054495 0.51982874
 0.51896393 0.5179703  0.51696104 0.5160627  0.51533043 0.5148051
 0.51445425 0.51416767 0.51383626 0.5134319  0.51299417 0.512557
 0.51220775 0.5119747  0.51183647 0.511712   0.51152354 0.5112792
 0.51093084 0.51057255 0.5102464  0.50995636 0.5096637  0.5093265
 0.5089507  0.5085416  0.5080976  0.5076224  0.5071237  0.5065824
 0.5060227  0.5053976  0.5046619  0.50385916 0.50306726 0.5023499
 0.50176656 0.5013221  0.50095516 0.5006452  0.5003129  0.4999331
 0.49949378 0.49894202 0.49823114 0.49732295 0.49617422 0.49480766
 0.49332735 0.4918964  0.4905816  0.48942718 0.48842838 0.48751777
 0.48657855 0.4855035  0.48423377 0.4828158  0.4813801  0.48008722
 0.47900885 0.47814968 0.47742975 0.4767084  0.47592863 0.47502077
 0.47401896 0.4730446  0.47221026 0.4715225  0.47092614 0.470277
 0.46950954 0.46856335 0.46752292 0.46647662 0.46551654 0.46468008
 0.46396482 0.4632788  0.4625137  0.46160036 0.4605721  0.45950526
 0.45852953 0.45768496 0.45695826 0.45624912 0.45546177 0.45462015
 0.45380613 0.45314994 0.4527182  0.45248842 0.45232928 0.45202214
 0.45145056 0.4505865  0.44962317 0.44870722 0.4479687  0.44739658
 0.4469088  0.44635752 0.4455687  0.44454172 0.44343042 0.44246405
 0.44181272 0.44149265 0.441369   0.4413083  0.4411142  0.44073588
 0.44021124 0.43968207 0.43924457 0.43888277 0.4384782  0.437924
 0.4371996  0.43641868 0.43571758 0.43532833 0.43530974 0.43557078
 0.435859   0.43592155 0.43555546 0.43470722 0.43350223 0.43226746
 0.43128607 0.43072593 0.43056923 0.43059206 0.430531   0.43018818
 0.42954952 0.42864367 0.427602   0.42648402 0.4252783  0.42387432
 0.42220214 0.4203805  0.41840097 0.4164317  0.41474724 0.41349253
 0.4126903  0.41217288 0.41170132 0.411074   0.4102116  0.40918508
 0.4081557  0.4072584  0.40650877 0.4058282  0.40513998 0.4043659
 0.40332925 0.40201572 0.40057623 0.39925173 0.39825553 0.39757365
 0.39706928 0.39649004 0.39570343 0.39458793 0.39311543 0.3913897
 0.38959232 0.38793072 0.3865924  0.3855187  0.38464847 0.3839328
 0.38321072 0.38243002 0.38156074 0.38064975 0.37975788 0.37899294
 0.37841168 0.37809405 0.37801853 0.37807667 0.3781557  0.3781209
 0.3779146  0.3774856  0.3768917  0.37633017 0.37578884 0.3753098
 0.37486202 0.3744266  0.37398994 0.3735787  0.37331462 0.37321696
 0.37322906 0.37329212 0.37322044 0.37294775 0.3724713  0.37194082
 0.37148058 0.3711658  0.3709475  0.37073955 0.3703991  0.3699295
 0.36938393 0.36887148 0.3685983  0.36867443 0.36901915 0.36941624
 0.36970568 0.3697296  0.3693987  0.36881748 0.36811224 0.36740887
 0.3668116  0.36620295 0.36544803 0.36456698 0.36358187 0.36270422
 0.36212993 0.3620178  0.36233255 0.3628065  0.3630945  0.3628355
 0.36198536 0.36076668 0.35940966 0.35815033 0.35718796 0.35652548
 0.35612822 0.3557805  0.35528484 0.35458004 0.35378212 0.35298464
 0.35234922 0.35180902 0.35133165 0.3507218  0.34997287 0.3491523
 0.3484785  0.34806538 0.34800768 0.34826204 0.3486538  0.34904304
 0.3491627  0.3489594  0.34841827 0.34764794 0.34687594 0.34628978
 0.3458948  0.34567446 0.34556288 0.34546325 0.3453044  0.3451156
 0.3449299  0.34483117 0.3448206  0.34478542 0.34472632 0.34458753
 0.34443808 0.3442361  0.34398144 0.34369648 0.34332278 0.34294912
 0.34262168 0.34235564 0.342293   0.34252056 0.34293926 0.34331104
 0.34344265 0.3432134  0.34264228 0.3418683  0.34113637 0.3407108
 0.340676   0.34098095 0.3414028  0.34169322 0.34172755 0.34149906
 0.34112778 0.340786   0.3404993  0.34030935 0.34002045 0.33958516
 0.33902743 0.33849055 0.33817637 0.3382306  0.3386202  0.33922234
 0.3397014  0.33983913 0.3394842  0.33874395 0.33792976 0.33734274
 0.33721173 0.33755916 0.33815128 0.3386499  0.33879006 0.33856085
 0.33815876 0.33785513 0.33783925 0.33808506 0.338513   0.33877513
 0.3386672  0.33804905 0.33696896 0.33561394 0.33425942 0.33327332
 0.33272696 0.33252737 0.3324491  0.33227417 0.33188668 0.3312608
 0.3304949  0.32975426 0.32903337 0.3283229  0.32765377 0.3269371
 0.32620266 0.32552573 0.32506496 0.32493472 0.3250214  0.3252173
 0.32530123 0.32515377 0.32469946 0.3239748  0.3232024  0.3225524
 0.32221308 0.32221034 0.3223893  0.32260504 0.3227151  0.3227041
 0.32265064 0.32261798 0.32260188 0.3225948  0.3225738  0.3225182
 0.32246822 0.32244727 0.32254335 0.32277077 0.32310534 0.32345816
 0.32371765 0.32374266 0.32354838 0.32323158 0.32294863 0.32276493
 0.32262745 0.32255107 0.32237098 0.32194054 0.32128224 0.32056898
 0.31988612 0.31940004 0.31913662 0.31900257 0.3189213  0.31876528
 0.31851846 0.3182051  0.3179242  0.3176791  0.3175077  0.31741133
 0.3172943  0.31706044 0.31664428 0.31608292 0.31556118 0.31515238
 0.31498533 0.31507704 0.31534955 0.31564704 0.3157968  0.3156899
 0.3153279  0.3147705  0.31410992 0.31345508 0.31294888 0.31257942
 0.31230012 0.3119356  0.31140858 0.31058398 0.3094523  0.3080425
 0.3064624  0.30496952 0.30367592 0.30267316 0.30198222 0.30151653
 0.3011122  0.30067885 0.3000951  0.29938495 0.29852253 0.29752833
 0.2964726  0.29551    0.29468712 0.29392916 0.29320782 0.29247448
 0.29172727 0.29089695 0.2900082  0.28920606 0.28852263 0.2880639
 0.2877173  0.2873934  0.28705814 0.286665   0.28627178 0.2858492
 0.2853953  0.284897   0.28433397 0.28373396 0.2831711  0.28269058
 0.28228256 0.28188303 0.28159013 0.28131163 0.2809796  0.28061032
 0.2802525  0.2800157  0.27992636 0.27998075 0.2801288  0.28022486
 0.28011033 0.27976483 0.27923992 0.27869013 0.2783045  0.27815944
 0.2782142  0.27838537 0.27864087 0.27884853 0.27894574 0.27891925
 0.27878344 0.27858165 0.27828184 0.2779033  0.2774116  0.27680358
 0.27616712 0.27566996 0.27539384 0.275323   0.27530596 0.27525014
 0.27501473 0.27461955 0.27416635 0.27384272 0.27367494 0.27380005
 0.27419123 0.27478367 0.27533823 0.27562863 0.27555165 0.27512863
 0.27450678 0.27385843 0.2733779  0.27314255 0.2731384  0.2732599
 0.27333117 0.27317166 0.2726093  0.27164567 0.27038845 0.2689219
 0.2674286  0.2662335  0.26540655 0.26486322 0.26436737 0.2637291
 0.26285246 0.26175594 0.26055002 0.2593652  0.25831333 0.25750196
 0.2569037  0.25639784 0.25593764 0.2554065  0.25474808 0.25401857
 0.2532806  0.25258765 0.25208572 0.25175807 0.2516358  0.251651
 0.25166476 0.25160038 0.25133345 0.25083736 0.25027496 0.24978362
 0.24937762 0.2490941  0.2489661  0.24885213 0.24873231 0.24857436
 0.2484489  0.24822845 0.24797866 0.24780963 0.24774396 0.2477869
 0.24799404 0.2484755  0.24920917 0.25012895 0.25107968 0.25185564
 0.2522089  0.252016   0.2513726  0.2506309  0.25009808 0.24995072
 0.25014406 0.25054786 0.25099823 0.25118205 0.25124764 0.25126132
 0.25135878 0.25161675 0.251888   0.2521001  0.2519244  0.25131533
 0.25035733 0.24945654 0.24885194 0.24861923 0.24886656 0.24920736
 0.24939013 0.24915104 0.24848095 0.24756673 0.24680951 0.2463799
 0.24656032 0.24722005 0.24801435 0.248734   0.24918076 0.24924356
 0.24897817 0.24870025 0.24850424 0.24848223 0.24867736 0.24891841
 0.24899973 0.24887498 0.24851605 0.24788652 0.24705099 0.2461303
 0.24518538 0.24424092 0.2432877  0.24230991 0.24141462 0.24062027
 0.23993358 0.23931508 0.23889126 0.23844482 0.23788469 0.2371033
 0.23614816 0.23516864 0.23431107 0.23363131 0.23319183 0.23278499
 0.23231265 0.23168485 0.23087217 0.23016708 0.22961971 0.22922839
 0.22897516 0.22872655 0.2284865  0.22812417 0.22769786 0.22723567
 0.22686474 0.22635418 0.22568923 0.22492751 0.22419052 0.2236698
 0.22338316 0.22332507 0.22324596 0.22301961 0.2223005  0.22131456
 0.22051416 0.22044279 0.22119366 0.22193523 0.2207979  0.21534427]
