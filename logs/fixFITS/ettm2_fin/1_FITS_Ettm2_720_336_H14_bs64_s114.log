Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19457536.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5128622
	speed: 0.0731s/iter; left time: 946.4162s
	iters: 200, epoch: 1 | loss: 0.4390127
	speed: 0.0821s/iter; left time: 1054.8030s
Epoch: 1 cost time: 22.29608464241028
Epoch: 1, Steps: 261 | Train Loss: 0.4602177 Vali Loss: 0.2103010 Test Loss: 0.2890623
Validation loss decreased (inf --> 0.210301).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4245509
	speed: 0.5323s/iter; left time: 6754.3302s
	iters: 200, epoch: 2 | loss: 0.4684555
	speed: 0.1165s/iter; left time: 1466.8146s
Epoch: 2 cost time: 31.862235069274902
Epoch: 2, Steps: 261 | Train Loss: 0.4026469 Vali Loss: 0.2019339 Test Loss: 0.2788179
Validation loss decreased (0.210301 --> 0.201934).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4102253
	speed: 0.5413s/iter; left time: 6728.3912s
	iters: 200, epoch: 3 | loss: 0.2954649
	speed: 0.1239s/iter; left time: 1527.6311s
Epoch: 3 cost time: 32.00435256958008
Epoch: 3, Steps: 261 | Train Loss: 0.3918597 Vali Loss: 0.1986489 Test Loss: 0.2750032
Validation loss decreased (0.201934 --> 0.198649).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4936701
	speed: 0.5578s/iter; left time: 6787.2889s
	iters: 200, epoch: 4 | loss: 0.3284155
	speed: 0.1156s/iter; left time: 1394.8907s
Epoch: 4 cost time: 30.97044014930725
Epoch: 4, Steps: 261 | Train Loss: 0.3866332 Vali Loss: 0.1964975 Test Loss: 0.2728049
Validation loss decreased (0.198649 --> 0.196497).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3812787
	speed: 0.4684s/iter; left time: 5577.4899s
	iters: 200, epoch: 5 | loss: 0.3373215
	speed: 0.1434s/iter; left time: 1692.6750s
Epoch: 5 cost time: 35.21386170387268
Epoch: 5, Steps: 261 | Train Loss: 0.3829253 Vali Loss: 0.1956069 Test Loss: 0.2715401
Validation loss decreased (0.196497 --> 0.195607).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3733286
	speed: 0.6201s/iter; left time: 7221.5380s
	iters: 200, epoch: 6 | loss: 0.3458129
	speed: 0.1398s/iter; left time: 1613.7826s
Epoch: 6 cost time: 37.229377031326294
Epoch: 6, Steps: 261 | Train Loss: 0.3819515 Vali Loss: 0.1949159 Test Loss: 0.2706824
Validation loss decreased (0.195607 --> 0.194916).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4550209
	speed: 0.6085s/iter; left time: 6927.8489s
	iters: 200, epoch: 7 | loss: 0.4714533
	speed: 0.1409s/iter; left time: 1590.3290s
Epoch: 7 cost time: 36.43060302734375
Epoch: 7, Steps: 261 | Train Loss: 0.3799390 Vali Loss: 0.1942233 Test Loss: 0.2696479
Validation loss decreased (0.194916 --> 0.194223).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3291706
	speed: 0.5958s/iter; left time: 6628.1071s
	iters: 200, epoch: 8 | loss: 0.3757976
	speed: 0.1082s/iter; left time: 1192.3677s
Epoch: 8 cost time: 30.251324892044067
Epoch: 8, Steps: 261 | Train Loss: 0.3793322 Vali Loss: 0.1937510 Test Loss: 0.2692032
Validation loss decreased (0.194223 --> 0.193751).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3446513
	speed: 0.5816s/iter; left time: 6317.8352s
	iters: 200, epoch: 9 | loss: 0.2931778
	speed: 0.1317s/iter; left time: 1417.8450s
Epoch: 9 cost time: 34.37982940673828
Epoch: 9, Steps: 261 | Train Loss: 0.3781748 Vali Loss: 0.1937901 Test Loss: 0.2690680
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5070029
	speed: 0.6583s/iter; left time: 6979.3420s
	iters: 200, epoch: 10 | loss: 0.3982140
	speed: 0.1362s/iter; left time: 1430.5122s
Epoch: 10 cost time: 37.16611385345459
Epoch: 10, Steps: 261 | Train Loss: 0.3775923 Vali Loss: 0.1933014 Test Loss: 0.2688375
Validation loss decreased (0.193751 --> 0.193301).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3322040
	speed: 0.6570s/iter; left time: 6794.3947s
	iters: 200, epoch: 11 | loss: 0.4102698
	speed: 0.1437s/iter; left time: 1471.6460s
Epoch: 11 cost time: 37.52349781990051
Epoch: 11, Steps: 261 | Train Loss: 0.3767842 Vali Loss: 0.1933514 Test Loss: 0.2686237
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5813174
	speed: 0.4629s/iter; left time: 4666.0170s
	iters: 200, epoch: 12 | loss: 0.3533948
	speed: 0.0690s/iter; left time: 688.9180s
Epoch: 12 cost time: 17.430738925933838
Epoch: 12, Steps: 261 | Train Loss: 0.3768743 Vali Loss: 0.1933146 Test Loss: 0.2685336
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3151031
	speed: 0.5729s/iter; left time: 5625.3958s
	iters: 200, epoch: 13 | loss: 0.3575438
	speed: 0.1356s/iter; left time: 1317.9149s
Epoch: 13 cost time: 37.2448570728302
Epoch: 13, Steps: 261 | Train Loss: 0.3764307 Vali Loss: 0.1931071 Test Loss: 0.2681892
Validation loss decreased (0.193301 --> 0.193107).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5236226
	speed: 0.5802s/iter; left time: 5545.3195s
	iters: 200, epoch: 14 | loss: 0.2780688
	speed: 0.0974s/iter; left time: 920.7828s
Epoch: 14 cost time: 27.540276765823364
Epoch: 14, Steps: 261 | Train Loss: 0.3763715 Vali Loss: 0.1929023 Test Loss: 0.2681724
Validation loss decreased (0.193107 --> 0.192902).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3259245
	speed: 0.5112s/iter; left time: 4752.7511s
	iters: 200, epoch: 15 | loss: 0.4504992
	speed: 0.1501s/iter; left time: 1380.8372s
Epoch: 15 cost time: 39.572810649871826
Epoch: 15, Steps: 261 | Train Loss: 0.3758750 Vali Loss: 0.1928833 Test Loss: 0.2682613
Validation loss decreased (0.192902 --> 0.192883).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4323370
	speed: 0.6483s/iter; left time: 5858.1777s
	iters: 200, epoch: 16 | loss: 0.3607631
	speed: 0.1101s/iter; left time: 983.9458s
Epoch: 16 cost time: 32.950135231018066
Epoch: 16, Steps: 261 | Train Loss: 0.3756711 Vali Loss: 0.1926350 Test Loss: 0.2679336
Validation loss decreased (0.192883 --> 0.192635).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4301091
	speed: 0.4850s/iter; left time: 4256.0359s
	iters: 200, epoch: 17 | loss: 0.3739873
	speed: 0.1443s/iter; left time: 1251.7654s
Epoch: 17 cost time: 38.29360747337341
Epoch: 17, Steps: 261 | Train Loss: 0.3748789 Vali Loss: 0.1928115 Test Loss: 0.2680135
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3833673
	speed: 0.6515s/iter; left time: 5547.1309s
	iters: 200, epoch: 18 | loss: 0.3829374
	speed: 0.1397s/iter; left time: 1175.0567s
Epoch: 18 cost time: 37.24351358413696
Epoch: 18, Steps: 261 | Train Loss: 0.3754179 Vali Loss: 0.1926536 Test Loss: 0.2679210
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4742998
	speed: 0.5970s/iter; left time: 4927.0176s
	iters: 200, epoch: 19 | loss: 0.3666595
	speed: 0.1324s/iter; left time: 1079.6021s
Epoch: 19 cost time: 35.03218173980713
Epoch: 19, Steps: 261 | Train Loss: 0.3751817 Vali Loss: 0.1925668 Test Loss: 0.2678769
Validation loss decreased (0.192635 --> 0.192567).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2900872
	speed: 0.5604s/iter; left time: 4478.3353s
	iters: 200, epoch: 20 | loss: 0.3969035
	speed: 0.1001s/iter; left time: 789.6946s
Epoch: 20 cost time: 27.137670755386353
Epoch: 20, Steps: 261 | Train Loss: 0.3752531 Vali Loss: 0.1927816 Test Loss: 0.2679087
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4847815
	speed: 0.3979s/iter; left time: 3076.0618s
	iters: 200, epoch: 21 | loss: 0.2766257
	speed: 0.0642s/iter; left time: 490.0552s
Epoch: 21 cost time: 18.506711959838867
Epoch: 21, Steps: 261 | Train Loss: 0.3752708 Vali Loss: 0.1926031 Test Loss: 0.2678680
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2954676
	speed: 0.2623s/iter; left time: 1959.3309s
	iters: 200, epoch: 22 | loss: 0.3578084
	speed: 0.1324s/iter; left time: 975.7626s
Epoch: 22 cost time: 31.7786386013031
Epoch: 22, Steps: 261 | Train Loss: 0.3746317 Vali Loss: 0.1927599 Test Loss: 0.2679766
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2688915729522705, mae:0.32630372047424316, rse:0.41884103417396545, corr:[0.5527759  0.55754703 0.55243653 0.55074364 0.55276936 0.5545684
 0.55386776 0.55231285 0.551865   0.5527306  0.55363643 0.5534315
 0.5525157  0.551933   0.55217457 0.5526898  0.55252355 0.5514466
 0.55016965 0.54942876 0.54936826 0.54949576 0.5492066  0.5484256
 0.5475841  0.54710025 0.54697347 0.54679847 0.5462269  0.545393
 0.5446819  0.544356   0.5442499  0.5439901  0.54334617 0.542464
 0.5416228  0.54102296 0.54060936 0.5401935  0.5396319  0.53894967
 0.5383279  0.5378626  0.53749543 0.53705055 0.53641355 0.5356109
 0.53475106 0.5339783  0.5333515  0.5328077  0.53218645 0.531452
 0.53071797 0.5301553  0.52976173 0.52945304 0.5291479  0.52882177
 0.5285347  0.5283699  0.5282435  0.5280111  0.52763814 0.5272271
 0.52689093 0.52674335 0.5266956  0.5265739  0.52630645 0.5259549
 0.52560574 0.52529496 0.5249148  0.524415   0.5238181  0.5232258
 0.5227303  0.5223394  0.52183723 0.52111405 0.52021486 0.5193601
 0.51877344 0.51845    0.51816666 0.51775306 0.51720583 0.516691
 0.516369   0.516154   0.51574856 0.51492757 0.51364994 0.51208425
 0.5105728  0.50935835 0.50829643 0.50718606 0.50599    0.5048463
 0.50390446 0.50309086 0.5022064  0.5010688  0.49977455 0.4985774
 0.49764886 0.49692902 0.4961744  0.4951903  0.4941102  0.49319664
 0.49260962 0.49220684 0.49161196 0.49054146 0.48911643 0.48775735
 0.48698726 0.48672235 0.48647615 0.48574474 0.4844586  0.483043
 0.48209587 0.48170337 0.48135716 0.480556   0.4792576  0.47786748
 0.47693965 0.47661394 0.47645777 0.47590753 0.4747412  0.47326136
 0.47195047 0.4711283  0.47063914 0.47019306 0.46969184 0.46922648
 0.46895722 0.46874204 0.4683771  0.46760976 0.46653864 0.46552476
 0.46495646 0.46478415 0.4644848  0.46369067 0.4624851  0.46133876
 0.46066546 0.46053445 0.46061108 0.46055263 0.46012858 0.45942655
 0.4586118  0.45784476 0.45717397 0.45665526 0.4563573  0.45636043
 0.45657682 0.45674276 0.45653328 0.45597124 0.45534137 0.45493105
 0.45474818 0.454505   0.45393577 0.4530987  0.45232275 0.4520111
 0.45204154 0.45199016 0.451559   0.45072046 0.44977242 0.44910157
 0.44885704 0.44880357 0.4485313  0.44781882 0.446773   0.44563758
 0.4445921  0.4436451  0.44251543 0.44109923 0.43958    0.43808562
 0.4367028  0.43541232 0.43428692 0.4333972  0.43273476 0.43206888
 0.4310715  0.42958158 0.42783773 0.4264339  0.4258213  0.42580718
 0.42559254 0.42465922 0.42313814 0.42168754 0.42089367 0.4205664
 0.4200959  0.4190177  0.4175489  0.416177   0.4151947  0.41445255
 0.4134467  0.41195    0.41035533 0.40922308 0.4089094  0.4090076
 0.40856865 0.4072024  0.4052434  0.40370464 0.40324655 0.40359926
 0.40374532 0.40300137 0.4014616  0.40002924 0.39953953 0.39988008
 0.4002364  0.3997407  0.39842108 0.39720306 0.39677435 0.39721844
 0.39791477 0.39826384 0.3980662  0.39758646 0.39730263 0.3971335
 0.39673305 0.39609057 0.39546973 0.39545864 0.39608654 0.3968163
 0.3969306  0.39620265 0.39507923 0.39426708 0.39401925 0.39401573
 0.39367497 0.3928429  0.39215624 0.3922528  0.39307976 0.39395428
 0.39426115 0.3939727  0.3933942  0.39295265 0.3925245  0.3917381
 0.3906303  0.38955584 0.38922596 0.38990742 0.39077574 0.39084193
 0.389725   0.38810647 0.3871825  0.3874526  0.3881246  0.38779488
 0.3860498  0.38394278 0.38279793 0.383046   0.38375875 0.38354927
 0.3820738  0.38004383 0.37876177 0.3788635  0.37972957 0.38002738
 0.3795175  0.3787444  0.3786275  0.37894103 0.37882265 0.37764078
 0.37588513 0.37470865 0.37516207 0.37673256 0.37784645 0.3777756
 0.37677276 0.37626487 0.3768769  0.37766573 0.3772597  0.37534004
 0.37318936 0.37305784 0.3752923  0.3777615  0.37823397 0.37650368
 0.37458628 0.37508282 0.3777672  0.37980714 0.3793938  0.37693828
 0.37610266 0.37847224 0.38103366 0.38033152 0.373897   0.36827937]
