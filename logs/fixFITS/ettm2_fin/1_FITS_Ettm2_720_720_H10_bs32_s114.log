Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7257600.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6147265
	speed: 0.1159s/iter; left time: 2984.3545s
	iters: 200, epoch: 1 | loss: 0.4986283
	speed: 0.1073s/iter; left time: 2752.6995s
	iters: 300, epoch: 1 | loss: 0.6506727
	speed: 0.1173s/iter; left time: 2997.2452s
	iters: 400, epoch: 1 | loss: 0.4299901
	speed: 0.1077s/iter; left time: 2741.1088s
	iters: 500, epoch: 1 | loss: 0.3621405
	speed: 0.1068s/iter; left time: 2706.7810s
Epoch: 1 cost time: 57.43770384788513
Epoch: 1, Steps: 517 | Train Loss: 0.5553317 Vali Loss: 0.2720099 Test Loss: 0.3629840
Validation loss decreased (inf --> 0.272010).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4083140
	speed: 0.7317s/iter; left time: 18464.8148s
	iters: 200, epoch: 2 | loss: 0.5047854
	speed: 0.1260s/iter; left time: 3166.0576s
	iters: 300, epoch: 2 | loss: 0.5520814
	speed: 0.1129s/iter; left time: 2827.0869s
	iters: 400, epoch: 2 | loss: 0.4933420
	speed: 0.1097s/iter; left time: 2734.4180s
	iters: 500, epoch: 2 | loss: 0.6635686
	speed: 0.1212s/iter; left time: 3010.5936s
Epoch: 2 cost time: 61.17518091201782
Epoch: 2, Steps: 517 | Train Loss: 0.5095312 Vali Loss: 0.2663236 Test Loss: 0.3565421
Validation loss decreased (0.272010 --> 0.266324).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2976347
	speed: 0.7335s/iter; left time: 18130.3085s
	iters: 200, epoch: 3 | loss: 0.3486871
	speed: 0.1181s/iter; left time: 2907.0075s
	iters: 300, epoch: 3 | loss: 0.3491807
	speed: 0.0976s/iter; left time: 2392.4908s
	iters: 400, epoch: 3 | loss: 0.3573803
	speed: 0.0997s/iter; left time: 2434.3680s
	iters: 500, epoch: 3 | loss: 0.4869729
	speed: 0.1125s/iter; left time: 2734.9203s
Epoch: 3 cost time: 56.911160707473755
Epoch: 3, Steps: 517 | Train Loss: 0.5033474 Vali Loss: 0.2643928 Test Loss: 0.3541110
Validation loss decreased (0.266324 --> 0.264393).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.8489177
	speed: 0.6995s/iter; left time: 16928.6791s
	iters: 200, epoch: 4 | loss: 0.6552709
	speed: 0.1078s/iter; left time: 2599.1429s
	iters: 300, epoch: 4 | loss: 0.6659749
	speed: 0.0984s/iter; left time: 2361.3212s
	iters: 400, epoch: 4 | loss: 0.5387899
	speed: 0.0806s/iter; left time: 1926.6031s
	iters: 500, epoch: 4 | loss: 0.4850803
	speed: 0.1102s/iter; left time: 2622.2752s
Epoch: 4 cost time: 51.86740279197693
Epoch: 4, Steps: 517 | Train Loss: 0.5006836 Vali Loss: 0.2635131 Test Loss: 0.3527507
Validation loss decreased (0.264393 --> 0.263513).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3748219
	speed: 0.2455s/iter; left time: 5813.2627s
	iters: 200, epoch: 5 | loss: 0.4527194
	speed: 0.0617s/iter; left time: 1455.6702s
	iters: 300, epoch: 5 | loss: 0.5124580
	speed: 0.1030s/iter; left time: 2419.0070s
	iters: 400, epoch: 5 | loss: 0.3415354
	speed: 0.0993s/iter; left time: 2322.1660s
	iters: 500, epoch: 5 | loss: 0.5952792
	speed: 0.1105s/iter; left time: 2573.8164s
Epoch: 5 cost time: 45.30589818954468
Epoch: 5, Steps: 517 | Train Loss: 0.4989782 Vali Loss: 0.2624697 Test Loss: 0.3515330
Validation loss decreased (0.263513 --> 0.262470).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3121331
	speed: 0.6635s/iter; left time: 15370.9112s
	iters: 200, epoch: 6 | loss: 0.4836451
	speed: 0.1063s/iter; left time: 2451.0532s
	iters: 300, epoch: 6 | loss: 0.5207767
	speed: 0.1005s/iter; left time: 2309.0528s
	iters: 400, epoch: 6 | loss: 0.5365044
	speed: 0.0944s/iter; left time: 2158.5014s
	iters: 500, epoch: 6 | loss: 0.6421626
	speed: 0.1019s/iter; left time: 2318.7679s
Epoch: 6 cost time: 52.704649686813354
Epoch: 6, Steps: 517 | Train Loss: 0.4974546 Vali Loss: 0.2623134 Test Loss: 0.3509374
Validation loss decreased (0.262470 --> 0.262313).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3843312
	speed: 0.6576s/iter; left time: 14893.4557s
	iters: 200, epoch: 7 | loss: 0.3535546
	speed: 0.0971s/iter; left time: 2189.3561s
	iters: 300, epoch: 7 | loss: 0.5384642
	speed: 0.1010s/iter; left time: 2266.3676s
	iters: 400, epoch: 7 | loss: 0.2985925
	speed: 0.0888s/iter; left time: 1984.9145s
	iters: 500, epoch: 7 | loss: 0.4021619
	speed: 0.0914s/iter; left time: 2034.3634s
Epoch: 7 cost time: 50.06655025482178
Epoch: 7, Steps: 517 | Train Loss: 0.4971268 Vali Loss: 0.2618738 Test Loss: 0.3510701
Validation loss decreased (0.262313 --> 0.261874).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3779905
	speed: 0.6449s/iter; left time: 14272.2653s
	iters: 200, epoch: 8 | loss: 0.4433313
	speed: 0.0968s/iter; left time: 2131.7129s
	iters: 300, epoch: 8 | loss: 0.3280469
	speed: 0.1045s/iter; left time: 2290.9698s
	iters: 400, epoch: 8 | loss: 0.3853180
	speed: 0.1070s/iter; left time: 2335.5074s
	iters: 500, epoch: 8 | loss: 0.2837333
	speed: 0.1101s/iter; left time: 2393.3076s
Epoch: 8 cost time: 54.74106979370117
Epoch: 8, Steps: 517 | Train Loss: 0.4964124 Vali Loss: 0.2612446 Test Loss: 0.3507301
Validation loss decreased (0.261874 --> 0.261245).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3643263
	speed: 0.7884s/iter; left time: 17041.8845s
	iters: 200, epoch: 9 | loss: 0.4191893
	speed: 0.1183s/iter; left time: 2545.3120s
	iters: 300, epoch: 9 | loss: 0.3136348
	speed: 0.1182s/iter; left time: 2530.8847s
	iters: 400, epoch: 9 | loss: 0.5651154
	speed: 0.1208s/iter; left time: 2574.0424s
	iters: 500, epoch: 9 | loss: 0.5447007
	speed: 0.1092s/iter; left time: 2316.9013s
Epoch: 9 cost time: 62.082282304763794
Epoch: 9, Steps: 517 | Train Loss: 0.4960504 Vali Loss: 0.2615848 Test Loss: 0.3506306
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6027872
	speed: 0.6900s/iter; left time: 14557.8109s
	iters: 200, epoch: 10 | loss: 0.5706211
	speed: 0.1019s/iter; left time: 2139.7073s
	iters: 300, epoch: 10 | loss: 0.5782067
	speed: 0.1010s/iter; left time: 2111.1156s
	iters: 400, epoch: 10 | loss: 0.4980967
	speed: 0.0969s/iter; left time: 2016.0999s
	iters: 500, epoch: 10 | loss: 0.5766174
	speed: 0.1002s/iter; left time: 2074.3346s
Epoch: 10 cost time: 52.20982885360718
Epoch: 10, Steps: 517 | Train Loss: 0.4956546 Vali Loss: 0.2614812 Test Loss: 0.3504329
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4171972
	speed: 0.5235s/iter; left time: 10774.4001s
	iters: 200, epoch: 11 | loss: 0.6831986
	speed: 0.0634s/iter; left time: 1297.5884s
	iters: 300, epoch: 11 | loss: 0.3052023
	speed: 0.0516s/iter; left time: 1051.2632s
	iters: 400, epoch: 11 | loss: 0.3824120
	speed: 0.0521s/iter; left time: 1056.8625s
	iters: 500, epoch: 11 | loss: 0.3981748
	speed: 0.0690s/iter; left time: 1392.9648s
Epoch: 11 cost time: 35.3763382434845
Epoch: 11, Steps: 517 | Train Loss: 0.4954894 Vali Loss: 0.2616675 Test Loss: 0.3501509
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.350067675113678, mae:0.3785286545753479, rse:0.4755765199661255, corr:[0.5296575  0.5399984  0.54013157 0.53690404 0.53504264 0.53543395
 0.5372452  0.53886646 0.53908837 0.5381061  0.53694516 0.53638643
 0.5366596  0.53746724 0.53817123 0.5380858  0.5371216  0.53575224
 0.53454363 0.533818   0.5336249  0.5337221  0.53377765 0.5335501
 0.53298616 0.53225553 0.5315566  0.53101283 0.5306244  0.5302943
 0.5298815  0.5293573  0.5287561  0.52813196 0.5275592  0.527072
 0.5266121  0.5261319  0.52556205 0.52491677 0.52425563 0.52361536
 0.5230426  0.52252614 0.5220174  0.52143395 0.5207368  0.5199528
 0.5191143  0.5182554  0.51747036 0.5168535  0.51638854 0.5160067
 0.5156128  0.51513547 0.5145761  0.5139956  0.51348686 0.5130927
 0.5128522  0.5127069  0.5125588  0.5123163  0.5119522  0.51155025
 0.51113355 0.5108153  0.51058865 0.51037425 0.51006657 0.50959533
 0.5089635  0.5082221  0.5074745  0.5068437  0.50638425 0.50600576
 0.5056338  0.5051379  0.50446934 0.503696   0.5029606  0.5023737
 0.50196224 0.50164235 0.5012855  0.50087124 0.50036305 0.49979082
 0.49920318 0.49859378 0.4979248  0.4971014  0.49600193 0.4946018
 0.49299526 0.49137256 0.4898685  0.48859686 0.4875821  0.48672473
 0.4858393  0.48478365 0.48351046 0.4821148  0.48077917 0.47968832
 0.47887993 0.47826448 0.47767493 0.47695696 0.47610858 0.47517708
 0.4742883  0.47357717 0.47307548 0.47265875 0.4721564  0.47138846
 0.47031027 0.46894857 0.46747836 0.46605355 0.46476617 0.46359274
 0.46245903 0.46126765 0.46000344 0.45876154 0.4577321  0.45703194
 0.45665973 0.4564282  0.45613563 0.45561    0.45480776 0.45388407
 0.45306727 0.4525463  0.45232543 0.4522166  0.45195463 0.45129368
 0.45020226 0.44883442 0.4475903  0.44676813 0.44646502 0.44647345
 0.446467   0.44612598 0.44527394 0.44403693 0.442753   0.44180292
 0.44139075 0.44142663 0.4416263  0.44172192 0.44149294 0.44093096
 0.44018853 0.43952796 0.4391235  0.43896034 0.4388676  0.43865535
 0.43819678 0.43748873 0.43659386 0.4357366  0.4350481  0.4345742
 0.43421045 0.4338351  0.4333585  0.43278533 0.43218768 0.4316652
 0.4312095  0.43076658 0.43025306 0.42958027 0.4287612  0.42790624
 0.42723688 0.42680728 0.4265615  0.42626247 0.42566586 0.42456943
 0.42296213 0.42110065 0.4191674  0.4174234  0.4160648  0.41505587
 0.4142258  0.41334403 0.41224432 0.41091606 0.4094766  0.40810058
 0.40691832 0.40595436 0.4051179  0.40430048 0.4034861  0.4027299
 0.40197957 0.4012137  0.40039247 0.3994501  0.39833277 0.3969735
 0.39543727 0.39385864 0.39250723 0.39149868 0.39075866 0.39008662
 0.389275   0.38822228 0.3869887  0.3856406  0.38438457 0.3834504
 0.38282833 0.38241312 0.3819839  0.38139525 0.38061762 0.37977317
 0.378998   0.37843758 0.3780756  0.37776873 0.3773638  0.3767413
 0.37591982 0.37501377 0.37426898 0.37397677 0.3740548  0.37428325
 0.3743323  0.3740014  0.3732941  0.37244445 0.37187007 0.37186408
 0.37245998 0.3734043  0.37418404 0.37442076 0.37397254 0.37302923
 0.37193385 0.37105724 0.37063476 0.3706866  0.37093982 0.37108818
 0.37085015 0.37013638 0.36913997 0.3681591  0.36743858 0.3670933
 0.36713874 0.36737704 0.36751804 0.36740956 0.36699787 0.36635253
 0.36564854 0.36495036 0.36431122 0.3638584  0.36359844 0.3635693
 0.36371195 0.36393023 0.3640595  0.3638845  0.36324206 0.36199582
 0.36028785 0.35845786 0.35677257 0.3554144  0.354471   0.35385337
 0.3534909  0.35322097 0.35294268 0.35265222 0.3524338  0.3522706
 0.35215124 0.35187083 0.3513268  0.35037413 0.34912944 0.3478089
 0.3467739  0.34621334 0.3461779  0.34652933 0.34697908 0.34730047
 0.347215   0.34670767 0.34585765 0.3448664  0.34399745 0.34343365
 0.343169   0.34315833 0.3433068  0.34349132 0.34361377 0.34365153
 0.34357825 0.3434447  0.34327862 0.34308153 0.34299386 0.34310097
 0.34350902 0.34412545 0.34477764 0.34524155 0.34524047 0.34470695
 0.3436958  0.34242243 0.34133643 0.3408429  0.34106117 0.3417875
 0.34268427 0.34338617 0.34367183 0.34349713 0.3430383  0.3425842
 0.3423267  0.34232455 0.34243459 0.34243712 0.34218597 0.34164405
 0.34092242 0.34022143 0.33964774 0.3393037  0.33904064 0.33877262
 0.33841997 0.33799338 0.33757943 0.33728686 0.33715755 0.33720508
 0.3372829  0.3373066  0.33717462 0.33690867 0.33661175 0.33637077
 0.3363035  0.33650306 0.33694723 0.33754763 0.3381706  0.3387186
 0.3390859  0.3391373  0.33875507 0.3378571  0.3365973  0.3351227
 0.33379662 0.3328938  0.33250552 0.3324577  0.3324191  0.3321884
 0.33157688 0.33061177 0.32948565 0.32849243 0.32788065 0.32767883
 0.32775047 0.32788146 0.32777417 0.32730183 0.32657766 0.3257352
 0.3249956  0.3244753  0.3242001  0.32408276 0.32386252 0.3234279
 0.32275382 0.32201475 0.3213988  0.32100227 0.32083353 0.32072175
 0.3205456  0.32020754 0.3196529  0.31900507 0.31844562 0.31817463
 0.31827343 0.31862974 0.31900167 0.31920183 0.31918243 0.31904247
 0.31899136 0.3191815  0.31968704 0.320386   0.32106146 0.32146466
 0.32143593 0.32095402 0.3202451  0.3195882  0.31917652 0.3189836
 0.31880522 0.31850752 0.31792617 0.3170566  0.31611952 0.31543773
 0.3151373  0.31526154 0.31563556 0.31596464 0.3160288  0.3157124
 0.31513777 0.3144958  0.31399816 0.31367636 0.31351966 0.3134557
 0.31333774 0.31308573 0.31272155 0.3123828  0.31224293 0.31227595
 0.31244174 0.3126155  0.3126887  0.31259182 0.31232458 0.31195962
 0.3115982  0.311268   0.3109286  0.31053427 0.31011358 0.3096651
 0.3092476  0.3088438  0.3084755  0.30802974 0.3074149  0.30651173
 0.30529806 0.30394992 0.30260852 0.30144638 0.30053696 0.29978502
 0.29896575 0.29792848 0.29657525 0.295056   0.2935818  0.2924135
 0.2917323  0.2915967  0.29178423 0.29194143 0.2918109  0.29125935
 0.29037    0.28927365 0.2882236  0.28750885 0.28715956 0.28711963
 0.2871057  0.28690132 0.28644258 0.2857546  0.2850215  0.28433657
 0.28378922 0.28336743 0.2829831  0.2825592  0.2820874  0.28158388
 0.28107405 0.28055966 0.28017458 0.27986276 0.27955988 0.27924952
 0.27894008 0.27869624 0.27853855 0.27847853 0.27849156 0.27847412
 0.2782951  0.27793795 0.27744862 0.27695748 0.27662066 0.27650103
 0.27656993 0.2767701  0.27710265 0.27743223 0.27766463 0.27774125
 0.27765486 0.2774528  0.27713048 0.27675375 0.27634236 0.27592713
 0.27553242 0.27521405 0.274965   0.27475494 0.274481   0.2741302
 0.27367532 0.27322018 0.2728522  0.2726297  0.27242917 0.27228972
 0.27221388 0.2722742  0.27243388 0.27264988 0.2729014  0.27312532
 0.27323526 0.27314946 0.27288333 0.27247763 0.27202085 0.27158996
 0.27121508 0.2708387  0.27030534 0.26953372 0.26851213 0.26724336
 0.26587498 0.26473284 0.2639132  0.26334757 0.26280028 0.262081
 0.26111266 0.25996774 0.25881645 0.2578289  0.25710863 0.2566795
 0.25640655 0.2560834  0.2556471  0.2550503  0.25435048 0.25370035
 0.2532074  0.25288317 0.25275478 0.2526647  0.2525362  0.25229082
 0.25187603 0.2513544  0.25074184 0.25008726 0.24953225 0.24912661
 0.24878713 0.24845794 0.24813768 0.24773258 0.24731302 0.24694711
 0.24676085 0.2466367  0.24658689 0.2466288  0.24668527 0.24667965
 0.24662286 0.24663727 0.24680737 0.24720798 0.24786451 0.24869674
 0.24949308 0.2500425  0.25025365 0.2502472  0.25014767 0.25005654
 0.24997863 0.24992609 0.24991146 0.24975055 0.24958892 0.24938747
 0.24916455 0.24895152 0.2486881  0.24847317 0.24817948 0.2478839
 0.24762546 0.24758159 0.24768655 0.24778341 0.2479259  0.24791741
 0.24782164 0.24764678 0.24746972 0.24732675 0.24728015 0.24711685
 0.24687709 0.24646905 0.24588273 0.24539371 0.24519052 0.24530903
 0.24571268 0.24644317 0.24726842 0.24802099 0.24864587 0.24900731
 0.24901792 0.24873517 0.24817888 0.2472915  0.24611637 0.24484093
 0.24366625 0.24274175 0.24205837 0.24146329 0.24087222 0.24012735
 0.23916137 0.23802936 0.23711745 0.23646511 0.23612507 0.23596561
 0.23588298 0.23580831 0.2356947  0.23549145 0.2353016  0.23502676
 0.23468801 0.23427701 0.2337714  0.23341234 0.23313853 0.23286149
 0.23254476 0.23208827 0.23156802 0.23093642 0.23033491 0.22984324
 0.22963287 0.22944976 0.22923812 0.22896571 0.22864383 0.22841085
 0.22827183 0.22823323 0.22809519 0.2278792  0.22728927 0.2264963
 0.2257788  0.22548506 0.22556013 0.22525142 0.22309358 0.21708034]
