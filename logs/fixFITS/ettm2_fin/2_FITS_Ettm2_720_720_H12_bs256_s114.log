Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  80539648.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.293238639831543
Epoch: 1, Steps: 64 | Train Loss: 0.5500802 Vali Loss: 0.3489432 Test Loss: 0.4811202
Validation loss decreased (inf --> 0.348943).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.272037982940674
Epoch: 2, Steps: 64 | Train Loss: 0.4375200 Vali Loss: 0.3182027 Test Loss: 0.4411824
Validation loss decreased (0.348943 --> 0.318203).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.892260789871216
Epoch: 3, Steps: 64 | Train Loss: 0.3869161 Vali Loss: 0.3042937 Test Loss: 0.4229741
Validation loss decreased (0.318203 --> 0.304294).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.306507110595703
Epoch: 4, Steps: 64 | Train Loss: 0.3590788 Vali Loss: 0.2961789 Test Loss: 0.4132445
Validation loss decreased (0.304294 --> 0.296179).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 12.216004848480225
Epoch: 5, Steps: 64 | Train Loss: 0.3420344 Vali Loss: 0.2912546 Test Loss: 0.4073132
Validation loss decreased (0.296179 --> 0.291255).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 12.560682773590088
Epoch: 6, Steps: 64 | Train Loss: 0.3292810 Vali Loss: 0.2877003 Test Loss: 0.4030882
Validation loss decreased (0.291255 --> 0.287700).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.616927146911621
Epoch: 7, Steps: 64 | Train Loss: 0.3197869 Vali Loss: 0.2854459 Test Loss: 0.3999169
Validation loss decreased (0.287700 --> 0.285446).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 13.307599067687988
Epoch: 8, Steps: 64 | Train Loss: 0.3130368 Vali Loss: 0.2832103 Test Loss: 0.3973278
Validation loss decreased (0.285446 --> 0.283210).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.262683868408203
Epoch: 9, Steps: 64 | Train Loss: 0.3059914 Vali Loss: 0.2815155 Test Loss: 0.3951673
Validation loss decreased (0.283210 --> 0.281516).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.532193422317505
Epoch: 10, Steps: 64 | Train Loss: 0.3003714 Vali Loss: 0.2801994 Test Loss: 0.3932414
Validation loss decreased (0.281516 --> 0.280199).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.597100019454956
Epoch: 11, Steps: 64 | Train Loss: 0.2965586 Vali Loss: 0.2783884 Test Loss: 0.3916843
Validation loss decreased (0.280199 --> 0.278388).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.717223405838013
Epoch: 12, Steps: 64 | Train Loss: 0.2922973 Vali Loss: 0.2777162 Test Loss: 0.3901658
Validation loss decreased (0.278388 --> 0.277716).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.755210638046265
Epoch: 13, Steps: 64 | Train Loss: 0.2891464 Vali Loss: 0.2763542 Test Loss: 0.3888524
Validation loss decreased (0.277716 --> 0.276354).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.762733221054077
Epoch: 14, Steps: 64 | Train Loss: 0.2856267 Vali Loss: 0.2755565 Test Loss: 0.3876778
Validation loss decreased (0.276354 --> 0.275557).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.47948408126831
Epoch: 15, Steps: 64 | Train Loss: 0.2830677 Vali Loss: 0.2748831 Test Loss: 0.3866282
Validation loss decreased (0.275557 --> 0.274883).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.008164167404175
Epoch: 16, Steps: 64 | Train Loss: 0.2814477 Vali Loss: 0.2742411 Test Loss: 0.3857383
Validation loss decreased (0.274883 --> 0.274241).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.089882612228394
Epoch: 17, Steps: 64 | Train Loss: 0.2788856 Vali Loss: 0.2734783 Test Loss: 0.3848507
Validation loss decreased (0.274241 --> 0.273478).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.01749324798584
Epoch: 18, Steps: 64 | Train Loss: 0.2770246 Vali Loss: 0.2729109 Test Loss: 0.3841109
Validation loss decreased (0.273478 --> 0.272911).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.42273473739624
Epoch: 19, Steps: 64 | Train Loss: 0.2763465 Vali Loss: 0.2725247 Test Loss: 0.3833641
Validation loss decreased (0.272911 --> 0.272525).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 11.183868408203125
Epoch: 20, Steps: 64 | Train Loss: 0.2742827 Vali Loss: 0.2719641 Test Loss: 0.3826896
Validation loss decreased (0.272525 --> 0.271964).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.357630491256714
Epoch: 21, Steps: 64 | Train Loss: 0.2729456 Vali Loss: 0.2714370 Test Loss: 0.3821076
Validation loss decreased (0.271964 --> 0.271437).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 11.157010555267334
Epoch: 22, Steps: 64 | Train Loss: 0.2717414 Vali Loss: 0.2710454 Test Loss: 0.3815292
Validation loss decreased (0.271437 --> 0.271045).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 13.304856061935425
Epoch: 23, Steps: 64 | Train Loss: 0.2707219 Vali Loss: 0.2705561 Test Loss: 0.3810285
Validation loss decreased (0.271045 --> 0.270556).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 11.839540004730225
Epoch: 24, Steps: 64 | Train Loss: 0.2695357 Vali Loss: 0.2704492 Test Loss: 0.3805351
Validation loss decreased (0.270556 --> 0.270449).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 12.590290307998657
Epoch: 25, Steps: 64 | Train Loss: 0.2694113 Vali Loss: 0.2699132 Test Loss: 0.3800837
Validation loss decreased (0.270449 --> 0.269913).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 11.155988931655884
Epoch: 26, Steps: 64 | Train Loss: 0.2680082 Vali Loss: 0.2695594 Test Loss: 0.3797280
Validation loss decreased (0.269913 --> 0.269559).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.7373685836792
Epoch: 27, Steps: 64 | Train Loss: 0.2674727 Vali Loss: 0.2694657 Test Loss: 0.3793732
Validation loss decreased (0.269559 --> 0.269466).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 11.144057512283325
Epoch: 28, Steps: 64 | Train Loss: 0.2672092 Vali Loss: 0.2690263 Test Loss: 0.3790106
Validation loss decreased (0.269466 --> 0.269026).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 10.888191223144531
Epoch: 29, Steps: 64 | Train Loss: 0.2656170 Vali Loss: 0.2687638 Test Loss: 0.3786774
Validation loss decreased (0.269026 --> 0.268764).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 11.214705228805542
Epoch: 30, Steps: 64 | Train Loss: 0.2659267 Vali Loss: 0.2687047 Test Loss: 0.3783643
Validation loss decreased (0.268764 --> 0.268705).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 10.144843816757202
Epoch: 31, Steps: 64 | Train Loss: 0.2656286 Vali Loss: 0.2684966 Test Loss: 0.3780871
Validation loss decreased (0.268705 --> 0.268497).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 10.446521759033203
Epoch: 32, Steps: 64 | Train Loss: 0.2650181 Vali Loss: 0.2685587 Test Loss: 0.3778569
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 11.177366018295288
Epoch: 33, Steps: 64 | Train Loss: 0.2642268 Vali Loss: 0.2681092 Test Loss: 0.3776296
Validation loss decreased (0.268497 --> 0.268109).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 11.114965200424194
Epoch: 34, Steps: 64 | Train Loss: 0.2639371 Vali Loss: 0.2679272 Test Loss: 0.3773906
Validation loss decreased (0.268109 --> 0.267927).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 11.32548213005066
Epoch: 35, Steps: 64 | Train Loss: 0.2637138 Vali Loss: 0.2676045 Test Loss: 0.3771920
Validation loss decreased (0.267927 --> 0.267605).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 10.776971101760864
Epoch: 36, Steps: 64 | Train Loss: 0.2637192 Vali Loss: 0.2675166 Test Loss: 0.3770004
Validation loss decreased (0.267605 --> 0.267517).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 11.109803438186646
Epoch: 37, Steps: 64 | Train Loss: 0.2632748 Vali Loss: 0.2675044 Test Loss: 0.3768366
Validation loss decreased (0.267517 --> 0.267504).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 12.205389022827148
Epoch: 38, Steps: 64 | Train Loss: 0.2630536 Vali Loss: 0.2673713 Test Loss: 0.3766580
Validation loss decreased (0.267504 --> 0.267371).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 11.736490726470947
Epoch: 39, Steps: 64 | Train Loss: 0.2620425 Vali Loss: 0.2672772 Test Loss: 0.3765007
Validation loss decreased (0.267371 --> 0.267277).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 11.26132607460022
Epoch: 40, Steps: 64 | Train Loss: 0.2622942 Vali Loss: 0.2673262 Test Loss: 0.3763552
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 10.701491117477417
Epoch: 41, Steps: 64 | Train Loss: 0.2618722 Vali Loss: 0.2670753 Test Loss: 0.3762110
Validation loss decreased (0.267277 --> 0.267075).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 10.768088817596436
Epoch: 42, Steps: 64 | Train Loss: 0.2618161 Vali Loss: 0.2671093 Test Loss: 0.3760679
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 11.79556941986084
Epoch: 43, Steps: 64 | Train Loss: 0.2620668 Vali Loss: 0.2667303 Test Loss: 0.3759583
Validation loss decreased (0.267075 --> 0.266730).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 11.771059274673462
Epoch: 44, Steps: 64 | Train Loss: 0.2610447 Vali Loss: 0.2669742 Test Loss: 0.3758263
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 12.45871114730835
Epoch: 45, Steps: 64 | Train Loss: 0.2616061 Vali Loss: 0.2664443 Test Loss: 0.3757344
Validation loss decreased (0.266730 --> 0.266444).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 12.601038455963135
Epoch: 46, Steps: 64 | Train Loss: 0.2610493 Vali Loss: 0.2667669 Test Loss: 0.3756274
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 12.116000175476074
Epoch: 47, Steps: 64 | Train Loss: 0.2612730 Vali Loss: 0.2667123 Test Loss: 0.3755228
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 12.725368976593018
Epoch: 48, Steps: 64 | Train Loss: 0.2605418 Vali Loss: 0.2664320 Test Loss: 0.3754390
Validation loss decreased (0.266444 --> 0.266432).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 12.111335754394531
Epoch: 49, Steps: 64 | Train Loss: 0.2608322 Vali Loss: 0.2661150 Test Loss: 0.3753410
Validation loss decreased (0.266432 --> 0.266115).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 11.709797859191895
Epoch: 50, Steps: 64 | Train Loss: 0.2602197 Vali Loss: 0.2664278 Test Loss: 0.3752601
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.0497355408796396e-05
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  80539648.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 12.677218198776245
Epoch: 1, Steps: 64 | Train Loss: 0.5027718 Vali Loss: 0.2634205 Test Loss: 0.3726989
Validation loss decreased (inf --> 0.263420).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.98866057395935
Epoch: 2, Steps: 64 | Train Loss: 0.4982522 Vali Loss: 0.2626039 Test Loss: 0.3712668
Validation loss decreased (0.263420 --> 0.262604).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.660741329193115
Epoch: 3, Steps: 64 | Train Loss: 0.4976618 Vali Loss: 0.2620850 Test Loss: 0.3704773
Validation loss decreased (0.262604 --> 0.262085).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.295344114303589
Epoch: 4, Steps: 64 | Train Loss: 0.4961687 Vali Loss: 0.2616636 Test Loss: 0.3700677
Validation loss decreased (0.262085 --> 0.261664).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.51152229309082
Epoch: 5, Steps: 64 | Train Loss: 0.4963836 Vali Loss: 0.2611727 Test Loss: 0.3697110
Validation loss decreased (0.261664 --> 0.261173).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 12.816761255264282
Epoch: 6, Steps: 64 | Train Loss: 0.4962806 Vali Loss: 0.2609413 Test Loss: 0.3695070
Validation loss decreased (0.261173 --> 0.260941).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.287803888320923
Epoch: 7, Steps: 64 | Train Loss: 0.4970003 Vali Loss: 0.2611680 Test Loss: 0.3693394
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.476783514022827
Epoch: 8, Steps: 64 | Train Loss: 0.4951982 Vali Loss: 0.2608832 Test Loss: 0.3692596
Validation loss decreased (0.260941 --> 0.260883).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.853297710418701
Epoch: 9, Steps: 64 | Train Loss: 0.4945815 Vali Loss: 0.2607833 Test Loss: 0.3690782
Validation loss decreased (0.260883 --> 0.260783).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 12.398054838180542
Epoch: 10, Steps: 64 | Train Loss: 0.4950374 Vali Loss: 0.2606899 Test Loss: 0.3689693
Validation loss decreased (0.260783 --> 0.260690).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.670890092849731
Epoch: 11, Steps: 64 | Train Loss: 0.4951354 Vali Loss: 0.2607000 Test Loss: 0.3688022
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 13.269753456115723
Epoch: 12, Steps: 64 | Train Loss: 0.4951734 Vali Loss: 0.2608270 Test Loss: 0.3687372
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 12.874326705932617
Epoch: 13, Steps: 64 | Train Loss: 0.4942729 Vali Loss: 0.2606738 Test Loss: 0.3686708
Validation loss decreased (0.260690 --> 0.260674).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.715277910232544
Epoch: 14, Steps: 64 | Train Loss: 0.4947534 Vali Loss: 0.2605819 Test Loss: 0.3685241
Validation loss decreased (0.260674 --> 0.260582).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 12.017625570297241
Epoch: 15, Steps: 64 | Train Loss: 0.4946608 Vali Loss: 0.2606909 Test Loss: 0.3684888
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.575128555297852
Epoch: 16, Steps: 64 | Train Loss: 0.4949404 Vali Loss: 0.2607126 Test Loss: 0.3683999
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 12.528876781463623
Epoch: 17, Steps: 64 | Train Loss: 0.4944760 Vali Loss: 0.2605685 Test Loss: 0.3683965
Validation loss decreased (0.260582 --> 0.260568).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 12.033312797546387
Epoch: 18, Steps: 64 | Train Loss: 0.4945868 Vali Loss: 0.2604853 Test Loss: 0.3683304
Validation loss decreased (0.260568 --> 0.260485).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 12.37555980682373
Epoch: 19, Steps: 64 | Train Loss: 0.4948715 Vali Loss: 0.2605274 Test Loss: 0.3683109
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 12.10619306564331
Epoch: 20, Steps: 64 | Train Loss: 0.4935092 Vali Loss: 0.2605233 Test Loss: 0.3682809
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.766461372375488
Epoch: 21, Steps: 64 | Train Loss: 0.4945860 Vali Loss: 0.2603166 Test Loss: 0.3683332
Validation loss decreased (0.260485 --> 0.260317).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 11.904414415359497
Epoch: 22, Steps: 64 | Train Loss: 0.4955310 Vali Loss: 0.2603798 Test Loss: 0.3682950
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.57769775390625
Epoch: 23, Steps: 64 | Train Loss: 0.4946151 Vali Loss: 0.2602516 Test Loss: 0.3682213
Validation loss decreased (0.260317 --> 0.260252).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 11.009809494018555
Epoch: 24, Steps: 64 | Train Loss: 0.4950072 Vali Loss: 0.2603518 Test Loss: 0.3681965
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.789727687835693
Epoch: 25, Steps: 64 | Train Loss: 0.4943457 Vali Loss: 0.2604337 Test Loss: 0.3681934
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 11.533117532730103
Epoch: 26, Steps: 64 | Train Loss: 0.4923878 Vali Loss: 0.2604598 Test Loss: 0.3681613
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.34894564747810364, mae:0.3778811991214752, rse:0.4748137593269348, corr:[0.53940237 0.54428256 0.5422581  0.5394682  0.5387055  0.53965265
 0.54074574 0.5406958  0.5396773  0.53871715 0.5384748  0.5388459
 0.5392345  0.53904516 0.53830564 0.53742605 0.5368382  0.53661704
 0.5364508  0.53594995 0.53504705 0.53398424 0.53315395 0.5327664
 0.53268635 0.53260446 0.53223443 0.53154594 0.5307251  0.53002805
 0.5295599  0.5292933  0.52900285 0.52848107 0.52775824 0.5269967
 0.5263043  0.5257398  0.5252152  0.5246595  0.52404636 0.5233938
 0.5227946  0.52227765 0.5218124  0.5212792  0.5206023  0.5198056
 0.51893425 0.51805645 0.51729244 0.51670855 0.51619595 0.5156252
 0.5149492  0.5142307  0.51358426 0.51312566 0.5128933  0.5127758
 0.51266384 0.5124703  0.5121821  0.51184213 0.5115252  0.51129705
 0.511046   0.51076853 0.5104441  0.5100827  0.5097246  0.50940055
 0.5090988  0.5087429  0.50825894 0.50767    0.50705856 0.50648564
 0.50600517 0.5055297  0.50492895 0.5041918  0.5033909  0.5026276
 0.5020018  0.5015365  0.50113595 0.50075823 0.5003265  0.4998598
 0.49940783 0.49893627 0.49835867 0.49757534 0.49648288 0.4950817
 0.49350673 0.49198234 0.49061155 0.48942706 0.4883735  0.4873505
 0.48628837 0.485173   0.4840362  0.4829462  0.4819372  0.4810007
 0.48005548 0.47910243 0.47818732 0.47734672 0.47660604 0.4758451
 0.47495398 0.47394744 0.47291878 0.4719537  0.47114402 0.47044256
 0.46979555 0.46906126 0.46822622 0.4673054  0.46638066 0.46553278
 0.46478796 0.4640511  0.46322244 0.4622825  0.46134502 0.4605429
 0.459958   0.45949465 0.45897013 0.45820868 0.45716205 0.4559971
 0.45493406 0.4541415  0.45359954 0.4531605  0.45265675 0.45196664
 0.45115864 0.45035487 0.44975504 0.449306   0.44887578 0.44832194
 0.44765475 0.44695765 0.44624183 0.4455279  0.4448041  0.44404805
 0.44326508 0.4425286  0.44193298 0.4415807  0.44135314 0.4411037
 0.44069186 0.44014582 0.43959472 0.43917996 0.43892643 0.43874598
 0.43847317 0.43798745 0.43724266 0.43646395 0.4358895  0.43562254
 0.43550926 0.43529627 0.43478698 0.43395317 0.43296453 0.4321228
 0.43153575 0.43114567 0.4307971  0.43031588 0.4296671  0.428949
 0.428348   0.42785093 0.42732397 0.42652842 0.4253554  0.42386585
 0.4223291  0.42110568 0.420126   0.41920814 0.41817722 0.416888
 0.41538748 0.41381863 0.41234964 0.41104513 0.4098527  0.40867755
 0.40747362 0.40626854 0.40513346 0.40414873 0.4033805  0.40279233
 0.4021579  0.40140387 0.400595   0.39983642 0.39917517 0.3984518
 0.39751709 0.39625984 0.39484426 0.3934416  0.39221096 0.391223
 0.39041272 0.38965    0.38885552 0.38787448 0.38674906 0.38565233
 0.38463306 0.38374385 0.3829285  0.38213307 0.38133076 0.38056552
 0.37985364 0.37926418 0.37879497 0.37841555 0.37811816 0.3778305
 0.37750396 0.377054   0.37651455 0.3760739  0.37573513 0.3755828
 0.37556884 0.3755828  0.37544194 0.37503833 0.3744979  0.37396938
 0.37360686 0.37352434 0.37357974 0.3736134  0.3734438  0.3730924
 0.37265643 0.37228563 0.37204242 0.3719022  0.37169033 0.37133247
 0.3708118  0.37021533 0.36978266 0.36962223 0.369609   0.36946908
 0.36905944 0.36836147 0.36753345 0.36693376 0.36675543 0.36693728
 0.36724466 0.36727285 0.36682773 0.36608008 0.3652758  0.36474368
 0.3645513  0.36456358 0.36448836 0.36404976 0.36316916 0.3618867
 0.36051044 0.3593865  0.35854128 0.35785258 0.35723463 0.35662058
 0.35611004 0.3556482  0.35511515 0.35443413 0.3536692  0.35290655
 0.35237694 0.3520785  0.35196948 0.3517082  0.35113347 0.35021228
 0.34918702 0.34829932 0.3477875  0.3476713  0.34773815 0.34779483
 0.3475754  0.3471453  0.34664202 0.3462346  0.34603354 0.34600174
 0.34594065 0.3457824  0.34556693 0.34535956 0.34521252 0.34513363
 0.3450283  0.34485465 0.3446232  0.34436703 0.3442739  0.34436476
 0.34463805 0.34487277 0.34492266 0.34478956 0.34450468 0.34427527
 0.34414393 0.34398136 0.3437502  0.34346575 0.34316432 0.34292507
 0.34288314 0.34304944 0.34327686 0.34334767 0.3431461  0.3427645
 0.3423816  0.34221888 0.34231997 0.34254795 0.3427195  0.34266123
 0.34235394 0.3419442  0.3415511  0.34135944 0.34125265 0.34117287
 0.3410096  0.3407072  0.34027445 0.33977538 0.3392501  0.33879784
 0.3383793  0.3380755  0.33791393 0.33793655 0.33814135 0.3383767
 0.33852947 0.3385532  0.33844733 0.3382896  0.3381868  0.33825836
 0.33851835 0.3388092  0.338915   0.33867535 0.33819556 0.33751625
 0.33683348 0.33622757 0.33567926 0.33508095 0.3343638  0.33366415
 0.33301952 0.3324494  0.33188874 0.33125645 0.33053228 0.32975337
 0.32906237 0.32861182 0.32831335 0.3280484  0.3277479  0.32726607
 0.32663712 0.32597545 0.3254532  0.32516283 0.32495502 0.3247491
 0.3244469  0.32413706 0.323902   0.32375643 0.3237031  0.32360053
 0.32342154 0.32319096 0.32295465 0.32282412 0.32281247 0.32287726
 0.32292148 0.32281837 0.32252803 0.32220832 0.32207072 0.32221836
 0.32259485 0.32295662 0.32312405 0.32301295 0.32274902 0.3225474
 0.3225478  0.32265997 0.32276738 0.3227343  0.32250625 0.3220959
 0.3215887  0.32123837 0.32104784 0.32088855 0.32067415 0.32040656
 0.32003868 0.31970417 0.31946033 0.31927237 0.3191054  0.31884974
 0.3185113  0.3181357  0.31784573 0.3176483  0.3175587  0.31752822
 0.31741306 0.31711313 0.3166055  0.31600204 0.3155255  0.31519166
 0.31501886 0.31492126 0.31478995 0.31455556 0.3142101  0.31381688
 0.31345907 0.31314892 0.312837   0.3124812  0.31212986 0.31178463
 0.31147084 0.31109837 0.310623   0.30990192 0.3089058  0.30765802
 0.30628204 0.30503866 0.30399445 0.30316725 0.30248392 0.30180365
 0.30098602 0.30005166 0.29904255 0.29813632 0.29737613 0.29674748
 0.29620028 0.2957409  0.2952823  0.29468998 0.2939502  0.29309222
 0.29221624 0.29133877 0.29050335 0.2898028  0.28917035 0.28863022
 0.28804982 0.28740963 0.28678805 0.2862332  0.28584003 0.28553894
 0.28526258 0.28495944 0.28460252 0.28423268 0.2839144  0.28364483
 0.283348   0.2829318  0.2825346  0.2821602  0.28184766 0.28165364
 0.28155324 0.2814984  0.28136185 0.2811033  0.2807865  0.2804731
 0.28019774 0.2800123  0.27989432 0.27981564 0.2797494  0.27963307
 0.2794275  0.27917737 0.27903354 0.27899495 0.2790139  0.27900025
 0.27887368 0.27863404 0.27829555 0.27798277 0.27773556 0.27753085
 0.27734634 0.277191   0.2770308  0.2768575  0.27664134 0.27646193
 0.27628297 0.27610424 0.27590215 0.27567732 0.2753331  0.27501193
 0.27476582 0.2746573  0.27455828 0.2743602  0.27406266 0.27373648
 0.27349865 0.2733551  0.2732676  0.27311656 0.27285662 0.27254838
 0.27229562 0.27216694 0.27205655 0.27181387 0.27125484 0.27018687
 0.26867768 0.26714298 0.2658394  0.26484725 0.26404208 0.2632725
 0.2624329  0.26150095 0.26055261 0.259665   0.25886875 0.2581944
 0.25757062 0.25691664 0.256289   0.2556758  0.25507236 0.25450394
 0.25393125 0.25329587 0.252683   0.25208226 0.25160196 0.25129268
 0.25111392 0.25105706 0.2509792  0.25078517 0.2505493  0.25031564
 0.250032   0.24974905 0.24957173 0.24943325 0.24937071 0.24934077
 0.24935949 0.24923632 0.24903427 0.24890126 0.24888492 0.24896474
 0.24912201 0.24940729 0.24977121 0.25024113 0.25082418 0.25148302
 0.2520159  0.2522249  0.25206953 0.25176197 0.25149256 0.25137755
 0.2513856  0.25147182 0.25157085 0.25142783 0.2512572  0.2511164
 0.2510977  0.25122824 0.2513428  0.25143757 0.2512785  0.2509197
 0.2504657  0.2502006  0.25012353 0.2500685  0.25006786 0.24984261
 0.24946392 0.24899626 0.24863772 0.24851367 0.24870998 0.24893087
 0.2491154  0.24907817 0.2487698  0.24850659 0.24850126 0.24872817
 0.24901216 0.24929109 0.24930136 0.249043   0.24879825 0.24874684
 0.24891768 0.24924818 0.24944909 0.24914546 0.24822317 0.24689965
 0.24549823 0.24427941 0.24328282 0.24236822 0.24149892 0.24061635
 0.23977645 0.239069   0.23876552 0.2386025  0.23838201 0.23789123
 0.23715504 0.2363802  0.23578961 0.23545566 0.23540035 0.2352702
 0.23487656 0.2341219  0.23304966 0.23210686 0.23143558 0.23107253
 0.23097286 0.23089717 0.23073302 0.2302318  0.22946782 0.22864239
 0.22817735 0.22799596 0.22803125 0.22799648 0.2275418  0.22659653
 0.22535959 0.22446452 0.22429647 0.22487752 0.22527355 0.22484091
 0.22345842 0.22201954 0.22182263 0.22306122 0.22368445 0.21950749]
