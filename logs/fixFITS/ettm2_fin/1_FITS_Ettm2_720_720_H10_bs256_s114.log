Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  58060800.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.390048503875732
Epoch: 1, Steps: 64 | Train Loss: 0.6681707 Vali Loss: 0.3221940 Test Loss: 0.4434964
Validation loss decreased (inf --> 0.322194).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.148473262786865
Epoch: 2, Steps: 64 | Train Loss: 0.5652093 Vali Loss: 0.2945668 Test Loss: 0.4100109
Validation loss decreased (0.322194 --> 0.294567).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.737485408782959
Epoch: 3, Steps: 64 | Train Loss: 0.5387556 Vali Loss: 0.2841958 Test Loss: 0.3984237
Validation loss decreased (0.294567 --> 0.284196).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.272475242614746
Epoch: 4, Steps: 64 | Train Loss: 0.5272359 Vali Loss: 0.2784052 Test Loss: 0.3925107
Validation loss decreased (0.284196 --> 0.278405).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.489898681640625
Epoch: 5, Steps: 64 | Train Loss: 0.5210732 Vali Loss: 0.2746423 Test Loss: 0.3886520
Validation loss decreased (0.278405 --> 0.274642).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.29166865348816
Epoch: 6, Steps: 64 | Train Loss: 0.5160195 Vali Loss: 0.2722252 Test Loss: 0.3858495
Validation loss decreased (0.274642 --> 0.272225).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.95717167854309
Epoch: 7, Steps: 64 | Train Loss: 0.5133069 Vali Loss: 0.2706955 Test Loss: 0.3838015
Validation loss decreased (0.272225 --> 0.270696).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.405304193496704
Epoch: 8, Steps: 64 | Train Loss: 0.5110309 Vali Loss: 0.2695762 Test Loss: 0.3820515
Validation loss decreased (0.270696 --> 0.269576).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.07521653175354
Epoch: 9, Steps: 64 | Train Loss: 0.5086325 Vali Loss: 0.2684822 Test Loss: 0.3806668
Validation loss decreased (0.269576 --> 0.268482).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.451580047607422
Epoch: 10, Steps: 64 | Train Loss: 0.5077995 Vali Loss: 0.2679620 Test Loss: 0.3795961
Validation loss decreased (0.268482 --> 0.267962).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.354281425476074
Epoch: 11, Steps: 64 | Train Loss: 0.5071105 Vali Loss: 0.2670086 Test Loss: 0.3786605
Validation loss decreased (0.267962 --> 0.267009).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.893285512924194
Epoch: 12, Steps: 64 | Train Loss: 0.5055822 Vali Loss: 0.2665151 Test Loss: 0.3778377
Validation loss decreased (0.267009 --> 0.266515).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.205085754394531
Epoch: 13, Steps: 64 | Train Loss: 0.5046168 Vali Loss: 0.2663039 Test Loss: 0.3771608
Validation loss decreased (0.266515 --> 0.266304).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.243478536605835
Epoch: 14, Steps: 64 | Train Loss: 0.5030859 Vali Loss: 0.2658627 Test Loss: 0.3765281
Validation loss decreased (0.266304 --> 0.265863).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.485384702682495
Epoch: 15, Steps: 64 | Train Loss: 0.5026755 Vali Loss: 0.2652891 Test Loss: 0.3760232
Validation loss decreased (0.265863 --> 0.265289).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.506738185882568
Epoch: 16, Steps: 64 | Train Loss: 0.5024729 Vali Loss: 0.2651408 Test Loss: 0.3755621
Validation loss decreased (0.265289 --> 0.265141).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.581368923187256
Epoch: 17, Steps: 64 | Train Loss: 0.5022269 Vali Loss: 0.2648822 Test Loss: 0.3751413
Validation loss decreased (0.265141 --> 0.264882).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.957903146743774
Epoch: 18, Steps: 64 | Train Loss: 0.5015007 Vali Loss: 0.2646689 Test Loss: 0.3747518
Validation loss decreased (0.264882 --> 0.264669).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.382329225540161
Epoch: 19, Steps: 64 | Train Loss: 0.5012807 Vali Loss: 0.2646067 Test Loss: 0.3744504
Validation loss decreased (0.264669 --> 0.264607).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.964653253555298
Epoch: 20, Steps: 64 | Train Loss: 0.5014344 Vali Loss: 0.2641025 Test Loss: 0.3741411
Validation loss decreased (0.264607 --> 0.264103).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.974896669387817
Epoch: 21, Steps: 64 | Train Loss: 0.5008399 Vali Loss: 0.2642784 Test Loss: 0.3738372
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.431891202926636
Epoch: 22, Steps: 64 | Train Loss: 0.5008931 Vali Loss: 0.2638988 Test Loss: 0.3736389
Validation loss decreased (0.264103 --> 0.263899).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.986347675323486
Epoch: 23, Steps: 64 | Train Loss: 0.5003461 Vali Loss: 0.2639010 Test Loss: 0.3734249
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.586288928985596
Epoch: 24, Steps: 64 | Train Loss: 0.5002277 Vali Loss: 0.2636318 Test Loss: 0.3732443
Validation loss decreased (0.263899 --> 0.263632).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 9.879799842834473
Epoch: 25, Steps: 64 | Train Loss: 0.5002988 Vali Loss: 0.2638378 Test Loss: 0.3730188
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 9.448147773742676
Epoch: 26, Steps: 64 | Train Loss: 0.4994492 Vali Loss: 0.2636310 Test Loss: 0.3728877
Validation loss decreased (0.263632 --> 0.263631).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 9.972717046737671
Epoch: 27, Steps: 64 | Train Loss: 0.5000435 Vali Loss: 0.2631911 Test Loss: 0.3727227
Validation loss decreased (0.263631 --> 0.263191).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.356585264205933
Epoch: 28, Steps: 64 | Train Loss: 0.4991435 Vali Loss: 0.2633986 Test Loss: 0.3725543
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.875197410583496
Epoch: 29, Steps: 64 | Train Loss: 0.4982189 Vali Loss: 0.2629548 Test Loss: 0.3724125
Validation loss decreased (0.263191 --> 0.262955).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 10.109338283538818
Epoch: 30, Steps: 64 | Train Loss: 0.4990016 Vali Loss: 0.2632654 Test Loss: 0.3722793
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 9.036321640014648
Epoch: 31, Steps: 64 | Train Loss: 0.4976083 Vali Loss: 0.2632963 Test Loss: 0.3722116
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 10.061619997024536
Epoch: 32, Steps: 64 | Train Loss: 0.4977314 Vali Loss: 0.2628534 Test Loss: 0.3720862
Validation loss decreased (0.262955 --> 0.262853).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.939982652664185
Epoch: 33, Steps: 64 | Train Loss: 0.4983879 Vali Loss: 0.2629139 Test Loss: 0.3719721
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 7.408189535140991
Epoch: 34, Steps: 64 | Train Loss: 0.4984221 Vali Loss: 0.2629781 Test Loss: 0.3719017
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 10.47776484489441
Epoch: 35, Steps: 64 | Train Loss: 0.4993375 Vali Loss: 0.2628980 Test Loss: 0.3718211
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3517400026321411, mae:0.3796738386154175, rse:0.47671109437942505, corr:[0.52440375 0.5369103  0.5412653  0.53865147 0.53584915 0.5353852
 0.5368795  0.5390576  0.54038614 0.5400689  0.53873354 0.53757286
 0.53726315 0.5377695  0.53860736 0.53909284 0.53874165 0.53763247
 0.5363196  0.53533566 0.53490233 0.534896   0.5350185  0.53493583
 0.53444386 0.5336438  0.532804   0.5321411  0.53169763 0.5313973
 0.5310983  0.5306467  0.5299654  0.52913207 0.5283136  0.5276035
 0.5269959  0.5264626  0.52595234 0.52542394 0.5248422  0.52421415
 0.5235809  0.5229378  0.5222683  0.52158445 0.5208972  0.52019185
 0.51945347 0.5186682  0.51789105 0.5171522  0.5164581  0.51586986
 0.51541686 0.5150311  0.51464653 0.5142359  0.51380044 0.5133434
 0.51296437 0.51269156 0.51250565 0.51232845 0.51206875 0.51171345
 0.5112174  0.5106748  0.51017064 0.509787   0.5095216  0.50929284
 0.50901884 0.5086075  0.5080375  0.50734276 0.5066335  0.5059899
 0.5054897  0.5050125  0.504463   0.50378233 0.5029921  0.502149
 0.5013608  0.5007099  0.5002167  0.49985966 0.4995034  0.49906603
 0.49847674 0.4976725  0.49670872 0.49564818 0.4945014  0.49328
 0.49198756 0.49063793 0.48923826 0.48785564 0.48659548 0.4855432
 0.48466632 0.48385775 0.48298797 0.4819992  0.48091143 0.47980556
 0.47877082 0.47787434 0.47710177 0.47635302 0.47554117 0.47457957
 0.47348785 0.4723781  0.47138643 0.47057372 0.46989504 0.46922767
 0.46845987 0.46750486 0.4664058  0.46524867 0.46417612 0.46329135
 0.4625885  0.46192986 0.46115482 0.460166   0.45900753 0.45781124
 0.45676726 0.45597324 0.45540988 0.45491412 0.45429537 0.45346996
 0.4524475  0.45135996 0.45037684 0.449609   0.44903636 0.4485042
 0.44786704 0.44704118 0.44612154 0.44523996 0.4445638  0.4441504
 0.44391784 0.4436394  0.44308287 0.44222048 0.4411612  0.44012806
 0.43934363 0.43889186 0.43868196 0.43852556 0.43819395 0.43759573
 0.43679798 0.43601117 0.4354357  0.43514428 0.4350575  0.4349903
 0.43474337 0.4342161  0.43341494 0.4325546  0.43182054 0.43134123
 0.4310575  0.4308033  0.4303776  0.42967457 0.4287525  0.4277988
 0.4270096  0.42651957 0.4262937  0.42613024 0.4258167  0.4251944
 0.42431465 0.42323324 0.4221239  0.42107427 0.4201011  0.41909015
 0.41794485 0.41669258 0.4152752  0.41379464 0.41240925 0.41121224
 0.4102053  0.4092938  0.40833575 0.40726274 0.40608913 0.40489244
 0.40376928 0.40275997 0.40180656 0.4008268  0.3997983  0.3987626
 0.3977081  0.39669833 0.39580384 0.39504707 0.39437103 0.3936132
 0.39266598 0.39147922 0.39018562 0.38892785 0.3878159  0.38687664
 0.38605702 0.38526005 0.3844272  0.3834618  0.38239902 0.381395
 0.38051024 0.37979582 0.37917227 0.37854344 0.37784392 0.3771004
 0.37637228 0.3757946  0.37542152 0.3751992  0.37499472 0.37464386
 0.37406704 0.3732833  0.37247425 0.37192738 0.37171927 0.3718209
 0.3720277  0.3721041  0.3718842  0.3713811  0.3708029  0.37037283
 0.3702408  0.37040037 0.37063468 0.37071052 0.37046397 0.36992732
 0.36926112 0.36868536 0.3683558  0.36829326 0.36833006 0.36829978
 0.36803353 0.36747476 0.36677086 0.36613458 0.36571372 0.3655294
 0.36552706 0.36551353 0.36530447 0.36488226 0.36432365 0.36378166
 0.36341456 0.36321002 0.36306384 0.3628994  0.3625846  0.36212534
 0.36157978 0.36106414 0.36063498 0.36022797 0.3597265  0.3589555
 0.35791996 0.3567625  0.3556251  0.35462317 0.35383475 0.3532068
 0.352673   0.3520737  0.35131016 0.3504098  0.3495232  0.3487655
 0.34825915 0.34789425 0.34758654 0.3471349  0.34650627 0.345727
 0.34498492 0.34438774 0.34400755 0.34383136 0.34373957 0.3436469
 0.34338447 0.3429658  0.3424334  0.34189022 0.34146324 0.34119883
 0.34100735 0.34080893 0.34053496 0.3401572  0.33972442 0.33935264
 0.33911186 0.33905002 0.33908543 0.33907008 0.33896098 0.33875313
 0.3385612  0.33845413 0.33848536 0.33862752 0.33872902 0.33870125
 0.33846453 0.3380073  0.3375112  0.33718005 0.33711195 0.3372458
 0.3374462  0.3375292  0.3374104  0.33707675 0.33666724 0.33638167
 0.33631787 0.33645457 0.3366144  0.33659878 0.33629647 0.33572403
 0.33505267 0.33451933 0.3342484  0.33428273 0.334419   0.33447582
 0.33429784 0.33386067 0.33328423 0.33276764 0.33246914 0.3324723
 0.33264875 0.33286333 0.33294275 0.33285117 0.33266369 0.33248547
 0.33244202 0.33258125 0.33280718 0.3329686  0.33294165 0.33273506
 0.332448   0.332163   0.33191544 0.3316435  0.33132237 0.33079395
 0.33004418 0.3291119  0.32811648 0.3271832  0.32638234 0.32580858
 0.3253599  0.32491705 0.32437342 0.32368094 0.32289267 0.32208374
 0.3213654  0.3208398  0.32043135 0.32005167 0.31967422 0.3191869
 0.31858337 0.31788895 0.3172215  0.31670702 0.31630558 0.31601906
 0.31578314 0.3155935  0.31542894 0.3152624  0.31511062 0.31492704
 0.31475464 0.31461006 0.31442443 0.31419307 0.31392404 0.31369397
 0.31358355 0.31360304 0.31369174 0.31379288 0.3138768  0.3139185
 0.31393778 0.31393534 0.31394625 0.31394753 0.31392378 0.31384236
 0.3136795  0.31341475 0.313145   0.3129724  0.31295955 0.313038
 0.31306124 0.31296933 0.31267944 0.31218672 0.31161174 0.31114742
 0.31083664 0.31070858 0.31065744 0.31053105 0.31027403 0.30988377
 0.30948788 0.3091979  0.3091068  0.30913925 0.30919796 0.30917972
 0.3089834  0.3086076  0.30813706 0.3077141  0.30747864 0.30739185
 0.3074024  0.30738413 0.3072611  0.30702084 0.3067236  0.30645975
 0.30631045 0.30625916 0.30621642 0.3060729  0.30580574 0.30539444
 0.30490488 0.30434006 0.30375355 0.30308762 0.30230677 0.30132988
 0.30014887 0.2989092  0.29769677 0.29661828 0.29572302 0.29496673
 0.2942356  0.2934801  0.29264286 0.29181015 0.2910097  0.2902708
 0.28959382 0.28902245 0.28849518 0.28791425 0.28726247 0.28654557
 0.28585073 0.2851525  0.2844569  0.28383213 0.28323704 0.2827104
 0.2821674  0.28158215 0.2810043  0.2804565  0.28001815 0.2796196
 0.27923906 0.2788363  0.2783828  0.27790025 0.2774539  0.27708682
 0.27679032 0.27650723 0.27628967 0.2760589  0.2757667  0.27542207
 0.27506542 0.2747477  0.27448276 0.27427232 0.27410322 0.2739232
 0.27365562 0.27331918 0.27297154 0.27270323 0.27259514 0.27260843
 0.27264145 0.27260244 0.27252424 0.27238405 0.27225474 0.27219862
 0.27224845 0.27238572 0.27247787 0.2724503  0.27222133 0.27182627
 0.2713728  0.2710444  0.27091536 0.27095065 0.27099317 0.2709552
 0.27074745 0.27043894 0.27015394 0.27003908 0.27004185 0.27015436
 0.27025777 0.27028394 0.27013505 0.2698111  0.2694462  0.26918593
 0.26912078 0.26920414 0.2693342  0.26937667 0.26926988 0.26902905
 0.26872438 0.2683978  0.26799977 0.2674714  0.2667228  0.2656282
 0.26422405 0.26279432 0.2615158  0.2604865  0.25965488 0.2589281
 0.2581959  0.2573868  0.25647783 0.25548223 0.2544665  0.25353488
 0.25272235 0.2520003  0.25141782 0.25092068 0.25046083 0.2500259
 0.24957858 0.24907216 0.24858622 0.24811105 0.24773219 0.24749136
 0.2473476  0.24726501 0.24711545 0.24681942 0.24645339 0.24608049
 0.24571012 0.24540009 0.24522653 0.24509063 0.24498098 0.24486436
 0.24479976 0.24468245 0.24459697 0.24464808 0.24482395 0.2450525
 0.2452774  0.24551427 0.24574798 0.24599452 0.24628758 0.24663007
 0.2469214  0.24704726 0.24698019 0.24687307 0.24683824 0.24692887
 0.24709171 0.24729085 0.24751957 0.24760498 0.2477026  0.2478042
 0.24794233 0.24811733 0.24821657 0.24828607 0.24818258 0.24799976
 0.2477899  0.24774356 0.24781612 0.24784896 0.24786894 0.24767347
 0.24735051 0.24696681 0.24667597 0.24658033 0.24675268 0.24695297
 0.24716274 0.24723622 0.24711591 0.24702758 0.24712633 0.24741596
 0.2478104  0.24829763 0.24865763 0.24881667 0.24890025 0.24896663
 0.24906911 0.24928558 0.24949723 0.24942596 0.24886575 0.2478123
 0.24640407 0.24488023 0.24345517 0.24223737 0.2413294  0.24064371
 0.24006099 0.23946719 0.23901494 0.23854592 0.23802699 0.23737659
 0.2366373  0.23592027 0.23534755 0.23496489 0.23484604 0.23474179
 0.23449479 0.23397815 0.23314975 0.23235866 0.23173702 0.23138697
 0.23132053 0.23131242 0.23122798 0.23079757 0.23005117 0.22912723
 0.22843596 0.22797    0.22780913 0.22786556 0.22792302 0.22782588
 0.22739084 0.22675034 0.22610076 0.2259065  0.22603637 0.22654255
 0.22719388 0.2276363  0.22750258 0.22659832 0.22464243 0.22166824]
