Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21288960.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3899705
	speed: 0.1225s/iter; left time: 784.2660s
Epoch: 1 cost time: 15.593648195266724
Epoch: 1, Steps: 130 | Train Loss: 0.5052473 Vali Loss: 0.2242997 Test Loss: 0.3045238
Validation loss decreased (inf --> 0.224300).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4261136
	speed: 0.3533s/iter; left time: 2215.6911s
Epoch: 2 cost time: 17.426320552825928
Epoch: 2, Steps: 130 | Train Loss: 0.4216676 Vali Loss: 0.2099502 Test Loss: 0.2875898
Validation loss decreased (0.224300 --> 0.209950).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5192937
	speed: 0.3497s/iter; left time: 2147.8143s
Epoch: 3 cost time: 17.977596521377563
Epoch: 3, Steps: 130 | Train Loss: 0.4055050 Vali Loss: 0.2052213 Test Loss: 0.2816302
Validation loss decreased (0.209950 --> 0.205221).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5019761
	speed: 0.3534s/iter; left time: 2124.2315s
Epoch: 4 cost time: 16.68898582458496
Epoch: 4, Steps: 130 | Train Loss: 0.3971763 Vali Loss: 0.2019614 Test Loss: 0.2781414
Validation loss decreased (0.205221 --> 0.201961).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3467788
	speed: 0.3369s/iter; left time: 1981.2829s
Epoch: 5 cost time: 15.872206926345825
Epoch: 5, Steps: 130 | Train Loss: 0.3934924 Vali Loss: 0.1999560 Test Loss: 0.2759152
Validation loss decreased (0.201961 --> 0.199956).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3316744
	speed: 0.3175s/iter; left time: 1825.8708s
Epoch: 6 cost time: 15.90461254119873
Epoch: 6, Steps: 130 | Train Loss: 0.3893545 Vali Loss: 0.1990080 Test Loss: 0.2746962
Validation loss decreased (0.199956 --> 0.199008).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3817227
	speed: 0.3263s/iter; left time: 1833.9288s
Epoch: 7 cost time: 17.248581886291504
Epoch: 7, Steps: 130 | Train Loss: 0.3872114 Vali Loss: 0.1979778 Test Loss: 0.2737405
Validation loss decreased (0.199008 --> 0.197978).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3942077
	speed: 0.3463s/iter; left time: 1901.3945s
Epoch: 8 cost time: 16.470641136169434
Epoch: 8, Steps: 130 | Train Loss: 0.3858342 Vali Loss: 0.1971403 Test Loss: 0.2725725
Validation loss decreased (0.197978 --> 0.197140).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4482314
	speed: 0.3265s/iter; left time: 1750.4787s
Epoch: 9 cost time: 15.247139930725098
Epoch: 9, Steps: 130 | Train Loss: 0.3840468 Vali Loss: 0.1959839 Test Loss: 0.2719646
Validation loss decreased (0.197140 --> 0.195984).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4095620
	speed: 0.3169s/iter; left time: 1657.9282s
Epoch: 10 cost time: 15.643742561340332
Epoch: 10, Steps: 130 | Train Loss: 0.3827726 Vali Loss: 0.1959551 Test Loss: 0.2715180
Validation loss decreased (0.195984 --> 0.195955).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4300961
	speed: 0.3215s/iter; left time: 1640.1318s
Epoch: 11 cost time: 16.401925563812256
Epoch: 11, Steps: 130 | Train Loss: 0.3819085 Vali Loss: 0.1959974 Test Loss: 0.2710934
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4808269
	speed: 0.3297s/iter; left time: 1639.1765s
Epoch: 12 cost time: 14.97397494316101
Epoch: 12, Steps: 130 | Train Loss: 0.3810634 Vali Loss: 0.1954028 Test Loss: 0.2708149
Validation loss decreased (0.195955 --> 0.195403).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3075265
	speed: 0.3175s/iter; left time: 1536.9593s
Epoch: 13 cost time: 15.144283533096313
Epoch: 13, Steps: 130 | Train Loss: 0.3808989 Vali Loss: 0.1952269 Test Loss: 0.2705709
Validation loss decreased (0.195403 --> 0.195227).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3125936
	speed: 0.3179s/iter; left time: 1497.4963s
Epoch: 14 cost time: 15.012436389923096
Epoch: 14, Steps: 130 | Train Loss: 0.3808124 Vali Loss: 0.1951667 Test Loss: 0.2703645
Validation loss decreased (0.195227 --> 0.195167).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4985681
	speed: 0.3361s/iter; left time: 1539.6007s
Epoch: 15 cost time: 16.565637350082397
Epoch: 15, Steps: 130 | Train Loss: 0.3803332 Vali Loss: 0.1950479 Test Loss: 0.2702729
Validation loss decreased (0.195167 --> 0.195048).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3641061
	speed: 0.3347s/iter; left time: 1489.9240s
Epoch: 16 cost time: 16.203531742095947
Epoch: 16, Steps: 130 | Train Loss: 0.3788337 Vali Loss: 0.1948195 Test Loss: 0.2699534
Validation loss decreased (0.195048 --> 0.194819).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4906793
	speed: 0.3264s/iter; left time: 1410.2494s
Epoch: 17 cost time: 16.44633913040161
Epoch: 17, Steps: 130 | Train Loss: 0.3792365 Vali Loss: 0.1946073 Test Loss: 0.2699288
Validation loss decreased (0.194819 --> 0.194607).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2907745
	speed: 0.3162s/iter; left time: 1325.1634s
Epoch: 18 cost time: 15.424033403396606
Epoch: 18, Steps: 130 | Train Loss: 0.3790860 Vali Loss: 0.1949729 Test Loss: 0.2696837
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2936670
	speed: 0.3301s/iter; left time: 1340.5022s
Epoch: 19 cost time: 16.003741025924683
Epoch: 19, Steps: 130 | Train Loss: 0.3782314 Vali Loss: 0.1944534 Test Loss: 0.2696075
Validation loss decreased (0.194607 --> 0.194453).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3652046
	speed: 0.2697s/iter; left time: 1060.3339s
Epoch: 20 cost time: 11.284698009490967
Epoch: 20, Steps: 130 | Train Loss: 0.3774630 Vali Loss: 0.1941430 Test Loss: 0.2694671
Validation loss decreased (0.194453 --> 0.194143).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5193526
	speed: 0.2945s/iter; left time: 1119.4493s
Epoch: 21 cost time: 14.707170009613037
Epoch: 21, Steps: 130 | Train Loss: 0.3778107 Vali Loss: 0.1941446 Test Loss: 0.2693650
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3120284
	speed: 0.3308s/iter; left time: 1214.3509s
Epoch: 22 cost time: 17.02750253677368
Epoch: 22, Steps: 130 | Train Loss: 0.3778846 Vali Loss: 0.1944423 Test Loss: 0.2693894
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3475742
	speed: 0.3680s/iter; left time: 1303.1259s
Epoch: 23 cost time: 17.454381227493286
Epoch: 23, Steps: 130 | Train Loss: 0.3781898 Vali Loss: 0.1939662 Test Loss: 0.2693068
Validation loss decreased (0.194143 --> 0.193966).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4097598
	speed: 0.3482s/iter; left time: 1187.8038s
Epoch: 24 cost time: 16.61420226097107
Epoch: 24, Steps: 130 | Train Loss: 0.3781866 Vali Loss: 0.1941394 Test Loss: 0.2691770
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3558780
	speed: 0.2654s/iter; left time: 870.6578s
Epoch: 25 cost time: 14.474485635757446
Epoch: 25, Steps: 130 | Train Loss: 0.3774644 Vali Loss: 0.1939733 Test Loss: 0.2691917
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4058280
	speed: 0.3269s/iter; left time: 1030.0806s
Epoch: 26 cost time: 16.37804412841797
Epoch: 26, Steps: 130 | Train Loss: 0.3770903 Vali Loss: 0.1941206 Test Loss: 0.2690881
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2699882388114929, mae:0.32717835903167725, rse:0.41969430446624756, corr:[0.54398054 0.5544707  0.5561495  0.5532947  0.55114275 0.5510851
 0.5526115  0.5543952  0.5551012  0.55445087 0.55326176 0.5524454
 0.552385   0.5529246  0.5535886  0.55375844 0.55317676 0.5521125
 0.55104226 0.5503009  0.5499899  0.54996425 0.5499391  0.54964995
 0.54901886 0.54820955 0.5474525  0.54687047 0.5464695  0.54617196
 0.54582876 0.54536384 0.54475397 0.54404634 0.54330647 0.54261714
 0.5420052  0.54148495 0.5409943  0.5404612  0.5398788  0.53926075
 0.5386226  0.5379676  0.5373316  0.5367384  0.5361746  0.53558713
 0.5349224  0.53417206 0.53337026 0.5325976  0.5319057  0.53134996
 0.53090215 0.53050476 0.5300842  0.52961904 0.52914006 0.5287147
 0.52841806 0.5282797  0.5281974  0.52803457 0.52772087 0.5272711
 0.5267371  0.52626455 0.52597713 0.5258732  0.5258149  0.5256597
 0.5253289  0.5248349  0.5242391  0.5236749  0.5232239  0.5228695
 0.5225011  0.52202564 0.52135396 0.5205268  0.51964605 0.51887935
 0.51835454 0.51804775 0.5178014  0.5174795  0.5170184  0.5164414
 0.51582676 0.51522624 0.5146389  0.513986   0.51313525 0.5119643
 0.51049006 0.50887364 0.5072943  0.50596833 0.5050022  0.5042993
 0.5036211  0.5027224  0.50152254 0.5000616  0.49860138 0.49744383
 0.49672422 0.4963145  0.4959329  0.49531028 0.49435145 0.49307054
 0.49165365 0.4904312  0.48962528 0.48914263 0.48873118 0.4880828
 0.48710257 0.48582554 0.48453715 0.48352    0.4829075  0.48259425
 0.48230648 0.4817361  0.48071277 0.47929844 0.47777212 0.47650275
 0.47575244 0.47545296 0.47528934 0.47489238 0.47404602 0.47281298
 0.47145385 0.4703711  0.4697886  0.46965018 0.4696466  0.4693835
 0.46867514 0.46753287 0.4663032  0.46531928 0.46478483 0.46457577
 0.46439826 0.46394283 0.46299914 0.4616714  0.4603223  0.45939997
 0.45909023 0.45923564 0.4594572  0.459439   0.45899597 0.45821565
 0.45736873 0.45680502 0.45667392 0.4568361  0.4569262  0.45664507
 0.45593837 0.45501825 0.45416036 0.4536791  0.45367026 0.45392698
 0.45408446 0.4538313  0.45304808 0.45189998 0.45070258 0.4498726
 0.4495053  0.4494274  0.4493689  0.4490462  0.448334   0.4473527
 0.44643185 0.4458417  0.44557953 0.44536003 0.4448147  0.44368568
 0.44200507 0.44011676 0.4383239  0.43687198 0.43585682 0.43505412
 0.4341796  0.43300322 0.4315258  0.42996398 0.42865568 0.4278146
 0.4273864  0.42705265 0.42641062 0.42528933 0.42385456 0.42251226
 0.42153007 0.42104954 0.4209223  0.4207895  0.42032203 0.41928485
 0.41774622 0.41599354 0.414511   0.4135553  0.4130016  0.41253123
 0.41179746 0.41065577 0.40926963 0.40785167 0.40673637 0.4061086
 0.40573016 0.40531978 0.40452075 0.40328538 0.40180397 0.40051275
 0.3997702  0.39972186 0.400067   0.4003608  0.400223   0.3995058
 0.39843008 0.39736938 0.39679465 0.39693975 0.39742267 0.39776447
 0.39756477 0.39673087 0.39548674 0.39434332 0.39391914 0.3943292
 0.39518577 0.395936   0.395999   0.39528635 0.39411837 0.39315987
 0.392951   0.39353514 0.3944633  0.39509228 0.3949167  0.39391714
 0.39248377 0.39125332 0.3908651  0.39140064 0.39233664 0.39293852
 0.39271522 0.39168128 0.39019734 0.38898456 0.38854575 0.38891387
 0.3897402  0.39029944 0.3901417  0.38934264 0.3883018  0.38761717
 0.38757604 0.3880294  0.3884475  0.38824764 0.38714483 0.3852341
 0.38322413 0.38205576 0.38195902 0.38247046 0.38285208 0.38245082
 0.38122454 0.3795127  0.37799045 0.3772818  0.37757927 0.3781839
 0.37842736 0.37772423 0.37616146 0.3742089  0.3728405  0.37273112
 0.37377563 0.37495714 0.37536266 0.3745365  0.37270436 0.37101412
 0.37043312 0.37148467 0.37345725 0.37503085 0.37524113 0.37391165
 0.37162727 0.36986336 0.36992162 0.37179267 0.3740852  0.3753225
 0.37461737 0.37222016 0.3697521  0.36931056 0.37194172 0.3759379
 0.3789752  0.3792011  0.3759399  0.3713432  0.37152094 0.3777304 ]
