Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=122, out_features=154, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16834048.0
params:  18942.0
Trainable parameters:  18942
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2626046
	speed: 0.1377s/iter; left time: 1789.7020s
	iters: 200, epoch: 1 | loss: 0.3026484
	speed: 0.1289s/iter; left time: 1663.3614s
Epoch: 1 cost time: 35.08326172828674
Epoch: 1, Steps: 262 | Train Loss: 0.3811791 Vali Loss: 0.1700801 Test Loss: 0.2360470
Validation loss decreased (inf --> 0.170080).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2775517
	speed: 0.6006s/iter; left time: 7650.8455s
	iters: 200, epoch: 2 | loss: 0.3230062
	speed: 0.1288s/iter; left time: 1627.4124s
Epoch: 2 cost time: 34.76772975921631
Epoch: 2, Steps: 262 | Train Loss: 0.3199875 Vali Loss: 0.1606404 Test Loss: 0.2267090
Validation loss decreased (0.170080 --> 0.160640).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3160864
	speed: 0.4916s/iter; left time: 6134.0348s
	iters: 200, epoch: 3 | loss: 0.4233709
	speed: 0.1259s/iter; left time: 1558.3973s
Epoch: 3 cost time: 34.7269926071167
Epoch: 3, Steps: 262 | Train Loss: 0.3072035 Vali Loss: 0.1573843 Test Loss: 0.2232119
Validation loss decreased (0.160640 --> 0.157384).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3488315
	speed: 0.6095s/iter; left time: 7445.5919s
	iters: 200, epoch: 4 | loss: 0.4474399
	speed: 0.1295s/iter; left time: 1569.4836s
Epoch: 4 cost time: 35.21522760391235
Epoch: 4, Steps: 262 | Train Loss: 0.3017411 Vali Loss: 0.1555270 Test Loss: 0.2214075
Validation loss decreased (0.157384 --> 0.155527).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3938552
	speed: 0.5467s/iter; left time: 6534.4702s
	iters: 200, epoch: 5 | loss: 0.2992575
	speed: 0.1123s/iter; left time: 1331.0494s
Epoch: 5 cost time: 31.00608229637146
Epoch: 5, Steps: 262 | Train Loss: 0.2985225 Vali Loss: 0.1543020 Test Loss: 0.2201036
Validation loss decreased (0.155527 --> 0.154302).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4513705
	speed: 0.5930s/iter; left time: 6932.8677s
	iters: 200, epoch: 6 | loss: 0.3308894
	speed: 0.1348s/iter; left time: 1562.0045s
Epoch: 6 cost time: 36.08938646316528
Epoch: 6, Steps: 262 | Train Loss: 0.2962325 Vali Loss: 0.1537056 Test Loss: 0.2196977
Validation loss decreased (0.154302 --> 0.153706).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2383678
	speed: 0.6065s/iter; left time: 6931.8196s
	iters: 200, epoch: 7 | loss: 0.2243135
	speed: 0.1162s/iter; left time: 1316.4444s
Epoch: 7 cost time: 33.18944072723389
Epoch: 7, Steps: 262 | Train Loss: 0.2947879 Vali Loss: 0.1534636 Test Loss: 0.2188432
Validation loss decreased (0.153706 --> 0.153464).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2296416
	speed: 0.5405s/iter; left time: 6035.5624s
	iters: 200, epoch: 8 | loss: 0.3169414
	speed: 0.1087s/iter; left time: 1203.0092s
Epoch: 8 cost time: 32.762200355529785
Epoch: 8, Steps: 262 | Train Loss: 0.2936535 Vali Loss: 0.1528041 Test Loss: 0.2182912
Validation loss decreased (0.153464 --> 0.152804).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4352718
	speed: 0.6210s/iter; left time: 6771.4725s
	iters: 200, epoch: 9 | loss: 0.3084109
	speed: 0.1431s/iter; left time: 1546.4087s
Epoch: 9 cost time: 38.64255928993225
Epoch: 9, Steps: 262 | Train Loss: 0.2927125 Vali Loss: 0.1527444 Test Loss: 0.2180297
Validation loss decreased (0.152804 --> 0.152744).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3153672
	speed: 0.6628s/iter; left time: 7054.6117s
	iters: 200, epoch: 10 | loss: 0.3169753
	speed: 0.1388s/iter; left time: 1463.7289s
Epoch: 10 cost time: 36.7175407409668
Epoch: 10, Steps: 262 | Train Loss: 0.2922447 Vali Loss: 0.1525799 Test Loss: 0.2180861
Validation loss decreased (0.152744 --> 0.152580).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2564350
	speed: 0.5948s/iter; left time: 6174.9207s
	iters: 200, epoch: 11 | loss: 0.2621543
	speed: 0.1099s/iter; left time: 1130.1964s
Epoch: 11 cost time: 30.620967626571655
Epoch: 11, Steps: 262 | Train Loss: 0.2917078 Vali Loss: 0.1522850 Test Loss: 0.2176423
Validation loss decreased (0.152580 --> 0.152285).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2586176
	speed: 0.5328s/iter; left time: 5391.8921s
	iters: 200, epoch: 12 | loss: 0.2612950
	speed: 0.1426s/iter; left time: 1428.8446s
Epoch: 12 cost time: 35.365535259246826
Epoch: 12, Steps: 262 | Train Loss: 0.2902466 Vali Loss: 0.1519430 Test Loss: 0.2172184
Validation loss decreased (0.152285 --> 0.151943).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3057655
	speed: 0.6480s/iter; left time: 6387.4763s
	iters: 200, epoch: 13 | loss: 0.2330183
	speed: 0.1360s/iter; left time: 1327.0510s
Epoch: 13 cost time: 35.994792461395264
Epoch: 13, Steps: 262 | Train Loss: 0.2910585 Vali Loss: 0.1518603 Test Loss: 0.2171215
Validation loss decreased (0.151943 --> 0.151860).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2607379
	speed: 0.5932s/iter; left time: 5691.5201s
	iters: 200, epoch: 14 | loss: 0.2268886
	speed: 0.1171s/iter; left time: 1111.7929s
Epoch: 14 cost time: 32.20426654815674
Epoch: 14, Steps: 262 | Train Loss: 0.2900987 Vali Loss: 0.1520578 Test Loss: 0.2172146
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3211846
	speed: 0.5963s/iter; left time: 5564.9239s
	iters: 200, epoch: 15 | loss: 0.3114041
	speed: 0.1356s/iter; left time: 1251.9034s
Epoch: 15 cost time: 35.6582305431366
Epoch: 15, Steps: 262 | Train Loss: 0.2902690 Vali Loss: 0.1519113 Test Loss: 0.2170967
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2815717
	speed: 0.5849s/iter; left time: 5305.8653s
	iters: 200, epoch: 16 | loss: 0.3969979
	speed: 0.1313s/iter; left time: 1177.9520s
Epoch: 16 cost time: 34.74049115180969
Epoch: 16, Steps: 262 | Train Loss: 0.2898518 Vali Loss: 0.1518613 Test Loss: 0.2170824
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.21770574152469635, mae:0.29189467430114746, rse:0.3776852786540985, corr:[0.5564069  0.5635207  0.5592871  0.55751324 0.5595     0.5619004
 0.56186575 0.56042665 0.5596866  0.56025994 0.5612261  0.5613233
 0.560553   0.55980974 0.5597434  0.5601077  0.5601447  0.55940723
 0.5582711  0.55735785 0.55694824 0.5567565  0.55631155 0.55550516
 0.5546569  0.55411    0.55390763 0.5537485  0.5532848  0.55251926
 0.5517304  0.55121195 0.5509207  0.55060214 0.5500631  0.54932785
 0.54854375 0.5478852  0.54737365 0.54690266 0.5463573  0.54570353
 0.54505175 0.5445247  0.5441217  0.54367584 0.54303485 0.54220134
 0.5412826  0.5404848  0.53992516 0.53953916 0.53907734 0.53838193
 0.53751177 0.53670835 0.5360769  0.53556156 0.53510046 0.53471625
 0.5344657  0.53434557 0.5342096  0.5339727  0.53363043 0.5333273
 0.53312147 0.53303283 0.53291583 0.53261024 0.5321186  0.5316278
 0.5313064  0.53113437 0.53087467 0.5303915  0.5297071  0.52897257
 0.5283317  0.52780217 0.52720857 0.526535   0.5258819  0.5253699
 0.5249801  0.524588   0.5239805  0.5232495  0.5226082  0.522192
 0.5219128  0.5214902  0.52065015 0.5195497  0.5185214  0.5177267
 0.51700264 0.5160278  0.5145519  0.5127843  0.5112586  0.5102788
 0.509675   0.5089006  0.5075668  0.50583196 0.5043041  0.5034241
 0.5029129  0.5023275  0.5014768  0.5005271  0.49990618 0.49959204
 0.49915835 0.49829474 0.4970935  0.49597618 0.49542168 0.49526313
 0.49503383 0.49419084 0.4928562  0.4915222  0.4907078  0.49047434
 0.49039325 0.48994753 0.489014   0.48790243 0.4869852  0.48624712
 0.4854223  0.48427472 0.4829921  0.4820473  0.4815632  0.48130724
 0.48080716 0.47995916 0.47911224 0.47888678 0.47928864 0.47952428
 0.47895774 0.4774143  0.4756673  0.47453457 0.4742404  0.47399747
 0.47317207 0.47182736 0.47053596 0.4698553  0.46969798 0.469474
 0.46868128 0.46762547 0.46702644 0.46743917 0.46811643 0.468256
 0.4675247  0.46660426 0.46645266 0.4671462  0.46763048 0.4671194
 0.4657494  0.46461895 0.464643   0.46555454 0.46606013 0.4654244
 0.46416017 0.46341664 0.4636329  0.46382496 0.46274942 0.4607033
 0.45908132 0.4592751  0.46048823 0.46055824 0.45873392 0.45654
 0.45674124 0.45910087 0.4599253  0.45754752 0.45484257 0.45866507]
