Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=74, out_features=93, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  24665088.0
params:  6975.0
Trainable parameters:  6975
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.708969116210938
Epoch: 1, Steps: 65 | Train Loss: 0.4678983 Vali Loss: 0.2057844 Test Loss: 0.2718257
Validation loss decreased (inf --> 0.205784).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.268579721450806
Epoch: 2, Steps: 65 | Train Loss: 0.3711489 Vali Loss: 0.1832770 Test Loss: 0.2447740
Validation loss decreased (0.205784 --> 0.183277).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.130505561828613
Epoch: 3, Steps: 65 | Train Loss: 0.3460305 Vali Loss: 0.1747771 Test Loss: 0.2359935
Validation loss decreased (0.183277 --> 0.174777).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.541666269302368
Epoch: 4, Steps: 65 | Train Loss: 0.3338051 Vali Loss: 0.1702637 Test Loss: 0.2316783
Validation loss decreased (0.174777 --> 0.170264).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.951030254364014
Epoch: 5, Steps: 65 | Train Loss: 0.3263425 Vali Loss: 0.1669485 Test Loss: 0.2287014
Validation loss decreased (0.170264 --> 0.166948).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.649686813354492
Epoch: 6, Steps: 65 | Train Loss: 0.3217104 Vali Loss: 0.1649807 Test Loss: 0.2266142
Validation loss decreased (0.166948 --> 0.164981).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.926450967788696
Epoch: 7, Steps: 65 | Train Loss: 0.3171257 Vali Loss: 0.1632377 Test Loss: 0.2249817
Validation loss decreased (0.164981 --> 0.163238).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.096523523330688
Epoch: 8, Steps: 65 | Train Loss: 0.3133798 Vali Loss: 0.1619125 Test Loss: 0.2237729
Validation loss decreased (0.163238 --> 0.161913).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.068177461624146
Epoch: 9, Steps: 65 | Train Loss: 0.3106882 Vali Loss: 0.1607808 Test Loss: 0.2225541
Validation loss decreased (0.161913 --> 0.160781).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.623788118362427
Epoch: 10, Steps: 65 | Train Loss: 0.3089487 Vali Loss: 0.1600011 Test Loss: 0.2217368
Validation loss decreased (0.160781 --> 0.160001).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.005106925964355
Epoch: 11, Steps: 65 | Train Loss: 0.3074392 Vali Loss: 0.1591927 Test Loss: 0.2211027
Validation loss decreased (0.160001 --> 0.159193).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.935606479644775
Epoch: 12, Steps: 65 | Train Loss: 0.3053929 Vali Loss: 0.1586417 Test Loss: 0.2205388
Validation loss decreased (0.159193 --> 0.158642).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.429701328277588
Epoch: 13, Steps: 65 | Train Loss: 0.3044247 Vali Loss: 0.1582450 Test Loss: 0.2200253
Validation loss decreased (0.158642 --> 0.158245).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.625198602676392
Epoch: 14, Steps: 65 | Train Loss: 0.3033924 Vali Loss: 0.1578448 Test Loss: 0.2196355
Validation loss decreased (0.158245 --> 0.157845).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.082082033157349
Epoch: 15, Steps: 65 | Train Loss: 0.3024867 Vali Loss: 0.1574304 Test Loss: 0.2192233
Validation loss decreased (0.157845 --> 0.157430).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.442371606826782
Epoch: 16, Steps: 65 | Train Loss: 0.3016506 Vali Loss: 0.1572606 Test Loss: 0.2189959
Validation loss decreased (0.157430 --> 0.157261).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.573681592941284
Epoch: 17, Steps: 65 | Train Loss: 0.3001510 Vali Loss: 0.1569597 Test Loss: 0.2187586
Validation loss decreased (0.157261 --> 0.156960).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.558376789093018
Epoch: 18, Steps: 65 | Train Loss: 0.2999367 Vali Loss: 0.1567719 Test Loss: 0.2184810
Validation loss decreased (0.156960 --> 0.156772).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 8.407248258590698
Epoch: 19, Steps: 65 | Train Loss: 0.3005692 Vali Loss: 0.1565149 Test Loss: 0.2182957
Validation loss decreased (0.156772 --> 0.156515).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 8.695724725723267
Epoch: 20, Steps: 65 | Train Loss: 0.2999706 Vali Loss: 0.1563174 Test Loss: 0.2180776
Validation loss decreased (0.156515 --> 0.156317).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.365972995758057
Epoch: 21, Steps: 65 | Train Loss: 0.2996572 Vali Loss: 0.1562793 Test Loss: 0.2179357
Validation loss decreased (0.156317 --> 0.156279).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.218049764633179
Epoch: 22, Steps: 65 | Train Loss: 0.2997698 Vali Loss: 0.1559149 Test Loss: 0.2177463
Validation loss decreased (0.156279 --> 0.155915).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.793010234832764
Epoch: 23, Steps: 65 | Train Loss: 0.2977834 Vali Loss: 0.1559789 Test Loss: 0.2176348
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 7.950278997421265
Epoch: 24, Steps: 65 | Train Loss: 0.2981592 Vali Loss: 0.1559363 Test Loss: 0.2174815
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.029146671295166
Epoch: 25, Steps: 65 | Train Loss: 0.2973593 Vali Loss: 0.1556662 Test Loss: 0.2174125
Validation loss decreased (0.155915 --> 0.155666).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 8.159507036209106
Epoch: 26, Steps: 65 | Train Loss: 0.2980000 Vali Loss: 0.1557451 Test Loss: 0.2172925
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 7.680334806442261
Epoch: 27, Steps: 65 | Train Loss: 0.2972963 Vali Loss: 0.1553818 Test Loss: 0.2172120
Validation loss decreased (0.155666 --> 0.155382).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 8.260609865188599
Epoch: 28, Steps: 65 | Train Loss: 0.2978272 Vali Loss: 0.1553926 Test Loss: 0.2171531
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.423939228057861
Epoch: 29, Steps: 65 | Train Loss: 0.2966588 Vali Loss: 0.1554552 Test Loss: 0.2171001
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 8.310851097106934
Epoch: 30, Steps: 65 | Train Loss: 0.2969345 Vali Loss: 0.1553036 Test Loss: 0.2170013
Validation loss decreased (0.155382 --> 0.155304).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 7.555877208709717
Epoch: 31, Steps: 65 | Train Loss: 0.2964761 Vali Loss: 0.1553201 Test Loss: 0.2169183
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 8.317931652069092
Epoch: 32, Steps: 65 | Train Loss: 0.2960590 Vali Loss: 0.1553339 Test Loss: 0.2168548
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 8.843690633773804
Epoch: 33, Steps: 65 | Train Loss: 0.2963086 Vali Loss: 0.1550439 Test Loss: 0.2167746
Validation loss decreased (0.155304 --> 0.155044).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 8.787333488464355
Epoch: 34, Steps: 65 | Train Loss: 0.2958950 Vali Loss: 0.1551974 Test Loss: 0.2167229
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 9.088168859481812
Epoch: 35, Steps: 65 | Train Loss: 0.2959707 Vali Loss: 0.1550428 Test Loss: 0.2166975
Validation loss decreased (0.155044 --> 0.155043).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.852582931518555
Epoch: 36, Steps: 65 | Train Loss: 0.2961472 Vali Loss: 0.1550580 Test Loss: 0.2166355
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 8.242278575897217
Epoch: 37, Steps: 65 | Train Loss: 0.2961122 Vali Loss: 0.1550067 Test Loss: 0.2166048
Validation loss decreased (0.155043 --> 0.155007).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 7.773691892623901
Epoch: 38, Steps: 65 | Train Loss: 0.2961098 Vali Loss: 0.1549764 Test Loss: 0.2165531
Validation loss decreased (0.155007 --> 0.154976).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 7.695710897445679
Epoch: 39, Steps: 65 | Train Loss: 0.2951050 Vali Loss: 0.1549382 Test Loss: 0.2165401
Validation loss decreased (0.154976 --> 0.154938).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 7.4492058753967285
Epoch: 40, Steps: 65 | Train Loss: 0.2958126 Vali Loss: 0.1548443 Test Loss: 0.2165050
Validation loss decreased (0.154938 --> 0.154844).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 7.026134967803955
Epoch: 41, Steps: 65 | Train Loss: 0.2957254 Vali Loss: 0.1547932 Test Loss: 0.2164361
Validation loss decreased (0.154844 --> 0.154793).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 7.624268054962158
Epoch: 42, Steps: 65 | Train Loss: 0.2962203 Vali Loss: 0.1548378 Test Loss: 0.2164180
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 7.115813255310059
Epoch: 43, Steps: 65 | Train Loss: 0.2947409 Vali Loss: 0.1547489 Test Loss: 0.2163745
Validation loss decreased (0.154793 --> 0.154749).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 7.155896425247192
Epoch: 44, Steps: 65 | Train Loss: 0.2958895 Vali Loss: 0.1547352 Test Loss: 0.2163183
Validation loss decreased (0.154749 --> 0.154735).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 7.58014178276062
Epoch: 45, Steps: 65 | Train Loss: 0.2954341 Vali Loss: 0.1548565 Test Loss: 0.2163198
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 7.866250038146973
Epoch: 46, Steps: 65 | Train Loss: 0.2948690 Vali Loss: 0.1547106 Test Loss: 0.2162605
Validation loss decreased (0.154735 --> 0.154711).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 8.063877820968628
Epoch: 47, Steps: 65 | Train Loss: 0.2951887 Vali Loss: 0.1545644 Test Loss: 0.2162645
Validation loss decreased (0.154711 --> 0.154564).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 8.963958501815796
Epoch: 48, Steps: 65 | Train Loss: 0.2951500 Vali Loss: 0.1546528 Test Loss: 0.2162444
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 8.539830923080444
Epoch: 49, Steps: 65 | Train Loss: 0.2948250 Vali Loss: 0.1546364 Test Loss: 0.2162330
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 8.503931999206543
Epoch: 50, Steps: 65 | Train Loss: 0.2947480 Vali Loss: 0.1545774 Test Loss: 0.2161872
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_192_FITS_ETTm2_ftM_sl720_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.22013120353221893, mae:0.2942561209201813, rse:0.3797833323478699, corr:[0.5506188  0.5585532  0.5633566  0.5634687  0.5613076  0.5594029
 0.5586901  0.5591213  0.5602535  0.5614876  0.56219256 0.5620393
 0.5612095  0.56015354 0.5593174  0.5588535  0.55874133 0.55879796
 0.55881095 0.5585536  0.55795026 0.55706537 0.55610925 0.5552952
 0.5547186  0.55436647 0.55414945 0.55394524 0.5536402  0.55314946
 0.5524604  0.55168474 0.5509079  0.55016935 0.5494913  0.5489053
 0.5483898  0.54792726 0.5474677  0.5469526  0.5463526  0.5456686
 0.544939   0.54419655 0.54346967 0.54278296 0.5421679  0.54162693
 0.54107136 0.5404253  0.5396656  0.5388364  0.53796345 0.537103
 0.5363239  0.5357395  0.53536785 0.53515524 0.5350191  0.53485924
 0.5346031  0.5342546  0.53382623 0.53337055 0.53294206 0.53264683
 0.5324752  0.53240514 0.53235745 0.532217   0.5319005  0.5314079
 0.5307789  0.530112   0.5295068  0.52906334 0.52878237 0.5285614
 0.528274   0.5278354  0.52716064 0.5262706  0.52526283 0.52429664
 0.52352315 0.5230747  0.5228475  0.5227398  0.5225803  0.5222382
 0.5216568  0.5208376  0.51975834 0.5185445  0.5173229  0.5161614
 0.51508516 0.5140954  0.5130679  0.5118899  0.51052636 0.5090172
 0.50751865 0.50619435 0.50513434 0.5043566  0.5037807  0.50329584
 0.50267553 0.50179344 0.5006394  0.4992973  0.49804878 0.4970511
 0.4963769  0.4959911  0.49573857 0.49537632 0.4947828  0.49382257
 0.49260607 0.4912612  0.49009654 0.48926702 0.48875427 0.48840627
 0.48802954 0.48743346 0.48649532 0.4852023  0.48371437 0.48227382
 0.4811994  0.4806699  0.4806424  0.4808331  0.480841   0.4804794
 0.479606   0.47831976 0.47684488 0.47556254 0.47481605 0.474619
 0.47475058 0.47474852 0.47441742 0.473567   0.4722593  0.4706198
 0.46904695 0.4679902  0.46761566 0.46774814 0.46802965 0.46814293
 0.46779826 0.4669103  0.4654946  0.46398914 0.46272868 0.4621392
 0.46228462 0.46295723 0.4636994  0.4640751  0.4637296  0.46260455
 0.46098903 0.45949957 0.45865244 0.45881248 0.4597885  0.46096638
 0.4616807  0.46146178 0.460205   0.45813873 0.45574978 0.45407757
 0.45367855 0.45442882 0.4556565  0.45633984 0.45573208 0.45366403
 0.45063177 0.44813442 0.44808376 0.4511114  0.4545386  0.4536402 ]
