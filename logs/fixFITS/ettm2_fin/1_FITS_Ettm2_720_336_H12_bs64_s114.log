Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14721280.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2997525
	speed: 0.1430s/iter; left time: 1851.4617s
	iters: 200, epoch: 1 | loss: 0.3776894
	speed: 0.1286s/iter; left time: 1652.8007s
Epoch: 1 cost time: 35.54067325592041
Epoch: 1, Steps: 261 | Train Loss: 0.4598301 Vali Loss: 0.2098738 Test Loss: 0.2874074
Validation loss decreased (inf --> 0.209874).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5286446
	speed: 0.6091s/iter; left time: 7729.0117s
	iters: 200, epoch: 2 | loss: 0.3915154
	speed: 0.1269s/iter; left time: 1597.8541s
Epoch: 2 cost time: 33.96978545188904
Epoch: 2, Steps: 261 | Train Loss: 0.4023857 Vali Loss: 0.2015140 Test Loss: 0.2779348
Validation loss decreased (0.209874 --> 0.201514).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3973835
	speed: 0.6432s/iter; left time: 7994.1290s
	iters: 200, epoch: 3 | loss: 0.3414578
	speed: 0.1721s/iter; left time: 2121.9767s
Epoch: 3 cost time: 45.24816012382507
Epoch: 3, Steps: 261 | Train Loss: 0.3918538 Vali Loss: 0.1982097 Test Loss: 0.2745185
Validation loss decreased (0.201514 --> 0.198210).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3540092
	speed: 0.8307s/iter; left time: 10107.5587s
	iters: 200, epoch: 4 | loss: 0.4335308
	speed: 0.1509s/iter; left time: 1821.4483s
Epoch: 4 cost time: 41.80296063423157
Epoch: 4, Steps: 261 | Train Loss: 0.3864441 Vali Loss: 0.1966380 Test Loss: 0.2726371
Validation loss decreased (0.198210 --> 0.196638).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3006506
	speed: 0.5956s/iter; left time: 7091.6805s
	iters: 200, epoch: 5 | loss: 0.2407469
	speed: 0.1179s/iter; left time: 1392.3120s
Epoch: 5 cost time: 31.295708894729614
Epoch: 5, Steps: 261 | Train Loss: 0.3832030 Vali Loss: 0.1954647 Test Loss: 0.2709461
Validation loss decreased (0.196638 --> 0.195465).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4359809
	speed: 0.6299s/iter; left time: 7335.8378s
	iters: 200, epoch: 6 | loss: 0.5309114
	speed: 0.1635s/iter; left time: 1887.6986s
Epoch: 6 cost time: 44.54657959938049
Epoch: 6, Steps: 261 | Train Loss: 0.3807680 Vali Loss: 0.1950898 Test Loss: 0.2706531
Validation loss decreased (0.195465 --> 0.195090).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3707309
	speed: 0.8620s/iter; left time: 9814.2400s
	iters: 200, epoch: 7 | loss: 0.4369813
	speed: 0.1833s/iter; left time: 2068.8620s
Epoch: 7 cost time: 50.49689745903015
Epoch: 7, Steps: 261 | Train Loss: 0.3802238 Vali Loss: 0.1945967 Test Loss: 0.2699264
Validation loss decreased (0.195090 --> 0.194597).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2497771
	speed: 0.7560s/iter; left time: 8409.5937s
	iters: 200, epoch: 8 | loss: 0.5417697
	speed: 0.1533s/iter; left time: 1689.6883s
Epoch: 8 cost time: 42.33253836631775
Epoch: 8, Steps: 261 | Train Loss: 0.3793964 Vali Loss: 0.1942397 Test Loss: 0.2695757
Validation loss decreased (0.194597 --> 0.194240).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3446023
	speed: 0.6971s/iter; left time: 7572.9217s
	iters: 200, epoch: 9 | loss: 0.3448603
	speed: 0.1436s/iter; left time: 1545.7607s
Epoch: 9 cost time: 38.658039569854736
Epoch: 9, Steps: 261 | Train Loss: 0.3787275 Vali Loss: 0.1940411 Test Loss: 0.2692975
Validation loss decreased (0.194240 --> 0.194041).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2789080
	speed: 0.7127s/iter; left time: 7555.5297s
	iters: 200, epoch: 10 | loss: 0.2807319
	speed: 0.1834s/iter; left time: 1925.6038s
Epoch: 10 cost time: 48.353368520736694
Epoch: 10, Steps: 261 | Train Loss: 0.3778300 Vali Loss: 0.1935651 Test Loss: 0.2688675
Validation loss decreased (0.194041 --> 0.193565).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3379339
	speed: 0.8114s/iter; left time: 8390.6164s
	iters: 200, epoch: 11 | loss: 0.3657013
	speed: 0.1814s/iter; left time: 1858.1087s
Epoch: 11 cost time: 48.336411476135254
Epoch: 11, Steps: 261 | Train Loss: 0.3777878 Vali Loss: 0.1931700 Test Loss: 0.2684989
Validation loss decreased (0.193565 --> 0.193170).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4366030
	speed: 0.7915s/iter; left time: 7978.8218s
	iters: 200, epoch: 12 | loss: 0.3759527
	speed: 0.1720s/iter; left time: 1716.0812s
Epoch: 12 cost time: 45.81165790557861
Epoch: 12, Steps: 261 | Train Loss: 0.3770497 Vali Loss: 0.1933035 Test Loss: 0.2683584
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4155694
	speed: 0.7503s/iter; left time: 7367.4828s
	iters: 200, epoch: 13 | loss: 0.3345450
	speed: 0.1688s/iter; left time: 1640.7290s
Epoch: 13 cost time: 45.20266842842102
Epoch: 13, Steps: 261 | Train Loss: 0.3768226 Vali Loss: 0.1933351 Test Loss: 0.2685639
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4054599
	speed: 0.7553s/iter; left time: 7218.9204s
	iters: 200, epoch: 14 | loss: 0.4221635
	speed: 0.1637s/iter; left time: 1548.6372s
Epoch: 14 cost time: 44.89498853683472
Epoch: 14, Steps: 261 | Train Loss: 0.3762542 Vali Loss: 0.1930629 Test Loss: 0.2681560
Validation loss decreased (0.193170 --> 0.193063).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5207099
	speed: 0.7700s/iter; left time: 7158.4164s
	iters: 200, epoch: 15 | loss: 0.5227470
	speed: 0.1628s/iter; left time: 1497.5204s
Epoch: 15 cost time: 43.88444232940674
Epoch: 15, Steps: 261 | Train Loss: 0.3761047 Vali Loss: 0.1931501 Test Loss: 0.2683451
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3296206
	speed: 0.7129s/iter; left time: 6441.7030s
	iters: 200, epoch: 16 | loss: 0.3459427
	speed: 0.1571s/iter; left time: 1403.8865s
Epoch: 16 cost time: 43.92907476425171
Epoch: 16, Steps: 261 | Train Loss: 0.3761095 Vali Loss: 0.1930566 Test Loss: 0.2682175
Validation loss decreased (0.193063 --> 0.193057).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4434845
	speed: 0.7994s/iter; left time: 7014.3412s
	iters: 200, epoch: 17 | loss: 0.4200729
	speed: 0.1615s/iter; left time: 1400.8753s
Epoch: 17 cost time: 45.677719593048096
Epoch: 17, Steps: 261 | Train Loss: 0.3758356 Vali Loss: 0.1930773 Test Loss: 0.2680916
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5235785
	speed: 0.8284s/iter; left time: 7052.7701s
	iters: 200, epoch: 18 | loss: 0.4413510
	speed: 0.1788s/iter; left time: 1504.7913s
Epoch: 18 cost time: 48.793567419052124
Epoch: 18, Steps: 261 | Train Loss: 0.3757875 Vali Loss: 0.1928189 Test Loss: 0.2678690
Validation loss decreased (0.193057 --> 0.192819).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3643593
	speed: 0.6868s/iter; left time: 5667.8268s
	iters: 200, epoch: 19 | loss: 0.5271571
	speed: 0.1699s/iter; left time: 1385.3584s
Epoch: 19 cost time: 43.48493313789368
Epoch: 19, Steps: 261 | Train Loss: 0.3752895 Vali Loss: 0.1928621 Test Loss: 0.2678964
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4236130
	speed: 0.7705s/iter; left time: 6157.9498s
	iters: 200, epoch: 20 | loss: 0.4165608
	speed: 0.1546s/iter; left time: 1220.3803s
Epoch: 20 cost time: 42.51342034339905
Epoch: 20, Steps: 261 | Train Loss: 0.3752948 Vali Loss: 0.1928556 Test Loss: 0.2679601
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4308962
	speed: 0.7556s/iter; left time: 5841.7731s
	iters: 200, epoch: 21 | loss: 0.3659229
	speed: 0.1743s/iter; left time: 1330.3317s
Epoch: 21 cost time: 46.90771174430847
Epoch: 21, Steps: 261 | Train Loss: 0.3750820 Vali Loss: 0.1928385 Test Loss: 0.2680522
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2688937187194824, mae:0.3263815939426422, rse:0.41884273290634155, corr:[0.54947484 0.5567947  0.5535657  0.55085266 0.5512263  0.5532556
 0.5543442  0.5535732  0.5522311  0.5516622  0.55222034 0.55331534
 0.5539188  0.5534754  0.5524792  0.5516384  0.5513343  0.5514147
 0.5513614  0.5507922  0.5498461  0.5489307  0.5483815  0.5482378
 0.54818225 0.5478632  0.5471932  0.5463663  0.54565877 0.5452683
 0.5451563  0.5450965  0.5447492  0.5440019  0.54304206 0.5421573
 0.54148453 0.54101914 0.54059315 0.5400674  0.53943735 0.5387637
 0.5381686  0.5377145  0.5373585  0.5369393  0.53633565 0.53552836
 0.5345739  0.5336567  0.53296727 0.53253466 0.53216344 0.5316977
 0.53108484 0.53040546 0.52975434 0.52923983 0.5288921  0.52863854
 0.52837723 0.52809924 0.5278259  0.5276033  0.5274591  0.52734476
 0.52713966 0.52683705 0.5265019  0.5262208  0.5260323  0.52588874
 0.5256512  0.5252157  0.52453285 0.52372444 0.5229571  0.52235657
 0.52188605 0.5214409  0.52086735 0.52020496 0.51955384 0.5190265
 0.51865673 0.51833636 0.5179081  0.5173291  0.51666707 0.516061
 0.5156054  0.5152125  0.51467645 0.51384336 0.51267314 0.51126045
 0.50987077 0.508754   0.5078555  0.5069891  0.50600064 0.5048553
 0.5036508  0.5025115  0.5015521  0.50071776 0.49992305 0.49907103
 0.49811667 0.49710536 0.49615896 0.49532717 0.49458668 0.49379137
 0.492868   0.49191073 0.49105564 0.4903281  0.48969546 0.48899108
 0.4881757  0.4872461  0.48637152 0.4856376  0.48501763 0.48438662
 0.48361397 0.48261717 0.48146242 0.4803551  0.47950193 0.4789368
 0.4785211  0.4780166  0.47724986 0.4762173  0.47508234 0.4741126
 0.4734177  0.47294074 0.4724643  0.47185495 0.47111976 0.47030413
 0.46953174 0.46879107 0.46811423 0.4673968  0.46663454 0.46588945
 0.46529588 0.46489105 0.46448055 0.46388096 0.46306694 0.4622484
 0.461677   0.46150842 0.46163812 0.4618258  0.4617301  0.46120784
 0.46034187 0.45945406 0.45881966 0.45849094 0.45825782 0.45790893
 0.45739597 0.45688042 0.45652202 0.4564531  0.45655295 0.45650974
 0.45606756 0.45520878 0.45415443 0.4532372  0.4526306  0.45233753
 0.45202875 0.4514614  0.45071033 0.4499987  0.44953886 0.4493504
 0.44925854 0.4489884  0.44834095 0.4473414  0.4462301  0.44524813
 0.4444711  0.44376716 0.4427195  0.44113162 0.43924126 0.43742338
 0.43603045 0.43505412 0.43420014 0.4331016  0.43165526 0.43006426
 0.42868382 0.4277206  0.42706034 0.42646688 0.42582572 0.42517123
 0.42446196 0.42375714 0.42305967 0.4222595  0.42125702 0.41997966
 0.4186127  0.41740334 0.4165857  0.41601646 0.415321   0.4142804
 0.41294086 0.41164455 0.4107958  0.41035992 0.4100907  0.40965167
 0.40870506 0.4073838  0.40598992 0.40492833 0.40430355 0.40394035
 0.403471   0.40274537 0.40181613 0.40099937 0.40055785 0.40040827
 0.40026397 0.39975238 0.39889395 0.39810085 0.39761332 0.39760092
 0.39789248 0.3981536  0.39807242 0.39764476 0.39728293 0.39720622
 0.3973303  0.39740488 0.39703572 0.39630345 0.3956017  0.39549315
 0.3960882  0.3969542  0.39742953 0.3970623  0.3958724  0.39452586
 0.39378944 0.393908   0.39455813 0.3950023  0.39465994 0.3935239
 0.39223024 0.3916218  0.39187017 0.39256808 0.39299    0.39274782
 0.3921133  0.39150327 0.39131904 0.39157775 0.3917519  0.39142
 0.3905109  0.38950354 0.3890269  0.38924924 0.38963237 0.38918936
 0.38746643 0.38497832 0.38266468 0.38147736 0.38164082 0.38237572
 0.3827902  0.38225317 0.38085818 0.37936962 0.37865025 0.37855533
 0.37857142 0.37797982 0.37680188 0.37554303 0.37516436 0.37596026
 0.37727395 0.37788594 0.37728167 0.3758451  0.37467337 0.37497473
 0.37642345 0.3779296  0.37829077 0.3771586  0.3754128  0.37457547
 0.3752857  0.37688354 0.37799603 0.37768948 0.37613916 0.37490574
 0.37539837 0.37738284 0.3792809  0.37954235 0.37815982 0.3765311
 0.3768621  0.37889278 0.3803489  0.37894002 0.3741938  0.37093058]
