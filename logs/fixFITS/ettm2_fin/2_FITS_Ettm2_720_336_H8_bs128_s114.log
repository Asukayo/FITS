Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=74, out_features=108, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14321664.0
params:  8100.0
Trainable parameters:  8100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3287545
	speed: 0.0700s/iter; left time: 447.9124s
Epoch: 1 cost time: 8.6673264503479
Epoch: 1, Steps: 130 | Train Loss: 0.3890538 Vali Loss: 0.2587617 Test Loss: 0.3481092
Validation loss decreased (inf --> 0.258762).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2830922
	speed: 0.1745s/iter; left time: 1094.4767s
Epoch: 2 cost time: 8.808506488800049
Epoch: 2, Steps: 130 | Train Loss: 0.2796963 Vali Loss: 0.2352330 Test Loss: 0.3164821
Validation loss decreased (0.258762 --> 0.235233).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1859181
	speed: 0.1807s/iter; left time: 1109.9477s
Epoch: 3 cost time: 8.294105529785156
Epoch: 3, Steps: 130 | Train Loss: 0.2340050 Vali Loss: 0.2249145 Test Loss: 0.3044066
Validation loss decreased (0.235233 --> 0.224914).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1812842
	speed: 0.1653s/iter; left time: 993.7238s
Epoch: 4 cost time: 8.221207618713379
Epoch: 4, Steps: 130 | Train Loss: 0.2078233 Vali Loss: 0.2196197 Test Loss: 0.2977057
Validation loss decreased (0.224914 --> 0.219620).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2026846
	speed: 0.1653s/iter; left time: 972.3895s
Epoch: 5 cost time: 8.797428131103516
Epoch: 5, Steps: 130 | Train Loss: 0.1900709 Vali Loss: 0.2158005 Test Loss: 0.2931419
Validation loss decreased (0.219620 --> 0.215801).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1566948
	speed: 0.1588s/iter; left time: 913.0711s
Epoch: 6 cost time: 8.111652612686157
Epoch: 6, Steps: 130 | Train Loss: 0.1777098 Vali Loss: 0.2128289 Test Loss: 0.2893870
Validation loss decreased (0.215801 --> 0.212829).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1261615
	speed: 0.1604s/iter; left time: 901.5394s
Epoch: 7 cost time: 8.116837978363037
Epoch: 7, Steps: 130 | Train Loss: 0.1678525 Vali Loss: 0.2106298 Test Loss: 0.2864893
Validation loss decreased (0.212829 --> 0.210630).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1769194
	speed: 0.1611s/iter; left time: 884.8412s
Epoch: 8 cost time: 7.981696367263794
Epoch: 8, Steps: 130 | Train Loss: 0.1605759 Vali Loss: 0.2085612 Test Loss: 0.2841316
Validation loss decreased (0.210630 --> 0.208561).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1627350
	speed: 0.1691s/iter; left time: 906.5397s
Epoch: 9 cost time: 8.287591695785522
Epoch: 9, Steps: 130 | Train Loss: 0.1555494 Vali Loss: 0.2069591 Test Loss: 0.2822072
Validation loss decreased (0.208561 --> 0.206959).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1440036
	speed: 0.1644s/iter; left time: 859.8210s
Epoch: 10 cost time: 8.639880657196045
Epoch: 10, Steps: 130 | Train Loss: 0.1511619 Vali Loss: 0.2054040 Test Loss: 0.2805742
Validation loss decreased (0.206959 --> 0.205404).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1618448
	speed: 0.1614s/iter; left time: 823.2920s
Epoch: 11 cost time: 8.678591251373291
Epoch: 11, Steps: 130 | Train Loss: 0.1479304 Vali Loss: 0.2041654 Test Loss: 0.2791993
Validation loss decreased (0.205404 --> 0.204165).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1206080
	speed: 0.1531s/iter; left time: 761.2046s
Epoch: 12 cost time: 7.644129037857056
Epoch: 12, Steps: 130 | Train Loss: 0.1452978 Vali Loss: 0.2035525 Test Loss: 0.2782047
Validation loss decreased (0.204165 --> 0.203552).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1090202
	speed: 0.1860s/iter; left time: 900.4010s
Epoch: 13 cost time: 8.384410381317139
Epoch: 13, Steps: 130 | Train Loss: 0.1429963 Vali Loss: 0.2026308 Test Loss: 0.2772542
Validation loss decreased (0.203552 --> 0.202631).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1287315
	speed: 0.1381s/iter; left time: 650.7165s
Epoch: 14 cost time: 7.521796941757202
Epoch: 14, Steps: 130 | Train Loss: 0.1419646 Vali Loss: 0.2018381 Test Loss: 0.2766956
Validation loss decreased (0.202631 --> 0.201838).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1496019
	speed: 0.1465s/iter; left time: 671.1201s
Epoch: 15 cost time: 7.792406320571899
Epoch: 15, Steps: 130 | Train Loss: 0.1402121 Vali Loss: 0.2012881 Test Loss: 0.2760703
Validation loss decreased (0.201838 --> 0.201288).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1469970
	speed: 0.1752s/iter; left time: 779.6864s
Epoch: 16 cost time: 8.902259588241577
Epoch: 16, Steps: 130 | Train Loss: 0.1396097 Vali Loss: 0.2005129 Test Loss: 0.2756007
Validation loss decreased (0.201288 --> 0.200513).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1368538
	speed: 0.1717s/iter; left time: 741.8702s
Epoch: 17 cost time: 9.123472213745117
Epoch: 17, Steps: 130 | Train Loss: 0.1387073 Vali Loss: 0.2000853 Test Loss: 0.2753051
Validation loss decreased (0.200513 --> 0.200085).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1429996
	speed: 0.1736s/iter; left time: 727.7288s
Epoch: 18 cost time: 8.922370910644531
Epoch: 18, Steps: 130 | Train Loss: 0.1379937 Vali Loss: 0.2000313 Test Loss: 0.2749732
Validation loss decreased (0.200085 --> 0.200031).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1422743
	speed: 0.1591s/iter; left time: 646.0557s
Epoch: 19 cost time: 7.431487798690796
Epoch: 19, Steps: 130 | Train Loss: 0.1374097 Vali Loss: 0.2001034 Test Loss: 0.2747702
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1328566
	speed: 0.1560s/iter; left time: 613.4322s
Epoch: 20 cost time: 9.234463691711426
Epoch: 20, Steps: 130 | Train Loss: 0.1367091 Vali Loss: 0.1993930 Test Loss: 0.2744289
Validation loss decreased (0.200031 --> 0.199393).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1285598
	speed: 0.1523s/iter; left time: 578.7535s
Epoch: 21 cost time: 7.748094797134399
Epoch: 21, Steps: 130 | Train Loss: 0.1365830 Vali Loss: 0.1993551 Test Loss: 0.2743488
Validation loss decreased (0.199393 --> 0.199355).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1345158
	speed: 0.1651s/iter; left time: 606.2038s
Epoch: 22 cost time: 8.952414274215698
Epoch: 22, Steps: 130 | Train Loss: 0.1364710 Vali Loss: 0.1994239 Test Loss: 0.2742104
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1756047
	speed: 0.1721s/iter; left time: 609.3188s
Epoch: 23 cost time: 9.282516479492188
Epoch: 23, Steps: 130 | Train Loss: 0.1358542 Vali Loss: 0.1989423 Test Loss: 0.2741149
Validation loss decreased (0.199355 --> 0.198942).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1453719
	speed: 0.1637s/iter; left time: 558.3702s
Epoch: 24 cost time: 7.905133247375488
Epoch: 24, Steps: 130 | Train Loss: 0.1360015 Vali Loss: 0.1985198 Test Loss: 0.2739580
Validation loss decreased (0.198942 --> 0.198520).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1340031
	speed: 0.1693s/iter; left time: 555.5726s
Epoch: 25 cost time: 8.91839051246643
Epoch: 25, Steps: 130 | Train Loss: 0.1356264 Vali Loss: 0.1984649 Test Loss: 0.2739346
Validation loss decreased (0.198520 --> 0.198465).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1426015
	speed: 0.1455s/iter; left time: 458.4094s
Epoch: 26 cost time: 7.834941387176514
Epoch: 26, Steps: 130 | Train Loss: 0.1355916 Vali Loss: 0.1985199 Test Loss: 0.2738647
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1267084
	speed: 0.1253s/iter; left time: 378.4491s
Epoch: 27 cost time: 5.890182018280029
Epoch: 27, Steps: 130 | Train Loss: 0.1354398 Vali Loss: 0.1981892 Test Loss: 0.2737863
Validation loss decreased (0.198465 --> 0.198189).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1375093
	speed: 0.1701s/iter; left time: 491.7720s
Epoch: 28 cost time: 8.339494943618774
Epoch: 28, Steps: 130 | Train Loss: 0.1356136 Vali Loss: 0.1985384 Test Loss: 0.2737189
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1293071
	speed: 0.1414s/iter; left time: 390.4991s
Epoch: 29 cost time: 7.864591836929321
Epoch: 29, Steps: 130 | Train Loss: 0.1351876 Vali Loss: 0.1981026 Test Loss: 0.2737201
Validation loss decreased (0.198189 --> 0.198103).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1342031
	speed: 0.2111s/iter; left time: 555.3435s
Epoch: 30 cost time: 10.033873081207275
Epoch: 30, Steps: 130 | Train Loss: 0.1352024 Vali Loss: 0.1978675 Test Loss: 0.2736671
Validation loss decreased (0.198103 --> 0.197867).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1266263
	speed: 0.2092s/iter; left time: 523.1209s
Epoch: 31 cost time: 11.138561725616455
Epoch: 31, Steps: 130 | Train Loss: 0.1351562 Vali Loss: 0.1979921 Test Loss: 0.2736595
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1415258
	speed: 0.1953s/iter; left time: 463.0455s
Epoch: 32 cost time: 9.6164710521698
Epoch: 32, Steps: 130 | Train Loss: 0.1351505 Vali Loss: 0.1976831 Test Loss: 0.2735986
Validation loss decreased (0.197867 --> 0.197683).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1076477
	speed: 0.1929s/iter; left time: 432.2870s
Epoch: 33 cost time: 9.379846334457397
Epoch: 33, Steps: 130 | Train Loss: 0.1349628 Vali Loss: 0.1982632 Test Loss: 0.2735685
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1529117
	speed: 0.1959s/iter; left time: 413.5120s
Epoch: 34 cost time: 9.030267715454102
Epoch: 34, Steps: 130 | Train Loss: 0.1349643 Vali Loss: 0.1980365 Test Loss: 0.2735821
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1099792
	speed: 0.1427s/iter; left time: 282.7163s
Epoch: 35 cost time: 8.31728458404541
Epoch: 35, Steps: 130 | Train Loss: 0.1349672 Vali Loss: 0.1979201 Test Loss: 0.2735337
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=74, out_features=108, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14321664.0
params:  8100.0
Trainable parameters:  8100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4211833
	speed: 0.0626s/iter; left time: 400.3838s
Epoch: 1 cost time: 7.25195050239563
Epoch: 1, Steps: 130 | Train Loss: 0.3822016 Vali Loss: 0.1961173 Test Loss: 0.2714534
Validation loss decreased (inf --> 0.196117).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3511081
	speed: 0.1658s/iter; left time: 1039.5746s
Epoch: 2 cost time: 9.169286012649536
Epoch: 2, Steps: 130 | Train Loss: 0.3806115 Vali Loss: 0.1952027 Test Loss: 0.2704114
Validation loss decreased (0.196117 --> 0.195203).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4563041
	speed: 0.1540s/iter; left time: 945.6911s
Epoch: 3 cost time: 8.362558126449585
Epoch: 3, Steps: 130 | Train Loss: 0.3792635 Vali Loss: 0.1944475 Test Loss: 0.2697754
Validation loss decreased (0.195203 --> 0.194448).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3090507
	speed: 0.1640s/iter; left time: 985.8792s
Epoch: 4 cost time: 7.558597564697266
Epoch: 4, Steps: 130 | Train Loss: 0.3792022 Vali Loss: 0.1949552 Test Loss: 0.2698897
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3672237
	speed: 0.1339s/iter; left time: 787.6276s
Epoch: 5 cost time: 6.864266633987427
Epoch: 5, Steps: 130 | Train Loss: 0.3779604 Vali Loss: 0.1945993 Test Loss: 0.2696900
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3601137
	speed: 0.1697s/iter; left time: 976.1324s
Epoch: 6 cost time: 8.783084154129028
Epoch: 6, Steps: 130 | Train Loss: 0.3778976 Vali Loss: 0.1943558 Test Loss: 0.2696512
Validation loss decreased (0.194448 --> 0.194356).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4635789
	speed: 0.1609s/iter; left time: 904.6242s
Epoch: 7 cost time: 7.758120775222778
Epoch: 7, Steps: 130 | Train Loss: 0.3778370 Vali Loss: 0.1942563 Test Loss: 0.2693236
Validation loss decreased (0.194356 --> 0.194256).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4143328
	speed: 0.1878s/iter; left time: 1031.0032s
Epoch: 8 cost time: 8.109036445617676
Epoch: 8, Steps: 130 | Train Loss: 0.3775342 Vali Loss: 0.1946765 Test Loss: 0.2694267
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4256304
	speed: 0.1688s/iter; left time: 904.6743s
Epoch: 9 cost time: 8.941330909729004
Epoch: 9, Steps: 130 | Train Loss: 0.3770371 Vali Loss: 0.1943524 Test Loss: 0.2693420
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3600638
	speed: 0.1706s/iter; left time: 892.1592s
Epoch: 10 cost time: 8.692925214767456
Epoch: 10, Steps: 130 | Train Loss: 0.3774001 Vali Loss: 0.1940947 Test Loss: 0.2690303
Validation loss decreased (0.194256 --> 0.194095).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4102015
	speed: 0.1549s/iter; left time: 790.2106s
Epoch: 11 cost time: 6.283525466918945
Epoch: 11, Steps: 130 | Train Loss: 0.3770511 Vali Loss: 0.1937129 Test Loss: 0.2690322
Validation loss decreased (0.194095 --> 0.193713).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3139830
	speed: 0.1451s/iter; left time: 721.4040s
Epoch: 12 cost time: 7.42662239074707
Epoch: 12, Steps: 130 | Train Loss: 0.3764896 Vali Loss: 0.1942670 Test Loss: 0.2691680
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2757955
	speed: 0.1551s/iter; left time: 750.8015s
Epoch: 13 cost time: 7.201945781707764
Epoch: 13, Steps: 130 | Train Loss: 0.3762316 Vali Loss: 0.1943506 Test Loss: 0.2690740
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4096814
	speed: 0.1488s/iter; left time: 700.9569s
Epoch: 14 cost time: 7.548012018203735
Epoch: 14, Steps: 130 | Train Loss: 0.3762932 Vali Loss: 0.1939887 Test Loss: 0.2689445
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2696813642978668, mae:0.3272535800933838, rse:0.41945573687553406, corr:[0.55308723 0.5569153  0.55858225 0.5578354  0.55603826 0.55448186
 0.55370027 0.5536746  0.55414295 0.55470407 0.55496067 0.5547308
 0.55411404 0.5533346  0.5526523  0.5521538  0.55183595 0.55161786
 0.5513476  0.5508859  0.5502042  0.5493551  0.54847676 0.5477243
 0.5471517  0.5467449  0.5464443  0.54615957 0.5457959  0.54530436
 0.5446723  0.5439949  0.54331636 0.54265213 0.5420197  0.5414576
 0.54093236 0.5404236  0.5398831  0.5392832  0.53862894 0.53793603
 0.5372409  0.5365745  0.53596133 0.5353979  0.5348854  0.5343923
 0.53382075 0.5331104  0.53227466 0.53139985 0.5305298  0.5297291
 0.5290436  0.5285391  0.52819216 0.5279564  0.52777874 0.52760404
 0.5273888  0.52715844 0.5269197  0.5266699  0.52642006 0.5262033
 0.525992   0.52579117 0.5255808  0.52532995 0.52501196 0.52464056
 0.52423066 0.5238074  0.52336746 0.5229311  0.52248865 0.52201515
 0.5214798  0.5208957  0.52021515 0.5194816  0.5187368  0.51804024
 0.51745385 0.5169958  0.51661855 0.51629215 0.51595986 0.51556826
 0.5150826  0.51446533 0.5136514  0.5126329  0.51143795 0.5100987
 0.50871176 0.50741714 0.5062285  0.50513214 0.5041146  0.50314766
 0.502212   0.5012535  0.50027543 0.4992487  0.4982209  0.4972419
 0.49631694 0.49543205 0.49459082 0.49378213 0.49304208 0.49231467
 0.4915204  0.49064913 0.4897256  0.48874256 0.48777518 0.48683447
 0.48602954 0.48536304 0.48486152 0.48443347 0.48393348 0.48324615
 0.48233652 0.4811975  0.47987303 0.47847247 0.47714505 0.4760145
 0.47518912 0.47468477 0.474419   0.47421986 0.47390684 0.473403
 0.47267157 0.4717969  0.47086093 0.4699946  0.46930853 0.46876675
 0.46831667 0.46780336 0.4672297  0.46653277 0.46572104 0.46478114
 0.4637941  0.46286735 0.46200642 0.4612249  0.46054888 0.4600348
 0.45965374 0.45935428 0.45905986 0.458792   0.4584991  0.45817935
 0.4578271  0.45747766 0.45714763 0.45685259 0.45653278 0.4561616
 0.45578393 0.45547816 0.45519862 0.4549408  0.45468158 0.4543867
 0.45402417 0.4535772  0.45303234 0.4524115  0.4517196  0.45109886
 0.45057186 0.4501471  0.4498644  0.44966552 0.4494409  0.44909605
 0.4486001  0.44795915 0.447163   0.4462041  0.44510522 0.44389796
 0.44265452 0.44149035 0.44029406 0.4389514  0.43750247 0.43599373
 0.43458098 0.43338266 0.43248537 0.4318488  0.43136004 0.43086088
 0.4302205  0.42933917 0.42814636 0.42670557 0.42524067 0.42401496
 0.4230438  0.42231822 0.42176974 0.42129126 0.42079917 0.4201318
 0.4192187  0.41803423 0.41672817 0.41543245 0.414209   0.41314036
 0.41222972 0.4114273  0.41068834 0.40981287 0.4087508  0.4075766
 0.4063104  0.40515816 0.40417275 0.4034216  0.4028168  0.40226555
 0.4016392  0.40094805 0.40019843 0.39949474 0.3989612  0.39862692
 0.39849007 0.39837912 0.3982146  0.39802966 0.39769408 0.39726275
 0.39682403 0.39648005 0.39624244 0.3960968  0.39613128 0.3962839
 0.39644653 0.39658165 0.39652544 0.39627764 0.39585072 0.39536673
 0.3949409  0.39462635 0.3944238  0.39430144 0.394166   0.39399815
 0.39374945 0.3933794  0.39300555 0.39272785 0.3925822  0.39255562
 0.3926064  0.3927147  0.39273682 0.3926238  0.39230558 0.39178988
 0.39124537 0.3907011  0.39024296 0.39001617 0.39000052 0.3901427
 0.39026698 0.3902294  0.38992596 0.38929838 0.38838524 0.3871939
 0.38592136 0.3848849  0.38409263 0.38341758 0.3827451  0.38193667
 0.38108572 0.38024116 0.3794646  0.378847   0.37854153 0.37838298
 0.37836054 0.37826732 0.37803957 0.37750727 0.37676635 0.3760034
 0.37546545 0.37514699 0.37508434 0.37520564 0.37532845 0.3754655
 0.37538055 0.37512195 0.37470892 0.37422335 0.37385526 0.37377647
 0.37391979 0.37422004 0.37451825 0.37460726 0.37429783 0.37371662
 0.37307483 0.372727   0.3729401  0.37355748 0.3743648  0.37486842
 0.37481505 0.37375885 0.37163216 0.3690958  0.36714226 0.36622944]
