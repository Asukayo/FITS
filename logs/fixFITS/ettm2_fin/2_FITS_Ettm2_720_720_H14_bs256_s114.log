Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  106688512.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 13.221119403839111
Epoch: 1, Steps: 64 | Train Loss: 0.5430054 Vali Loss: 0.3475010 Test Loss: 0.4811647
Validation loss decreased (inf --> 0.347501).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 13.22405743598938
Epoch: 2, Steps: 64 | Train Loss: 0.4312723 Vali Loss: 0.3157107 Test Loss: 0.4397455
Validation loss decreased (0.347501 --> 0.315711).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 13.598397970199585
Epoch: 3, Steps: 64 | Train Loss: 0.3822659 Vali Loss: 0.3018746 Test Loss: 0.4213917
Validation loss decreased (0.315711 --> 0.301875).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 13.074060916900635
Epoch: 4, Steps: 64 | Train Loss: 0.3552389 Vali Loss: 0.2941207 Test Loss: 0.4116497
Validation loss decreased (0.301875 --> 0.294121).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 13.119448900222778
Epoch: 5, Steps: 64 | Train Loss: 0.3388422 Vali Loss: 0.2896803 Test Loss: 0.4056213
Validation loss decreased (0.294121 --> 0.289680).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 12.641616106033325
Epoch: 6, Steps: 64 | Train Loss: 0.3275613 Vali Loss: 0.2865614 Test Loss: 0.4014733
Validation loss decreased (0.289680 --> 0.286561).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.65512752532959
Epoch: 7, Steps: 64 | Train Loss: 0.3174066 Vali Loss: 0.2838569 Test Loss: 0.3982534
Validation loss decreased (0.286561 --> 0.283857).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.896456718444824
Epoch: 8, Steps: 64 | Train Loss: 0.3098036 Vali Loss: 0.2824739 Test Loss: 0.3957188
Validation loss decreased (0.283857 --> 0.282474).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.270090579986572
Epoch: 9, Steps: 64 | Train Loss: 0.3039276 Vali Loss: 0.2804942 Test Loss: 0.3935231
Validation loss decreased (0.282474 --> 0.280494).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.389759302139282
Epoch: 10, Steps: 64 | Train Loss: 0.2986288 Vali Loss: 0.2790060 Test Loss: 0.3916965
Validation loss decreased (0.280494 --> 0.279006).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.222097158432007
Epoch: 11, Steps: 64 | Train Loss: 0.2942447 Vali Loss: 0.2780249 Test Loss: 0.3900533
Validation loss decreased (0.279006 --> 0.278025).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.578169584274292
Epoch: 12, Steps: 64 | Train Loss: 0.2902718 Vali Loss: 0.2768297 Test Loss: 0.3886617
Validation loss decreased (0.278025 --> 0.276830).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.609485864639282
Epoch: 13, Steps: 64 | Train Loss: 0.2868862 Vali Loss: 0.2759375 Test Loss: 0.3873739
Validation loss decreased (0.276830 --> 0.275938).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 13.12959909439087
Epoch: 14, Steps: 64 | Train Loss: 0.2837256 Vali Loss: 0.2751086 Test Loss: 0.3862821
Validation loss decreased (0.275938 --> 0.275109).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 13.218694686889648
Epoch: 15, Steps: 64 | Train Loss: 0.2817789 Vali Loss: 0.2742942 Test Loss: 0.3852989
Validation loss decreased (0.275109 --> 0.274294).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 12.973809719085693
Epoch: 16, Steps: 64 | Train Loss: 0.2793322 Vali Loss: 0.2737978 Test Loss: 0.3843503
Validation loss decreased (0.274294 --> 0.273798).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 12.778462886810303
Epoch: 17, Steps: 64 | Train Loss: 0.2768313 Vali Loss: 0.2730364 Test Loss: 0.3835304
Validation loss decreased (0.273798 --> 0.273036).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.090431213378906
Epoch: 18, Steps: 64 | Train Loss: 0.2756979 Vali Loss: 0.2724443 Test Loss: 0.3827772
Validation loss decreased (0.273036 --> 0.272444).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.941897869110107
Epoch: 19, Steps: 64 | Train Loss: 0.2733321 Vali Loss: 0.2717875 Test Loss: 0.3821555
Validation loss decreased (0.272444 --> 0.271787).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 12.023861408233643
Epoch: 20, Steps: 64 | Train Loss: 0.2725283 Vali Loss: 0.2715888 Test Loss: 0.3814731
Validation loss decreased (0.271787 --> 0.271589).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.10325002670288
Epoch: 21, Steps: 64 | Train Loss: 0.2708981 Vali Loss: 0.2710090 Test Loss: 0.3808867
Validation loss decreased (0.271589 --> 0.271009).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 11.356072187423706
Epoch: 22, Steps: 64 | Train Loss: 0.2700973 Vali Loss: 0.2707664 Test Loss: 0.3803547
Validation loss decreased (0.271009 --> 0.270766).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 11.32007384300232
Epoch: 23, Steps: 64 | Train Loss: 0.2690816 Vali Loss: 0.2703388 Test Loss: 0.3799161
Validation loss decreased (0.270766 --> 0.270339).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 11.43206262588501
Epoch: 24, Steps: 64 | Train Loss: 0.2683239 Vali Loss: 0.2697299 Test Loss: 0.3794561
Validation loss decreased (0.270339 --> 0.269730).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 11.026815414428711
Epoch: 25, Steps: 64 | Train Loss: 0.2673993 Vali Loss: 0.2694924 Test Loss: 0.3790388
Validation loss decreased (0.269730 --> 0.269492).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 11.073675155639648
Epoch: 26, Steps: 64 | Train Loss: 0.2668883 Vali Loss: 0.2693004 Test Loss: 0.3786913
Validation loss decreased (0.269492 --> 0.269300).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 11.106006145477295
Epoch: 27, Steps: 64 | Train Loss: 0.2665529 Vali Loss: 0.2688425 Test Loss: 0.3783575
Validation loss decreased (0.269300 --> 0.268843).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 11.527734279632568
Epoch: 28, Steps: 64 | Train Loss: 0.2653200 Vali Loss: 0.2688006 Test Loss: 0.3780201
Validation loss decreased (0.268843 --> 0.268801).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 12.917404413223267
Epoch: 29, Steps: 64 | Train Loss: 0.2646102 Vali Loss: 0.2685362 Test Loss: 0.3777398
Validation loss decreased (0.268801 --> 0.268536).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 12.753373622894287
Epoch: 30, Steps: 64 | Train Loss: 0.2636774 Vali Loss: 0.2685535 Test Loss: 0.3774534
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 12.296319484710693
Epoch: 31, Steps: 64 | Train Loss: 0.2640068 Vali Loss: 0.2682406 Test Loss: 0.3771926
Validation loss decreased (0.268536 --> 0.268241).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 12.724718570709229
Epoch: 32, Steps: 64 | Train Loss: 0.2632697 Vali Loss: 0.2679963 Test Loss: 0.3769319
Validation loss decreased (0.268241 --> 0.267996).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 12.028581619262695
Epoch: 33, Steps: 64 | Train Loss: 0.2626057 Vali Loss: 0.2674987 Test Loss: 0.3767353
Validation loss decreased (0.267996 --> 0.267499).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 11.503012895584106
Epoch: 34, Steps: 64 | Train Loss: 0.2625889 Vali Loss: 0.2675822 Test Loss: 0.3765248
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 11.111759662628174
Epoch: 35, Steps: 64 | Train Loss: 0.2624318 Vali Loss: 0.2674596 Test Loss: 0.3763354
Validation loss decreased (0.267499 --> 0.267460).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 11.144442796707153
Epoch: 36, Steps: 64 | Train Loss: 0.2621843 Vali Loss: 0.2674228 Test Loss: 0.3761627
Validation loss decreased (0.267460 --> 0.267423).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 10.963781356811523
Epoch: 37, Steps: 64 | Train Loss: 0.2618477 Vali Loss: 0.2671149 Test Loss: 0.3759839
Validation loss decreased (0.267423 --> 0.267115).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 10.93930697441101
Epoch: 38, Steps: 64 | Train Loss: 0.2609267 Vali Loss: 0.2672229 Test Loss: 0.3758444
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 10.954588413238525
Epoch: 39, Steps: 64 | Train Loss: 0.2608641 Vali Loss: 0.2670012 Test Loss: 0.3757017
Validation loss decreased (0.267115 --> 0.267001).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 11.158170700073242
Epoch: 40, Steps: 64 | Train Loss: 0.2606091 Vali Loss: 0.2668568 Test Loss: 0.3755699
Validation loss decreased (0.267001 --> 0.266857).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 11.10351037979126
Epoch: 41, Steps: 64 | Train Loss: 0.2608076 Vali Loss: 0.2666561 Test Loss: 0.3754287
Validation loss decreased (0.266857 --> 0.266656).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 10.828293323516846
Epoch: 42, Steps: 64 | Train Loss: 0.2608647 Vali Loss: 0.2665081 Test Loss: 0.3753240
Validation loss decreased (0.266656 --> 0.266508).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 10.837052583694458
Epoch: 43, Steps: 64 | Train Loss: 0.2600215 Vali Loss: 0.2666674 Test Loss: 0.3751817
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 11.59708046913147
Epoch: 44, Steps: 64 | Train Loss: 0.2600404 Vali Loss: 0.2665628 Test Loss: 0.3750613
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 13.476131916046143
Epoch: 45, Steps: 64 | Train Loss: 0.2601502 Vali Loss: 0.2663197 Test Loss: 0.3749707
Validation loss decreased (0.266508 --> 0.266320).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 14.054470539093018
Epoch: 46, Steps: 64 | Train Loss: 0.2596211 Vali Loss: 0.2662515 Test Loss: 0.3748809
Validation loss decreased (0.266320 --> 0.266252).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 13.087238788604736
Epoch: 47, Steps: 64 | Train Loss: 0.2598215 Vali Loss: 0.2664434 Test Loss: 0.3748026
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 11.27805209159851
Epoch: 48, Steps: 64 | Train Loss: 0.2593259 Vali Loss: 0.2661764 Test Loss: 0.3747163
Validation loss decreased (0.266252 --> 0.266176).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 11.54268217086792
Epoch: 49, Steps: 64 | Train Loss: 0.2591037 Vali Loss: 0.2661041 Test Loss: 0.3746390
Validation loss decreased (0.266176 --> 0.266104).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 10.8947913646698
Epoch: 50, Steps: 64 | Train Loss: 0.2591723 Vali Loss: 0.2659279 Test Loss: 0.3745645
Validation loss decreased (0.266104 --> 0.265928).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  106688512.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.43054747581482
Epoch: 1, Steps: 64 | Train Loss: 0.5013285 Vali Loss: 0.2634098 Test Loss: 0.3721975
Validation loss decreased (inf --> 0.263410).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.469467401504517
Epoch: 2, Steps: 64 | Train Loss: 0.4982331 Vali Loss: 0.2623149 Test Loss: 0.3712388
Validation loss decreased (0.263410 --> 0.262315).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 12.64368200302124
Epoch: 3, Steps: 64 | Train Loss: 0.4968332 Vali Loss: 0.2617836 Test Loss: 0.3704681
Validation loss decreased (0.262315 --> 0.261784).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.866722106933594
Epoch: 4, Steps: 64 | Train Loss: 0.4954232 Vali Loss: 0.2614539 Test Loss: 0.3699298
Validation loss decreased (0.261784 --> 0.261454).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.686652898788452
Epoch: 5, Steps: 64 | Train Loss: 0.4959685 Vali Loss: 0.2612916 Test Loss: 0.3698095
Validation loss decreased (0.261454 --> 0.261292).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.878846883773804
Epoch: 6, Steps: 64 | Train Loss: 0.4955971 Vali Loss: 0.2611993 Test Loss: 0.3695324
Validation loss decreased (0.261292 --> 0.261199).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.329519748687744
Epoch: 7, Steps: 64 | Train Loss: 0.4951654 Vali Loss: 0.2611158 Test Loss: 0.3692783
Validation loss decreased (0.261199 --> 0.261116).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.48094916343689
Epoch: 8, Steps: 64 | Train Loss: 0.4952845 Vali Loss: 0.2611714 Test Loss: 0.3690213
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 13.018800735473633
Epoch: 9, Steps: 64 | Train Loss: 0.4962619 Vali Loss: 0.2605538 Test Loss: 0.3689855
Validation loss decreased (0.261116 --> 0.260554).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 13.752845048904419
Epoch: 10, Steps: 64 | Train Loss: 0.4956234 Vali Loss: 0.2611313 Test Loss: 0.3688102
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.98931884765625
Epoch: 11, Steps: 64 | Train Loss: 0.4945214 Vali Loss: 0.2607896 Test Loss: 0.3686956
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 13.325706720352173
Epoch: 12, Steps: 64 | Train Loss: 0.4945195 Vali Loss: 0.2605525 Test Loss: 0.3687278
Validation loss decreased (0.260554 --> 0.260552).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.885909795761108
Epoch: 13, Steps: 64 | Train Loss: 0.4949279 Vali Loss: 0.2603391 Test Loss: 0.3686285
Validation loss decreased (0.260552 --> 0.260339).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.895920276641846
Epoch: 14, Steps: 64 | Train Loss: 0.4941082 Vali Loss: 0.2607062 Test Loss: 0.3684875
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 12.39598822593689
Epoch: 15, Steps: 64 | Train Loss: 0.4938644 Vali Loss: 0.2606128 Test Loss: 0.3684721
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.797602891921997
Epoch: 16, Steps: 64 | Train Loss: 0.4942298 Vali Loss: 0.2602813 Test Loss: 0.3684399
Validation loss decreased (0.260339 --> 0.260281).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.427022695541382
Epoch: 17, Steps: 64 | Train Loss: 0.4941869 Vali Loss: 0.2601775 Test Loss: 0.3683954
Validation loss decreased (0.260281 --> 0.260177).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 12.740710735321045
Epoch: 18, Steps: 64 | Train Loss: 0.4947876 Vali Loss: 0.2600178 Test Loss: 0.3683769
Validation loss decreased (0.260177 --> 0.260018).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.557238101959229
Epoch: 19, Steps: 64 | Train Loss: 0.4936605 Vali Loss: 0.2602160 Test Loss: 0.3683294
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 13.272891283035278
Epoch: 20, Steps: 64 | Train Loss: 0.4945036 Vali Loss: 0.2603531 Test Loss: 0.3682997
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 12.419305801391602
Epoch: 21, Steps: 64 | Train Loss: 0.4935225 Vali Loss: 0.2604946 Test Loss: 0.3682541
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3490224778652191, mae:0.377966046333313, rse:0.47486600279808044, corr:[0.5398836  0.54466987 0.5416595  0.53939337 0.5398355  0.5412038
 0.54122776 0.5399234  0.53887695 0.5389227  0.53962123 0.53990835
 0.5393181  0.53838366 0.53783643 0.53776044 0.5377068  0.5371929
 0.5362479  0.53533727 0.5348209  0.5345786  0.5342475  0.53362185
 0.5328487  0.5322525  0.5319309  0.53166467 0.53114635 0.5303352
 0.52948916 0.5289316  0.528675   0.52847207 0.52806985 0.5273953
 0.5265698  0.52582115 0.52522427 0.5247364  0.52424383 0.5236791
 0.5231051  0.5225644  0.5220419  0.52142847 0.520653   0.519766
 0.51886886 0.5180729  0.5174778  0.5170288  0.51650184 0.51579416
 0.5150032  0.5143154  0.51382846 0.5135021  0.51322263 0.5128878
 0.51255643 0.5123085  0.51215214 0.51199526 0.5117474  0.51143134
 0.51105195 0.5107404  0.51050085 0.5102498  0.5099173  0.5095181
 0.50912654 0.5087735  0.50839424 0.50791067 0.5073016  0.5066072
 0.5059615  0.5053905  0.5048254  0.50420654 0.5035177  0.5028017
 0.50214887 0.50159144 0.5010581  0.50052905 0.49995852 0.4993972
 0.4989096  0.49841544 0.49776262 0.49683955 0.4956144  0.49421751
 0.492867   0.49169517 0.49057832 0.48937082 0.4880391  0.4866919
 0.48551157 0.4845831  0.483797   0.4829716  0.4820199  0.4810083
 0.48005244 0.4792758  0.4786402  0.47796714 0.47715876 0.47616962
 0.475115   0.47417253 0.4733732  0.47256994 0.47163847 0.4705316
 0.4694237  0.46845752 0.46773884 0.46715915 0.46657446 0.46588844
 0.46509588 0.46417767 0.46313477 0.46203527 0.4610269  0.46022686
 0.45964664 0.45910242 0.45838055 0.4573809  0.456211   0.455179
 0.4544967  0.45412594 0.45381773 0.45336717 0.45274067 0.4519866
 0.4511991  0.45033726 0.4494008  0.4483379  0.44728908 0.44648418
 0.44609252 0.44596106 0.44568583 0.44505504 0.4441782  0.44336
 0.4428232  0.44252476 0.44221374 0.44172704 0.44099814 0.44021544
 0.43961382 0.43933326 0.43926117 0.4391578  0.4388322  0.43826517
 0.4375559  0.43683216 0.4361269  0.43554497 0.43510047 0.43476894
 0.43443576 0.43399462 0.43339512 0.43269992 0.43202582 0.4314536
 0.43087503 0.43021932 0.42955565 0.42899725 0.4286422  0.42842612
 0.42816108 0.42755064 0.42651117 0.42520693 0.42398039 0.42299283
 0.42213023 0.42118338 0.41980225 0.41805166 0.4163659  0.41509807
 0.41428137 0.41359645 0.4127033  0.41148198 0.4100958  0.4088451
 0.4079082  0.40719107 0.40644753 0.40549704 0.40437338 0.4031922
 0.4019447  0.40071437 0.39961246 0.39868504 0.39787415 0.39694753
 0.39577553 0.39436144 0.39302063 0.39195746 0.3911972  0.39056265
 0.38979945 0.38879374 0.38766915 0.38653585 0.38558108 0.38485575
 0.38413733 0.3833112  0.38237983 0.38147864 0.38071558 0.38009086
 0.37942687 0.37863806 0.37776497 0.37700325 0.3765653  0.3764026
 0.37628993 0.37593696 0.37533295 0.37476674 0.3743632  0.37423193
 0.37425944 0.3742856  0.37419334 0.3740232  0.37396303 0.37402222
 0.3741158  0.37419635 0.3741444  0.3739697  0.37364665 0.37321043
 0.37265658 0.3720626  0.3715472  0.371249   0.371115   0.37104255
 0.37082875 0.3703412  0.3697114  0.36910492 0.368614   0.36822867
 0.3679956  0.36787853 0.36778504 0.3676964  0.36753425 0.36725497
 0.36693218 0.36652973 0.36606342 0.36563534 0.36521086 0.36486435
 0.36461735 0.3645044  0.36443225 0.3641727  0.36353874 0.36244577
 0.3611846  0.360172   0.35944664 0.35877362 0.35791153 0.35676092
 0.35560954 0.35468966 0.354027   0.35343936 0.35273692 0.35182807
 0.35102046 0.35056555 0.35062823 0.35082462 0.3507727  0.35022333
 0.34934318 0.34842837 0.34781554 0.34754843 0.34737492 0.3471091
 0.34658164 0.34602314 0.3456357  0.34549522 0.34551176 0.34550494
 0.34530133 0.3449759  0.3446706  0.34444058 0.34425482 0.34408963
 0.34392643 0.34385365 0.34391728 0.34403017 0.34419727 0.34432942
 0.34444866 0.344441   0.34425133 0.34390327 0.34342948 0.34308884
 0.3430134  0.34312683 0.3433102  0.3433537  0.3430712  0.34252062
 0.3420673  0.3420075  0.34231064 0.34264264 0.34266886 0.34235358
 0.34192738 0.34174198 0.34186435 0.34202904 0.34191397 0.34139377
 0.34070313 0.34026012 0.3401871  0.34036016 0.34033236 0.33999172
 0.3395059  0.33920178 0.3392126  0.33932638 0.33915028 0.33859077
 0.33783966 0.33743086 0.3376107  0.33818945 0.33869484 0.3387189
 0.33834755 0.33804247 0.33818346 0.33873928 0.3393061  0.33950207
 0.33921546 0.3386346  0.3380767  0.33771884 0.33759773 0.3374288
 0.33707842 0.33650017 0.3357589  0.33490807 0.33400276 0.33321851
 0.33257857 0.33207095 0.33159125 0.33102039 0.3303128  0.32953453
 0.32888874 0.3285566  0.3284151  0.3282561  0.3279186  0.32723376
 0.32630363 0.32536417 0.32467863 0.3243484  0.32418597 0.3240506
 0.32378295 0.3234023  0.3229235  0.32238662 0.3219599  0.32174817
 0.3218819  0.32228947 0.322686   0.32281142 0.32252645 0.32196847
 0.32144278 0.3211725  0.32117373 0.3213436  0.32153586 0.3216514
 0.32170555 0.32170576 0.32171717 0.32175338 0.32185665 0.32204553
 0.32228374 0.32241175 0.32236546 0.32212994 0.3217791  0.32139826
 0.32104287 0.32083645 0.32062685 0.32025567 0.3197853  0.31944308
 0.31929317 0.31933025 0.31931064 0.31897184 0.31831503 0.31755814
 0.3171033  0.31711143 0.3174175  0.31757128 0.31728983 0.31660312
 0.31580397 0.31528428 0.3151919  0.3153952  0.31566367 0.31566253
 0.31536293 0.3148985  0.31445587 0.31413636 0.31390986 0.3136933
 0.3134204  0.3130604  0.31263658 0.3122242  0.31194168 0.31175235
 0.31159523 0.31130686 0.31083032 0.31006664 0.30904552 0.30779737
 0.3063893  0.30502877 0.3038123  0.30287293 0.3022403  0.30174437
 0.30109787 0.30017287 0.2989903  0.29785258 0.29699633 0.2964949
 0.29619527 0.29588187 0.29531473 0.2944143  0.29339826 0.29251435
 0.29188573 0.29134938 0.29072106 0.2900066  0.28925893 0.2887192
 0.28836367 0.2880871  0.28773847 0.28716686 0.2864487  0.2856865
 0.2850196  0.28449756 0.28403702 0.28355113 0.28304544 0.2825824
 0.2822301  0.2819969  0.28197166 0.28196797 0.2818373  0.28158814
 0.2813089  0.28112862 0.2810274  0.28093487 0.28078336 0.2805214
 0.28018263 0.27990225 0.27973652 0.27965474 0.27956188 0.27932453
 0.27891275 0.27847153 0.2782727  0.2783664  0.2786494  0.2789152
 0.27898684 0.278865   0.27863055 0.27845106 0.27833194 0.27815026
 0.27782574 0.27738726 0.2769065  0.27649814 0.27620086 0.27606323
 0.27597365 0.2758975  0.27584222 0.2758479  0.2757878  0.2756923
 0.27549046 0.27524635 0.27496716 0.2747247  0.2745894  0.27451116
 0.2743895  0.2741205  0.27377233 0.27346542 0.273314   0.27331024
 0.2733013  0.27312678 0.2726633  0.2719679  0.27110392 0.2699789
 0.2685716  0.26712367 0.26574418 0.26451924 0.26346895 0.2626051
 0.26187336 0.261172   0.2604486  0.2596874  0.2588938  0.25812733
 0.25735036 0.2565119  0.2557042  0.25495785 0.2543088  0.25380856
 0.25339594 0.25296944 0.25257653 0.25218567 0.2518915  0.25169802
 0.25150087 0.25126484 0.25088194 0.25038472 0.24998164 0.24975123
 0.24955752 0.24932377 0.24907187 0.24877332 0.24860738 0.24864928
 0.24890338 0.24904637 0.24900892 0.248921   0.24892025 0.24910426
 0.24945512 0.24986942 0.25008923 0.2500777  0.24999921 0.25012973
 0.2505371  0.25105804 0.25146407 0.251688   0.25171587 0.2516343
 0.251535   0.25156638 0.2517781  0.25189647 0.25203642 0.25210664
 0.25210434 0.25208125 0.25198248 0.25194675 0.25179583 0.25154597
 0.25120145 0.25095007 0.2507418  0.2504833  0.25037417 0.25025007
 0.25020584 0.25016513 0.25009432 0.24992344 0.24974142 0.24944875
 0.24928693 0.24923532 0.24914463 0.24904327 0.24891233 0.24874759
 0.24863534 0.24881011 0.24905944 0.2491908  0.24921142 0.24917333
 0.24919371 0.24941823 0.24966349 0.24949129 0.2486585  0.24729732
 0.245766   0.24441715 0.243353   0.24242507 0.24159631 0.24082872
 0.2401843  0.2396858  0.23949061 0.23922613 0.23875932 0.238052
 0.23727737 0.23660825 0.23604402 0.23543155 0.23482442 0.2341401
 0.23354544 0.23312214 0.23279588 0.23268676 0.23253311 0.23212795
 0.2314632  0.23058456 0.22979064 0.22914246 0.22871694 0.22833109
 0.22792058 0.22719015 0.2264042  0.2259692  0.22598842 0.2261186
 0.22572213 0.22475651 0.22373782 0.22360782 0.22417574 0.22466615
 0.22406697 0.22251453 0.22157171 0.22233276 0.22272943 0.21697418]
