Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19457536.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3568774
	speed: 0.1724s/iter; left time: 2232.4044s
	iters: 200, epoch: 1 | loss: 0.2740619
	speed: 0.1829s/iter; left time: 2351.0322s
Epoch: 1 cost time: 47.432058334350586
Epoch: 1, Steps: 261 | Train Loss: 0.3382220 Vali Loss: 0.2371604 Test Loss: 0.3196177
Validation loss decreased (inf --> 0.237160).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2260374
	speed: 0.8483s/iter; left time: 10765.3140s
	iters: 200, epoch: 2 | loss: 0.2347637
	speed: 0.1792s/iter; left time: 2255.5875s
Epoch: 2 cost time: 47.33473587036133
Epoch: 2, Steps: 261 | Train Loss: 0.2186122 Vali Loss: 0.2194319 Test Loss: 0.2980978
Validation loss decreased (0.237160 --> 0.219432).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1868978
	speed: 0.7683s/iter; left time: 9549.2509s
	iters: 200, epoch: 3 | loss: 0.1427472
	speed: 0.1605s/iter; left time: 1979.2101s
Epoch: 3 cost time: 43.821911334991455
Epoch: 3, Steps: 261 | Train Loss: 0.1781806 Vali Loss: 0.2113953 Test Loss: 0.2885374
Validation loss decreased (0.219432 --> 0.211395).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1930912
	speed: 0.7628s/iter; left time: 9281.9668s
	iters: 200, epoch: 4 | loss: 0.1316605
	speed: 0.1767s/iter; left time: 2132.4187s
Epoch: 4 cost time: 45.96762657165527
Epoch: 4, Steps: 261 | Train Loss: 0.1571769 Vali Loss: 0.2058575 Test Loss: 0.2822119
Validation loss decreased (0.211395 --> 0.205858).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1459786
	speed: 0.7686s/iter; left time: 9151.5145s
	iters: 200, epoch: 5 | loss: 0.1262241
	speed: 0.1722s/iter; left time: 2032.6660s
Epoch: 5 cost time: 45.30812072753906
Epoch: 5, Steps: 261 | Train Loss: 0.1451889 Vali Loss: 0.2025849 Test Loss: 0.2781757
Validation loss decreased (0.205858 --> 0.202585).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1379473
	speed: 0.7544s/iter; left time: 8785.4176s
	iters: 200, epoch: 6 | loss: 0.1268583
	speed: 0.1572s/iter; left time: 1815.0952s
Epoch: 6 cost time: 42.794071197509766
Epoch: 6, Steps: 261 | Train Loss: 0.1388035 Vali Loss: 0.2003751 Test Loss: 0.2757414
Validation loss decreased (0.202585 --> 0.200375).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1611965
	speed: 0.7258s/iter; left time: 8263.5138s
	iters: 200, epoch: 7 | loss: 0.1633504
	speed: 0.1547s/iter; left time: 1746.0857s
Epoch: 7 cost time: 42.41943693161011
Epoch: 7, Steps: 261 | Train Loss: 0.1347615 Vali Loss: 0.1986067 Test Loss: 0.2738999
Validation loss decreased (0.200375 --> 0.198607).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1160290
	speed: 0.6669s/iter; left time: 7418.2460s
	iters: 200, epoch: 8 | loss: 0.1294101
	speed: 0.1580s/iter; left time: 1742.1257s
Epoch: 8 cost time: 40.03692173957825
Epoch: 8, Steps: 261 | Train Loss: 0.1326540 Vali Loss: 0.1975563 Test Loss: 0.2729532
Validation loss decreased (0.198607 --> 0.197556).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1199094
	speed: 0.7255s/iter; left time: 7881.4955s
	iters: 200, epoch: 9 | loss: 0.1036603
	speed: 0.1852s/iter; left time: 1993.8088s
Epoch: 9 cost time: 48.195838928222656
Epoch: 9, Steps: 261 | Train Loss: 0.1312273 Vali Loss: 0.1972483 Test Loss: 0.2725968
Validation loss decreased (0.197556 --> 0.197248).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1712206
	speed: 0.8469s/iter; left time: 8978.8044s
	iters: 200, epoch: 10 | loss: 0.1378577
	speed: 0.1467s/iter; left time: 1540.3188s
Epoch: 10 cost time: 43.25245547294617
Epoch: 10, Steps: 261 | Train Loss: 0.1304615 Vali Loss: 0.1965967 Test Loss: 0.2723349
Validation loss decreased (0.197248 --> 0.196597).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1162557
	speed: 0.8001s/iter; left time: 8273.8698s
	iters: 200, epoch: 11 | loss: 0.1395755
	speed: 0.1842s/iter; left time: 1886.0103s
Epoch: 11 cost time: 48.495020151138306
Epoch: 11, Steps: 261 | Train Loss: 0.1298969 Vali Loss: 0.1964859 Test Loss: 0.2721113
Validation loss decreased (0.196597 --> 0.196486).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1965001
	speed: 0.8132s/iter; left time: 8197.3865s
	iters: 200, epoch: 12 | loss: 0.1229623
	speed: 0.1851s/iter; left time: 1847.2356s
Epoch: 12 cost time: 48.89585208892822
Epoch: 12, Steps: 261 | Train Loss: 0.1297792 Vali Loss: 0.1964167 Test Loss: 0.2720444
Validation loss decreased (0.196486 --> 0.196417).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1089977
	speed: 0.8415s/iter; left time: 8262.5822s
	iters: 200, epoch: 13 | loss: 0.1234976
	speed: 0.1896s/iter; left time: 1842.5257s
Epoch: 13 cost time: 50.89546251296997
Epoch: 13, Steps: 261 | Train Loss: 0.1295830 Vali Loss: 0.1962423 Test Loss: 0.2719061
Validation loss decreased (0.196417 --> 0.196242).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1755645
	speed: 0.7416s/iter; left time: 7087.9407s
	iters: 200, epoch: 14 | loss: 0.0978020
	speed: 0.1533s/iter; left time: 1449.5155s
Epoch: 14 cost time: 40.83109974861145
Epoch: 14, Steps: 261 | Train Loss: 0.1295520 Vali Loss: 0.1960533 Test Loss: 0.2719324
Validation loss decreased (0.196242 --> 0.196053).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1133220
	speed: 0.6852s/iter; left time: 6370.5191s
	iters: 200, epoch: 15 | loss: 0.1536789
	speed: 0.1525s/iter; left time: 1402.5158s
Epoch: 15 cost time: 39.357945919036865
Epoch: 15, Steps: 261 | Train Loss: 0.1294060 Vali Loss: 0.1960675 Test Loss: 0.2720185
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1465830
	speed: 0.6710s/iter; left time: 6062.8822s
	iters: 200, epoch: 16 | loss: 0.1236256
	speed: 0.1373s/iter; left time: 1227.2330s
Epoch: 16 cost time: 37.719701051712036
Epoch: 16, Steps: 261 | Train Loss: 0.1293635 Vali Loss: 0.1958403 Test Loss: 0.2718300
Validation loss decreased (0.196053 --> 0.195840).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1456897
	speed: 0.6775s/iter; left time: 5945.2043s
	iters: 200, epoch: 17 | loss: 0.1278218
	speed: 0.1564s/iter; left time: 1356.7261s
Epoch: 17 cost time: 42.424691915512085
Epoch: 17, Steps: 261 | Train Loss: 0.1291435 Vali Loss: 0.1960696 Test Loss: 0.2719240
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1348030
	speed: 0.7777s/iter; left time: 6620.9411s
	iters: 200, epoch: 18 | loss: 0.1319388
	speed: 0.1698s/iter; left time: 1428.8338s
Epoch: 18 cost time: 45.71833610534668
Epoch: 18, Steps: 261 | Train Loss: 0.1293576 Vali Loss: 0.1959042 Test Loss: 0.2718579
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1621279
	speed: 0.6955s/iter; left time: 5739.9291s
	iters: 200, epoch: 19 | loss: 0.1277046
	speed: 0.1500s/iter; left time: 1222.9697s
Epoch: 19 cost time: 40.06157970428467
Epoch: 19, Steps: 261 | Train Loss: 0.1293050 Vali Loss: 0.1959016 Test Loss: 0.2718974
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19457536.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3507642
	speed: 0.1651s/iter; left time: 2138.8214s
	iters: 200, epoch: 1 | loss: 0.4067038
	speed: 0.1494s/iter; left time: 1919.7269s
Epoch: 1 cost time: 41.14181470870972
Epoch: 1, Steps: 261 | Train Loss: 0.3797499 Vali Loss: 0.1942776 Test Loss: 0.2700152
Validation loss decreased (inf --> 0.194278).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4288810
	speed: 0.6933s/iter; left time: 8797.4964s
	iters: 200, epoch: 2 | loss: 0.4973542
	speed: 0.1526s/iter; left time: 1920.9452s
Epoch: 2 cost time: 40.90766000747681
Epoch: 2, Steps: 261 | Train Loss: 0.3777285 Vali Loss: 0.1937499 Test Loss: 0.2689594
Validation loss decreased (0.194278 --> 0.193750).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5121149
	speed: 0.6949s/iter; left time: 8636.7198s
	iters: 200, epoch: 3 | loss: 0.3143496
	speed: 0.1283s/iter; left time: 1581.8615s
Epoch: 3 cost time: 34.22864890098572
Epoch: 3, Steps: 261 | Train Loss: 0.3768486 Vali Loss: 0.1929504 Test Loss: 0.2683211
Validation loss decreased (0.193750 --> 0.192950).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3057860
	speed: 0.5239s/iter; left time: 6374.2614s
	iters: 200, epoch: 4 | loss: 0.4041051
	speed: 0.1205s/iter; left time: 1453.6972s
Epoch: 4 cost time: 32.76834154129028
Epoch: 4, Steps: 261 | Train Loss: 0.3755357 Vali Loss: 0.1930899 Test Loss: 0.2683816
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3698981
	speed: 0.4700s/iter; left time: 5596.3103s
	iters: 200, epoch: 5 | loss: 0.5861207
	speed: 0.0970s/iter; left time: 1145.6685s
Epoch: 5 cost time: 26.179636478424072
Epoch: 5, Steps: 261 | Train Loss: 0.3757356 Vali Loss: 0.1929132 Test Loss: 0.2679600
Validation loss decreased (0.192950 --> 0.192913).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5396976
	speed: 0.5336s/iter; left time: 6214.6654s
	iters: 200, epoch: 6 | loss: 0.4000451
	speed: 0.1193s/iter; left time: 1377.3263s
Epoch: 6 cost time: 31.336947441101074
Epoch: 6, Steps: 261 | Train Loss: 0.3753030 Vali Loss: 0.1923226 Test Loss: 0.2677535
Validation loss decreased (0.192913 --> 0.192323).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5026428
	speed: 0.4546s/iter; left time: 5175.5683s
	iters: 200, epoch: 7 | loss: 0.5335112
	speed: 0.1082s/iter; left time: 1220.5595s
Epoch: 7 cost time: 28.75514268875122
Epoch: 7, Steps: 261 | Train Loss: 0.3743823 Vali Loss: 0.1929553 Test Loss: 0.2682173
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4022395
	speed: 0.5385s/iter; left time: 5990.1868s
	iters: 200, epoch: 8 | loss: 0.3883426
	speed: 0.1355s/iter; left time: 1493.9880s
Epoch: 8 cost time: 35.02451300621033
Epoch: 8, Steps: 261 | Train Loss: 0.3746218 Vali Loss: 0.1926979 Test Loss: 0.2676917
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3530466
	speed: 0.6368s/iter; left time: 6917.8571s
	iters: 200, epoch: 9 | loss: 0.3668527
	speed: 0.1357s/iter; left time: 1460.7986s
Epoch: 9 cost time: 36.01821541786194
Epoch: 9, Steps: 261 | Train Loss: 0.3748931 Vali Loss: 0.1927709 Test Loss: 0.2678518
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_336_FITS_ETTm2_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.2687787115573883, mae:0.3262522518634796, rse:0.41875314712524414, corr:[0.5570836  0.5614522  0.5585376  0.5558528  0.5557291  0.5568446
 0.5570827  0.55597204 0.55479294 0.5545098  0.5549686  0.5553406
 0.55504364 0.5543298  0.55383444 0.55374473 0.5536724  0.5530974
 0.55199814 0.55088615 0.55023414 0.55003744 0.5498772  0.54936546
 0.54852796 0.5477293  0.5472455  0.5470295  0.5467804  0.5462502
 0.5454822  0.5447513  0.544207   0.543819   0.5433962  0.5427689
 0.54192257 0.5410327  0.5402852  0.5397712  0.5394035  0.53898865
 0.53844064 0.5378058  0.53719074 0.5366239  0.5360291  0.53532326
 0.5344921  0.53364784 0.53299224 0.5325943  0.5322316  0.5316907
 0.5309244  0.5300687  0.52931684 0.5288417  0.5286462  0.5285575
 0.52841115 0.52819264 0.52797294 0.52781826 0.5277383  0.52769053
 0.5275932  0.5274065  0.5270917  0.5266482  0.5261304  0.52562577
 0.52518964 0.5248357  0.5245014  0.52413964 0.5236717  0.5230732
 0.52239096 0.5217388  0.5211407  0.5206296  0.52014935 0.51965445
 0.51914346 0.5186066  0.5180215  0.51741976 0.51684445 0.5163544
 0.51597416 0.5156006  0.5150619  0.51425976 0.5131945  0.5119663
 0.5107648  0.5097083  0.50865537 0.50746804 0.50613195 0.50477344
 0.503563   0.5025419  0.5016349  0.5006508  0.49954733 0.4984182
 0.49740398 0.4965923  0.4959115  0.49517477 0.49432096 0.49339446
 0.49255884 0.49194977 0.49144503 0.49073383 0.48966953 0.4883129
 0.48705345 0.48611382 0.48549733 0.4849346  0.48424414 0.48347563
 0.4828185  0.48228142 0.4816729  0.48082617 0.47976866 0.478715
 0.4779308  0.47743034 0.47696203 0.47625023 0.4752542  0.47422972
 0.47344074 0.4729728  0.47259337 0.4720649  0.47135985 0.470571
 0.46982944 0.46900383 0.4679569  0.4665492  0.46505556 0.46399075
 0.46372652 0.46400228 0.46408403 0.4635451  0.46255016 0.46168453
 0.46133563 0.4613613  0.46122187 0.46057913 0.45947134 0.4584484
 0.45803854 0.45827731 0.45861232 0.45846584 0.45765346 0.4565855
 0.45592916 0.4559809  0.4563204  0.45640066 0.45596182 0.4551632
 0.45434746 0.45367882 0.45302764 0.4522124  0.45122495 0.45034832
 0.4496293  0.44893909 0.44815367 0.44725585 0.44648755 0.44619212
 0.44646212 0.44696787 0.44716644 0.4467107  0.4456211  0.4441516
 0.44261128 0.44118723 0.4397654  0.43844372 0.43754974 0.43712524
 0.4368466  0.43612638 0.43464723 0.4325855  0.43062556 0.4293779
 0.42885125 0.42850554 0.42783076 0.42686218 0.42608547 0.42580816
 0.42566547 0.42516834 0.42411017 0.4227847  0.42179054 0.42133647
 0.4211249  0.42060474 0.41962546 0.4183631  0.41715914 0.41622657
 0.4153601  0.41427767 0.4130414  0.41191813 0.41130066 0.4111391
 0.4107957  0.40982258 0.40818706 0.40653124 0.40551147 0.40519264
 0.40493235 0.40415186 0.40274256 0.40136614 0.40087667 0.40128136
 0.40176693 0.4014347  0.40025726 0.39906833 0.3986029  0.39903495
 0.39966452 0.39973003 0.39905092 0.39819404 0.39804852 0.39864874
 0.39924163 0.39917552 0.3983115  0.39737335 0.3970694  0.39748168
 0.39788836 0.39759028 0.39663264 0.39578894 0.39573357 0.39637363
 0.39682546 0.39636394 0.39525262 0.39436138 0.39433825 0.39493755
 0.39529476 0.39480713 0.3935694  0.3925341  0.39237353 0.39288774
 0.39339337 0.39322352 0.39254305 0.3921452  0.39237157 0.39286456
 0.3928666  0.39209637 0.39102882 0.39047077 0.3906357  0.3906628
 0.38974085 0.38795498 0.38599724 0.38484648 0.3847693  0.38491884
 0.38446438 0.38317904 0.38175297 0.3811759  0.38165778 0.3820886
 0.38166568 0.38027444 0.37887463 0.3782114  0.37830108 0.37835625
 0.3778778  0.3770591  0.3769018  0.3778616  0.37913078 0.3796371
 0.3786288  0.37687936 0.37581217 0.3762463  0.37761405 0.37867704
 0.3786451  0.37796962 0.37760252 0.3778391  0.37800044 0.37756962
 0.3767514  0.3764942  0.37733573 0.3784106  0.37868926 0.37780985
 0.37713602 0.37774447 0.37900752 0.3787805  0.3742519  0.36538288]
