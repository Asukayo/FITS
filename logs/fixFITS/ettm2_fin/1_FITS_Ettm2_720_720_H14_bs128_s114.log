Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=5, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:5
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  53344256.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5142851
	speed: 0.1539s/iter; left time: 977.3365s
Epoch: 1 cost time: 19.798733949661255
Epoch: 1, Steps: 129 | Train Loss: 0.6042605 Vali Loss: 0.2909977 Test Loss: 0.3938048
Validation loss decreased (inf --> 0.290998).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4916472
	speed: 0.4304s/iter; left time: 2677.9639s
Epoch: 2 cost time: 21.21207046508789
Epoch: 2, Steps: 129 | Train Loss: 0.5299358 Vali Loss: 0.2760588 Test Loss: 0.3782948
Validation loss decreased (0.290998 --> 0.276059).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4823540
	speed: 0.3847s/iter; left time: 2343.9489s
Epoch: 3 cost time: 16.90292739868164
Epoch: 3, Steps: 129 | Train Loss: 0.5177957 Vali Loss: 0.2706993 Test Loss: 0.3723671
Validation loss decreased (0.276059 --> 0.270699).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5413460
	speed: 0.3392s/iter; left time: 2022.9305s
Epoch: 4 cost time: 17.038787603378296
Epoch: 4, Steps: 129 | Train Loss: 0.5114030 Vali Loss: 0.2677783 Test Loss: 0.3688733
Validation loss decreased (0.270699 --> 0.267778).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4530446
	speed: 0.3696s/iter; left time: 2156.7835s
Epoch: 5 cost time: 20.426896333694458
Epoch: 5, Steps: 129 | Train Loss: 0.5082345 Vali Loss: 0.2661790 Test Loss: 0.3667767
Validation loss decreased (0.267778 --> 0.266179).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5407225
	speed: 0.4442s/iter; left time: 2534.8149s
Epoch: 6 cost time: 20.727874994277954
Epoch: 6, Steps: 129 | Train Loss: 0.5055505 Vali Loss: 0.2651262 Test Loss: 0.3652713
Validation loss decreased (0.266179 --> 0.265126).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3740971
	speed: 0.4350s/iter; left time: 2426.0534s
Epoch: 7 cost time: 21.761160850524902
Epoch: 7, Steps: 129 | Train Loss: 0.5029027 Vali Loss: 0.2644904 Test Loss: 0.3639166
Validation loss decreased (0.265126 --> 0.264490).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5513240
	speed: 0.4357s/iter; left time: 2373.4251s
Epoch: 8 cost time: 19.76174759864807
Epoch: 8, Steps: 129 | Train Loss: 0.5022255 Vali Loss: 0.2635367 Test Loss: 0.3629541
Validation loss decreased (0.264490 --> 0.263537).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4877816
	speed: 0.4424s/iter; left time: 2352.9652s
Epoch: 9 cost time: 21.192952871322632
Epoch: 9, Steps: 129 | Train Loss: 0.5009326 Vali Loss: 0.2634142 Test Loss: 0.3623018
Validation loss decreased (0.263537 --> 0.263414).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5712675
	speed: 0.4267s/iter; left time: 2214.7966s
Epoch: 10 cost time: 20.20957589149475
Epoch: 10, Steps: 129 | Train Loss: 0.4995768 Vali Loss: 0.2630499 Test Loss: 0.3618076
Validation loss decreased (0.263414 --> 0.263050).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3853972
	speed: 0.4263s/iter; left time: 2157.4360s
Epoch: 11 cost time: 21.299720764160156
Epoch: 11, Steps: 129 | Train Loss: 0.4988694 Vali Loss: 0.2624615 Test Loss: 0.3613724
Validation loss decreased (0.263050 --> 0.262461).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5098794
	speed: 0.3793s/iter; left time: 1870.5699s
Epoch: 12 cost time: 15.306805610656738
Epoch: 12, Steps: 129 | Train Loss: 0.4983312 Vali Loss: 0.2624362 Test Loss: 0.3608247
Validation loss decreased (0.262461 --> 0.262436).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4842534
	speed: 0.3471s/iter; left time: 1667.1952s
Epoch: 13 cost time: 18.614697217941284
Epoch: 13, Steps: 129 | Train Loss: 0.4975013 Vali Loss: 0.2622969 Test Loss: 0.3605051
Validation loss decreased (0.262436 --> 0.262297).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5648803
	speed: 0.4225s/iter; left time: 1974.8133s
Epoch: 14 cost time: 20.081300258636475
Epoch: 14, Steps: 129 | Train Loss: 0.4976209 Vali Loss: 0.2621525 Test Loss: 0.3602526
Validation loss decreased (0.262297 --> 0.262152).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4645671
	speed: 0.4270s/iter; left time: 1940.7671s
Epoch: 15 cost time: 21.25812864303589
Epoch: 15, Steps: 129 | Train Loss: 0.4977506 Vali Loss: 0.2618160 Test Loss: 0.3600851
Validation loss decreased (0.262152 --> 0.261816).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4353738
	speed: 0.4234s/iter; left time: 1869.6129s
Epoch: 16 cost time: 19.86309027671814
Epoch: 16, Steps: 129 | Train Loss: 0.4971146 Vali Loss: 0.2617227 Test Loss: 0.3597605
Validation loss decreased (0.261816 --> 0.261723).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.6172180
	speed: 0.4159s/iter; left time: 1782.7542s
Epoch: 17 cost time: 19.725752353668213
Epoch: 17, Steps: 129 | Train Loss: 0.4970549 Vali Loss: 0.2616025 Test Loss: 0.3595919
Validation loss decreased (0.261723 --> 0.261603).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4110714
	speed: 0.3976s/iter; left time: 1653.4031s
Epoch: 18 cost time: 19.04148244857788
Epoch: 18, Steps: 129 | Train Loss: 0.4971024 Vali Loss: 0.2614330 Test Loss: 0.3595555
Validation loss decreased (0.261603 --> 0.261433).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5377355
	speed: 0.4166s/iter; left time: 1678.3795s
Epoch: 19 cost time: 20.98083758354187
Epoch: 19, Steps: 129 | Train Loss: 0.4962394 Vali Loss: 0.2614545 Test Loss: 0.3593301
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4777886
	speed: 0.4118s/iter; left time: 1605.8681s
Epoch: 20 cost time: 19.202786207199097
Epoch: 20, Steps: 129 | Train Loss: 0.4962349 Vali Loss: 0.2613271 Test Loss: 0.3592477
Validation loss decreased (0.261433 --> 0.261327).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5145468
	speed: 0.3991s/iter; left time: 1504.8769s
Epoch: 21 cost time: 19.217666387557983
Epoch: 21, Steps: 129 | Train Loss: 0.4954753 Vali Loss: 0.2610081 Test Loss: 0.3591381
Validation loss decreased (0.261327 --> 0.261008).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5672928
	speed: 0.4100s/iter; left time: 1493.1589s
Epoch: 22 cost time: 19.023398637771606
Epoch: 22, Steps: 129 | Train Loss: 0.4955968 Vali Loss: 0.2611618 Test Loss: 0.3590198
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4421185
	speed: 0.3957s/iter; left time: 1390.0669s
Epoch: 23 cost time: 19.356158018112183
Epoch: 23, Steps: 129 | Train Loss: 0.4959449 Vali Loss: 0.2611232 Test Loss: 0.3589455
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.6086960
	speed: 0.3751s/iter; left time: 1269.4814s
Epoch: 24 cost time: 18.55458354949951
Epoch: 24, Steps: 129 | Train Loss: 0.4957069 Vali Loss: 0.2612135 Test Loss: 0.3588059
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3501136600971222, mae:0.3785245418548584, rse:0.4756077527999878, corr:[0.53037494 0.5407804  0.5359651  0.5334934  0.5355431  0.53853947
 0.5386081  0.5366845  0.5356787  0.5364825  0.5380015  0.53842753
 0.5374291  0.536328   0.5361557  0.5367447  0.53715557 0.53661823
 0.53543854 0.534496   0.53423107 0.5343521  0.5342573  0.53368527
 0.53296036 0.5325268  0.53241783 0.53227884 0.5317203  0.5308005
 0.5299527  0.529503   0.5293568  0.52916735 0.52865547 0.5278562
 0.5270096  0.52632016 0.5257892  0.5252815  0.5246648  0.5239725
 0.5233591  0.5228593  0.52239186 0.5218203  0.52106416 0.52019066
 0.51934415 0.51863486 0.51813126 0.5177116  0.51716226 0.51644194
 0.51566136 0.514999   0.51457477 0.5143079  0.5140208  0.5136175
 0.51319265 0.51284516 0.5126485  0.51253855 0.5123713  0.5120978
 0.5116949  0.51128435 0.5108983  0.5105091  0.51008594 0.5096551
 0.50928795 0.5089916  0.50867176 0.50820005 0.5075453  0.5067823
 0.50610757 0.50557595 0.5051138  0.5045718  0.50388235 0.50309825
 0.5023788  0.5018191  0.50136447 0.5009065  0.5003144  0.49964702
 0.49905536 0.49856326 0.49806568 0.4973597  0.49628234 0.49489012
 0.49343726 0.49216706 0.49106905 0.49000698 0.48887095 0.4876797
 0.48653907 0.48553684 0.48458427 0.48351443 0.48226288 0.4809763
 0.47990885 0.4791999  0.47866306 0.47796544 0.47695372 0.47570473
 0.47454867 0.47373942 0.47314906 0.47241327 0.4713169  0.46996975
 0.46880642 0.4680984  0.4677165  0.46719676 0.46623585 0.4649069
 0.46363103 0.4627453  0.46219575 0.46164542 0.46084583 0.45979655
 0.45872983 0.4578353  0.45710883 0.45639    0.45560014 0.45486125
 0.4543079  0.45389864 0.45339182 0.45258814 0.4515305  0.45046234
 0.44969743 0.44928455 0.44902042 0.44858435 0.4479347  0.4472989
 0.44690752 0.4466318  0.44604623 0.44494656 0.44350222 0.4422008
 0.44147655 0.44131163 0.4412679  0.44094256 0.44024906 0.43948242
 0.43894252 0.43866065 0.43835577 0.4377694  0.43698147 0.43635538
 0.43613985 0.43617195 0.43597987 0.4353235  0.43431458 0.433401
 0.43292564 0.4328236  0.4326706  0.43212563 0.43125972 0.43042958
 0.42988154 0.42958212 0.42926356 0.42868835 0.42793936 0.42728767
 0.42693812 0.42666125 0.4260974  0.42500633 0.4235444  0.42206895
 0.42084706 0.41987053 0.4187849  0.41748598 0.4161847  0.4151214
 0.41432306 0.41351867 0.41237244 0.41081318 0.40912485 0.4076844
 0.406589   0.40559423 0.4043932  0.40298212 0.40173125 0.40096155
 0.40049377 0.3999237  0.39894867 0.39762607 0.3963326  0.39534372
 0.3946281  0.3938092  0.39266452 0.39123955 0.38991144 0.38902262
 0.38851377 0.38798007 0.38706157 0.38563892 0.38408694 0.38294008
 0.38232574 0.38202903 0.38166597 0.38105917 0.38028225 0.37952355
 0.37881556 0.37812033 0.3774082  0.37678325 0.37641108 0.3762453
 0.37605998 0.37559387 0.3749187  0.3744324  0.37436908 0.3747074
 0.37504634 0.37495798 0.37430355 0.37342173 0.372887   0.3728943
 0.37319788 0.3734131  0.3732737  0.3729182  0.3726056  0.3724815
 0.3723626  0.3719991  0.37133324 0.3706018  0.3700487  0.36977658
 0.36957493 0.3691987  0.36873844 0.368458   0.36850595 0.36871147
 0.36875305 0.36832118 0.36741778 0.3664638  0.36585537 0.36568537
 0.3657542  0.3656731  0.3652964  0.36481693 0.3643859  0.36408493
 0.36378086 0.3633674  0.3628574  0.36231402 0.3617565  0.36101645
 0.3600369  0.35894156 0.35786793 0.35696062 0.3562636  0.35561863
 0.35492554 0.3541321  0.3533778  0.35287133 0.3526667  0.35249913
 0.35213283 0.35137886 0.35044608 0.34952825 0.34881967 0.34822822
 0.347638   0.34695277 0.3463299  0.34598732 0.34595135 0.34609616
 0.34607577 0.34583423 0.34544578 0.34505415 0.34470943 0.3442973
 0.34366888 0.34295383 0.34244686 0.34232524 0.34247798 0.34261394
 0.34245318 0.34203747 0.34163418 0.34146717 0.34159917 0.34175798
 0.34173006 0.3414144  0.34100932 0.34076685 0.34068596 0.34066942
 0.3405457  0.34026703 0.34009394 0.34022084 0.34052822 0.3406968
 0.34053448 0.34008855 0.33962253 0.3393593  0.3393399  0.33945513
 0.33955076 0.33963016 0.33972633 0.33979797 0.33971408 0.33934155
 0.33876178 0.33826536 0.338069   0.33819956 0.33827972 0.3380494
 0.33749855 0.3369491  0.33676794 0.33701956 0.3373665  0.33743006
 0.3370112  0.33641127 0.3360319  0.33608416 0.33640236 0.33658987
 0.3364705  0.33619335 0.33599937 0.33597285 0.33595842 0.3358105
 0.33558384 0.33547854 0.33562702 0.33586475 0.3358701  0.33522978
 0.33398968 0.33256462 0.33147162 0.3308776  0.33052287 0.330111
 0.3294288  0.32860368 0.3278863  0.32737786 0.3269515  0.32639465
 0.32570523 0.3251249  0.32478413 0.32465115 0.32452664 0.32408088
 0.3232743  0.32231465 0.3215159  0.32101902 0.32065368 0.32033092
 0.32004562 0.31996176 0.320077   0.3201559  0.31993225 0.3192505
 0.31837386 0.31773707 0.31755596 0.3177318  0.3179207  0.3179053
 0.31775466 0.31768662 0.3178287  0.31810856 0.3183293  0.31836388
 0.31832397 0.31839865 0.31870508 0.31907284 0.31923655 0.3190555
 0.31866342 0.31832865 0.31830457 0.3185424  0.3187703  0.31871188
 0.3183303  0.3179249  0.31763947 0.31741318 0.31710684 0.31668913
 0.31619883 0.3158788  0.31579834 0.31578845 0.31562614 0.31514516
 0.31447682 0.31386536 0.31351894 0.31335667 0.31323555 0.3130553
 0.3127922  0.31251138 0.31224528 0.31199795 0.31181014 0.31166857
 0.3116814  0.31183156 0.3119527  0.31180406 0.31126508 0.31049854
 0.3098853  0.30968687 0.30982745 0.3099701  0.30982655 0.30926535
 0.30846322 0.3076263  0.30692366 0.30626917 0.30556154 0.30471286
 0.30375174 0.30281383 0.30185053 0.30082667 0.2997575  0.29873627
 0.29785797 0.29718155 0.29655257 0.2958484  0.29496458 0.29398733
 0.29310516 0.2924835  0.29202154 0.2915039  0.2908676  0.29016262
 0.28949532 0.28876576 0.2878541  0.28680065 0.2857499  0.2850604
 0.28481394 0.2848488  0.28482166 0.28440183 0.28363276 0.28275692
 0.28209686 0.2817207  0.28141853 0.2809756  0.28040394 0.2799097
 0.2796651  0.27959567 0.27956715 0.27930874 0.2788448  0.27848998
 0.27848843 0.27875176 0.27886373 0.27849823 0.27773646 0.2770132
 0.27674362 0.27699482 0.27738208 0.27745533 0.27711603 0.27664316
 0.27643636 0.27663797 0.27706996 0.2772817  0.2770875  0.2766888
 0.2764423  0.276513   0.27667424 0.27661818 0.2761579  0.27546284
 0.27490905 0.27473435 0.27479658 0.2747805  0.2744736  0.2740535
 0.27376744 0.27378407 0.27400273 0.27419215 0.27409363 0.27383012
 0.2735581  0.27343    0.27335045 0.27321652 0.27306515 0.27300847
 0.2731212  0.2732785  0.27330226 0.2730452  0.27258372 0.2721342
 0.2718512  0.27169254 0.2714057  0.27080724 0.2698323  0.2684717
 0.26686963 0.2653346  0.26396295 0.26280594 0.2618852  0.2612118
 0.2606773  0.26008183 0.25929072 0.25832152 0.25734028 0.25653568
 0.25586352 0.25512516 0.25426728 0.25333712 0.25252149 0.25198513
 0.25162825 0.25118434 0.2505798  0.24987277 0.2493945  0.24932311
 0.2495097  0.24968414 0.24952978 0.249057   0.24863283 0.24847646
 0.24841705 0.24820127 0.24772634 0.24705577 0.24660753 0.24663906
 0.24709219 0.24740574 0.24733408 0.24703857 0.24684423 0.24698791
 0.24741247 0.24783945 0.24793331 0.24777293 0.2477537  0.24823456
 0.24910322 0.24987428 0.2501481  0.25001407 0.24980983 0.24984902
 0.25010777 0.25038752 0.25051498 0.25035906 0.25034767 0.25055972
 0.25082356 0.25083676 0.25039718 0.24987517 0.2495393  0.24963185
 0.24992687 0.25012833 0.24991114 0.2493389  0.2489964  0.24898206
 0.24925295 0.24937831 0.2491449  0.24868585 0.24845165 0.24851611
 0.24889544 0.24917245 0.24901602 0.24865884 0.24844483 0.24853206
 0.24883622 0.24927439 0.24951904 0.24953379 0.24955362 0.24966261
 0.24978518 0.24986418 0.2497814  0.249414   0.248761   0.24786226
 0.24671549 0.24540822 0.24411054 0.24298757 0.24222957 0.24173117
 0.24126844 0.24067043 0.2402036  0.23978725 0.23947431 0.23911281
 0.23857106 0.23784961 0.2370833  0.23644611 0.23618688 0.23606668
 0.2358528  0.23532833 0.23446488 0.23373342 0.2332454  0.23294412
 0.23271166 0.23234873 0.23192835 0.23139092 0.23084825 0.23024122
 0.22969957 0.22902356 0.22841798 0.22809997 0.22794747 0.22762273
 0.22683193 0.22591996 0.22539228 0.22575624 0.22624849 0.22618261
 0.22531554 0.22441633 0.22474186 0.22594608 0.22507542 0.21784674]
