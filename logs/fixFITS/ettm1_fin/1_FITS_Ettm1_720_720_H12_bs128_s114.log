Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  40269824.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4777902
	speed: 0.1626s/iter; left time: 1032.8655s
Epoch: 1 cost time: 21.03242540359497
Epoch: 1, Steps: 129 | Train Loss: 0.5762339 Vali Loss: 1.1050233 Test Loss: 0.5070755
Validation loss decreased (inf --> 1.105023).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4412302
	speed: 0.4434s/iter; left time: 2758.9230s
Epoch: 2 cost time: 21.58270263671875
Epoch: 2, Steps: 129 | Train Loss: 0.4423435 Vali Loss: 1.0139829 Test Loss: 0.4501051
Validation loss decreased (1.105023 --> 1.013983).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4095180
	speed: 0.5243s/iter; left time: 3194.7586s
Epoch: 3 cost time: 27.37984585762024
Epoch: 3, Steps: 129 | Train Loss: 0.4198423 Vali Loss: 0.9815822 Test Loss: 0.4310941
Validation loss decreased (1.013983 --> 0.981582).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4017073
	speed: 0.5369s/iter; left time: 3202.0400s
Epoch: 4 cost time: 25.80106496810913
Epoch: 4, Steps: 129 | Train Loss: 0.4095878 Vali Loss: 0.9654071 Test Loss: 0.4222330
Validation loss decreased (0.981582 --> 0.965407).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3951632
	speed: 0.4983s/iter; left time: 2907.5880s
Epoch: 5 cost time: 24.149393796920776
Epoch: 5, Steps: 129 | Train Loss: 0.4044563 Vali Loss: 0.9551386 Test Loss: 0.4182684
Validation loss decreased (0.965407 --> 0.955139).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4209563
	speed: 0.5140s/iter; left time: 2933.1121s
Epoch: 6 cost time: 25.78160858154297
Epoch: 6, Steps: 129 | Train Loss: 0.4016067 Vali Loss: 0.9493464 Test Loss: 0.4165855
Validation loss decreased (0.955139 --> 0.949346).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3870297
	speed: 0.5463s/iter; left time: 3046.8930s
Epoch: 7 cost time: 25.947410583496094
Epoch: 7, Steps: 129 | Train Loss: 0.3999654 Vali Loss: 0.9453079 Test Loss: 0.4161204
Validation loss decreased (0.949346 --> 0.945308).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3974090
	speed: 0.5256s/iter; left time: 2863.7358s
Epoch: 8 cost time: 25.567765712738037
Epoch: 8, Steps: 129 | Train Loss: 0.3990164 Vali Loss: 0.9423677 Test Loss: 0.4159511
Validation loss decreased (0.945308 --> 0.942368).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3859938
	speed: 0.5528s/iter; left time: 2940.2764s
Epoch: 9 cost time: 27.018231630325317
Epoch: 9, Steps: 129 | Train Loss: 0.3984220 Vali Loss: 0.9410735 Test Loss: 0.4160479
Validation loss decreased (0.942368 --> 0.941074).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4019271
	speed: 0.5189s/iter; left time: 2692.9620s
Epoch: 10 cost time: 24.978006601333618
Epoch: 10, Steps: 129 | Train Loss: 0.3980525 Vali Loss: 0.9387561 Test Loss: 0.4164546
Validation loss decreased (0.941074 --> 0.938756).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4139153
	speed: 0.5121s/iter; left time: 2591.6666s
Epoch: 11 cost time: 24.566140174865723
Epoch: 11, Steps: 129 | Train Loss: 0.3978275 Vali Loss: 0.9379894 Test Loss: 0.4167444
Validation loss decreased (0.938756 --> 0.937989).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4165392
	speed: 0.4947s/iter; left time: 2440.0808s
Epoch: 12 cost time: 24.05343532562256
Epoch: 12, Steps: 129 | Train Loss: 0.3977183 Vali Loss: 0.9385463 Test Loss: 0.4167628
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3796456
	speed: 0.5081s/iter; left time: 2440.6238s
Epoch: 13 cost time: 25.043694972991943
Epoch: 13, Steps: 129 | Train Loss: 0.3974318 Vali Loss: 0.9365411 Test Loss: 0.4169660
Validation loss decreased (0.937989 --> 0.936541).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4030271
	speed: 0.5266s/iter; left time: 2461.1012s
Epoch: 14 cost time: 25.799109935760498
Epoch: 14, Steps: 129 | Train Loss: 0.3974619 Vali Loss: 0.9366951 Test Loss: 0.4169905
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3813418
	speed: 0.4777s/iter; left time: 2171.2419s
Epoch: 15 cost time: 21.914783239364624
Epoch: 15, Steps: 129 | Train Loss: 0.3972400 Vali Loss: 0.9351342 Test Loss: 0.4173228
Validation loss decreased (0.936541 --> 0.935134).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4012840
	speed: 0.4427s/iter; left time: 1954.9251s
Epoch: 16 cost time: 23.227083683013916
Epoch: 16, Steps: 129 | Train Loss: 0.3970954 Vali Loss: 0.9358961 Test Loss: 0.4172403
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3730542
	speed: 0.5056s/iter; left time: 2167.4540s
Epoch: 17 cost time: 24.63799476623535
Epoch: 17, Steps: 129 | Train Loss: 0.3971970 Vali Loss: 0.9348567 Test Loss: 0.4173732
Validation loss decreased (0.935134 --> 0.934857).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4071875
	speed: 0.4904s/iter; left time: 2039.0700s
Epoch: 18 cost time: 23.603264570236206
Epoch: 18, Steps: 129 | Train Loss: 0.3970012 Vali Loss: 0.9345605 Test Loss: 0.4174500
Validation loss decreased (0.934857 --> 0.934560).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3757749
	speed: 0.5140s/iter; left time: 2070.9152s
Epoch: 19 cost time: 26.71426749229431
Epoch: 19, Steps: 129 | Train Loss: 0.3969774 Vali Loss: 0.9340900 Test Loss: 0.4173550
Validation loss decreased (0.934560 --> 0.934090).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4037348
	speed: 0.5878s/iter; left time: 2292.4681s
Epoch: 20 cost time: 29.06609296798706
Epoch: 20, Steps: 129 | Train Loss: 0.3969535 Vali Loss: 0.9338977 Test Loss: 0.4175878
Validation loss decreased (0.934090 --> 0.933898).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3864713
	speed: 0.5848s/iter; left time: 2205.1184s
Epoch: 21 cost time: 26.31541132926941
Epoch: 21, Steps: 129 | Train Loss: 0.3969359 Vali Loss: 0.9337335 Test Loss: 0.4173859
Validation loss decreased (0.933898 --> 0.933733).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3950476
	speed: 0.5358s/iter; left time: 1951.3340s
Epoch: 22 cost time: 25.20592451095581
Epoch: 22, Steps: 129 | Train Loss: 0.3967936 Vali Loss: 0.9336770 Test Loss: 0.4174418
Validation loss decreased (0.933733 --> 0.933677).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4084373
	speed: 0.5109s/iter; left time: 1794.8091s
Epoch: 23 cost time: 25.11232280731201
Epoch: 23, Steps: 129 | Train Loss: 0.3967604 Vali Loss: 0.9336535 Test Loss: 0.4175200
Validation loss decreased (0.933677 --> 0.933654).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4290866
	speed: 0.5174s/iter; left time: 1750.8465s
Epoch: 24 cost time: 25.382771015167236
Epoch: 24, Steps: 129 | Train Loss: 0.3967756 Vali Loss: 0.9331152 Test Loss: 0.4176151
Validation loss decreased (0.933654 --> 0.933115).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3862657
	speed: 0.5080s/iter; left time: 1653.5236s
Epoch: 25 cost time: 23.9674334526062
Epoch: 25, Steps: 129 | Train Loss: 0.3967687 Vali Loss: 0.9331051 Test Loss: 0.4176504
Validation loss decreased (0.933115 --> 0.933105).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3989145
	speed: 0.5120s/iter; left time: 1600.5800s
Epoch: 26 cost time: 25.099678993225098
Epoch: 26, Steps: 129 | Train Loss: 0.3967482 Vali Loss: 0.9333310 Test Loss: 0.4176027
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4080042
	speed: 0.5178s/iter; left time: 1551.8193s
Epoch: 27 cost time: 26.290754795074463
Epoch: 27, Steps: 129 | Train Loss: 0.3967451 Vali Loss: 0.9326539 Test Loss: 0.4176156
Validation loss decreased (0.933105 --> 0.932654).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4008055
	speed: 0.5183s/iter; left time: 1486.5115s
Epoch: 28 cost time: 25.046189069747925
Epoch: 28, Steps: 129 | Train Loss: 0.3966296 Vali Loss: 0.9328700 Test Loss: 0.4177346
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3813088
	speed: 0.4929s/iter; left time: 1349.9912s
Epoch: 29 cost time: 23.263061046600342
Epoch: 29, Steps: 129 | Train Loss: 0.3966642 Vali Loss: 0.9326517 Test Loss: 0.4176055
Validation loss decreased (0.932654 --> 0.932652).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4124777
	speed: 0.4877s/iter; left time: 1272.9495s
Epoch: 30 cost time: 26.10881781578064
Epoch: 30, Steps: 129 | Train Loss: 0.3965697 Vali Loss: 0.9329346 Test Loss: 0.4177923
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3810680
	speed: 0.4786s/iter; left time: 1187.3353s
Epoch: 31 cost time: 21.257091283798218
Epoch: 31, Steps: 129 | Train Loss: 0.3967049 Vali Loss: 0.9329947 Test Loss: 0.4177557
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3883787
	speed: 0.4162s/iter; left time: 978.9325s
Epoch: 32 cost time: 20.211743116378784
Epoch: 32, Steps: 129 | Train Loss: 0.3966457 Vali Loss: 0.9321386 Test Loss: 0.4177908
Validation loss decreased (0.932652 --> 0.932139).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3789015
	speed: 0.5359s/iter; left time: 1191.2848s
Epoch: 33 cost time: 26.93661069869995
Epoch: 33, Steps: 129 | Train Loss: 0.3965305 Vali Loss: 0.9318067 Test Loss: 0.4177284
Validation loss decreased (0.932139 --> 0.931807).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3977000
	speed: 0.5720s/iter; left time: 1197.8650s
Epoch: 34 cost time: 28.149578094482422
Epoch: 34, Steps: 129 | Train Loss: 0.3965395 Vali Loss: 0.9321306 Test Loss: 0.4177551
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3946798
	speed: 0.5191s/iter; left time: 1020.0389s
Epoch: 35 cost time: 23.001763820648193
Epoch: 35, Steps: 129 | Train Loss: 0.3965511 Vali Loss: 0.9327039 Test Loss: 0.4177486
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4398026
	speed: 0.4158s/iter; left time: 763.3679s
Epoch: 36 cost time: 18.79251766204834
Epoch: 36, Steps: 129 | Train Loss: 0.3965804 Vali Loss: 0.9323170 Test Loss: 0.4177205
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4151659905910492, mae:0.4114267826080322, rse:0.6130298972129822, corr:[0.5253469  0.5302191  0.5329767  0.5341378  0.5350443  0.5363562
 0.53773713 0.5388387  0.53966767 0.5403867  0.5412217  0.5418833
 0.54210156 0.54175574 0.54098934 0.5400897  0.53928906 0.53852594
 0.5374982  0.53616863 0.5347353  0.53336084 0.5321772  0.5313513
 0.53051335 0.5295348  0.5284039  0.52729523 0.5267012  0.526869
 0.5276868  0.5287977  0.5295921  0.5298911  0.5296595  0.52943015
 0.5293235  0.52938074 0.5294105  0.52913487 0.5286795  0.5281811
 0.5278898  0.527932   0.5280627  0.52805763 0.5279259  0.52772313
 0.52751726 0.527358   0.5273394  0.52732426 0.5272396  0.5269522
 0.5265982  0.52633965 0.52626824 0.52628374 0.52633035 0.5262337
 0.5260332  0.52584577 0.52576506 0.52571476 0.52574915 0.52585244
 0.52595705 0.5260546  0.5262107  0.5263857  0.5265763  0.5266895
 0.52666086 0.52649534 0.52630776 0.5261167  0.5259304  0.52579165
 0.5257091  0.5256763  0.52564317 0.5255711  0.52547336 0.5253321
 0.5252523  0.52522284 0.5252723  0.5253549  0.52546746 0.52567154
 0.5259507  0.5262644  0.5265716  0.52679557 0.5268858  0.5267507
 0.5264289  0.52603483 0.525534   0.5251021  0.5247993  0.52462715
 0.5245478  0.5244278  0.52424735 0.524075   0.52386993 0.52371615
 0.5235876  0.5234942  0.5233702  0.52313626 0.5227918  0.5224082
 0.52203995 0.52175474 0.5215633  0.5213851  0.5211576  0.5209143
 0.5206713  0.5204728  0.5203214  0.5202791  0.5202715  0.5201217
 0.5197783  0.5193774  0.51906145 0.5189799  0.51906514 0.51918775
 0.5192669  0.51923406 0.5190623  0.51889086 0.5188749  0.5188968
 0.5189081  0.5188764  0.5187439  0.5186346  0.5186907  0.5189524
 0.51930207 0.5195063  0.51950777 0.5193455  0.5191925  0.51914024
 0.51920706 0.51937586 0.51951593 0.51953757 0.5194368  0.51936024
 0.51936287 0.5194387  0.5195645  0.51960635 0.5196153  0.5196304
 0.51972103 0.5199082  0.5201653  0.5204037  0.5205563  0.5206142
 0.5205677  0.5204617  0.5204573  0.5206113  0.5208196  0.5209905
 0.5210545  0.5209887  0.52086294 0.52078    0.5207959  0.52093524
 0.5211248  0.5212729  0.52128214 0.5212089  0.5211364  0.5211519
 0.5213166  0.52164084 0.5220224  0.52232075 0.52244985 0.5223722
 0.52210873 0.5217481  0.52127886 0.5207096  0.5200666  0.51939046
 0.5186771  0.517965   0.51724935 0.51657164 0.5159066  0.5152356
 0.5145516  0.5138618  0.51320153 0.5126401  0.51218563 0.51179665
 0.5113733  0.5108871  0.5103118  0.5096615  0.50893563 0.50827473
 0.5077693  0.50737214 0.5070611  0.5067724  0.5065164  0.50639534
 0.5065155  0.50675726 0.5070584  0.5073273  0.507444   0.507419
 0.50732636 0.50725555 0.50724614 0.50725514 0.5072737  0.50730264
 0.5073373  0.5074213  0.5074944  0.50767106 0.5078741  0.50808305
 0.50820214 0.50813717 0.5079641  0.50781125 0.50777    0.5078046
 0.50786304 0.50788933 0.5078327  0.5077234  0.50756496 0.5074696
 0.50740415 0.5074322  0.50748825 0.5075287  0.50755304 0.5075764
 0.5076145  0.50768906 0.5077822  0.5078783  0.50797176 0.5081314
 0.50826406 0.50836784 0.5084387  0.5085106  0.50855905 0.50853443
 0.50846475 0.50836337 0.5082834  0.50825036 0.5082519  0.508266
 0.5082261  0.5081851  0.50809413 0.5080479  0.5080525  0.50811654
 0.5082808  0.5084326  0.50848544 0.5083928  0.50811154 0.5076679
 0.50715494 0.50668347 0.5062458  0.5057583  0.50519323 0.5045846
 0.5039987  0.5034587  0.5030466  0.50270414 0.50234884 0.5019366
 0.5014329  0.5009064  0.50046456 0.50013894 0.49991858 0.4997352
 0.4996218  0.49948114 0.49928954 0.49906406 0.49888834 0.49875316
 0.49866503 0.49864885 0.49862432 0.49856883 0.4984856  0.49843383
 0.49842364 0.49842378 0.49842304 0.49839455 0.49830776 0.49814036
 0.49791464 0.49772388 0.49757716 0.4975069  0.49747166 0.49744138
 0.49746448 0.4975054  0.4975236  0.49753538 0.49752536 0.4975629
 0.49759734 0.49752045 0.49745882 0.4974531  0.49752367 0.4976055
 0.49759653 0.49750432 0.49733996 0.49717903 0.49709502 0.49714598
 0.49723825 0.497337   0.49736273 0.4972689  0.49711883 0.49696288
 0.49683017 0.49676916 0.49676964 0.49680683 0.49688753 0.49699348
 0.49704355 0.49707416 0.49705675 0.4969903  0.49688685 0.49676445
 0.49659798 0.49648517 0.4964579  0.49645334 0.49646765 0.4964702
 0.4964547  0.49645987 0.49652222 0.4966875  0.49692523 0.49721283
 0.49751055 0.49778312 0.49800327 0.49814785 0.49820727 0.4981778
 0.49802408 0.49772575 0.49731994 0.49682626 0.49627262 0.49582553
 0.49548578 0.49522525 0.49497637 0.4947185  0.49438578 0.4940133
 0.49364418 0.49331224 0.49300775 0.49268472 0.49226832 0.49186182
 0.49149808 0.49127418 0.49111986 0.49100745 0.49086368 0.49068016
 0.49043941 0.49020648 0.49003437 0.48998618 0.49007192 0.49018705
 0.49039036 0.49052447 0.4906332  0.49065214 0.49065268 0.49070925
 0.49075258 0.49081054 0.49081331 0.49078226 0.490713   0.49070463
 0.49074945 0.49085814 0.4909537  0.49108988 0.49119255 0.49133512
 0.49141988 0.49136844 0.491223   0.49106055 0.4909042  0.49078554
 0.49069652 0.49063167 0.49059474 0.49057323 0.49056795 0.49057803
 0.49055666 0.4905261  0.4904727  0.4904206  0.49039695 0.49039513
 0.49047375 0.49060142 0.4907385  0.4908269  0.4908463  0.49087653
 0.4908488  0.49082783 0.49080753 0.4908001  0.4907108  0.49058872
 0.49045235 0.49040005 0.49041572 0.4905122  0.4906219  0.49066353
 0.49062037 0.4904781  0.49032205 0.49022087 0.49021748 0.49029663
 0.49038005 0.4904501  0.49044105 0.49028075 0.48995626 0.48952118
 0.48902702 0.48853278 0.48806018 0.4875651  0.48701862 0.486467
 0.4859579  0.4854083  0.48483896 0.48427713 0.48367065 0.48303032
 0.48239863 0.48185936 0.48139507 0.48093528 0.48045802 0.48000103
 0.47956216 0.47920978 0.47894445 0.47863716 0.4784052  0.47821507
 0.47811228 0.47808    0.47807315 0.47813803 0.4782326  0.478371
 0.47853297 0.4786458  0.47871074 0.47884703 0.4790244  0.47920343
 0.47930652 0.47938147 0.47945806 0.4795064  0.4795627  0.47969443
 0.47987932 0.48005134 0.48014912 0.4801327  0.48013458 0.4802689
 0.48048648 0.4806704  0.48071343 0.48066497 0.48057535 0.48048028
 0.480437   0.48045477 0.48053148 0.4806248  0.48066148 0.48060268
 0.48047912 0.48034537 0.48027027 0.48025826 0.48027277 0.48031318
 0.48033202 0.48033482 0.48034495 0.4803387  0.4803409  0.48043218
 0.48049614 0.48050076 0.48045078 0.48038185 0.4802938  0.48024708
 0.48024437 0.48028895 0.48034453 0.48039055 0.48042265 0.4804272
 0.48039865 0.48043427 0.48043823 0.4804386  0.4804572  0.48053655
 0.4806985  0.48090848 0.48111847 0.48117694 0.4809801  0.48051292
 0.47990513 0.4793474  0.47889522 0.478485   0.478035   0.47757557
 0.47707653 0.47659206 0.476148   0.47576353 0.47542566 0.47502846
 0.47454837 0.47394615 0.4732653  0.4725745  0.47200167 0.47155952
 0.47121787 0.47090766 0.47063273 0.4703834  0.47014055 0.4698891
 0.46966887 0.46945933 0.46933684 0.46934327 0.4694894  0.46966287
 0.46987882 0.47003868 0.47015095 0.47026122 0.47038555 0.4705755
 0.47079617 0.47093967 0.47104988 0.4711196  0.47119334 0.47127536
 0.4713973  0.47156024 0.4716499  0.47170845 0.4718912  0.47219187
 0.47244358 0.47261533 0.47265354 0.47256842 0.47250578 0.4725139
 0.47249734 0.47249255 0.47247985 0.4723766  0.47224325 0.4720478
 0.47177646 0.47151724 0.47134346 0.47116265 0.47103274 0.4709036
 0.4707744  0.47064757 0.47055134 0.4705146  0.47051844 0.4705846
 0.47062385 0.47051704 0.47033632 0.47016796 0.47006705 0.47004136
 0.4700722  0.47010988 0.47012946 0.47014168 0.47015724 0.47016472
 0.47015893 0.4701917  0.47022003 0.47018045 0.47019783 0.47031176
 0.47055683 0.47088718 0.4712026  0.4713989  0.47133812 0.47102302
 0.47059333 0.47025713 0.4700138  0.46972474 0.46930972 0.46880785
 0.4683413  0.46794352 0.46771908 0.46755955 0.4672465  0.46692228
 0.46649238 0.46617454 0.46596417 0.46589637 0.4657949  0.46550196
 0.46505392 0.4645571  0.46423006 0.4640319  0.46399394 0.4640397
 0.46400675 0.4639483  0.46387592 0.46384493 0.4639807  0.46414462
 0.46435916 0.464525   0.46467313 0.46496615 0.4652976  0.46571228
 0.46612826 0.466446   0.466601   0.46664622 0.4668985  0.46740863
 0.46820197 0.46898243 0.4693436  0.46949703 0.46878597 0.46456778]
