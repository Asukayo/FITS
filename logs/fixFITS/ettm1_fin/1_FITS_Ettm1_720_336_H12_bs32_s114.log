Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7360640.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3779634
	speed: 0.1581s/iter; left time: 4119.0318s
	iters: 200, epoch: 1 | loss: 0.3396130
	speed: 0.1518s/iter; left time: 3938.9001s
	iters: 300, epoch: 1 | loss: 0.3597206
	speed: 0.1523s/iter; left time: 3936.8198s
	iters: 400, epoch: 1 | loss: 0.3518519
	speed: 0.1555s/iter; left time: 4003.5727s
	iters: 500, epoch: 1 | loss: 0.3428225
	speed: 0.1554s/iter; left time: 3985.1846s
Epoch: 1 cost time: 80.97881650924683
Epoch: 1, Steps: 523 | Train Loss: 0.3836606 Vali Loss: 0.6867527 Test Loss: 0.3669536
Validation loss decreased (inf --> 0.686753).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3252272
	speed: 1.1231s/iter; left time: 28671.6152s
	iters: 200, epoch: 2 | loss: 0.3159457
	speed: 0.1623s/iter; left time: 4126.0904s
	iters: 300, epoch: 2 | loss: 0.3302833
	speed: 0.1520s/iter; left time: 3850.2865s
	iters: 400, epoch: 2 | loss: 0.3834018
	speed: 0.1534s/iter; left time: 3870.6103s
	iters: 500, epoch: 2 | loss: 0.3099946
	speed: 0.1506s/iter; left time: 3785.2517s
Epoch: 2 cost time: 83.15174150466919
Epoch: 2, Steps: 523 | Train Loss: 0.3408440 Vali Loss: 0.6677334 Test Loss: 0.3667502
Validation loss decreased (0.686753 --> 0.667733).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3471403
	speed: 1.0428s/iter; left time: 26075.9427s
	iters: 200, epoch: 3 | loss: 0.3306614
	speed: 0.1177s/iter; left time: 2932.5241s
	iters: 300, epoch: 3 | loss: 0.3049569
	speed: 0.1320s/iter; left time: 3275.1155s
	iters: 400, epoch: 3 | loss: 0.2889700
	speed: 0.1544s/iter; left time: 3814.7760s
	iters: 500, epoch: 3 | loss: 0.3946512
	speed: 0.1565s/iter; left time: 3849.8194s
Epoch: 3 cost time: 74.60827231407166
Epoch: 3, Steps: 523 | Train Loss: 0.3386131 Vali Loss: 0.6637959 Test Loss: 0.3669857
Validation loss decreased (0.667733 --> 0.663796).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3282327
	speed: 1.1687s/iter; left time: 28612.1523s
	iters: 200, epoch: 4 | loss: 0.4417757
	speed: 0.2178s/iter; left time: 5309.4969s
	iters: 300, epoch: 4 | loss: 0.3069070
	speed: 0.2251s/iter; left time: 5465.8604s
	iters: 400, epoch: 4 | loss: 0.3242870
	speed: 0.2221s/iter; left time: 5371.4288s
	iters: 500, epoch: 4 | loss: 0.3209341
	speed: 0.2044s/iter; left time: 4922.5774s
Epoch: 4 cost time: 112.09635186195374
Epoch: 4, Steps: 523 | Train Loss: 0.3378456 Vali Loss: 0.6606987 Test Loss: 0.3669445
Validation loss decreased (0.663796 --> 0.660699).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3055999
	speed: 1.2686s/iter; left time: 30393.7095s
	iters: 200, epoch: 5 | loss: 0.3636181
	speed: 0.1941s/iter; left time: 4631.4544s
	iters: 300, epoch: 5 | loss: 0.3495663
	speed: 0.1895s/iter; left time: 4501.3431s
	iters: 400, epoch: 5 | loss: 0.3387026
	speed: 0.1911s/iter; left time: 4522.4019s
	iters: 500, epoch: 5 | loss: 0.3533542
	speed: 0.1840s/iter; left time: 4334.1637s
Epoch: 5 cost time: 101.29555559158325
Epoch: 5, Steps: 523 | Train Loss: 0.3374193 Vali Loss: 0.6585385 Test Loss: 0.3666206
Validation loss decreased (0.660699 --> 0.658538).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3246185
	speed: 1.4099s/iter; left time: 33043.0316s
	iters: 200, epoch: 6 | loss: 0.3156865
	speed: 0.2105s/iter; left time: 4911.1611s
	iters: 300, epoch: 6 | loss: 0.3254964
	speed: 0.2003s/iter; left time: 4653.2162s
	iters: 400, epoch: 6 | loss: 0.3636850
	speed: 0.1864s/iter; left time: 4311.5749s
	iters: 500, epoch: 6 | loss: 0.3093576
	speed: 0.1901s/iter; left time: 4379.2124s
Epoch: 6 cost time: 105.59020447731018
Epoch: 6, Steps: 523 | Train Loss: 0.3372169 Vali Loss: 0.6568742 Test Loss: 0.3668060
Validation loss decreased (0.658538 --> 0.656874).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3404456
	speed: 1.2150s/iter; left time: 27840.2089s
	iters: 200, epoch: 7 | loss: 0.3140086
	speed: 0.0928s/iter; left time: 2118.0384s
	iters: 300, epoch: 7 | loss: 0.3314947
	speed: 0.0991s/iter; left time: 2250.3656s
	iters: 400, epoch: 7 | loss: 0.3258707
	speed: 0.0731s/iter; left time: 1653.1383s
	iters: 500, epoch: 7 | loss: 0.3840620
	speed: 0.0928s/iter; left time: 2090.3097s
Epoch: 7 cost time: 55.27415204048157
Epoch: 7, Steps: 523 | Train Loss: 0.3370168 Vali Loss: 0.6574685 Test Loss: 0.3673778
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3303913
	speed: 0.8292s/iter; left time: 18566.3459s
	iters: 200, epoch: 8 | loss: 0.3714389
	speed: 0.1502s/iter; left time: 3347.3282s
	iters: 300, epoch: 8 | loss: 0.3800766
	speed: 0.1482s/iter; left time: 3289.4317s
	iters: 400, epoch: 8 | loss: 0.3358447
	speed: 0.1459s/iter; left time: 3223.4311s
	iters: 500, epoch: 8 | loss: 0.3581333
	speed: 0.1465s/iter; left time: 3220.8150s
Epoch: 8 cost time: 77.52690029144287
Epoch: 8, Steps: 523 | Train Loss: 0.3369082 Vali Loss: 0.6562291 Test Loss: 0.3666435
Validation loss decreased (0.656874 --> 0.656229).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3093725
	speed: 0.9277s/iter; left time: 20285.2273s
	iters: 200, epoch: 9 | loss: 0.3585059
	speed: 0.1282s/iter; left time: 2789.4444s
	iters: 300, epoch: 9 | loss: 0.2936224
	speed: 0.1335s/iter; left time: 2893.1298s
	iters: 400, epoch: 9 | loss: 0.3266033
	speed: 0.1168s/iter; left time: 2518.5092s
	iters: 500, epoch: 9 | loss: 0.3211261
	speed: 0.0712s/iter; left time: 1528.7173s
Epoch: 9 cost time: 59.998051166534424
Epoch: 9, Steps: 523 | Train Loss: 0.3368358 Vali Loss: 0.6544139 Test Loss: 0.3673386
Validation loss decreased (0.656229 --> 0.654414).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3180876
	speed: 0.4232s/iter; left time: 9032.7938s
	iters: 200, epoch: 10 | loss: 0.4181167
	speed: 0.1354s/iter; left time: 2875.7783s
	iters: 300, epoch: 10 | loss: 0.3532615
	speed: 0.1545s/iter; left time: 3266.1930s
	iters: 400, epoch: 10 | loss: 0.2762083
	speed: 0.1553s/iter; left time: 3268.9514s
	iters: 500, epoch: 10 | loss: 0.3500086
	speed: 0.1642s/iter; left time: 3439.9698s
Epoch: 10 cost time: 78.10134243965149
Epoch: 10, Steps: 523 | Train Loss: 0.3367024 Vali Loss: 0.6542180 Test Loss: 0.3666596
Validation loss decreased (0.654414 --> 0.654218).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3763693
	speed: 1.1923s/iter; left time: 24824.9742s
	iters: 200, epoch: 11 | loss: 0.3454519
	speed: 0.1767s/iter; left time: 3662.4288s
	iters: 300, epoch: 11 | loss: 0.3594227
	speed: 0.1436s/iter; left time: 2960.7054s
	iters: 400, epoch: 11 | loss: 0.3437219
	speed: 0.1317s/iter; left time: 2703.2221s
	iters: 500, epoch: 11 | loss: 0.3103600
	speed: 0.1342s/iter; left time: 2739.6419s
Epoch: 11 cost time: 81.05708050727844
Epoch: 11, Steps: 523 | Train Loss: 0.3366765 Vali Loss: 0.6548902 Test Loss: 0.3670677
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2648746
	speed: 1.0721s/iter; left time: 21761.1411s
	iters: 200, epoch: 12 | loss: 0.2926310
	speed: 0.1687s/iter; left time: 3406.7699s
	iters: 300, epoch: 12 | loss: 0.3321951
	speed: 0.1644s/iter; left time: 3304.6424s
	iters: 400, epoch: 12 | loss: 0.3305725
	speed: 0.1672s/iter; left time: 3343.2891s
	iters: 500, epoch: 12 | loss: 0.3548535
	speed: 0.1633s/iter; left time: 3249.3069s
Epoch: 12 cost time: 87.16692304611206
Epoch: 12, Steps: 523 | Train Loss: 0.3365864 Vali Loss: 0.6531267 Test Loss: 0.3676977
Validation loss decreased (0.654218 --> 0.653127).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3059118
	speed: 1.0993s/iter; left time: 21738.3267s
	iters: 200, epoch: 13 | loss: 0.3147846
	speed: 0.1698s/iter; left time: 3340.0668s
	iters: 300, epoch: 13 | loss: 0.3199081
	speed: 0.1710s/iter; left time: 3347.4114s
	iters: 400, epoch: 13 | loss: 0.3055134
	speed: 0.1621s/iter; left time: 3156.6003s
	iters: 500, epoch: 13 | loss: 0.3375680
	speed: 0.1518s/iter; left time: 2940.1598s
Epoch: 13 cost time: 87.23091530799866
Epoch: 13, Steps: 523 | Train Loss: 0.3365780 Vali Loss: 0.6547055 Test Loss: 0.3663158
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3567652
	speed: 1.0814s/iter; left time: 20818.6889s
	iters: 200, epoch: 14 | loss: 0.3376317
	speed: 0.1477s/iter; left time: 2829.2055s
	iters: 300, epoch: 14 | loss: 0.3330311
	speed: 0.1588s/iter; left time: 3024.8582s
	iters: 400, epoch: 14 | loss: 0.3629089
	speed: 0.1555s/iter; left time: 2947.7590s
	iters: 500, epoch: 14 | loss: 0.3373115
	speed: 0.1404s/iter; left time: 2645.9426s
Epoch: 14 cost time: 79.695565700531
Epoch: 14, Steps: 523 | Train Loss: 0.3365067 Vali Loss: 0.6549721 Test Loss: 0.3667093
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2824048
	speed: 0.9943s/iter; left time: 18623.1453s
	iters: 200, epoch: 15 | loss: 0.3591596
	speed: 0.1712s/iter; left time: 3189.6510s
	iters: 300, epoch: 15 | loss: 0.4049910
	speed: 0.1693s/iter; left time: 3136.5543s
	iters: 400, epoch: 15 | loss: 0.3439557
	speed: 0.1722s/iter; left time: 3173.3280s
	iters: 500, epoch: 15 | loss: 0.3029675
	speed: 0.1833s/iter; left time: 3359.4957s
Epoch: 15 cost time: 92.4115400314331
Epoch: 15, Steps: 523 | Train Loss: 0.3363985 Vali Loss: 0.6542299 Test Loss: 0.3666213
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3675921559333801, mae:0.38545486330986023, rse:0.5769410729408264, corr:[0.53785986 0.5460275  0.5497491  0.5508337  0.5518106  0.5536319
 0.5557144  0.55729574 0.55819815 0.558691   0.5592369  0.5597909
 0.5601568  0.5601035  0.5596002  0.55877995 0.5578174  0.55674934
 0.5554514  0.553982   0.552542   0.551185   0.54991597 0.54885995
 0.54777807 0.5466898  0.54562867 0.54465395 0.54406244 0.5439681
 0.5443125  0.5449452  0.5455106  0.5459435  0.54614145 0.546384
 0.5465465  0.54659843 0.5464777  0.5460876  0.5456664  0.5452871
 0.54501635 0.5448666  0.5446377  0.5442964  0.54406756 0.544069
 0.5442396  0.5444012  0.5445009  0.5444439  0.54429126 0.5440564
 0.54389405 0.54382426 0.5437297  0.5434155  0.5429185  0.5422891
 0.5417641  0.5415261  0.5415574  0.54161656 0.5416791  0.54174167
 0.54183614 0.5420432  0.54239434 0.5427288  0.54288816 0.5427687
 0.54242563 0.5420563  0.54188323 0.5419059  0.5419725  0.5419373
 0.54172206 0.5413926  0.5411056  0.541011   0.54118526 0.5415108
 0.5418592  0.54202765 0.5419208  0.5415279  0.54101396 0.5406355
 0.5404961  0.5405829  0.540805   0.5410416  0.54114515 0.54101783
 0.5407055  0.5403867  0.540075   0.5399021  0.5398889  0.5399422
 0.5399311  0.5397073  0.5393745  0.5391335  0.53905255 0.5392276
 0.53948915 0.5396418  0.5394897  0.5389585  0.5381939  0.53748864
 0.5370517  0.5369581  0.5370995  0.53721076 0.53710264 0.5367893
 0.53637296 0.536011   0.53577286 0.53572875 0.53577024 0.5357009
 0.5355039  0.5353592  0.5353899  0.53566486 0.5360274  0.5363009
 0.5364124  0.5363234  0.536063   0.53582555 0.535783   0.53579205
 0.5357879  0.53571993 0.53553075 0.53538257 0.5354561  0.5357985
 0.5362539  0.5365153  0.5364596  0.53609794 0.5356515  0.53530514
 0.5351871  0.535334   0.53556633 0.53568196 0.5355717  0.53533226
 0.53503686 0.5347428  0.53456604 0.53445745 0.5344818  0.53459215
 0.53475964 0.5349179  0.5350632  0.5352138  0.53540444 0.53565633
 0.5358948  0.5360319  0.536134   0.5362428  0.53633124 0.5364387
 0.53659475 0.536794   0.53698605 0.53712046 0.5371241  0.53702146
 0.536827   0.536603   0.53637284 0.5362407  0.5362498  0.5363843
 0.53661907 0.5369576  0.5373489  0.5376705  0.53783166 0.53781116
 0.5376096  0.5373711  0.53710306 0.53680015 0.53644335 0.5360004
 0.5353771  0.53457975 0.5336851  0.53286004 0.53220725 0.531743
 0.5313878  0.53096807 0.53034276 0.5295343  0.5286478  0.52781916
 0.5271294  0.52666074 0.5263565  0.5260947  0.52568245 0.5251761
 0.52469933 0.52428985 0.52403176 0.5238845  0.5238373  0.52390766
 0.5241067  0.52426714 0.52436703 0.5243965  0.5243029  0.52413416
 0.5239852  0.52393925 0.5239791  0.52399105 0.5239466  0.5238695
 0.52376884 0.52373207 0.5236875  0.5237969  0.52401704 0.5243538
 0.5247038  0.52487993 0.5248817  0.5247771  0.52467537 0.5245756
 0.5244875  0.52441674 0.52430797 0.5241904  0.5240191  0.523868
 0.52367455 0.52354693 0.5234526  0.52339035 0.5233915  0.52345973
 0.52356905 0.523696   0.52376693 0.5237674  0.523744   0.5238516
 0.5240356  0.5242214  0.52433896 0.52438676 0.5243691  0.52428913
 0.5242538  0.5242762  0.52435493 0.524466   0.524541   0.5245835
 0.5245708  0.5245843  0.5245657  0.5245563  0.52447885 0.52433187
 0.52421975 0.5242189  0.52433544 0.52457017 0.5247922  0.52480435
 0.52453005 0.5240753  0.52350646 0.5228022  0.5220745  0.5214136
 0.52088475 0.52038896 0.5199415  0.5194357  0.51885414 0.5182364
 0.51761335 0.517062   0.51664484 0.5162879  0.51586884 0.5152716
 0.5146507  0.51406246 0.51366776 0.513551   0.5137338  0.5139997
 0.51414156 0.5140764  0.5137555  0.5133698  0.5131406  0.5131956
 0.5134232  0.51353586 0.5134315  0.5132094  0.5130395  0.51299596
 0.51297516 0.51288205 0.5125387  0.51206595 0.51170456 0.51168036
 0.5120607  0.5125293  0.5127909  0.5128943  0.51268214 0.5107909 ]
