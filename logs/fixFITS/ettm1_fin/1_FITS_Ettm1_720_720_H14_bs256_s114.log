Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  106688512.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 12.204695701599121
Epoch: 1, Steps: 64 | Train Loss: 0.6496475 Vali Loss: 1.2468858 Test Loss: 0.5920128
Validation loss decreased (inf --> 1.246886).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 12.18583869934082
Epoch: 2, Steps: 64 | Train Loss: 0.4862396 Vali Loss: 1.0953839 Test Loss: 0.4902896
Validation loss decreased (1.246886 --> 1.095384).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.546513795852661
Epoch: 3, Steps: 64 | Train Loss: 0.4455398 Vali Loss: 1.0391873 Test Loss: 0.4582810
Validation loss decreased (1.095384 --> 1.039187).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 12.197832345962524
Epoch: 4, Steps: 64 | Train Loss: 0.4294581 Vali Loss: 1.0103022 Test Loss: 0.4436235
Validation loss decreased (1.039187 --> 1.010302).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 12.503066062927246
Epoch: 5, Steps: 64 | Train Loss: 0.4205802 Vali Loss: 0.9942884 Test Loss: 0.4352526
Validation loss decreased (1.010302 --> 0.994288).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.594982385635376
Epoch: 6, Steps: 64 | Train Loss: 0.4144802 Vali Loss: 0.9815539 Test Loss: 0.4295277
Validation loss decreased (0.994288 --> 0.981554).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.799869060516357
Epoch: 7, Steps: 64 | Train Loss: 0.4104401 Vali Loss: 0.9735416 Test Loss: 0.4259498
Validation loss decreased (0.981554 --> 0.973542).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.514476776123047
Epoch: 8, Steps: 64 | Train Loss: 0.4074817 Vali Loss: 0.9667211 Test Loss: 0.4233710
Validation loss decreased (0.973542 --> 0.966721).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.266798973083496
Epoch: 9, Steps: 64 | Train Loss: 0.4053261 Vali Loss: 0.9616532 Test Loss: 0.4215737
Validation loss decreased (0.966721 --> 0.961653).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.61255145072937
Epoch: 10, Steps: 64 | Train Loss: 0.4036584 Vali Loss: 0.9564008 Test Loss: 0.4204043
Validation loss decreased (0.961653 --> 0.956401).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.871594429016113
Epoch: 11, Steps: 64 | Train Loss: 0.4023821 Vali Loss: 0.9540651 Test Loss: 0.4197078
Validation loss decreased (0.956401 --> 0.954065).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.134719371795654
Epoch: 12, Steps: 64 | Train Loss: 0.4014692 Vali Loss: 0.9522155 Test Loss: 0.4192700
Validation loss decreased (0.954065 --> 0.952215).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.555959939956665
Epoch: 13, Steps: 64 | Train Loss: 0.4006938 Vali Loss: 0.9487852 Test Loss: 0.4189380
Validation loss decreased (0.952215 --> 0.948785).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.858068943023682
Epoch: 14, Steps: 64 | Train Loss: 0.3998304 Vali Loss: 0.9474626 Test Loss: 0.4186643
Validation loss decreased (0.948785 --> 0.947463).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.292480945587158
Epoch: 15, Steps: 64 | Train Loss: 0.3993180 Vali Loss: 0.9461093 Test Loss: 0.4186978
Validation loss decreased (0.947463 --> 0.946109).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 10.88466191291809
Epoch: 16, Steps: 64 | Train Loss: 0.3992244 Vali Loss: 0.9450517 Test Loss: 0.4186742
Validation loss decreased (0.946109 --> 0.945052).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.872411012649536
Epoch: 17, Steps: 64 | Train Loss: 0.3987659 Vali Loss: 0.9437984 Test Loss: 0.4188281
Validation loss decreased (0.945052 --> 0.943798).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.41411542892456
Epoch: 18, Steps: 64 | Train Loss: 0.3984743 Vali Loss: 0.9425278 Test Loss: 0.4187638
Validation loss decreased (0.943798 --> 0.942528).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.78853464126587
Epoch: 19, Steps: 64 | Train Loss: 0.3981023 Vali Loss: 0.9424180 Test Loss: 0.4189051
Validation loss decreased (0.942528 --> 0.942418).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 11.450501680374146
Epoch: 20, Steps: 64 | Train Loss: 0.3980456 Vali Loss: 0.9409535 Test Loss: 0.4189684
Validation loss decreased (0.942418 --> 0.940953).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.732993841171265
Epoch: 21, Steps: 64 | Train Loss: 0.3977353 Vali Loss: 0.9408526 Test Loss: 0.4191670
Validation loss decreased (0.940953 --> 0.940853).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 11.632676601409912
Epoch: 22, Steps: 64 | Train Loss: 0.3977591 Vali Loss: 0.9405149 Test Loss: 0.4192980
Validation loss decreased (0.940853 --> 0.940515).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 11.365427255630493
Epoch: 23, Steps: 64 | Train Loss: 0.3974668 Vali Loss: 0.9405887 Test Loss: 0.4194410
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 10.86316442489624
Epoch: 24, Steps: 64 | Train Loss: 0.3975293 Vali Loss: 0.9398050 Test Loss: 0.4194708
Validation loss decreased (0.940515 --> 0.939805).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 11.460731983184814
Epoch: 25, Steps: 64 | Train Loss: 0.3975678 Vali Loss: 0.9398292 Test Loss: 0.4195129
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 11.021018743515015
Epoch: 26, Steps: 64 | Train Loss: 0.3973609 Vali Loss: 0.9391986 Test Loss: 0.4195534
Validation loss decreased (0.939805 --> 0.939199).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 11.215865850448608
Epoch: 27, Steps: 64 | Train Loss: 0.3972852 Vali Loss: 0.9382203 Test Loss: 0.4195454
Validation loss decreased (0.939199 --> 0.938220).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 11.530645608901978
Epoch: 28, Steps: 64 | Train Loss: 0.3971681 Vali Loss: 0.9370691 Test Loss: 0.4196906
Validation loss decreased (0.938220 --> 0.937069).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 12.199153184890747
Epoch: 29, Steps: 64 | Train Loss: 0.3972225 Vali Loss: 0.9380368 Test Loss: 0.4197868
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 11.006320238113403
Epoch: 30, Steps: 64 | Train Loss: 0.3971127 Vali Loss: 0.9372269 Test Loss: 0.4197920
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 11.674447536468506
Epoch: 31, Steps: 64 | Train Loss: 0.3971711 Vali Loss: 0.9375154 Test Loss: 0.4197695
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41391652822494507, mae:0.4109659492969513, rse:0.6121067404747009, corr:[0.5238687  0.530824   0.5347001  0.5356949  0.53655523 0.5379145
 0.5391931  0.5400012  0.54052365 0.54104245 0.54145944 0.5414213
 0.5410581  0.54071987 0.5405393  0.54019153 0.5392112  0.53762996
 0.53595114 0.53479564 0.53429    0.5338166  0.5327259  0.53110117
 0.52937317 0.52839696 0.52834725 0.5284974  0.5282715  0.5276456
 0.5272513  0.527849   0.5292626  0.53064543 0.5310309  0.53051037
 0.52961344 0.5291905  0.529392   0.52962637 0.52947134 0.5288393
 0.5282325  0.5281835  0.5284906  0.5286365  0.52841675 0.5279627
 0.52762085 0.5275788  0.5277097  0.5276269  0.5272548  0.5267508
 0.5265052  0.5266232  0.5268238  0.5267364  0.5263524  0.52585244
 0.5256225  0.5257846  0.5260783  0.52614534 0.5260635  0.5260541
 0.5262588  0.52660096 0.5268621  0.5268698  0.52674574 0.52665156
 0.5266941  0.5268167  0.5268954  0.52678216 0.52648854 0.5261927
 0.5260452  0.5260979  0.52624506 0.5263219  0.5262281  0.52597684
 0.525758   0.52562344 0.5256106  0.52565587 0.5257402  0.52592015
 0.5262107  0.5265607  0.5269139  0.52717215 0.52727944 0.52716607
 0.52688116 0.52652586 0.5260703  0.5256925  0.52542454 0.5252515
 0.5251156  0.5249071  0.52466196 0.52448386 0.5243287  0.5242273
 0.52407503 0.52386093 0.5235977  0.5233262  0.5231205  0.5230094
 0.522914   0.5227476  0.5224548  0.5220697  0.52173585 0.5216118
 0.5216282  0.5216022  0.52138066 0.52109975 0.5209517  0.5209519
 0.5209696  0.5208481  0.5205019  0.52012837 0.5199502  0.5200542
 0.5203069  0.52039796 0.52011174 0.5196049  0.51925516 0.51918644
 0.5193562  0.519538   0.5195051  0.51934713 0.51930016 0.5194928
 0.5198059  0.51999426 0.5199918  0.51986223 0.5197528  0.5197097
 0.51971227 0.5197508  0.5197731  0.51975983 0.51971644 0.5197074
 0.5196863  0.51965356 0.5196975  0.5198267  0.5200845  0.52035487
 0.520516   0.5205448  0.5205418  0.5206248  0.5208346  0.5211221
 0.5213312  0.52138996 0.52139133 0.52139467 0.52139485 0.52141213
 0.52146935 0.5215307  0.5215604  0.52153593 0.5214823  0.5214654
 0.52151215 0.52161336 0.52170306 0.5217755  0.52185404 0.52197236
 0.52219564 0.5225354  0.5228833  0.5230941  0.52312195 0.52294827
 0.5226505  0.5222946  0.5218246  0.52121073 0.5204809  0.5197281
 0.51902115 0.5184031  0.51780057 0.51716006 0.516428   0.51565224
 0.51493317 0.5143255  0.51381034 0.51335317 0.51286966 0.51232934
 0.51173764 0.5111693  0.51062745 0.5100894  0.50949    0.5089179
 0.50843287 0.5079987  0.5076559  0.50741166 0.50731206 0.50738835
 0.50761706 0.5078055  0.5079242  0.507998   0.5080071  0.507992
 0.5079743  0.50797707 0.50798905 0.5079644  0.5079343  0.50793356
 0.5079698  0.50806606 0.50814444 0.5082851  0.5084154  0.50856227
 0.50868577 0.5087132  0.50866187 0.5085421  0.50836915 0.5081882
 0.5080935  0.50813234 0.5082299  0.508276   0.50817597 0.5080205
 0.5078833  0.50790304 0.5080234  0.5081137  0.50810087 0.5080292
 0.5080286  0.50818944 0.508457   0.5086817  0.5087468  0.5087289
 0.5086411  0.50860655 0.50864565 0.50871897 0.50873363 0.50864536
 0.50852865 0.50844043 0.5084303  0.5084653  0.50846994 0.50839597
 0.50821805 0.50807655 0.5079893  0.508039   0.50816065 0.5082653
 0.50834537 0.5083315  0.50824267 0.50811243 0.5079208  0.50764805
 0.50726736 0.50678724 0.5062143  0.50557745 0.504973   0.5044734
 0.504079   0.5037084  0.50336015 0.5029726  0.5025323  0.5020745
 0.5016118  0.50120765 0.5009188  0.50071347 0.5005344  0.50030875
 0.50009894 0.49984977 0.49958333 0.49933368 0.49915028 0.49898884
 0.4988461  0.49878025 0.49873358 0.49867085 0.4985617  0.49845293
 0.4983724  0.4983326  0.49834257 0.49836722 0.49833778 0.49821308
 0.49802208 0.49785668 0.4977029  0.4975675  0.49742693 0.4972991
 0.49728763 0.49735755 0.49741086 0.49740118 0.49731818 0.49729133
 0.49733347 0.49733123 0.49732012 0.49727917 0.49725002 0.49727023
 0.4973331  0.4974344  0.4975037  0.49752453 0.49751776 0.49753985
 0.49754992 0.4975521  0.497484   0.49730718 0.497113   0.4969953
 0.49700704 0.49714    0.49727365 0.49729884 0.49722147 0.49712908
 0.49703878 0.4970356  0.49705076 0.497017   0.49689165 0.4967054
 0.4965159  0.4964609  0.49655578 0.49668062 0.49674547 0.49670717
 0.49662012 0.49659392 0.49668577 0.49689227 0.4971233  0.49733648
 0.49754784 0.49780595 0.4980802  0.49833697 0.49847895 0.49843952
 0.49820372 0.49783787 0.49744537 0.49705514 0.49663055 0.49625355
 0.49590278 0.4955908  0.49532676 0.49513826 0.49494776 0.4947266
 0.49444884 0.49411765 0.49374384 0.4933265  0.49284607 0.4924328
 0.4921065  0.4919299  0.4917847  0.491611   0.49134776 0.49107334
 0.49085394 0.49075976 0.49076143 0.49081424 0.4909043  0.49099052
 0.49119365 0.49135122 0.4914615  0.49142635 0.4913363  0.49132365
 0.49136627 0.49148813 0.49157053 0.4915704  0.49146795 0.49137965
 0.4913223  0.491317   0.49130034 0.4913521  0.4914355  0.4916149
 0.4917443  0.49171832 0.4915861  0.4914506  0.49135828 0.49132845
 0.49130005 0.49122924 0.49112853 0.49102366 0.49095115 0.49092737
 0.49088976 0.49082634 0.49070862 0.49058756 0.4905183  0.490519
 0.4905982  0.490685   0.49073237 0.49073905 0.490752   0.4908558
 0.49090868 0.49090305 0.49081415 0.4907036  0.49056974 0.49050036
 0.4904905  0.49053374 0.49052423 0.49044746 0.4903122  0.49016216
 0.49006373 0.49001154 0.49000317 0.490014   0.49004444 0.49011788
 0.49022096 0.49036002 0.49041748 0.49027193 0.48992255 0.48948413
 0.48905855 0.48867217 0.48824596 0.4876829  0.48699892 0.48633856
 0.4858343  0.48538575 0.48493266 0.48441887 0.4837812  0.48308986
 0.48244503 0.48193693 0.4815002  0.48104033 0.48056433 0.48015016
 0.479788   0.4794791  0.47914308 0.47865924 0.47825828 0.47801664
 0.4779864  0.47802925 0.47798365 0.47789237 0.47783428 0.47795063
 0.47823825 0.47851503 0.4786759  0.47880435 0.47890925 0.47902521
 0.47912058 0.4792287  0.47933817 0.47940478 0.47947693 0.47963136
 0.4798455  0.48005453 0.48021325 0.48030123 0.48042807 0.48064068
 0.48083538 0.4809137  0.48085514 0.48076183 0.48067543 0.4805917
 0.4805241  0.48048365 0.48048723 0.48053008 0.48055828 0.48053718
 0.48048162 0.48042715 0.4804192  0.4804362  0.48042896 0.48039192
 0.48031223 0.48025432 0.4802671  0.4803071  0.48033473 0.48039022
 0.4803781  0.4803582  0.48037803 0.4804255  0.48041996 0.4803606
 0.48025653 0.4801729  0.4801354  0.4801429  0.48016104 0.4801562
 0.48011193 0.48012394 0.48012638 0.48014796 0.48018894 0.48026526
 0.4803915  0.48056254 0.48074767 0.48081237 0.48067087 0.48032108
 0.47984686 0.47935304 0.47885188 0.47832668 0.4777732  0.4772741
 0.47679016 0.47634184 0.47592425 0.4755427  0.47518474 0.4747564
 0.47425357 0.47365442 0.47303042 0.4724459  0.4719811  0.47159338
 0.4712138  0.47080252 0.4704381  0.47015905 0.4699375  0.46971062
 0.46950096 0.46930483 0.46922794 0.469303   0.46949977 0.46966696
 0.46983752 0.46996364 0.47007805 0.47019204 0.47027215 0.47034004
 0.47041255 0.47047535 0.47061908 0.47077015 0.4708672  0.47084707
 0.47079432 0.47083986 0.47094965 0.4711443  0.4714809  0.47185397
 0.47209445 0.47224253 0.47230783 0.4722886  0.47227678 0.47225988
 0.47215477 0.4720762  0.47208098 0.47209617 0.47213462 0.47207388
 0.47184324 0.47154626 0.47131652 0.47110453 0.47097847 0.4708732
 0.47076952 0.47065508 0.4705616  0.47051415 0.47048092 0.47049564
 0.4704917  0.4703751  0.47024375 0.47015804 0.47010952 0.47006002
 0.46999857 0.46993813 0.4699132  0.4699331  0.46994203 0.46986288
 0.46971896 0.46969837 0.4698606  0.47013405 0.47048807 0.47077832
 0.47096008 0.4711014  0.4713168  0.471618   0.47180817 0.47171694
 0.47133547 0.47087127 0.47045925 0.47010112 0.46978667 0.46951044
 0.4692511  0.46889243 0.46849906 0.46806058 0.46755332 0.46730527
 0.46718657 0.46717197 0.46694785 0.46647972 0.46585917 0.4653313
 0.46513304 0.46513525 0.46505868 0.46455416 0.46388265 0.46350718
 0.4636257  0.46413836 0.46446937 0.46430534 0.4639803  0.4638693
 0.46432495 0.4650062  0.46537066 0.46529117 0.464957   0.46505344
 0.46582744 0.46684778 0.4673488  0.46706355 0.4668348  0.46747902
 0.46911716 0.47064626 0.47097406 0.4707641  0.46939826 0.46186435]
