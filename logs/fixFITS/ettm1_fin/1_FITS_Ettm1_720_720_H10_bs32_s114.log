Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7257600.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5304527
	speed: 0.1277s/iter; left time: 3289.5257s
	iters: 200, epoch: 1 | loss: 0.4788300
	speed: 0.1253s/iter; left time: 3214.2521s
	iters: 300, epoch: 1 | loss: 0.4029694
	speed: 0.1191s/iter; left time: 3042.0298s
	iters: 400, epoch: 1 | loss: 0.4063082
	speed: 0.1146s/iter; left time: 2916.3779s
	iters: 500, epoch: 1 | loss: 0.3869061
	speed: 0.1185s/iter; left time: 3004.0260s
Epoch: 1 cost time: 62.44728088378906
Epoch: 1, Steps: 517 | Train Loss: 0.4677971 Vali Loss: 0.9739118 Test Loss: 0.4188787
Validation loss decreased (inf --> 0.973912).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4330782
	speed: 0.8832s/iter; left time: 22286.3621s
	iters: 200, epoch: 2 | loss: 0.4377569
	speed: 0.1313s/iter; left time: 3301.0764s
	iters: 300, epoch: 2 | loss: 0.3877260
	speed: 0.1330s/iter; left time: 3330.5602s
	iters: 400, epoch: 2 | loss: 0.3484603
	speed: 0.1276s/iter; left time: 3180.8755s
	iters: 500, epoch: 2 | loss: 0.3565813
	speed: 0.1222s/iter; left time: 3034.4432s
Epoch: 2 cost time: 68.7099986076355
Epoch: 2, Steps: 517 | Train Loss: 0.4025500 Vali Loss: 0.9451447 Test Loss: 0.4140685
Validation loss decreased (0.973912 --> 0.945145).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4209682
	speed: 0.7787s/iter; left time: 19247.3325s
	iters: 200, epoch: 3 | loss: 0.3950735
	speed: 0.1138s/iter; left time: 2802.5756s
	iters: 300, epoch: 3 | loss: 0.3928844
	speed: 0.1175s/iter; left time: 2881.3860s
	iters: 400, epoch: 3 | loss: 0.3688774
	speed: 0.1155s/iter; left time: 2820.5985s
	iters: 500, epoch: 3 | loss: 0.3861235
	speed: 0.1199s/iter; left time: 2916.4814s
Epoch: 3 cost time: 62.1068069934845
Epoch: 3, Steps: 517 | Train Loss: 0.3990179 Vali Loss: 0.9394424 Test Loss: 0.4154412
Validation loss decreased (0.945145 --> 0.939442).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4086834
	speed: 0.8340s/iter; left time: 20182.0929s
	iters: 200, epoch: 4 | loss: 0.4094038
	speed: 0.1267s/iter; left time: 3052.7797s
	iters: 300, epoch: 4 | loss: 0.4927084
	speed: 0.1241s/iter; left time: 2977.3454s
	iters: 400, epoch: 4 | loss: 0.4012063
	speed: 0.1185s/iter; left time: 2831.7541s
	iters: 500, epoch: 4 | loss: 0.3806413
	speed: 0.1258s/iter; left time: 2994.1288s
Epoch: 4 cost time: 65.16169357299805
Epoch: 4, Steps: 517 | Train Loss: 0.3983096 Vali Loss: 0.9378057 Test Loss: 0.4160196
Validation loss decreased (0.939442 --> 0.937806).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3883404
	speed: 0.8575s/iter; left time: 20308.9078s
	iters: 200, epoch: 5 | loss: 0.4045095
	speed: 0.1236s/iter; left time: 2915.5776s
	iters: 300, epoch: 5 | loss: 0.3811435
	speed: 0.1145s/iter; left time: 2688.7579s
	iters: 400, epoch: 5 | loss: 0.4116685
	speed: 0.1137s/iter; left time: 2658.3801s
	iters: 500, epoch: 5 | loss: 0.3994971
	speed: 0.1200s/iter; left time: 2793.6185s
Epoch: 5 cost time: 62.61750674247742
Epoch: 5, Steps: 517 | Train Loss: 0.3979630 Vali Loss: 0.9361985 Test Loss: 0.4160227
Validation loss decreased (0.937806 --> 0.936199).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3299479
	speed: 0.7970s/iter; left time: 18463.1815s
	iters: 200, epoch: 6 | loss: 0.4218501
	speed: 0.1219s/iter; left time: 2812.1906s
	iters: 300, epoch: 6 | loss: 0.4558238
	speed: 0.1194s/iter; left time: 2741.7699s
	iters: 400, epoch: 6 | loss: 0.4741263
	speed: 0.1186s/iter; left time: 2711.6366s
	iters: 500, epoch: 6 | loss: 0.3912120
	speed: 0.1181s/iter; left time: 2688.6608s
Epoch: 6 cost time: 63.1642484664917
Epoch: 6, Steps: 517 | Train Loss: 0.3977629 Vali Loss: 0.9330919 Test Loss: 0.4160295
Validation loss decreased (0.936199 --> 0.933092).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3620392
	speed: 0.7375s/iter; left time: 16703.9155s
	iters: 200, epoch: 7 | loss: 0.4234053
	speed: 0.1128s/iter; left time: 2544.3544s
	iters: 300, epoch: 7 | loss: 0.3966300
	speed: 0.0930s/iter; left time: 2088.5053s
	iters: 400, epoch: 7 | loss: 0.3949411
	speed: 0.1079s/iter; left time: 2411.5372s
	iters: 500, epoch: 7 | loss: 0.3591737
	speed: 0.1116s/iter; left time: 2482.9967s
Epoch: 7 cost time: 56.85725998878479
Epoch: 7, Steps: 517 | Train Loss: 0.3976210 Vali Loss: 0.9342400 Test Loss: 0.4157718
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3663577
	speed: 0.5960s/iter; left time: 13191.3459s
	iters: 200, epoch: 8 | loss: 0.3790903
	speed: 0.0688s/iter; left time: 1515.6395s
	iters: 300, epoch: 8 | loss: 0.4079241
	speed: 0.0786s/iter; left time: 1723.3749s
	iters: 400, epoch: 8 | loss: 0.4090246
	speed: 0.0696s/iter; left time: 1520.1295s
	iters: 500, epoch: 8 | loss: 0.3788800
	speed: 0.0605s/iter; left time: 1314.2117s
Epoch: 8 cost time: 37.0891797542572
Epoch: 8, Steps: 517 | Train Loss: 0.3974898 Vali Loss: 0.9329153 Test Loss: 0.4165570
Validation loss decreased (0.933092 --> 0.932915).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4060309
	speed: 0.4417s/iter; left time: 9547.0118s
	iters: 200, epoch: 9 | loss: 0.3873826
	speed: 0.0624s/iter; left time: 1343.2196s
	iters: 300, epoch: 9 | loss: 0.3703655
	speed: 0.1089s/iter; left time: 2331.2262s
	iters: 400, epoch: 9 | loss: 0.3449205
	speed: 0.1039s/iter; left time: 2215.2133s
	iters: 500, epoch: 9 | loss: 0.3662931
	speed: 0.1002s/iter; left time: 2126.1914s
Epoch: 9 cost time: 46.772013664245605
Epoch: 9, Steps: 517 | Train Loss: 0.3974735 Vali Loss: 0.9338763 Test Loss: 0.4158158
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3627282
	speed: 0.6322s/iter; left time: 13337.5493s
	iters: 200, epoch: 10 | loss: 0.4205343
	speed: 0.1076s/iter; left time: 2258.3655s
	iters: 300, epoch: 10 | loss: 0.3716009
	speed: 0.0946s/iter; left time: 1976.0686s
	iters: 400, epoch: 10 | loss: 0.4481093
	speed: 0.1044s/iter; left time: 2172.2129s
	iters: 500, epoch: 10 | loss: 0.3620987
	speed: 0.1015s/iter; left time: 2100.1921s
Epoch: 10 cost time: 53.84124684333801
Epoch: 10, Steps: 517 | Train Loss: 0.3973652 Vali Loss: 0.9324893 Test Loss: 0.4163369
Validation loss decreased (0.932915 --> 0.932489).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4056231
	speed: 0.9090s/iter; left time: 18709.0647s
	iters: 200, epoch: 11 | loss: 0.3751862
	speed: 0.1482s/iter; left time: 3034.2659s
	iters: 300, epoch: 11 | loss: 0.4253734
	speed: 0.1445s/iter; left time: 2944.4132s
	iters: 400, epoch: 11 | loss: 0.3661022
	speed: 0.1383s/iter; left time: 2805.0345s
	iters: 500, epoch: 11 | loss: 0.4008808
	speed: 0.1299s/iter; left time: 2621.8107s
Epoch: 11 cost time: 73.1643967628479
Epoch: 11, Steps: 517 | Train Loss: 0.3973756 Vali Loss: 0.9314645 Test Loss: 0.4162836
Validation loss decreased (0.932489 --> 0.931464).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4292187
	speed: 0.8084s/iter; left time: 16219.5845s
	iters: 200, epoch: 12 | loss: 0.4014781
	speed: 0.1161s/iter; left time: 2316.9056s
	iters: 300, epoch: 12 | loss: 0.4055119
	speed: 0.1193s/iter; left time: 2370.2070s
	iters: 400, epoch: 12 | loss: 0.4073724
	speed: 0.1165s/iter; left time: 2302.1317s
	iters: 500, epoch: 12 | loss: 0.3349885
	speed: 0.1186s/iter; left time: 2331.3766s
Epoch: 12 cost time: 61.96611428260803
Epoch: 12, Steps: 517 | Train Loss: 0.3972946 Vali Loss: 0.9321464 Test Loss: 0.4159583
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4029343
	speed: 0.7865s/iter; left time: 15373.2300s
	iters: 200, epoch: 13 | loss: 0.4013934
	speed: 0.1312s/iter; left time: 2550.4888s
	iters: 300, epoch: 13 | loss: 0.3914064
	speed: 0.1260s/iter; left time: 2437.8084s
	iters: 400, epoch: 13 | loss: 0.4128993
	speed: 0.1286s/iter; left time: 2475.3610s
	iters: 500, epoch: 13 | loss: 0.4162840
	speed: 0.1321s/iter; left time: 2529.9378s
Epoch: 13 cost time: 67.44956946372986
Epoch: 13, Steps: 517 | Train Loss: 0.3972102 Vali Loss: 0.9311879 Test Loss: 0.4156767
Validation loss decreased (0.931464 --> 0.931188).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3811736
	speed: 0.8739s/iter; left time: 16629.5444s
	iters: 200, epoch: 14 | loss: 0.3721042
	speed: 0.1430s/iter; left time: 2706.3510s
	iters: 300, epoch: 14 | loss: 0.4550014
	speed: 0.1378s/iter; left time: 2594.7699s
	iters: 400, epoch: 14 | loss: 0.3957149
	speed: 0.1459s/iter; left time: 2733.3075s
	iters: 500, epoch: 14 | loss: 0.4046218
	speed: 0.1400s/iter; left time: 2607.5258s
Epoch: 14 cost time: 73.9292163848877
Epoch: 14, Steps: 517 | Train Loss: 0.3972186 Vali Loss: 0.9322170 Test Loss: 0.4158997
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3519779
	speed: 0.8742s/iter; left time: 16184.6726s
	iters: 200, epoch: 15 | loss: 0.3715241
	speed: 0.1254s/iter; left time: 2308.1019s
	iters: 300, epoch: 15 | loss: 0.4341987
	speed: 0.1332s/iter; left time: 2439.3566s
	iters: 400, epoch: 15 | loss: 0.4339519
	speed: 0.1322s/iter; left time: 2408.1582s
	iters: 500, epoch: 15 | loss: 0.3900788
	speed: 0.1244s/iter; left time: 2253.2943s
Epoch: 15 cost time: 68.35514569282532
Epoch: 15, Steps: 517 | Train Loss: 0.3971585 Vali Loss: 0.9303717 Test Loss: 0.4165540
Validation loss decreased (0.931188 --> 0.930372).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4603851
	speed: 0.9180s/iter; left time: 16519.4298s
	iters: 200, epoch: 16 | loss: 0.4194079
	speed: 0.1374s/iter; left time: 2459.5279s
	iters: 300, epoch: 16 | loss: 0.4348400
	speed: 0.1407s/iter; left time: 2503.7392s
	iters: 400, epoch: 16 | loss: 0.3544990
	speed: 0.1358s/iter; left time: 2403.3526s
	iters: 500, epoch: 16 | loss: 0.3811917
	speed: 0.1447s/iter; left time: 2546.1096s
Epoch: 16 cost time: 73.4571578502655
Epoch: 16, Steps: 517 | Train Loss: 0.3971198 Vali Loss: 0.9312502 Test Loss: 0.4162233
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3966631
	speed: 0.8531s/iter; left time: 14910.6328s
	iters: 200, epoch: 17 | loss: 0.4034001
	speed: 0.1344s/iter; left time: 2335.1859s
	iters: 300, epoch: 17 | loss: 0.4014011
	speed: 0.1426s/iter; left time: 2463.5152s
	iters: 400, epoch: 17 | loss: 0.3719182
	speed: 0.1321s/iter; left time: 2269.2875s
	iters: 500, epoch: 17 | loss: 0.3764201
	speed: 0.1420s/iter; left time: 2425.8612s
Epoch: 17 cost time: 71.20433688163757
Epoch: 17, Steps: 517 | Train Loss: 0.3970930 Vali Loss: 0.9305413 Test Loss: 0.4160772
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3868412
	speed: 0.9455s/iter; left time: 16036.8344s
	iters: 200, epoch: 18 | loss: 0.4073997
	speed: 0.1567s/iter; left time: 2642.7385s
	iters: 300, epoch: 18 | loss: 0.3904428
	speed: 0.1460s/iter; left time: 2447.8968s
	iters: 400, epoch: 18 | loss: 0.4555729
	speed: 0.1459s/iter; left time: 2431.0413s
	iters: 500, epoch: 18 | loss: 0.3907727
	speed: 0.1512s/iter; left time: 2503.4058s
Epoch: 18 cost time: 78.69974493980408
Epoch: 18, Steps: 517 | Train Loss: 0.3971009 Vali Loss: 0.9311152 Test Loss: 0.4160334
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41635823249816895, mae:0.41199415922164917, rse:0.6139094233512878, corr:[0.52591306 0.5319934  0.53559744 0.53663915 0.5365463  0.53677136
 0.53764105 0.53891176 0.5402305  0.5412242  0.54184604 0.5420679
 0.54207677 0.5418497  0.54126495 0.54030204 0.5391461  0.53801095
 0.5369578  0.5360113  0.53512037 0.53411376 0.5328579  0.53148586
 0.5299245  0.5284756  0.527367   0.526673   0.526542   0.52689993
 0.52753925 0.52830327 0.5289095  0.52935296 0.52952504 0.5296116
 0.52947485 0.529196   0.52884805 0.52841896 0.52811575 0.52795905
 0.52797216 0.52813923 0.5282461  0.5281682  0.52800804 0.5278317
 0.5276537  0.5274786  0.52741736 0.5274124  0.52744323 0.5273522
 0.52718604 0.52702117 0.52693677 0.52690035 0.52690315 0.52675545
 0.52643156 0.5259939  0.52555454 0.5251908  0.5251222  0.5254153
 0.52591246 0.5263973  0.5267309  0.52679104 0.526643   0.5263571
 0.526037   0.5257801  0.5256796  0.5256625  0.52562755 0.5255411
 0.52540404 0.5252647  0.5251676  0.52512944 0.525155   0.5251652
 0.5251934  0.5251911  0.52519953 0.525221   0.5253106  0.52555966
 0.5259323  0.52631813 0.5266171  0.52673703 0.52668375 0.5264478
 0.52614146 0.5259135  0.52567315 0.5255087  0.5254029  0.5253274
 0.5252605  0.5251044  0.5248669  0.524616   0.5242783  0.5239349
 0.52359986 0.5233805  0.5233057  0.5233238  0.5233667  0.5233707
 0.52324647 0.5229696  0.52256405 0.52208    0.52160895 0.5212658
 0.52103114 0.5208267  0.5205372  0.5202121  0.51990086 0.51961845
 0.5194162  0.5193899  0.51949847 0.51969606 0.5198322  0.51982945
 0.5197308  0.51957834 0.5193723  0.5192127  0.51921755 0.5192921
 0.51944935 0.51968163 0.5198766  0.5200152  0.52010393 0.52015704
 0.5201676  0.5200719  0.51994073 0.5198252  0.51980335 0.51984715
 0.51990455 0.5199935  0.52004546 0.5200098  0.51985776 0.51969635
 0.5195458  0.5194342  0.5194288  0.51948166 0.51965356 0.5199035
 0.52017754 0.52040803 0.5205807  0.5207112  0.5208646  0.52110016
 0.52136636 0.5215734  0.5217367  0.5218166  0.52173877 0.5215326
 0.52129424 0.52112645 0.521106   0.52122515 0.521393   0.5215346
 0.5215689  0.52149    0.5212935  0.52112365 0.5210795  0.52119344
 0.5214424  0.52177715 0.5220993  0.5222995  0.5223009  0.5221009
 0.52174675 0.5213746  0.5209843  0.52058226 0.5201804  0.5197971
 0.51936054 0.5188323  0.5181472  0.5173361  0.516431   0.5155021
 0.5146351  0.51389045 0.51328367 0.51281977 0.5124195  0.5119939
 0.51146597 0.51085037 0.51016474 0.5094272  0.5086178  0.5078678
 0.5072927  0.5068983  0.5067124  0.5066742  0.5067388  0.50690335
 0.5071772  0.5073911  0.50750434 0.5075123  0.50740725 0.5072735
 0.50720954 0.50727665 0.5074513  0.50761205 0.50769603 0.5076803
 0.5075735  0.5074624  0.5073313  0.5073505  0.50751    0.5078315
 0.50820345 0.50843716 0.50848895 0.50838387 0.5081759  0.5078802
 0.5075653  0.50731844 0.50720197 0.5072712  0.50746953 0.5077895
 0.5080822  0.50832754 0.5084396  0.50839627 0.5082547  0.5080972
 0.50800633 0.5080389  0.508167   0.5083334  0.50847334 0.50862324
 0.5086637  0.50861    0.50849783 0.50842965 0.508444   0.5085055
 0.50861436 0.50871164 0.5087728  0.50877696 0.508708   0.50858974
 0.50841224 0.5082885  0.5081892  0.50819755 0.50828326 0.50840175
 0.50857705 0.508729   0.5088051  0.50878745 0.50862384 0.5082742
 0.5077653  0.5072024  0.50663084 0.5060422  0.5054754  0.5049782
 0.504591   0.50426036 0.50399166 0.5036898  0.5032803  0.5027731
 0.50218266 0.50159925 0.50112855 0.50078034 0.50051206 0.50022775
 0.49996594 0.49963817 0.4992628  0.49890575 0.49870014 0.49864832
 0.49872598 0.49888924 0.49898386 0.4989304  0.4987109  0.4984075
 0.49808842 0.49779564 0.4975925  0.4974975  0.49748287 0.49748796
 0.497466   0.4974289  0.49733332 0.49721017 0.49705237 0.49688557
 0.4968079  0.49681023 0.4968417  0.49689624 0.4969295  0.49699584
 0.49701172 0.49684775 0.4966223  0.49641964 0.4963386  0.496413
 0.49657503 0.49680173 0.49699268 0.49709484 0.4970798  0.4970025
 0.49684364 0.49670827 0.4966421  0.49664283 0.4967269  0.49683955
 0.49691245 0.49692965 0.4968795  0.49677572 0.49668667 0.49664864
 0.49660465 0.49659696 0.49658045 0.49653694 0.4964545  0.49633688
 0.49613985 0.49595946 0.49583945 0.49574816 0.4957145  0.49572307
 0.49575797 0.4958333  0.49595267 0.49615228 0.49640772 0.4967273
 0.49709204 0.49746406 0.4978009  0.49803394 0.4981342  0.4981283
 0.49800953 0.4977875  0.49751043 0.49716896 0.49673942 0.496354
 0.49600074 0.49567094 0.4953289  0.49497277 0.49452323 0.49400505
 0.49346125 0.4929717  0.49260405 0.4923749  0.49220917 0.49212238
 0.49203813 0.49195656 0.49178553 0.49155933 0.49129543 0.49105227
 0.49083233 0.49065882 0.4905384  0.49050483 0.49057227 0.4906855
 0.4909347  0.4911615  0.49136958 0.49143186 0.49138212 0.49130344
 0.49117982 0.4911069  0.491058   0.4910461  0.49102068 0.49103492
 0.49104464 0.49105716 0.49101725 0.4910089  0.49098575 0.49104223
 0.49108452 0.49102178 0.49089316 0.49077278 0.4906698  0.4905947
 0.4905309  0.49047893 0.49044582 0.4904275  0.4904324  0.49046394
 0.49046    0.49043253 0.4903439  0.49021456 0.49008295 0.48996192
 0.48993275 0.48998174 0.49007395 0.49014392 0.4901617  0.49019355
 0.49016884 0.49015647 0.49017832 0.49028423 0.49039087 0.49050558
 0.4905727  0.49061352 0.49058193 0.49052006 0.49045348 0.49038687
 0.49035153 0.49030876 0.49028316 0.4902721  0.49027732 0.49028865
 0.49026346 0.49023342 0.4901815  0.4900347  0.4897457  0.4893219
 0.4887839  0.48819256 0.48758677 0.48694274 0.4862363  0.48553064
 0.4849109  0.48432085 0.48380312 0.4833813  0.4829809  0.48256597
 0.48212898 0.48171607 0.48131183 0.4808576  0.48034343 0.47982216
 0.47931108 0.47888994 0.47857878 0.47825918 0.47804072 0.47787884
 0.47780004 0.47775778 0.47768718 0.4776354  0.47761142 0.47768885
 0.47789723 0.47816962 0.47845083 0.4787843  0.47910288 0.4793609
 0.47951666 0.4796499  0.47980002 0.47990027 0.47994438 0.47999126
 0.4800698  0.4802008  0.48039222 0.4805988  0.48087633 0.48124042
 0.4815708  0.48174477 0.48169744 0.48153582 0.48135445 0.48119745
 0.48111427 0.48111126 0.48117837 0.48128563 0.48138264 0.48142982
 0.48141918 0.4813568  0.4812878  0.48123312 0.48120266 0.48123115
 0.4812744  0.48129925 0.48127475 0.48115396 0.48096508 0.48083565
 0.48073334 0.48069486 0.4807362  0.48085028 0.48096046 0.48104694
 0.481079   0.48108527 0.48108774 0.4811053  0.48112312 0.48110712
 0.48102233 0.48096833 0.48088732 0.48084074 0.4808508  0.48092723
 0.48104435 0.48113504 0.48118815 0.4811306  0.48091826 0.48053673
 0.4800579  0.47960398 0.47920895 0.47883448 0.4784331  0.4780628
 0.4776791  0.47729358 0.4768708  0.4763985  0.47588566 0.47529683
 0.47470003 0.47411928 0.4735857  0.47309482 0.47268453 0.47232473
 0.47198585 0.47163248 0.4713038  0.4710194  0.47077876 0.47057727
 0.47044504 0.47032255 0.47022632 0.47016215 0.47017062 0.47021303
 0.47035834 0.47053152 0.47068533 0.4707853  0.470804   0.47080573
 0.47082323 0.470828   0.47088763 0.47097796 0.4710913  0.4711994
 0.47133693 0.47154653 0.4717589  0.47199917 0.4723665  0.4728046
 0.47314543 0.47334865 0.47338352 0.47326693 0.4731447  0.47307482
 0.47299898 0.47296047 0.4729427  0.47287664 0.47280657 0.47269598
 0.47252566 0.47237742 0.47232473 0.472301   0.47235137 0.47239175
 0.4723801  0.472269   0.47206706 0.47181052 0.47152916 0.4713213
 0.47118753 0.47107765 0.47104144 0.47107798 0.4711513  0.47122157
 0.47128123 0.47134432 0.47143474 0.4715462  0.4716266  0.47161338
 0.47149023 0.4713602  0.47127062 0.47120494 0.4712408  0.47135043
 0.4714923  0.47161034 0.47166714 0.47167096 0.47156802 0.47135258
 0.4710675  0.4708138  0.4705667  0.47023386 0.46976116 0.46917957
 0.46858317 0.46802405 0.4676585  0.46747717 0.4673217  0.4672684
 0.4671052  0.46688846 0.46656168 0.46624964 0.4659702  0.46570146
 0.46544462 0.46514118 0.4648314  0.4644491  0.4641119  0.46391425
 0.46382135 0.46386138 0.46391365 0.4639062  0.46390525 0.4638682
 0.46394747 0.4641412  0.4644562  0.46490228 0.4652882  0.4656208
 0.46588874 0.4661561  0.46645895 0.46677825 0.46717897 0.4675788
 0.46805018 0.46856323 0.46893138 0.46919686 0.46884832 0.4665532 ]
