Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10067456.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5248874
	speed: 0.1550s/iter; left time: 3990.6928s
	iters: 200, epoch: 1 | loss: 0.4176719
	speed: 0.1593s/iter; left time: 4087.3960s
	iters: 300, epoch: 1 | loss: 0.3630499
	speed: 0.1646s/iter; left time: 4205.4067s
	iters: 400, epoch: 1 | loss: 0.4567212
	speed: 0.1715s/iter; left time: 4364.4246s
	iters: 500, epoch: 1 | loss: 0.3900546
	speed: 0.1788s/iter; left time: 4533.9682s
Epoch: 1 cost time: 85.8337893486023
Epoch: 1, Steps: 517 | Train Loss: 0.4677994 Vali Loss: 0.9703714 Test Loss: 0.4215123
Validation loss decreased (inf --> 0.970371).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4401708
	speed: 1.1597s/iter; left time: 29264.4214s
	iters: 200, epoch: 2 | loss: 0.4184513
	speed: 0.1520s/iter; left time: 3821.1870s
	iters: 300, epoch: 2 | loss: 0.3865193
	speed: 0.1666s/iter; left time: 4171.3731s
	iters: 400, epoch: 2 | loss: 0.3731551
	speed: 0.1466s/iter; left time: 3655.9833s
	iters: 500, epoch: 2 | loss: 0.3918069
	speed: 0.1358s/iter; left time: 3373.5800s
Epoch: 2 cost time: 78.25506377220154
Epoch: 2, Steps: 517 | Train Loss: 0.4025935 Vali Loss: 0.9451151 Test Loss: 0.4140730
Validation loss decreased (0.970371 --> 0.945115).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3745562
	speed: 1.0070s/iter; left time: 24891.1414s
	iters: 200, epoch: 3 | loss: 0.4070481
	speed: 0.1677s/iter; left time: 4127.8750s
	iters: 300, epoch: 3 | loss: 0.3790161
	speed: 0.1740s/iter; left time: 4266.6295s
	iters: 400, epoch: 3 | loss: 0.3739763
	speed: 0.1726s/iter; left time: 4213.7647s
	iters: 500, epoch: 3 | loss: 0.4092805
	speed: 0.1699s/iter; left time: 4130.2616s
Epoch: 3 cost time: 87.14038133621216
Epoch: 3, Steps: 517 | Train Loss: 0.3989044 Vali Loss: 0.9394420 Test Loss: 0.4148937
Validation loss decreased (0.945115 --> 0.939442).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3733577
	speed: 1.1516s/iter; left time: 27868.0220s
	iters: 200, epoch: 4 | loss: 0.3616241
	speed: 0.1705s/iter; left time: 4109.2120s
	iters: 300, epoch: 4 | loss: 0.4124485
	speed: 0.1663s/iter; left time: 3991.1444s
	iters: 400, epoch: 4 | loss: 0.3953946
	speed: 0.1657s/iter; left time: 3959.1481s
	iters: 500, epoch: 4 | loss: 0.4414594
	speed: 0.1624s/iter; left time: 3865.2674s
Epoch: 4 cost time: 88.10161519050598
Epoch: 4, Steps: 517 | Train Loss: 0.3980436 Vali Loss: 0.9365543 Test Loss: 0.4154393
Validation loss decreased (0.939442 --> 0.936554).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3657212
	speed: 1.0050s/iter; left time: 23802.4899s
	iters: 200, epoch: 5 | loss: 0.3778330
	speed: 0.1569s/iter; left time: 3700.3664s
	iters: 300, epoch: 5 | loss: 0.3931412
	speed: 0.1533s/iter; left time: 3599.2377s
	iters: 400, epoch: 5 | loss: 0.3891405
	speed: 0.1519s/iter; left time: 3551.5601s
	iters: 500, epoch: 5 | loss: 0.3792894
	speed: 0.1305s/iter; left time: 3039.2249s
Epoch: 5 cost time: 77.34479975700378
Epoch: 5, Steps: 517 | Train Loss: 0.3976911 Vali Loss: 0.9344477 Test Loss: 0.4153131
Validation loss decreased (0.936554 --> 0.934448).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3700869
	speed: 0.8350s/iter; left time: 19344.4393s
	iters: 200, epoch: 6 | loss: 0.3750719
	speed: 0.1302s/iter; left time: 3002.1169s
	iters: 300, epoch: 6 | loss: 0.4363719
	speed: 0.1358s/iter; left time: 3117.9160s
	iters: 400, epoch: 6 | loss: 0.4349417
	speed: 0.1455s/iter; left time: 3326.1559s
	iters: 500, epoch: 6 | loss: 0.4361950
	speed: 0.1193s/iter; left time: 2715.7790s
Epoch: 6 cost time: 69.68656706809998
Epoch: 6, Steps: 517 | Train Loss: 0.3974445 Vali Loss: 0.9337706 Test Loss: 0.4161143
Validation loss decreased (0.934448 --> 0.933771).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4143671
	speed: 0.7259s/iter; left time: 16441.5745s
	iters: 200, epoch: 7 | loss: 0.3916203
	speed: 0.0966s/iter; left time: 2179.1471s
	iters: 300, epoch: 7 | loss: 0.3951424
	speed: 0.0620s/iter; left time: 1392.6311s
	iters: 400, epoch: 7 | loss: 0.4200962
	speed: 0.0580s/iter; left time: 1297.0139s
	iters: 500, epoch: 7 | loss: 0.3921572
	speed: 0.0888s/iter; left time: 1975.8896s
Epoch: 7 cost time: 42.38449573516846
Epoch: 7, Steps: 517 | Train Loss: 0.3972978 Vali Loss: 0.9337217 Test Loss: 0.4162676
Validation loss decreased (0.933771 --> 0.933722).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4241960
	speed: 0.7953s/iter; left time: 17601.3750s
	iters: 200, epoch: 8 | loss: 0.3801187
	speed: 0.1241s/iter; left time: 2734.4269s
	iters: 300, epoch: 8 | loss: 0.4484269
	speed: 0.1207s/iter; left time: 2648.1829s
	iters: 400, epoch: 8 | loss: 0.4121043
	speed: 0.1261s/iter; left time: 2753.1717s
	iters: 500, epoch: 8 | loss: 0.3208068
	speed: 0.1102s/iter; left time: 2394.4551s
Epoch: 8 cost time: 62.83739519119263
Epoch: 8, Steps: 517 | Train Loss: 0.3972360 Vali Loss: 0.9323866 Test Loss: 0.4158504
Validation loss decreased (0.933722 --> 0.932387).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4092245
	speed: 0.6948s/iter; left time: 15018.8456s
	iters: 200, epoch: 9 | loss: 0.3812824
	speed: 0.1294s/iter; left time: 2784.5870s
	iters: 300, epoch: 9 | loss: 0.3899930
	speed: 0.1121s/iter; left time: 2400.2197s
	iters: 400, epoch: 9 | loss: 0.4111340
	speed: 0.1111s/iter; left time: 2368.2438s
	iters: 500, epoch: 9 | loss: 0.4247093
	speed: 0.1425s/iter; left time: 3023.4503s
Epoch: 9 cost time: 65.13731122016907
Epoch: 9, Steps: 517 | Train Loss: 0.3972250 Vali Loss: 0.9324868 Test Loss: 0.4158535
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4071196
	speed: 0.8662s/iter; left time: 18275.0030s
	iters: 200, epoch: 10 | loss: 0.4421906
	speed: 0.1272s/iter; left time: 2671.9499s
	iters: 300, epoch: 10 | loss: 0.4283600
	speed: 0.1274s/iter; left time: 2663.1751s
	iters: 400, epoch: 10 | loss: 0.3747590
	speed: 0.1218s/iter; left time: 2532.5894s
	iters: 500, epoch: 10 | loss: 0.4102069
	speed: 0.1038s/iter; left time: 2147.9375s
Epoch: 10 cost time: 63.02791166305542
Epoch: 10, Steps: 517 | Train Loss: 0.3970711 Vali Loss: 0.9323480 Test Loss: 0.4155082
Validation loss decreased (0.932387 --> 0.932348).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3593253
	speed: 0.8140s/iter; left time: 16752.0737s
	iters: 200, epoch: 11 | loss: 0.3512006
	speed: 0.1291s/iter; left time: 2643.9397s
	iters: 300, epoch: 11 | loss: 0.4187898
	speed: 0.1266s/iter; left time: 2579.8234s
	iters: 400, epoch: 11 | loss: 0.3728507
	speed: 0.1311s/iter; left time: 2658.2788s
	iters: 500, epoch: 11 | loss: 0.4298663
	speed: 0.1290s/iter; left time: 2604.1829s
Epoch: 11 cost time: 67.97667169570923
Epoch: 11, Steps: 517 | Train Loss: 0.3971024 Vali Loss: 0.9324319 Test Loss: 0.4158018
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4107076
	speed: 0.9622s/iter; left time: 19305.0203s
	iters: 200, epoch: 12 | loss: 0.3764380
	speed: 0.1380s/iter; left time: 2755.8955s
	iters: 300, epoch: 12 | loss: 0.4160303
	speed: 0.1244s/iter; left time: 2471.1228s
	iters: 400, epoch: 12 | loss: 0.3697404
	speed: 0.1251s/iter; left time: 2471.6455s
	iters: 500, epoch: 12 | loss: 0.4153962
	speed: 0.1265s/iter; left time: 2487.9152s
Epoch: 12 cost time: 69.32634592056274
Epoch: 12, Steps: 517 | Train Loss: 0.3970230 Vali Loss: 0.9313567 Test Loss: 0.4162959
Validation loss decreased (0.932348 --> 0.931357).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3754279
	speed: 0.8455s/iter; left time: 16527.8783s
	iters: 200, epoch: 13 | loss: 0.4174520
	speed: 0.1204s/iter; left time: 2341.1278s
	iters: 300, epoch: 13 | loss: 0.3804003
	speed: 0.1283s/iter; left time: 2481.8218s
	iters: 400, epoch: 13 | loss: 0.4566655
	speed: 0.1260s/iter; left time: 2425.3878s
	iters: 500, epoch: 13 | loss: 0.4419326
	speed: 0.1240s/iter; left time: 2373.8040s
Epoch: 13 cost time: 65.50844287872314
Epoch: 13, Steps: 517 | Train Loss: 0.3969188 Vali Loss: 0.9311838 Test Loss: 0.4154935
Validation loss decreased (0.931357 --> 0.931184).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4009756
	speed: 0.8486s/iter; left time: 16148.0146s
	iters: 200, epoch: 14 | loss: 0.4088790
	speed: 0.1314s/iter; left time: 2487.6954s
	iters: 300, epoch: 14 | loss: 0.3924121
	speed: 0.1215s/iter; left time: 2288.4032s
	iters: 400, epoch: 14 | loss: 0.4168434
	speed: 0.1186s/iter; left time: 2221.8808s
	iters: 500, epoch: 14 | loss: 0.3625995
	speed: 0.1420s/iter; left time: 2646.3874s
Epoch: 14 cost time: 67.51152539253235
Epoch: 14, Steps: 517 | Train Loss: 0.3969015 Vali Loss: 0.9325256 Test Loss: 0.4167697
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3743158
	speed: 0.8822s/iter; left time: 16332.3829s
	iters: 200, epoch: 15 | loss: 0.3847304
	speed: 0.1295s/iter; left time: 2383.9970s
	iters: 300, epoch: 15 | loss: 0.4222253
	speed: 0.1119s/iter; left time: 2049.6923s
	iters: 400, epoch: 15 | loss: 0.3650231
	speed: 0.0977s/iter; left time: 1778.5861s
	iters: 500, epoch: 15 | loss: 0.4156342
	speed: 0.0964s/iter; left time: 1745.8420s
Epoch: 15 cost time: 57.48162508010864
Epoch: 15, Steps: 517 | Train Loss: 0.3968510 Vali Loss: 0.9320471 Test Loss: 0.4157167
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3820943
	speed: 0.8044s/iter; left time: 14475.2520s
	iters: 200, epoch: 16 | loss: 0.3531662
	speed: 0.1281s/iter; left time: 2293.1284s
	iters: 300, epoch: 16 | loss: 0.4128152
	speed: 0.1263s/iter; left time: 2247.8884s
	iters: 400, epoch: 16 | loss: 0.4274630
	speed: 0.1253s/iter; left time: 2216.6875s
	iters: 500, epoch: 16 | loss: 0.3805476
	speed: 0.1284s/iter; left time: 2260.1334s
Epoch: 16 cost time: 66.58801317214966
Epoch: 16, Steps: 517 | Train Loss: 0.3967911 Vali Loss: 0.9317483 Test Loss: 0.4161481
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.415306955575943, mae:0.411717027425766, rse:0.61313396692276, corr:[0.52634156 0.531245   0.5331667  0.533465   0.5337948  0.53492653
 0.536523   0.53799534 0.5390921  0.53992194 0.5407793  0.5414668
 0.54182833 0.5417492  0.54124874 0.54051113 0.53975666 0.53898704
 0.5379923  0.5368032  0.5355429  0.5341972  0.5327785  0.5315256
 0.5303012  0.5292422  0.528432   0.52789056 0.5277653  0.5279813
 0.5283767  0.5288718  0.529215   0.5294649  0.52957463 0.52987367
 0.5301586  0.5302933  0.5301463  0.52961767 0.5289982  0.52847224
 0.52823544 0.52830815 0.5283928  0.5283017  0.5281489  0.52805424
 0.5280493  0.52808136 0.5281456  0.52808124 0.5278759  0.5274687
 0.5270606  0.52681917 0.5267869  0.5268003  0.5267848  0.5265616
 0.52618617 0.5258054  0.52555794 0.52541065 0.5254496  0.52565926
 0.52592754 0.52614826 0.5263082  0.5263518  0.526358   0.52634704
 0.5263297  0.5262997  0.5262836  0.52618176 0.5259342  0.525606
 0.5253137  0.5251906  0.5252604  0.5254449  0.52563757 0.52567565
 0.52558374 0.52535474 0.5250956  0.5248711  0.5247538  0.5248314
 0.5250601  0.525359   0.5256745  0.52593255 0.5260967  0.52607805
 0.52590203 0.52565235 0.5252383  0.5248083  0.52442384 0.52412987
 0.5239605  0.5238502  0.5238052  0.52385837 0.52385366 0.5237609
 0.52348477 0.52307194 0.52259403 0.522148   0.5218687  0.5218417
 0.52198815 0.5221684  0.5222189  0.5219992  0.5215089  0.5209333
 0.5204434  0.5201713  0.52008885 0.5201428  0.5201552  0.5199282
 0.51946026 0.51895714 0.51857096 0.51841944 0.5183964  0.5183804
 0.51833916 0.5182629  0.5181382  0.5180646  0.5181336  0.5181932
 0.51824695 0.5183608  0.5185519  0.5189196  0.5194544  0.5200161
 0.5203848  0.52035016 0.51997864 0.51946694 0.51912403 0.519064
 0.5192221  0.5194946  0.5196965  0.5197159  0.519547   0.5193586
 0.5192142  0.51912194 0.51910794 0.51908046 0.5191127  0.51921946
 0.51940536 0.5196215  0.5198255  0.519987   0.5201187  0.52023005
 0.52025306 0.5201397  0.520022   0.5200052  0.52010965 0.5203437
 0.5206311  0.5208233  0.5207873  0.5204606  0.519874   0.5192444
 0.5188068  0.5187366  0.5190218  0.5196195  0.5203737  0.52109605
 0.52165407 0.5220544  0.5223165  0.5224495  0.5224613  0.52231973
 0.5219913  0.5215421  0.5209625  0.52029324 0.5195993  0.5189366
 0.5182793  0.51761204 0.51689434 0.5161843  0.5154892  0.5148274
 0.51419276 0.51356226 0.5129156  0.51229787 0.51174706 0.51130545
 0.5109405  0.5106386  0.5103043  0.5098358  0.509126   0.5082918
 0.50749815 0.50681204 0.50633043 0.5060197  0.50586027 0.5058769
 0.5061205  0.50645375 0.50685626 0.50727946 0.5076046  0.5078146
 0.50793487 0.50802344 0.5081111  0.5081553  0.5081765  0.5081913
 0.50817513 0.5081501  0.50806344 0.50809366 0.50823987 0.50852895
 0.50882167 0.5089157  0.50880563 0.5085687  0.50831354 0.5080631
 0.50786823 0.5077638  0.5077244  0.5077405  0.50771874 0.50768006
 0.5075389  0.50738955 0.5072569  0.5071914  0.50722593 0.50733006
 0.5074297  0.50747806 0.5074554  0.5074072  0.5074074  0.5075868
 0.5078492  0.5081288  0.5083205  0.5083802  0.508264   0.50797784
 0.5076609  0.5074329  0.5074111  0.5076064  0.5079043  0.50816536
 0.5082293  0.5081283  0.5078614  0.50761944 0.50751793 0.5076435
 0.5080444  0.5085429  0.5089332  0.50906456 0.5088177  0.5081896
 0.50733465 0.5064965  0.5057857  0.50517    0.5046227  0.5041119
 0.50362813 0.50312316 0.5026654  0.5022241  0.50178164 0.5013713
 0.5009958  0.50070894 0.5005488  0.5004751  0.50041467 0.50027776
 0.5001247  0.49988803 0.4995739  0.49920386 0.49886817 0.49858072
 0.4983852  0.49834564 0.49839395 0.4984795  0.4985455  0.4985883
 0.49858248 0.49851385 0.49842036 0.49832347 0.4982131  0.4980626
 0.49786776 0.4977151  0.497615   0.49760655 0.4976368  0.49763823
 0.49764565 0.4976318  0.4976003  0.49764055 0.49777284 0.4980546
 0.4983431  0.49841037 0.49832544 0.49812862 0.4979105  0.49772134
 0.49756777 0.49751094 0.49752754 0.4976052  0.49770683 0.4977894
 0.49772704 0.49754077 0.49726114 0.49692023 0.4966204  0.49638182
 0.49618003 0.49602306 0.49589518 0.49581745 0.49587157 0.4960825
 0.49633884 0.49658632 0.49668425 0.4965796  0.496321   0.4960139
 0.49571168 0.49557194 0.49562004 0.49574333 0.49588507 0.49599406
 0.49608177 0.49620086 0.4963684  0.4966058  0.4968534  0.4970968
 0.49731845 0.49751705 0.49769005 0.4978048  0.4978065  0.49766976
 0.49738666 0.4970142  0.49664506 0.4962845  0.49588817 0.49554217
 0.49520758 0.4948731  0.49453726 0.49425986 0.49399805 0.49374914
 0.49346864 0.4931107  0.49264058 0.4920532  0.49136603 0.49076912
 0.490321   0.49011007 0.4900061  0.4899197  0.48974097 0.48947108
 0.4891342  0.48886287 0.4887696  0.4889276  0.48926485 0.48954153
 0.48971134 0.48960075 0.48935172 0.48904064 0.48887914 0.48899707
 0.48926216 0.48958752 0.48978305 0.48982063 0.48972282 0.4896698
 0.4897128  0.48987606 0.49004588 0.49024448 0.49037492 0.49055195
 0.49069643 0.4907382  0.49073523 0.49074626 0.49073702 0.49066496
 0.49048087 0.49018866 0.48984736 0.48952684 0.48930758 0.4892185
 0.4891946  0.4892222  0.4892514  0.48925814 0.48922095 0.48911703
 0.48906955 0.4891358  0.48934206 0.48963663 0.48996127 0.49031994
 0.490563   0.49068213 0.4906615  0.4905649  0.49036905 0.49018255
 0.4900415  0.49003643 0.4901111  0.4902384  0.4903405  0.49034658
 0.49028075 0.49014303 0.4900183  0.48993933 0.48992306 0.48994923
 0.48996204 0.48998627 0.49001062 0.4899697  0.4898238  0.48957035
 0.48918536 0.48866493 0.48801744 0.48723757 0.48638275 0.48561195
 0.48507595 0.48469338 0.48441294 0.4841373  0.48368257 0.48300776
 0.48221338 0.48150358 0.48098773 0.48064357 0.4804012  0.48018664
 0.4798767  0.4795004  0.47909766 0.47862774 0.47830492 0.47811854
 0.4780642  0.47802708 0.47789583 0.47774813 0.47765356 0.47773707
 0.4780186  0.47837222 0.47870213 0.47900942 0.47917688 0.47916934
 0.47901237 0.47890806 0.47901365 0.47932807 0.47980827 0.48034412
 0.48074192 0.48089132 0.4808091  0.48060024 0.4805524  0.4808507
 0.48138008 0.481849   0.48201242 0.48189685 0.48160452 0.48125094
 0.48096943 0.48081017 0.48073843 0.48066315 0.4805097  0.48030055
 0.48014703 0.48014382 0.4803136  0.48056158 0.48074463 0.4808039
 0.48070735 0.48055732 0.48047313 0.48045748 0.48049235 0.4806118
 0.4806745  0.48067543 0.48067892 0.48077402 0.48095337 0.48119166
 0.48137936 0.48144382 0.4813541  0.48117214 0.4810016  0.48089877
 0.48086488 0.48096997 0.48103645 0.4810327  0.48095122 0.48086143
 0.48083284 0.48087892 0.48098162 0.48098156 0.4807618  0.48029375
 0.47969308 0.4791263  0.47863108 0.47813788 0.47759083 0.47704494
 0.47648832 0.4759839  0.47552952 0.47510943 0.47468752 0.4741748
 0.47360083 0.4729837  0.47239643 0.47189295 0.4715349  0.47127318
 0.47104153 0.4707651  0.47047737 0.47020206 0.46994072 0.46968326
 0.46947238 0.4692944  0.46923098 0.46932337 0.46955192 0.46975207
 0.46991855 0.46996006 0.4699131  0.46987453 0.46991217 0.47010356
 0.47039276 0.47062427 0.47080904 0.4709401  0.47107962 0.4712485
 0.47149527 0.4718126  0.47202703 0.47209448 0.4721346  0.47218296
 0.47217903 0.47220272 0.47226816 0.47238514 0.47262895 0.47294483
 0.47313344 0.47318855 0.473114   0.47290006 0.47268042 0.47244936
 0.4721642  0.47186887 0.47161648 0.47133788 0.47115007 0.47105587
 0.4710553  0.47108164 0.47106868 0.47098032 0.4708078  0.47067332
 0.47062418 0.47063282 0.4707559  0.47096363 0.47116342 0.47128174
 0.47130802 0.47127604 0.4712409  0.47122097 0.47117576 0.47103202
 0.47078487 0.47058257 0.4705313  0.47067958 0.47113273 0.47178766
 0.47245455 0.47291374 0.47304025 0.4728536  0.47240475 0.47184637
 0.4713711  0.47113168 0.47100666 0.4707352  0.47017813 0.46939045
 0.46856964 0.46786368 0.46745268 0.46724144 0.46695504 0.46666682
 0.4662448  0.46591693 0.46571276 0.4656921  0.46566066 0.4654126
 0.464954   0.46442908 0.46415338 0.46417552 0.46451327 0.46496966
 0.46520686 0.46513796 0.46478075 0.46436113 0.46420676 0.4642943
 0.46459928 0.46485564 0.4649014  0.4648572  0.46476826 0.46492282
 0.46540987 0.4661436  0.46684718 0.46724433 0.46739498 0.46733817
 0.4673547  0.46758807 0.46792474 0.4685604  0.46889678 0.46643505]
