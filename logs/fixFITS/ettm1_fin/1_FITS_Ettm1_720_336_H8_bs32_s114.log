Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=74, out_features=108, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3580416.0
params:  8100.0
Trainable parameters:  8100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3779587
	speed: 0.2128s/iter; left time: 5543.1370s
	iters: 200, epoch: 1 | loss: 0.4391357
	speed: 0.2005s/iter; left time: 5202.4854s
	iters: 300, epoch: 1 | loss: 0.3236149
	speed: 0.2024s/iter; left time: 5233.5220s
	iters: 400, epoch: 1 | loss: 0.3335124
	speed: 0.2083s/iter; left time: 5364.2689s
	iters: 500, epoch: 1 | loss: 0.3203031
	speed: 0.2101s/iter; left time: 5389.9301s
Epoch: 1 cost time: 108.37946963310242
Epoch: 1, Steps: 523 | Train Loss: 0.3924839 Vali Loss: 0.6884528 Test Loss: 0.3684986
Validation loss decreased (inf --> 0.688453).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3192900
	speed: 1.3694s/iter; left time: 34957.4677s
	iters: 200, epoch: 2 | loss: 0.3267845
	speed: 0.1893s/iter; left time: 4813.7973s
	iters: 300, epoch: 2 | loss: 0.3413217
	speed: 0.1868s/iter; left time: 4732.1991s
	iters: 400, epoch: 2 | loss: 0.3909476
	speed: 0.1843s/iter; left time: 4650.4982s
	iters: 500, epoch: 2 | loss: 0.3663740
	speed: 0.1834s/iter; left time: 4608.5321s
Epoch: 2 cost time: 98.1504476070404
Epoch: 2, Steps: 523 | Train Loss: 0.3418410 Vali Loss: 0.6706156 Test Loss: 0.3678655
Validation loss decreased (0.688453 --> 0.670616).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3583223
	speed: 1.3245s/iter; left time: 33119.1855s
	iters: 200, epoch: 3 | loss: 0.3503643
	speed: 0.1865s/iter; left time: 4643.6303s
	iters: 300, epoch: 3 | loss: 0.2929609
	speed: 0.1864s/iter; left time: 4623.4767s
	iters: 400, epoch: 3 | loss: 0.2782803
	speed: 0.1974s/iter; left time: 4876.2430s
	iters: 500, epoch: 3 | loss: 0.2921983
	speed: 0.1871s/iter; left time: 4603.9045s
Epoch: 3 cost time: 100.94368577003479
Epoch: 3, Steps: 523 | Train Loss: 0.3393870 Vali Loss: 0.6663011 Test Loss: 0.3680257
Validation loss decreased (0.670616 --> 0.666301).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3861399
	speed: 1.4014s/iter; left time: 34308.9009s
	iters: 200, epoch: 4 | loss: 0.3202971
	speed: 0.2035s/iter; left time: 4961.6983s
	iters: 300, epoch: 4 | loss: 0.3568254
	speed: 0.2150s/iter; left time: 5219.8014s
	iters: 400, epoch: 4 | loss: 0.3453152
	speed: 0.2219s/iter; left time: 5366.2255s
	iters: 500, epoch: 4 | loss: 0.3095423
	speed: 0.2032s/iter; left time: 4893.4298s
Epoch: 4 cost time: 110.72266936302185
Epoch: 4, Steps: 523 | Train Loss: 0.3385591 Vali Loss: 0.6592776 Test Loss: 0.3668965
Validation loss decreased (0.666301 --> 0.659278).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3304892
	speed: 1.3135s/iter; left time: 31469.0356s
	iters: 200, epoch: 5 | loss: 0.3108084
	speed: 0.1767s/iter; left time: 4216.5978s
	iters: 300, epoch: 5 | loss: 0.3771742
	speed: 0.1960s/iter; left time: 4656.8232s
	iters: 400, epoch: 5 | loss: 0.3207017
	speed: 0.1874s/iter; left time: 4434.1816s
	iters: 500, epoch: 5 | loss: 0.3880018
	speed: 0.1833s/iter; left time: 4318.8642s
Epoch: 5 cost time: 98.69327473640442
Epoch: 5, Steps: 523 | Train Loss: 0.3382298 Vali Loss: 0.6598141 Test Loss: 0.3672136
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3151602
	speed: 1.3054s/iter; left time: 30594.3656s
	iters: 200, epoch: 6 | loss: 0.3839662
	speed: 0.1946s/iter; left time: 4540.1372s
	iters: 300, epoch: 6 | loss: 0.3291924
	speed: 0.1922s/iter; left time: 4465.0345s
	iters: 400, epoch: 6 | loss: 0.3717851
	speed: 0.1820s/iter; left time: 4211.3862s
	iters: 500, epoch: 6 | loss: 0.3338320
	speed: 0.1811s/iter; left time: 4171.4105s
Epoch: 6 cost time: 99.45453596115112
Epoch: 6, Steps: 523 | Train Loss: 0.3379475 Vali Loss: 0.6565940 Test Loss: 0.3678532
Validation loss decreased (0.659278 --> 0.656594).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3178203
	speed: 1.3762s/iter; left time: 31532.6370s
	iters: 200, epoch: 7 | loss: 0.3515771
	speed: 0.2156s/iter; left time: 4918.8246s
	iters: 300, epoch: 7 | loss: 0.3610550
	speed: 0.2013s/iter; left time: 4573.0437s
	iters: 400, epoch: 7 | loss: 0.3193875
	speed: 0.2040s/iter; left time: 4613.4537s
	iters: 500, epoch: 7 | loss: 0.3825004
	speed: 0.2005s/iter; left time: 4513.9692s
Epoch: 7 cost time: 109.4112138748169
Epoch: 7, Steps: 523 | Train Loss: 0.3376938 Vali Loss: 0.6607895 Test Loss: 0.3671873
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4039448
	speed: 1.3944s/iter; left time: 31221.1299s
	iters: 200, epoch: 8 | loss: 0.3538382
	speed: 0.1949s/iter; left time: 4345.2200s
	iters: 300, epoch: 8 | loss: 0.3452073
	speed: 0.2012s/iter; left time: 4464.1263s
	iters: 400, epoch: 8 | loss: 0.3584422
	speed: 0.1803s/iter; left time: 3982.1905s
	iters: 500, epoch: 8 | loss: 0.3929296
	speed: 0.1926s/iter; left time: 4234.8341s
Epoch: 8 cost time: 102.88158535957336
Epoch: 8, Steps: 523 | Train Loss: 0.3376671 Vali Loss: 0.6576847 Test Loss: 0.3671777
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3202564
	speed: 1.2973s/iter; left time: 28367.0455s
	iters: 200, epoch: 9 | loss: 0.2975396
	speed: 0.1821s/iter; left time: 3963.7764s
	iters: 300, epoch: 9 | loss: 0.3681833
	speed: 0.2016s/iter; left time: 4367.2930s
	iters: 400, epoch: 9 | loss: 0.4011375
	speed: 0.2001s/iter; left time: 4316.4083s
	iters: 500, epoch: 9 | loss: 0.3200352
	speed: 0.2012s/iter; left time: 4319.1319s
Epoch: 9 cost time: 100.7814691066742
Epoch: 9, Steps: 523 | Train Loss: 0.3375596 Vali Loss: 0.6574798 Test Loss: 0.3668977
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.367752343416214, mae:0.38582152128219604, rse:0.5770667791366577, corr:[0.5354925  0.5443645  0.5514379  0.5552858  0.55644387 0.5564555
 0.55640084 0.55678326 0.55766094 0.5587379  0.55966115 0.56005615
 0.55988115 0.55928403 0.5584902  0.55769616 0.55696976 0.5562488
 0.5553659  0.55423945 0.5529073  0.55137783 0.54972357 0.5481882
 0.5468304  0.5458569  0.54533666 0.5451846  0.5453627  0.545745
 0.54614717 0.5464923  0.54666686 0.54673064 0.5466236  0.5464748
 0.5462012  0.54586977 0.5455179  0.5451169  0.5448164  0.5446689
 0.5447446  0.5450916  0.54553217 0.54588234 0.54607105 0.54601353
 0.54564935 0.5450036  0.544291   0.54368126 0.54331875 0.5431377
 0.54308397 0.5430517  0.5429626  0.54276705 0.5425375  0.5423112
 0.542181   0.542227   0.5424237  0.5426553  0.5428943  0.5430874
 0.5431724  0.54315126 0.5430796  0.5430025  0.5429788  0.54301095
 0.5430368  0.543028   0.54299927 0.54293126 0.5428073  0.54265195
 0.5424851  0.5423345  0.5422045  0.5420809  0.5419669  0.54184455
 0.54175633 0.5417026  0.54172677 0.54181886 0.5419821  0.5422358
 0.5425272  0.5427737  0.5429022  0.54289305 0.54274464 0.5424773
 0.5421706  0.5419411  0.54173094 0.5415686  0.54144406 0.54134303
 0.5412515  0.5411187  0.5409785  0.5408744  0.5407476  0.5406313
 0.540482   0.54031456 0.54012203 0.5398764  0.5395716  0.5392393
 0.53890574 0.5385966  0.5383286  0.53809786 0.53791404 0.5378086
 0.53773504 0.53762704 0.5374001  0.53709865 0.5367678  0.53643125
 0.53615457 0.5360351  0.53609574 0.5363482  0.5366739  0.5369628
 0.5371651  0.5372222  0.5370632  0.53673226 0.53634006 0.53591454
 0.53559655 0.53550416 0.53561014 0.53588855 0.53626496 0.5366452
 0.53692085 0.53698236 0.5368484  0.5365644  0.5362209  0.53585935
 0.53550863 0.5352472  0.5350711  0.5349764  0.53492767 0.53496575
 0.5350391  0.5350837  0.5350995  0.53501505 0.5348908  0.53473973
 0.5346145  0.5345458  0.5345899  0.53477305 0.53510314 0.5355608
 0.5360288  0.53639627 0.53668684 0.53691286 0.53704655 0.5371056
 0.53711027 0.53706783 0.5369778  0.5368464  0.5366619  0.53646535
 0.536283   0.53616613 0.53611946 0.53620565 0.53642416 0.536726
 0.5370746  0.5374264  0.53773534 0.5379307  0.537958   0.5378184
 0.53749573 0.53709346 0.5365862  0.53597873 0.53531456 0.5346877
 0.53411794 0.5336288  0.5331956  0.5327964  0.53237444 0.5318745
 0.53129154 0.5306275  0.5299134  0.5292395  0.5286554  0.52816856
 0.5277429  0.52736306 0.5269703  0.5265305  0.52596563 0.5253525
 0.5247925  0.5243044  0.52396387 0.5237394  0.52362275 0.52362067
 0.523735   0.5238392  0.52389616 0.5239075  0.52385205 0.52378815
 0.5237677  0.5238372  0.52398795 0.5241521  0.5243154  0.52447915
 0.5246417  0.52484447 0.5249798  0.5251191  0.52520424 0.5252668
 0.52530265 0.5252353  0.52510285 0.52498645 0.5249481  0.52494264
 0.52493435 0.5249199  0.5248838  0.52487004 0.5248477  0.5248662
 0.52486265 0.5249051  0.5249389  0.5249218  0.5248603  0.52477574
 0.5247034  0.5246954  0.5247531  0.5248527  0.52495694 0.52510554
 0.5251868  0.52517384 0.52506596 0.52491856 0.52477354 0.5246456
 0.52460045 0.52462953 0.52471524 0.5248246  0.5248993  0.5249128
 0.5248233  0.52471036 0.52456826 0.52450526 0.52451265 0.5245593
 0.52462167 0.52463263 0.5245023  0.5242266  0.5238062  0.5232367
 0.5225943  0.522041   0.5216259  0.521264   0.52092904 0.52060056
 0.52028143 0.51990837 0.51952106 0.5190533  0.51846874 0.5177826
 0.5169856  0.5161401  0.5153719  0.5147494  0.5142975  0.5139697
 0.51382864 0.5137464  0.51363796 0.51344204 0.51321954 0.51299036
 0.51284176 0.512886   0.5130808  0.5133721  0.51367533 0.5139551
 0.5141203  0.5140638  0.51379055 0.5133971  0.51299435 0.51267475
 0.5124619  0.512373   0.512277   0.5120968  0.5117359  0.51124054
 0.51088625 0.51084685 0.5111317  0.51161873 0.51184505 0.5108998 ]
