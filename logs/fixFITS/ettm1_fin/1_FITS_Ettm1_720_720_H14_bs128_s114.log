Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  53344256.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4713628
	speed: 0.1615s/iter; left time: 1025.9702s
Epoch: 1 cost time: 20.66348886489868
Epoch: 1, Steps: 129 | Train Loss: 0.5682870 Vali Loss: 1.0971011 Test Loss: 0.4912394
Validation loss decreased (inf --> 1.097101).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4408864
	speed: 0.4272s/iter; left time: 2657.7852s
Epoch: 2 cost time: 20.219843864440918
Epoch: 2, Steps: 129 | Train Loss: 0.4373877 Vali Loss: 1.0095725 Test Loss: 0.4408593
Validation loss decreased (1.097101 --> 1.009573).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4393829
	speed: 0.3765s/iter; left time: 2293.9252s
Epoch: 3 cost time: 17.16175937652588
Epoch: 3, Steps: 129 | Train Loss: 0.4167032 Vali Loss: 0.9785575 Test Loss: 0.4257708
Validation loss decreased (1.009573 --> 0.978557).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4112376
	speed: 0.3600s/iter; left time: 2147.1913s
Epoch: 4 cost time: 17.348860263824463
Epoch: 4, Steps: 129 | Train Loss: 0.4078609 Vali Loss: 0.9630958 Test Loss: 0.4190601
Validation loss decreased (0.978557 --> 0.963096).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4121488
	speed: 0.3686s/iter; left time: 2150.8892s
Epoch: 5 cost time: 19.097366333007812
Epoch: 5, Steps: 129 | Train Loss: 0.4034250 Vali Loss: 0.9535199 Test Loss: 0.4167151
Validation loss decreased (0.963096 --> 0.953520).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4125293
	speed: 0.4382s/iter; left time: 2500.1485s
Epoch: 6 cost time: 20.89422345161438
Epoch: 6, Steps: 129 | Train Loss: 0.4010019 Vali Loss: 0.9485734 Test Loss: 0.4159581
Validation loss decreased (0.953520 --> 0.948573).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3987193
	speed: 0.4323s/iter; left time: 2410.7417s
Epoch: 7 cost time: 20.681934356689453
Epoch: 7, Steps: 129 | Train Loss: 0.3995748 Vali Loss: 0.9460921 Test Loss: 0.4158199
Validation loss decreased (0.948573 --> 0.946092).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3907969
	speed: 0.3755s/iter; left time: 2045.7700s
Epoch: 8 cost time: 16.109894037246704
Epoch: 8, Steps: 129 | Train Loss: 0.3987973 Vali Loss: 0.9427107 Test Loss: 0.4161589
Validation loss decreased (0.946092 --> 0.942711).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4049196
	speed: 0.2680s/iter; left time: 1425.2614s
Epoch: 9 cost time: 10.782543182373047
Epoch: 9, Steps: 129 | Train Loss: 0.3982343 Vali Loss: 0.9410669 Test Loss: 0.4162967
Validation loss decreased (0.942711 --> 0.941067).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4010754
	speed: 0.1140s/iter; left time: 591.5891s
Epoch: 10 cost time: 3.514836072921753
Epoch: 10, Steps: 129 | Train Loss: 0.3977911 Vali Loss: 0.9393880 Test Loss: 0.4166046
Validation loss decreased (0.941067 --> 0.939388).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4020870
	speed: 0.0648s/iter; left time: 328.1863s
Epoch: 11 cost time: 2.9273529052734375
Epoch: 11, Steps: 129 | Train Loss: 0.3976627 Vali Loss: 0.9379526 Test Loss: 0.4168022
Validation loss decreased (0.939388 --> 0.937953).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3966692
	speed: 0.0665s/iter; left time: 328.1916s
Epoch: 12 cost time: 3.1995182037353516
Epoch: 12, Steps: 129 | Train Loss: 0.3973123 Vali Loss: 0.9371023 Test Loss: 0.4169407
Validation loss decreased (0.937953 --> 0.937102).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3877452
	speed: 0.2529s/iter; left time: 1214.7374s
Epoch: 13 cost time: 16.713829517364502
Epoch: 13, Steps: 129 | Train Loss: 0.3973403 Vali Loss: 0.9370509 Test Loss: 0.4173344
Validation loss decreased (0.937102 --> 0.937051).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4075300
	speed: 0.3633s/iter; left time: 1697.8985s
Epoch: 14 cost time: 17.772650003433228
Epoch: 14, Steps: 129 | Train Loss: 0.3971441 Vali Loss: 0.9361699 Test Loss: 0.4172252
Validation loss decreased (0.937051 --> 0.936170).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4042930
	speed: 0.3638s/iter; left time: 1653.6019s
Epoch: 15 cost time: 17.915921688079834
Epoch: 15, Steps: 129 | Train Loss: 0.3970243 Vali Loss: 0.9354770 Test Loss: 0.4173098
Validation loss decreased (0.936170 --> 0.935477).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4042820
	speed: 0.3729s/iter; left time: 1646.8304s
Epoch: 16 cost time: 18.081849336624146
Epoch: 16, Steps: 129 | Train Loss: 0.3970843 Vali Loss: 0.9356627 Test Loss: 0.4175036
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4171390
	speed: 0.3739s/iter; left time: 1602.9464s
Epoch: 17 cost time: 18.359305143356323
Epoch: 17, Steps: 129 | Train Loss: 0.3966545 Vali Loss: 0.9345164 Test Loss: 0.4176614
Validation loss decreased (0.935477 --> 0.934516).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3739221
	speed: 0.3757s/iter; left time: 1562.2957s
Epoch: 18 cost time: 18.37324333190918
Epoch: 18, Steps: 129 | Train Loss: 0.3967245 Vali Loss: 0.9349927 Test Loss: 0.4175438
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4347203
	speed: 0.3632s/iter; left time: 1463.3067s
Epoch: 19 cost time: 16.43437671661377
Epoch: 19, Steps: 129 | Train Loss: 0.3966631 Vali Loss: 0.9339170 Test Loss: 0.4176542
Validation loss decreased (0.934516 --> 0.933917).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3986853
	speed: 0.3210s/iter; left time: 1251.9254s
Epoch: 20 cost time: 16.62629985809326
Epoch: 20, Steps: 129 | Train Loss: 0.3967284 Vali Loss: 0.9338535 Test Loss: 0.4174021
Validation loss decreased (0.933917 --> 0.933854).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3856497
	speed: 0.3017s/iter; left time: 1137.6930s
Epoch: 21 cost time: 13.666569232940674
Epoch: 21, Steps: 129 | Train Loss: 0.3968153 Vali Loss: 0.9333807 Test Loss: 0.4173643
Validation loss decreased (0.933854 --> 0.933381).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3927359
	speed: 0.2804s/iter; left time: 1021.1812s
Epoch: 22 cost time: 15.12562370300293
Epoch: 22, Steps: 129 | Train Loss: 0.3966933 Vali Loss: 0.9337705 Test Loss: 0.4175423
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3715461
	speed: 0.4296s/iter; left time: 1509.3584s
Epoch: 23 cost time: 21.877028703689575
Epoch: 23, Steps: 129 | Train Loss: 0.3966155 Vali Loss: 0.9325792 Test Loss: 0.4176467
Validation loss decreased (0.933381 --> 0.932579).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3795237
	speed: 0.4441s/iter; left time: 1502.8097s
Epoch: 24 cost time: 20.66758680343628
Epoch: 24, Steps: 129 | Train Loss: 0.3966294 Vali Loss: 0.9334812 Test Loss: 0.4176603
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4054810
	speed: 0.4499s/iter; left time: 1464.3999s
Epoch: 25 cost time: 22.521329402923584
Epoch: 25, Steps: 129 | Train Loss: 0.3965763 Vali Loss: 0.9337168 Test Loss: 0.4175917
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3923988
	speed: 0.4290s/iter; left time: 1341.1955s
Epoch: 26 cost time: 20.052141427993774
Epoch: 26, Steps: 129 | Train Loss: 0.3965545 Vali Loss: 0.9320918 Test Loss: 0.4175968
Validation loss decreased (0.932579 --> 0.932092).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4001097
	speed: 0.4228s/iter; left time: 1267.0709s
Epoch: 27 cost time: 20.9797101020813
Epoch: 27, Steps: 129 | Train Loss: 0.3964511 Vali Loss: 0.9325192 Test Loss: 0.4176319
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3971063
	speed: 0.4164s/iter; left time: 1194.2016s
Epoch: 28 cost time: 19.890588998794556
Epoch: 28, Steps: 129 | Train Loss: 0.3965149 Vali Loss: 0.9321903 Test Loss: 0.4175907
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3869763
	speed: 0.3368s/iter; left time: 922.3628s
Epoch: 29 cost time: 15.219332456588745
Epoch: 29, Steps: 129 | Train Loss: 0.3965475 Vali Loss: 0.9327549 Test Loss: 0.4175247
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.415016770362854, mae:0.4113183915615082, rse:0.6129196882247925, corr:[0.52376944 0.53072333 0.53409123 0.5347535  0.5353711  0.53689206
 0.53863645 0.5396893  0.5400295  0.5401615  0.540512   0.5410095
 0.5415031  0.5417341  0.5414834  0.5406386  0.5392826  0.5377911
 0.53654695 0.5356998  0.53498113 0.53392386 0.53237844 0.5308051
 0.5294945  0.5287781  0.5283835  0.52780634 0.5271345  0.5267382
 0.52706045 0.5282212  0.5295403  0.5303675  0.53026056 0.5298057
 0.529482   0.52962124 0.5299399  0.52987784 0.5293768  0.5286259
 0.5281105  0.5281033  0.5282478  0.5281868  0.52798337 0.52784187
 0.5278638  0.52790105 0.52778924 0.5273567  0.5268005  0.5263703
 0.5263492  0.5266067  0.52679294 0.52663165 0.5262846  0.52596897
 0.52595085 0.52617484 0.5263757  0.526289   0.52612764 0.5261654
 0.52644247 0.52677333 0.5269669  0.5268932  0.5267144  0.52659976
 0.52661294 0.5266404  0.5265918  0.5263569  0.52598727 0.5256912
 0.5255789  0.5255958  0.525577   0.52541023 0.52515626 0.52495134
 0.52501184 0.5252558  0.5255214  0.52559894 0.52551067 0.5255019
 0.52573913 0.52619153 0.5266696  0.5269199  0.52683705 0.52645
 0.52599263 0.52570224 0.52548784 0.5253428  0.52514863 0.5248714
 0.52458733 0.5243453  0.5242335  0.5242518  0.5241827  0.52397245
 0.5235914  0.5232055  0.52294856 0.52282274 0.5227641  0.5226845
 0.52249414 0.52222085 0.5219297  0.5216518  0.52141154 0.52122873
 0.5210004  0.52067083 0.52025396 0.51998556 0.51997626 0.52007264
 0.5200777  0.51992565 0.5196457  0.51944536 0.51939946 0.51946104
 0.51953065 0.5194607  0.5191999  0.5189518  0.5189453  0.5190534
 0.5191695  0.5192104  0.5191276  0.5191007  0.51926446 0.51956105
 0.5197839  0.51973194 0.5194903  0.5192518  0.5192051  0.5192852
 0.51934624 0.5193575  0.51931816 0.51929677 0.5193381  0.5194785
 0.5195895  0.51956856 0.51948905 0.5194089  0.5194945  0.5197232
 0.5199961  0.5202149  0.52036685 0.5204872  0.5206199  0.5207781
 0.52088296 0.5208906  0.520907   0.5209717  0.52101135 0.52099764
 0.5209264  0.5208121  0.5207116  0.52069896 0.5207993  0.5210092
 0.5212246  0.52134633 0.5213135  0.5212453  0.52126676 0.521449
 0.52179277 0.5222447  0.5226688  0.52293324 0.52300596 0.5228717
 0.5225902  0.5222296  0.5217386  0.52111    0.5203821  0.51963353
 0.51889133 0.51817816 0.51744395 0.51670223 0.51596826 0.5153031
 0.5147239  0.5141697  0.513558   0.5128948  0.5122119  0.5115961
 0.5110757  0.5106508  0.5102019  0.5096337  0.50892013 0.5082907
 0.5079288  0.50776726 0.5076769  0.50748163 0.50719315 0.5069968
 0.5070855  0.5073452  0.5076541  0.5078513  0.50779116 0.5075436
 0.5072867  0.507181   0.5072476  0.50735873 0.5074359  0.507462
 0.50746125 0.5075133  0.50756246 0.5077103  0.5078758  0.50806415
 0.5082079  0.50820905 0.50812036 0.5080099  0.50790364 0.507764
 0.50761956 0.5075418  0.5075531  0.5076503  0.50772905 0.5077745
 0.5077039  0.50761986 0.50753534 0.5074778  0.5074858  0.50757444
 0.5077359  0.5079422  0.5081174  0.50820637 0.5082082  0.5082425
 0.50826514 0.5083105  0.5083771  0.50846523 0.50850594 0.5084343
 0.5082992  0.5081475  0.50805265 0.5080273  0.5080234  0.5080109
 0.5079548  0.5079717  0.50803703 0.50820225 0.5083799  0.5084922
 0.508584   0.50862646 0.50863653 0.50861186 0.5084707  0.5081599
 0.50770295 0.5072006  0.5066933  0.5061514  0.50557774 0.5049801
 0.5043852  0.5037819  0.5032634  0.5028159  0.50240153 0.5020055
 0.5015737  0.5011368  0.50074804 0.5004124  0.50011593 0.4998098
 0.4995882  0.4993934  0.499224   0.4990752  0.4989815  0.49888965
 0.49878868 0.4987214  0.49862993 0.49851438 0.49840328 0.49838448
 0.4984624  0.49856207 0.49861205 0.49856627 0.49842516 0.49822202
 0.49802312 0.4979108  0.49782705 0.497747   0.49762267 0.49746767
 0.49738184 0.49733388 0.4972641  0.4971844  0.497113   0.49717632
 0.4973277  0.4973907  0.49739572 0.4973231  0.4972244  0.49715218
 0.49711615 0.497146   0.4971592  0.4971186  0.4970402  0.49701113
 0.49701652 0.49708802 0.49714956 0.4970998  0.49696493 0.4968006
 0.49667522 0.49666563 0.49672917 0.49678957 0.4968313  0.49686372
 0.4968616  0.49692073 0.4970178  0.49709806 0.4971178  0.49707562
 0.49696925 0.49689206 0.49683666 0.4967203  0.49657312 0.49645728
 0.49644777 0.4965746  0.49677038 0.49696928 0.49710506 0.4972345
 0.49744213 0.4977724  0.49814826 0.4984392  0.4985334  0.49842092
 0.4981562  0.49783504 0.4975251  0.49718833 0.49675024 0.49630237
 0.4958557  0.49546835 0.49516234 0.49495623 0.4947529  0.49452114
 0.49424377 0.4939347  0.49359092 0.49317238 0.4926266  0.492107
 0.491674   0.49142218 0.49122554 0.490995   0.49064758 0.49027216
 0.4899779  0.48989367 0.4900074  0.4902358  0.49046257 0.49054527
 0.49061877 0.49063426 0.4907299  0.49084544 0.49098364 0.49112305
 0.4911395  0.49108863 0.49096683 0.4908527  0.49075335 0.4907243
 0.49069658 0.4906754  0.49063623 0.49071112 0.4908489  0.49108636
 0.49124533 0.49122465 0.49109557 0.49097613 0.49089992 0.49085307
 0.49076164 0.49058825 0.4903675  0.49016136 0.4900367  0.49000707
 0.48998615 0.4899531  0.48988542 0.48983634 0.48986456 0.489955
 0.49011776 0.49027035 0.4903745  0.49041846 0.49044138 0.49053058
 0.49055988 0.49054015 0.49046907 0.49041983 0.49036154 0.49035922
 0.4903604  0.49035296 0.49025646 0.4901401  0.4900628  0.4900397
 0.49006915 0.49006793 0.49003458 0.49000022 0.49002174 0.4901108
 0.4901999  0.49026605 0.49023378 0.4900466  0.48974323 0.48941028
 0.48905817 0.48863894 0.48809424 0.4874135  0.48670307 0.4861275
 0.4857545  0.48538882 0.48492822 0.48435125 0.48366356 0.48298526
 0.48240778 0.48194817 0.48147982 0.48090327 0.48028404 0.479798
 0.47949037 0.4793523  0.4792356  0.47891793 0.47854918 0.47819233
 0.47796947 0.4778683  0.47780037 0.47779888 0.4778498  0.47800693
 0.47826535 0.47851107 0.47870272 0.47891653 0.47910744 0.47926992
 0.47938102 0.4795209  0.4797035  0.47983027 0.47988373 0.47993034
 0.48001146 0.4801493  0.480326   0.48046184 0.48057896 0.4806948
 0.48076352 0.48078147 0.48077616 0.4808212  0.4808682  0.48081616
 0.48066193 0.48049346 0.48044384 0.48055    0.48070902 0.4807786
 0.4807059  0.4805485  0.48044273 0.48044658 0.480505   0.48054436
 0.4804812  0.48036253 0.48028857 0.48026225 0.4802588  0.48029804
 0.4802502  0.48016712 0.48015752 0.4802913  0.4804827  0.4806483
 0.48068306 0.48060095 0.4804644  0.48036987 0.48035225 0.48037544
 0.48038042 0.48044956 0.48049113 0.48054254 0.48059833 0.4806659
 0.48075685 0.4808828  0.48106655 0.48117495 0.4810601  0.48065364
 0.4800369  0.47939518 0.47883624 0.47835964 0.47789007 0.47740418
 0.47680408 0.47614387 0.4755304  0.47507316 0.47478566 0.4744892
 0.47406554 0.47343162 0.47268826 0.471989   0.47149283 0.47116545
 0.47090057 0.4705981  0.47029427 0.47003588 0.46984068 0.4696766
 0.4695374  0.46934894 0.46917963 0.46911675 0.4692547  0.4695309
 0.46990955 0.47016567 0.47024044 0.47021613 0.47022188 0.47040984
 0.47073486 0.47097158 0.47106007 0.47099048 0.47091284 0.47095236
 0.47118747 0.47153017 0.4717214  0.47175318 0.47185495 0.47213042
 0.47245425 0.4727244  0.47278398 0.47258854 0.47232398 0.47216266
 0.47213474 0.47231102 0.4725754  0.47271028 0.47271842 0.47255045
 0.4722414  0.4719395  0.47172904 0.47147647 0.4712338  0.4709799
 0.47077808 0.47066715 0.47065517 0.4706957  0.47070232 0.47071233
 0.47071803 0.47069058 0.47073278 0.4708343  0.47089586 0.47084093
 0.47069803 0.47054285 0.47043368 0.47035804 0.47026065 0.47011486
 0.4699962  0.4700938  0.47038394 0.47068393 0.47093922 0.4710804
 0.4711762  0.471341   0.47162423 0.47193378 0.4720217  0.47177008
 0.47128287 0.47084692 0.4705624  0.4703097  0.4699677  0.46951747
 0.4690679  0.46865752 0.46838072 0.46811625 0.4676502  0.4672045
 0.46676853 0.4665797  0.46649483 0.4663596  0.46593738 0.46522525
 0.4645413  0.4641463  0.4641419  0.46420023 0.46419743 0.4641193
 0.46399376 0.46403643 0.46420443 0.4643703  0.46452048 0.46451175
 0.4645275  0.4646012  0.4647942  0.4651629  0.46543396 0.4656728
 0.4659875  0.4664437  0.46692142 0.46716774 0.46724832 0.46730006
 0.4677746  0.4686863  0.46942386 0.4698058  0.46886116 0.46314245]
