Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=90, out_features=114, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  36771840.0
params:  10374.0
Trainable parameters:  10374
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 13.249011516571045
Epoch: 1, Steps: 65 | Train Loss: 0.5081558 Vali Loss: 1.0317988 Test Loss: 0.6232175
Validation loss decreased (inf --> 1.031799).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 12.89783501625061
Epoch: 2, Steps: 65 | Train Loss: 0.3782983 Vali Loss: 0.8581554 Test Loss: 0.5196396
Validation loss decreased (1.031799 --> 0.858155).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 13.914347171783447
Epoch: 3, Steps: 65 | Train Loss: 0.3112771 Vali Loss: 0.7798625 Test Loss: 0.4793251
Validation loss decreased (0.858155 --> 0.779862).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 13.21746277809143
Epoch: 4, Steps: 65 | Train Loss: 0.2709160 Vali Loss: 0.7395803 Test Loss: 0.4615203
Validation loss decreased (0.779862 --> 0.739580).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 13.366493225097656
Epoch: 5, Steps: 65 | Train Loss: 0.2436998 Vali Loss: 0.7152295 Test Loss: 0.4534166
Validation loss decreased (0.739580 --> 0.715229).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 13.1295006275177
Epoch: 6, Steps: 65 | Train Loss: 0.2240281 Vali Loss: 0.6976169 Test Loss: 0.4471958
Validation loss decreased (0.715229 --> 0.697617).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.975546836853027
Epoch: 7, Steps: 65 | Train Loss: 0.2089743 Vali Loss: 0.6849000 Test Loss: 0.4434487
Validation loss decreased (0.697617 --> 0.684900).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.840221405029297
Epoch: 8, Steps: 65 | Train Loss: 0.1969121 Vali Loss: 0.6742166 Test Loss: 0.4398694
Validation loss decreased (0.684900 --> 0.674217).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.12925910949707
Epoch: 9, Steps: 65 | Train Loss: 0.1870123 Vali Loss: 0.6662192 Test Loss: 0.4378054
Validation loss decreased (0.674217 --> 0.666219).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 13.026281118392944
Epoch: 10, Steps: 65 | Train Loss: 0.1785079 Vali Loss: 0.6582164 Test Loss: 0.4353245
Validation loss decreased (0.666219 --> 0.658216).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 14.137779474258423
Epoch: 11, Steps: 65 | Train Loss: 0.1712737 Vali Loss: 0.6525057 Test Loss: 0.4330266
Validation loss decreased (0.658216 --> 0.652506).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 15.35419511795044
Epoch: 12, Steps: 65 | Train Loss: 0.1648883 Vali Loss: 0.6459685 Test Loss: 0.4307451
Validation loss decreased (0.652506 --> 0.645968).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 14.995883703231812
Epoch: 13, Steps: 65 | Train Loss: 0.1593014 Vali Loss: 0.6418610 Test Loss: 0.4291198
Validation loss decreased (0.645968 --> 0.641861).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 15.459068298339844
Epoch: 14, Steps: 65 | Train Loss: 0.1543342 Vali Loss: 0.6382488 Test Loss: 0.4272616
Validation loss decreased (0.641861 --> 0.638249).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 15.027825832366943
Epoch: 15, Steps: 65 | Train Loss: 0.1498786 Vali Loss: 0.6336257 Test Loss: 0.4257401
Validation loss decreased (0.638249 --> 0.633626).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 14.776833534240723
Epoch: 16, Steps: 65 | Train Loss: 0.1458449 Vali Loss: 0.6300877 Test Loss: 0.4241133
Validation loss decreased (0.633626 --> 0.630088).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 14.868428230285645
Epoch: 17, Steps: 65 | Train Loss: 0.1422164 Vali Loss: 0.6266094 Test Loss: 0.4224792
Validation loss decreased (0.630088 --> 0.626609).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 15.258715152740479
Epoch: 18, Steps: 65 | Train Loss: 0.1389710 Vali Loss: 0.6245760 Test Loss: 0.4210623
Validation loss decreased (0.626609 --> 0.624576).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 15.497714042663574
Epoch: 19, Steps: 65 | Train Loss: 0.1359438 Vali Loss: 0.6207872 Test Loss: 0.4194032
Validation loss decreased (0.624576 --> 0.620787).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 15.43349313735962
Epoch: 20, Steps: 65 | Train Loss: 0.1333224 Vali Loss: 0.6185389 Test Loss: 0.4181647
Validation loss decreased (0.620787 --> 0.618539).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 15.5700101852417
Epoch: 21, Steps: 65 | Train Loss: 0.1308068 Vali Loss: 0.6165316 Test Loss: 0.4169020
Validation loss decreased (0.618539 --> 0.616532).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 16.169426918029785
Epoch: 22, Steps: 65 | Train Loss: 0.1285478 Vali Loss: 0.6139829 Test Loss: 0.4155878
Validation loss decreased (0.616532 --> 0.613983).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 16.212772846221924
Epoch: 23, Steps: 65 | Train Loss: 0.1264901 Vali Loss: 0.6122499 Test Loss: 0.4145521
Validation loss decreased (0.613983 --> 0.612250).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 16.2244131565094
Epoch: 24, Steps: 65 | Train Loss: 0.1246206 Vali Loss: 0.6101832 Test Loss: 0.4134518
Validation loss decreased (0.612250 --> 0.610183).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 17.054882526397705
Epoch: 25, Steps: 65 | Train Loss: 0.1229182 Vali Loss: 0.6086512 Test Loss: 0.4124578
Validation loss decreased (0.610183 --> 0.608651).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 16.624593257904053
Epoch: 26, Steps: 65 | Train Loss: 0.1212079 Vali Loss: 0.6062916 Test Loss: 0.4115507
Validation loss decreased (0.608651 --> 0.606292).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 14.326906442642212
Epoch: 27, Steps: 65 | Train Loss: 0.1198024 Vali Loss: 0.6049517 Test Loss: 0.4105636
Validation loss decreased (0.606292 --> 0.604952).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 13.767245292663574
Epoch: 28, Steps: 65 | Train Loss: 0.1184055 Vali Loss: 0.6027722 Test Loss: 0.4097051
Validation loss decreased (0.604952 --> 0.602772).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 13.367654085159302
Epoch: 29, Steps: 65 | Train Loss: 0.1170650 Vali Loss: 0.6022041 Test Loss: 0.4088604
Validation loss decreased (0.602772 --> 0.602204).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 13.889861345291138
Epoch: 30, Steps: 65 | Train Loss: 0.1159585 Vali Loss: 0.6008618 Test Loss: 0.4080412
Validation loss decreased (0.602204 --> 0.600862).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 14.866784811019897
Epoch: 31, Steps: 65 | Train Loss: 0.1148436 Vali Loss: 0.5993914 Test Loss: 0.4073288
Validation loss decreased (0.600862 --> 0.599391).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 15.311840295791626
Epoch: 32, Steps: 65 | Train Loss: 0.1138009 Vali Loss: 0.5982937 Test Loss: 0.4066448
Validation loss decreased (0.599391 --> 0.598294).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 16.034878492355347
Epoch: 33, Steps: 65 | Train Loss: 0.1128285 Vali Loss: 0.5966901 Test Loss: 0.4059292
Validation loss decreased (0.598294 --> 0.596690).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 15.97844123840332
Epoch: 34, Steps: 65 | Train Loss: 0.1119412 Vali Loss: 0.5957686 Test Loss: 0.4054308
Validation loss decreased (0.596690 --> 0.595769).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 15.534614562988281
Epoch: 35, Steps: 65 | Train Loss: 0.1110649 Vali Loss: 0.5945837 Test Loss: 0.4047351
Validation loss decreased (0.595769 --> 0.594584).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 16.79136633872986
Epoch: 36, Steps: 65 | Train Loss: 0.1102723 Vali Loss: 0.5943407 Test Loss: 0.4041454
Validation loss decreased (0.594584 --> 0.594341).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 17.28294086456299
Epoch: 37, Steps: 65 | Train Loss: 0.1095766 Vali Loss: 0.5931808 Test Loss: 0.4035996
Validation loss decreased (0.594341 --> 0.593181).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 16.806373119354248
Epoch: 38, Steps: 65 | Train Loss: 0.1088524 Vali Loss: 0.5923769 Test Loss: 0.4030104
Validation loss decreased (0.593181 --> 0.592377).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 16.56604814529419
Epoch: 39, Steps: 65 | Train Loss: 0.1082201 Vali Loss: 0.5909340 Test Loss: 0.4026013
Validation loss decreased (0.592377 --> 0.590934).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 15.333862543106079
Epoch: 40, Steps: 65 | Train Loss: 0.1075747 Vali Loss: 0.5910523 Test Loss: 0.4020961
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 16.125654220581055
Epoch: 41, Steps: 65 | Train Loss: 0.1070107 Vali Loss: 0.5903060 Test Loss: 0.4017262
Validation loss decreased (0.590934 --> 0.590306).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 14.842011451721191
Epoch: 42, Steps: 65 | Train Loss: 0.1065192 Vali Loss: 0.5894288 Test Loss: 0.4012221
Validation loss decreased (0.590306 --> 0.589429).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 16.472113370895386
Epoch: 43, Steps: 65 | Train Loss: 0.1059733 Vali Loss: 0.5887887 Test Loss: 0.4008942
Validation loss decreased (0.589429 --> 0.588789).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 16.058753728866577
Epoch: 44, Steps: 65 | Train Loss: 0.1055200 Vali Loss: 0.5885915 Test Loss: 0.4004695
Validation loss decreased (0.588789 --> 0.588592).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 16.614023685455322
Epoch: 45, Steps: 65 | Train Loss: 0.1050590 Vali Loss: 0.5869772 Test Loss: 0.4000632
Validation loss decreased (0.588592 --> 0.586977).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 15.385639905929565
Epoch: 46, Steps: 65 | Train Loss: 0.1046253 Vali Loss: 0.5860086 Test Loss: 0.3997592
Validation loss decreased (0.586977 --> 0.586009).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 16.06858205795288
Epoch: 47, Steps: 65 | Train Loss: 0.1042297 Vali Loss: 0.5863515 Test Loss: 0.3994870
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 15.11020541191101
Epoch: 48, Steps: 65 | Train Loss: 0.1038115 Vali Loss: 0.5856538 Test Loss: 0.3991044
Validation loss decreased (0.586009 --> 0.585654).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 15.22649335861206
Epoch: 49, Steps: 65 | Train Loss: 0.1035235 Vali Loss: 0.5852493 Test Loss: 0.3987767
Validation loss decreased (0.585654 --> 0.585249).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 14.51581597328186
Epoch: 50, Steps: 65 | Train Loss: 0.1031901 Vali Loss: 0.5853428 Test Loss: 0.3985614
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.0497355408796396e-05
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=90, out_features=114, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  36771840.0
params:  10374.0
Trainable parameters:  10374
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 15.33210039138794
Epoch: 1, Steps: 65 | Train Loss: 0.3155828 Vali Loss: 0.5398459 Test Loss: 0.3624350
Validation loss decreased (inf --> 0.539846).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 15.347302198410034
Epoch: 2, Steps: 65 | Train Loss: 0.3041193 Vali Loss: 0.5267915 Test Loss: 0.3515846
Validation loss decreased (0.539846 --> 0.526791).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 15.565101861953735
Epoch: 3, Steps: 65 | Train Loss: 0.3013468 Vali Loss: 0.5208907 Test Loss: 0.3463784
Validation loss decreased (0.526791 --> 0.520891).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 16.175403594970703
Epoch: 4, Steps: 65 | Train Loss: 0.2999967 Vali Loss: 0.5202410 Test Loss: 0.3426186
Validation loss decreased (0.520891 --> 0.520241).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 15.927625894546509
Epoch: 5, Steps: 65 | Train Loss: 0.2994296 Vali Loss: 0.5173565 Test Loss: 0.3415865
Validation loss decreased (0.520241 --> 0.517357).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 16.50573229789734
Epoch: 6, Steps: 65 | Train Loss: 0.2990054 Vali Loss: 0.5170038 Test Loss: 0.3405335
Validation loss decreased (0.517357 --> 0.517004).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 16.06711196899414
Epoch: 7, Steps: 65 | Train Loss: 0.2989837 Vali Loss: 0.5151505 Test Loss: 0.3398210
Validation loss decreased (0.517004 --> 0.515151).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 15.329406499862671
Epoch: 8, Steps: 65 | Train Loss: 0.2987926 Vali Loss: 0.5156384 Test Loss: 0.3398259
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 15.103832960128784
Epoch: 9, Steps: 65 | Train Loss: 0.2986987 Vali Loss: 0.5150216 Test Loss: 0.3396222
Validation loss decreased (0.515151 --> 0.515022).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 15.783800840377808
Epoch: 10, Steps: 65 | Train Loss: 0.2984286 Vali Loss: 0.5146569 Test Loss: 0.3390360
Validation loss decreased (0.515022 --> 0.514657).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 15.93971848487854
Epoch: 11, Steps: 65 | Train Loss: 0.2984486 Vali Loss: 0.5147004 Test Loss: 0.3393858
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 16.557924270629883
Epoch: 12, Steps: 65 | Train Loss: 0.2983099 Vali Loss: 0.5134667 Test Loss: 0.3396468
Validation loss decreased (0.514657 --> 0.513467).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 16.92965269088745
Epoch: 13, Steps: 65 | Train Loss: 0.2982303 Vali Loss: 0.5136757 Test Loss: 0.3393513
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 16.373435974121094
Epoch: 14, Steps: 65 | Train Loss: 0.2981368 Vali Loss: 0.5130769 Test Loss: 0.3391833
Validation loss decreased (0.513467 --> 0.513077).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 18.189998626708984
Epoch: 15, Steps: 65 | Train Loss: 0.2982154 Vali Loss: 0.5130426 Test Loss: 0.3391202
Validation loss decreased (0.513077 --> 0.513043).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 16.30884337425232
Epoch: 16, Steps: 65 | Train Loss: 0.2981891 Vali Loss: 0.5129373 Test Loss: 0.3389918
Validation loss decreased (0.513043 --> 0.512937).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 15.523898601531982
Epoch: 17, Steps: 65 | Train Loss: 0.2982531 Vali Loss: 0.5132385 Test Loss: 0.3393019
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 15.753752946853638
Epoch: 18, Steps: 65 | Train Loss: 0.2979818 Vali Loss: 0.5123109 Test Loss: 0.3391041
Validation loss decreased (0.512937 --> 0.512311).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 15.879629135131836
Epoch: 19, Steps: 65 | Train Loss: 0.2979405 Vali Loss: 0.5133832 Test Loss: 0.3393214
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 16.52579092979431
Epoch: 20, Steps: 65 | Train Loss: 0.2979262 Vali Loss: 0.5123349 Test Loss: 0.3392086
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 15.572907209396362
Epoch: 21, Steps: 65 | Train Loss: 0.2981641 Vali Loss: 0.5132914 Test Loss: 0.3389947
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.33886969089508057, mae:0.36889198422431946, rse:0.5541378855705261, corr:[0.54244703 0.5522186  0.55876565 0.5610872  0.5612089  0.56132036
 0.56216526 0.56356645 0.56496817 0.5658029  0.5659912  0.5657648
 0.5655538  0.56546843 0.56531835 0.5648238  0.5638584  0.5625036
 0.56093615 0.5594266  0.5581759  0.55711704 0.5560712  0.5549891
 0.55365956 0.55229175 0.5511517  0.55039257 0.55021507 0.55054665
 0.55114084 0.5517559  0.55210805 0.5522443  0.5522051  0.55225754
 0.5522866  0.5522585  0.5521005  0.5516895  0.55122113 0.55083793
 0.550705   0.5509157  0.55122125 0.5513673  0.5513076  0.551026
 0.5505502  0.55002904 0.5497122  0.54962766 0.5496958  0.5496653
 0.5494724  0.54916245 0.5488821  0.5487128  0.5487808  0.54892725
 0.5490387  0.54903615 0.54889005 0.5485823  0.54833436 0.5483038
 0.5484861  0.54877335 0.54904443 0.54917294 0.5491841  0.5490986
 0.5489728  0.54887575 0.54888004 0.54888916 0.54880106 0.54858613
 0.5482781  0.54796016 0.54771775 0.54757357 0.5475341  0.5474884
 0.54739475 0.5471859  0.54690987 0.54660594 0.54639804 0.5464474
 0.54675287 0.547197   0.54762703 0.54789037 0.54788524 0.54757416
 0.54707646 0.5465868  0.5460849  0.5456429  0.54530215 0.5450875
 0.5450042  0.5449581  0.5449697  0.54505795 0.5450625  0.5449578
 0.5446844  0.5443017  0.54388684 0.54349834 0.543193   0.54299885
 0.5428733  0.5427441  0.54254556 0.5422229  0.5418357  0.54152596
 0.54134834 0.54127276 0.5411806  0.5410641  0.5409     0.5406393
 0.5403368  0.54013723 0.54007787 0.5401675  0.54024285 0.5401978
 0.54006356 0.5398818  0.53965056 0.5394913  0.5395355  0.53962964
 0.53976536 0.5399188  0.53996885 0.53997475 0.5400576  0.54029924
 0.5406704  0.5409785  0.5411618  0.54117084 0.54108185 0.54090714
 0.5407087  0.5406648  0.54075646 0.54089254 0.540931   0.5408911
 0.5407429  0.5404738  0.5402482  0.54006684 0.54008263 0.54024523
 0.5404776  0.5406257  0.54063874 0.5405509  0.5404917  0.540559
 0.5407335  0.54089874 0.5410437  0.5411022  0.5409024  0.5404819
 0.54000986 0.5396996  0.53966147 0.5398728  0.54009515 0.5401545
 0.5399099  0.5394374  0.53887165 0.5385775  0.5387572  0.5393268
 0.5399001  0.54033196 0.54068726 0.5411131  0.54158753 0.5419094 ]
