Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  40269824.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4818515
	speed: 0.1381s/iter; left time: 877.2876s
Epoch: 1 cost time: 17.64297842979431
Epoch: 1, Steps: 129 | Train Loss: 0.5810338 Vali Loss: 1.3304949 Test Loss: 0.6844462
Validation loss decreased (inf --> 1.330495).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3786285
	speed: 0.3662s/iter; left time: 2278.7055s
Epoch: 2 cost time: 17.91936469078064
Epoch: 2, Steps: 129 | Train Loss: 0.4020409 Vali Loss: 1.1819956 Test Loss: 0.5823658
Validation loss decreased (1.330495 --> 1.181996).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3246785
	speed: 0.4116s/iter; left time: 2507.6665s
Epoch: 3 cost time: 20.89261770248413
Epoch: 3, Steps: 129 | Train Loss: 0.3382943 Vali Loss: 1.1097155 Test Loss: 0.5341976
Validation loss decreased (1.181996 --> 1.109715).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2949796
	speed: 0.4129s/iter; left time: 2462.4349s
Epoch: 4 cost time: 19.90139937400818
Epoch: 4, Steps: 129 | Train Loss: 0.3030477 Vali Loss: 1.0673048 Test Loss: 0.5054752
Validation loss decreased (1.109715 --> 1.067305).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2678440
	speed: 0.4487s/iter; left time: 2618.1443s
Epoch: 5 cost time: 22.035149335861206
Epoch: 5, Steps: 129 | Train Loss: 0.2805855 Vali Loss: 1.0391995 Test Loss: 0.4872772
Validation loss decreased (1.067305 --> 1.039199).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2732958
	speed: 0.4408s/iter; left time: 2515.3045s
Epoch: 6 cost time: 22.053243398666382
Epoch: 6, Steps: 129 | Train Loss: 0.2652785 Vali Loss: 1.0199511 Test Loss: 0.4747009
Validation loss decreased (1.039199 --> 1.019951).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2454510
	speed: 0.4273s/iter; left time: 2383.0353s
Epoch: 7 cost time: 21.326147317886353
Epoch: 7, Steps: 129 | Train Loss: 0.2542866 Vali Loss: 1.0047371 Test Loss: 0.4645548
Validation loss decreased (1.019951 --> 1.004737).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2414841
	speed: 0.4250s/iter; left time: 2315.6092s
Epoch: 8 cost time: 19.64932107925415
Epoch: 8, Steps: 129 | Train Loss: 0.2461086 Vali Loss: 0.9932518 Test Loss: 0.4567935
Validation loss decreased (1.004737 --> 0.993252).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2312978
	speed: 0.3520s/iter; left time: 1872.1269s
Epoch: 9 cost time: 19.500173568725586
Epoch: 9, Steps: 129 | Train Loss: 0.2398444 Vali Loss: 0.9851906 Test Loss: 0.4510446
Validation loss decreased (0.993252 --> 0.985191).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2341554
	speed: 0.3841s/iter; left time: 1993.5596s
Epoch: 10 cost time: 18.64855170249939
Epoch: 10, Steps: 129 | Train Loss: 0.2349018 Vali Loss: 0.9762691 Test Loss: 0.4455865
Validation loss decreased (0.985191 --> 0.976269).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2387144
	speed: 0.3929s/iter; left time: 1988.3505s
Epoch: 11 cost time: 19.66716432571411
Epoch: 11, Steps: 129 | Train Loss: 0.2309654 Vali Loss: 0.9709054 Test Loss: 0.4416518
Validation loss decreased (0.976269 --> 0.970905).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2352869
	speed: 0.3875s/iter; left time: 1911.2941s
Epoch: 12 cost time: 19.5329692363739
Epoch: 12, Steps: 129 | Train Loss: 0.2277874 Vali Loss: 0.9671147 Test Loss: 0.4377160
Validation loss decreased (0.970905 --> 0.967115).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2162840
	speed: 0.4028s/iter; left time: 1934.5102s
Epoch: 13 cost time: 19.959685564041138
Epoch: 13, Steps: 129 | Train Loss: 0.2250805 Vali Loss: 0.9619578 Test Loss: 0.4350830
Validation loss decreased (0.967115 --> 0.961958).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2242216
	speed: 0.3938s/iter; left time: 1840.7189s
Epoch: 14 cost time: 18.65538477897644
Epoch: 14, Steps: 129 | Train Loss: 0.2229682 Vali Loss: 0.9591602 Test Loss: 0.4323148
Validation loss decreased (0.961958 --> 0.959160).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2119334
	speed: 0.3975s/iter; left time: 1806.8457s
Epoch: 15 cost time: 19.94384765625
Epoch: 15, Steps: 129 | Train Loss: 0.2210778 Vali Loss: 0.9549509 Test Loss: 0.4301753
Validation loss decreased (0.959160 --> 0.954951).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2213106
	speed: 0.4069s/iter; left time: 1796.6748s
Epoch: 16 cost time: 19.976824283599854
Epoch: 16, Steps: 129 | Train Loss: 0.2195034 Vali Loss: 0.9539466 Test Loss: 0.4282758
Validation loss decreased (0.954951 --> 0.953947).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2063118
	speed: 0.3904s/iter; left time: 1673.7705s
Epoch: 17 cost time: 19.15712594985962
Epoch: 17, Steps: 129 | Train Loss: 0.2182876 Vali Loss: 0.9511178 Test Loss: 0.4268339
Validation loss decreased (0.953947 --> 0.951118).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2232283
	speed: 0.4102s/iter; left time: 1705.4198s
Epoch: 18 cost time: 21.834158897399902
Epoch: 18, Steps: 129 | Train Loss: 0.2171021 Vali Loss: 0.9493814 Test Loss: 0.4255070
Validation loss decreased (0.951118 --> 0.949381).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2051007
	speed: 0.5097s/iter; left time: 2053.6345s
Epoch: 19 cost time: 25.569399118423462
Epoch: 19, Steps: 129 | Train Loss: 0.2161513 Vali Loss: 0.9476476 Test Loss: 0.4243395
Validation loss decreased (0.949381 --> 0.947648).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2187089
	speed: 0.5181s/iter; left time: 2020.4085s
Epoch: 20 cost time: 25.69118046760559
Epoch: 20, Steps: 129 | Train Loss: 0.2153379 Vali Loss: 0.9465146 Test Loss: 0.4234658
Validation loss decreased (0.947648 --> 0.946515).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2086288
	speed: 0.4735s/iter; left time: 1785.4848s
Epoch: 21 cost time: 21.777519464492798
Epoch: 21, Steps: 129 | Train Loss: 0.2146213 Vali Loss: 0.9455283 Test Loss: 0.4224442
Validation loss decreased (0.946515 --> 0.945528).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2130870
	speed: 0.4078s/iter; left time: 1485.2383s
Epoch: 22 cost time: 19.77521824836731
Epoch: 22, Steps: 129 | Train Loss: 0.2139477 Vali Loss: 0.9447980 Test Loss: 0.4218574
Validation loss decreased (0.945528 --> 0.944798).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2185791
	speed: 0.4038s/iter; left time: 1418.7153s
Epoch: 23 cost time: 19.42696523666382
Epoch: 23, Steps: 129 | Train Loss: 0.2133924 Vali Loss: 0.9440650 Test Loss: 0.4212232
Validation loss decreased (0.944798 --> 0.944065).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2294750
	speed: 0.3909s/iter; left time: 1322.8082s
Epoch: 24 cost time: 18.765456199645996
Epoch: 24, Steps: 129 | Train Loss: 0.2129239 Vali Loss: 0.9429343 Test Loss: 0.4207362
Validation loss decreased (0.944065 --> 0.942934).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2079438
	speed: 0.3829s/iter; left time: 1246.4994s
Epoch: 25 cost time: 19.232321739196777
Epoch: 25, Steps: 129 | Train Loss: 0.2125036 Vali Loss: 0.9424106 Test Loss: 0.4202194
Validation loss decreased (0.942934 --> 0.942411).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2132831
	speed: 0.3875s/iter; left time: 1211.4357s
Epoch: 26 cost time: 18.780261278152466
Epoch: 26, Steps: 129 | Train Loss: 0.2121224 Vali Loss: 0.9423286 Test Loss: 0.4198552
Validation loss decreased (0.942411 --> 0.942329).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2172718
	speed: 0.3844s/iter; left time: 1151.9976s
Epoch: 27 cost time: 18.77833366394043
Epoch: 27, Steps: 129 | Train Loss: 0.2117885 Vali Loss: 0.9412949 Test Loss: 0.4194271
Validation loss decreased (0.942329 --> 0.941295).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2136951
	speed: 0.4023s/iter; left time: 1153.7337s
Epoch: 28 cost time: 19.316692113876343
Epoch: 28, Steps: 129 | Train Loss: 0.2114343 Vali Loss: 0.9412326 Test Loss: 0.4192867
Validation loss decreased (0.941295 --> 0.941233).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2033356
	speed: 0.3824s/iter; left time: 1047.4297s
Epoch: 29 cost time: 17.573567867279053
Epoch: 29, Steps: 129 | Train Loss: 0.2111906 Vali Loss: 0.9407197 Test Loss: 0.4189396
Validation loss decreased (0.941233 --> 0.940720).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2189815
	speed: 0.3671s/iter; left time: 958.1895s
Epoch: 30 cost time: 20.947397232055664
Epoch: 30, Steps: 129 | Train Loss: 0.2109007 Vali Loss: 0.9407718 Test Loss: 0.4188547
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2034127
	speed: 0.4462s/iter; left time: 1107.0457s
Epoch: 31 cost time: 21.684622526168823
Epoch: 31, Steps: 129 | Train Loss: 0.2107603 Vali Loss: 0.9407593 Test Loss: 0.4186617
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2060997
	speed: 0.4386s/iter; left time: 1031.7025s
Epoch: 32 cost time: 21.062344312667847
Epoch: 32, Steps: 129 | Train Loss: 0.2105371 Vali Loss: 0.9397432 Test Loss: 0.4186130
Validation loss decreased (0.940720 --> 0.939743).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2013671
	speed: 0.4290s/iter; left time: 953.6696s
Epoch: 33 cost time: 20.074795961380005
Epoch: 33, Steps: 129 | Train Loss: 0.2103093 Vali Loss: 0.9392604 Test Loss: 0.4183838
Validation loss decreased (0.939743 --> 0.939260).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2103693
	speed: 0.3882s/iter; left time: 812.9624s
Epoch: 34 cost time: 19.34883189201355
Epoch: 34, Steps: 129 | Train Loss: 0.2101554 Vali Loss: 0.9395158 Test Loss: 0.4182950
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2093441
	speed: 0.4189s/iter; left time: 823.0867s
Epoch: 35 cost time: 20.998127222061157
Epoch: 35, Steps: 129 | Train Loss: 0.2100200 Vali Loss: 0.9400021 Test Loss: 0.4182062
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2313117
	speed: 0.4179s/iter; left time: 767.2291s
Epoch: 36 cost time: 20.136679887771606
Epoch: 36, Steps: 129 | Train Loss: 0.2099064 Vali Loss: 0.9395348 Test Loss: 0.4181451
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  40269824.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3808653
	speed: 0.1510s/iter; left time: 958.9958s
Epoch: 1 cost time: 19.74053144454956
Epoch: 1, Steps: 129 | Train Loss: 0.3987573 Vali Loss: 0.9336722 Test Loss: 0.4180893
Validation loss decreased (inf --> 0.933672).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3773206
	speed: 0.4046s/iter; left time: 2517.6150s
Epoch: 2 cost time: 19.737082719802856
Epoch: 2, Steps: 129 | Train Loss: 0.3976114 Vali Loss: 0.9325980 Test Loss: 0.4178745
Validation loss decreased (0.933672 --> 0.932598).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4017103
	speed: 0.3880s/iter; left time: 2364.0148s
Epoch: 3 cost time: 18.660175800323486
Epoch: 3, Steps: 129 | Train Loss: 0.3973996 Vali Loss: 0.9329171 Test Loss: 0.4182682
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3982919
	speed: 0.3825s/iter; left time: 2281.3712s
Epoch: 4 cost time: 18.675294399261475
Epoch: 4, Steps: 129 | Train Loss: 0.3972121 Vali Loss: 0.9313828 Test Loss: 0.4187025
Validation loss decreased (0.932598 --> 0.931383).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3941603
	speed: 0.3986s/iter; left time: 2325.8542s
Epoch: 5 cost time: 18.90482473373413
Epoch: 5, Steps: 129 | Train Loss: 0.3970462 Vali Loss: 0.9315405 Test Loss: 0.4182605
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4041167
	speed: 0.4181s/iter; left time: 2385.5522s
Epoch: 6 cost time: 21.577690601348877
Epoch: 6, Steps: 129 | Train Loss: 0.3969571 Vali Loss: 0.9306774 Test Loss: 0.4179989
Validation loss decreased (0.931383 --> 0.930677).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3979006
	speed: 0.4729s/iter; left time: 2637.6315s
Epoch: 7 cost time: 22.998616695404053
Epoch: 7, Steps: 129 | Train Loss: 0.3968551 Vali Loss: 0.9323448 Test Loss: 0.4182568
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4063897
	speed: 0.4722s/iter; left time: 2572.3979s
Epoch: 8 cost time: 23.193347454071045
Epoch: 8, Steps: 129 | Train Loss: 0.3968933 Vali Loss: 0.9318551 Test Loss: 0.4177693
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4143828
	speed: 0.4500s/iter; left time: 2393.4360s
Epoch: 9 cost time: 20.88135552406311
Epoch: 9, Steps: 129 | Train Loss: 0.3967555 Vali Loss: 0.9315808 Test Loss: 0.4176787
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4153529703617096, mae:0.4116920530796051, rse:0.613167941570282, corr:[0.524425   0.53456587 0.5399634  0.5407029  0.5401799  0.54066956
 0.542267   0.5439142  0.5446678  0.5444952  0.54413396 0.54404473
 0.5441965  0.5441038  0.54331607 0.54189396 0.54032534 0.5390465
 0.5380825  0.5372058  0.53613126 0.53464687 0.5328573  0.5312294
 0.5299182  0.52901924 0.5283497  0.52769125 0.52716774 0.5270232
 0.5274509  0.5283903  0.5293302  0.52987576 0.52975506 0.5293557
 0.52898586 0.5289445  0.52916026 0.52928156 0.5291666  0.5287606
 0.5283048  0.5281184  0.5281933  0.5283322  0.5283697  0.5281721
 0.5277777  0.52739155 0.52728814 0.52742696 0.5276499  0.5276876
 0.527573   0.52745795 0.52748454 0.52759635 0.52768344 0.5275136
 0.52709633 0.52660596 0.5262519  0.52608526 0.5261376  0.52626264
 0.52624696 0.52604985 0.52582544 0.52569604 0.52573746 0.5258147
 0.52575344 0.52548695 0.5251344  0.5248037  0.5245975  0.52457726
 0.5246816  0.5248298  0.52493775 0.52494264 0.5248302  0.52459437
 0.52432716 0.52398854 0.5236151  0.52328223 0.52318627 0.5235112
 0.5242504  0.5252072  0.5261033  0.52663326 0.5266669  0.52622575
 0.525584   0.52507854 0.5247748  0.52477133 0.5249352  0.5250729
 0.52505267 0.52479166 0.5244004  0.5240546  0.5237707  0.52362764
 0.5235594  0.5235241  0.5234289  0.52319825 0.5228538  0.5224737
 0.52210194 0.5218167  0.5216532  0.5215745  0.5215152  0.5214624
 0.5213546  0.5211542  0.5208372  0.52053475 0.52029973 0.5200735
 0.5198281  0.519618   0.5194626  0.51944524 0.51951826 0.5196297
 0.5197361  0.519738   0.51953894 0.51921695 0.5189352  0.5186756
 0.51849246 0.5183905  0.51828367 0.5182294  0.51832455 0.5186289
 0.5190755  0.5194565  0.51967216 0.51967984 0.5195615  0.5193918
 0.51925576 0.5192454  0.519302   0.5193385  0.51931375 0.51932764
 0.5193925  0.5194885  0.5195939  0.519564   0.5194348  0.5192493
 0.51912445 0.51916385 0.51940876 0.5197684  0.5200958  0.5202771
 0.52023894 0.5200303  0.5198845  0.51993316 0.52010417 0.52028716
 0.52037454 0.5203207  0.5201859  0.5200951  0.52013695 0.5203635
 0.520682   0.52095836 0.5210384  0.5209598  0.52082795 0.5207834
 0.52093506 0.5213196  0.52181435 0.5222188  0.52239907 0.52230984
 0.5219857  0.5215492  0.52100086 0.5203534  0.51963174 0.5188789
 0.51810884 0.5173889  0.5167493  0.5162473  0.5158359  0.5154491
 0.5150085  0.51446134 0.5138015  0.51310563 0.5124416  0.5118475
 0.5112876  0.51075494 0.51020145 0.5095792  0.50881857 0.5080389
 0.50736535 0.50684017 0.5065499  0.50647986 0.50659174 0.5068382
 0.5071607  0.5073467  0.50737476 0.5072976  0.5071527  0.5070449
 0.5070549  0.50719994 0.50742316 0.5076098  0.50773776 0.50781035
 0.507832   0.5078513  0.50781006 0.50785625 0.507956   0.508124
 0.50825936 0.50822437 0.5080662  0.50790656 0.5078646  0.5079353
 0.50806576 0.5081626  0.50811976 0.50794137 0.5076224  0.50731975
 0.507068   0.5069956  0.5070314  0.5070782  0.50707024 0.50698245
 0.506831   0.5066877  0.50660163 0.50660986 0.5067232  0.50701314
 0.5073471  0.50764596 0.5078418  0.50794196 0.507958   0.5079143
 0.5079088  0.5079448  0.5080067  0.5080538  0.50804037 0.5079931
 0.5079145  0.507919   0.5079233  0.5079343  0.50785214 0.5076415
 0.50740623 0.50718755 0.507048   0.50704104 0.507092   0.5070804
 0.5069367  0.5066734  0.5062784  0.50574136 0.5051315  0.5045276
 0.5039873  0.5034707  0.503016   0.5025545  0.50204706 0.50152105
 0.5009967  0.5005603  0.50029016 0.5001577  0.50009114 0.49999115
 0.49990726 0.49976215 0.49956056 0.49932373 0.4991206  0.4989259
 0.4987507  0.49866733 0.49864048 0.49866304 0.49870142 0.49875277
 0.49875948 0.4986732  0.49852446 0.49837267 0.49825218 0.4981479
 0.49803796 0.49794382 0.4978164  0.49767086 0.49748716 0.4972765
 0.497143   0.49709538 0.49711433 0.4972194  0.49735296 0.49752834
 0.4976224  0.49748448 0.4972751  0.49711046 0.49709052 0.49717754
 0.4972352  0.49721652 0.49705535 0.49680582 0.49657366 0.49648702
 0.49651393 0.49664637 0.4968061  0.4969101  0.4969919  0.49706885
 0.49714068 0.49723086 0.4972916  0.49727148 0.49719846 0.49709865
 0.49694696 0.496834   0.49673805 0.4966274  0.49647817 0.4962958
 0.4960496  0.49588627 0.49586162 0.49590546 0.49598163 0.49600008
 0.49590778 0.49574637 0.49559984 0.49560678 0.49580684 0.49621826
 0.496759   0.49730355 0.49775565 0.4980331  0.4981367  0.49813476
 0.49805152 0.49788517 0.49763092 0.49723333 0.49665222 0.49605602
 0.4954983  0.49504596 0.49470773 0.49449497 0.49429512 0.4940685
 0.49377948 0.49344394 0.4930745  0.4926772  0.49221563 0.49180594
 0.4914529  0.49122313 0.49102968 0.49086094 0.49067014 0.49048758
 0.49031475 0.49020648 0.490184   0.490272   0.4904453  0.49059573
 0.49080706 0.4909681  0.49117002 0.49133    0.49146435 0.49159002
 0.4916067  0.49157256 0.49144915 0.4912921  0.4911109  0.49100026
 0.49092552 0.49088845 0.49082854 0.49083313 0.49086365 0.49100766
 0.491141   0.49113396 0.49099505 0.49079272 0.4905622  0.4903649
 0.49023506 0.49020854 0.49030238 0.49048036 0.49069983 0.49089685
 0.49095953 0.49087656 0.4906501  0.4903485  0.49006099 0.48982936
 0.4897528  0.4898014  0.48991325 0.49000347 0.49004364 0.49009708
 0.49006915 0.4900063  0.48992756 0.48991203 0.48990563 0.4899754
 0.49010345 0.49030977 0.4904829  0.4905894  0.49058035 0.49043724
 0.49024326 0.49004877 0.48995367 0.48996782 0.49003997 0.49007708
 0.48999065 0.48984686 0.48971584 0.48961642 0.48955727 0.48951012
 0.4893748  0.48905754 0.48851103 0.48774365 0.48687503 0.48611227
 0.4856058  0.48523092 0.48488495 0.48445934 0.4838293  0.48304018
 0.48224223 0.48162332 0.48120448 0.48087707 0.4805371  0.48016632
 0.47973946 0.47934613 0.4790106  0.47861174 0.47827095 0.47795433
 0.4777312  0.47761935 0.4776108  0.47778133 0.47808284 0.47849357
 0.47892416 0.47921684 0.47933024 0.47940454 0.4794629  0.47952637
 0.47954232 0.47954044 0.4795093  0.479382   0.4792242  0.47918314
 0.47933406 0.4796726  0.48008728 0.48040885 0.48066488 0.4809118
 0.4811342  0.48129466 0.4813556  0.48137242 0.48133913 0.4812282
 0.48107478 0.4809422  0.48091593 0.4810184  0.48116338 0.48124057
 0.48119318 0.48102984 0.48083556 0.4806919  0.4806396  0.48069575
 0.48075384 0.48074245 0.48065677 0.48052195 0.48044333 0.4805648
 0.4807798  0.48102006 0.48121664 0.48133305 0.48133144 0.48127806
 0.48120597 0.48116782 0.48117632 0.48122165 0.48126638 0.4812531
 0.48113707 0.48101172 0.48079517 0.48056588 0.48038456 0.48031703
 0.4803793  0.48054087 0.4807764  0.48096868 0.48101413 0.48083913
 0.48046625 0.47996712 0.47936362 0.47868326 0.4780024  0.47749186
 0.47714785 0.47692809 0.47667107 0.47623777 0.47558466 0.47473472
 0.47387207 0.47313052 0.47259498 0.47224212 0.4720089  0.4717428
 0.4713405  0.47077534 0.47017187 0.4696581  0.46932292 0.4691801
 0.46920007 0.46922454 0.46918628 0.46906912 0.46895444 0.4688912
 0.46901485 0.46925318 0.46951538 0.46971893 0.46981815 0.4698871
 0.46996787 0.47003105 0.47013378 0.47023273 0.47030997 0.47033793
 0.47037086 0.4704708  0.47059333 0.47080573 0.47122833 0.4717852
 0.47224542 0.47254297 0.47262323 0.4725289  0.4724534  0.47250018
 0.47260547 0.47277158 0.47291297 0.47290382 0.47280332 0.47262916
 0.47242936 0.4723205  0.47233114 0.47228298 0.47213626 0.47180507
 0.4713504  0.47090763 0.47065422 0.4706787  0.4708845  0.4711548
 0.4713153  0.4712643  0.47113582 0.47106662 0.47111407 0.47124702
 0.47138914 0.47146845 0.47146797 0.4714112  0.47131246 0.47116175
 0.47096136 0.470781   0.47062457 0.47049284 0.47051302 0.47071728
 0.4710906  0.47153494 0.471903   0.47205484 0.4718658  0.47133854
 0.4706116  0.46993658 0.46941948 0.46900675 0.46860585 0.4681638
 0.46769217 0.46717414 0.46673238 0.4663615  0.4659509  0.46565157
 0.46534607 0.46514377 0.46496776 0.46484026 0.4646442  0.46428508
 0.46380657 0.46330073 0.4629528  0.4627561  0.46276367 0.46290773
 0.46298492 0.46298382 0.46288413 0.46276438 0.46281737 0.46300793
 0.46340454 0.46392158 0.4645058  0.46513304 0.46554077 0.46573633
 0.4657996  0.4659632  0.4664297  0.46715963 0.46795866 0.46841192
 0.46855164 0.4686634  0.46895778 0.46943945 0.46837035 0.46166068]
