Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=106, out_features=134, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50907136.0
params:  14338.0
Trainable parameters:  14338
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 12.791018009185791
Epoch: 1, Steps: 65 | Train Loss: 0.5111522 Vali Loss: 0.7540795 Test Loss: 0.4440502
Validation loss decreased (inf --> 0.754079).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 13.251558303833008
Epoch: 2, Steps: 65 | Train Loss: 0.3581188 Vali Loss: 0.6256714 Test Loss: 0.3721240
Validation loss decreased (0.754079 --> 0.625671).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 12.923719882965088
Epoch: 3, Steps: 65 | Train Loss: 0.3280033 Vali Loss: 0.5837263 Test Loss: 0.3527559
Validation loss decreased (0.625671 --> 0.583726).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 13.14082670211792
Epoch: 4, Steps: 65 | Train Loss: 0.3161697 Vali Loss: 0.5641971 Test Loss: 0.3455663
Validation loss decreased (0.583726 --> 0.564197).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 12.627979040145874
Epoch: 5, Steps: 65 | Train Loss: 0.3102922 Vali Loss: 0.5519333 Test Loss: 0.3426586
Validation loss decreased (0.564197 --> 0.551933).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 13.015740394592285
Epoch: 6, Steps: 65 | Train Loss: 0.3066059 Vali Loss: 0.5440061 Test Loss: 0.3413419
Validation loss decreased (0.551933 --> 0.544006).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.71084189414978
Epoch: 7, Steps: 65 | Train Loss: 0.3046496 Vali Loss: 0.5400817 Test Loss: 0.3404839
Validation loss decreased (0.544006 --> 0.540082).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 14.077705383300781
Epoch: 8, Steps: 65 | Train Loss: 0.3033343 Vali Loss: 0.5359851 Test Loss: 0.3400726
Validation loss decreased (0.540082 --> 0.535985).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.06576919555664
Epoch: 9, Steps: 65 | Train Loss: 0.3024659 Vali Loss: 0.5330989 Test Loss: 0.3396222
Validation loss decreased (0.535985 --> 0.533099).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 12.944833040237427
Epoch: 10, Steps: 65 | Train Loss: 0.3017503 Vali Loss: 0.5297992 Test Loss: 0.3392867
Validation loss decreased (0.533099 --> 0.529799).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.745585918426514
Epoch: 11, Steps: 65 | Train Loss: 0.3009945 Vali Loss: 0.5286585 Test Loss: 0.3391288
Validation loss decreased (0.529799 --> 0.528659).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.96236538887024
Epoch: 12, Steps: 65 | Train Loss: 0.3006322 Vali Loss: 0.5274324 Test Loss: 0.3390966
Validation loss decreased (0.528659 --> 0.527432).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 13.673770904541016
Epoch: 13, Steps: 65 | Train Loss: 0.3001261 Vali Loss: 0.5271994 Test Loss: 0.3387734
Validation loss decreased (0.527432 --> 0.527199).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 12.673656225204468
Epoch: 14, Steps: 65 | Train Loss: 0.2999008 Vali Loss: 0.5252197 Test Loss: 0.3388192
Validation loss decreased (0.527199 --> 0.525220).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 12.538241147994995
Epoch: 15, Steps: 65 | Train Loss: 0.2996046 Vali Loss: 0.5242950 Test Loss: 0.3386866
Validation loss decreased (0.525220 --> 0.524295).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 12.515825271606445
Epoch: 16, Steps: 65 | Train Loss: 0.2993097 Vali Loss: 0.5229436 Test Loss: 0.3386821
Validation loss decreased (0.524295 --> 0.522944).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 13.81385350227356
Epoch: 17, Steps: 65 | Train Loss: 0.2992344 Vali Loss: 0.5222008 Test Loss: 0.3383754
Validation loss decreased (0.522944 --> 0.522201).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 12.334304332733154
Epoch: 18, Steps: 65 | Train Loss: 0.2991747 Vali Loss: 0.5217922 Test Loss: 0.3385928
Validation loss decreased (0.522201 --> 0.521792).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 12.74792194366455
Epoch: 19, Steps: 65 | Train Loss: 0.2991562 Vali Loss: 0.5211100 Test Loss: 0.3384954
Validation loss decreased (0.521792 --> 0.521110).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 12.452597379684448
Epoch: 20, Steps: 65 | Train Loss: 0.2989515 Vali Loss: 0.5207690 Test Loss: 0.3383987
Validation loss decreased (0.521110 --> 0.520769).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 12.816288232803345
Epoch: 21, Steps: 65 | Train Loss: 0.2984212 Vali Loss: 0.5207273 Test Loss: 0.3384048
Validation loss decreased (0.520769 --> 0.520727).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 12.628718614578247
Epoch: 22, Steps: 65 | Train Loss: 0.2988162 Vali Loss: 0.5192404 Test Loss: 0.3384323
Validation loss decreased (0.520727 --> 0.519240).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 12.405181646347046
Epoch: 23, Steps: 65 | Train Loss: 0.2987850 Vali Loss: 0.5185775 Test Loss: 0.3384353
Validation loss decreased (0.519240 --> 0.518577).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 12.122514963150024
Epoch: 24, Steps: 65 | Train Loss: 0.2985928 Vali Loss: 0.5194525 Test Loss: 0.3384084
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 12.264019966125488
Epoch: 25, Steps: 65 | Train Loss: 0.2986750 Vali Loss: 0.5190473 Test Loss: 0.3383281
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 11.525784969329834
Epoch: 26, Steps: 65 | Train Loss: 0.2984980 Vali Loss: 0.5183764 Test Loss: 0.3382554
Validation loss decreased (0.518577 --> 0.518376).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.973729372024536
Epoch: 27, Steps: 65 | Train Loss: 0.2983522 Vali Loss: 0.5183720 Test Loss: 0.3382057
Validation loss decreased (0.518376 --> 0.518372).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 12.519564390182495
Epoch: 28, Steps: 65 | Train Loss: 0.2984245 Vali Loss: 0.5181292 Test Loss: 0.3382655
Validation loss decreased (0.518372 --> 0.518129).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 13.268288612365723
Epoch: 29, Steps: 65 | Train Loss: 0.2980708 Vali Loss: 0.5175301 Test Loss: 0.3382405
Validation loss decreased (0.518129 --> 0.517530).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 12.946248769760132
Epoch: 30, Steps: 65 | Train Loss: 0.2980475 Vali Loss: 0.5174299 Test Loss: 0.3382222
Validation loss decreased (0.517530 --> 0.517430).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 14.357264041900635
Epoch: 31, Steps: 65 | Train Loss: 0.2984978 Vali Loss: 0.5173145 Test Loss: 0.3382865
Validation loss decreased (0.517430 --> 0.517314).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 11.89269232749939
Epoch: 32, Steps: 65 | Train Loss: 0.2980131 Vali Loss: 0.5163238 Test Loss: 0.3382543
Validation loss decreased (0.517314 --> 0.516324).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 11.387242078781128
Epoch: 33, Steps: 65 | Train Loss: 0.2981718 Vali Loss: 0.5172411 Test Loss: 0.3382398
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 10.451025247573853
Epoch: 34, Steps: 65 | Train Loss: 0.2980932 Vali Loss: 0.5170005 Test Loss: 0.3382315
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 10.831922054290771
Epoch: 35, Steps: 65 | Train Loss: 0.2980459 Vali Loss: 0.5172359 Test Loss: 0.3382300
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.33813241124153137, mae:0.3683209717273712, rse:0.5535348057746887, corr:[0.5407746  0.5515601  0.5577413  0.5592563  0.5596541  0.5608007
 0.5625183  0.56395936 0.5646188  0.56478494 0.5650662  0.56549644
 0.5658073  0.5656422  0.56488454 0.56379944 0.56281334 0.56206685
 0.56127286 0.56019056 0.55880547 0.5571905  0.55560696 0.55443686
 0.5535175  0.55270296 0.55181897 0.55083585 0.5501772  0.55019283
 0.55088896 0.55192953 0.55268586 0.55288357 0.55248845 0.5521042
 0.5519981  0.55218875 0.5523638  0.55211955 0.5515199  0.5507743
 0.5502997  0.5503647  0.55068165 0.5508604  0.5507826  0.5504801
 0.55011314 0.54989475 0.5499573  0.5501026  0.5501244  0.54986554
 0.54948175 0.54922307 0.54922396 0.54931813 0.5493613  0.54912835
 0.548696   0.5483188  0.5482427  0.54841745 0.5487746  0.5491207
 0.5493115  0.5493645  0.54944    0.54961354 0.5498651  0.5500175
 0.5499412  0.54966486 0.5493595  0.54911476 0.5489689  0.5488967
 0.5488147  0.5486704  0.5484598  0.5481963  0.5479633  0.5477712
 0.54765916 0.54755646 0.5474296  0.5472346  0.5470492  0.54703194
 0.54720193 0.5474654  0.54769766 0.5477869  0.5476675  0.5473518
 0.5469702  0.5466664  0.54635584 0.54604787 0.5457503  0.5455083
 0.54537624 0.545306   0.54529816 0.54533154 0.5452431  0.5450529
 0.5447739  0.54451    0.54430485 0.54411566 0.5439094  0.5436821
 0.5434225  0.5431897  0.5430385  0.54289633 0.54269433 0.5424099
 0.54201007 0.5415717  0.5412325  0.54117304 0.54132444 0.5413964
 0.54118586 0.5407344  0.5402282  0.5399835  0.5400436  0.5402557
 0.540442   0.54042447 0.54014915 0.5398657  0.5398861  0.54010385
 0.5404051  0.54059446 0.5404864  0.5402479  0.54019964 0.54053015
 0.5411096  0.541545   0.5416449  0.54139334 0.5410498  0.5408302
 0.5408357  0.5410517  0.5412253  0.5411451  0.5407935  0.5404555
 0.5403466  0.5404559  0.54070276 0.54077363 0.54064    0.5403399
 0.5401018  0.5400592  0.5402347  0.54051    0.5407437  0.5408276
 0.54071766 0.5404662  0.54029    0.5402803  0.54024374 0.540014
 0.53952074 0.53890234 0.53838235 0.5381858  0.53828406 0.5385272
 0.53853995 0.5381791  0.53755987 0.53715664 0.53726906 0.5377532
 0.53801876 0.5380379  0.5382851  0.539142   0.53993905 0.5381277 ]
