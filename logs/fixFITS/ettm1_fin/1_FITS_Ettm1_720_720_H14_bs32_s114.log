Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13336064.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4865407
	speed: 0.1560s/iter; left time: 4018.4339s
	iters: 200, epoch: 1 | loss: 0.4170183
	speed: 0.1670s/iter; left time: 4283.6911s
	iters: 300, epoch: 1 | loss: 0.4009430
	speed: 0.1746s/iter; left time: 4460.7629s
	iters: 400, epoch: 1 | loss: 0.4023440
	speed: 0.1635s/iter; left time: 4160.2527s
	iters: 500, epoch: 1 | loss: 0.3493643
	speed: 0.1664s/iter; left time: 4217.2259s
Epoch: 1 cost time: 85.57184886932373
Epoch: 1, Steps: 517 | Train Loss: 0.4631563 Vali Loss: 0.9700422 Test Loss: 0.4183573
Validation loss decreased (inf --> 0.970042).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3917540
	speed: 1.1501s/iter; left time: 29021.7824s
	iters: 200, epoch: 2 | loss: 0.3908713
	speed: 0.1768s/iter; left time: 4443.2635s
	iters: 300, epoch: 2 | loss: 0.3705388
	speed: 0.1806s/iter; left time: 4522.3052s
	iters: 400, epoch: 2 | loss: 0.3487034
	speed: 0.1707s/iter; left time: 4256.5564s
	iters: 500, epoch: 2 | loss: 0.3665170
	speed: 0.1448s/iter; left time: 3595.1632s
Epoch: 2 cost time: 87.89545392990112
Epoch: 2, Steps: 517 | Train Loss: 0.4020340 Vali Loss: 0.9442633 Test Loss: 0.4143307
Validation loss decreased (0.970042 --> 0.944263).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3524590
	speed: 1.0880s/iter; left time: 26890.8746s
	iters: 200, epoch: 3 | loss: 0.4158526
	speed: 0.1570s/iter; left time: 3865.3404s
	iters: 300, epoch: 3 | loss: 0.3944664
	speed: 0.1636s/iter; left time: 4010.7242s
	iters: 400, epoch: 3 | loss: 0.4259334
	speed: 0.1688s/iter; left time: 4121.6612s
	iters: 500, epoch: 3 | loss: 0.4280007
	speed: 0.1729s/iter; left time: 4204.0466s
Epoch: 3 cost time: 86.6751070022583
Epoch: 3, Steps: 517 | Train Loss: 0.3987357 Vali Loss: 0.9391162 Test Loss: 0.4149600
Validation loss decreased (0.944263 --> 0.939116).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3883250
	speed: 1.1235s/iter; left time: 27187.7539s
	iters: 200, epoch: 4 | loss: 0.3950288
	speed: 0.1688s/iter; left time: 4069.2698s
	iters: 300, epoch: 4 | loss: 0.4166485
	speed: 0.1686s/iter; left time: 4046.7506s
	iters: 400, epoch: 4 | loss: 0.3851050
	speed: 0.1644s/iter; left time: 3929.5374s
	iters: 500, epoch: 4 | loss: 0.3469140
	speed: 0.1661s/iter; left time: 3954.0891s
Epoch: 4 cost time: 88.2030303478241
Epoch: 4, Steps: 517 | Train Loss: 0.3979800 Vali Loss: 0.9377401 Test Loss: 0.4156190
Validation loss decreased (0.939116 --> 0.937740).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3984901
	speed: 1.1049s/iter; left time: 26166.1827s
	iters: 200, epoch: 5 | loss: 0.3995118
	speed: 0.1738s/iter; left time: 4098.5235s
	iters: 300, epoch: 5 | loss: 0.3716374
	speed: 0.1739s/iter; left time: 4082.8043s
	iters: 400, epoch: 5 | loss: 0.3732896
	speed: 0.1696s/iter; left time: 3966.0549s
	iters: 500, epoch: 5 | loss: 0.4071443
	speed: 0.1637s/iter; left time: 3811.0055s
Epoch: 5 cost time: 87.76958870887756
Epoch: 5, Steps: 517 | Train Loss: 0.3976023 Vali Loss: 0.9347472 Test Loss: 0.4157221
Validation loss decreased (0.937740 --> 0.934747).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4469457
	speed: 0.9939s/iter; left time: 23025.2023s
	iters: 200, epoch: 6 | loss: 0.4207673
	speed: 0.1190s/iter; left time: 2745.1908s
	iters: 300, epoch: 6 | loss: 0.4094218
	speed: 0.1406s/iter; left time: 3228.9680s
	iters: 400, epoch: 6 | loss: 0.4406368
	speed: 0.1710s/iter; left time: 3911.2252s
	iters: 500, epoch: 6 | loss: 0.3999763
	speed: 0.1732s/iter; left time: 3942.3358s
Epoch: 6 cost time: 79.5460913181305
Epoch: 6, Steps: 517 | Train Loss: 0.3973260 Vali Loss: 0.9340853 Test Loss: 0.4155606
Validation loss decreased (0.934747 --> 0.934085).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4223998
	speed: 1.1138s/iter; left time: 25226.9655s
	iters: 200, epoch: 7 | loss: 0.4031811
	speed: 0.1664s/iter; left time: 3752.6543s
	iters: 300, epoch: 7 | loss: 0.3774787
	speed: 0.1635s/iter; left time: 3669.6060s
	iters: 400, epoch: 7 | loss: 0.3718120
	speed: 0.1715s/iter; left time: 3833.2709s
	iters: 500, epoch: 7 | loss: 0.4133227
	speed: 0.1279s/iter; left time: 2845.8022s
Epoch: 7 cost time: 82.28660488128662
Epoch: 7, Steps: 517 | Train Loss: 0.3972107 Vali Loss: 0.9325200 Test Loss: 0.4158239
Validation loss decreased (0.934085 --> 0.932520).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3908783
	speed: 1.0598s/iter; left time: 23455.2703s
	iters: 200, epoch: 8 | loss: 0.4277461
	speed: 0.1681s/iter; left time: 3704.0953s
	iters: 300, epoch: 8 | loss: 0.3915658
	speed: 0.1635s/iter; left time: 3586.2129s
	iters: 400, epoch: 8 | loss: 0.4247186
	speed: 0.1681s/iter; left time: 3670.5996s
	iters: 500, epoch: 8 | loss: 0.3912002
	speed: 0.1429s/iter; left time: 3106.2771s
Epoch: 8 cost time: 83.6802146434784
Epoch: 8, Steps: 517 | Train Loss: 0.3971099 Vali Loss: 0.9336134 Test Loss: 0.4156572
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3680861
	speed: 1.0798s/iter; left time: 23339.8995s
	iters: 200, epoch: 9 | loss: 0.4003055
	speed: 0.1641s/iter; left time: 3531.6164s
	iters: 300, epoch: 9 | loss: 0.3880333
	speed: 0.1504s/iter; left time: 3219.8099s
	iters: 400, epoch: 9 | loss: 0.4752170
	speed: 0.1573s/iter; left time: 3352.3285s
	iters: 500, epoch: 9 | loss: 0.3864962
	speed: 0.1610s/iter; left time: 3415.3761s
Epoch: 9 cost time: 84.24232769012451
Epoch: 9, Steps: 517 | Train Loss: 0.3970022 Vali Loss: 0.9327003 Test Loss: 0.4155261
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4220575
	speed: 1.0999s/iter; left time: 23204.6595s
	iters: 200, epoch: 10 | loss: 0.4198205
	speed: 0.1668s/iter; left time: 3503.2911s
	iters: 300, epoch: 10 | loss: 0.4188527
	speed: 0.1629s/iter; left time: 3404.0829s
	iters: 400, epoch: 10 | loss: 0.4140797
	speed: 0.1619s/iter; left time: 3366.4319s
	iters: 500, epoch: 10 | loss: 0.4296773
	speed: 0.1559s/iter; left time: 3227.6789s
Epoch: 10 cost time: 84.51037740707397
Epoch: 10, Steps: 517 | Train Loss: 0.3969077 Vali Loss: 0.9312860 Test Loss: 0.4163905
Validation loss decreased (0.932520 --> 0.931286).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3887250
	speed: 1.0574s/iter; left time: 21761.7878s
	iters: 200, epoch: 11 | loss: 0.4266175
	speed: 0.1514s/iter; left time: 3100.5137s
	iters: 300, epoch: 11 | loss: 0.4178330
	speed: 0.1552s/iter; left time: 3163.5656s
	iters: 400, epoch: 11 | loss: 0.4097154
	speed: 0.1670s/iter; left time: 3387.7160s
	iters: 500, epoch: 11 | loss: 0.4101777
	speed: 0.1635s/iter; left time: 3298.7432s
Epoch: 11 cost time: 82.25748443603516
Epoch: 11, Steps: 517 | Train Loss: 0.3968252 Vali Loss: 0.9307176 Test Loss: 0.4154906
Validation loss decreased (0.931286 --> 0.930718).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4265834
	speed: 1.0433s/iter; left time: 20931.8643s
	iters: 200, epoch: 12 | loss: 0.3523453
	speed: 0.1617s/iter; left time: 3228.1191s
	iters: 300, epoch: 12 | loss: 0.4054928
	speed: 0.1510s/iter; left time: 2999.2277s
	iters: 400, epoch: 12 | loss: 0.3511832
	speed: 0.1574s/iter; left time: 3109.8842s
	iters: 500, epoch: 12 | loss: 0.3890487
	speed: 0.1589s/iter; left time: 3123.9008s
Epoch: 12 cost time: 82.9325270652771
Epoch: 12, Steps: 517 | Train Loss: 0.3968092 Vali Loss: 0.9312078 Test Loss: 0.4160838
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3727604
	speed: 1.0595s/iter; left time: 20709.5565s
	iters: 200, epoch: 13 | loss: 0.4132529
	speed: 0.1630s/iter; left time: 3169.8054s
	iters: 300, epoch: 13 | loss: 0.3656322
	speed: 0.1263s/iter; left time: 2443.1656s
	iters: 400, epoch: 13 | loss: 0.4158247
	speed: 0.1303s/iter; left time: 2508.7159s
	iters: 500, epoch: 13 | loss: 0.4574454
	speed: 0.1367s/iter; left time: 2618.2771s
Epoch: 13 cost time: 75.19526720046997
Epoch: 13, Steps: 517 | Train Loss: 0.3968315 Vali Loss: 0.9294851 Test Loss: 0.4156370
Validation loss decreased (0.930718 --> 0.929485).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3553826
	speed: 1.0995s/iter; left time: 20924.0717s
	iters: 200, epoch: 14 | loss: 0.4355993
	speed: 0.1627s/iter; left time: 3080.5811s
	iters: 300, epoch: 14 | loss: 0.3803926
	speed: 0.1698s/iter; left time: 3197.9741s
	iters: 400, epoch: 14 | loss: 0.3616651
	speed: 0.1704s/iter; left time: 3191.5870s
	iters: 500, epoch: 14 | loss: 0.4298305
	speed: 0.1630s/iter; left time: 3036.1706s
Epoch: 14 cost time: 85.84426856040955
Epoch: 14, Steps: 517 | Train Loss: 0.3967144 Vali Loss: 0.9308257 Test Loss: 0.4160578
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3853708
	speed: 1.0769s/iter; left time: 19936.3157s
	iters: 200, epoch: 15 | loss: 0.3757898
	speed: 0.1678s/iter; left time: 3089.0045s
	iters: 300, epoch: 15 | loss: 0.4288780
	speed: 0.1573s/iter; left time: 2880.6472s
	iters: 400, epoch: 15 | loss: 0.3944523
	speed: 0.1609s/iter; left time: 2931.3110s
	iters: 500, epoch: 15 | loss: 0.4118228
	speed: 0.1683s/iter; left time: 3049.1323s
Epoch: 15 cost time: 86.12848472595215
Epoch: 15, Steps: 517 | Train Loss: 0.3966684 Vali Loss: 0.9295278 Test Loss: 0.4156682
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3778092
	speed: 1.0398s/iter; left time: 18711.8381s
	iters: 200, epoch: 16 | loss: 0.4445509
	speed: 0.1565s/iter; left time: 2800.9210s
	iters: 300, epoch: 16 | loss: 0.3716916
	speed: 0.1469s/iter; left time: 2614.9733s
	iters: 400, epoch: 16 | loss: 0.3587003
	speed: 0.1475s/iter; left time: 2609.4948s
	iters: 500, epoch: 16 | loss: 0.4475129
	speed: 0.1488s/iter; left time: 2618.1354s
Epoch: 16 cost time: 79.17640614509583
Epoch: 16, Steps: 517 | Train Loss: 0.3966758 Vali Loss: 0.9305354 Test Loss: 0.4155733
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4154558777809143, mae:0.41178086400032043, rse:0.613243818283081, corr:[0.52602595 0.5324783  0.53419125 0.53414685 0.5348066  0.53670466
 0.5387261  0.53977907 0.5400332  0.5400938  0.5404147  0.540911
 0.5413696  0.5415127  0.5412022  0.540368   0.53913385 0.5378388
 0.5366476  0.535552   0.5343866  0.5330039  0.5315163  0.53033817
 0.5293569  0.52857685 0.52775186 0.5267048  0.52586854 0.52562356
 0.5261331  0.527265   0.52836746 0.5290214  0.52894104 0.5286913
 0.52843654 0.52827996 0.5280383  0.52750117 0.5269114  0.5264663
 0.5263566  0.52653295 0.5265768  0.5263169  0.52602017 0.52589333
 0.5259261  0.52594644 0.52595997 0.5259402  0.5260407  0.5261883
 0.52637833 0.5264628  0.526343   0.5260113  0.52571785 0.5254671
 0.5252827  0.52511424 0.5249533  0.5247812  0.52486867 0.52529347
 0.52582914 0.52622473 0.5264508  0.5265523  0.52671945 0.5269506
 0.52711016 0.527023   0.52667576 0.52607274 0.5253528  0.52476376
 0.5244548  0.52445    0.52461904 0.5247606  0.52475864 0.524555
 0.52430856 0.5240715  0.5239387  0.5238928  0.5239524  0.5241994
 0.5245865  0.52503234 0.52545196 0.5257396  0.5258544  0.5257704
 0.525633   0.5256145  0.5255771  0.525521   0.5253604  0.5250766
 0.5247379  0.52440584 0.52423435 0.5242864  0.5243469  0.52434367
 0.52416235 0.5238872  0.5236074  0.5233297  0.5230369  0.5227141
 0.5223405  0.52201265 0.5218268  0.5217736  0.5217993  0.52185994
 0.52182716 0.52162385 0.52122104 0.5207843  0.5203928  0.52000207
 0.5196402  0.5194288  0.51935357 0.519375   0.51931614 0.5191183
 0.5189185  0.5188315  0.518837   0.5188849  0.5188919  0.51864403
 0.51829183 0.5181321  0.5182819  0.51869804 0.51909536 0.5191714
 0.5188406  0.51823664 0.5177217  0.5175094  0.51762146 0.5178311
 0.5180011  0.51820487 0.51844686 0.5186481  0.51867527 0.51856965
 0.518368   0.5181562  0.5181026  0.5181683  0.51838297 0.5186707
 0.5189881  0.5192664  0.519436   0.51939917 0.51916754 0.51890844
 0.51882327 0.5190304  0.5195335  0.5200751  0.5202955  0.52011555
 0.5197274  0.5194401  0.5194635  0.5197464  0.52001256 0.52006775
 0.51986784 0.5196094  0.5194962  0.51969075 0.52009857 0.52048546
 0.5206555  0.52065194 0.52062327 0.52067876 0.5208246  0.5209624
 0.5210203  0.52099216 0.5207829  0.5203725  0.5198051  0.51917386
 0.51848996 0.51775724 0.51695824 0.5161884  0.5155109  0.51499146
 0.5146138  0.5142581  0.513781   0.5131494  0.51239586 0.5116332
 0.510939   0.5104129  0.5099967  0.50958306 0.50903904 0.5084591
 0.50793755 0.50746244 0.5070835  0.50676197 0.5065302  0.5064811
 0.50668293 0.50691944 0.50707763 0.5070543  0.50680393 0.5064954
 0.5063684  0.50657284 0.5070176  0.5073806  0.5074664  0.50730294
 0.5070709  0.5070345  0.5072044  0.50762635 0.50802684 0.50825745
 0.50821644 0.5079115  0.507555   0.50730765 0.50721323 0.50714564
 0.50703144 0.5069269  0.5068893  0.50697994 0.5070952  0.5072216
 0.50721407 0.50715977 0.5070799  0.50707304 0.5072172  0.50747854
 0.5077149  0.50778526 0.50761634 0.5073303  0.5071783  0.5074305
 0.5079554  0.5085035  0.5088028  0.508775   0.50848836 0.5081284
 0.5079568  0.50803524 0.50829303 0.5085363  0.5085704  0.5083758
 0.50803804 0.5078231  0.5077698  0.50788784 0.5079739  0.50788033
 0.50768834 0.50749135 0.5074189  0.5075249  0.5076584  0.5075964
 0.5072299  0.5066476  0.50598717 0.5053796  0.50492847 0.5045982
 0.504268   0.5037727  0.5031505  0.50245905 0.50182676 0.5013632
 0.50102514 0.5007619  0.5005086  0.50016785 0.49972415 0.4992291
 0.49889883 0.49872494 0.49869457 0.49873623 0.49880877 0.49878642
 0.49863523 0.498445   0.49824464 0.49813545 0.49817064 0.49834812
 0.4985325  0.49857318 0.49846062 0.49828205 0.49814478 0.49808612
 0.49807507 0.49809176 0.49805003 0.4980067  0.4980161  0.49809968
 0.49827647 0.4983994  0.4983333  0.49812075 0.49787074 0.4978054
 0.4978894  0.49790305 0.49786648 0.49777728 0.49771717 0.49771398
 0.49770537 0.49769527 0.4976021  0.49745247 0.49732783 0.49733478
 0.49742094 0.497564   0.49764395 0.4975304  0.4972693  0.49692377
 0.4965783  0.49636692 0.49632365 0.49638864 0.49649757 0.49656954
 0.4964878  0.49633306 0.4961619  0.4960419  0.49599388 0.4959635
 0.4958478  0.4957296  0.49566528 0.49566582 0.495796   0.49602136
 0.496252   0.49641925 0.4964962  0.4965752  0.49672812 0.49704352
 0.49749392 0.49796236 0.49832088 0.49846655 0.4983819  0.49814913
 0.49784186 0.49751863 0.49720117 0.4968222  0.49634165 0.49591255
 0.49555445 0.49526763 0.49499637 0.49472645 0.4943798  0.49401954
 0.493704   0.49345195 0.49319607 0.49282777 0.4922642  0.49167663
 0.49117103 0.4909074  0.49078646 0.49073038 0.4906606  0.4906408
 0.4906712  0.4907597  0.490842   0.4908851  0.4909115  0.4909226
 0.49106687 0.4911396  0.4911155  0.49089745 0.49063602 0.4905369
 0.4905886  0.4908027  0.4909956  0.49108854 0.4910166  0.4909096
 0.49080053 0.49076137 0.4907502  0.49087393 0.4910318  0.49123657
 0.49128446 0.4910549  0.49067438 0.49036047 0.49024162 0.49031788
 0.49046427 0.4905878  0.49067417 0.49071184 0.49069458 0.4906397
 0.4905602  0.49053323 0.49053797 0.49051097 0.4903831  0.49010217
 0.48980105 0.48961285 0.48966417 0.48992708 0.49024847 0.4904806
 0.4903941  0.4900764  0.48971975 0.4895592  0.48957923 0.48975667
 0.4899525  0.49016985 0.49034584 0.49049982 0.49057198 0.49047384
 0.4902408  0.4899351  0.4897584  0.48981994 0.49008265 0.49038133
 0.4905096  0.4904486  0.4902491  0.48997292 0.48969996 0.48947966
 0.48926094 0.4889392  0.48841888 0.48767325 0.4868223  0.4860861
 0.48561487 0.48524913 0.48485646 0.4843366  0.48360246 0.4827591
 0.48198095 0.48143312 0.4810864  0.48080555 0.4805033  0.48018137
 0.4798249  0.47953787 0.47932497 0.47904056 0.47881693 0.47859865
 0.478404   0.47819385 0.4779685  0.47789222 0.47804195 0.4784479
 0.47896183 0.4793064  0.47937387 0.47932252 0.47925994 0.47931513
 0.4794785  0.47973993 0.4799856  0.48005724 0.4799874  0.47993383
 0.48000163 0.48021778 0.48051867 0.48076037 0.4809409  0.48107317
 0.4811268  0.48112792 0.4811402  0.4812836  0.4814975  0.4815954
 0.48148626 0.4812205  0.48097447 0.48087576 0.48090842 0.48094347
 0.48086986 0.48065615 0.48040193 0.48025963 0.4803023  0.48046702
 0.48050424 0.48036498 0.48020804 0.4801568  0.48026255 0.48050448
 0.48059976 0.48040983 0.48001713 0.4796761  0.47954643 0.4796756
 0.4798887  0.48001522 0.4799484  0.4797304  0.47950277 0.4794158
 0.4795449  0.47995874 0.48040104 0.4807264  0.4808452  0.48081645
 0.4807717  0.48082113 0.48103127 0.48124945 0.48127177 0.4809512
 0.48031345 0.4795163  0.47868326 0.47789454 0.47720695 0.47672182
 0.47637066 0.47608063 0.47574154 0.47530177 0.47479284 0.47422394
 0.4736895  0.4731732  0.47264713 0.47209445 0.47160015 0.4711936
 0.47089785 0.47068357 0.47053182 0.47038218 0.47021478 0.4700339
 0.469873   0.4696454  0.46937045 0.46912628 0.46909627 0.4692919
 0.46968278 0.46998867 0.47008124 0.47001898 0.46999392 0.4702506
 0.4707273  0.47108325 0.4711677  0.47096124 0.47069204 0.4706033
 0.4708371  0.4712894  0.47163653 0.47181773 0.4720425  0.4723691
 0.4726322  0.47274432 0.47263923 0.47239947 0.47232524 0.47253466
 0.47280762 0.47301182 0.47303316 0.47284663 0.47271496 0.47274426
 0.47285947 0.47291073 0.47270265 0.472036   0.47120807 0.4705786
 0.47041202 0.47063604 0.47097942 0.4711566  0.4710183  0.47072908
 0.47046256 0.47030953 0.4703963  0.47064158 0.47085604 0.47092536
 0.470903   0.47091025 0.47100347 0.4710929  0.4710338  0.4707696
 0.47040534 0.47021082 0.47029114 0.47055167 0.47092348 0.47120327
 0.4713183  0.47132853 0.47137505 0.47152764 0.47165495 0.47164664
 0.47148594 0.47124836 0.4708807  0.47033343 0.46970588 0.46915892
 0.4688003  0.46853694 0.4683141  0.46796256 0.467351   0.46680143
 0.4663152  0.46608266 0.46595377 0.46585247 0.4656344  0.46529198
 0.46495888 0.46465853 0.46440968 0.46404204 0.4637091  0.46360955
 0.46373996 0.46404174 0.46417323 0.46394613 0.4635614  0.4631991
 0.46323404 0.46361467 0.46410957 0.4645366  0.46458307 0.46440703
 0.4642557  0.4643313  0.46465537 0.46510807 0.46569774 0.4662787
 0.4669276  0.46751118 0.46775287 0.46790585 0.4675274  0.46402675]
