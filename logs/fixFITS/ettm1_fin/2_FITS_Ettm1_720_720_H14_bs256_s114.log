Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  106688512.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.243170738220215
Epoch: 1, Steps: 64 | Train Loss: 0.6450464 Vali Loss: 1.4784091 Test Loss: 0.7748665
Validation loss decreased (inf --> 1.478409).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 14.60172963142395
Epoch: 2, Steps: 64 | Train Loss: 0.4841958 Vali Loss: 1.3112490 Test Loss: 0.6589546
Validation loss decreased (1.478409 --> 1.311249).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 13.083133935928345
Epoch: 3, Steps: 64 | Train Loss: 0.4124815 Vali Loss: 1.2263242 Test Loss: 0.6019512
Validation loss decreased (1.311249 --> 1.226324).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 14.354258060455322
Epoch: 4, Steps: 64 | Train Loss: 0.3718472 Vali Loss: 1.1716667 Test Loss: 0.5660576
Validation loss decreased (1.226324 --> 1.171667).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 13.296908617019653
Epoch: 5, Steps: 64 | Train Loss: 0.3448622 Vali Loss: 1.1353449 Test Loss: 0.5417243
Validation loss decreased (1.171667 --> 1.135345).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 13.957505464553833
Epoch: 6, Steps: 64 | Train Loss: 0.3248167 Vali Loss: 1.1058706 Test Loss: 0.5230454
Validation loss decreased (1.135345 --> 1.105871).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 13.414723634719849
Epoch: 7, Steps: 64 | Train Loss: 0.3093935 Vali Loss: 1.0856098 Test Loss: 0.5101140
Validation loss decreased (1.105871 --> 1.085610).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 13.735465288162231
Epoch: 8, Steps: 64 | Train Loss: 0.2970774 Vali Loss: 1.0677421 Test Loss: 0.4991741
Validation loss decreased (1.085610 --> 1.067742).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 13.594542503356934
Epoch: 9, Steps: 64 | Train Loss: 0.2870952 Vali Loss: 1.0538568 Test Loss: 0.4907307
Validation loss decreased (1.067742 --> 1.053857).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 13.09837031364441
Epoch: 10, Steps: 64 | Train Loss: 0.2788599 Vali Loss: 1.0414598 Test Loss: 0.4836412
Validation loss decreased (1.053857 --> 1.041460).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.63726019859314
Epoch: 11, Steps: 64 | Train Loss: 0.2719468 Vali Loss: 1.0330350 Test Loss: 0.4782451
Validation loss decreased (1.041460 --> 1.033035).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 12.548508644104004
Epoch: 12, Steps: 64 | Train Loss: 0.2661781 Vali Loss: 1.0256094 Test Loss: 0.4733874
Validation loss decreased (1.033035 --> 1.025609).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 12.641241073608398
Epoch: 13, Steps: 64 | Train Loss: 0.2611944 Vali Loss: 1.0175298 Test Loss: 0.4693065
Validation loss decreased (1.025609 --> 1.017530).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 13.009435653686523
Epoch: 14, Steps: 64 | Train Loss: 0.2567963 Vali Loss: 1.0117447 Test Loss: 0.4655121
Validation loss decreased (1.017530 --> 1.011745).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 14.712733268737793
Epoch: 15, Steps: 64 | Train Loss: 0.2530971 Vali Loss: 1.0068586 Test Loss: 0.4627212
Validation loss decreased (1.011745 --> 1.006859).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 13.464426279067993
Epoch: 16, Steps: 64 | Train Loss: 0.2499902 Vali Loss: 1.0021073 Test Loss: 0.4597985
Validation loss decreased (1.006859 --> 1.002107).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 14.049891710281372
Epoch: 17, Steps: 64 | Train Loss: 0.2470547 Vali Loss: 0.9980959 Test Loss: 0.4575233
Validation loss decreased (1.002107 --> 0.998096).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 12.797415018081665
Epoch: 18, Steps: 64 | Train Loss: 0.2445279 Vali Loss: 0.9937218 Test Loss: 0.4550938
Validation loss decreased (0.998096 --> 0.993722).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.14790964126587
Epoch: 19, Steps: 64 | Train Loss: 0.2421901 Vali Loss: 0.9915633 Test Loss: 0.4534754
Validation loss decreased (0.993722 --> 0.991563).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 12.785076379776001
Epoch: 20, Steps: 64 | Train Loss: 0.2402581 Vali Loss: 0.9877318 Test Loss: 0.4515620
Validation loss decreased (0.991563 --> 0.987732).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 12.591376066207886
Epoch: 21, Steps: 64 | Train Loss: 0.2383554 Vali Loss: 0.9851968 Test Loss: 0.4499471
Validation loss decreased (0.987732 --> 0.985197).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 13.05049443244934
Epoch: 22, Steps: 64 | Train Loss: 0.2368070 Vali Loss: 0.9830335 Test Loss: 0.4484917
Validation loss decreased (0.985197 --> 0.983033).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 12.876180410385132
Epoch: 23, Steps: 64 | Train Loss: 0.2352621 Vali Loss: 0.9812377 Test Loss: 0.4472270
Validation loss decreased (0.983033 --> 0.981238).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 12.561774492263794
Epoch: 24, Steps: 64 | Train Loss: 0.2339833 Vali Loss: 0.9788765 Test Loss: 0.4459465
Validation loss decreased (0.981238 --> 0.978876).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 12.874854803085327
Epoch: 25, Steps: 64 | Train Loss: 0.2328190 Vali Loss: 0.9772448 Test Loss: 0.4447584
Validation loss decreased (0.978876 --> 0.977245).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 12.44674301147461
Epoch: 26, Steps: 64 | Train Loss: 0.2316499 Vali Loss: 0.9753364 Test Loss: 0.4437289
Validation loss decreased (0.977245 --> 0.975336).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 13.30135178565979
Epoch: 27, Steps: 64 | Train Loss: 0.2306461 Vali Loss: 0.9731806 Test Loss: 0.4426650
Validation loss decreased (0.975336 --> 0.973181).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 13.759063005447388
Epoch: 28, Steps: 64 | Train Loss: 0.2296571 Vali Loss: 0.9708282 Test Loss: 0.4418391
Validation loss decreased (0.973181 --> 0.970828).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 13.93934679031372
Epoch: 29, Steps: 64 | Train Loss: 0.2288501 Vali Loss: 0.9705816 Test Loss: 0.4410399
Validation loss decreased (0.970828 --> 0.970582).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 13.614729404449463
Epoch: 30, Steps: 64 | Train Loss: 0.2280130 Vali Loss: 0.9686872 Test Loss: 0.4402434
Validation loss decreased (0.970582 --> 0.968687).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 11.23337697982788
Epoch: 31, Steps: 64 | Train Loss: 0.2273481 Vali Loss: 0.9680323 Test Loss: 0.4394264
Validation loss decreased (0.968687 --> 0.968032).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 12.858047485351562
Epoch: 32, Steps: 64 | Train Loss: 0.2266534 Vali Loss: 0.9667673 Test Loss: 0.4388153
Validation loss decreased (0.968032 --> 0.966767).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 12.487895727157593
Epoch: 33, Steps: 64 | Train Loss: 0.2260382 Vali Loss: 0.9663634 Test Loss: 0.4381115
Validation loss decreased (0.966767 --> 0.966363).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 11.415585041046143
Epoch: 34, Steps: 64 | Train Loss: 0.2254340 Vali Loss: 0.9648598 Test Loss: 0.4375694
Validation loss decreased (0.966363 --> 0.964860).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 11.38199257850647
Epoch: 35, Steps: 64 | Train Loss: 0.2249316 Vali Loss: 0.9637193 Test Loss: 0.4369867
Validation loss decreased (0.964860 --> 0.963719).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 11.435890436172485
Epoch: 36, Steps: 64 | Train Loss: 0.2243172 Vali Loss: 0.9632813 Test Loss: 0.4364848
Validation loss decreased (0.963719 --> 0.963281).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 11.247266292572021
Epoch: 37, Steps: 64 | Train Loss: 0.2238609 Vali Loss: 0.9626078 Test Loss: 0.4360504
Validation loss decreased (0.963281 --> 0.962608).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 11.164727926254272
Epoch: 38, Steps: 64 | Train Loss: 0.2233650 Vali Loss: 0.9616783 Test Loss: 0.4356310
Validation loss decreased (0.962608 --> 0.961678).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 11.978749752044678
Epoch: 39, Steps: 64 | Train Loss: 0.2231003 Vali Loss: 0.9612444 Test Loss: 0.4351806
Validation loss decreased (0.961678 --> 0.961244).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 11.614186763763428
Epoch: 40, Steps: 64 | Train Loss: 0.2225838 Vali Loss: 0.9604489 Test Loss: 0.4347755
Validation loss decreased (0.961244 --> 0.960449).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 11.82268238067627
Epoch: 41, Steps: 64 | Train Loss: 0.2222723 Vali Loss: 0.9596107 Test Loss: 0.4344502
Validation loss decreased (0.960449 --> 0.959611).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 12.964399814605713
Epoch: 42, Steps: 64 | Train Loss: 0.2219222 Vali Loss: 0.9587201 Test Loss: 0.4340714
Validation loss decreased (0.959611 --> 0.958720).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 12.787089586257935
Epoch: 43, Steps: 64 | Train Loss: 0.2216229 Vali Loss: 0.9594874 Test Loss: 0.4337744
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 12.676859378814697
Epoch: 44, Steps: 64 | Train Loss: 0.2214110 Vali Loss: 0.9579884 Test Loss: 0.4334503
Validation loss decreased (0.958720 --> 0.957988).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 12.711515426635742
Epoch: 45, Steps: 64 | Train Loss: 0.2210287 Vali Loss: 0.9578837 Test Loss: 0.4331546
Validation loss decreased (0.957988 --> 0.957884).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 11.52179741859436
Epoch: 46, Steps: 64 | Train Loss: 0.2208295 Vali Loss: 0.9567789 Test Loss: 0.4329157
Validation loss decreased (0.957884 --> 0.956779).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 11.89355754852295
Epoch: 47, Steps: 64 | Train Loss: 0.2205893 Vali Loss: 0.9566141 Test Loss: 0.4326377
Validation loss decreased (0.956779 --> 0.956614).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 11.074268579483032
Epoch: 48, Steps: 64 | Train Loss: 0.2203944 Vali Loss: 0.9561944 Test Loss: 0.4324082
Validation loss decreased (0.956614 --> 0.956194).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 10.057043075561523
Epoch: 49, Steps: 64 | Train Loss: 0.2200803 Vali Loss: 0.9560308 Test Loss: 0.4321971
Validation loss decreased (0.956194 --> 0.956031).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 11.076843500137329
Epoch: 50, Steps: 64 | Train Loss: 0.2199456 Vali Loss: 0.9553183 Test Loss: 0.4319853
Validation loss decreased (0.956031 --> 0.955318).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  106688512.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.458024024963379
Epoch: 1, Steps: 64 | Train Loss: 0.4061636 Vali Loss: 0.9422353 Test Loss: 0.4238374
Validation loss decreased (inf --> 0.942235).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.483982563018799
Epoch: 2, Steps: 64 | Train Loss: 0.4009424 Vali Loss: 0.9351363 Test Loss: 0.4208626
Validation loss decreased (0.942235 --> 0.935136).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.176975011825562
Epoch: 3, Steps: 64 | Train Loss: 0.3986358 Vali Loss: 0.9346184 Test Loss: 0.4204449
Validation loss decreased (0.935136 --> 0.934618).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.097373008728027
Epoch: 4, Steps: 64 | Train Loss: 0.3976601 Vali Loss: 0.9333267 Test Loss: 0.4203276
Validation loss decreased (0.934618 --> 0.933327).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.691096067428589
Epoch: 5, Steps: 64 | Train Loss: 0.3971336 Vali Loss: 0.9329404 Test Loss: 0.4205095
Validation loss decreased (0.933327 --> 0.932940).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.073412895202637
Epoch: 6, Steps: 64 | Train Loss: 0.3966928 Vali Loss: 0.9320605 Test Loss: 0.4205424
Validation loss decreased (0.932940 --> 0.932061).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.34813666343689
Epoch: 7, Steps: 64 | Train Loss: 0.3967449 Vali Loss: 0.9322078 Test Loss: 0.4203190
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.107796430587769
Epoch: 8, Steps: 64 | Train Loss: 0.3967943 Vali Loss: 0.9323094 Test Loss: 0.4206807
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.128914594650269
Epoch: 9, Steps: 64 | Train Loss: 0.3964197 Vali Loss: 0.9325266 Test Loss: 0.4209121
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41491439938545227, mae:0.4117562174797058, rse:0.6128441095352173, corr:[0.5242553  0.53514516 0.5405465  0.5409142  0.54106474 0.54273015
 0.5447533  0.54552275 0.5449657  0.54432374 0.54449594 0.5451149
 0.54526365 0.5444582  0.5430242  0.5416103  0.54051626 0.5395581
 0.5383867  0.53698087 0.53558785 0.5343346  0.53315157 0.53198063
 0.5306054  0.5292572  0.52818716 0.5274417  0.5270505  0.5269441
 0.5271609  0.52785957 0.5288738  0.5298295  0.53012913 0.52982247
 0.5291927  0.52883554 0.52893496 0.52914864 0.52916896 0.5287946
 0.5283298  0.52823746 0.528483   0.52872235 0.5287283  0.52846473
 0.52812916 0.5279667  0.5280463  0.5280801  0.52791804 0.52757746
 0.5273747  0.52747864 0.5277305  0.527787   0.5275513  0.5271002
 0.5267688  0.52674717 0.5268876  0.52685666 0.5266493  0.52643704
 0.5264066  0.5266076  0.5269105  0.52709615 0.5271752  0.5272418
 0.52735746 0.52744466 0.5273804  0.5270303  0.52647287 0.5260052
 0.5258541  0.52602714 0.52630055 0.52640706 0.52624536 0.5259046
 0.5256714  0.52563083 0.52575433 0.5258999  0.526025   0.5262231
 0.52655435 0.52693534 0.5271719  0.52705055 0.52657527 0.52591395
 0.52538514 0.5251985  0.5251767  0.52519226 0.5250565  0.52475005
 0.5243992  0.524117   0.52404416 0.5241887  0.52431446 0.5243292
 0.524133   0.52380246 0.52344674 0.52314013 0.52292573 0.5227856
 0.5226357  0.5224155  0.5220966  0.5217283  0.52143615 0.5213266
 0.5212943  0.5211432  0.5207214  0.5201883  0.5198128  0.5197443
 0.5199251  0.52013093 0.52006936 0.51974046 0.51933384 0.5191649
 0.5193661  0.5196933  0.51978356 0.5195758  0.5193206  0.51919436
 0.51934046 0.5196369  0.5197818  0.51973194 0.519661   0.51977986
 0.5201074  0.52042043 0.52056456 0.5204896  0.520321   0.520166
 0.52005917 0.5199944  0.5198583  0.5196244  0.5193967  0.51937973
 0.5195563  0.51978296 0.51994497 0.5199209  0.5198347  0.5197928
 0.5198552  0.52000356 0.5202143  0.5204499  0.52068275 0.5208925
 0.5209887  0.5209266  0.52082086 0.52075344 0.5207073  0.52069247
 0.5207112  0.52073264 0.5207393  0.5207156  0.5206489  0.5205767
 0.5205253  0.52054715 0.52061766 0.5207538  0.52093256 0.5211352
 0.5213654  0.5216729  0.52204317 0.52239674 0.5226648  0.52276486
 0.5226815  0.52243    0.5219603  0.52130395 0.520535   0.5197448
 0.51895314 0.51818335 0.517414   0.51667905 0.5159814  0.51534396
 0.51477474 0.5142406  0.5136933  0.51315266 0.5126211  0.5121112
 0.5116163  0.5111527  0.5106681  0.51013464 0.5095169  0.5089202
 0.5083923  0.50786    0.5073594  0.5069454  0.50674707 0.5068404
 0.50716364 0.50741434 0.50750995 0.50750494 0.507461   0.5074674
 0.507528   0.507576   0.50754756 0.50744075 0.50739735 0.5075337
 0.50783145 0.50821143 0.508452   0.5086032  0.5086383  0.5086898
 0.5087847  0.5088436  0.5088608  0.50881493 0.5086716  0.508414
 0.508152   0.5080361  0.50810647 0.5082886  0.5083811  0.5083397
 0.50815237 0.50803006 0.5080451  0.50818354 0.50835454 0.50845283
 0.5084511  0.5084154  0.5084098  0.5084597  0.50854474 0.5087081
 0.5088305  0.50892496 0.50900674 0.50910455 0.5091467  0.5090544
 0.50887567 0.5086526  0.5084628  0.5083314  0.50823325 0.50815934
 0.50808674 0.5081244  0.5082227  0.50840586 0.50856984 0.50864625
 0.5087032  0.50873667 0.5087633  0.5087615  0.5086182  0.5082522
 0.5076813  0.50703084 0.5063964  0.5058169  0.5053411  0.5049374
 0.5045264  0.50399774 0.50342923 0.5028802  0.5024169  0.5020661
 0.50172937 0.50136036 0.5009787  0.5006235  0.500353   0.5001685
 0.5001316  0.500097   0.49996716 0.49971178 0.49943304 0.49916354
 0.49895635 0.4988875  0.49888942 0.4989139  0.49891412 0.4989118
 0.49890894 0.49888897 0.4988614  0.498798   0.49865618 0.49841926
 0.4981486  0.4979799  0.49789253 0.49784014 0.4977028  0.49742946
 0.49715084 0.49696147 0.49692553 0.49707147 0.49728844 0.4975288
 0.4976449  0.497507   0.49730948 0.49719092 0.49725622 0.49745464
 0.49762377 0.49769163 0.49759558 0.49740192 0.49723056 0.49721262
 0.49729076 0.49740085 0.49739528 0.4971609  0.496793   0.49647063
 0.4963709  0.4965665  0.49690118 0.49715307 0.49721912 0.4971181
 0.49690318 0.49676317 0.496716   0.49669924 0.4966476  0.4965307
 0.49633583 0.49620607 0.49617472 0.49615532 0.49612236 0.49607262
 0.4960672  0.4961996  0.49647337 0.49684146 0.4971725  0.4974297
 0.49765116 0.49791983 0.4982552  0.49859342 0.49882227 0.49887127
 0.49872717 0.4984545  0.49813575 0.49774936 0.49722797 0.49668556
 0.49617976 0.49580923 0.49559775 0.4955074  0.49534184 0.49502054
 0.494572   0.4941381  0.4938288  0.493613   0.49331734 0.49294227
 0.4924862  0.4921265  0.49187538 0.4917217  0.49152836 0.49125263
 0.49090704 0.49062905 0.49050876 0.49055347 0.49070835 0.49084094
 0.4910333  0.4911512  0.4912747  0.49131685 0.49130988 0.49129063
 0.49118465 0.49107638 0.49096018 0.4908874  0.49083453 0.49085623
 0.49087027 0.49087083 0.49082306 0.49086443 0.49095857 0.49115348
 0.49126494 0.4911434  0.490855   0.4905636  0.49037907 0.49034077
 0.4903554  0.49032897 0.49025694 0.49021143 0.49029863 0.49053913
 0.49079195 0.4909265  0.4908272  0.49054515 0.49025843 0.49011755
 0.49021825 0.49044773 0.49066576 0.49076375 0.49075735 0.49076355
 0.49068847 0.49057147 0.4904345  0.4903599  0.49032933 0.4904118
 0.49054924 0.4907078  0.490759   0.49070483 0.49058527 0.4904622
 0.49042377 0.49044344 0.49051067 0.4905646  0.4905828  0.4905947
 0.49061278 0.49070552 0.4908313  0.49085134 0.49068493 0.49035367
 0.48991284 0.48944303 0.48897395 0.48844093 0.48780373 0.4871444
 0.4865904  0.48607963 0.48560312 0.48509163 0.48441827 0.48360354
 0.4827853  0.4821496  0.48171106 0.48134166 0.48092777 0.48044574
 0.4798969  0.47942325 0.4790926  0.47880393 0.47867644 0.4786273
 0.47861785 0.47855002 0.47836918 0.47820738 0.47816223 0.4783529
 0.47874087 0.47909644 0.47928607 0.4793969  0.47945514 0.4795362
 0.4796509  0.4798463  0.48005053 0.48011646 0.4800647  0.48002368
 0.48006618 0.48019695 0.48034236 0.4804233  0.48052555 0.4807024
 0.48086277 0.48090327 0.48078808 0.48063594 0.48052332 0.48047528
 0.48051837 0.48062924 0.4807834  0.48092297 0.48098284 0.48094124
 0.48083797 0.48072115 0.48063728 0.48056486 0.480467   0.48037803
 0.48030144 0.4802872  0.48036414 0.48045295 0.48048145 0.48045993
 0.48030853 0.48014322 0.48011565 0.48029423 0.4805689  0.48082325
 0.4809376  0.48093104 0.4808791  0.48087177 0.480928   0.48097438
 0.48092946 0.48088464 0.48081362 0.48080817 0.48089477 0.48106715
 0.48127234 0.48144004 0.48155922 0.48156124 0.48142236 0.48116753
 0.48083076 0.48041502 0.47985774 0.47913408 0.47835025 0.47775704
 0.47737193 0.47708496 0.47672698 0.47624138 0.47569692 0.47516176
 0.4747309  0.47431216 0.47379276 0.4731085  0.47238097 0.47173807
 0.47126824 0.47092956 0.47066897 0.47042385 0.47019556 0.4700252
 0.46996716 0.46992335 0.46987596 0.46983373 0.46987727 0.46999714
 0.47026885 0.47055766 0.47077057 0.4708692  0.47086722 0.47088984
 0.47098318 0.47109458 0.4712563  0.47139728 0.47149912 0.47154936
 0.47161803 0.4717613  0.47188625 0.47201258 0.47225127 0.47256365
 0.47281715 0.47305304 0.47325018 0.47335756 0.47342703 0.4734616
 0.47342977 0.4734891  0.4736843  0.47387537 0.47396156 0.47375944
 0.47323677 0.47258857 0.47205618 0.47166115 0.47148135 0.4714386
 0.471498   0.47160685 0.47170204 0.4717048  0.47155634 0.47136813
 0.47122934 0.47118145 0.47127825 0.4714052  0.47141474 0.47127807
 0.47110742 0.471056   0.47119385 0.4714549  0.47167164 0.47172397
 0.47162417 0.47152153 0.47145298 0.47136238 0.47129157 0.47123015
 0.471239   0.47136405 0.4715777  0.4717686  0.47176677 0.4715236
 0.47111955 0.4707299  0.47037384 0.46995547 0.46940714 0.46876585
 0.46813902 0.46755797 0.46711773 0.4667377  0.4662601  0.46586818
 0.4654798  0.46523222 0.46501854 0.46483698 0.46459955 0.46427882
 0.4639277  0.46352673 0.46311426 0.4626001  0.46217486 0.46202913
 0.46218863 0.46265566 0.4631529  0.46348152 0.46369678 0.46376705
 0.46389976 0.46413553 0.46448097 0.46498892 0.46545473 0.46589306
 0.46625417 0.46657428 0.4669411  0.46739805 0.46804905 0.4685973
 0.46895513 0.46922985 0.46968177 0.47053474 0.46904445 0.45830393]
