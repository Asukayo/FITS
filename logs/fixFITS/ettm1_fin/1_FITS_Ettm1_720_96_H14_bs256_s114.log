Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_96_FITS_ETTm1_ftM_sl720_ll48_pl96_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 11425
test 11425
Model(
  (freq_upsampler): Linear(in_features=122, out_features=138, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  60340224.0
params:  16974.0
Trainable parameters:  16974
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.080328464508057
Epoch: 1, Steps: 65 | Train Loss: 0.3999722 Vali Loss: 0.5268091 Test Loss: 0.3448455
Validation loss decreased (inf --> 0.526809).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.038032054901123
Epoch: 2, Steps: 65 | Train Loss: 0.2950110 Vali Loss: 0.4606116 Test Loss: 0.3191134
Validation loss decreased (0.526809 --> 0.460612).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.65853500366211
Epoch: 3, Steps: 65 | Train Loss: 0.2798028 Vali Loss: 0.4416415 Test Loss: 0.3144253
Validation loss decreased (0.460612 --> 0.441642).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.062653064727783
Epoch: 4, Steps: 65 | Train Loss: 0.2741124 Vali Loss: 0.4293238 Test Loss: 0.3127234
Validation loss decreased (0.441642 --> 0.429324).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.390987157821655
Epoch: 5, Steps: 65 | Train Loss: 0.2708981 Vali Loss: 0.4234391 Test Loss: 0.3112865
Validation loss decreased (0.429324 --> 0.423439).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.116332054138184
Epoch: 6, Steps: 65 | Train Loss: 0.2690622 Vali Loss: 0.4177171 Test Loss: 0.3103290
Validation loss decreased (0.423439 --> 0.417717).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.384121656417847
Epoch: 7, Steps: 65 | Train Loss: 0.2671052 Vali Loss: 0.4144200 Test Loss: 0.3099603
Validation loss decreased (0.417717 --> 0.414420).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.90168809890747
Epoch: 8, Steps: 65 | Train Loss: 0.2664376 Vali Loss: 0.4113018 Test Loss: 0.3097398
Validation loss decreased (0.414420 --> 0.411302).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.541613340377808
Epoch: 9, Steps: 65 | Train Loss: 0.2657459 Vali Loss: 0.4098724 Test Loss: 0.3095416
Validation loss decreased (0.411302 --> 0.409872).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.94164752960205
Epoch: 10, Steps: 65 | Train Loss: 0.2653095 Vali Loss: 0.4075122 Test Loss: 0.3093266
Validation loss decreased (0.409872 --> 0.407512).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.835246801376343
Epoch: 11, Steps: 65 | Train Loss: 0.2648505 Vali Loss: 0.4056378 Test Loss: 0.3091749
Validation loss decreased (0.407512 --> 0.405638).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.171974182128906
Epoch: 12, Steps: 65 | Train Loss: 0.2641454 Vali Loss: 0.4041633 Test Loss: 0.3091637
Validation loss decreased (0.405638 --> 0.404163).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.382720947265625
Epoch: 13, Steps: 65 | Train Loss: 0.2639412 Vali Loss: 0.4037397 Test Loss: 0.3089900
Validation loss decreased (0.404163 --> 0.403740).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.34169316291809
Epoch: 14, Steps: 65 | Train Loss: 0.2638571 Vali Loss: 0.4030761 Test Loss: 0.3089910
Validation loss decreased (0.403740 --> 0.403076).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.301454305648804
Epoch: 15, Steps: 65 | Train Loss: 0.2635817 Vali Loss: 0.4014214 Test Loss: 0.3089988
Validation loss decreased (0.403076 --> 0.401421).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.122751951217651
Epoch: 16, Steps: 65 | Train Loss: 0.2632073 Vali Loss: 0.4006179 Test Loss: 0.3089516
Validation loss decreased (0.401421 --> 0.400618).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.063470602035522
Epoch: 17, Steps: 65 | Train Loss: 0.2631988 Vali Loss: 0.4004776 Test Loss: 0.3088669
Validation loss decreased (0.400618 --> 0.400478).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.846721887588501
Epoch: 18, Steps: 65 | Train Loss: 0.2629436 Vali Loss: 0.4015880 Test Loss: 0.3087821
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.981626033782959
Epoch: 19, Steps: 65 | Train Loss: 0.2630320 Vali Loss: 0.3999088 Test Loss: 0.3088455
Validation loss decreased (0.400478 --> 0.399909).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.65407419204712
Epoch: 20, Steps: 65 | Train Loss: 0.2628540 Vali Loss: 0.4003948 Test Loss: 0.3089460
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 10.474817037582397
Epoch: 21, Steps: 65 | Train Loss: 0.2626548 Vali Loss: 0.4001083 Test Loss: 0.3087327
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.272607803344727
Epoch: 22, Steps: 65 | Train Loss: 0.2626024 Vali Loss: 0.3989900 Test Loss: 0.3088036
Validation loss decreased (0.399909 --> 0.398990).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 11.625090837478638
Epoch: 23, Steps: 65 | Train Loss: 0.2626801 Vali Loss: 0.3992505 Test Loss: 0.3086465
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.34857439994812
Epoch: 24, Steps: 65 | Train Loss: 0.2624651 Vali Loss: 0.3979002 Test Loss: 0.3085348
Validation loss decreased (0.398990 --> 0.397900).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.7914137840271
Epoch: 25, Steps: 65 | Train Loss: 0.2623259 Vali Loss: 0.3985504 Test Loss: 0.3086495
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 12.06090521812439
Epoch: 26, Steps: 65 | Train Loss: 0.2624128 Vali Loss: 0.3979453 Test Loss: 0.3086974
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 11.544395923614502
Epoch: 27, Steps: 65 | Train Loss: 0.2623925 Vali Loss: 0.3982927 Test Loss: 0.3086846
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_96_FITS_ETTm1_ftM_sl720_ll48_pl96_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
mse:0.309054434299469, mae:0.35194048285484314, rse:0.5289922952651978, corr:[0.5435125  0.55597544 0.5609835  0.5618334  0.56296813 0.5651779
 0.56710595 0.56784815 0.56807035 0.56860465 0.56948304 0.57005125
 0.56998533 0.5695339  0.5691091  0.56870306 0.56799793 0.5668136
 0.5652872  0.5639042  0.5629011  0.56191957 0.56059754 0.55900747
 0.55730057 0.556088   0.5554697  0.5549396  0.5543278  0.55371815
 0.5535599  0.55423987 0.55537325 0.55632013 0.55643487 0.5560601
 0.55561227 0.55553013 0.555654   0.5554923  0.55500764 0.55437773
 0.55406    0.5542681  0.5544619  0.55424553 0.55379283 0.5535103
 0.55361193 0.55383146 0.5538517  0.55347645 0.5529564  0.55263937
 0.5528123  0.55324024 0.5534739  0.5532593  0.55294514 0.5527906
 0.5529364  0.55310696 0.5529938  0.55244565 0.5519623  0.55199033
 0.5524408  0.5528726  0.55297863 0.5527292  0.5524969  0.5525097
 0.5526337  0.5525716  0.5522491  0.5516491  0.55100554 0.55057347
 0.55032104 0.55002135 0.5494651  0.54862434 0.5478766  0.54740494
 0.54721415 0.54688025 0.5461605  0.54527456 0.5448161  0.5451597
 0.545684   0.5454964  0.5449051  0.5451773  0.54654455 0.54550624]
