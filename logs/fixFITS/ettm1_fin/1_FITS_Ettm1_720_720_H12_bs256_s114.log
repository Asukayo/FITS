Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  80539648.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.570042371749878
Epoch: 1, Steps: 64 | Train Loss: 0.6573653 Vali Loss: 1.2545147 Test Loss: 0.6117072
Validation loss decreased (inf --> 1.254515).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 12.11258316040039
Epoch: 2, Steps: 64 | Train Loss: 0.4936467 Vali Loss: 1.1022341 Test Loss: 0.5064035
Validation loss decreased (1.254515 --> 1.102234).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 12.499959945678711
Epoch: 3, Steps: 64 | Train Loss: 0.4512792 Vali Loss: 1.0442193 Test Loss: 0.4707178
Validation loss decreased (1.102234 --> 1.044219).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 13.132784128189087
Epoch: 4, Steps: 64 | Train Loss: 0.4341006 Vali Loss: 1.0151765 Test Loss: 0.4537523
Validation loss decreased (1.044219 --> 1.015177).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 12.807040691375732
Epoch: 5, Steps: 64 | Train Loss: 0.4238982 Vali Loss: 0.9973069 Test Loss: 0.4432788
Validation loss decreased (1.015177 --> 0.997307).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 12.421334505081177
Epoch: 6, Steps: 64 | Train Loss: 0.4177206 Vali Loss: 0.9846213 Test Loss: 0.4359417
Validation loss decreased (0.997307 --> 0.984621).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 13.191146850585938
Epoch: 7, Steps: 64 | Train Loss: 0.4128025 Vali Loss: 0.9753211 Test Loss: 0.4307672
Validation loss decreased (0.984621 --> 0.975321).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 13.249153852462769
Epoch: 8, Steps: 64 | Train Loss: 0.4094723 Vali Loss: 0.9681680 Test Loss: 0.4269949
Validation loss decreased (0.975321 --> 0.968168).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 14.91054892539978
Epoch: 9, Steps: 64 | Train Loss: 0.4068308 Vali Loss: 0.9625102 Test Loss: 0.4243484
Validation loss decreased (0.968168 --> 0.962510).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 15.438698291778564
Epoch: 10, Steps: 64 | Train Loss: 0.4043954 Vali Loss: 0.9584082 Test Loss: 0.4226196
Validation loss decreased (0.962510 --> 0.958408).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 14.617072105407715
Epoch: 11, Steps: 64 | Train Loss: 0.4034150 Vali Loss: 0.9547863 Test Loss: 0.4213269
Validation loss decreased (0.958408 --> 0.954786).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 13.745748281478882
Epoch: 12, Steps: 64 | Train Loss: 0.4021103 Vali Loss: 0.9521798 Test Loss: 0.4203951
Validation loss decreased (0.954786 --> 0.952180).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 13.449527978897095
Epoch: 13, Steps: 64 | Train Loss: 0.4012613 Vali Loss: 0.9504402 Test Loss: 0.4198371
Validation loss decreased (0.952180 --> 0.950440).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 13.553618669509888
Epoch: 14, Steps: 64 | Train Loss: 0.4003695 Vali Loss: 0.9483048 Test Loss: 0.4195823
Validation loss decreased (0.950440 --> 0.948305).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 13.289450407028198
Epoch: 15, Steps: 64 | Train Loss: 0.3999350 Vali Loss: 0.9463235 Test Loss: 0.4193057
Validation loss decreased (0.948305 --> 0.946324).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 13.879194498062134
Epoch: 16, Steps: 64 | Train Loss: 0.3993187 Vali Loss: 0.9452425 Test Loss: 0.4190040
Validation loss decreased (0.946324 --> 0.945242).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 13.429545640945435
Epoch: 17, Steps: 64 | Train Loss: 0.3989188 Vali Loss: 0.9448180 Test Loss: 0.4189533
Validation loss decreased (0.945242 --> 0.944818).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 13.241682052612305
Epoch: 18, Steps: 64 | Train Loss: 0.3987010 Vali Loss: 0.9434466 Test Loss: 0.4191542
Validation loss decreased (0.944818 --> 0.943447).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 13.413572788238525
Epoch: 19, Steps: 64 | Train Loss: 0.3984810 Vali Loss: 0.9426474 Test Loss: 0.4191062
Validation loss decreased (0.943447 --> 0.942647).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 12.653055429458618
Epoch: 20, Steps: 64 | Train Loss: 0.3983880 Vali Loss: 0.9418803 Test Loss: 0.4190736
Validation loss decreased (0.942647 --> 0.941880).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 13.59513258934021
Epoch: 21, Steps: 64 | Train Loss: 0.3982280 Vali Loss: 0.9411141 Test Loss: 0.4191347
Validation loss decreased (0.941880 --> 0.941114).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 14.78021764755249
Epoch: 22, Steps: 64 | Train Loss: 0.3979708 Vali Loss: 0.9407834 Test Loss: 0.4190861
Validation loss decreased (0.941114 --> 0.940783).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 14.794031858444214
Epoch: 23, Steps: 64 | Train Loss: 0.3980855 Vali Loss: 0.9397846 Test Loss: 0.4191259
Validation loss decreased (0.940783 --> 0.939785).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 13.720796346664429
Epoch: 24, Steps: 64 | Train Loss: 0.3977891 Vali Loss: 0.9389459 Test Loss: 0.4192691
Validation loss decreased (0.939785 --> 0.938946).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 13.307512760162354
Epoch: 25, Steps: 64 | Train Loss: 0.3977621 Vali Loss: 0.9393716 Test Loss: 0.4192905
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 13.428112983703613
Epoch: 26, Steps: 64 | Train Loss: 0.3978552 Vali Loss: 0.9394765 Test Loss: 0.4194349
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 14.189221858978271
Epoch: 27, Steps: 64 | Train Loss: 0.3974516 Vali Loss: 0.9375833 Test Loss: 0.4195857
Validation loss decreased (0.938946 --> 0.937583).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 12.96194338798523
Epoch: 28, Steps: 64 | Train Loss: 0.3974534 Vali Loss: 0.9384004 Test Loss: 0.4196090
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 12.859071493148804
Epoch: 29, Steps: 64 | Train Loss: 0.3974922 Vali Loss: 0.9370331 Test Loss: 0.4196405
Validation loss decreased (0.937583 --> 0.937033).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 12.274547576904297
Epoch: 30, Steps: 64 | Train Loss: 0.3974457 Vali Loss: 0.9374402 Test Loss: 0.4196659
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 12.982316732406616
Epoch: 31, Steps: 64 | Train Loss: 0.3973587 Vali Loss: 0.9382337 Test Loss: 0.4197688
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 14.52683401107788
Epoch: 32, Steps: 64 | Train Loss: 0.3975424 Vali Loss: 0.9369891 Test Loss: 0.4197349
Validation loss decreased (0.937033 --> 0.936989).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 12.109010457992554
Epoch: 33, Steps: 64 | Train Loss: 0.3974087 Vali Loss: 0.9371253 Test Loss: 0.4197855
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 10.847188472747803
Epoch: 34, Steps: 64 | Train Loss: 0.3975499 Vali Loss: 0.9368018 Test Loss: 0.4198647
Validation loss decreased (0.936989 --> 0.936802).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 11.147162437438965
Epoch: 35, Steps: 64 | Train Loss: 0.3973597 Vali Loss: 0.9367247 Test Loss: 0.4199542
Validation loss decreased (0.936802 --> 0.936725).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 10.794175386428833
Epoch: 36, Steps: 64 | Train Loss: 0.3969891 Vali Loss: 0.9366500 Test Loss: 0.4199209
Validation loss decreased (0.936725 --> 0.936650).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 10.983700275421143
Epoch: 37, Steps: 64 | Train Loss: 0.3971080 Vali Loss: 0.9363925 Test Loss: 0.4199313
Validation loss decreased (0.936650 --> 0.936392).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 9.916212320327759
Epoch: 38, Steps: 64 | Train Loss: 0.3968886 Vali Loss: 0.9359277 Test Loss: 0.4199822
Validation loss decreased (0.936392 --> 0.935928).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 7.314836502075195
Epoch: 39, Steps: 64 | Train Loss: 0.3972175 Vali Loss: 0.9367253 Test Loss: 0.4199793
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 7.512538433074951
Epoch: 40, Steps: 64 | Train Loss: 0.3970102 Vali Loss: 0.9362146 Test Loss: 0.4199897
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 7.484741926193237
Epoch: 41, Steps: 64 | Train Loss: 0.3970328 Vali Loss: 0.9356064 Test Loss: 0.4200500
Validation loss decreased (0.935928 --> 0.935606).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 6.424114465713501
Epoch: 42, Steps: 64 | Train Loss: 0.3969852 Vali Loss: 0.9353268 Test Loss: 0.4201436
Validation loss decreased (0.935606 --> 0.935327).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 11.152266502380371
Epoch: 43, Steps: 64 | Train Loss: 0.3967511 Vali Loss: 0.9365659 Test Loss: 0.4201172
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 11.268481731414795
Epoch: 44, Steps: 64 | Train Loss: 0.3971039 Vali Loss: 0.9361552 Test Loss: 0.4201405
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 10.455878257751465
Epoch: 45, Steps: 64 | Train Loss: 0.3969715 Vali Loss: 0.9361347 Test Loss: 0.4201437
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4144671857357025, mae:0.4111383259296417, rse:0.6125137209892273, corr:[0.52434146 0.5296903  0.532412   0.5336332  0.53495646 0.5368151
 0.5384245  0.5393038  0.5398087  0.5404505  0.5414606  0.5423583
 0.54255795 0.5418341  0.5405541  0.5393662  0.5386901  0.5383716
 0.53778785 0.5365493  0.53479123 0.5329465  0.531543   0.5309637
 0.53075916 0.530405   0.5295118  0.528182   0.5271654  0.52711546
 0.52812856 0.52969927 0.5309028  0.5312597  0.5306918  0.52994233
 0.5295211  0.529645   0.52999103 0.53001946 0.5296068  0.5288504
 0.5281948  0.52803886 0.5282473  0.5284607  0.5284569  0.5281485
 0.5276423  0.52717227 0.52698433 0.52696496 0.52693146 0.5266665
 0.52625954 0.5259224  0.5258515  0.5259889  0.52620244 0.52623737
 0.5260732  0.5258383  0.5257111  0.5257236  0.52591866 0.52617913
 0.5263598  0.5264315  0.52649665 0.5266107  0.52683145 0.5270505
 0.5271334  0.52703595 0.52685875 0.52664703 0.5264557  0.52636325
 0.526383   0.52648735 0.52657837 0.5265693  0.5264445  0.52622837
 0.5260757  0.52601844 0.5260918  0.5262274  0.5263763  0.5265497
 0.5267493  0.526961   0.5271866  0.52737486 0.52747303 0.52735585
 0.5270141  0.52653295 0.5259127  0.5253915  0.52508676 0.5250185
 0.5250903  0.52509755 0.524932   0.52463275 0.52422845 0.523905
 0.52372277 0.52370274 0.52371687 0.5235872  0.52324855 0.5227869
 0.5223311  0.5220403  0.5219677  0.5220195  0.5220328  0.5219187
 0.5216254  0.52123547 0.52088475 0.520772   0.5208842  0.52099264
 0.52090013 0.5205819  0.5201273  0.51978064 0.51965725 0.5197399
 0.51992035 0.5200037  0.51983744 0.51950353 0.51922715 0.519075
 0.51911277 0.5192919  0.5194353  0.5194962  0.519515   0.51958245
 0.51971763 0.51982236 0.51986694 0.5198449  0.5198222  0.51982087
 0.5198534  0.5199387  0.52001417 0.5200281  0.5199753  0.5199618
 0.51999897 0.52007663 0.5201842  0.5202212  0.5202375  0.52026266
 0.52034783 0.5205204  0.52077985 0.52106017 0.5213019  0.52149606
 0.5216003  0.5216119  0.52163595 0.52170753 0.5217709  0.5217972
 0.52176553 0.52166384 0.52152437 0.52139616 0.521317   0.5213306
 0.5214274  0.5215659  0.52165353 0.5216983  0.5217226  0.5217747
 0.52193296 0.52222955 0.5225811  0.5228466  0.52293164 0.52277154
 0.52239376 0.5218984  0.5213346  0.5207578  0.52018696 0.51959705
 0.51891124 0.5181225  0.5172536  0.51643884 0.51575816 0.51523453
 0.5148071  0.51436275 0.5138039  0.5131484  0.51245934 0.51183593
 0.51130575 0.5108709  0.51045    0.50993973 0.5092581  0.5085335
 0.5079321  0.50752604 0.5073657  0.5073537  0.5073797  0.50739
 0.5074137  0.5073963  0.5074204  0.5075362  0.5076718  0.50777215
 0.50778985 0.50772774 0.5076156  0.5074937  0.5074519  0.5075296
 0.5076878  0.5078826  0.50800014 0.50812495 0.50823    0.5083605
 0.50846124 0.50844234 0.5083517  0.5082703  0.5082425  0.5082434
 0.508248   0.50823283 0.50817716 0.5081149  0.50804406 0.50803465
 0.50804013 0.5080941  0.5081409  0.50816095 0.5081725  0.5082037
 0.508269   0.5083718  0.50847524 0.5085553  0.50861156 0.50873166
 0.50883126 0.50891125 0.5089602  0.50900203 0.5090289  0.50900674
 0.5089602  0.50887513 0.5087746  0.50867283 0.5085893  0.5085548
 0.5085318  0.50855714 0.50852865 0.50848085 0.5083987  0.5083201
 0.5083472  0.50842804 0.5084913  0.50844777 0.508195   0.50773394
 0.50715744 0.5066115  0.5061388  0.50569385 0.50522834 0.50472003
 0.5041805  0.5036156  0.50312996 0.5027221  0.50236046 0.50202364
 0.5016502  0.50125057 0.500879   0.5005654  0.5003249  0.50013864
 0.5000511  0.49994907 0.49976462 0.4994759  0.4991652  0.4988856
 0.49872112 0.49875003 0.49888456 0.49901375 0.4990203  0.49889514
 0.49867946 0.49847174 0.4983889  0.49845165 0.49856254 0.49856558
 0.4983761  0.49805573 0.4977062  0.49748236 0.49742404 0.49748188
 0.49760213 0.49766168 0.4976059  0.49750832 0.49744543 0.49752036
 0.49765894 0.49768972 0.49766994 0.49762043 0.49759203 0.4976002
 0.49760616 0.49762052 0.49761248 0.4975938  0.4975802  0.49760914
 0.4976153  0.49760744 0.49755126 0.49742702 0.49729773 0.4972014
 0.4971599  0.4971977  0.49726427 0.49730334 0.49730894 0.49729565
 0.49722084 0.4971749  0.49714714 0.49713355 0.49710512 0.49703366
 0.49688083 0.4967292  0.49663603 0.49659187 0.4966118  0.49665657
 0.49668035 0.49668986 0.4967154  0.49683148 0.49704936 0.49736625
 0.49771968 0.49804163 0.49824437 0.49832857 0.49831522 0.49824312
 0.49810976 0.4978912  0.49757677 0.49713427 0.49655795 0.49602175
 0.4955855  0.49528322 0.49508762 0.494961   0.49477747 0.4945006
 0.49413022 0.49372184 0.49332926 0.49297884 0.49263248 0.4923516
 0.4920806  0.491832   0.4915222  0.49118933 0.49086055 0.4906253
 0.49050036 0.4904972  0.49056852 0.49067247 0.49076366 0.49077463
 0.4908371  0.49088356 0.49098948 0.49106348 0.49110526 0.4911252
 0.49105325 0.4909678  0.49087664 0.49083927 0.49083984 0.49091572
 0.49099758 0.4910724  0.49109697 0.49116996 0.49126017 0.49143225
 0.49156696 0.4915702  0.4914599  0.49130544 0.49114612 0.49102965
 0.49095812 0.4909147  0.49088508 0.49085474 0.4908285  0.49082074
 0.49080145 0.49079168 0.49076048 0.4907151  0.49067333 0.49063757
 0.49066234 0.49072117 0.4907803  0.49078888 0.49074546 0.49074244
 0.49071288 0.49071634 0.49073404 0.49076316 0.49071196 0.4906201
 0.49051547 0.49047935 0.49049285 0.49055392 0.49059612 0.4905536
 0.4904299  0.4902523  0.4901186  0.49008918 0.4901626  0.4902785
 0.49034247 0.49035153 0.490285   0.49012965 0.4898957  0.48961067
 0.4892542  0.48881462 0.48828238 0.4876539  0.48697996 0.48637697
 0.48591763 0.48549354 0.48505947 0.4845641  0.4839297  0.48318943
 0.48244128 0.4818192  0.4813342  0.48090637 0.4804733  0.4800343
 0.47957128 0.4791573  0.47881937 0.47847193 0.47824705 0.4781087
 0.47807184 0.47807547 0.47804737 0.47803733 0.47805154 0.47815868
 0.47836626 0.47858202 0.47874945 0.478917   0.47903347 0.4790999
 0.47911662 0.4791817  0.47932127 0.47945365 0.47954506 0.47963035
 0.4797178  0.4798316  0.47999102 0.48016438 0.4804038  0.48071375
 0.48098135 0.4811069  0.48105884 0.48095465 0.48086748 0.48081306
 0.48078826 0.48076147 0.48072723 0.48069847 0.4806759  0.48065192
 0.4806293  0.4805846  0.48052087 0.48043597 0.48034853 0.48031944
 0.48033842 0.48039907 0.48048007 0.48052052 0.48052886 0.48059535
 0.4806376  0.48066685 0.4806898  0.48071533 0.48071498 0.48071074
 0.4806939  0.48068392 0.48066893 0.4806406  0.48058692 0.48049343
 0.48037532 0.4803533  0.48037547 0.48046303 0.4805835  0.48069847
 0.4807838  0.48082766 0.48086646 0.48084384 0.48070392 0.4803969
 0.47994158 0.47941235 0.47882742 0.4781985  0.4775718  0.47707653
 0.47668612 0.47637066 0.47602144 0.47557926 0.4750552  0.47445658
 0.47388712 0.47337085 0.47290778 0.47245452 0.47201687 0.47156492
 0.47110504 0.47065762 0.47030762 0.47007346 0.469918   0.46978435
 0.4696668  0.46951637 0.46939966 0.4693763  0.4694846  0.46963802
 0.46985176 0.4700179  0.47011566 0.47017387 0.47023445 0.47038105
 0.47059977 0.47077575 0.4709197  0.47099364 0.4710345  0.47107273
 0.47118223 0.47138795 0.4715592  0.4716864  0.47186798 0.4720929
 0.47223884 0.47234535 0.47239813 0.472401   0.47244865 0.47252864
 0.47252092 0.47248137 0.47244266 0.4723713  0.47233853 0.47226614
 0.4720752  0.47181037 0.4715573  0.471283   0.4711088  0.47100258
 0.47093385 0.47085536 0.47076252 0.4706858  0.4706368  0.4706646
 0.47068146 0.4705593  0.47035912 0.47016138 0.47002876 0.47000143
 0.47007233 0.47016335 0.47019535 0.47013673 0.47000414 0.46985632
 0.46978828 0.4699128  0.47015563 0.4703451  0.47048774 0.47057322
 0.470684   0.47089258 0.47119635 0.47150216 0.47159404 0.47137043
 0.47090763 0.47043982 0.4700754  0.46977848 0.46950927 0.46925536
 0.46900183 0.46865472 0.46828613 0.46786907 0.46732756 0.46693698
 0.46663073 0.46651998 0.4664205  0.4662488  0.46585208 0.46523586
 0.4646505  0.46429256 0.46428087 0.464364   0.46438658 0.46421772
 0.46383178 0.46355644 0.46357122 0.46388006 0.46438292 0.46467033
 0.46464717 0.46438026 0.46424308 0.46461242 0.46530274 0.46608216
 0.4665336  0.4664627  0.46611643 0.46601915 0.4666744  0.46781942
 0.46892697 0.46950603 0.46964142 0.47000635 0.46910128 0.4617112 ]
