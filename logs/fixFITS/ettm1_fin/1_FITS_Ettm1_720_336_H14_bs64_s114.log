Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19457536.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4177856
	speed: 0.1462s/iter; left time: 1893.9444s
	iters: 200, epoch: 1 | loss: 0.3781857
	speed: 0.1472s/iter; left time: 1892.1042s
Epoch: 1 cost time: 37.273760080337524
Epoch: 1, Steps: 261 | Train Loss: 0.4296281 Vali Loss: 0.7299978 Test Loss: 0.3839052
Validation loss decreased (inf --> 0.729998).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3647776
	speed: 0.5956s/iter; left time: 7557.7220s
	iters: 200, epoch: 2 | loss: 0.3705009
	speed: 0.1369s/iter; left time: 1723.8735s
Epoch: 2 cost time: 36.46484398841858
Epoch: 2, Steps: 261 | Train Loss: 0.3487410 Vali Loss: 0.6881733 Test Loss: 0.3671146
Validation loss decreased (0.729998 --> 0.688173).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3325802
	speed: 0.5655s/iter; left time: 7028.2334s
	iters: 200, epoch: 3 | loss: 0.3648226
	speed: 0.1143s/iter; left time: 1408.8854s
Epoch: 3 cost time: 31.267138242721558
Epoch: 3, Steps: 261 | Train Loss: 0.3410006 Vali Loss: 0.6728121 Test Loss: 0.3667621
Validation loss decreased (0.688173 --> 0.672812).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3213712
	speed: 0.5840s/iter; left time: 7106.1924s
	iters: 200, epoch: 4 | loss: 0.3451365
	speed: 0.1300s/iter; left time: 1569.2736s
Epoch: 4 cost time: 35.4328088760376
Epoch: 4, Steps: 261 | Train Loss: 0.3391057 Vali Loss: 0.6666823 Test Loss: 0.3668222
Validation loss decreased (0.672812 --> 0.666682).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3425799
	speed: 0.5956s/iter; left time: 7092.1141s
	iters: 200, epoch: 5 | loss: 0.3284016
	speed: 0.1319s/iter; left time: 1557.8670s
Epoch: 5 cost time: 34.677212953567505
Epoch: 5, Steps: 261 | Train Loss: 0.3380579 Vali Loss: 0.6642931 Test Loss: 0.3667174
Validation loss decreased (0.666682 --> 0.664293).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3214412
	speed: 0.5720s/iter; left time: 6661.2345s
	iters: 200, epoch: 6 | loss: 0.3633335
	speed: 0.1133s/iter; left time: 1308.4484s
Epoch: 6 cost time: 31.12689471244812
Epoch: 6, Steps: 261 | Train Loss: 0.3376251 Vali Loss: 0.6604896 Test Loss: 0.3666087
Validation loss decreased (0.664293 --> 0.660490).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3374561
	speed: 0.4657s/iter; left time: 5301.6020s
	iters: 200, epoch: 7 | loss: 0.3177642
	speed: 0.1179s/iter; left time: 1330.0843s
Epoch: 7 cost time: 31.26231598854065
Epoch: 7, Steps: 261 | Train Loss: 0.3371798 Vali Loss: 0.6590197 Test Loss: 0.3671667
Validation loss decreased (0.660490 --> 0.659020).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3149061
	speed: 0.6141s/iter; left time: 6830.7465s
	iters: 200, epoch: 8 | loss: 0.3334760
	speed: 0.1454s/iter; left time: 1602.8413s
Epoch: 8 cost time: 38.18866038322449
Epoch: 8, Steps: 261 | Train Loss: 0.3369690 Vali Loss: 0.6596074 Test Loss: 0.3667681
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3283785
	speed: 0.5291s/iter; left time: 5747.8668s
	iters: 200, epoch: 9 | loss: 0.3506590
	speed: 0.1190s/iter; left time: 1281.0778s
Epoch: 9 cost time: 30.026654481887817
Epoch: 9, Steps: 261 | Train Loss: 0.3367572 Vali Loss: 0.6569544 Test Loss: 0.3668582
Validation loss decreased (0.659020 --> 0.656954).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3312232
	speed: 0.4974s/iter; left time: 5273.4058s
	iters: 200, epoch: 10 | loss: 0.3321910
	speed: 0.1276s/iter; left time: 1340.4104s
Epoch: 10 cost time: 31.87804102897644
Epoch: 10, Steps: 261 | Train Loss: 0.3366466 Vali Loss: 0.6555669 Test Loss: 0.3668841
Validation loss decreased (0.656954 --> 0.655567).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3193859
	speed: 0.5932s/iter; left time: 6133.8253s
	iters: 200, epoch: 11 | loss: 0.3065782
	speed: 0.1335s/iter; left time: 1367.0416s
Epoch: 11 cost time: 34.851513385772705
Epoch: 11, Steps: 261 | Train Loss: 0.3364318 Vali Loss: 0.6559220 Test Loss: 0.3668276
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3534079
	speed: 0.5025s/iter; left time: 5064.8927s
	iters: 200, epoch: 12 | loss: 0.3237216
	speed: 0.1293s/iter; left time: 1289.9928s
Epoch: 12 cost time: 32.68934202194214
Epoch: 12, Steps: 261 | Train Loss: 0.3363427 Vali Loss: 0.6572066 Test Loss: 0.3668458
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3169149
	speed: 0.6127s/iter; left time: 6015.9517s
	iters: 200, epoch: 13 | loss: 0.3024368
	speed: 0.1292s/iter; left time: 1255.3844s
Epoch: 13 cost time: 35.6220600605011
Epoch: 13, Steps: 261 | Train Loss: 0.3362979 Vali Loss: 0.6546744 Test Loss: 0.3668610
Validation loss decreased (0.655567 --> 0.654674).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3238037
	speed: 0.5524s/iter; left time: 5280.2748s
	iters: 200, epoch: 14 | loss: 0.3220273
	speed: 0.1097s/iter; left time: 1037.2165s
Epoch: 14 cost time: 28.663745880126953
Epoch: 14, Steps: 261 | Train Loss: 0.3363768 Vali Loss: 0.6558145 Test Loss: 0.3669167
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3108436
	speed: 0.5687s/iter; left time: 5287.4114s
	iters: 200, epoch: 15 | loss: 0.3172451
	speed: 0.1332s/iter; left time: 1224.9582s
Epoch: 15 cost time: 36.22671437263489
Epoch: 15, Steps: 261 | Train Loss: 0.3362106 Vali Loss: 0.6549364 Test Loss: 0.3666724
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3386755
	speed: 0.6006s/iter; left time: 5427.0642s
	iters: 200, epoch: 16 | loss: 0.3616447
	speed: 0.1327s/iter; left time: 1185.6601s
Epoch: 16 cost time: 34.67802143096924
Epoch: 16, Steps: 261 | Train Loss: 0.3362884 Vali Loss: 0.6547126 Test Loss: 0.3665690
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3663618564605713, mae:0.38488996028900146, rse:0.5759747624397278, corr:[0.5351313  0.5454162  0.549554   0.55022544 0.55113286 0.5532437
 0.55545837 0.5567327  0.55717    0.55730355 0.55754995 0.55786484
 0.5582049  0.55850726 0.55864006 0.5583523  0.55745333 0.55606866
 0.55453885 0.55324894 0.5522532  0.5512376  0.5499792  0.5486979
 0.5474731  0.5465733  0.54588336 0.5450802  0.5442946  0.54382306
 0.54399055 0.54487973 0.54594433 0.5466488  0.54658926 0.546269
 0.5460386  0.54614305 0.5463435  0.54616755 0.5456092  0.5448526
 0.544299   0.5441991  0.5442873  0.5443097  0.544309   0.54434013
 0.54435724 0.5442042  0.5438694  0.5433853  0.5430215  0.5429475
 0.54323065 0.54359585 0.5436744  0.54330575 0.54280907 0.5424783
 0.5425135  0.5427914  0.54300576 0.54289395 0.54268426 0.5426446
 0.54282326 0.54307485 0.5432502  0.543281   0.5432713  0.5432804
 0.54327077 0.54315984 0.54296637 0.5427328  0.5425289  0.54242235
 0.5423413  0.5421759  0.54189396 0.541579   0.5414005  0.54137415
 0.5414413  0.5413803  0.5410765  0.54057276 0.54015034 0.54013574
 0.5405567  0.54119503 0.5417469  0.54201096 0.54195946 0.5416855
 0.54132664 0.54099816 0.54058605 0.54015356 0.5397906  0.53956205
 0.53944707 0.53926826 0.5390063  0.538726   0.5384261  0.5382379
 0.5381031  0.53797716 0.53779    0.53751594 0.5372519  0.53711665
 0.53710264 0.5371278  0.5370855  0.53693485 0.5367896  0.5367985
 0.5369165  0.5369964  0.5368856  0.536684   0.53651124 0.53636146
 0.5361989  0.5360209  0.5358216  0.5357432  0.5357958  0.5358753
 0.5358427  0.5355465  0.5349851  0.53447336 0.53438514 0.53463835
 0.53504205 0.53530335 0.5352137  0.5349572  0.5348404  0.5350288
 0.53543067 0.53579926 0.5360866  0.5362894  0.5364275  0.536393
 0.5361417  0.53578925 0.5354511  0.53525555 0.53518593 0.5351739
 0.53506684 0.53481    0.5346128  0.5345945  0.5348835  0.53530353
 0.53563863 0.53576314 0.5357727  0.5358419  0.53606224 0.53639394
 0.5366568  0.53676844 0.53686804 0.5370188  0.53708375 0.5369542
 0.536614   0.5362     0.53591955 0.5359246  0.5361428  0.53640336
 0.5364902  0.53637195 0.5361606  0.536141   0.5364267  0.5368932
 0.5372996  0.5375223  0.5375656  0.53750783 0.5374633  0.5374556
 0.53739244 0.537221   0.5368242  0.536223   0.5355321  0.5348539
 0.53412575 0.5332914  0.53235394 0.53145313 0.5307391  0.5302864
 0.53000665 0.5296679  0.52910805 0.5284103  0.5277432  0.52723765
 0.52685505 0.5264957  0.52599907 0.52536273 0.5246672  0.5241821
 0.52398556 0.52384084 0.5235649  0.5230797  0.5226146  0.5225076
 0.52291083 0.5235282  0.52407694 0.52432233 0.5242132  0.5239456
 0.5237329  0.5236817  0.52376145 0.5238007  0.52376825 0.52375466
 0.52382904 0.5240523  0.5242444  0.52445894 0.52460134 0.52476275
 0.5249611  0.52507746 0.52509844 0.52500004 0.5247937  0.52446747
 0.5241458  0.52399224 0.5239823  0.52402896 0.52391267 0.52367437
 0.5233502  0.52320504 0.5232088  0.5232398  0.52321225 0.52312547
 0.52308893 0.523243   0.52356035 0.52389497 0.5240714  0.52409357
 0.52394223 0.5237981  0.5238515  0.5241548  0.5244938  0.52455235
 0.5242924  0.5238421  0.5234561  0.52337766 0.5236143  0.5239398
 0.52403575 0.523936   0.5237446  0.5237174  0.52387065 0.52409977
 0.52427614 0.5242972  0.52417195 0.524053   0.5239966  0.5239089
 0.5236917  0.5233297  0.5227712  0.5219726  0.52113885 0.5204679
 0.52006567 0.51977986 0.51951456 0.519075   0.51843655 0.51771885
 0.51703805 0.5165266  0.5162292  0.51601326 0.51576054 0.5154381
 0.51527655 0.5151974  0.5150671  0.5147224  0.51424134 0.51377714
 0.5135989  0.5138309  0.51416296 0.51426214 0.51397467 0.51353455
 0.5132344  0.5131109  0.513054   0.5128885  0.5125976  0.5123563
 0.5123054  0.51250577 0.5127012  0.51272005 0.5125806  0.51252836
 0.5128056  0.513238   0.5136574  0.5142208  0.5143936  0.51097894]
