Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_96_FITS_ETTm1_ftM_sl720_ll48_pl96_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 11425
test 11425
Model(
  (freq_upsampler): Linear(in_features=90, out_features=102, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4112640.0
params:  9282.0
Trainable parameters:  9282
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2827376
	speed: 0.1474s/iter; left time: 3870.1919s
	iters: 200, epoch: 1 | loss: 0.2820864
	speed: 0.1457s/iter; left time: 3809.9973s
	iters: 300, epoch: 1 | loss: 0.2703421
	speed: 0.1303s/iter; left time: 3393.2099s
	iters: 400, epoch: 1 | loss: 0.2528675
	speed: 0.1152s/iter; left time: 2988.4168s
	iters: 500, epoch: 1 | loss: 0.2564890
	speed: 0.1145s/iter; left time: 2960.7564s
Epoch: 1 cost time: 68.49540543556213
Epoch: 1, Steps: 527 | Train Loss: 0.2975488 Vali Loss: 0.4176119 Test Loss: 0.3124685
Validation loss decreased (inf --> 0.417612).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2142975
	speed: 0.8197s/iter; left time: 21087.0795s
	iters: 200, epoch: 2 | loss: 0.2651551
	speed: 0.1330s/iter; left time: 3407.6232s
	iters: 300, epoch: 2 | loss: 0.2578144
	speed: 0.1268s/iter; left time: 3235.2033s
	iters: 400, epoch: 2 | loss: 0.3107757
	speed: 0.1244s/iter; left time: 3164.0147s
	iters: 500, epoch: 2 | loss: 0.3050898
	speed: 0.1213s/iter; left time: 3072.3452s
Epoch: 2 cost time: 70.38987231254578
Epoch: 2, Steps: 527 | Train Loss: 0.2675336 Vali Loss: 0.4063474 Test Loss: 0.3099006
Validation loss decreased (0.417612 --> 0.406347).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2639416
	speed: 0.9268s/iter; left time: 23353.5690s
	iters: 200, epoch: 3 | loss: 0.2968854
	speed: 0.1266s/iter; left time: 3176.1731s
	iters: 300, epoch: 3 | loss: 0.2773706
	speed: 0.1220s/iter; left time: 3048.7427s
	iters: 400, epoch: 3 | loss: 0.2699677
	speed: 0.1124s/iter; left time: 2798.7632s
	iters: 500, epoch: 3 | loss: 0.2392995
	speed: 0.1204s/iter; left time: 2984.7152s
Epoch: 3 cost time: 64.30033659934998
Epoch: 3, Steps: 527 | Train Loss: 0.2651975 Vali Loss: 0.4022681 Test Loss: 0.3102493
Validation loss decreased (0.406347 --> 0.402268).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2726276
	speed: 0.9454s/iter; left time: 23322.5177s
	iters: 200, epoch: 4 | loss: 0.2644617
	speed: 0.1445s/iter; left time: 3550.5970s
	iters: 300, epoch: 4 | loss: 0.2687855
	speed: 0.1505s/iter; left time: 3682.7678s
	iters: 400, epoch: 4 | loss: 0.2454772
	speed: 0.1481s/iter; left time: 3608.1932s
	iters: 500, epoch: 4 | loss: 0.2984565
	speed: 0.1518s/iter; left time: 3683.8586s
Epoch: 4 cost time: 77.49655961990356
Epoch: 4, Steps: 527 | Train Loss: 0.2643465 Vali Loss: 0.4015670 Test Loss: 0.3107782
Validation loss decreased (0.402268 --> 0.401567).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2775192
	speed: 0.9849s/iter; left time: 23778.2802s
	iters: 200, epoch: 5 | loss: 0.2781741
	speed: 0.1176s/iter; left time: 2828.2214s
	iters: 300, epoch: 5 | loss: 0.2468110
	speed: 0.1169s/iter; left time: 2798.0224s
	iters: 400, epoch: 5 | loss: 0.2554569
	speed: 0.1255s/iter; left time: 2991.7344s
	iters: 500, epoch: 5 | loss: 0.2607598
	speed: 0.1177s/iter; left time: 2793.3702s
Epoch: 5 cost time: 63.67268490791321
Epoch: 5, Steps: 527 | Train Loss: 0.2639330 Vali Loss: 0.3991673 Test Loss: 0.3100084
Validation loss decreased (0.401567 --> 0.399167).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2431727
	speed: 0.8194s/iter; left time: 19351.3808s
	iters: 200, epoch: 6 | loss: 0.2336058
	speed: 0.1144s/iter; left time: 2689.5165s
	iters: 300, epoch: 6 | loss: 0.2612712
	speed: 0.1177s/iter; left time: 2756.3905s
	iters: 400, epoch: 6 | loss: 0.3075317
	speed: 0.1128s/iter; left time: 2629.4113s
	iters: 500, epoch: 6 | loss: 0.2397686
	speed: 0.1117s/iter; left time: 2593.6585s
Epoch: 6 cost time: 61.741695165634155
Epoch: 6, Steps: 527 | Train Loss: 0.2636577 Vali Loss: 0.3947794 Test Loss: 0.3107388
Validation loss decreased (0.399167 --> 0.394779).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3256870
	speed: 0.9093s/iter; left time: 20993.6856s
	iters: 200, epoch: 7 | loss: 0.2587486
	speed: 0.1452s/iter; left time: 3338.3670s
	iters: 300, epoch: 7 | loss: 0.2592276
	speed: 0.1275s/iter; left time: 2918.3605s
	iters: 400, epoch: 7 | loss: 0.2782538
	speed: 0.1093s/iter; left time: 2491.9474s
	iters: 500, epoch: 7 | loss: 0.2425629
	speed: 0.1116s/iter; left time: 2531.4579s
Epoch: 7 cost time: 67.96896123886108
Epoch: 7, Steps: 527 | Train Loss: 0.2634885 Vali Loss: 0.3983451 Test Loss: 0.3079900
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2687255
	speed: 0.7990s/iter; left time: 18027.2631s
	iters: 200, epoch: 8 | loss: 0.3044029
	speed: 0.1212s/iter; left time: 2722.9619s
	iters: 300, epoch: 8 | loss: 0.2522634
	speed: 0.1180s/iter; left time: 2639.7505s
	iters: 400, epoch: 8 | loss: 0.2521007
	speed: 0.1202s/iter; left time: 2675.9500s
	iters: 500, epoch: 8 | loss: 0.2466581
	speed: 0.1185s/iter; left time: 2625.3771s
Epoch: 8 cost time: 64.12281370162964
Epoch: 8, Steps: 527 | Train Loss: 0.2633748 Vali Loss: 0.3956179 Test Loss: 0.3095835
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2686282
	speed: 0.8084s/iter; left time: 17812.7268s
	iters: 200, epoch: 9 | loss: 0.2340243
	speed: 0.1550s/iter; left time: 3399.3553s
	iters: 300, epoch: 9 | loss: 0.2735024
	speed: 0.1488s/iter; left time: 3248.2043s
	iters: 400, epoch: 9 | loss: 0.2730137
	speed: 0.1534s/iter; left time: 3334.6450s
	iters: 500, epoch: 9 | loss: 0.2696929
	speed: 0.1590s/iter; left time: 3439.2281s
Epoch: 9 cost time: 79.98967385292053
Epoch: 9, Steps: 527 | Train Loss: 0.2631795 Vali Loss: 0.3968149 Test Loss: 0.3102892
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_96_FITS_ETTm1_ftM_sl720_ll48_pl96_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
mse:0.31123554706573486, mae:0.35367050766944885, rse:0.5308557152748108, corr:[0.54449236 0.5545611  0.56060505 0.562637   0.56294554 0.56339926
 0.56445825 0.56585675 0.5671557  0.5680096  0.56849545 0.5687768
 0.56910133 0.56938934 0.56934625 0.5687294  0.5675859  0.56613576
 0.5645623  0.5630347  0.5616718  0.56038857 0.5590598  0.557757
 0.556361   0.55509835 0.5541326  0.553465   0.5532235  0.5533614
 0.55374795 0.55428517 0.55475247 0.55513763 0.55529565 0.5553437
 0.55513465 0.5547352  0.5542447  0.5536974  0.5533173  0.5531293
 0.5531071  0.5532408  0.5532869  0.5531208  0.552886   0.5527168
 0.55263937 0.5525824  0.55256164 0.55250734 0.55243313 0.5522425
 0.5520376  0.55194336 0.552019   0.552125   0.55220544 0.55204713
 0.55168134 0.5512499  0.5509502  0.5508479  0.55107    0.55153954
 0.55200386 0.5522749  0.55234545 0.5522931  0.5523211  0.55246836
 0.5526508  0.5527274  0.5526239  0.5522203  0.551517   0.55068177
 0.54992837 0.54941195 0.5491583  0.5489913  0.5487184  0.54810506
 0.5472405  0.54634184 0.54575706 0.54561    0.54585725 0.54628986
 0.5464981  0.54632264 0.54604065 0.54605687 0.546557   0.5469108 ]
