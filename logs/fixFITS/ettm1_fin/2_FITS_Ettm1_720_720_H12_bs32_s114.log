Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10067456.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4918314
	speed: 0.1220s/iter; left time: 3140.8293s
	iters: 200, epoch: 1 | loss: 0.3808481
	speed: 0.1345s/iter; left time: 3449.8481s
	iters: 300, epoch: 1 | loss: 0.3239873
	speed: 0.1442s/iter; left time: 3684.8596s
	iters: 400, epoch: 1 | loss: 0.3315812
	speed: 0.1415s/iter; left time: 3602.2991s
	iters: 500, epoch: 1 | loss: 0.2862954
	speed: 0.1487s/iter; left time: 3770.4959s
Epoch: 1 cost time: 71.63631772994995
Epoch: 1, Steps: 517 | Train Loss: 0.4108813 Vali Loss: 1.0676011 Test Loss: 0.5051010
Validation loss decreased (inf --> 1.067601).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2962350
	speed: 1.0010s/iter; left time: 25258.2349s
	iters: 200, epoch: 2 | loss: 0.2700492
	speed: 0.1447s/iter; left time: 3637.3252s
	iters: 300, epoch: 2 | loss: 0.2415953
	speed: 0.1274s/iter; left time: 3188.5065s
	iters: 400, epoch: 2 | loss: 0.2286682
	speed: 0.1273s/iter; left time: 3174.6013s
	iters: 500, epoch: 2 | loss: 0.2385748
	speed: 0.1321s/iter; left time: 3279.4507s
Epoch: 2 cost time: 71.50300908088684
Epoch: 2, Steps: 517 | Train Loss: 0.2574332 Vali Loss: 0.9855406 Test Loss: 0.4481082
Validation loss decreased (1.067601 --> 0.985541).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2210842
	speed: 0.7931s/iter; left time: 19601.8908s
	iters: 200, epoch: 3 | loss: 0.2333300
	speed: 0.1095s/iter; left time: 2694.6345s
	iters: 300, epoch: 3 | loss: 0.2173985
	speed: 0.1127s/iter; left time: 2762.4864s
	iters: 400, epoch: 3 | loss: 0.2101453
	speed: 0.1154s/iter; left time: 2818.0031s
	iters: 500, epoch: 3 | loss: 0.2274463
	speed: 0.1266s/iter; left time: 3078.2601s
Epoch: 3 cost time: 60.50852584838867
Epoch: 3, Steps: 517 | Train Loss: 0.2274543 Vali Loss: 0.9572376 Test Loss: 0.4273782
Validation loss decreased (0.985541 --> 0.957238).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2072426
	speed: 0.6859s/iter; left time: 16598.6262s
	iters: 200, epoch: 4 | loss: 0.1982961
	speed: 0.1062s/iter; left time: 2559.0371s
	iters: 300, epoch: 4 | loss: 0.2244023
	speed: 0.1064s/iter; left time: 2552.4139s
	iters: 400, epoch: 4 | loss: 0.2132660
	speed: 0.0967s/iter; left time: 2311.0437s
	iters: 500, epoch: 4 | loss: 0.2373696
	speed: 0.1263s/iter; left time: 3005.6020s
Epoch: 4 cost time: 58.850664377212524
Epoch: 4, Steps: 517 | Train Loss: 0.2166244 Vali Loss: 0.9454929 Test Loss: 0.4188328
Validation loss decreased (0.957238 --> 0.945493).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1991608
	speed: 0.8466s/iter; left time: 20049.1770s
	iters: 200, epoch: 5 | loss: 0.2013275
	speed: 0.1198s/iter; left time: 2825.9898s
	iters: 300, epoch: 5 | loss: 0.2107046
	speed: 0.1199s/iter; left time: 2815.0299s
	iters: 400, epoch: 5 | loss: 0.2078661
	speed: 0.1158s/iter; left time: 2707.3534s
	iters: 500, epoch: 5 | loss: 0.2020004
	speed: 0.1096s/iter; left time: 2550.8344s
Epoch: 5 cost time: 60.99729657173157
Epoch: 5, Steps: 517 | Train Loss: 0.2121066 Vali Loss: 0.9409272 Test Loss: 0.4157425
Validation loss decreased (0.945493 --> 0.940927).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1957350
	speed: 0.6820s/iter; left time: 15800.2038s
	iters: 200, epoch: 6 | loss: 0.1981119
	speed: 0.1053s/iter; left time: 2428.5432s
	iters: 300, epoch: 6 | loss: 0.2286732
	speed: 0.1234s/iter; left time: 2833.5467s
	iters: 400, epoch: 6 | loss: 0.2291210
	speed: 0.1154s/iter; left time: 2638.4063s
	iters: 500, epoch: 6 | loss: 0.2284942
	speed: 0.1170s/iter; left time: 2664.2679s
Epoch: 6 cost time: 59.87125539779663
Epoch: 6, Steps: 517 | Train Loss: 0.2101285 Vali Loss: 0.9395174 Test Loss: 0.4154596
Validation loss decreased (0.940927 --> 0.939517).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2178384
	speed: 0.8453s/iter; left time: 19146.1967s
	iters: 200, epoch: 7 | loss: 0.2052207
	speed: 0.1259s/iter; left time: 2840.0325s
	iters: 300, epoch: 7 | loss: 0.2085235
	speed: 0.1267s/iter; left time: 2844.2897s
	iters: 400, epoch: 7 | loss: 0.2219568
	speed: 0.1210s/iter; left time: 2704.5641s
	iters: 500, epoch: 7 | loss: 0.2069577
	speed: 0.1235s/iter; left time: 2746.8605s
Epoch: 7 cost time: 65.12243700027466
Epoch: 7, Steps: 517 | Train Loss: 0.2093077 Vali Loss: 0.9396435 Test Loss: 0.4162618
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2231405
	speed: 0.8074s/iter; left time: 17868.5487s
	iters: 200, epoch: 8 | loss: 0.2000372
	speed: 0.1143s/iter; left time: 2518.6471s
	iters: 300, epoch: 8 | loss: 0.2362081
	speed: 0.1023s/iter; left time: 2244.2819s
	iters: 400, epoch: 8 | loss: 0.2175307
	speed: 0.1348s/iter; left time: 2943.8465s
	iters: 500, epoch: 8 | loss: 0.1697884
	speed: 0.1364s/iter; left time: 2964.7865s
Epoch: 8 cost time: 62.29364633560181
Epoch: 8, Steps: 517 | Train Loss: 0.2090168 Vali Loss: 0.9388882 Test Loss: 0.4163105
Validation loss decreased (0.939517 --> 0.938888).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2160480
	speed: 0.7521s/iter; left time: 16257.0307s
	iters: 200, epoch: 9 | loss: 0.2014266
	speed: 0.0995s/iter; left time: 2139.8654s
	iters: 300, epoch: 9 | loss: 0.2047866
	speed: 0.1349s/iter; left time: 2889.7655s
	iters: 400, epoch: 9 | loss: 0.2159600
	speed: 0.1174s/iter; left time: 2502.7477s
	iters: 500, epoch: 9 | loss: 0.2214461
	speed: 0.1234s/iter; left time: 2617.0498s
Epoch: 9 cost time: 61.10431456565857
Epoch: 9, Steps: 517 | Train Loss: 0.2089363 Vali Loss: 0.9394854 Test Loss: 0.4170673
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2154757
	speed: 0.6498s/iter; left time: 13709.5434s
	iters: 200, epoch: 10 | loss: 0.2308922
	speed: 0.0727s/iter; left time: 1527.3769s
	iters: 300, epoch: 10 | loss: 0.2246787
	speed: 0.1339s/iter; left time: 2798.7037s
	iters: 400, epoch: 10 | loss: 0.1971848
	speed: 0.1315s/iter; left time: 2734.4741s
	iters: 500, epoch: 10 | loss: 0.2144312
	speed: 0.1429s/iter; left time: 2957.4825s
Epoch: 10 cost time: 58.38473677635193
Epoch: 10, Steps: 517 | Train Loss: 0.2088491 Vali Loss: 0.9397829 Test Loss: 0.4171717
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1903940
	speed: 0.7498s/iter; left time: 15431.7192s
	iters: 200, epoch: 11 | loss: 0.1849332
	speed: 0.1247s/iter; left time: 2553.1981s
	iters: 300, epoch: 11 | loss: 0.2201916
	speed: 0.1231s/iter; left time: 2509.8042s
	iters: 400, epoch: 11 | loss: 0.1963285
	speed: 0.1173s/iter; left time: 2378.3333s
	iters: 500, epoch: 11 | loss: 0.2244504
	speed: 0.1212s/iter; left time: 2446.8856s
Epoch: 11 cost time: 63.806713342666626
Epoch: 11, Steps: 517 | Train Loss: 0.2088714 Vali Loss: 0.9398944 Test Loss: 0.4175069
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10067456.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4036450
	speed: 0.1410s/iter; left time: 3631.9558s
	iters: 200, epoch: 1 | loss: 0.3563778
	speed: 0.1322s/iter; left time: 3390.3875s
	iters: 300, epoch: 1 | loss: 0.4326374
	speed: 0.1342s/iter; left time: 3429.1446s
	iters: 400, epoch: 1 | loss: 0.3870826
	speed: 0.1237s/iter; left time: 3147.9685s
	iters: 500, epoch: 1 | loss: 0.3804095
	speed: 0.1166s/iter; left time: 2955.6726s
Epoch: 1 cost time: 67.16870641708374
Epoch: 1, Steps: 517 | Train Loss: 0.3984808 Vali Loss: 0.9341214 Test Loss: 0.4160329
Validation loss decreased (inf --> 0.934121).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3663417
	speed: 0.7814s/iter; left time: 19718.2766s
	iters: 200, epoch: 2 | loss: 0.3707263
	speed: 0.1028s/iter; left time: 2584.4323s
	iters: 300, epoch: 2 | loss: 0.4261721
	speed: 0.1026s/iter; left time: 2567.9211s
	iters: 400, epoch: 2 | loss: 0.3861769
	speed: 0.0975s/iter; left time: 2431.8102s
	iters: 500, epoch: 2 | loss: 0.4134001
	speed: 0.0959s/iter; left time: 2381.7811s
Epoch: 2 cost time: 53.22489666938782
Epoch: 2, Steps: 517 | Train Loss: 0.3977093 Vali Loss: 0.9340600 Test Loss: 0.4166684
Validation loss decreased (0.934121 --> 0.934060).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3585503
	speed: 0.5937s/iter; left time: 14675.5979s
	iters: 200, epoch: 3 | loss: 0.4037279
	speed: 0.0966s/iter; left time: 2377.3210s
	iters: 300, epoch: 3 | loss: 0.3990303
	speed: 0.1072s/iter; left time: 2629.1803s
	iters: 400, epoch: 3 | loss: 0.3811763
	speed: 0.1046s/iter; left time: 2554.2822s
	iters: 500, epoch: 3 | loss: 0.3982725
	speed: 0.0986s/iter; left time: 2396.4884s
Epoch: 3 cost time: 52.55740690231323
Epoch: 3, Steps: 517 | Train Loss: 0.3975153 Vali Loss: 0.9314131 Test Loss: 0.4165899
Validation loss decreased (0.934060 --> 0.931413).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3762525
	speed: 0.6155s/iter; left time: 14895.5312s
	iters: 200, epoch: 4 | loss: 0.3499803
	speed: 0.1048s/iter; left time: 2525.6650s
	iters: 300, epoch: 4 | loss: 0.4068544
	speed: 0.0986s/iter; left time: 2367.4547s
	iters: 400, epoch: 4 | loss: 0.4138362
	speed: 0.1141s/iter; left time: 2727.5700s
	iters: 500, epoch: 4 | loss: 0.4006793
	speed: 0.1011s/iter; left time: 2405.3675s
Epoch: 4 cost time: 53.93809700012207
Epoch: 4, Steps: 517 | Train Loss: 0.3973141 Vali Loss: 0.9322248 Test Loss: 0.4161123
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4020271
	speed: 0.6722s/iter; left time: 15920.7241s
	iters: 200, epoch: 5 | loss: 0.3800503
	speed: 0.1065s/iter; left time: 2511.3405s
	iters: 300, epoch: 5 | loss: 0.3773859
	speed: 0.1071s/iter; left time: 2514.0338s
	iters: 400, epoch: 5 | loss: 0.4032987
	speed: 0.1085s/iter; left time: 2536.0616s
	iters: 500, epoch: 5 | loss: 0.4234812
	speed: 0.1086s/iter; left time: 2528.6162s
Epoch: 5 cost time: 56.93420648574829
Epoch: 5, Steps: 517 | Train Loss: 0.3971869 Vali Loss: 0.9312758 Test Loss: 0.4162256
Validation loss decreased (0.931413 --> 0.931276).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4040206
	speed: 0.6925s/iter; left time: 16042.0103s
	iters: 200, epoch: 6 | loss: 0.4418051
	speed: 0.1131s/iter; left time: 2608.4041s
	iters: 300, epoch: 6 | loss: 0.4160860
	speed: 0.1072s/iter; left time: 2461.4460s
	iters: 400, epoch: 6 | loss: 0.3915924
	speed: 0.1096s/iter; left time: 2506.9932s
	iters: 500, epoch: 6 | loss: 0.4173225
	speed: 0.1182s/iter; left time: 2690.5682s
Epoch: 6 cost time: 58.24397325515747
Epoch: 6, Steps: 517 | Train Loss: 0.3971297 Vali Loss: 0.9306330 Test Loss: 0.4156038
Validation loss decreased (0.931276 --> 0.930633).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4028025
	speed: 0.7595s/iter; left time: 17201.6289s
	iters: 200, epoch: 7 | loss: 0.4136704
	speed: 0.1146s/iter; left time: 2583.5257s
	iters: 300, epoch: 7 | loss: 0.3890546
	speed: 0.1269s/iter; left time: 2848.2847s
	iters: 400, epoch: 7 | loss: 0.3642014
	speed: 0.1102s/iter; left time: 2462.1573s
	iters: 500, epoch: 7 | loss: 0.3586993
	speed: 0.1537s/iter; left time: 3419.4659s
Epoch: 7 cost time: 66.02944874763489
Epoch: 7, Steps: 517 | Train Loss: 0.3971034 Vali Loss: 0.9308272 Test Loss: 0.4158145
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4025402
	speed: 0.8909s/iter; left time: 19717.9859s
	iters: 200, epoch: 8 | loss: 0.3646627
	speed: 0.1121s/iter; left time: 2469.8223s
	iters: 300, epoch: 8 | loss: 0.4209603
	speed: 0.1334s/iter; left time: 2925.2930s
	iters: 400, epoch: 8 | loss: 0.3918358
	speed: 0.1274s/iter; left time: 2780.9716s
	iters: 500, epoch: 8 | loss: 0.4371823
	speed: 0.2377s/iter; left time: 5166.7118s
Epoch: 8 cost time: 77.63002705574036
Epoch: 8, Steps: 517 | Train Loss: 0.3970365 Vali Loss: 0.9301706 Test Loss: 0.4158735
Validation loss decreased (0.930633 --> 0.930171).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4155675
	speed: 1.0949s/iter; left time: 23666.1344s
	iters: 200, epoch: 9 | loss: 0.3871538
	speed: 0.1764s/iter; left time: 3796.3179s
	iters: 300, epoch: 9 | loss: 0.4099604
	speed: 0.1888s/iter; left time: 4042.5647s
	iters: 400, epoch: 9 | loss: 0.4265229
	speed: 0.1569s/iter; left time: 3343.7214s
	iters: 500, epoch: 9 | loss: 0.3565345
	speed: 0.1486s/iter; left time: 3153.2716s
Epoch: 9 cost time: 86.00337529182434
Epoch: 9, Steps: 517 | Train Loss: 0.3969684 Vali Loss: 0.9306476 Test Loss: 0.4159847
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3724456
	speed: 0.6470s/iter; left time: 13650.9641s
	iters: 200, epoch: 10 | loss: 0.4241847
	speed: 0.1036s/iter; left time: 2175.0865s
	iters: 300, epoch: 10 | loss: 0.3654497
	speed: 0.1039s/iter; left time: 2171.8051s
	iters: 400, epoch: 10 | loss: 0.4415624
	speed: 0.0949s/iter; left time: 1973.5160s
	iters: 500, epoch: 10 | loss: 0.3886867
	speed: 0.1112s/iter; left time: 2301.9947s
Epoch: 10 cost time: 55.198569536209106
Epoch: 10, Steps: 517 | Train Loss: 0.3969191 Vali Loss: 0.9306071 Test Loss: 0.4158454
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3986585
	speed: 1.2603s/iter; left time: 25938.6388s
	iters: 200, epoch: 11 | loss: 0.3754153
	speed: 0.2471s/iter; left time: 5060.2141s
	iters: 300, epoch: 11 | loss: 0.4278190
	speed: 0.2941s/iter; left time: 5993.0922s
	iters: 400, epoch: 11 | loss: 0.3606957
	speed: 0.1651s/iter; left time: 3348.2480s
	iters: 500, epoch: 11 | loss: 0.4179173
	speed: 0.1662s/iter; left time: 3354.9565s
Epoch: 11 cost time: 116.18347191810608
Epoch: 11, Steps: 517 | Train Loss: 0.3968392 Vali Loss: 0.9297950 Test Loss: 0.4160636
Validation loss decreased (0.930171 --> 0.929795).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3613809
	speed: 1.0007s/iter; left time: 20078.5737s
	iters: 200, epoch: 12 | loss: 0.4055129
	speed: 0.0498s/iter; left time: 993.8480s
	iters: 300, epoch: 12 | loss: 0.4140285
	speed: 0.0729s/iter; left time: 1447.5444s
	iters: 400, epoch: 12 | loss: 0.3924345
	speed: 0.0625s/iter; left time: 1235.9298s
	iters: 500, epoch: 12 | loss: 0.3966429
	speed: 0.0802s/iter; left time: 1576.7565s
Epoch: 12 cost time: 41.019203901290894
Epoch: 12, Steps: 517 | Train Loss: 0.3968376 Vali Loss: 0.9303426 Test Loss: 0.4165004
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3725072
	speed: 0.7494s/iter; left time: 14649.0364s
	iters: 200, epoch: 13 | loss: 0.3808748
	speed: 0.1103s/iter; left time: 2144.8647s
	iters: 300, epoch: 13 | loss: 0.4162386
	speed: 0.1048s/iter; left time: 2027.6935s
	iters: 400, epoch: 13 | loss: 0.3559777
	speed: 0.1139s/iter; left time: 2192.0622s
	iters: 500, epoch: 13 | loss: 0.3837333
	speed: 0.1082s/iter; left time: 2072.5331s
Epoch: 13 cost time: 58.15041708946228
Epoch: 13, Steps: 517 | Train Loss: 0.3968579 Vali Loss: 0.9311011 Test Loss: 0.4155996
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3658136
	speed: 0.7826s/iter; left time: 14892.2797s
	iters: 200, epoch: 14 | loss: 0.3470815
	speed: 0.1136s/iter; left time: 2150.5322s
	iters: 300, epoch: 14 | loss: 0.3689670
	speed: 0.1287s/iter; left time: 2424.0164s
	iters: 400, epoch: 14 | loss: 0.4451998
	speed: 0.1462s/iter; left time: 2738.8202s
	iters: 500, epoch: 14 | loss: 0.4385756
	speed: 0.1641s/iter; left time: 3057.3252s
Epoch: 14 cost time: 71.48040533065796
Epoch: 14, Steps: 517 | Train Loss: 0.3967209 Vali Loss: 0.9307504 Test Loss: 0.4158250
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4158695638179779, mae:0.41195109486579895, rse:0.6135491132736206, corr:[0.523947   0.5320136  0.53560394 0.5365333  0.537243   0.53874606
 0.5404047  0.54136103 0.5416065  0.5417402  0.5423719  0.5432912
 0.5439578  0.5438327  0.5428518  0.54142976 0.54009205 0.5390283
 0.53799003 0.5367707  0.5353348  0.5336885  0.53196657 0.5304928
 0.5291276  0.5278768  0.5266857  0.5256124  0.52506727 0.52529436
 0.5262538  0.5276409  0.5288103  0.5295165  0.5296464  0.52963823
 0.5295294  0.52942413 0.52926743 0.52889645 0.5284756  0.52808553
 0.52786326 0.5278671  0.5279099  0.52791077 0.52802765 0.5283425
 0.528763   0.5290607  0.52911115 0.52875555 0.52809924 0.5272627
 0.5266253  0.526412   0.5265459  0.52667195 0.52661204 0.5262496
 0.5258156  0.5255787  0.52563006 0.52575433 0.5258272  0.5257293
 0.52542025 0.52504903 0.5248621  0.5248854  0.52505296 0.5251439
 0.52500975 0.5246592  0.5242917  0.52400017 0.52380425 0.52372336
 0.5237383  0.5238276  0.52393556 0.52400184 0.52400047 0.5238326
 0.52356833 0.523235   0.52297443 0.5228616  0.522989   0.52344537
 0.5241276  0.5248222  0.5253289  0.5255036  0.5253616  0.52495277
 0.5244721  0.52413285 0.52389425 0.52386194 0.5239901  0.52418315
 0.524363   0.52440864 0.5243397  0.52425665 0.5241076  0.5239843
 0.52384686 0.52371675 0.5235334  0.5232204  0.52278554 0.5223004
 0.52180696 0.52137077 0.5210339  0.5207762  0.5206089  0.5206104
 0.52073586 0.5208808  0.5209025  0.5208445  0.52070063 0.5204075
 0.5200146  0.5197019  0.5195834  0.51975757 0.5200899  0.52036935
 0.52044463 0.5202202  0.51969767 0.5191116  0.5187613  0.5186731
 0.5188704  0.5192477  0.5195355  0.5196462  0.5196187  0.5195505
 0.5194954  0.51939726 0.51932555 0.51932764 0.5194789  0.519689
 0.51985496 0.5199514  0.51989555 0.51966304 0.5193152  0.51906025
 0.5189471  0.5189269  0.51898766 0.5189898  0.5189978  0.5190258
 0.5191169  0.51927143 0.5194922  0.51974416 0.5200067  0.52027214
 0.5204782  0.52056974 0.5206277  0.5206789  0.5206601  0.52060205
 0.5205499  0.5205001  0.5203993  0.52017665 0.5197803  0.51932305
 0.518941   0.5188091  0.51894015 0.5193378  0.5198399  0.5202394
 0.5204079  0.52042377 0.5204262  0.52050936 0.5206887  0.5209122
 0.5210653  0.52109075 0.5208485  0.5203142  0.51960695 0.51891327
 0.5183093  0.51780576 0.5172949  0.5167195  0.5159843  0.51510036
 0.5141434  0.51321995 0.5124201  0.5118396  0.5114661  0.5112227
 0.51095814 0.5106213  0.5101509  0.509524   0.50870514 0.50786155
 0.5071521  0.5065935  0.506236   0.5060313  0.5059682  0.5060974
 0.5064617  0.5068637  0.5072097  0.5074036  0.5073395  0.5070678
 0.50672364 0.50645536 0.5063383  0.50631076 0.5063423  0.50642866
 0.50655234 0.5067957  0.5071032  0.50760746 0.50815916 0.5086678
 0.5089514  0.5088497  0.5084833  0.5080777  0.5078377  0.50776774
 0.5078031  0.5078549  0.5078048  0.5076589  0.50739133 0.507154
 0.50692344 0.5068249  0.50680846 0.50684017 0.50691533 0.5070493
 0.5072504  0.50753033 0.5078329  0.50808847 0.50824493 0.5083595
 0.5083448  0.5082114  0.507997   0.5078177  0.5077288  0.5077184
 0.50781816 0.50796133 0.5081005  0.50818324 0.50816804 0.5080904
 0.507947   0.50785726 0.5077661  0.50774497 0.50775164 0.5077638
 0.50785404 0.50796384 0.5080145  0.50794977 0.50766635 0.5071287
 0.50644463 0.5058151  0.50532836 0.50494444 0.5046641  0.5044323
 0.50418794 0.503786   0.50324684 0.5025298  0.5016701  0.5008013
 0.5000393  0.49955115 0.4994169  0.4995225  0.49966848 0.4996363
 0.4994457  0.49906066 0.4986021  0.49821642 0.49805486 0.49805167
 0.49811083 0.49818224 0.49813274 0.49797276 0.49777883 0.49768317
 0.49769816 0.49775568 0.49783665 0.49792907 0.49800315 0.4980036
 0.49789572 0.49773443 0.4975025  0.49728054 0.49710837 0.49702358
 0.49711668 0.497284   0.49736345 0.4973078  0.49709556 0.49688604
 0.49670035 0.49643543 0.49622896 0.49611622 0.49615508 0.49632263
 0.49654064 0.49680027 0.4969935  0.49705097 0.49694586 0.4967349
 0.49642503 0.49618572 0.4961227  0.49623722 0.4965058  0.49679908
 0.49697208 0.4970161  0.4969698  0.49693027 0.49702415 0.49723807
 0.4974142  0.49750173 0.49741802 0.4971695  0.49683788 0.49650738
 0.4961502  0.49588564 0.49574164 0.49565554 0.4956597  0.49573463
 0.49586508 0.49605528 0.49626872 0.49652362 0.49677643 0.4970516
 0.49733913 0.49760136 0.49782196 0.49790493 0.49782223 0.49764407
 0.49742624 0.49720156 0.49697566 0.49666178 0.4961847  0.49569052
 0.4952178  0.49483287 0.49454558 0.49437463 0.49419284 0.4939356
 0.49355963 0.4931002  0.492637   0.49222788 0.49184573 0.49156561
 0.49130657 0.49107483 0.49075404 0.4903848  0.4900164  0.4897767
 0.48968112 0.48972967 0.48984978 0.48999515 0.49013087 0.49019447
 0.49038017 0.49059772 0.49088266 0.49103656 0.49101806 0.4908716
 0.49055335 0.4902126  0.48990992 0.48977232 0.48977333 0.48991534
 0.4900388  0.4900864  0.48997852 0.48988974 0.48984888 0.49001285
 0.49021888 0.49026152 0.49015033 0.48997688 0.48979956 0.48967138
 0.48959595 0.4895467  0.48948646 0.48937935 0.48926002 0.48920074
 0.4892356  0.4894327  0.48974058 0.49005073 0.49025154 0.49023184
 0.49008185 0.48989582 0.489804   0.4898361  0.48997608 0.4902216
 0.4903851  0.49043125 0.490343   0.49021506 0.49003172 0.48990405
 0.48983267 0.4898837  0.4899761  0.4901058  0.49021584 0.49023822
 0.49019337 0.49004188 0.48985595 0.48968533 0.48959768 0.48962307
 0.48972282 0.4899083  0.49011505 0.4902059  0.49008825 0.48975304
 0.48924455 0.48865393 0.4880575  0.4874372  0.48676276 0.4860797
 0.4854842  0.4849166  0.4844373  0.48406225 0.48364222 0.4830607
 0.48227242 0.48137507 0.48047578 0.4796703  0.47906455 0.47873047
 0.47856387 0.4784782  0.47832927 0.47792032 0.47745487 0.4770357
 0.476853   0.4769402  0.47719318 0.4775699  0.47795966 0.47835848
 0.47875175 0.4790391  0.4791851  0.47930643 0.47938237 0.47942573
 0.4794207  0.47948354 0.47965553 0.47982657 0.4799316  0.47998878
 0.4800043  0.48002586 0.48008734 0.48017985 0.48040852 0.4807921
 0.48118305 0.48140177 0.48133913 0.481103   0.48081836 0.4805575
 0.48038387 0.480299   0.48029017 0.4802991  0.48025912 0.4801464
 0.47999927 0.479853   0.4797486  0.4796609  0.47955665 0.479488
 0.47945312 0.47948962 0.4796181  0.4797677  0.47987464 0.47993445
 0.47980553 0.47953907 0.47930256 0.4792657  0.47942802 0.47972342
 0.47997585 0.48007953 0.48003572 0.47997776 0.48004723 0.48027343
 0.48055798 0.48086405 0.4809741  0.48089895 0.48073086 0.4806403
 0.48072436 0.48093963 0.48117632 0.4812354  0.4810088  0.48051786
 0.47995672 0.47952887 0.4792312  0.47891074 0.47840026 0.47772002
 0.47690704 0.4761335  0.47553778 0.47519472 0.47506258 0.4749476
 0.47473598 0.4743179  0.4737082  0.47299623 0.47236714 0.47189358
 0.4715747  0.47133502 0.4711451  0.47094393 0.47069734 0.47041637
 0.47019377 0.47001764 0.46992573 0.46990526 0.46993336 0.46989998
 0.4698731  0.4698252  0.46980113 0.46985134 0.46997118 0.47019047
 0.47046486 0.47068167 0.4708765  0.47102568 0.471153   0.47125387
 0.471363   0.47150335 0.4715705  0.47159535 0.47171417 0.4719124
 0.47204813 0.47216094 0.4722544  0.47236356 0.4725901  0.47289452
 0.47308233 0.4731417  0.47307238 0.47288153 0.47273135 0.47265673
 0.47260913 0.47255665 0.4724484  0.47214904 0.47175804 0.47134754
 0.4710598  0.4709754  0.47108355 0.47128296 0.47143257 0.47150546
 0.4714633  0.47131816 0.47123677 0.47129783 0.47147024 0.47164768
 0.47174007 0.47169575 0.47153965 0.47134086 0.4711511  0.47098058
 0.47081733 0.470689   0.47056875 0.4704581  0.47051325 0.47081074
 0.4713364  0.47194323 0.47241646 0.47258395 0.47235283 0.47179416
 0.47110593 0.47055423 0.47021636 0.46996146 0.46959758 0.46903434
 0.46833545 0.46761116 0.46710303 0.4668676  0.46675292 0.4667721
 0.46664053 0.46637565 0.465955   0.4655645  0.46528074 0.46510813
 0.46502963 0.46492526 0.46476522 0.46445224 0.4641003  0.46381873
 0.463601   0.46350378 0.46346194 0.46346286 0.46363303 0.4639385
 0.46444628 0.46503454 0.4656035  0.4661096  0.46634966 0.46636218
 0.46622717 0.46614936 0.46630105 0.46669787 0.46728912 0.46774608
 0.4678995  0.46773657 0.46737877 0.46731213 0.46721828 0.4647575 ]
