Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_96_FITS_ETTm1_ftM_sl720_ll48_pl96_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 11425
test 11425
Model(
  (freq_upsampler): Linear(in_features=106, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  45588480.0
params:  12840.0
Trainable parameters:  12840
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.488614559173584
Epoch: 1, Steps: 65 | Train Loss: 0.4188663 Vali Loss: 0.5480357 Test Loss: 0.3526942
Validation loss decreased (inf --> 0.548036).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.71191143989563
Epoch: 2, Steps: 65 | Train Loss: 0.2982858 Vali Loss: 0.4672269 Test Loss: 0.3198997
Validation loss decreased (0.548036 --> 0.467227).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 13.284467697143555
Epoch: 3, Steps: 65 | Train Loss: 0.2804283 Vali Loss: 0.4427675 Test Loss: 0.3143993
Validation loss decreased (0.467227 --> 0.442768).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 14.33774995803833
Epoch: 4, Steps: 65 | Train Loss: 0.2740746 Vali Loss: 0.4289809 Test Loss: 0.3128171
Validation loss decreased (0.442768 --> 0.428981).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 15.043013334274292
Epoch: 5, Steps: 65 | Train Loss: 0.2710666 Vali Loss: 0.4232377 Test Loss: 0.3115064
Validation loss decreased (0.428981 --> 0.423238).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 14.454462051391602
Epoch: 6, Steps: 65 | Train Loss: 0.2692164 Vali Loss: 0.4176619 Test Loss: 0.3107769
Validation loss decreased (0.423238 --> 0.417662).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 13.213110446929932
Epoch: 7, Steps: 65 | Train Loss: 0.2678722 Vali Loss: 0.4143069 Test Loss: 0.3102754
Validation loss decreased (0.417662 --> 0.414307).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.418070316314697
Epoch: 8, Steps: 65 | Train Loss: 0.2670808 Vali Loss: 0.4111673 Test Loss: 0.3102814
Validation loss decreased (0.414307 --> 0.411167).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.879485368728638
Epoch: 9, Steps: 65 | Train Loss: 0.2662634 Vali Loss: 0.4096639 Test Loss: 0.3099998
Validation loss decreased (0.411167 --> 0.409664).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 12.752774477005005
Epoch: 10, Steps: 65 | Train Loss: 0.2655593 Vali Loss: 0.4089183 Test Loss: 0.3098263
Validation loss decreased (0.409664 --> 0.408918).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.10538387298584
Epoch: 11, Steps: 65 | Train Loss: 0.2651373 Vali Loss: 0.4056821 Test Loss: 0.3097613
Validation loss decreased (0.408918 --> 0.405682).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.063841819763184
Epoch: 12, Steps: 65 | Train Loss: 0.2648583 Vali Loss: 0.4059008 Test Loss: 0.3093573
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.866652727127075
Epoch: 13, Steps: 65 | Train Loss: 0.2642543 Vali Loss: 0.4051144 Test Loss: 0.3093774
Validation loss decreased (0.405682 --> 0.405114).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.92199969291687
Epoch: 14, Steps: 65 | Train Loss: 0.2641910 Vali Loss: 0.4038861 Test Loss: 0.3094434
Validation loss decreased (0.405114 --> 0.403886).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 13.684259414672852
Epoch: 15, Steps: 65 | Train Loss: 0.2639541 Vali Loss: 0.4036378 Test Loss: 0.3095887
Validation loss decreased (0.403886 --> 0.403638).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 13.492810487747192
Epoch: 16, Steps: 65 | Train Loss: 0.2637151 Vali Loss: 0.4020098 Test Loss: 0.3092243
Validation loss decreased (0.403638 --> 0.402010).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 13.594812631607056
Epoch: 17, Steps: 65 | Train Loss: 0.2637307 Vali Loss: 0.4025325 Test Loss: 0.3091970
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 13.808480739593506
Epoch: 18, Steps: 65 | Train Loss: 0.2634809 Vali Loss: 0.4006639 Test Loss: 0.3094079
Validation loss decreased (0.402010 --> 0.400664).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 13.210355281829834
Epoch: 19, Steps: 65 | Train Loss: 0.2632871 Vali Loss: 0.4009420 Test Loss: 0.3091521
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 13.969814538955688
Epoch: 20, Steps: 65 | Train Loss: 0.2633992 Vali Loss: 0.4004659 Test Loss: 0.3089790
Validation loss decreased (0.400664 --> 0.400466).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 13.70773434638977
Epoch: 21, Steps: 65 | Train Loss: 0.2631098 Vali Loss: 0.3995874 Test Loss: 0.3090361
Validation loss decreased (0.400466 --> 0.399587).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 13.267269611358643
Epoch: 22, Steps: 65 | Train Loss: 0.2630826 Vali Loss: 0.4006112 Test Loss: 0.3091539
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 13.089416742324829
Epoch: 23, Steps: 65 | Train Loss: 0.2628316 Vali Loss: 0.3996698 Test Loss: 0.3090029
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 12.170878887176514
Epoch: 24, Steps: 65 | Train Loss: 0.2627759 Vali Loss: 0.3991067 Test Loss: 0.3090749
Validation loss decreased (0.399587 --> 0.399107).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 13.033085346221924
Epoch: 25, Steps: 65 | Train Loss: 0.2629959 Vali Loss: 0.3988706 Test Loss: 0.3090192
Validation loss decreased (0.399107 --> 0.398871).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 13.333832263946533
Epoch: 26, Steps: 65 | Train Loss: 0.2625978 Vali Loss: 0.3994606 Test Loss: 0.3090124
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 13.112871646881104
Epoch: 27, Steps: 65 | Train Loss: 0.2625743 Vali Loss: 0.3984388 Test Loss: 0.3089643
Validation loss decreased (0.398871 --> 0.398439).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 13.077685832977295
Epoch: 28, Steps: 65 | Train Loss: 0.2626673 Vali Loss: 0.3989728 Test Loss: 0.3089659
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 12.824375629425049
Epoch: 29, Steps: 65 | Train Loss: 0.2626512 Vali Loss: 0.3983406 Test Loss: 0.3089398
Validation loss decreased (0.398439 --> 0.398341).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 13.234285831451416
Epoch: 30, Steps: 65 | Train Loss: 0.2623791 Vali Loss: 0.3984871 Test Loss: 0.3090468
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 13.770188808441162
Epoch: 31, Steps: 65 | Train Loss: 0.2623549 Vali Loss: 0.3980412 Test Loss: 0.3090557
Validation loss decreased (0.398341 --> 0.398041).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 13.006887435913086
Epoch: 32, Steps: 65 | Train Loss: 0.2623566 Vali Loss: 0.3976266 Test Loss: 0.3090911
Validation loss decreased (0.398041 --> 0.397627).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 12.36319375038147
Epoch: 33, Steps: 65 | Train Loss: 0.2623479 Vali Loss: 0.3979033 Test Loss: 0.3090084
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 12.185635089874268
Epoch: 34, Steps: 65 | Train Loss: 0.2625432 Vali Loss: 0.3975550 Test Loss: 0.3090178
Validation loss decreased (0.397627 --> 0.397555).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 12.081621170043945
Epoch: 35, Steps: 65 | Train Loss: 0.2623378 Vali Loss: 0.3984488 Test Loss: 0.3090200
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 12.298768520355225
Epoch: 36, Steps: 65 | Train Loss: 0.2624162 Vali Loss: 0.3974909 Test Loss: 0.3089435
Validation loss decreased (0.397555 --> 0.397491).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 13.505901336669922
Epoch: 37, Steps: 65 | Train Loss: 0.2623973 Vali Loss: 0.3978200 Test Loss: 0.3089050
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 13.235588788986206
Epoch: 38, Steps: 65 | Train Loss: 0.2624336 Vali Loss: 0.3974955 Test Loss: 0.3090493
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 13.399042844772339
Epoch: 39, Steps: 65 | Train Loss: 0.2621942 Vali Loss: 0.3972627 Test Loss: 0.3089743
Validation loss decreased (0.397491 --> 0.397263).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 14.134667873382568
Epoch: 40, Steps: 65 | Train Loss: 0.2623518 Vali Loss: 0.3965950 Test Loss: 0.3090083
Validation loss decreased (0.397263 --> 0.396595).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 14.386516094207764
Epoch: 41, Steps: 65 | Train Loss: 0.2625299 Vali Loss: 0.3976764 Test Loss: 0.3090613
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 14.296746730804443
Epoch: 42, Steps: 65 | Train Loss: 0.2620424 Vali Loss: 0.3974884 Test Loss: 0.3090316
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 13.772031307220459
Epoch: 43, Steps: 65 | Train Loss: 0.2624237 Vali Loss: 0.3972745 Test Loss: 0.3089627
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_96_FITS_ETTm1_ftM_sl720_ll48_pl96_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
mse:0.30946084856987, mae:0.35208144783973694, rse:0.5293400287628174, corr:[0.54406023 0.55482906 0.56015503 0.56162125 0.56254333 0.56427294
 0.56626606 0.56761795 0.568157   0.568391   0.5688877  0.56952435
 0.5699426  0.569772   0.5689959  0.5679821  0.5671123  0.56640965
 0.56554484 0.56433976 0.56288046 0.5613221  0.5598951  0.5588354
 0.5578285  0.5567351  0.55548286 0.55420864 0.5534784  0.55356705
 0.55432546 0.55534035 0.55602443 0.55623025 0.55600774 0.5559146
 0.5559472  0.55599207 0.55583096 0.55526155 0.55458856 0.554097
 0.5540105  0.55430996 0.5545397  0.55443627 0.55415946 0.553957
 0.5539462  0.554023   0.5540796  0.55393046 0.55359715 0.5531507
 0.5529016  0.5529923  0.5532666  0.55336523 0.5532415  0.5528491
 0.5524642  0.55233514 0.55249125 0.55261934 0.5526533  0.55258447
 0.5524808  0.552483   0.55267096 0.55285805 0.5529009  0.55269223
 0.5522906  0.5518729  0.5516694  0.55158937 0.55144    0.5510937
 0.55049837 0.5497568  0.5490988  0.54864925 0.5484544  0.5482593
 0.5478814  0.54721576 0.5465068  0.5459866  0.54581296 0.54599446
 0.54612464 0.5460125  0.5461422  0.54706484 0.5484387  0.5478526 ]
