Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  38915072.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4309905
	speed: 0.1621s/iter; left time: 1037.8785s
Epoch: 1 cost time: 20.916223287582397
Epoch: 1, Steps: 130 | Train Loss: 0.4860518 Vali Loss: 0.8041313 Test Loss: 0.4287428
Validation loss decreased (inf --> 0.804131).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3771114
	speed: 0.4240s/iter; left time: 2659.2011s
Epoch: 2 cost time: 20.469526052474976
Epoch: 2, Steps: 130 | Train Loss: 0.3700672 Vali Loss: 0.7260783 Test Loss: 0.3830077
Validation loss decreased (0.804131 --> 0.726078).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3332315
	speed: 0.4350s/iter; left time: 2671.3802s
Epoch: 3 cost time: 20.791990041732788
Epoch: 3, Steps: 130 | Train Loss: 0.3512512 Vali Loss: 0.6976835 Test Loss: 0.3698891
Validation loss decreased (0.726078 --> 0.697683).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3331432
	speed: 0.4106s/iter; left time: 2467.8290s
Epoch: 4 cost time: 20.296327114105225
Epoch: 4, Steps: 130 | Train Loss: 0.3441461 Vali Loss: 0.6866250 Test Loss: 0.3664555
Validation loss decreased (0.697683 --> 0.686625).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3371252
	speed: 0.4323s/iter; left time: 2542.3040s
Epoch: 5 cost time: 20.213622570037842
Epoch: 5, Steps: 130 | Train Loss: 0.3414740 Vali Loss: 0.6785029 Test Loss: 0.3656435
Validation loss decreased (0.686625 --> 0.678503).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3348897
	speed: 0.3419s/iter; left time: 1966.5066s
Epoch: 6 cost time: 16.44865345954895
Epoch: 6, Steps: 130 | Train Loss: 0.3397606 Vali Loss: 0.6725380 Test Loss: 0.3657001
Validation loss decreased (0.678503 --> 0.672538).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3121658
	speed: 0.3665s/iter; left time: 2060.1649s
Epoch: 7 cost time: 17.51752257347107
Epoch: 7, Steps: 130 | Train Loss: 0.3390589 Vali Loss: 0.6700554 Test Loss: 0.3660849
Validation loss decreased (0.672538 --> 0.670055).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3348517
	speed: 0.3893s/iter; left time: 2137.7880s
Epoch: 8 cost time: 20.34744143486023
Epoch: 8, Steps: 130 | Train Loss: 0.3384404 Vali Loss: 0.6676326 Test Loss: 0.3660024
Validation loss decreased (0.670055 --> 0.667633).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3452344
	speed: 0.4099s/iter; left time: 2197.6302s
Epoch: 9 cost time: 18.312630891799927
Epoch: 9, Steps: 130 | Train Loss: 0.3379907 Vali Loss: 0.6647646 Test Loss: 0.3663458
Validation loss decreased (0.667633 --> 0.664765).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3864351
	speed: 0.3247s/iter; left time: 1698.6895s
Epoch: 10 cost time: 16.091108560562134
Epoch: 10, Steps: 130 | Train Loss: 0.3375151 Vali Loss: 0.6618051 Test Loss: 0.3663175
Validation loss decreased (0.664765 --> 0.661805).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3489792
	speed: 0.3332s/iter; left time: 1699.4400s
Epoch: 11 cost time: 15.90789246559143
Epoch: 11, Steps: 130 | Train Loss: 0.3373099 Vali Loss: 0.6605265 Test Loss: 0.3663109
Validation loss decreased (0.661805 --> 0.660527).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3195893
	speed: 0.3354s/iter; left time: 1667.2203s
Epoch: 12 cost time: 18.267497777938843
Epoch: 12, Steps: 130 | Train Loss: 0.3368049 Vali Loss: 0.6603970 Test Loss: 0.3663807
Validation loss decreased (0.660527 --> 0.660397).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3300284
	speed: 0.4055s/iter; left time: 1962.9717s
Epoch: 13 cost time: 19.414757013320923
Epoch: 13, Steps: 130 | Train Loss: 0.3368351 Vali Loss: 0.6597534 Test Loss: 0.3661329
Validation loss decreased (0.660397 --> 0.659753).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3440485
	speed: 0.3741s/iter; left time: 1762.4479s
Epoch: 14 cost time: 18.09043526649475
Epoch: 14, Steps: 130 | Train Loss: 0.3368394 Vali Loss: 0.6589196 Test Loss: 0.3662432
Validation loss decreased (0.659753 --> 0.658920).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3226914
	speed: 0.3640s/iter; left time: 1667.5409s
Epoch: 15 cost time: 17.645917892456055
Epoch: 15, Steps: 130 | Train Loss: 0.3367248 Vali Loss: 0.6597759 Test Loss: 0.3661393
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3418278
	speed: 0.3484s/iter; left time: 1550.7980s
Epoch: 16 cost time: 17.036417722702026
Epoch: 16, Steps: 130 | Train Loss: 0.3363690 Vali Loss: 0.6581829 Test Loss: 0.3659940
Validation loss decreased (0.658920 --> 0.658183).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3339066
	speed: 0.3914s/iter; left time: 1691.3892s
Epoch: 17 cost time: 18.788256406784058
Epoch: 17, Steps: 130 | Train Loss: 0.3364458 Vali Loss: 0.6585437 Test Loss: 0.3662039
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3366486
	speed: 0.3970s/iter; left time: 1663.7512s
Epoch: 18 cost time: 19.767937183380127
Epoch: 18, Steps: 130 | Train Loss: 0.3363650 Vali Loss: 0.6577961 Test Loss: 0.3661634
Validation loss decreased (0.658183 --> 0.657796).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3256898
	speed: 0.3939s/iter; left time: 1599.6514s
Epoch: 19 cost time: 18.70433473587036
Epoch: 19, Steps: 130 | Train Loss: 0.3363884 Vali Loss: 0.6586781 Test Loss: 0.3662542
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3398368
	speed: 0.3455s/iter; left time: 1358.0657s
Epoch: 20 cost time: 16.711439609527588
Epoch: 20, Steps: 130 | Train Loss: 0.3361441 Vali Loss: 0.6577841 Test Loss: 0.3662297
Validation loss decreased (0.657796 --> 0.657784).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3510688
	speed: 0.3675s/iter; left time: 1396.7810s
Epoch: 21 cost time: 18.853015184402466
Epoch: 21, Steps: 130 | Train Loss: 0.3363887 Vali Loss: 0.6567304 Test Loss: 0.3661183
Validation loss decreased (0.657784 --> 0.656730).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3438264
	speed: 0.3897s/iter; left time: 1430.6906s
Epoch: 22 cost time: 19.01936936378479
Epoch: 22, Steps: 130 | Train Loss: 0.3363679 Vali Loss: 0.6559066 Test Loss: 0.3662973
Validation loss decreased (0.656730 --> 0.655907).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3570424
	speed: 0.4010s/iter; left time: 1420.0280s
Epoch: 23 cost time: 19.53887915611267
Epoch: 23, Steps: 130 | Train Loss: 0.3363884 Vali Loss: 0.6573935 Test Loss: 0.3663892
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3383979
	speed: 0.4183s/iter; left time: 1426.6725s
Epoch: 24 cost time: 19.813004732131958
Epoch: 24, Steps: 130 | Train Loss: 0.3361434 Vali Loss: 0.6572303 Test Loss: 0.3661824
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3516019
	speed: 0.4105s/iter; left time: 1346.8509s
Epoch: 25 cost time: 20.526050806045532
Epoch: 25, Steps: 130 | Train Loss: 0.3362803 Vali Loss: 0.6559456 Test Loss: 0.3661468
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3660920560359955, mae:0.38472554087638855, rse:0.5757626295089722, corr:[0.5365906  0.54653835 0.55069065 0.5514002  0.55230415 0.5543112
 0.55632263 0.55730593 0.5574182  0.5573554  0.5576643  0.5582685
 0.55888987 0.5591861  0.5589263  0.55802697 0.55663884 0.55512863
 0.55382    0.55288166 0.5520999  0.55100507 0.54944456 0.5478483
 0.5465203  0.54582024 0.54551756 0.5450482  0.5443713  0.5437991
 0.54384    0.54474586 0.54600745 0.5469803  0.54711276 0.5468044
 0.54645085 0.54643035 0.54660547 0.5465216  0.54610866 0.5454409
 0.54487485 0.54469216 0.54466283 0.5445517  0.544446   0.54447865
 0.5446569  0.5447775  0.54468656 0.5442509  0.5436604  0.54317874
 0.5430984  0.5433331  0.5435519  0.5434711  0.5432006  0.54289913
 0.5427965  0.54289263 0.5430019  0.5429366  0.5429019  0.5430725
 0.543408   0.54371583 0.543829   0.54371774 0.5435797  0.54358095
 0.5437037  0.5437933  0.54372495 0.5434378  0.54302883 0.5427259
 0.5426453  0.54273564 0.54281193 0.5426987  0.5423901  0.54198426
 0.5416915  0.54155946 0.54155856 0.54155666 0.5415317  0.5415994
 0.54180425 0.5421206  0.5424472  0.54266655 0.54266816 0.54240775
 0.5419622  0.54150486 0.54104096 0.54067785 0.54044586 0.54029244
 0.54013824 0.5398635  0.539565   0.5393818  0.53928477 0.5392953
 0.53924817 0.53907686 0.5387675  0.5383636  0.53798836 0.537738
 0.5375795  0.53742754 0.53720087 0.5368873  0.53661674 0.5365421
 0.53661984 0.5366993  0.53663456 0.53652215 0.5364666  0.536425
 0.53632534 0.53615    0.5359055  0.5357519  0.5357171  0.5357355
 0.5357306  0.53559047 0.53530216 0.5350855  0.53516    0.535342
 0.53547734 0.5354376  0.5351942  0.53502905 0.5351766  0.5355717
 0.53593075 0.5359696  0.5357543  0.5354986  0.53543943 0.53552693
 0.53558266 0.53552145 0.5353223  0.53511596 0.5350259  0.5351129
 0.5352215  0.53517747 0.53503853 0.53486204 0.5348663  0.5350366
 0.5352697  0.5354304  0.5355076  0.535596   0.5358038  0.53614396
 0.53645444 0.53660876 0.53668725 0.536739   0.5367256  0.5366701
 0.5365877  0.5364711  0.53630686 0.5361233  0.5359411  0.5358395
 0.53582937 0.53589344 0.53595257 0.5360488  0.5362131  0.5364586
 0.53678834 0.53720725 0.5376333  0.53792775 0.5380359  0.5379681
 0.537763   0.5375253  0.53721154 0.53678936 0.53625774 0.53564924
 0.53498036 0.5343114  0.5336844  0.5331313  0.53260654 0.5320615
 0.5314736  0.5308231  0.5301389  0.5295246  0.5290087  0.5285551
 0.528068   0.52750945 0.52682763 0.52605957 0.52524894 0.52462465
 0.52430683 0.5241741  0.5241191  0.524      0.5238463  0.5237893
 0.5239411  0.5241582  0.5243919  0.5245672  0.5245934  0.5245011
 0.5243669  0.524267   0.5242274  0.52417976 0.5241555  0.5242289
 0.52440447 0.5246936  0.52489656 0.52508324 0.52517986 0.52527237
 0.52537256 0.5254034  0.52540106 0.52539057 0.525396   0.5253596
 0.5252891  0.5252482  0.5251917  0.5251214  0.5249453  0.5247454
 0.5245018  0.5243978  0.5243925  0.5243934  0.5243748  0.5243673
 0.5243985  0.5244802  0.52451897 0.5244331  0.52423924 0.5241451
 0.5241853  0.5243594  0.52457696 0.52473795 0.5247274  0.52450156
 0.5242307  0.52402896 0.52397525 0.52406627 0.52417463 0.5242087
 0.5241048  0.5240171  0.5239688  0.5240906  0.5243007  0.5245134
 0.52472293 0.52489316 0.5249579  0.52492344 0.5247387  0.5243334
 0.52378976 0.52328694 0.5228502  0.52234066 0.52178377 0.5212162
 0.52072304 0.5202533  0.51984483 0.519361   0.5187552  0.51809883
 0.5174741  0.5170088  0.5167243  0.5164752  0.5161242  0.5156187
 0.5151992  0.5148513  0.5145306  0.5141381  0.51375926 0.5134916
 0.5134769  0.51370984 0.5138211  0.51356465 0.5130197  0.5126157
 0.5126565  0.5129727  0.51319224 0.51299614 0.51243997 0.5119241
 0.511795   0.51214105 0.5125422  0.5126773  0.5125231  0.51236814
 0.5125006  0.5128082  0.5132011  0.513999   0.51463425 0.51117927]
