Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  42577920.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 14.453786849975586
Epoch: 1, Steps: 65 | Train Loss: 0.5676017 Vali Loss: 1.2603085 Test Loss: 0.7611250
Validation loss decreased (inf --> 1.260309).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 15.56656789779663
Epoch: 2, Steps: 65 | Train Loss: 0.4365550 Vali Loss: 1.1008146 Test Loss: 0.6551529
Validation loss decreased (1.260309 --> 1.100815).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 14.786582231521606
Epoch: 3, Steps: 65 | Train Loss: 0.3670936 Vali Loss: 1.0085245 Test Loss: 0.5964995
Validation loss decreased (1.100815 --> 1.008525).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 14.437807083129883
Epoch: 4, Steps: 65 | Train Loss: 0.3232554 Vali Loss: 0.9518755 Test Loss: 0.5627984
Validation loss decreased (1.008525 --> 0.951876).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 15.073557615280151
Epoch: 5, Steps: 65 | Train Loss: 0.2923658 Vali Loss: 0.9097162 Test Loss: 0.5362294
Validation loss decreased (0.951876 --> 0.909716).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 14.699593305587769
Epoch: 6, Steps: 65 | Train Loss: 0.2692452 Vali Loss: 0.8746554 Test Loss: 0.5150123
Validation loss decreased (0.909716 --> 0.874655).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 15.045982122421265
Epoch: 7, Steps: 65 | Train Loss: 0.2512128 Vali Loss: 0.8499798 Test Loss: 0.4988722
Validation loss decreased (0.874655 --> 0.849980).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 15.359545946121216
Epoch: 8, Steps: 65 | Train Loss: 0.2366388 Vali Loss: 0.8316002 Test Loss: 0.4869197
Validation loss decreased (0.849980 --> 0.831600).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 15.37005066871643
Epoch: 9, Steps: 65 | Train Loss: 0.2246868 Vali Loss: 0.8111156 Test Loss: 0.4751810
Validation loss decreased (0.831600 --> 0.811116).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 15.566209316253662
Epoch: 10, Steps: 65 | Train Loss: 0.2147929 Vali Loss: 0.8012480 Test Loss: 0.4662440
Validation loss decreased (0.811116 --> 0.801248).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 15.576133966445923
Epoch: 11, Steps: 65 | Train Loss: 0.2066626 Vali Loss: 0.7890987 Test Loss: 0.4588879
Validation loss decreased (0.801248 --> 0.789099).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 15.630198955535889
Epoch: 12, Steps: 65 | Train Loss: 0.1995302 Vali Loss: 0.7811259 Test Loss: 0.4525948
Validation loss decreased (0.789099 --> 0.781126).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 13.618136644363403
Epoch: 13, Steps: 65 | Train Loss: 0.1935001 Vali Loss: 0.7712234 Test Loss: 0.4465516
Validation loss decreased (0.781126 --> 0.771223).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 16.008675575256348
Epoch: 14, Steps: 65 | Train Loss: 0.1882140 Vali Loss: 0.7655469 Test Loss: 0.4418947
Validation loss decreased (0.771223 --> 0.765547).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 14.994708776473999
Epoch: 15, Steps: 65 | Train Loss: 0.1836366 Vali Loss: 0.7604750 Test Loss: 0.4374650
Validation loss decreased (0.765547 --> 0.760475).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 14.888838291168213
Epoch: 16, Steps: 65 | Train Loss: 0.1796875 Vali Loss: 0.7529937 Test Loss: 0.4335549
Validation loss decreased (0.760475 --> 0.752994).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 15.957664728164673
Epoch: 17, Steps: 65 | Train Loss: 0.1761996 Vali Loss: 0.7481503 Test Loss: 0.4301719
Validation loss decreased (0.752994 --> 0.748150).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 16.77367401123047
Epoch: 18, Steps: 65 | Train Loss: 0.1730120 Vali Loss: 0.7436745 Test Loss: 0.4271330
Validation loss decreased (0.748150 --> 0.743674).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 16.41934037208557
Epoch: 19, Steps: 65 | Train Loss: 0.1701976 Vali Loss: 0.7419630 Test Loss: 0.4241819
Validation loss decreased (0.743674 --> 0.741963).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 16.794065475463867
Epoch: 20, Steps: 65 | Train Loss: 0.1676923 Vali Loss: 0.7386619 Test Loss: 0.4216799
Validation loss decreased (0.741963 --> 0.738662).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 14.867360353469849
Epoch: 21, Steps: 65 | Train Loss: 0.1653678 Vali Loss: 0.7325431 Test Loss: 0.4191798
Validation loss decreased (0.738662 --> 0.732543).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 14.792394876480103
Epoch: 22, Steps: 65 | Train Loss: 0.1633169 Vali Loss: 0.7322882 Test Loss: 0.4171112
Validation loss decreased (0.732543 --> 0.732288).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 14.456003665924072
Epoch: 23, Steps: 65 | Train Loss: 0.1614714 Vali Loss: 0.7272131 Test Loss: 0.4149738
Validation loss decreased (0.732288 --> 0.727213).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 15.437645196914673
Epoch: 24, Steps: 65 | Train Loss: 0.1597194 Vali Loss: 0.7267384 Test Loss: 0.4132193
Validation loss decreased (0.727213 --> 0.726738).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 15.32905912399292
Epoch: 25, Steps: 65 | Train Loss: 0.1582423 Vali Loss: 0.7227275 Test Loss: 0.4115753
Validation loss decreased (0.726738 --> 0.722728).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 15.333607196807861
Epoch: 26, Steps: 65 | Train Loss: 0.1567502 Vali Loss: 0.7218973 Test Loss: 0.4099247
Validation loss decreased (0.722728 --> 0.721897).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 15.91203498840332
Epoch: 27, Steps: 65 | Train Loss: 0.1555415 Vali Loss: 0.7180814 Test Loss: 0.4085686
Validation loss decreased (0.721897 --> 0.718081).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 14.900346279144287
Epoch: 28, Steps: 65 | Train Loss: 0.1543462 Vali Loss: 0.7180195 Test Loss: 0.4072086
Validation loss decreased (0.718081 --> 0.718019).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 15.182758569717407
Epoch: 29, Steps: 65 | Train Loss: 0.1531542 Vali Loss: 0.7163607 Test Loss: 0.4060787
Validation loss decreased (0.718019 --> 0.716361).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 16.80172896385193
Epoch: 30, Steps: 65 | Train Loss: 0.1521375 Vali Loss: 0.7142104 Test Loss: 0.4048429
Validation loss decreased (0.716361 --> 0.714210).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 15.664646863937378
Epoch: 31, Steps: 65 | Train Loss: 0.1511611 Vali Loss: 0.7133926 Test Loss: 0.4039230
Validation loss decreased (0.714210 --> 0.713393).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 16.50960683822632
Epoch: 32, Steps: 65 | Train Loss: 0.1503765 Vali Loss: 0.7119522 Test Loss: 0.4028146
Validation loss decreased (0.713393 --> 0.711952).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 15.234901428222656
Epoch: 33, Steps: 65 | Train Loss: 0.1495172 Vali Loss: 0.7114356 Test Loss: 0.4018269
Validation loss decreased (0.711952 --> 0.711436).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 15.229057550430298
Epoch: 34, Steps: 65 | Train Loss: 0.1487181 Vali Loss: 0.7113765 Test Loss: 0.4010106
Validation loss decreased (0.711436 --> 0.711377).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 15.139634370803833
Epoch: 35, Steps: 65 | Train Loss: 0.1480211 Vali Loss: 0.7074749 Test Loss: 0.4002477
Validation loss decreased (0.711377 --> 0.707475).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 14.22439193725586
Epoch: 36, Steps: 65 | Train Loss: 0.1473312 Vali Loss: 0.7076424 Test Loss: 0.3995591
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 14.807312488555908
Epoch: 37, Steps: 65 | Train Loss: 0.1466381 Vali Loss: 0.7086845 Test Loss: 0.3987535
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 14.180931329727173
Epoch: 38, Steps: 65 | Train Loss: 0.1460799 Vali Loss: 0.7076313 Test Loss: 0.3979748
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  42577920.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 14.38196063041687
Epoch: 1, Steps: 65 | Train Loss: 0.3541740 Vali Loss: 0.6738229 Test Loss: 0.3747499
Validation loss decreased (inf --> 0.673823).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 13.649819612503052
Epoch: 2, Steps: 65 | Train Loss: 0.3429422 Vali Loss: 0.6668494 Test Loss: 0.3692037
Validation loss decreased (0.673823 --> 0.666849).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 14.864449977874756
Epoch: 3, Steps: 65 | Train Loss: 0.3392128 Vali Loss: 0.6613280 Test Loss: 0.3677719
Validation loss decreased (0.666849 --> 0.661328).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 15.851033926010132
Epoch: 4, Steps: 65 | Train Loss: 0.3382166 Vali Loss: 0.6597320 Test Loss: 0.3675405
Validation loss decreased (0.661328 --> 0.659732).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 14.546736001968384
Epoch: 5, Steps: 65 | Train Loss: 0.3379571 Vali Loss: 0.6558846 Test Loss: 0.3675095
Validation loss decreased (0.659732 --> 0.655885).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 13.644368171691895
Epoch: 6, Steps: 65 | Train Loss: 0.3374715 Vali Loss: 0.6559104 Test Loss: 0.3674322
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 14.74345326423645
Epoch: 7, Steps: 65 | Train Loss: 0.3374429 Vali Loss: 0.6565238 Test Loss: 0.3676244
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 13.91553807258606
Epoch: 8, Steps: 65 | Train Loss: 0.3371708 Vali Loss: 0.6560506 Test Loss: 0.3676701
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3669893145561218, mae:0.3859567642211914, rse:0.576467752456665, corr:[0.54069513 0.5499383  0.55691075 0.5599888  0.56015325 0.5595532
 0.5594298  0.560061   0.5611395  0.56207204 0.56243044 0.5621191
 0.5614569  0.56072754 0.5599824  0.5591379  0.55810297 0.5568482
 0.5553906  0.5538973  0.5525805  0.5514475  0.55037445 0.5492972
 0.54802746 0.54670465 0.5455769  0.5448945  0.5449342  0.54564685
 0.5466931  0.5476773  0.548208   0.5482735  0.5479793  0.54772455
 0.54762244 0.5477169  0.54785484 0.5477427  0.54734075 0.54669726
 0.546068   0.5457437  0.5457251  0.54585093 0.5459737  0.5458999
 0.5455296  0.54494685 0.5444771  0.54432803 0.54455143 0.54489106
 0.5451523  0.54519814 0.54501575 0.544687   0.54442227 0.5442945
 0.5443157  0.5444205  0.54448205 0.54437894 0.5442056  0.5440781
 0.5440716  0.54421246 0.54444885 0.54464513 0.5447235  0.5446412
 0.5443957  0.54407156 0.54378426 0.54356253 0.5433694  0.5431842
 0.54297817 0.54275584 0.5425208  0.54227275 0.5420416  0.54183537
 0.5417167  0.5416772  0.5417269  0.54181725 0.5419311  0.5420843
 0.542253   0.5423919  0.542457   0.5424246  0.54227847 0.54202133
 0.54170704 0.5414337  0.54113156 0.5408536  0.5406396  0.5405058
 0.54046196 0.54042786 0.54041463 0.5404183  0.54032415 0.5401394
 0.539816   0.5394003  0.5389554  0.5385338  0.53819084 0.5379854
 0.5379285  0.538007   0.53815675 0.53827244 0.5382849  0.53819245
 0.53797096 0.5376415  0.5372348  0.53690445 0.5367037  0.5365783
 0.53646046 0.53634334 0.5361951  0.5360749  0.5359843  0.5359577
 0.536042   0.5361727  0.53619164 0.5360453  0.53578514 0.5354107
 0.5351193  0.5350647  0.53519934 0.5354553  0.53571033 0.5358549
 0.5358335  0.53563976 0.5354291  0.53532916 0.535439   0.5356837
 0.5359471  0.5361771  0.5362749  0.53621304 0.5360203  0.5358576
 0.535776   0.5357682  0.5358605  0.53595114 0.53607094 0.5361865
 0.5363012  0.53639    0.53645474 0.53650117 0.53656685 0.53670514
 0.5368829  0.5370322  0.5371722  0.53727543 0.5372703  0.5371818
 0.53708524 0.53705645 0.53711885 0.53724384 0.53732777 0.537326
 0.5372009  0.5369997  0.5367578  0.5366309  0.53669804 0.5369442
 0.53731304 0.5377275  0.53808725 0.5382807  0.53825516 0.5380409
 0.53767675 0.5373207  0.5369561  0.5365556  0.5361194  0.53567076
 0.53516114 0.53458387 0.53392833 0.53323555 0.5325268  0.53182584
 0.53117585 0.5305801  0.5300307  0.52955395 0.52912396 0.52869177
 0.5281981  0.52765477 0.5270549  0.52641064 0.52567375 0.5249403
 0.52430445 0.52377594 0.52342945 0.523237   0.52319306 0.5232995
 0.5235541  0.5237902  0.5239717  0.52409726 0.5241406  0.52415186
 0.52417874 0.52424675 0.5243394  0.5243814  0.52439594 0.5244446
 0.52456516 0.52480656 0.5250067  0.52521944 0.5253424  0.5254004
 0.5253966  0.5252875  0.5251677  0.52513903 0.5252572  0.52541596
 0.5255233  0.5255278  0.52537173 0.5251247  0.5248045  0.5245729
 0.5244262  0.5244701  0.52461535 0.5247679  0.524876   0.5249002
 0.5248398  0.52473867 0.5246049  0.5244858  0.5244414  0.5245892
 0.5248321  0.5250722  0.52521515 0.525226   0.525087   0.524811
 0.5245538  0.5244045  0.52440286 0.52451813 0.5246305  0.5246573
 0.52451855 0.52432114 0.52410007 0.5240374  0.5241436  0.52437437
 0.524675   0.5249296  0.5250197  0.52493286 0.52466404 0.52419007
 0.52358854 0.52302206 0.52251303 0.5219385  0.5213012  0.52063036
 0.52001786 0.5194534  0.51902634 0.5186484  0.5182354  0.51775324
 0.51716655 0.5165526  0.516054   0.51570594 0.51544255 0.5151047
 0.51473016 0.5142081  0.51358914 0.5129987  0.5126619  0.5126025
 0.51276994 0.513054   0.5131832  0.51305324 0.51270914 0.51237214
 0.5122364  0.51232773 0.51263463 0.51303804 0.51333904 0.5134075
 0.5132308  0.5130299  0.5129137  0.51299417 0.51317483 0.5132937
 0.513422   0.5135495  0.5135784  0.51312053 0.51078117 0.50428087]
