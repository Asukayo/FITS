Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=122, out_features=154, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  67336192.0
params:  18942.0
Trainable parameters:  18942
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.244515895843506
Epoch: 1, Steps: 65 | Train Loss: 0.5292883 Vali Loss: 0.9995432 Test Loss: 0.6143373
Validation loss decreased (inf --> 0.999543).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.323955059051514
Epoch: 2, Steps: 65 | Train Loss: 0.3898784 Vali Loss: 0.8488634 Test Loss: 0.5259748
Validation loss decreased (0.999543 --> 0.848863).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.042774677276611
Epoch: 3, Steps: 65 | Train Loss: 0.3213911 Vali Loss: 0.7816890 Test Loss: 0.4944220
Validation loss decreased (0.848863 --> 0.781689).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.932347059249878
Epoch: 4, Steps: 65 | Train Loss: 0.2805333 Vali Loss: 0.7476177 Test Loss: 0.4823301
Validation loss decreased (0.781689 --> 0.747618).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.29286527633667
Epoch: 5, Steps: 65 | Train Loss: 0.2528717 Vali Loss: 0.7266451 Test Loss: 0.4742630
Validation loss decreased (0.747618 --> 0.726645).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.23244571685791
Epoch: 6, Steps: 65 | Train Loss: 0.2326957 Vali Loss: 0.7112141 Test Loss: 0.4689504
Validation loss decreased (0.726645 --> 0.711214).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.392749071121216
Epoch: 7, Steps: 65 | Train Loss: 0.2169274 Vali Loss: 0.6990962 Test Loss: 0.4648637
Validation loss decreased (0.711214 --> 0.699096).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.803033113479614
Epoch: 8, Steps: 65 | Train Loss: 0.2040945 Vali Loss: 0.6897546 Test Loss: 0.4613654
Validation loss decreased (0.699096 --> 0.689755).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.605601787567139
Epoch: 9, Steps: 65 | Train Loss: 0.1934791 Vali Loss: 0.6815881 Test Loss: 0.4574309
Validation loss decreased (0.689755 --> 0.681588).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 13.029064655303955
Epoch: 10, Steps: 65 | Train Loss: 0.1844984 Vali Loss: 0.6750599 Test Loss: 0.4545731
Validation loss decreased (0.681588 --> 0.675060).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.949034214019775
Epoch: 11, Steps: 65 | Train Loss: 0.1766474 Vali Loss: 0.6686525 Test Loss: 0.4509347
Validation loss decreased (0.675060 --> 0.668653).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.28102731704712
Epoch: 12, Steps: 65 | Train Loss: 0.1697220 Vali Loss: 0.6618194 Test Loss: 0.4479978
Validation loss decreased (0.668653 --> 0.661819).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.296846866607666
Epoch: 13, Steps: 65 | Train Loss: 0.1636063 Vali Loss: 0.6577252 Test Loss: 0.4454100
Validation loss decreased (0.661819 --> 0.657725).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.826754570007324
Epoch: 14, Steps: 65 | Train Loss: 0.1582728 Vali Loss: 0.6525496 Test Loss: 0.4426455
Validation loss decreased (0.657725 --> 0.652550).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.390944242477417
Epoch: 15, Steps: 65 | Train Loss: 0.1534545 Vali Loss: 0.6503090 Test Loss: 0.4407004
Validation loss decreased (0.652550 --> 0.650309).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 10.226515769958496
Epoch: 16, Steps: 65 | Train Loss: 0.1490825 Vali Loss: 0.6446824 Test Loss: 0.4384440
Validation loss decreased (0.650309 --> 0.644682).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.709859371185303
Epoch: 17, Steps: 65 | Train Loss: 0.1451028 Vali Loss: 0.6423033 Test Loss: 0.4366045
Validation loss decreased (0.644682 --> 0.642303).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.76755666732788
Epoch: 18, Steps: 65 | Train Loss: 0.1416059 Vali Loss: 0.6390272 Test Loss: 0.4347778
Validation loss decreased (0.642303 --> 0.639027).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.785678386688232
Epoch: 19, Steps: 65 | Train Loss: 0.1383638 Vali Loss: 0.6362168 Test Loss: 0.4327652
Validation loss decreased (0.639027 --> 0.636217).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.453693389892578
Epoch: 20, Steps: 65 | Train Loss: 0.1354513 Vali Loss: 0.6334773 Test Loss: 0.4310511
Validation loss decreased (0.636217 --> 0.633477).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 10.297666072845459
Epoch: 21, Steps: 65 | Train Loss: 0.1327728 Vali Loss: 0.6299108 Test Loss: 0.4292136
Validation loss decreased (0.633477 --> 0.629911).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.852632999420166
Epoch: 22, Steps: 65 | Train Loss: 0.1302894 Vali Loss: 0.6278411 Test Loss: 0.4275488
Validation loss decreased (0.629911 --> 0.627841).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 11.750791311264038
Epoch: 23, Steps: 65 | Train Loss: 0.1281570 Vali Loss: 0.6258729 Test Loss: 0.4262907
Validation loss decreased (0.627841 --> 0.625873).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 12.212903499603271
Epoch: 24, Steps: 65 | Train Loss: 0.1260565 Vali Loss: 0.6230510 Test Loss: 0.4249058
Validation loss decreased (0.625873 --> 0.623051).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 12.537657260894775
Epoch: 25, Steps: 65 | Train Loss: 0.1241913 Vali Loss: 0.6217183 Test Loss: 0.4235642
Validation loss decreased (0.623051 --> 0.621718).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 12.543522596359253
Epoch: 26, Steps: 65 | Train Loss: 0.1224437 Vali Loss: 0.6186973 Test Loss: 0.4222006
Validation loss decreased (0.621718 --> 0.618697).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.752385377883911
Epoch: 27, Steps: 65 | Train Loss: 0.1208068 Vali Loss: 0.6170648 Test Loss: 0.4211647
Validation loss decreased (0.618697 --> 0.617065).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 11.318291902542114
Epoch: 28, Steps: 65 | Train Loss: 0.1193427 Vali Loss: 0.6156664 Test Loss: 0.4200601
Validation loss decreased (0.617065 --> 0.615666).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 13.749472856521606
Epoch: 29, Steps: 65 | Train Loss: 0.1179470 Vali Loss: 0.6147065 Test Loss: 0.4191479
Validation loss decreased (0.615666 --> 0.614706).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 14.48487401008606
Epoch: 30, Steps: 65 | Train Loss: 0.1167149 Vali Loss: 0.6130881 Test Loss: 0.4179686
Validation loss decreased (0.614706 --> 0.613088).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 14.003358602523804
Epoch: 31, Steps: 65 | Train Loss: 0.1154842 Vali Loss: 0.6115410 Test Loss: 0.4170142
Validation loss decreased (0.613088 --> 0.611541).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 12.812764406204224
Epoch: 32, Steps: 65 | Train Loss: 0.1143490 Vali Loss: 0.6104235 Test Loss: 0.4160737
Validation loss decreased (0.611541 --> 0.610424).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 14.297629833221436
Epoch: 33, Steps: 65 | Train Loss: 0.1133198 Vali Loss: 0.6091183 Test Loss: 0.4152620
Validation loss decreased (0.610424 --> 0.609118).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 13.852957487106323
Epoch: 34, Steps: 65 | Train Loss: 0.1123204 Vali Loss: 0.6076980 Test Loss: 0.4144896
Validation loss decreased (0.609118 --> 0.607698).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 14.009284496307373
Epoch: 35, Steps: 65 | Train Loss: 0.1114334 Vali Loss: 0.6063032 Test Loss: 0.4137079
Validation loss decreased (0.607698 --> 0.606303).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 14.450966119766235
Epoch: 36, Steps: 65 | Train Loss: 0.1106528 Vali Loss: 0.6056240 Test Loss: 0.4130360
Validation loss decreased (0.606303 --> 0.605624).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 15.913130283355713
Epoch: 37, Steps: 65 | Train Loss: 0.1097730 Vali Loss: 0.6047451 Test Loss: 0.4122208
Validation loss decreased (0.605624 --> 0.604745).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 15.39850640296936
Epoch: 38, Steps: 65 | Train Loss: 0.1090403 Vali Loss: 0.6036860 Test Loss: 0.4116004
Validation loss decreased (0.604745 --> 0.603686).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 13.642789602279663
Epoch: 39, Steps: 65 | Train Loss: 0.1083613 Vali Loss: 0.6024315 Test Loss: 0.4110118
Validation loss decreased (0.603686 --> 0.602432).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 12.272141456604004
Epoch: 40, Steps: 65 | Train Loss: 0.1077335 Vali Loss: 0.6022449 Test Loss: 0.4103848
Validation loss decreased (0.602432 --> 0.602245).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 14.448981046676636
Epoch: 41, Steps: 65 | Train Loss: 0.1070823 Vali Loss: 0.5999253 Test Loss: 0.4098226
Validation loss decreased (0.602245 --> 0.599925).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 14.193042993545532
Epoch: 42, Steps: 65 | Train Loss: 0.1065133 Vali Loss: 0.6006746 Test Loss: 0.4092790
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 14.16476321220398
Epoch: 43, Steps: 65 | Train Loss: 0.1059011 Vali Loss: 0.6000372 Test Loss: 0.4087692
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 14.145782232284546
Epoch: 44, Steps: 65 | Train Loss: 0.1054156 Vali Loss: 0.5973642 Test Loss: 0.4082764
Validation loss decreased (0.599925 --> 0.597364).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 13.896688461303711
Epoch: 45, Steps: 65 | Train Loss: 0.1049391 Vali Loss: 0.5978829 Test Loss: 0.4077665
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 13.747187852859497
Epoch: 46, Steps: 65 | Train Loss: 0.1044976 Vali Loss: 0.5968938 Test Loss: 0.4072930
Validation loss decreased (0.597364 --> 0.596894).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 14.162941455841064
Epoch: 47, Steps: 65 | Train Loss: 0.1040366 Vali Loss: 0.5969637 Test Loss: 0.4068629
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 13.974307775497437
Epoch: 48, Steps: 65 | Train Loss: 0.1035830 Vali Loss: 0.5962331 Test Loss: 0.4064636
Validation loss decreased (0.596894 --> 0.596233).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 14.634542465209961
Epoch: 49, Steps: 65 | Train Loss: 0.1032217 Vali Loss: 0.5953799 Test Loss: 0.4060368
Validation loss decreased (0.596233 --> 0.595380).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 15.66788935661316
Epoch: 50, Steps: 65 | Train Loss: 0.1028734 Vali Loss: 0.5950292 Test Loss: 0.4057317
Validation loss decreased (0.595380 --> 0.595029).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=122, out_features=154, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  67336192.0
params:  18942.0
Trainable parameters:  18942
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 16.258464813232422
Epoch: 1, Steps: 65 | Train Loss: 0.3170741 Vali Loss: 0.5435431 Test Loss: 0.3635293
Validation loss decreased (inf --> 0.543543).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 15.754607200622559
Epoch: 2, Steps: 65 | Train Loss: 0.3041121 Vali Loss: 0.5275271 Test Loss: 0.3507612
Validation loss decreased (0.543543 --> 0.527527).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 13.857877254486084
Epoch: 3, Steps: 65 | Train Loss: 0.3004529 Vali Loss: 0.5221460 Test Loss: 0.3454851
Validation loss decreased (0.527527 --> 0.522146).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 13.217693328857422
Epoch: 4, Steps: 65 | Train Loss: 0.2993697 Vali Loss: 0.5183691 Test Loss: 0.3423343
Validation loss decreased (0.522146 --> 0.518369).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 13.924993753433228
Epoch: 5, Steps: 65 | Train Loss: 0.2984803 Vali Loss: 0.5170403 Test Loss: 0.3404710
Validation loss decreased (0.518369 --> 0.517040).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 12.72159743309021
Epoch: 6, Steps: 65 | Train Loss: 0.2985626 Vali Loss: 0.5164328 Test Loss: 0.3399367
Validation loss decreased (0.517040 --> 0.516433).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 13.874579668045044
Epoch: 7, Steps: 65 | Train Loss: 0.2980848 Vali Loss: 0.5150586 Test Loss: 0.3394614
Validation loss decreased (0.516433 --> 0.515059).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 13.725578308105469
Epoch: 8, Steps: 65 | Train Loss: 0.2980671 Vali Loss: 0.5143401 Test Loss: 0.3390704
Validation loss decreased (0.515059 --> 0.514340).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 13.11213731765747
Epoch: 9, Steps: 65 | Train Loss: 0.2980042 Vali Loss: 0.5144533 Test Loss: 0.3388374
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 13.309123277664185
Epoch: 10, Steps: 65 | Train Loss: 0.2978460 Vali Loss: 0.5130287 Test Loss: 0.3389573
Validation loss decreased (0.514340 --> 0.513029).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 14.07577109336853
Epoch: 11, Steps: 65 | Train Loss: 0.2976881 Vali Loss: 0.5131702 Test Loss: 0.3386602
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 13.539787292480469
Epoch: 12, Steps: 65 | Train Loss: 0.2975369 Vali Loss: 0.5132059 Test Loss: 0.3385844
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 12.348583459854126
Epoch: 13, Steps: 65 | Train Loss: 0.2975865 Vali Loss: 0.5123489 Test Loss: 0.3386453
Validation loss decreased (0.513029 --> 0.512349).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 14.245579242706299
Epoch: 14, Steps: 65 | Train Loss: 0.2976275 Vali Loss: 0.5118398 Test Loss: 0.3383452
Validation loss decreased (0.512349 --> 0.511840).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 13.5982027053833
Epoch: 15, Steps: 65 | Train Loss: 0.2975004 Vali Loss: 0.5126895 Test Loss: 0.3383040
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 13.347782135009766
Epoch: 16, Steps: 65 | Train Loss: 0.2974389 Vali Loss: 0.5131997 Test Loss: 0.3382257
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 14.310365200042725
Epoch: 17, Steps: 65 | Train Loss: 0.2974081 Vali Loss: 0.5125072 Test Loss: 0.3383126
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.33824411034584045, mae:0.3685236871242523, rse:0.5536262392997742, corr:[0.54122597 0.55360025 0.55847126 0.5589849  0.55996835 0.5623271
 0.56443745 0.56501234 0.564641   0.5645731  0.5652784  0.5661078
 0.566309   0.56571114 0.56476164 0.5638933  0.56309843 0.5621215
 0.5608065  0.5593858  0.55816    0.55710286 0.5559812  0.55472535
 0.55320424 0.55180997 0.5508245  0.5501757  0.54987055 0.5498217
 0.55007076 0.55071294 0.55147856 0.5520703  0.5520953  0.551885
 0.5516262  0.5515777  0.55164593 0.5514849  0.55109906 0.5505688
 0.5502146  0.5502878  0.5504937  0.55052185 0.55037844 0.5501747
 0.5499863  0.5498161  0.5497064  0.54956776 0.5494351  0.54929906
 0.5492754  0.5493304  0.5493291  0.5491176  0.5488577  0.54859006
 0.5484422  0.54837906 0.5482992  0.5480495  0.54785955 0.5478983
 0.5481132  0.54834574 0.5484891  0.5485125  0.548546   0.5486101
 0.5486361  0.548538   0.5483746  0.54816496 0.54796815 0.5478323
 0.547705   0.54751474 0.54725426 0.5469538  0.54673916 0.5466224
 0.54662555 0.5466065  0.5464879  0.54623574 0.5460024  0.546023
 0.54629636 0.54666173 0.5469356  0.54701847 0.5469095  0.54665154
 0.5463493  0.5460819  0.54568344 0.5452132  0.5448193  0.5446367
 0.54466474 0.5446872  0.544666   0.5446462  0.54455084 0.544467
 0.5443108  0.544053   0.54369503 0.5432755  0.5429256  0.5427341
 0.5426074  0.54237115 0.5419011  0.5412211  0.5406192  0.5404119
 0.5405966  0.54090005 0.54097456 0.5407984  0.54052156 0.54024297
 0.54002786 0.53989697 0.5397784  0.5397143  0.5396645  0.53963864
 0.53969884 0.5397499  0.5396356  0.5394448  0.5393811  0.5393556
 0.53945047 0.5396716  0.5398652  0.5400713  0.5403108  0.54054564
 0.5407075  0.54067355 0.5405616  0.5404368  0.5403518  0.5401778
 0.5399092  0.5397766  0.5398318  0.5399999  0.54011726 0.5401742
 0.5401632  0.54008543 0.5400964  0.5400539  0.5400092  0.53987545
 0.5397227  0.53962827 0.539708   0.5399357  0.540192   0.54035264
 0.5403934  0.5403607  0.5404227  0.54054695 0.54045403 0.5401558
 0.5398649  0.5397873  0.53983194 0.53979224 0.53943497 0.53895104
 0.5386148  0.5386087  0.53867674 0.5386439  0.53845716 0.5383722
 0.53861284 0.5393552  0.5402174  0.540693   0.54061395 0.54015696]
