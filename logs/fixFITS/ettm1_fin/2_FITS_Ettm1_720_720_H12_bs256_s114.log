Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  80539648.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.984972715377808
Epoch: 1, Steps: 64 | Train Loss: 0.6627157 Vali Loss: 1.4925194 Test Loss: 0.7975172
Validation loss decreased (inf --> 1.492519).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 12.36188817024231
Epoch: 2, Steps: 64 | Train Loss: 0.4995141 Vali Loss: 1.3303345 Test Loss: 0.6836998
Validation loss decreased (1.492519 --> 1.330335).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.417754411697388
Epoch: 3, Steps: 64 | Train Loss: 0.4256896 Vali Loss: 1.2434752 Test Loss: 0.6240775
Validation loss decreased (1.330335 --> 1.243475).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.271785259246826
Epoch: 4, Steps: 64 | Train Loss: 0.3831028 Vali Loss: 1.1876621 Test Loss: 0.5861101
Validation loss decreased (1.243475 --> 1.187662).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.257320880889893
Epoch: 5, Steps: 64 | Train Loss: 0.3544424 Vali Loss: 1.1498826 Test Loss: 0.5608224
Validation loss decreased (1.187662 --> 1.149883).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.44956374168396
Epoch: 6, Steps: 64 | Train Loss: 0.3338206 Vali Loss: 1.1211820 Test Loss: 0.5418556
Validation loss decreased (1.149883 --> 1.121182).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.173216342926025
Epoch: 7, Steps: 64 | Train Loss: 0.3176227 Vali Loss: 1.0984077 Test Loss: 0.5268583
Validation loss decreased (1.121182 --> 1.098408).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.373305559158325
Epoch: 8, Steps: 64 | Train Loss: 0.3049221 Vali Loss: 1.0806041 Test Loss: 0.5149950
Validation loss decreased (1.098408 --> 1.080604).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.109123945236206
Epoch: 9, Steps: 64 | Train Loss: 0.2944435 Vali Loss: 1.0659177 Test Loss: 0.5056689
Validation loss decreased (1.080604 --> 1.065918).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.022236347198486
Epoch: 10, Steps: 64 | Train Loss: 0.2855987 Vali Loss: 1.0539985 Test Loss: 0.4979558
Validation loss decreased (1.065918 --> 1.053998).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.813653945922852
Epoch: 11, Steps: 64 | Train Loss: 0.2787105 Vali Loss: 1.0436296 Test Loss: 0.4916155
Validation loss decreased (1.053998 --> 1.043630).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.213388681411743
Epoch: 12, Steps: 64 | Train Loss: 0.2725932 Vali Loss: 1.0353425 Test Loss: 0.4860859
Validation loss decreased (1.043630 --> 1.035342).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 12.501884698867798
Epoch: 13, Steps: 64 | Train Loss: 0.2674274 Vali Loss: 1.0284913 Test Loss: 0.4816275
Validation loss decreased (1.035342 --> 1.028491).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.375658750534058
Epoch: 14, Steps: 64 | Train Loss: 0.2628429 Vali Loss: 1.0214159 Test Loss: 0.4774985
Validation loss decreased (1.028491 --> 1.021416).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.999249935150146
Epoch: 15, Steps: 64 | Train Loss: 0.2590651 Vali Loss: 1.0155035 Test Loss: 0.4739226
Validation loss decreased (1.021416 --> 1.015504).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 10.778374433517456
Epoch: 16, Steps: 64 | Train Loss: 0.2555938 Vali Loss: 1.0109797 Test Loss: 0.4707371
Validation loss decreased (1.015504 --> 1.010980).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.154848098754883
Epoch: 17, Steps: 64 | Train Loss: 0.2525694 Vali Loss: 1.0075724 Test Loss: 0.4682560
Validation loss decreased (1.010980 --> 1.007572).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.176500797271729
Epoch: 18, Steps: 64 | Train Loss: 0.2499513 Vali Loss: 1.0029576 Test Loss: 0.4659473
Validation loss decreased (1.007572 --> 1.002958).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.651211977005005
Epoch: 19, Steps: 64 | Train Loss: 0.2476263 Vali Loss: 0.9993021 Test Loss: 0.4634051
Validation loss decreased (1.002958 --> 0.999302).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.716359615325928
Epoch: 20, Steps: 64 | Train Loss: 0.2455319 Vali Loss: 0.9963170 Test Loss: 0.4615395
Validation loss decreased (0.999302 --> 0.996317).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 10.991081476211548
Epoch: 21, Steps: 64 | Train Loss: 0.2436376 Vali Loss: 0.9932346 Test Loss: 0.4596293
Validation loss decreased (0.996317 --> 0.993235).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.537229776382446
Epoch: 22, Steps: 64 | Train Loss: 0.2418578 Vali Loss: 0.9907780 Test Loss: 0.4578204
Validation loss decreased (0.993235 --> 0.990778).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.951337814331055
Epoch: 23, Steps: 64 | Train Loss: 0.2403845 Vali Loss: 0.9879516 Test Loss: 0.4563679
Validation loss decreased (0.990778 --> 0.987952).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 11.147716999053955
Epoch: 24, Steps: 64 | Train Loss: 0.2388889 Vali Loss: 0.9852487 Test Loss: 0.4549321
Validation loss decreased (0.987952 --> 0.985249).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.831009864807129
Epoch: 25, Steps: 64 | Train Loss: 0.2376214 Vali Loss: 0.9840140 Test Loss: 0.4535773
Validation loss decreased (0.985249 --> 0.984014).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.03942608833313
Epoch: 26, Steps: 64 | Train Loss: 0.2365080 Vali Loss: 0.9825684 Test Loss: 0.4523493
Validation loss decreased (0.984014 --> 0.982568).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.640682220458984
Epoch: 27, Steps: 64 | Train Loss: 0.2352593 Vali Loss: 0.9792770 Test Loss: 0.4513421
Validation loss decreased (0.982568 --> 0.979277).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.199125289916992
Epoch: 28, Steps: 64 | Train Loss: 0.2342945 Vali Loss: 0.9787925 Test Loss: 0.4502833
Validation loss decreased (0.979277 --> 0.978793).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.735654592514038
Epoch: 29, Steps: 64 | Train Loss: 0.2333780 Vali Loss: 0.9759513 Test Loss: 0.4491943
Validation loss decreased (0.978793 --> 0.975951).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 10.829746723175049
Epoch: 30, Steps: 64 | Train Loss: 0.2325238 Vali Loss: 0.9753644 Test Loss: 0.4482659
Validation loss decreased (0.975951 --> 0.975364).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 11.17694640159607
Epoch: 31, Steps: 64 | Train Loss: 0.2317319 Vali Loss: 0.9750553 Test Loss: 0.4474820
Validation loss decreased (0.975364 --> 0.975055).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 10.851233959197998
Epoch: 32, Steps: 64 | Train Loss: 0.2311077 Vali Loss: 0.9728316 Test Loss: 0.4466706
Validation loss decreased (0.975055 --> 0.972832).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 11.848447561264038
Epoch: 33, Steps: 64 | Train Loss: 0.2303851 Vali Loss: 0.9721748 Test Loss: 0.4459265
Validation loss decreased (0.972832 --> 0.972175).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 12.946760654449463
Epoch: 34, Steps: 64 | Train Loss: 0.2298030 Vali Loss: 0.9708965 Test Loss: 0.4452754
Validation loss decreased (0.972175 --> 0.970896).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 12.348946809768677
Epoch: 35, Steps: 64 | Train Loss: 0.2291581 Vali Loss: 0.9699279 Test Loss: 0.4446011
Validation loss decreased (0.970896 --> 0.969928).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 10.767772436141968
Epoch: 36, Steps: 64 | Train Loss: 0.2284345 Vali Loss: 0.9690876 Test Loss: 0.4439581
Validation loss decreased (0.969928 --> 0.969088).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 11.02837610244751
Epoch: 37, Steps: 64 | Train Loss: 0.2279931 Vali Loss: 0.9681403 Test Loss: 0.4433576
Validation loss decreased (0.969088 --> 0.968140).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 10.918108463287354
Epoch: 38, Steps: 64 | Train Loss: 0.2274049 Vali Loss: 0.9669316 Test Loss: 0.4428665
Validation loss decreased (0.968140 --> 0.966932).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 10.320308685302734
Epoch: 39, Steps: 64 | Train Loss: 0.2271279 Vali Loss: 0.9671839 Test Loss: 0.4423619
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 11.414184093475342
Epoch: 40, Steps: 64 | Train Loss: 0.2266219 Vali Loss: 0.9660519 Test Loss: 0.4418958
Validation loss decreased (0.966932 --> 0.966052).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 11.384786367416382
Epoch: 41, Steps: 64 | Train Loss: 0.2262511 Vali Loss: 0.9648967 Test Loss: 0.4414353
Validation loss decreased (0.966052 --> 0.964897).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 10.856844425201416
Epoch: 42, Steps: 64 | Train Loss: 0.2258375 Vali Loss: 0.9641614 Test Loss: 0.4411135
Validation loss decreased (0.964897 --> 0.964161).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 10.819438695907593
Epoch: 43, Steps: 64 | Train Loss: 0.2254156 Vali Loss: 0.9648132 Test Loss: 0.4406536
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 10.328819751739502
Epoch: 44, Steps: 64 | Train Loss: 0.2252462 Vali Loss: 0.9639674 Test Loss: 0.4403076
Validation loss decreased (0.964161 --> 0.963967).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 9.089219570159912
Epoch: 45, Steps: 64 | Train Loss: 0.2248744 Vali Loss: 0.9635276 Test Loss: 0.4399341
Validation loss decreased (0.963967 --> 0.963528).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.719335556030273
Epoch: 46, Steps: 64 | Train Loss: 0.2245591 Vali Loss: 0.9616354 Test Loss: 0.4396221
Validation loss decreased (0.963528 --> 0.961635).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 10.169328212738037
Epoch: 47, Steps: 64 | Train Loss: 0.2243910 Vali Loss: 0.9618388 Test Loss: 0.4392894
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 8.278712511062622
Epoch: 48, Steps: 64 | Train Loss: 0.2240621 Vali Loss: 0.9606719 Test Loss: 0.4390664
Validation loss decreased (0.961635 --> 0.960672).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 10.596980571746826
Epoch: 49, Steps: 64 | Train Loss: 0.2238094 Vali Loss: 0.9613564 Test Loss: 0.4387796
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 9.985707998275757
Epoch: 50, Steps: 64 | Train Loss: 0.2236577 Vali Loss: 0.9608898 Test Loss: 0.4385123
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.0497355408796396e-05
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  80539648.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.155532836914062
Epoch: 1, Steps: 64 | Train Loss: 0.4089636 Vali Loss: 0.9453475 Test Loss: 0.4283667
Validation loss decreased (inf --> 0.945348).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.031468629837036
Epoch: 2, Steps: 64 | Train Loss: 0.4024107 Vali Loss: 0.9380620 Test Loss: 0.4236417
Validation loss decreased (0.945348 --> 0.938062).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.27730655670166
Epoch: 3, Steps: 64 | Train Loss: 0.3996349 Vali Loss: 0.9353243 Test Loss: 0.4216560
Validation loss decreased (0.938062 --> 0.935324).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.403593301773071
Epoch: 4, Steps: 64 | Train Loss: 0.3984118 Vali Loss: 0.9339643 Test Loss: 0.4212396
Validation loss decreased (0.935324 --> 0.933964).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.736036777496338
Epoch: 5, Steps: 64 | Train Loss: 0.3977514 Vali Loss: 0.9329023 Test Loss: 0.4209327
Validation loss decreased (0.933964 --> 0.932902).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.722805261611938
Epoch: 6, Steps: 64 | Train Loss: 0.3973388 Vali Loss: 0.9321876 Test Loss: 0.4207178
Validation loss decreased (0.932902 --> 0.932188).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 13.345171213150024
Epoch: 7, Steps: 64 | Train Loss: 0.3969935 Vali Loss: 0.9319890 Test Loss: 0.4205275
Validation loss decreased (0.932188 --> 0.931989).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.270564794540405
Epoch: 8, Steps: 64 | Train Loss: 0.3968757 Vali Loss: 0.9315459 Test Loss: 0.4210325
Validation loss decreased (0.931989 --> 0.931546).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.502995729446411
Epoch: 9, Steps: 64 | Train Loss: 0.3969866 Vali Loss: 0.9318180 Test Loss: 0.4208914
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 12.121192932128906
Epoch: 10, Steps: 64 | Train Loss: 0.3968775 Vali Loss: 0.9316840 Test Loss: 0.4207123
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.790006160736084
Epoch: 11, Steps: 64 | Train Loss: 0.3966981 Vali Loss: 0.9304060 Test Loss: 0.4206630
Validation loss decreased (0.931546 --> 0.930406).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.591538906097412
Epoch: 12, Steps: 64 | Train Loss: 0.3967517 Vali Loss: 0.9316405 Test Loss: 0.4209489
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.638941764831543
Epoch: 13, Steps: 64 | Train Loss: 0.3966881 Vali Loss: 0.9315215 Test Loss: 0.4208579
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.715817213058472
Epoch: 14, Steps: 64 | Train Loss: 0.3965673 Vali Loss: 0.9313381 Test Loss: 0.4211351
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.415105938911438, mae:0.4116235673427582, rse:0.6129854917526245, corr:[0.52274346 0.5327671  0.53908837 0.5405909  0.540428   0.54100066
 0.54247403 0.5439318  0.54455644 0.5442768  0.543743   0.54339814
 0.5433145  0.5431365  0.54247475 0.54132867 0.5400416  0.53889775
 0.5378549  0.5367859  0.53558046 0.5341493  0.5325903  0.531231
 0.5300199  0.52895993 0.52795285 0.5269909  0.5264228  0.5265605
 0.52745193 0.5288172  0.5299843  0.5305507  0.53032166 0.52980036
 0.5293429  0.52923584 0.5293688  0.52937615 0.5291489  0.5286736
 0.5282289  0.5281227  0.52828366 0.52847415 0.52854    0.5283753
 0.5280177  0.52764666 0.52752054 0.5275896  0.5277103  0.527626
 0.52736837 0.5270878  0.52695405 0.526962   0.52706593 0.5270714
 0.5269642  0.5268217  0.5267229  0.5266299  0.52659506 0.5265765
 0.52650684 0.52641374 0.5264113  0.52650595 0.52666867 0.5267285
 0.52655345 0.5261697  0.52579206 0.5255552  0.5255044  0.525608
 0.5257324  0.52576184 0.5256455  0.52542645 0.5252268  0.52510107
 0.5251272  0.5251894  0.5252159  0.52514005 0.5250305  0.5250379
 0.5252162  0.52549946 0.5257661  0.52586883 0.5257631  0.5254493
 0.525073   0.5247988  0.5245452  0.52438325 0.52427614 0.5242046
 0.5241837  0.52417105 0.52418315 0.5242294  0.524165   0.5239868
 0.5236762  0.52333975 0.5230524  0.5228263  0.5226542  0.522512
 0.52233833 0.5221319  0.52191347 0.5216826  0.5214538  0.5212716
 0.5210977  0.52091545 0.5207134  0.5206034  0.52058613 0.5205241
 0.5203236  0.5200201  0.519672   0.5194335  0.51932645 0.51931757
 0.5193613  0.5193418  0.51915413 0.5188958  0.5187592  0.51872903
 0.51882356 0.5189923  0.51908326 0.51911235 0.5191808  0.51939064
 0.5197284  0.5200205  0.52017283 0.5201414  0.5200151  0.5198677
 0.5197805  0.51980346 0.51983327 0.5197396  0.5194695  0.51916236
 0.5189149  0.5188226  0.5189461  0.51914567 0.5193879  0.5195981
 0.5197659  0.51992005 0.52010846 0.5203179  0.520509   0.5206495
 0.5206818  0.52061284 0.5205958  0.5206965  0.52082855 0.52091515
 0.52090085 0.520772   0.5205865  0.52042836 0.52035147 0.5203971
 0.52051854 0.52065843 0.52072316 0.5207653  0.5208415  0.52099997
 0.5212681  0.5216357  0.5220038  0.5222473  0.52231026 0.5222046
 0.521983   0.5217282  0.52136266 0.52082866 0.5201198  0.5193118
 0.518478   0.5177316  0.5171041  0.51662105 0.5161857  0.5157067
 0.515117   0.51441735 0.5136753  0.5130216  0.5125083  0.5121061
 0.51169413 0.5112109  0.5106119  0.509907   0.50910187 0.5083533
 0.50776917 0.50732476 0.5070337  0.5068514  0.50677127 0.5068381
 0.50708777 0.507343   0.50754994 0.5076723  0.5076542  0.5075457
 0.5074353  0.5073944  0.50742984 0.50746864 0.5075049  0.5075518
 0.5076062  0.50770444 0.5077576  0.5078759  0.50799125 0.5081213
 0.50819886 0.5081344  0.5080025  0.50790447 0.50790447 0.5079351
 0.5079393  0.50789315 0.5077834  0.50767875 0.50758076 0.50757056
 0.5075508  0.5075405  0.5074633  0.5073151  0.50716156 0.5070829
 0.5071246  0.5072803  0.5074548  0.5075601  0.5075721  0.50760925
 0.50764567 0.5077128  0.5077972  0.5078745  0.5078639  0.50770295
 0.5074716  0.50725824 0.50717866 0.50727344 0.507487   0.50772667
 0.50785184 0.50788856 0.50778604 0.50768065 0.5076138  0.50762385
 0.50777906 0.5079779  0.50811887 0.5081299  0.5079359  0.50753886
 0.5070298  0.5065498  0.50613046 0.50570405 0.5052399  0.50473344
 0.504211   0.50365293 0.50315225 0.5026935  0.50225186 0.50182426
 0.5013822  0.5009786  0.50067085 0.50044066 0.50025177 0.5000561
 0.49991572 0.49974298 0.4995104  0.49923056 0.4989932  0.49880704
 0.49870634 0.4987397  0.49882182 0.49889132 0.4988987  0.49886966
 0.4988107  0.49871883 0.4986239  0.49853873 0.49843466 0.49828044
 0.49808112 0.4979251  0.49780947 0.49774992 0.49767986 0.49754244
 0.49738565 0.49719882 0.4970158  0.4969411  0.49699402 0.49720877
 0.49742672 0.49743468 0.4973325  0.4971885  0.49709973 0.49708423
 0.49707693 0.4970942  0.4970875  0.49707127 0.49706507 0.49711233
 0.4971348  0.4971511  0.4971363  0.4970551  0.49695903 0.49686038
 0.49674678 0.49664706 0.49655473 0.49647257 0.49644813 0.4964791
 0.4964834  0.49650273 0.4965026  0.49646997 0.4964097  0.49634418
 0.49625129 0.49622867 0.4962975  0.4963705  0.49642324 0.49640194
 0.4963052  0.4962121  0.49620458 0.4963809  0.4967174  0.4971784
 0.49765712 0.498059   0.49833605 0.49847102 0.49848565 0.49843433
 0.49832347 0.4981441  0.49790427 0.4975684  0.49711663 0.4967066
 0.4963592  0.49608502 0.49583942 0.49559978 0.49525642 0.49480438
 0.4942813  0.49377933 0.49336323 0.49303412 0.4927108  0.4924384
 0.49217162 0.4919573  0.49172157 0.49149203 0.4912437  0.49100158
 0.49074453 0.4905047  0.49030334 0.49019146 0.4901841  0.4901953
 0.4903192  0.49040535 0.49050203 0.49052843 0.49055308 0.4906553
 0.49075893 0.4908908  0.49095187 0.49093047 0.49080023 0.49066916
 0.4905671  0.49055794 0.49059802 0.4907491  0.49089855 0.49108177
 0.4911679  0.49107596 0.49087852 0.4906791  0.49050623 0.49037042
 0.49024507 0.49011266 0.48998567 0.48988375 0.48985657 0.4899314
 0.4900489  0.49018043 0.4902428  0.49020898 0.49010333 0.4899617
 0.48991385 0.48998088 0.49014288 0.4903134  0.49042553 0.4905127
 0.49047798 0.4904001  0.49032503 0.49031463 0.4902772  0.49022797
 0.4901254  0.49003348 0.48993796 0.48990896 0.48994505 0.4899863
 0.4900054  0.48994777 0.48987123 0.48982927 0.4898712  0.48998877
 0.4901053  0.4902087  0.49024895 0.49015757 0.48992977 0.48961616
 0.4892417  0.48882377 0.48836207 0.48782885 0.48724845 0.48672894
 0.48634773 0.48597586 0.4855502  0.48502737 0.48432624 0.4835043
 0.48270026 0.4820766  0.48162806 0.48123473 0.48080504 0.48034245
 0.4798463  0.479434   0.47914618 0.47885653 0.47866577 0.47850794
 0.47841898 0.47838008 0.47834656 0.47837168 0.47842583 0.47853395
 0.47868448 0.47879755 0.47885925 0.47897694 0.47912407 0.4792753
 0.4793759  0.4794907  0.47964117 0.47976255 0.47986597 0.48000467
 0.48017448 0.48033756 0.4804486  0.48045814 0.48048285 0.48062474
 0.48084262 0.48103628 0.4811079  0.48109123 0.48101377 0.48089102
 0.48078877 0.48074982 0.48080096 0.48089835 0.48094156 0.48085636
 0.4806463  0.48038402 0.48019657 0.48015597 0.48025003 0.48044074
 0.48059916 0.48065954 0.48062986 0.4805308  0.48043704 0.48045656
 0.48046717 0.48043096 0.48035663 0.48030436 0.4803045  0.48041257
 0.48058552 0.4807561  0.4808441  0.48082572 0.48071888 0.48056692
 0.48041713 0.48040122 0.48041904 0.48046896 0.48052582 0.4806045
 0.48072535 0.4808871  0.48108324 0.48119316 0.48110846 0.48078495
 0.48028454 0.47973236 0.47916788 0.4785757  0.4779545  0.47740856
 0.4769251  0.47651285 0.4761199  0.4757108  0.4752807  0.4747796
 0.47425264 0.4736987  0.4731382  0.47258693 0.47210914 0.47169718
 0.47132906 0.47096068 0.47062683 0.47033447 0.47008827 0.46988013
 0.46973956 0.46960598 0.4695167  0.4695153  0.46964598 0.46983713
 0.47010463 0.47032103 0.47044048 0.4704763  0.4704752  0.4705387
 0.47067103 0.4707726  0.47086313 0.47091588 0.47096476 0.47104236
 0.47120503 0.4714508  0.47162956 0.4717243  0.47187212 0.47210747
 0.47235236 0.47263816 0.47289854 0.4730807  0.4732383  0.47335818
 0.47334686 0.47327387 0.47317383 0.47301012 0.47284666 0.4726402
 0.4723566  0.47207043 0.47185978 0.4716668  0.4715578  0.47148967
 0.47145483 0.47143665 0.47143468 0.4714445  0.47140634 0.47132486
 0.4711513  0.4708652  0.47062278 0.47052425 0.47056866 0.47067636
 0.4707583  0.47075224 0.4706692  0.47057772 0.4705352  0.4705555
 0.47062358 0.47075018 0.47086757 0.47089812 0.4709414  0.47103918
 0.47122717 0.4714759  0.47170928 0.47183084 0.47171664 0.47135434
 0.47085917 0.47043803 0.470124   0.46980357 0.4693836  0.468865
 0.4683326  0.46782255 0.46745443 0.46716478 0.46675998 0.4663334
 0.46577227 0.46525607 0.46480718 0.4645293  0.4643307  0.46411246
 0.46387935 0.4636392  0.46347952 0.46331733 0.46319744 0.46310344
 0.46295097 0.46286938 0.46291584 0.4631223  0.4635257  0.46390635
 0.464211   0.46434623 0.46441582 0.46465105 0.46499488 0.46550915
 0.46612415 0.46675712 0.46730617 0.4676972  0.46802214 0.46819392
 0.46843508 0.46896112 0.46963018 0.47012234 0.46829394 0.4590182 ]
