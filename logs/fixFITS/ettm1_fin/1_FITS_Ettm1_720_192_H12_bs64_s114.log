Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=106, out_features=134, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12726784.0
params:  14338.0
Trainable parameters:  14338
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3425596
	speed: 0.1383s/iter; left time: 1797.5463s
	iters: 200, epoch: 1 | loss: 0.3461653
	speed: 0.1354s/iter; left time: 1747.0432s
Epoch: 1 cost time: 35.66106605529785
Epoch: 1, Steps: 262 | Train Loss: 0.3810965 Vali Loss: 0.5658957 Test Loss: 0.3450882
Validation loss decreased (inf --> 0.565896).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2776839
	speed: 0.5920s/iter; left time: 7541.7203s
	iters: 200, epoch: 2 | loss: 0.3016354
	speed: 0.1113s/iter; left time: 1406.7625s
Epoch: 2 cost time: 31.013169765472412
Epoch: 2, Steps: 262 | Train Loss: 0.3072607 Vali Loss: 0.5371038 Test Loss: 0.3392721
Validation loss decreased (0.565896 --> 0.537104).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3090310
	speed: 0.5255s/iter; left time: 6557.1478s
	iters: 200, epoch: 3 | loss: 0.2861598
	speed: 0.1139s/iter; left time: 1410.2146s
Epoch: 3 cost time: 30.994741678237915
Epoch: 3, Steps: 262 | Train Loss: 0.3021885 Vali Loss: 0.5274166 Test Loss: 0.3391266
Validation loss decreased (0.537104 --> 0.527417).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2931704
	speed: 0.5248s/iter; left time: 6410.8663s
	iters: 200, epoch: 4 | loss: 0.2775420
	speed: 0.1183s/iter; left time: 1433.7124s
Epoch: 4 cost time: 31.390376329421997
Epoch: 4, Steps: 262 | Train Loss: 0.3002311 Vali Loss: 0.5250711 Test Loss: 0.3383235
Validation loss decreased (0.527417 --> 0.525071).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2955613
	speed: 0.5223s/iter; left time: 6243.0126s
	iters: 200, epoch: 5 | loss: 0.2715320
	speed: 0.1115s/iter; left time: 1322.0877s
Epoch: 5 cost time: 30.852611780166626
Epoch: 5, Steps: 262 | Train Loss: 0.2995228 Vali Loss: 0.5194347 Test Loss: 0.3385463
Validation loss decreased (0.525071 --> 0.519435).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2659798
	speed: 0.4703s/iter; left time: 5497.8490s
	iters: 200, epoch: 6 | loss: 0.3099135
	speed: 0.0701s/iter; left time: 812.6143s
Epoch: 6 cost time: 19.064265966415405
Epoch: 6, Steps: 262 | Train Loss: 0.2989591 Vali Loss: 0.5168176 Test Loss: 0.3382664
Validation loss decreased (0.519435 --> 0.516818).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3054772
	speed: 0.4107s/iter; left time: 4693.5871s
	iters: 200, epoch: 7 | loss: 0.2873742
	speed: 0.1118s/iter; left time: 1266.6042s
Epoch: 7 cost time: 32.380961894989014
Epoch: 7, Steps: 262 | Train Loss: 0.2986876 Vali Loss: 0.5158073 Test Loss: 0.3384424
Validation loss decreased (0.516818 --> 0.515807).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3314972
	speed: 0.5420s/iter; left time: 6052.6180s
	iters: 200, epoch: 8 | loss: 0.2921874
	speed: 0.1148s/iter; left time: 1270.4548s
Epoch: 8 cost time: 30.435330629348755
Epoch: 8, Steps: 262 | Train Loss: 0.2984197 Vali Loss: 0.5169175 Test Loss: 0.3385060
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3007158
	speed: 0.5190s/iter; left time: 5659.2779s
	iters: 200, epoch: 9 | loss: 0.2806120
	speed: 0.1120s/iter; left time: 1210.2722s
Epoch: 9 cost time: 30.285908937454224
Epoch: 9, Steps: 262 | Train Loss: 0.2982099 Vali Loss: 0.5144342 Test Loss: 0.3384141
Validation loss decreased (0.515807 --> 0.514434).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3336943
	speed: 0.5088s/iter; left time: 5415.2792s
	iters: 200, epoch: 10 | loss: 0.3074223
	speed: 0.1116s/iter; left time: 1176.3987s
Epoch: 10 cost time: 29.953974723815918
Epoch: 10, Steps: 262 | Train Loss: 0.2980693 Vali Loss: 0.5144186 Test Loss: 0.3385588
Validation loss decreased (0.514434 --> 0.514419).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3202129
	speed: 0.5059s/iter; left time: 5252.2642s
	iters: 200, epoch: 11 | loss: 0.2746816
	speed: 0.1098s/iter; left time: 1128.7669s
Epoch: 11 cost time: 29.833991050720215
Epoch: 11, Steps: 262 | Train Loss: 0.2979323 Vali Loss: 0.5148745 Test Loss: 0.3379432
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2819959
	speed: 0.5100s/iter; left time: 5160.2778s
	iters: 200, epoch: 12 | loss: 0.2922196
	speed: 0.1149s/iter; left time: 1151.2551s
Epoch: 12 cost time: 31.1402587890625
Epoch: 12, Steps: 262 | Train Loss: 0.2978298 Vali Loss: 0.5128361 Test Loss: 0.3388806
Validation loss decreased (0.514419 --> 0.512836).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2929956
	speed: 0.5744s/iter; left time: 5661.3827s
	iters: 200, epoch: 13 | loss: 0.2928188
	speed: 0.1236s/iter; left time: 1206.2943s
Epoch: 13 cost time: 33.490309715270996
Epoch: 13, Steps: 262 | Train Loss: 0.2977591 Vali Loss: 0.5146134 Test Loss: 0.3386318
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2821122
	speed: 0.5311s/iter; left time: 5095.9460s
	iters: 200, epoch: 14 | loss: 0.3062937
	speed: 0.1155s/iter; left time: 1097.0693s
Epoch: 14 cost time: 30.604072093963623
Epoch: 14, Steps: 262 | Train Loss: 0.2976246 Vali Loss: 0.5136631 Test Loss: 0.3386209
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3062535
	speed: 0.5119s/iter; left time: 4778.0289s
	iters: 200, epoch: 15 | loss: 0.3089493
	speed: 0.1123s/iter; left time: 1036.5716s
Epoch: 15 cost time: 30.44359016418457
Epoch: 15, Steps: 262 | Train Loss: 0.2977852 Vali Loss: 0.5128937 Test Loss: 0.3383394
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.338870108127594, mae:0.3687135577201843, rse:0.5541382431983948, corr:[0.5415753  0.5515139  0.5564508  0.55753744 0.55799764 0.5593822
 0.5613415  0.56292385 0.56365013 0.5637827  0.5639339  0.5642763
 0.5647135  0.56489193 0.5645786  0.56384987 0.5629583  0.56204313
 0.5609988  0.55980754 0.5585447  0.5572077  0.55584514 0.55467194
 0.5535255  0.55243987 0.55143243 0.5505283  0.5500374  0.5500824
 0.5505897  0.5513702  0.552001   0.55234796 0.5523266  0.55233335
 0.55237126 0.55244386 0.55243534 0.55217385 0.55183846 0.55150753
 0.5512825  0.5512006  0.55102676 0.5506615  0.5503128  0.5501612
 0.55023193 0.55042475 0.5506387  0.55064577 0.5503939  0.5498988
 0.5494454  0.5492737  0.5493913  0.5495072  0.54948187 0.5491476
 0.5486924  0.54842424 0.5485208  0.54879445 0.54910374 0.5492346
 0.54907    0.5487208  0.548486   0.5485079  0.548789   0.54911274
 0.5492707  0.54916525 0.54888016 0.5485026  0.54815793 0.5479154
 0.5477375  0.54758084 0.54742646 0.5472672  0.54717773 0.5471527
 0.5472361  0.54733086 0.547355   0.5471989  0.54691213 0.5466971
 0.5466765  0.54686636 0.54717714 0.54743993 0.54747903 0.5471882
 0.5466631  0.54614013 0.5456969  0.5454604  0.5454337  0.5455131
 0.5455511  0.5453663  0.5450005  0.5446085  0.5442307  0.54399943
 0.543873   0.5438151  0.54372287 0.54349434 0.5431306  0.5427303
 0.542365   0.5420803  0.54183877 0.54150003 0.54103047 0.5405466
 0.54017943 0.5400433  0.5401364  0.5403927  0.5405776  0.5404102
 0.5398474  0.5391376  0.5385901  0.5385097  0.5388298  0.5392792
 0.5396238  0.5396987  0.5394847  0.5392402  0.5392766  0.5395082
 0.5398659  0.54020435 0.5403343  0.54036564 0.54050076 0.54087114
 0.5414097  0.5418392  0.54204565 0.54199624 0.5418291  0.5416047
 0.54138726 0.5412914  0.54126227 0.54122686 0.541138   0.5410801
 0.541048   0.5409637  0.5408685  0.54067016 0.5405168  0.54045457
 0.5405275  0.5406555  0.5407822  0.5408589  0.54090905 0.54096437
 0.54098964 0.54091847 0.54084295 0.5408232  0.5407747  0.5407051
 0.5406089  0.5404618  0.5402084  0.53986895 0.5394636  0.53918755
 0.53911036 0.5392211  0.5393458  0.53948444 0.5396268  0.5398132
 0.5400329  0.54050684 0.5413576  0.542429   0.5429915  0.5416275 ]
