Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9728768.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3707329
	speed: 0.1567s/iter; left time: 4082.4224s
	iters: 200, epoch: 1 | loss: 0.3818050
	speed: 0.1426s/iter; left time: 3701.5537s
	iters: 300, epoch: 1 | loss: 0.3544000
	speed: 0.1375s/iter; left time: 3553.5768s
	iters: 400, epoch: 1 | loss: 0.3755364
	speed: 0.1421s/iter; left time: 3659.6662s
	iters: 500, epoch: 1 | loss: 0.3350034
	speed: 0.1395s/iter; left time: 3579.2561s
Epoch: 1 cost time: 74.62782073020935
Epoch: 1, Steps: 523 | Train Loss: 0.3916969 Vali Loss: 0.6908887 Test Loss: 0.3681287
Validation loss decreased (inf --> 0.690889).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3487071
	speed: 0.9355s/iter; left time: 23882.4585s
	iters: 200, epoch: 2 | loss: 0.3257140
	speed: 0.1427s/iter; left time: 3629.4234s
	iters: 300, epoch: 2 | loss: 0.3017485
	speed: 0.1411s/iter; left time: 3573.9103s
	iters: 400, epoch: 2 | loss: 0.3467057
	speed: 0.1385s/iter; left time: 3494.3657s
	iters: 500, epoch: 2 | loss: 0.3843762
	speed: 0.1444s/iter; left time: 3628.8717s
Epoch: 2 cost time: 75.35038185119629
Epoch: 2, Steps: 523 | Train Loss: 0.3411916 Vali Loss: 0.6702926 Test Loss: 0.3668477
Validation loss decreased (0.690889 --> 0.670293).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3716893
	speed: 0.9724s/iter; left time: 24314.9206s
	iters: 200, epoch: 3 | loss: 0.3554907
	speed: 0.1431s/iter; left time: 3563.2306s
	iters: 300, epoch: 3 | loss: 0.3417513
	speed: 0.1382s/iter; left time: 3427.0900s
	iters: 400, epoch: 3 | loss: 0.3504614
	speed: 0.1380s/iter; left time: 3408.1370s
	iters: 500, epoch: 3 | loss: 0.3149876
	speed: 0.1354s/iter; left time: 3330.6323s
Epoch: 3 cost time: 73.94468593597412
Epoch: 3, Steps: 523 | Train Loss: 0.3385491 Vali Loss: 0.6617051 Test Loss: 0.3666943
Validation loss decreased (0.670293 --> 0.661705).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3784200
	speed: 0.9572s/iter; left time: 23433.1548s
	iters: 200, epoch: 4 | loss: 0.4182978
	speed: 0.1241s/iter; left time: 3026.1147s
	iters: 300, epoch: 4 | loss: 0.3591255
	speed: 0.1234s/iter; left time: 2996.1410s
	iters: 400, epoch: 4 | loss: 0.2817628
	speed: 0.1251s/iter; left time: 3025.6530s
	iters: 500, epoch: 4 | loss: 0.3170643
	speed: 0.1286s/iter; left time: 3096.1087s
Epoch: 4 cost time: 66.31944680213928
Epoch: 4, Steps: 523 | Train Loss: 0.3376482 Vali Loss: 0.6582654 Test Loss: 0.3674900
Validation loss decreased (0.661705 --> 0.658265).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3787327
	speed: 0.8630s/iter; left time: 20675.4617s
	iters: 200, epoch: 5 | loss: 0.3207573
	speed: 0.1241s/iter; left time: 2959.7128s
	iters: 300, epoch: 5 | loss: 0.3300050
	speed: 0.1288s/iter; left time: 3059.0777s
	iters: 400, epoch: 5 | loss: 0.3102961
	speed: 0.1297s/iter; left time: 3067.4379s
	iters: 500, epoch: 5 | loss: 0.3622571
	speed: 0.1415s/iter; left time: 3334.5462s
Epoch: 5 cost time: 68.90873098373413
Epoch: 5, Steps: 523 | Train Loss: 0.3373074 Vali Loss: 0.6579524 Test Loss: 0.3659339
Validation loss decreased (0.658265 --> 0.657952).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3612802
	speed: 1.0375s/iter; left time: 24314.2022s
	iters: 200, epoch: 6 | loss: 0.3601065
	speed: 0.1552s/iter; left time: 3621.1199s
	iters: 300, epoch: 6 | loss: 0.2929933
	speed: 0.1538s/iter; left time: 3573.5325s
	iters: 400, epoch: 6 | loss: 0.3433380
	speed: 0.1513s/iter; left time: 3500.3389s
	iters: 500, epoch: 6 | loss: 0.3258768
	speed: 0.1496s/iter; left time: 3447.2525s
Epoch: 6 cost time: 81.16916847229004
Epoch: 6, Steps: 523 | Train Loss: 0.3369850 Vali Loss: 0.6564192 Test Loss: 0.3665478
Validation loss decreased (0.657952 --> 0.656419).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3519975
	speed: 1.0604s/iter; left time: 24297.4916s
	iters: 200, epoch: 7 | loss: 0.3314019
	speed: 0.1453s/iter; left time: 3315.6071s
	iters: 300, epoch: 7 | loss: 0.3648149
	speed: 0.1491s/iter; left time: 3385.9739s
	iters: 400, epoch: 7 | loss: 0.3634175
	speed: 0.1552s/iter; left time: 3508.6495s
	iters: 500, epoch: 7 | loss: 0.3429022
	speed: 0.1537s/iter; left time: 3459.4882s
Epoch: 7 cost time: 80.01064324378967
Epoch: 7, Steps: 523 | Train Loss: 0.3368729 Vali Loss: 0.6596495 Test Loss: 0.3667205
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3384134
	speed: 1.0090s/iter; left time: 22591.1604s
	iters: 200, epoch: 8 | loss: 0.3507920
	speed: 0.1401s/iter; left time: 3122.4189s
	iters: 300, epoch: 8 | loss: 0.3645328
	speed: 0.1377s/iter; left time: 3055.1692s
	iters: 400, epoch: 8 | loss: 0.3323050
	speed: 0.1354s/iter; left time: 2991.5557s
	iters: 500, epoch: 8 | loss: 0.3160103
	speed: 0.1413s/iter; left time: 3108.1399s
Epoch: 8 cost time: 73.56434321403503
Epoch: 8, Steps: 523 | Train Loss: 0.3366550 Vali Loss: 0.6531783 Test Loss: 0.3675136
Validation loss decreased (0.656419 --> 0.653178).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3511348
	speed: 0.9705s/iter; left time: 21221.8837s
	iters: 200, epoch: 9 | loss: 0.3501178
	speed: 0.1442s/iter; left time: 3138.3359s
	iters: 300, epoch: 9 | loss: 0.3398495
	speed: 0.1461s/iter; left time: 3164.8578s
	iters: 400, epoch: 9 | loss: 0.3427826
	speed: 0.1398s/iter; left time: 3014.7950s
	iters: 500, epoch: 9 | loss: 0.3322234
	speed: 0.1361s/iter; left time: 2921.4142s
Epoch: 9 cost time: 75.12921786308289
Epoch: 9, Steps: 523 | Train Loss: 0.3366267 Vali Loss: 0.6561401 Test Loss: 0.3668739
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3847783
	speed: 1.0234s/iter; left time: 21843.2685s
	iters: 200, epoch: 10 | loss: 0.3461296
	speed: 0.1271s/iter; left time: 2699.7109s
	iters: 300, epoch: 10 | loss: 0.3478420
	speed: 0.1244s/iter; left time: 2630.3293s
	iters: 400, epoch: 10 | loss: 0.3046376
	speed: 0.1600s/iter; left time: 3367.0262s
	iters: 500, epoch: 10 | loss: 0.3359780
	speed: 0.1691s/iter; left time: 3541.6917s
Epoch: 10 cost time: 77.29726696014404
Epoch: 10, Steps: 523 | Train Loss: 0.3364309 Vali Loss: 0.6554130 Test Loss: 0.3665783
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3298725
	speed: 1.0023s/iter; left time: 20868.2408s
	iters: 200, epoch: 11 | loss: 0.3434837
	speed: 0.1384s/iter; left time: 2868.8040s
	iters: 300, epoch: 11 | loss: 0.3441348
	speed: 0.1588s/iter; left time: 3274.6719s
	iters: 400, epoch: 11 | loss: 0.3372432
	speed: 0.1603s/iter; left time: 3289.6763s
	iters: 500, epoch: 11 | loss: 0.3641515
	speed: 0.1686s/iter; left time: 3443.0329s
Epoch: 11 cost time: 80.30873608589172
Epoch: 11, Steps: 523 | Train Loss: 0.3364359 Vali Loss: 0.6551245 Test Loss: 0.3666414
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.36741694808006287, mae:0.385450541973114, rse:0.5768035650253296, corr:[0.5363949  0.5464308  0.5496495  0.5497251  0.55057585 0.5529491
 0.5553162  0.55644214 0.5566724  0.5569253  0.5577505  0.5587944
 0.5594689  0.5594552  0.5588313  0.5577985  0.55653584 0.5552765
 0.5541134  0.5530329  0.5519251  0.5506033  0.54907036 0.547667
 0.54641664 0.54542416 0.544524   0.54353905 0.5427541  0.54247856
 0.5429285  0.54401165 0.5451387  0.5459069  0.5460574  0.5460833
 0.5462072  0.5465419  0.54681724 0.54665107 0.54618305 0.5457194
 0.54565346 0.54603386 0.5463459  0.54619074 0.54564536 0.54502475
 0.5447311  0.5449218  0.5454668  0.5459354  0.54609406 0.5458355
 0.5453567  0.5448446  0.5443987  0.5440515  0.54397166 0.54413474
 0.5444293  0.5445843  0.5443136  0.5435119  0.54263794 0.54221517
 0.54239964 0.5429387  0.543442   0.5436251  0.54351103 0.5432825
 0.54312915 0.54313153 0.5432543  0.54330593 0.54316324 0.5429003
 0.54264486 0.54250306 0.5424704  0.5424399  0.5423243  0.5420237
 0.5415983  0.54111785 0.5407328  0.5405062  0.54052866 0.5408979
 0.54151696 0.54213876 0.5425393  0.5426258  0.54242194 0.54203206
 0.5416213  0.5413428  0.5410755  0.5407772  0.5403934  0.5399209
 0.539422   0.5389255  0.53860104 0.5385592  0.53865266 0.5387885
 0.53876346 0.5385465  0.53817856 0.5377277  0.53729767 0.5369626
 0.5366961  0.53648376 0.5363227  0.536225   0.5362525  0.5363983
 0.536467   0.5362949  0.5358973  0.53558964 0.5355766  0.53576237
 0.5359057  0.53583556 0.5355196  0.53520805 0.5350793  0.5351827
 0.5354396  0.5356298  0.53560644 0.5355049  0.53553414 0.5355727
 0.53555644 0.5354183  0.53518426 0.53514457 0.535459   0.53596437
 0.53630877 0.5362005  0.5357619  0.53531533 0.53518486 0.53530025
 0.5353907  0.5353081  0.53502935 0.5347409  0.53463745 0.53479904
 0.53502285 0.5350709  0.5349566  0.53474313 0.53469694 0.5348675
 0.5351639  0.53544843 0.5357516  0.53617424 0.53673    0.53728575
 0.53759205 0.53750974 0.53721696 0.536924   0.5367122  0.5366227
 0.5365967  0.5365252  0.5363389  0.53610003 0.5358855  0.5357714
 0.5357145  0.53569174 0.5356608  0.5357122  0.53589386 0.5361854
 0.5365093  0.5368239  0.537051   0.53708917 0.53694516 0.5366615
 0.53627527 0.53592944 0.53562623 0.53535455 0.5350779  0.5347493
 0.5342713  0.53363496 0.5328744  0.53211737 0.53145736 0.53093207
 0.5304974  0.5300145  0.5293866  0.52868146 0.5279929  0.52740395
 0.52693254 0.52659905 0.5262962  0.5259216  0.5253504  0.52472377
 0.524208   0.52382153 0.5235831  0.5234079  0.523305   0.5233224
 0.52348137 0.5235947  0.5236386  0.5236278  0.5235819  0.5235898
 0.5236818  0.52379125 0.5238374  0.52375495 0.5236512  0.5236778
 0.5238501  0.5241207  0.524228   0.52427167 0.5242721  0.52437747
 0.5245541  0.5246199  0.5245477  0.52438253 0.52425253 0.5242084
 0.5242975  0.5245056  0.52467084 0.52472246 0.52458286 0.5244244
 0.52430356 0.5243576  0.5244105  0.52428913 0.52398854 0.52365315
 0.5234673  0.52354646 0.5238037  0.5240757  0.5241864  0.52414745
 0.52395767 0.52373445 0.5236005  0.52366483 0.5238701  0.5240061
 0.5239753  0.5237465  0.5234126  0.5231227  0.52294225 0.5228946
 0.5229044  0.5230106  0.52307427 0.52310115 0.5229959  0.5227903
 0.5226699  0.5227914  0.5231507  0.5236168  0.52390164 0.5237624
 0.523248   0.52265954 0.5221897  0.52179235 0.52140665 0.52093506
 0.52038395 0.5197477  0.51918536 0.5186725  0.5181713  0.5176769
 0.51721644 0.51689935 0.5167761  0.51671433 0.51654387 0.51615644
 0.5157654  0.5153873  0.515021   0.5145907  0.5141312  0.5136663
 0.51335764 0.5133287  0.5133824  0.5133294  0.51304555 0.5126271
 0.5121864  0.511716   0.51129156 0.51103413 0.51107156 0.51140755
 0.5118336  0.5121447  0.5120678  0.5116687  0.5112519  0.51118153
 0.5116381  0.51235145 0.51300526 0.5136709  0.513876   0.5106636 ]
