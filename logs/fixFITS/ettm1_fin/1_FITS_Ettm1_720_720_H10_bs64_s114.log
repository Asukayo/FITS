Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14515200.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5380737
	speed: 0.1211s/iter; left time: 1550.4711s
	iters: 200, epoch: 1 | loss: 0.4451547
	speed: 0.1145s/iter; left time: 1454.1694s
Epoch: 1 cost time: 30.418028116226196
Epoch: 1, Steps: 258 | Train Loss: 0.5148773 Vali Loss: 1.0234624 Test Loss: 0.4453326
Validation loss decreased (inf --> 1.023462).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4203031
	speed: 0.4918s/iter; left time: 6168.8635s
	iters: 200, epoch: 2 | loss: 0.4166641
	speed: 0.1163s/iter; left time: 1447.5980s
Epoch: 2 cost time: 31.18751358985901
Epoch: 2, Steps: 258 | Train Loss: 0.4148431 Vali Loss: 0.9673131 Test Loss: 0.4188263
Validation loss decreased (1.023462 --> 0.967313).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3918861
	speed: 0.4999s/iter; left time: 6141.4182s
	iters: 200, epoch: 3 | loss: 0.4086730
	speed: 0.1088s/iter; left time: 1325.8170s
Epoch: 3 cost time: 28.813154220581055
Epoch: 3, Steps: 258 | Train Loss: 0.4032265 Vali Loss: 0.9506522 Test Loss: 0.4144337
Validation loss decreased (0.967313 --> 0.950652).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4184019
	speed: 0.4877s/iter; left time: 5865.6322s
	iters: 200, epoch: 4 | loss: 0.4219819
	speed: 0.1081s/iter; left time: 1289.2421s
Epoch: 4 cost time: 29.524677276611328
Epoch: 4, Steps: 258 | Train Loss: 0.3999426 Vali Loss: 0.9433703 Test Loss: 0.4143131
Validation loss decreased (0.950652 --> 0.943370).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4151027
	speed: 0.4787s/iter; left time: 5633.6527s
	iters: 200, epoch: 5 | loss: 0.4395168
	speed: 0.1197s/iter; left time: 1397.0299s
Epoch: 5 cost time: 30.649493932724
Epoch: 5, Steps: 258 | Train Loss: 0.3987872 Vali Loss: 0.9396282 Test Loss: 0.4149978
Validation loss decreased (0.943370 --> 0.939628).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3846772
	speed: 0.4824s/iter; left time: 5553.3699s
	iters: 200, epoch: 6 | loss: 0.4128349
	speed: 0.1102s/iter; left time: 1257.7993s
Epoch: 6 cost time: 29.539151430130005
Epoch: 6, Steps: 258 | Train Loss: 0.3981571 Vali Loss: 0.9370371 Test Loss: 0.4160465
Validation loss decreased (0.939628 --> 0.937037).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3683546
	speed: 0.4916s/iter; left time: 5531.9031s
	iters: 200, epoch: 7 | loss: 0.3822961
	speed: 0.1131s/iter; left time: 1261.5838s
Epoch: 7 cost time: 30.429139614105225
Epoch: 7, Steps: 258 | Train Loss: 0.3980315 Vali Loss: 0.9366630 Test Loss: 0.4159429
Validation loss decreased (0.937037 --> 0.936663).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3831772
	speed: 0.4876s/iter; left time: 5361.2074s
	iters: 200, epoch: 8 | loss: 0.3745782
	speed: 0.1064s/iter; left time: 1158.7942s
Epoch: 8 cost time: 28.14514684677124
Epoch: 8, Steps: 258 | Train Loss: 0.3976055 Vali Loss: 0.9349158 Test Loss: 0.4162522
Validation loss decreased (0.936663 --> 0.934916).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3889568
	speed: 0.5051s/iter; left time: 5423.7814s
	iters: 200, epoch: 9 | loss: 0.3767785
	speed: 0.1173s/iter; left time: 1247.3684s
Epoch: 9 cost time: 31.98529624938965
Epoch: 9, Steps: 258 | Train Loss: 0.3976061 Vali Loss: 0.9344810 Test Loss: 0.4165640
Validation loss decreased (0.934916 --> 0.934481).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3813459
	speed: 0.5185s/iter; left time: 5433.4863s
	iters: 200, epoch: 10 | loss: 0.3997947
	speed: 0.1187s/iter; left time: 1232.0688s
Epoch: 10 cost time: 31.182100296020508
Epoch: 10, Steps: 258 | Train Loss: 0.3976103 Vali Loss: 0.9338166 Test Loss: 0.4165704
Validation loss decreased (0.934481 --> 0.933817).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4338440
	speed: 0.4877s/iter; left time: 4984.9806s
	iters: 200, epoch: 11 | loss: 0.3951672
	speed: 0.1154s/iter; left time: 1167.8351s
Epoch: 11 cost time: 29.80861806869507
Epoch: 11, Steps: 258 | Train Loss: 0.3972906 Vali Loss: 0.9326274 Test Loss: 0.4165479
Validation loss decreased (0.933817 --> 0.932627).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3683916
	speed: 0.4953s/iter; left time: 4934.3687s
	iters: 200, epoch: 12 | loss: 0.3785320
	speed: 0.1086s/iter; left time: 1071.0495s
Epoch: 12 cost time: 29.204963207244873
Epoch: 12, Steps: 258 | Train Loss: 0.3973798 Vali Loss: 0.9329584 Test Loss: 0.4165280
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3949504
	speed: 0.4677s/iter; left time: 4538.9138s
	iters: 200, epoch: 13 | loss: 0.3904267
	speed: 0.1149s/iter; left time: 1103.5207s
Epoch: 13 cost time: 29.608636617660522
Epoch: 13, Steps: 258 | Train Loss: 0.3973301 Vali Loss: 0.9327163 Test Loss: 0.4166679
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4007659
	speed: 0.4471s/iter; left time: 4224.2166s
	iters: 200, epoch: 14 | loss: 0.4193838
	speed: 0.1153s/iter; left time: 1077.3227s
Epoch: 14 cost time: 29.450522661209106
Epoch: 14, Steps: 258 | Train Loss: 0.3972012 Vali Loss: 0.9333272 Test Loss: 0.4165662
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41554802656173706, mae:0.41172555088996887, rse:0.6133118271827698, corr:[0.52287525 0.5303965  0.5358448  0.5379769  0.538136   0.53815836
 0.53871125 0.53970057 0.5408248  0.5417337  0.5423446  0.54262364
 0.54271084 0.5425748  0.54205537 0.541059   0.5397196  0.5382908
 0.53695834 0.53582895 0.53486896 0.5338772  0.5326551  0.53127545
 0.5297419  0.52846843 0.5277868  0.5277322  0.52825165 0.52906036
 0.5298374  0.5303988  0.5305695  0.5305158  0.5303437  0.53032917
 0.5303225  0.530237   0.52994865 0.52935606 0.5286915  0.52816397
 0.52797925 0.5281891  0.52851236 0.52866715 0.5285747  0.5282205
 0.52768046 0.527133   0.5268408  0.5267989  0.52688473 0.52680475
 0.52648455 0.52599317 0.5255372  0.5252904  0.52539414 0.5256793
 0.52598244 0.5261663  0.52615994 0.52594    0.5257244  0.52570856
 0.52594185 0.526334   0.5267277  0.52692646 0.5269127  0.52670324
 0.5263786  0.5260542  0.5258617  0.52577704 0.525714   0.52561766
 0.52546334 0.52528244 0.52512115 0.52501816 0.5249948  0.52497715
 0.52496195 0.5248635  0.5247057  0.5245173  0.52440137 0.52449644
 0.5248245  0.5253016  0.5258105  0.5262015  0.52641094 0.52637005
 0.5261481  0.5258646  0.52547127 0.5250906  0.5247361  0.5244092
 0.52411216 0.5238058  0.5235212  0.52333874 0.52319485 0.52312946
 0.52309877 0.52311647 0.52314234 0.5231094  0.5229802  0.52275944
 0.52245235 0.5221137  0.5218039  0.5215524  0.5213834  0.5213436
 0.5213709  0.52138436 0.5212801  0.5210908  0.5208373  0.520503
 0.5201177  0.5197777  0.51950955 0.51936114 0.5192605  0.5191589
 0.5190623  0.51894665 0.5187601  0.5185698  0.5184888  0.51847893
 0.51859623 0.5188444  0.51909757 0.5193009  0.5194186  0.51945317
 0.5194125  0.51928097 0.5191353  0.51901895 0.51897603 0.518965
 0.5189509  0.5189808  0.5190452  0.5191296  0.51919335 0.5192675
 0.5192725  0.5191623  0.51898    0.51874477 0.51862454 0.5187057
 0.5190107  0.51947314 0.5199991  0.52048856 0.52089363 0.52121305
 0.521415   0.5214892  0.5215324  0.52157724 0.5215821  0.5215504
 0.5215037  0.5214662  0.5214586  0.52147484 0.5214859  0.5214901
 0.52146995 0.5214399  0.5213746  0.5213409  0.5213725  0.52147067
 0.5216504  0.52189696 0.5221572  0.52235377 0.5224332  0.52236164
 0.52214396 0.52183604 0.52139014 0.52080107 0.5201072  0.51937616
 0.5186344  0.5179218  0.517222   0.51655716 0.5159149  0.51529175
 0.5146878  0.51409334 0.51350796 0.5129622  0.51243883 0.51190424
 0.51131624 0.5106888  0.510039   0.50937855 0.5086832  0.50803226
 0.50748867 0.50703377 0.50668716 0.5064258  0.5062577  0.50622743
 0.5063801  0.50657505 0.50675863 0.5068862  0.50689477 0.50681543
 0.50671107 0.5066509  0.50666463 0.5067279  0.50686467 0.50708145
 0.50736034 0.50769967 0.50798774 0.5082793  0.5085023  0.50867605
 0.50876015 0.5086916  0.5085299  0.5083701  0.50827724 0.50824034
 0.5082502  0.50828123 0.508297   0.5082919  0.50822735 0.5081577
 0.5080453  0.5079637  0.50790215 0.50785863 0.50785244 0.50788784
 0.50795597 0.5080497  0.5081237  0.5081429  0.5081043  0.5080977
 0.5080722  0.5080585  0.5080715  0.5081445  0.5082517  0.5083201
 0.50833535 0.508268   0.5081408  0.50798166 0.5078237  0.5077196
 0.50767434 0.50776297 0.50790066 0.5080874  0.5082271  0.50824493
 0.5081605  0.50793207 0.5075934  0.50722224 0.5068551  0.5065121
 0.50620633 0.50595134 0.50568944 0.50532717 0.504825   0.50421053
 0.50354284 0.5028494  0.5022301  0.50168717 0.50121284 0.50083166
 0.5005259  0.5003072  0.5001912  0.50013435 0.500071   0.49992037
 0.49973094 0.49944648 0.49909008 0.49872312 0.49846077 0.49831623
 0.49828777 0.49837297 0.49846804 0.49852875 0.49853048 0.498518
 0.4984942  0.498445   0.49838427 0.49831682 0.49822906 0.4981006
 0.49792567 0.49774313 0.49753886 0.497356   0.4971859  0.49703804
 0.49697298 0.49697447 0.49700704 0.49708673 0.49720088 0.49740627
 0.49761894 0.49768862 0.49767578 0.49759868 0.49749932 0.4974005
 0.49728984 0.49721798 0.49717918 0.49716938 0.4971517  0.49712077
 0.4969877  0.49679327 0.4965594  0.4963242  0.49617702 0.49614066
 0.49618706 0.4963013  0.49642304 0.49651104 0.4965814  0.49665508
 0.49667835 0.49670827 0.49671695 0.49670506 0.49666724 0.49660417
 0.49648273 0.49639177 0.49637493 0.49640712 0.4964998  0.49661902
 0.49672675 0.496817   0.49688312 0.49698272 0.4971457  0.49742725
 0.49783108 0.49830395 0.4987329  0.4990107  0.49905872 0.4988837
 0.498512   0.49801612 0.49750945 0.49703792 0.49659374 0.496269
 0.49600977 0.4957525  0.49542785 0.49502414 0.494489   0.49387035
 0.4932429  0.492692   0.492287   0.49203876 0.49186993 0.49178615
 0.49170595 0.49163428 0.4914874  0.49131012 0.49112448 0.49099505
 0.4909162  0.49089113 0.4908964  0.49093768 0.49101415 0.49106747
 0.49118212 0.49122334 0.491223   0.49110714 0.49094757 0.49085182
 0.49081644 0.49091178 0.49107692 0.49127683 0.49142268 0.4915348
 0.49157414 0.49156687 0.49150702 0.4914933  0.49149874 0.4915878
 0.49164617 0.49157438 0.49140036 0.491189   0.49094915 0.49071646
 0.49049678 0.4903151  0.49019748 0.49014622 0.49015617 0.49021184
 0.49023777 0.49023566 0.4901769  0.49009806 0.49004883 0.4900483
 0.49016175 0.4903561  0.49057007 0.4907191  0.49075145 0.49072722
 0.49058586 0.4904202  0.49028143 0.49024737 0.4902538  0.49030787
 0.4903512  0.4903935  0.49038568 0.49035987 0.49032408 0.4902732
 0.49021724 0.4901253  0.49002865 0.489954   0.48993683 0.48999876
 0.49010912 0.4902695  0.4904085  0.49042666 0.49026194 0.48992214
 0.48943263 0.48886818 0.4882889  0.48769292 0.48705772 0.48640594
 0.4857652  0.48505348 0.484302   0.4835577  0.48280773 0.4820819
 0.4814292  0.48091835 0.4805253  0.4801712  0.47980723 0.47944605
 0.47907862 0.4787635  0.47851476 0.47824118 0.4780552  0.4779298
 0.47788075 0.47785982 0.47779754 0.4777248  0.47763273 0.47759193
 0.4776425  0.47774142 0.47785652 0.47803253 0.47820407 0.4783321
 0.47837982 0.47843173 0.4785465  0.47869182 0.478875   0.47914004
 0.47947833 0.47986445 0.48025635 0.48056892 0.48084933 0.48112994
 0.48133728 0.48139384 0.48126787 0.4810656  0.48086607 0.48069853
 0.48058784 0.48053214 0.48051304 0.480505   0.48046446 0.48037127
 0.48024485 0.48011878 0.48006183 0.48010805 0.48025507 0.48050258
 0.48075587 0.48094293 0.4810022  0.48089015 0.48065555 0.48045802
 0.48029825 0.4802377  0.48029196 0.48042887 0.4805364  0.48055956
 0.4804647  0.48030242 0.4801419  0.48004913 0.480053   0.48012286
 0.48020297 0.48035488 0.48048866 0.4806448  0.480842   0.481085
 0.4813438  0.48154783 0.48165375 0.48158106 0.48129463 0.4808005
 0.48018298 0.47957802 0.47902808 0.478501   0.47796    0.47747102
 0.47699365 0.47653624 0.47606444 0.4755643  0.47504646 0.4744796
 0.4739256  0.4734071  0.47294816 0.4725461  0.47222862 0.47195864
 0.4716913  0.47137275 0.47103018 0.4706891  0.4703805  0.4701389
 0.47001982 0.46998912 0.47004387 0.47015572 0.47031668 0.47046423
 0.47065672 0.47083595 0.47098944 0.4711081  0.47117823 0.47123295
 0.47127497 0.4712573  0.47123477 0.47119197 0.47113502 0.47105455
 0.47099242 0.4709857  0.4709748  0.47099835 0.47117662 0.47147894
 0.4717468  0.4719487  0.4720382  0.47200313 0.4719765  0.472001
 0.4720086  0.47205016 0.4721192  0.47213763 0.47213045 0.4720299
 0.47178105 0.47144386 0.47109893 0.4707186  0.47040668 0.4701599
 0.4699955  0.46991053 0.46992514 0.4700314  0.47017998 0.47036302
 0.47049555 0.47048083 0.47038317 0.47025207 0.47010967 0.469972
 0.46987766 0.46985617 0.46992853 0.47008872 0.47029167 0.47047183
 0.4705987  0.4707447  0.47091702 0.47106865 0.47125766 0.4714549
 0.47163472 0.4717564  0.47179008 0.4717367  0.47154486 0.4712192
 0.47082233 0.47048727 0.4702187  0.46991965 0.4695353  0.4690872
 0.46865398 0.4682487  0.46797267 0.46778238 0.46749732 0.46718904
 0.46667516 0.46608892 0.46547794 0.46503016 0.4647641  0.46459708
 0.46445355 0.4642283  0.4639483  0.46358308 0.46329424 0.46319586
 0.46324366 0.46344125 0.46363014 0.46371624 0.46375883 0.46373206
 0.4638045  0.46399805 0.4643412  0.46485192 0.4652885  0.46558675
 0.46570972 0.4657495  0.46581686 0.46599758 0.46647865 0.46722373
 0.46824226 0.46928343 0.4697886  0.46941838 0.46733803 0.46220618]
