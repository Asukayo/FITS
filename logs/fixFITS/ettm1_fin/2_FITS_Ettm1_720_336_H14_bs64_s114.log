Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19457536.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4588594
	speed: 0.1316s/iter; left time: 1704.9040s
	iters: 200, epoch: 1 | loss: 0.3387482
	speed: 0.1278s/iter; left time: 1641.8487s
Epoch: 1 cost time: 33.86014914512634
Epoch: 1, Steps: 261 | Train Loss: 0.4233851 Vali Loss: 0.9480479 Test Loss: 0.5601069
Validation loss decreased (inf --> 0.948048).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2728511
	speed: 0.5848s/iter; left time: 7421.1418s
	iters: 200, epoch: 2 | loss: 0.2368812
	speed: 0.1277s/iter; left time: 1608.3168s
Epoch: 2 cost time: 34.32376790046692
Epoch: 2, Steps: 261 | Train Loss: 0.2532484 Vali Loss: 0.8200856 Test Loss: 0.4742967
Validation loss decreased (0.948048 --> 0.820086).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2034452
	speed: 0.4696s/iter; left time: 5836.1154s
	iters: 200, epoch: 3 | loss: 0.1936570
	speed: 0.1280s/iter; left time: 1577.9855s
Epoch: 3 cost time: 30.856533765792847
Epoch: 3, Steps: 261 | Train Loss: 0.1957841 Vali Loss: 0.7653407 Test Loss: 0.4368832
Validation loss decreased (0.820086 --> 0.765341).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1635658
	speed: 0.5579s/iter; left time: 6787.9567s
	iters: 200, epoch: 4 | loss: 0.1608197
	speed: 0.0964s/iter; left time: 1163.9089s
Epoch: 4 cost time: 27.08318543434143
Epoch: 4, Steps: 261 | Train Loss: 0.1672219 Vali Loss: 0.7347131 Test Loss: 0.4130535
Validation loss decreased (0.765341 --> 0.734713).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1535079
	speed: 0.5636s/iter; left time: 6710.4293s
	iters: 200, epoch: 5 | loss: 0.1451991
	speed: 0.1472s/iter; left time: 1737.6832s
Epoch: 5 cost time: 38.69926834106445
Epoch: 5, Steps: 261 | Train Loss: 0.1505774 Vali Loss: 0.7135500 Test Loss: 0.3957309
Validation loss decreased (0.734713 --> 0.713550).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1351109
	speed: 0.6380s/iter; left time: 7429.6154s
	iters: 200, epoch: 6 | loss: 0.1457740
	speed: 0.1305s/iter; left time: 1506.4360s
Epoch: 6 cost time: 36.05490016937256
Epoch: 6, Steps: 261 | Train Loss: 0.1402731 Vali Loss: 0.7002468 Test Loss: 0.3871865
Validation loss decreased (0.713550 --> 0.700247).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1355522
	speed: 0.4848s/iter; left time: 5519.2857s
	iters: 200, epoch: 7 | loss: 0.1252996
	speed: 0.0938s/iter; left time: 1058.5065s
Epoch: 7 cost time: 24.619622468948364
Epoch: 7, Steps: 261 | Train Loss: 0.1336103 Vali Loss: 0.6898257 Test Loss: 0.3797892
Validation loss decreased (0.700247 --> 0.689826).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1230853
	speed: 0.5034s/iter; left time: 5599.6607s
	iters: 200, epoch: 8 | loss: 0.1265807
	speed: 0.1219s/iter; left time: 1343.7448s
Epoch: 8 cost time: 32.64178824424744
Epoch: 8, Steps: 261 | Train Loss: 0.1292414 Vali Loss: 0.6853385 Test Loss: 0.3756505
Validation loss decreased (0.689826 --> 0.685338).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1233959
	speed: 0.5390s/iter; left time: 5854.6885s
	iters: 200, epoch: 9 | loss: 0.1306114
	speed: 0.1183s/iter; left time: 1272.9615s
Epoch: 9 cost time: 31.863145112991333
Epoch: 9, Steps: 261 | Train Loss: 0.1262934 Vali Loss: 0.6797280 Test Loss: 0.3731326
Validation loss decreased (0.685338 --> 0.679728).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1227521
	speed: 0.5322s/iter; left time: 5642.6989s
	iters: 200, epoch: 10 | loss: 0.1227642
	speed: 0.0965s/iter; left time: 1013.8523s
Epoch: 10 cost time: 27.707478523254395
Epoch: 10, Steps: 261 | Train Loss: 0.1243068 Vali Loss: 0.6761176 Test Loss: 0.3714941
Validation loss decreased (0.679728 --> 0.676118).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1172945
	speed: 0.4882s/iter; left time: 5048.6673s
	iters: 200, epoch: 11 | loss: 0.1136134
	speed: 0.1296s/iter; left time: 1326.9206s
Epoch: 11 cost time: 35.13248825073242
Epoch: 11, Steps: 261 | Train Loss: 0.1228811 Vali Loss: 0.6746321 Test Loss: 0.3701195
Validation loss decreased (0.676118 --> 0.674632).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1279148
	speed: 0.5648s/iter; left time: 5693.6043s
	iters: 200, epoch: 12 | loss: 0.1179267
	speed: 0.1203s/iter; left time: 1200.2211s
Epoch: 12 cost time: 32.11721444129944
Epoch: 12, Steps: 261 | Train Loss: 0.1219206 Vali Loss: 0.6753289 Test Loss: 0.3695657
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1154456
	speed: 0.5585s/iter; left time: 5484.0157s
	iters: 200, epoch: 13 | loss: 0.1105362
	speed: 0.1182s/iter; left time: 1148.7154s
Epoch: 13 cost time: 32.01886749267578
Epoch: 13, Steps: 261 | Train Loss: 0.1212520 Vali Loss: 0.6725515 Test Loss: 0.3692078
Validation loss decreased (0.674632 --> 0.672552).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1169436
	speed: 0.4653s/iter; left time: 4447.1066s
	iters: 200, epoch: 14 | loss: 0.1168164
	speed: 0.1177s/iter; left time: 1112.8755s
Epoch: 14 cost time: 31.235835552215576
Epoch: 14, Steps: 261 | Train Loss: 0.1208317 Vali Loss: 0.6735136 Test Loss: 0.3692281
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1123693
	speed: 0.5650s/iter; left time: 5252.8085s
	iters: 200, epoch: 15 | loss: 0.1150625
	speed: 0.1278s/iter; left time: 1175.1925s
Epoch: 15 cost time: 33.11508584022522
Epoch: 15, Steps: 261 | Train Loss: 0.1204670 Vali Loss: 0.6725795 Test Loss: 0.3692079
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1195715
	speed: 0.5098s/iter; left time: 4606.9429s
	iters: 200, epoch: 16 | loss: 0.1280558
	speed: 0.1115s/iter; left time: 996.5841s
Epoch: 16 cost time: 29.67586898803711
Epoch: 16, Steps: 261 | Train Loss: 0.1202959 Vali Loss: 0.6725848 Test Loss: 0.3693233
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19457536.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3349473
	speed: 0.1158s/iter; left time: 1499.1483s
	iters: 200, epoch: 1 | loss: 0.3468279
	speed: 0.1085s/iter; left time: 1394.1460s
Epoch: 1 cost time: 28.58838963508606
Epoch: 1, Steps: 261 | Train Loss: 0.3390938 Vali Loss: 0.6596341 Test Loss: 0.3672845
Validation loss decreased (inf --> 0.659634).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3312639
	speed: 0.3831s/iter; left time: 4861.0126s
	iters: 200, epoch: 2 | loss: 0.3533492
	speed: 0.0786s/iter; left time: 989.1500s
Epoch: 2 cost time: 22.63456416130066
Epoch: 2, Steps: 261 | Train Loss: 0.3375751 Vali Loss: 0.6556886 Test Loss: 0.3678553
Validation loss decreased (0.659634 --> 0.655689).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3464799
	speed: 0.4441s/iter; left time: 5519.8028s
	iters: 200, epoch: 3 | loss: 0.3213634
	speed: 0.0884s/iter; left time: 1089.4105s
Epoch: 3 cost time: 25.823720455169678
Epoch: 3, Steps: 261 | Train Loss: 0.3370885 Vali Loss: 0.6573018 Test Loss: 0.3672096
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3544219
	speed: 0.5035s/iter; left time: 6126.6291s
	iters: 200, epoch: 4 | loss: 0.3287953
	speed: 0.0898s/iter; left time: 1083.4644s
Epoch: 4 cost time: 26.6795175075531
Epoch: 4, Steps: 261 | Train Loss: 0.3367138 Vali Loss: 0.6565030 Test Loss: 0.3668728
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3232182
	speed: 0.4315s/iter; left time: 5138.1015s
	iters: 200, epoch: 5 | loss: 0.3405139
	speed: 0.0784s/iter; left time: 925.3137s
Epoch: 5 cost time: 22.773802518844604
Epoch: 5, Steps: 261 | Train Loss: 0.3367842 Vali Loss: 0.6543576 Test Loss: 0.3669958
Validation loss decreased (0.655689 --> 0.654358).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3254379
	speed: 0.3949s/iter; left time: 4599.0041s
	iters: 200, epoch: 6 | loss: 0.3443044
	speed: 0.0814s/iter; left time: 940.1672s
Epoch: 6 cost time: 22.397458791732788
Epoch: 6, Steps: 261 | Train Loss: 0.3366072 Vali Loss: 0.6546938 Test Loss: 0.3677907
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3259955
	speed: 0.3299s/iter; left time: 3755.5390s
	iters: 200, epoch: 7 | loss: 0.3772257
	speed: 0.0876s/iter; left time: 988.5558s
Epoch: 7 cost time: 23.634800910949707
Epoch: 7, Steps: 261 | Train Loss: 0.3365578 Vali Loss: 0.6548045 Test Loss: 0.3676021
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3248861
	speed: 0.4087s/iter; left time: 4545.8575s
	iters: 200, epoch: 8 | loss: 0.3278691
	speed: 0.0873s/iter; left time: 962.6961s
Epoch: 8 cost time: 23.93236255645752
Epoch: 8, Steps: 261 | Train Loss: 0.3363523 Vali Loss: 0.6555637 Test Loss: 0.3668695
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3664945960044861, mae:0.3851906359195709, rse:0.5760790705680847, corr:[0.53753275 0.5493665  0.5532842  0.552831   0.5530217  0.5550854
 0.55746347 0.55849594 0.5583756  0.55829597 0.55898124 0.5599851
 0.56041473 0.55989593 0.5588473  0.5578023  0.5568955  0.5559509
 0.5547514  0.55340356 0.5521523  0.55099994 0.5498185  0.5486022
 0.5472231  0.5459704  0.545022   0.5443066  0.54388607 0.5437682
 0.5440412  0.5448101  0.5457404  0.54641175 0.54635596 0.5458538
 0.5452363  0.5450194  0.54527444 0.54556537 0.54564273 0.5454444
 0.54529434 0.54551303 0.54585385 0.545899   0.5454887  0.5447405
 0.5440288  0.54369694 0.5438394  0.5441025  0.5442024  0.54405767
 0.54397213 0.5441466  0.5444697  0.5446094  0.54443306 0.543911
 0.5433572  0.5430704  0.54304963 0.5430232  0.5429641  0.5429032
 0.54286474 0.54285777 0.5428425  0.5427326  0.54256177 0.5423744
 0.54220223 0.54205304 0.54194176 0.5418414  0.5417465  0.54169977
 0.5416909  0.54168797 0.5416521  0.54157865 0.541522   0.54146546
 0.54139704 0.5412002  0.54084533 0.54036164 0.5399438  0.53984165
 0.54010254 0.54059196 0.54107654 0.5413592  0.5413322  0.54101866
 0.54057384 0.5402095  0.53993124 0.5398318  0.5399164  0.5401048
 0.54027927 0.5402591  0.54007167 0.5398172  0.5394934  0.5392077
 0.5389332  0.53871584 0.538549   0.5383823  0.53821576 0.5380364
 0.53776    0.5373467  0.5368177  0.53626686 0.5358826  0.5358079
 0.53591686 0.5359707  0.53575474 0.5353798  0.5350279  0.5347982
 0.5347043  0.5347352  0.5348115  0.5349457  0.53502655 0.5349468
 0.53467834 0.534201   0.5336125  0.53326225 0.5334274  0.5338205
 0.5341319  0.534116   0.53374493 0.5334214  0.53354245 0.53411275
 0.5347595  0.5350395  0.5349236  0.5346203  0.5344521  0.5344427
 0.53443676 0.5343571  0.53420293 0.5341205  0.5342188  0.5345083
 0.5347303  0.53464967 0.53436613 0.5340812  0.5341627  0.53459835
 0.5351264  0.53542215 0.53539544 0.53522    0.53515726 0.5353008
 0.53552765 0.53572804 0.53594124 0.5361471  0.5362664  0.53631216
 0.53631157 0.5362924  0.53623    0.53612274 0.5359997  0.5359847
 0.5361088  0.5363384  0.53651327 0.53659874 0.53662026 0.5366773
 0.53688663 0.53734946 0.5379783  0.53852105 0.5387691  0.53863025
 0.5381544  0.5375694  0.5369869  0.5365018  0.53614235 0.5358408
 0.5354122  0.5347633  0.53391546 0.5329752  0.5320204  0.5311101
 0.5302883  0.5295487  0.5288834  0.5283543  0.5279717  0.52770364
 0.52745545 0.5271835  0.5267677  0.5261685  0.5253893  0.5246719
 0.5242255  0.5240139  0.5239405  0.5237856  0.5234963  0.52321374
 0.52314097 0.5232541  0.5235774  0.5239843  0.52423424 0.52423483
 0.524014   0.5237256  0.5235823  0.5236456  0.5238834  0.5241794
 0.5243611  0.5244125  0.524293   0.52431065 0.5244994  0.524848
 0.5251587  0.52520204 0.52505684 0.5249384  0.525012   0.5251751
 0.52522695 0.5250305  0.52452624 0.52390844 0.5233377  0.5231033
 0.5232115  0.5236722  0.5241526  0.52437663 0.5242722  0.5239574
 0.5236208  0.5234321  0.52337044 0.5233526  0.5233168  0.52337193
 0.5234985  0.5237221  0.5240298  0.5243702  0.5246366  0.5247317
 0.52471936 0.5246349  0.5245228  0.5244214  0.52431375 0.5242175
 0.5241092  0.5240924  0.52409834 0.52417666 0.5242674  0.52438176
 0.5245771  0.5247974  0.5248981  0.5247764  0.5243389  0.5236047
 0.52280694 0.52223206 0.52189696 0.52157074 0.52114874 0.5205932
 0.5200019  0.5194141  0.5189514  0.51853245 0.51809597 0.5176468
 0.5172095  0.5168677  0.51663935 0.5163744  0.5159559  0.5153793
 0.51497394 0.51479524 0.5147495  0.5146065  0.51430225 0.5138464
 0.51347035 0.5134364  0.513636   0.5138434  0.5138392  0.51363504
 0.5133544  0.51308495 0.51294184 0.51297736 0.5131484  0.51329273
 0.5132059  0.5128669  0.5122407  0.5116813  0.51158166 0.5118937
 0.51223236 0.51209736 0.5115551  0.511401   0.51110774 0.5062503 ]
