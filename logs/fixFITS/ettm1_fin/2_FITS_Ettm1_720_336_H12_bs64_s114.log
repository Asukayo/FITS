Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14721280.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4155468
	speed: 0.1417s/iter; left time: 1834.6373s
	iters: 200, epoch: 1 | loss: 0.3242065
	speed: 0.1535s/iter; left time: 1973.1240s
Epoch: 1 cost time: 39.277331590652466
Epoch: 1, Steps: 261 | Train Loss: 0.4094154 Vali Loss: 0.9239279 Test Loss: 0.5325979
Validation loss decreased (inf --> 0.923928).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2538343
	speed: 0.6557s/iter; left time: 8320.9936s
	iters: 200, epoch: 2 | loss: 0.2188564
	speed: 0.1414s/iter; left time: 1780.1755s
Epoch: 2 cost time: 38.35809016227722
Epoch: 2, Steps: 261 | Train Loss: 0.2400249 Vali Loss: 0.7943202 Test Loss: 0.4516084
Validation loss decreased (0.923928 --> 0.794320).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1932490
	speed: 0.6018s/iter; left time: 7479.5307s
	iters: 200, epoch: 3 | loss: 0.1783802
	speed: 0.1274s/iter; left time: 1571.0524s
Epoch: 3 cost time: 35.28355431556702
Epoch: 3, Steps: 261 | Train Loss: 0.1845257 Vali Loss: 0.7410838 Test Loss: 0.4155768
Validation loss decreased (0.794320 --> 0.741084).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1568943
	speed: 0.5124s/iter; left time: 6234.8471s
	iters: 200, epoch: 4 | loss: 0.1452401
	speed: 0.1291s/iter; left time: 1557.9662s
Epoch: 4 cost time: 34.55916118621826
Epoch: 4, Steps: 261 | Train Loss: 0.1583912 Vali Loss: 0.7155577 Test Loss: 0.3971218
Validation loss decreased (0.741084 --> 0.715558).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1400999
	speed: 0.5404s/iter; left time: 6434.2133s
	iters: 200, epoch: 5 | loss: 0.1318603
	speed: 0.1372s/iter; left time: 1619.4483s
Epoch: 5 cost time: 34.056408643722534
Epoch: 5, Steps: 261 | Train Loss: 0.1441061 Vali Loss: 0.6997073 Test Loss: 0.3852448
Validation loss decreased (0.715558 --> 0.699707).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1263391
	speed: 0.6973s/iter; left time: 8120.2139s
	iters: 200, epoch: 6 | loss: 0.1348946
	speed: 0.1514s/iter; left time: 1748.4003s
Epoch: 6 cost time: 41.614198446273804
Epoch: 6, Steps: 261 | Train Loss: 0.1356787 Vali Loss: 0.6891182 Test Loss: 0.3774483
Validation loss decreased (0.699707 --> 0.689118).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1253130
	speed: 0.8495s/iter; left time: 9671.1467s
	iters: 200, epoch: 7 | loss: 0.1253444
	speed: 0.1816s/iter; left time: 2049.3608s
Epoch: 7 cost time: 47.677165269851685
Epoch: 7, Steps: 261 | Train Loss: 0.1305028 Vali Loss: 0.6822382 Test Loss: 0.3732340
Validation loss decreased (0.689118 --> 0.682238).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1376628
	speed: 0.7566s/iter; left time: 8416.9338s
	iters: 200, epoch: 8 | loss: 0.1288107
	speed: 0.1194s/iter; left time: 1316.3555s
Epoch: 8 cost time: 34.98679876327515
Epoch: 8, Steps: 261 | Train Loss: 0.1272610 Vali Loss: 0.6790985 Test Loss: 0.3700834
Validation loss decreased (0.682238 --> 0.679098).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1289264
	speed: 0.4067s/iter; left time: 4418.3116s
	iters: 200, epoch: 9 | loss: 0.1125043
	speed: 0.0753s/iter; left time: 810.3200s
Epoch: 9 cost time: 17.32515835762024
Epoch: 9, Steps: 261 | Train Loss: 0.1251876 Vali Loss: 0.6768862 Test Loss: 0.3688315
Validation loss decreased (0.679098 --> 0.676886).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1219518
	speed: 0.1759s/iter; left time: 1864.4204s
	iters: 200, epoch: 10 | loss: 0.1167088
	speed: 0.0426s/iter; left time: 447.8177s
Epoch: 10 cost time: 11.840210437774658
Epoch: 10, Steps: 261 | Train Loss: 0.1238146 Vali Loss: 0.6737282 Test Loss: 0.3682627
Validation loss decreased (0.676886 --> 0.673728).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1294337
	speed: 0.1757s/iter; left time: 1816.6388s
	iters: 200, epoch: 11 | loss: 0.1236838
	speed: 0.0376s/iter; left time: 384.9905s
Epoch: 11 cost time: 10.676624536514282
Epoch: 11, Steps: 261 | Train Loss: 0.1229093 Vali Loss: 0.6741152 Test Loss: 0.3683488
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1186786
	speed: 0.1736s/iter; left time: 1749.6467s
	iters: 200, epoch: 12 | loss: 0.1262841
	speed: 0.0442s/iter; left time: 440.6512s
Epoch: 12 cost time: 11.991662502288818
Epoch: 12, Steps: 261 | Train Loss: 0.1223105 Vali Loss: 0.6732691 Test Loss: 0.3679867
Validation loss decreased (0.673728 --> 0.673269).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1278979
	speed: 0.1821s/iter; left time: 1787.7248s
	iters: 200, epoch: 13 | loss: 0.1227820
	speed: 0.0392s/iter; left time: 380.5482s
Epoch: 13 cost time: 10.77432632446289
Epoch: 13, Steps: 261 | Train Loss: 0.1219152 Vali Loss: 0.6729701 Test Loss: 0.3683496
Validation loss decreased (0.673269 --> 0.672970).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1235368
	speed: 0.1705s/iter; left time: 1630.0718s
	iters: 200, epoch: 14 | loss: 0.1234352
	speed: 0.0367s/iter; left time: 346.8920s
Epoch: 14 cost time: 10.311350107192993
Epoch: 14, Steps: 261 | Train Loss: 0.1216213 Vali Loss: 0.6734247 Test Loss: 0.3688570
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1223633
	speed: 0.1762s/iter; left time: 1638.0161s
	iters: 200, epoch: 15 | loss: 0.1264160
	speed: 0.0582s/iter; left time: 535.6403s
Epoch: 15 cost time: 13.912945032119751
Epoch: 15, Steps: 261 | Train Loss: 0.1214581 Vali Loss: 0.6734723 Test Loss: 0.3690322
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1213401
	speed: 0.2984s/iter; left time: 2696.1668s
	iters: 200, epoch: 16 | loss: 0.1256256
	speed: 0.0733s/iter; left time: 655.4136s
Epoch: 16 cost time: 18.49975872039795
Epoch: 16, Steps: 261 | Train Loss: 0.1212812 Vali Loss: 0.6723983 Test Loss: 0.3694201
Validation loss decreased (0.672970 --> 0.672398).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1124788
	speed: 0.2832s/iter; left time: 2485.4942s
	iters: 200, epoch: 17 | loss: 0.1153229
	speed: 0.0344s/iter; left time: 298.8501s
Epoch: 17 cost time: 9.349814414978027
Epoch: 17, Steps: 261 | Train Loss: 0.1212338 Vali Loss: 0.6736724 Test Loss: 0.3699687
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1233862
	speed: 0.1377s/iter; left time: 1172.1315s
	iters: 200, epoch: 18 | loss: 0.1284328
	speed: 0.0265s/iter; left time: 223.3493s
Epoch: 18 cost time: 9.287227869033813
Epoch: 18, Steps: 261 | Train Loss: 0.1210889 Vali Loss: 0.6730759 Test Loss: 0.3701131
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1117239
	speed: 0.1805s/iter; left time: 1489.7417s
	iters: 200, epoch: 19 | loss: 0.1147223
	speed: 0.0531s/iter; left time: 433.2522s
Epoch: 19 cost time: 13.727416276931763
Epoch: 19, Steps: 261 | Train Loss: 0.1210771 Vali Loss: 0.6731725 Test Loss: 0.3702614
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14721280.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3140582
	speed: 0.0707s/iter; left time: 915.1824s
	iters: 200, epoch: 1 | loss: 0.3673839
	speed: 0.0565s/iter; left time: 725.6063s
Epoch: 1 cost time: 15.54875111579895
Epoch: 1, Steps: 261 | Train Loss: 0.3393384 Vali Loss: 0.6612765 Test Loss: 0.3679143
Validation loss decreased (inf --> 0.661277).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3345483
	speed: 0.1953s/iter; left time: 2478.0440s
	iters: 200, epoch: 2 | loss: 0.3229657
	speed: 0.0315s/iter; left time: 396.2551s
Epoch: 2 cost time: 9.510471820831299
Epoch: 2, Steps: 261 | Train Loss: 0.3378329 Vali Loss: 0.6585320 Test Loss: 0.3678234
Validation loss decreased (0.661277 --> 0.658532).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3320319
	speed: 0.1568s/iter; left time: 1948.4488s
	iters: 200, epoch: 3 | loss: 0.3282716
	speed: 0.0257s/iter; left time: 316.9796s
Epoch: 3 cost time: 7.747375249862671
Epoch: 3, Steps: 261 | Train Loss: 0.3373760 Vali Loss: 0.6570389 Test Loss: 0.3677181
Validation loss decreased (0.658532 --> 0.657039).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3575778
	speed: 0.1414s/iter; left time: 1720.2118s
	iters: 200, epoch: 4 | loss: 0.3382537
	speed: 0.0281s/iter; left time: 339.3198s
Epoch: 4 cost time: 8.80691647529602
Epoch: 4, Steps: 261 | Train Loss: 0.3369385 Vali Loss: 0.6577648 Test Loss: 0.3675622
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3540624
	speed: 0.1407s/iter; left time: 1675.7568s
	iters: 200, epoch: 5 | loss: 0.3215120
	speed: 0.0342s/iter; left time: 403.5436s
Epoch: 5 cost time: 8.921655654907227
Epoch: 5, Steps: 261 | Train Loss: 0.3368981 Vali Loss: 0.6569799 Test Loss: 0.3672357
Validation loss decreased (0.657039 --> 0.656980).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3497447
	speed: 0.1565s/iter; left time: 1822.5790s
	iters: 200, epoch: 6 | loss: 0.3472182
	speed: 0.0291s/iter; left time: 335.7618s
Epoch: 6 cost time: 7.944639682769775
Epoch: 6, Steps: 261 | Train Loss: 0.3366939 Vali Loss: 0.6538657 Test Loss: 0.3672150
Validation loss decreased (0.656980 --> 0.653866).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3173766
	speed: 0.1568s/iter; left time: 1784.6492s
	iters: 200, epoch: 7 | loss: 0.3339361
	speed: 0.0266s/iter; left time: 299.6721s
Epoch: 7 cost time: 9.155012369155884
Epoch: 7, Steps: 261 | Train Loss: 0.3367139 Vali Loss: 0.6559357 Test Loss: 0.3665819
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3733763
	speed: 0.3066s/iter; left time: 3410.5335s
	iters: 200, epoch: 8 | loss: 0.3660142
	speed: 0.1180s/iter; left time: 1301.1986s
Epoch: 8 cost time: 32.50334596633911
Epoch: 8, Steps: 261 | Train Loss: 0.3366432 Vali Loss: 0.6533569 Test Loss: 0.3674916
Validation loss decreased (0.653866 --> 0.653357).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3440026
	speed: 0.5523s/iter; left time: 5999.7667s
	iters: 200, epoch: 9 | loss: 0.3526433
	speed: 0.1175s/iter; left time: 1264.2088s
Epoch: 9 cost time: 32.09521555900574
Epoch: 9, Steps: 261 | Train Loss: 0.3365962 Vali Loss: 0.6548332 Test Loss: 0.3672370
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3312263
	speed: 0.5421s/iter; left time: 5747.1420s
	iters: 200, epoch: 10 | loss: 0.3223158
	speed: 0.1154s/iter; left time: 1212.3215s
Epoch: 10 cost time: 31.12432289123535
Epoch: 10, Steps: 261 | Train Loss: 0.3365916 Vali Loss: 0.6547914 Test Loss: 0.3670645
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3273744
	speed: 0.5590s/iter; left time: 5780.3456s
	iters: 200, epoch: 11 | loss: 0.3046148
	speed: 0.1163s/iter; left time: 1190.7763s
Epoch: 11 cost time: 32.13356113433838
Epoch: 11, Steps: 261 | Train Loss: 0.3365212 Vali Loss: 0.6527750 Test Loss: 0.3673089
Validation loss decreased (0.653357 --> 0.652775).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3246269
	speed: 0.5424s/iter; left time: 5467.1731s
	iters: 200, epoch: 12 | loss: 0.3131071
	speed: 0.1234s/iter; left time: 1231.3505s
Epoch: 12 cost time: 32.98178815841675
Epoch: 12, Steps: 261 | Train Loss: 0.3362979 Vali Loss: 0.6537849 Test Loss: 0.3672685
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3289791
	speed: 0.6509s/iter; left time: 6391.3251s
	iters: 200, epoch: 13 | loss: 0.3175889
	speed: 0.1514s/iter; left time: 1470.9871s
Epoch: 13 cost time: 40.85031771659851
Epoch: 13, Steps: 261 | Train Loss: 0.3363888 Vali Loss: 0.6541641 Test Loss: 0.3668742
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3559899
	speed: 0.6660s/iter; left time: 6366.0368s
	iters: 200, epoch: 14 | loss: 0.3297594
	speed: 0.1300s/iter; left time: 1229.5604s
Epoch: 14 cost time: 35.706069231033325
Epoch: 14, Steps: 261 | Train Loss: 0.3362914 Vali Loss: 0.6532219 Test Loss: 0.3672995
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3668278455734253, mae:0.3851407766342163, rse:0.5763409733772278, corr:[0.5381524  0.5479086  0.55237144 0.5531963  0.553555   0.554909
 0.5568136  0.5582771  0.558905   0.55906093 0.55941033 0.55999905
 0.56048226 0.56037736 0.55955577 0.5583338  0.5571688  0.556256
 0.55537796 0.55430627 0.5529771  0.5513931  0.5497348  0.5483599
 0.54719776 0.54622704 0.54530483 0.5443483  0.5436535  0.54346216
 0.54387105 0.5447822  0.54569924 0.54633343 0.54645324 0.5463672
 0.54610085 0.5458011  0.5455198  0.54519105 0.5450136  0.54503536
 0.5452842  0.5456754  0.5458674  0.54567677 0.5452526  0.54477876
 0.544405   0.54419667 0.54419893 0.54424524 0.5442599  0.5441619
 0.5441166  0.54422694 0.5444181  0.5444291  0.5441631  0.54355466
 0.54286134 0.54244256 0.5424924  0.5428376  0.5433001  0.54361695
 0.5435897  0.54327846 0.5429484  0.54279596 0.54290223 0.543136
 0.54328936 0.54322624 0.54296345 0.5425393  0.54204804 0.54163885
 0.54139715 0.54134506 0.5413882  0.54136014 0.5411483  0.5406866
 0.54011893 0.5396192  0.5394047  0.539538   0.5399907  0.5406745
 0.5413666  0.54186213 0.5420584  0.54198664 0.5417343  0.5413788
 0.5409997  0.54068065 0.540291   0.5398788  0.53953457 0.53934854
 0.539353   0.53941745 0.53948677 0.539502   0.5393223  0.5390273
 0.538651   0.5382952  0.53797454 0.537613   0.53716266 0.53665936
 0.53616583 0.5357981  0.53562015 0.5355701  0.53557444 0.5355939
 0.5355617  0.53547764 0.5353664  0.5353771  0.5355047  0.5355991
 0.53556556 0.53545445 0.53531724 0.53528666 0.5353335  0.53540134
 0.5354784  0.5354897  0.53536487 0.5352048  0.5351607  0.53510916
 0.5350546  0.5349881  0.5348513  0.53477985 0.5349047  0.5352512
 0.5357004  0.5359867  0.53601974 0.5358234  0.5356286  0.5355672
 0.53569245 0.5359864  0.5362726  0.5363961  0.53629935 0.53613484
 0.53598416 0.5358564  0.53578764 0.5356527  0.53551537 0.5354017
 0.53539616 0.5355127  0.5357426  0.5360264  0.5363184  0.53660625
 0.53681916 0.5368873  0.5368576  0.5367172  0.5364087  0.5360161
 0.5356894  0.5355802  0.5357271  0.53604543 0.53630227 0.5363422
 0.5360984  0.535699   0.5353351  0.53528476 0.53563744 0.53625816
 0.5368887  0.537337   0.53751385 0.5374298  0.5372165  0.537046
 0.53695977 0.536993   0.5369089  0.536518   0.5357908  0.5348744
 0.5339531  0.53325266 0.53285176 0.5326831  0.5325112  0.5321485
 0.53154385 0.5307546  0.5298993  0.52914184 0.52854127 0.52804655
 0.5275278  0.52696776 0.5263714  0.5258091  0.5252653  0.52483237
 0.52451485 0.52418965 0.5238507  0.523503   0.5232801  0.52335256
 0.5237902  0.524321   0.5247338  0.5248674  0.52466595 0.5242901
 0.5239672  0.5238555  0.5239636  0.5241385  0.5242864  0.5243785
 0.5244128  0.5244833  0.5244945  0.5245861  0.52468014 0.5247857
 0.5248366  0.52471    0.5244782  0.5242636  0.5241723  0.52414644
 0.5241441  0.524174   0.5242086  0.5243018  0.5243906  0.5245219
 0.5245688  0.52457374 0.5244551  0.52422196 0.52396536 0.5237763
 0.5237112  0.52381474 0.524036   0.52431023 0.5245822  0.52489746
 0.5251446  0.52529484 0.5253739  0.5254482  0.5254773  0.5253616
 0.52512133 0.5247662  0.52437305 0.5240613  0.5239133  0.523978
 0.5241501  0.5243769  0.5244927  0.5245254  0.5244418  0.5243085
 0.52426183 0.5243459  0.52446604 0.5245418  0.52444816 0.5240883
 0.52351105 0.5228948  0.5223107  0.5216965  0.52106297 0.5204139
 0.519801   0.51921314 0.51876503 0.5184049  0.5180842  0.51775455
 0.51735353 0.5169214  0.5165629  0.51630217 0.5160886  0.51579994
 0.5154977  0.51509774 0.5146683  0.51431924 0.51419777 0.51423305
 0.5143218  0.51437974 0.51428294 0.51409596 0.5139424  0.51396054
 0.5141128  0.5142252  0.5141999  0.51403606 0.513816   0.51364774
 0.5135715  0.51363313 0.51371104 0.5137502  0.51362115 0.5132409
 0.5127334  0.5122598  0.51211447 0.5124934  0.5126073  0.5097851 ]
