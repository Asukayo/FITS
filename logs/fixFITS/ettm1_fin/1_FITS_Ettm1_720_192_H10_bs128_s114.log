Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=90, out_features=114, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  18385920.0
params:  10374.0
Trainable parameters:  10374
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3411760
	speed: 0.1252s/iter; left time: 807.9052s
Epoch: 1 cost time: 16.697149991989136
Epoch: 1, Steps: 131 | Train Loss: 0.4212371 Vali Loss: 0.6204712 Test Loss: 0.3608311
Validation loss decreased (inf --> 0.620471).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3212585
	speed: 0.3711s/iter; left time: 2345.5508s
Epoch: 2 cost time: 18.26096487045288
Epoch: 2, Steps: 131 | Train Loss: 0.3209436 Vali Loss: 0.5645211 Test Loss: 0.3432887
Validation loss decreased (0.620471 --> 0.564521).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3019760
	speed: 0.3505s/iter; left time: 2169.5169s
Epoch: 3 cost time: 17.326980113983154
Epoch: 3, Steps: 131 | Train Loss: 0.3091975 Vali Loss: 0.5460481 Test Loss: 0.3407467
Validation loss decreased (0.564521 --> 0.546048).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3255178
	speed: 0.3528s/iter; left time: 2137.1852s
Epoch: 4 cost time: 16.696720600128174
Epoch: 4, Steps: 131 | Train Loss: 0.3049349 Vali Loss: 0.5376971 Test Loss: 0.3402367
Validation loss decreased (0.546048 --> 0.537697).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3142765
	speed: 0.3601s/iter; left time: 2134.2391s
Epoch: 5 cost time: 17.115890741348267
Epoch: 5, Steps: 131 | Train Loss: 0.3027323 Vali Loss: 0.5322112 Test Loss: 0.3396530
Validation loss decreased (0.537697 --> 0.532211).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2899708
	speed: 0.3590s/iter; left time: 2080.5248s
Epoch: 6 cost time: 16.419963121414185
Epoch: 6, Steps: 131 | Train Loss: 0.3015390 Vali Loss: 0.5281454 Test Loss: 0.3394976
Validation loss decreased (0.532211 --> 0.528145).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3068703
	speed: 0.3858s/iter; left time: 2185.7994s
Epoch: 7 cost time: 17.76897883415222
Epoch: 7, Steps: 131 | Train Loss: 0.3007728 Vali Loss: 0.5248064 Test Loss: 0.3390181
Validation loss decreased (0.528145 --> 0.524806).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3089623
	speed: 0.4145s/iter; left time: 2293.6661s
Epoch: 8 cost time: 20.37651515007019
Epoch: 8, Steps: 131 | Train Loss: 0.3003490 Vali Loss: 0.5242696 Test Loss: 0.3389041
Validation loss decreased (0.524806 --> 0.524270).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2970853
	speed: 0.4322s/iter; left time: 2335.1267s
Epoch: 9 cost time: 19.72745990753174
Epoch: 9, Steps: 131 | Train Loss: 0.2995656 Vali Loss: 0.5219750 Test Loss: 0.3388253
Validation loss decreased (0.524270 --> 0.521975).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2955338
	speed: 0.3951s/iter; left time: 2083.0400s
Epoch: 10 cost time: 18.332642793655396
Epoch: 10, Steps: 131 | Train Loss: 0.2996071 Vali Loss: 0.5210292 Test Loss: 0.3388668
Validation loss decreased (0.521975 --> 0.521029).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3079334
	speed: 0.3725s/iter; left time: 1915.2532s
Epoch: 11 cost time: 17.111478805541992
Epoch: 11, Steps: 131 | Train Loss: 0.2990741 Vali Loss: 0.5193360 Test Loss: 0.3387775
Validation loss decreased (0.521029 --> 0.519336).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2987026
	speed: 0.3684s/iter; left time: 1845.7131s
Epoch: 12 cost time: 17.867132902145386
Epoch: 12, Steps: 131 | Train Loss: 0.2989756 Vali Loss: 0.5179366 Test Loss: 0.3386954
Validation loss decreased (0.519336 --> 0.517937).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3083968
	speed: 0.3647s/iter; left time: 1779.5459s
Epoch: 13 cost time: 17.362892866134644
Epoch: 13, Steps: 131 | Train Loss: 0.2989256 Vali Loss: 0.5176500 Test Loss: 0.3392222
Validation loss decreased (0.517937 --> 0.517650).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3106554
	speed: 0.3471s/iter; left time: 1648.0503s
Epoch: 14 cost time: 17.338080883026123
Epoch: 14, Steps: 131 | Train Loss: 0.2987012 Vali Loss: 0.5180475 Test Loss: 0.3385197
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2933840
	speed: 0.3503s/iter; left time: 1617.3971s
Epoch: 15 cost time: 17.657052755355835
Epoch: 15, Steps: 131 | Train Loss: 0.2985560 Vali Loss: 0.5163370 Test Loss: 0.3386458
Validation loss decreased (0.517650 --> 0.516337).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3125706
	speed: 0.3649s/iter; left time: 1636.7973s
Epoch: 16 cost time: 17.71200919151306
Epoch: 16, Steps: 131 | Train Loss: 0.2985540 Vali Loss: 0.5160608 Test Loss: 0.3387475
Validation loss decreased (0.516337 --> 0.516061).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3129613
	speed: 0.3581s/iter; left time: 1559.6923s
Epoch: 17 cost time: 17.54233741760254
Epoch: 17, Steps: 131 | Train Loss: 0.2984814 Vali Loss: 0.5147838 Test Loss: 0.3388695
Validation loss decreased (0.516061 --> 0.514784).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2781661
	speed: 0.3538s/iter; left time: 1494.3402s
Epoch: 18 cost time: 17.32197093963623
Epoch: 18, Steps: 131 | Train Loss: 0.2985139 Vali Loss: 0.5164855 Test Loss: 0.3386212
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2887893
	speed: 0.3643s/iter; left time: 1490.9202s
Epoch: 19 cost time: 17.888412952423096
Epoch: 19, Steps: 131 | Train Loss: 0.2983075 Vali Loss: 0.5155677 Test Loss: 0.3387127
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3103711
	speed: 0.3566s/iter; left time: 1412.7917s
Epoch: 20 cost time: 17.444242477416992
Epoch: 20, Steps: 131 | Train Loss: 0.2983272 Vali Loss: 0.5145127 Test Loss: 0.3387021
Validation loss decreased (0.514784 --> 0.514513).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2921080
	speed: 0.3558s/iter; left time: 1363.1282s
Epoch: 21 cost time: 16.34940004348755
Epoch: 21, Steps: 131 | Train Loss: 0.2981646 Vali Loss: 0.5155642 Test Loss: 0.3386082
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2835573
	speed: 0.2843s/iter; left time: 1051.7563s
Epoch: 22 cost time: 12.979636669158936
Epoch: 22, Steps: 131 | Train Loss: 0.2982412 Vali Loss: 0.5142634 Test Loss: 0.3386226
Validation loss decreased (0.514513 --> 0.514263).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2927274
	speed: 0.2994s/iter; left time: 1068.6919s
Epoch: 23 cost time: 16.75784921646118
Epoch: 23, Steps: 131 | Train Loss: 0.2981317 Vali Loss: 0.5142658 Test Loss: 0.3387435
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3042487
	speed: 0.3674s/iter; left time: 1263.2908s
Epoch: 24 cost time: 18.639145135879517
Epoch: 24, Steps: 131 | Train Loss: 0.2980271 Vali Loss: 0.5146549 Test Loss: 0.3385618
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3009558
	speed: 0.3864s/iter; left time: 1277.6751s
Epoch: 25 cost time: 19.461750268936157
Epoch: 25, Steps: 131 | Train Loss: 0.2981777 Vali Loss: 0.5145314 Test Loss: 0.3386529
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.33856201171875, mae:0.368615061044693, rse:0.553886353969574, corr:[0.541514   0.55040777 0.5562469  0.55842775 0.5589099  0.5594931
 0.56064534 0.56212455 0.5634744  0.5643249  0.56475484 0.5649486
 0.5651504  0.5652783  0.5650836  0.56438524 0.5632405  0.56186634
 0.5604636  0.5591953  0.5581076  0.5570398  0.55581605 0.55448854
 0.55297875 0.55159855 0.55062306 0.5501012  0.5501078  0.5504899
 0.5510053  0.55147904 0.5517183  0.551846   0.55189633 0.552076
 0.5521976  0.55219567 0.55200493 0.5515489  0.5510583  0.5506848
 0.5505579  0.55072755 0.5509376  0.5509695  0.5508364  0.5505929
 0.5503021  0.5500782  0.55006677 0.55019265 0.55033094 0.5502497
 0.54993886 0.5495064  0.5491428  0.54893285 0.548973   0.54907554
 0.5491236  0.5490485  0.5488544  0.5485506  0.54835945 0.5484027
 0.54863375 0.5489248  0.5491723  0.54928476 0.54930466 0.5492624
 0.54919434 0.54913616 0.54912335 0.54906666 0.54890084 0.5486353
 0.5483158  0.54801875 0.5478079  0.5476817  0.54763234 0.54756856
 0.5474808  0.5473386  0.5472011  0.5470914  0.54708    0.5472521
 0.54755265 0.5478546  0.5480454  0.5480577  0.5478741  0.5475181
 0.5471063  0.54677796 0.54645234 0.54614276 0.5458566  0.54561657
 0.545456   0.5453305  0.54528123 0.5453368  0.5453479  0.5452937
 0.5451163  0.54486096 0.5445718  0.5442676  0.5439663  0.54367906
 0.54338187 0.54305226 0.5426814  0.5422604  0.5418715  0.5416258
 0.5415163  0.54144806 0.5412703  0.5409794  0.54060596 0.5401922
 0.53986055 0.5397621  0.5398773  0.5401211  0.5402738  0.5402118
 0.53999686 0.5397409  0.5395205  0.5394761  0.53968644 0.5399411
 0.5401776  0.5403503  0.54037565 0.54036933 0.54047364 0.5407359
 0.5410451  0.5411627  0.5410233  0.54064053 0.5402007  0.53985345
 0.53973794 0.5399637  0.5403663  0.54072165 0.5408328  0.5407156
 0.5404164  0.5400647  0.53991675 0.5399771  0.54028356 0.54066247
 0.540967   0.5410448  0.5409116  0.54070795 0.54063505 0.5407977
 0.54111093 0.5413874  0.5415673  0.5415761  0.541308   0.54086167
 0.5404348  0.5402024  0.5402104  0.54036254 0.5403701  0.5400712
 0.5394241  0.53863704 0.53793675 0.53767097 0.537937   0.5385073
 0.5389373  0.53915805 0.5394134  0.53996366 0.5405488  0.54016733]
