Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7257600.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5006710
	speed: 0.1161s/iter; left time: 2988.5922s
	iters: 200, epoch: 1 | loss: 0.4152440
	speed: 0.1062s/iter; left time: 2723.8301s
	iters: 300, epoch: 1 | loss: 0.3390805
	speed: 0.0720s/iter; left time: 1838.5445s
	iters: 400, epoch: 1 | loss: 0.3051139
	speed: 0.0809s/iter; left time: 2057.9088s
	iters: 500, epoch: 1 | loss: 0.2689840
	speed: 0.0707s/iter; left time: 1793.2276s
Epoch: 1 cost time: 45.66970229148865
Epoch: 1, Steps: 517 | Train Loss: 0.4011174 Vali Loss: 1.0610898 Test Loss: 0.4922263
Validation loss decreased (inf --> 1.061090).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2718517
	speed: 0.6112s/iter; left time: 15421.9599s
	iters: 200, epoch: 2 | loss: 0.2688774
	speed: 0.1034s/iter; left time: 2598.1383s
	iters: 300, epoch: 2 | loss: 0.2378340
	speed: 0.1035s/iter; left time: 2590.9367s
	iters: 400, epoch: 2 | loss: 0.2122334
	speed: 0.1046s/iter; left time: 2608.7879s
	iters: 500, epoch: 2 | loss: 0.2138216
	speed: 0.1020s/iter; left time: 2533.5216s
Epoch: 2 cost time: 53.670095682144165
Epoch: 2, Steps: 517 | Train Loss: 0.2520606 Vali Loss: 0.9795254 Test Loss: 0.4414352
Validation loss decreased (1.061090 --> 0.979525).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2382665
	speed: 0.6993s/iter; left time: 17285.6863s
	iters: 200, epoch: 3 | loss: 0.2227262
	speed: 0.1100s/iter; left time: 2707.4374s
	iters: 300, epoch: 3 | loss: 0.2193893
	speed: 0.1064s/iter; left time: 2609.6693s
	iters: 400, epoch: 3 | loss: 0.2044585
	speed: 0.1087s/iter; left time: 2654.0867s
	iters: 500, epoch: 3 | loss: 0.2142566
	speed: 0.1169s/iter; left time: 2843.1810s
Epoch: 3 cost time: 57.83884572982788
Epoch: 3, Steps: 517 | Train Loss: 0.2253639 Vali Loss: 0.9549573 Test Loss: 0.4241152
Validation loss decreased (0.979525 --> 0.954957).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2208747
	speed: 0.8267s/iter; left time: 20005.4033s
	iters: 200, epoch: 4 | loss: 0.2230275
	speed: 0.1204s/iter; left time: 2900.8014s
	iters: 300, epoch: 4 | loss: 0.2634631
	speed: 0.1121s/iter; left time: 2691.2775s
	iters: 400, epoch: 4 | loss: 0.2159982
	speed: 0.1229s/iter; left time: 2937.3449s
	iters: 500, epoch: 4 | loss: 0.2042142
	speed: 0.1165s/iter; left time: 2771.7257s
Epoch: 4 cost time: 62.51573324203491
Epoch: 4, Steps: 517 | Train Loss: 0.2163149 Vali Loss: 0.9457091 Test Loss: 0.4172824
Validation loss decreased (0.954957 --> 0.945709).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2078215
	speed: 0.7224s/iter; left time: 17107.9465s
	iters: 200, epoch: 5 | loss: 0.2163558
	speed: 0.1030s/iter; left time: 2430.1489s
	iters: 300, epoch: 5 | loss: 0.2037096
	speed: 0.1078s/iter; left time: 2530.3377s
	iters: 400, epoch: 5 | loss: 0.2203187
	speed: 0.1027s/iter; left time: 2400.6481s
	iters: 500, epoch: 5 | loss: 0.2131553
	speed: 0.1108s/iter; left time: 2580.4002s
Epoch: 5 cost time: 56.00261163711548
Epoch: 5, Steps: 517 | Train Loss: 0.2126249 Vali Loss: 0.9419875 Test Loss: 0.4151720
Validation loss decreased (0.945709 --> 0.941988).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1794673
	speed: 0.7468s/iter; left time: 17300.2305s
	iters: 200, epoch: 6 | loss: 0.2225927
	speed: 0.1118s/iter; left time: 2577.9817s
	iters: 300, epoch: 6 | loss: 0.2408444
	speed: 0.1155s/iter; left time: 2653.3457s
	iters: 400, epoch: 6 | loss: 0.2497064
	speed: 0.1143s/iter; left time: 2613.2707s
	iters: 500, epoch: 6 | loss: 0.2071644
	speed: 0.1120s/iter; left time: 2549.2976s
Epoch: 6 cost time: 59.30154609680176
Epoch: 6, Steps: 517 | Train Loss: 0.2110880 Vali Loss: 0.9390123 Test Loss: 0.4153605
Validation loss decreased (0.941988 --> 0.939012).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1910029
	speed: 0.7092s/iter; left time: 16061.9369s
	iters: 200, epoch: 7 | loss: 0.2228658
	speed: 0.1143s/iter; left time: 2578.4186s
	iters: 300, epoch: 7 | loss: 0.2096963
	speed: 0.1060s/iter; left time: 2378.5675s
	iters: 400, epoch: 7 | loss: 0.2084562
	speed: 0.1053s/iter; left time: 2352.7493s
	iters: 500, epoch: 7 | loss: 0.1907033
	speed: 0.0992s/iter; left time: 2207.4788s
Epoch: 7 cost time: 55.68183708190918
Epoch: 7, Steps: 517 | Train Loss: 0.2104659 Vali Loss: 0.9405231 Test Loss: 0.4159051
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1944530
	speed: 0.6952s/iter; left time: 15385.6365s
	iters: 200, epoch: 8 | loss: 0.2009568
	speed: 0.1023s/iter; left time: 2252.8544s
	iters: 300, epoch: 8 | loss: 0.2155783
	speed: 0.1073s/iter; left time: 2353.2263s
	iters: 400, epoch: 8 | loss: 0.2163538
	speed: 0.1040s/iter; left time: 2271.3581s
	iters: 500, epoch: 8 | loss: 0.2012590
	speed: 0.1004s/iter; left time: 2181.7736s
Epoch: 8 cost time: 54.42438888549805
Epoch: 8, Steps: 517 | Train Loss: 0.2102109 Vali Loss: 0.9396596 Test Loss: 0.4171913
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2150765
	speed: 0.7404s/iter; left time: 16004.1318s
	iters: 200, epoch: 9 | loss: 0.2032617
	speed: 0.1148s/iter; left time: 2470.5994s
	iters: 300, epoch: 9 | loss: 0.1954761
	speed: 0.1192s/iter; left time: 2551.8325s
	iters: 400, epoch: 9 | loss: 0.1848190
	speed: 0.1116s/iter; left time: 2377.8227s
	iters: 500, epoch: 9 | loss: 0.1933314
	speed: 0.1072s/iter; left time: 2273.2784s
Epoch: 9 cost time: 59.39457058906555
Epoch: 9, Steps: 517 | Train Loss: 0.2101487 Vali Loss: 0.9412801 Test Loss: 0.4170488
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7257600.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3917701
	speed: 0.1234s/iter; left time: 3178.1462s
	iters: 200, epoch: 1 | loss: 0.4004469
	speed: 0.1021s/iter; left time: 2618.5335s
	iters: 300, epoch: 1 | loss: 0.3835520
	speed: 0.1066s/iter; left time: 2722.5589s
	iters: 400, epoch: 1 | loss: 0.4100105
	speed: 0.1093s/iter; left time: 2780.5285s
	iters: 500, epoch: 1 | loss: 0.4064257
	speed: 0.1125s/iter; left time: 2851.8543s
Epoch: 1 cost time: 57.32157588005066
Epoch: 1, Steps: 517 | Train Loss: 0.3988189 Vali Loss: 0.9333832 Test Loss: 0.4169456
Validation loss decreased (inf --> 0.933383).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4124908
	speed: 0.7106s/iter; left time: 17930.8853s
	iters: 200, epoch: 2 | loss: 0.4154692
	speed: 0.1040s/iter; left time: 2613.1823s
	iters: 300, epoch: 2 | loss: 0.4108170
	speed: 0.0971s/iter; left time: 2431.1203s
	iters: 400, epoch: 2 | loss: 0.4019012
	speed: 0.1071s/iter; left time: 2670.8798s
	iters: 500, epoch: 2 | loss: 0.4413477
	speed: 0.1017s/iter; left time: 2526.5753s
Epoch: 2 cost time: 54.41995573043823
Epoch: 2, Steps: 517 | Train Loss: 0.3980540 Vali Loss: 0.9317942 Test Loss: 0.4161887
Validation loss decreased (0.933383 --> 0.931794).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4505483
	speed: 0.6831s/iter; left time: 16884.8636s
	iters: 200, epoch: 3 | loss: 0.3699206
	speed: 0.1013s/iter; left time: 2492.5128s
	iters: 300, epoch: 3 | loss: 0.4090722
	speed: 0.1034s/iter; left time: 2534.8213s
	iters: 400, epoch: 3 | loss: 0.3629471
	speed: 0.0998s/iter; left time: 2435.7193s
	iters: 500, epoch: 3 | loss: 0.4488261
	speed: 0.0944s/iter; left time: 2295.6000s
Epoch: 3 cost time: 52.91418719291687
Epoch: 3, Steps: 517 | Train Loss: 0.3977592 Vali Loss: 0.9319665 Test Loss: 0.4165483
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3940812
	speed: 0.6015s/iter; left time: 14555.4666s
	iters: 200, epoch: 4 | loss: 0.3884933
	speed: 0.0941s/iter; left time: 2266.7088s
	iters: 300, epoch: 4 | loss: 0.3504883
	speed: 0.1175s/iter; left time: 2819.4363s
	iters: 400, epoch: 4 | loss: 0.4007478
	speed: 0.1028s/iter; left time: 2456.9091s
	iters: 500, epoch: 4 | loss: 0.4492243
	speed: 0.0877s/iter; left time: 2088.1213s
Epoch: 4 cost time: 48.01331663131714
Epoch: 4, Steps: 517 | Train Loss: 0.3975967 Vali Loss: 0.9317209 Test Loss: 0.4169296
Validation loss decreased (0.931794 --> 0.931721).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3939716
	speed: 0.6784s/iter; left time: 16066.8036s
	iters: 200, epoch: 5 | loss: 0.4236919
	speed: 0.0913s/iter; left time: 2152.8873s
	iters: 300, epoch: 5 | loss: 0.3991958
	speed: 0.1158s/iter; left time: 2719.8634s
	iters: 400, epoch: 5 | loss: 0.4021813
	speed: 0.1029s/iter; left time: 2406.9864s
	iters: 500, epoch: 5 | loss: 0.4534833
	speed: 0.1136s/iter; left time: 2644.1841s
Epoch: 5 cost time: 55.891488790512085
Epoch: 5, Steps: 517 | Train Loss: 0.3975009 Vali Loss: 0.9317607 Test Loss: 0.4163400
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3766477
	speed: 0.7658s/iter; left time: 17740.7999s
	iters: 200, epoch: 6 | loss: 0.3746262
	speed: 0.1180s/iter; left time: 2722.0329s
	iters: 300, epoch: 6 | loss: 0.3998738
	speed: 0.1228s/iter; left time: 2819.8794s
	iters: 400, epoch: 6 | loss: 0.4347562
	speed: 0.1199s/iter; left time: 2741.1951s
	iters: 500, epoch: 6 | loss: 0.3477538
	speed: 0.1221s/iter; left time: 2780.1788s
Epoch: 6 cost time: 63.57905554771423
Epoch: 6, Steps: 517 | Train Loss: 0.3975586 Vali Loss: 0.9321796 Test Loss: 0.4160875
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3677907
	speed: 0.7204s/iter; left time: 16315.7304s
	iters: 200, epoch: 7 | loss: 0.4130448
	speed: 0.1085s/iter; left time: 2446.8682s
	iters: 300, epoch: 7 | loss: 0.4309489
	speed: 0.1020s/iter; left time: 2290.2873s
	iters: 400, epoch: 7 | loss: 0.4116178
	speed: 0.1054s/iter; left time: 2354.9391s
	iters: 500, epoch: 7 | loss: 0.4173190
	speed: 0.1047s/iter; left time: 2329.5067s
Epoch: 7 cost time: 56.01965951919556
Epoch: 7, Steps: 517 | Train Loss: 0.3973838 Vali Loss: 0.9323112 Test Loss: 0.4166971
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4167340099811554, mae:0.41237005591392517, rse:0.6141864061355591, corr:[0.5234844  0.5328362  0.53877825 0.5405357  0.540017   0.5395095
 0.53996927 0.54131925 0.542963   0.5441284  0.5445089  0.54421365
 0.5436729  0.54310685 0.5424559  0.5415679  0.5403475  0.53883296
 0.53719395 0.5357083  0.53453386 0.53351885 0.5324163  0.5311858
 0.5296772  0.52811766 0.5268001  0.5260067  0.5260137  0.5267153
 0.5276921  0.52853674 0.5288659  0.5287476  0.52835816 0.5281269
 0.5280835  0.5282058  0.5283088  0.52814275 0.5277544  0.5272458
 0.5268964  0.5269617  0.52736175 0.5278629  0.5282772  0.52841157
 0.52819    0.52774346 0.5274372  0.5274692  0.5278927  0.5284134
 0.5288203  0.5289482  0.52877    0.5283265  0.52779007 0.52721155
 0.52667445 0.5262287  0.52587986 0.5256011  0.52552986 0.52574605
 0.5262095  0.52679986 0.5273737  0.5277381  0.527849   0.52769035
 0.52731115 0.5268256  0.52639395 0.52605486 0.52579314 0.52562016
 0.5255188  0.52547526 0.5254379  0.52535284 0.5252161  0.5249993
 0.52480584 0.52466756 0.5246816  0.5248541  0.5251679  0.52558833
 0.5259991  0.5262672  0.52631825 0.5261343  0.5258264  0.5254815
 0.52521825 0.5250856  0.52487546 0.5245652  0.5240879  0.52345914
 0.5227954  0.5222063  0.52185965 0.5218725  0.5221021  0.52243906
 0.52268326 0.52275985 0.52263796 0.5223478  0.52200586 0.5217456
 0.52162576 0.52165735 0.5217696  0.52184755 0.5218184  0.5217063
 0.52149636 0.52120525 0.5208523  0.5205744  0.5204092  0.5202605
 0.5200303  0.5197115  0.51929104 0.5188638  0.51848936 0.5182702
 0.5183112  0.51858115 0.5189188  0.5192222  0.51944363 0.5194421
 0.51927334 0.5190653  0.51885915 0.5187498  0.51879364 0.518987
 0.51927674 0.51955163 0.5197958  0.5199896  0.52013624 0.52019936
 0.5201548  0.52008915 0.5200293  0.5199818  0.5199243  0.51991206
 0.5198714  0.51976186 0.5196197  0.51942223 0.51929635 0.5192755
 0.5193569  0.5194787  0.5195819  0.51961964 0.51961946 0.5196576
 0.5197519  0.5198946  0.5201313  0.520419   0.5206293  0.5206997
 0.52062774 0.52045983 0.520294   0.520208   0.5202173  0.5203173
 0.5204538  0.5205927  0.52067536 0.5207613  0.520881   0.5210443
 0.5212439  0.5214767  0.5217003  0.5218566  0.5218985  0.5218236
 0.5216533  0.5214704  0.5212053  0.5208055  0.5202657  0.5196352
 0.518916   0.51814485 0.5173361  0.51656735 0.51586056 0.51523626
 0.51470035 0.5142337  0.51381475 0.51346636 0.5131675  0.5128769
 0.51253414 0.5121187  0.5116018  0.5109577  0.5101412  0.5092515
 0.5084101  0.5076727  0.50715387 0.50687164 0.5068136  0.5069574
 0.50727814 0.50756687 0.5077652  0.50787127 0.50787026 0.5078383
 0.5078437  0.5079118  0.5080035  0.50800896 0.5079136  0.50774163
 0.5075331  0.5073722  0.5072032  0.5071525  0.5071603  0.5072436
 0.507336   0.50731885 0.5072287  0.50713694 0.5071208  0.50714713
 0.5071895  0.507232   0.5072332  0.50720924 0.5071171  0.5070533
 0.50698763 0.5070242  0.5071181  0.5072291  0.5073356  0.5074215
 0.5074815  0.50754094 0.5075968  0.50766176 0.5077442  0.5079156
 0.5080557  0.5081212  0.50807256 0.50795096 0.50778764 0.50761503
 0.50751853 0.50750387 0.50755525 0.50761116 0.5076031  0.507511
 0.50731844 0.5071636  0.507063   0.50712156 0.50729716 0.5075107
 0.50773776 0.50787616 0.5078832  0.50779724 0.5076302  0.50740236
 0.5071549  0.5069521  0.50673646 0.5063852  0.5058544  0.5051725
 0.5044312  0.503684   0.5030647  0.50256985 0.50215065 0.5017789
 0.5013859  0.50098443 0.5006413  0.50037897 0.50020427 0.5000836
 0.5000817  0.500095   0.50007254 0.49998045 0.49986705 0.49971312
 0.49954122 0.49941638 0.49931273 0.49924374 0.49920654 0.49922282
 0.49924365 0.49919954 0.4990891  0.49893916 0.4987762  0.49861938
 0.49848083 0.4984033  0.49834096 0.498289   0.49820042 0.49806514
 0.49795854 0.49786773 0.4977596  0.49764463 0.49751186 0.4974514
 0.49741802 0.49727112 0.49709266 0.49691316 0.4967945  0.49676082
 0.49677286 0.49686044 0.49696112 0.4970417  0.49705833 0.49703735
 0.49693674 0.49684104 0.49680337 0.49683586 0.4969799  0.49719054
 0.49737933 0.49750754 0.49753073 0.49744374 0.49733087 0.49726138
 0.49720573 0.49724048 0.49732128 0.49741477 0.49747226 0.49744076
 0.49723175 0.49692932 0.49660158 0.49626887 0.4960229  0.49590567
 0.49592978 0.4960999  0.49637243 0.49672636 0.49709353 0.49745634
 0.49779946 0.49809778 0.49833292 0.49845836 0.49845248 0.4983457
 0.49814487 0.49786898 0.49756065 0.4971977  0.49672008 0.49623126
 0.49570978 0.49516827 0.49461505 0.49412555 0.4936961  0.4933814
 0.49318177 0.49307373 0.49299267 0.49284914 0.49251288 0.4920303
 0.49141502 0.49083167 0.49033633 0.49006152 0.49002936 0.49022397
 0.49051994 0.49080616 0.49098185 0.49103937 0.49101615 0.4909252
 0.4909376  0.49098304 0.4910964  0.491142   0.4911099  0.49102885
 0.49085385 0.4906742  0.4905015  0.4904073  0.49038723 0.49048886
 0.49062723 0.49074733 0.4907429  0.49066606 0.4904777  0.4903193
 0.49016374 0.48996434 0.48977143 0.4896347  0.4895235  0.4894098
 0.48927158 0.48912606 0.48902044 0.48900667 0.4891162  0.48933375
 0.4895457  0.4896908  0.48969126 0.48956347 0.4893882  0.4892339
 0.48923117 0.48939872 0.48970312 0.49004024 0.49030998 0.4904954
 0.49050236 0.49041593 0.49032238 0.4903512  0.490469   0.49069616
 0.49093464 0.49113238 0.49115783 0.4910084  0.49071816 0.49035513
 0.49005285 0.48986247 0.48985657 0.49002257 0.49030444 0.49060372
 0.49079332 0.4908452  0.49073267 0.49043024 0.48996517 0.4894333
 0.48891565 0.4884833  0.48815393 0.48786232 0.4875307  0.48714605
 0.48672634 0.48618597 0.48556042 0.48489797 0.4841752  0.48340368
 0.4826143  0.48188567 0.48123837 0.48064658 0.48012576 0.47972652
 0.47943422 0.47926617 0.47916555 0.47896418 0.47873506 0.4784578
 0.47821766 0.47805867 0.4779862  0.4780668  0.47825748 0.47854954
 0.4788873  0.47915325 0.47930035 0.47940525 0.4794533  0.47944698
 0.47936836 0.47928488 0.47921455 0.4790899  0.47892177 0.4788058
 0.47881016 0.47897652 0.47929192 0.4796597  0.4800841  0.48052654
 0.48085016 0.48093936 0.4807538  0.48042968 0.4800931  0.4798195
 0.4796704  0.47964272 0.47970527 0.47980022 0.47987002 0.4798824
 0.479854   0.4798089  0.47979254 0.4798057  0.479826   0.47986427
 0.47986576 0.47982106 0.47974363 0.4796291  0.47954684 0.47962564
 0.47978368 0.4799731  0.48013252 0.48022124 0.48018497 0.4800696
 0.47992468 0.47982562 0.47980285 0.47985503 0.4799396  0.48000056
 0.479997   0.48003337 0.48007488 0.4801936  0.48039103 0.48063347
 0.48085967 0.48100525 0.48108563 0.4810776  0.48098347 0.4807959
 0.4805462  0.48028436 0.47998348 0.47957096 0.47901133 0.4783999
 0.47774535 0.47710013 0.47646075 0.4758219  0.4751951  0.47454253
 0.47392994 0.47336996 0.47287837 0.47244635 0.47211772 0.47186717
 0.47168186 0.47152874 0.4714153  0.47132006 0.47121134 0.47105592
 0.47089353 0.4707104  0.4705631  0.47048885 0.4705118  0.47056308
 0.4706649  0.4707267  0.47072688 0.47068378 0.47062472 0.4706387
 0.4707555  0.47091925 0.47114345 0.47135547 0.471513   0.47158468
 0.47161722 0.47167876 0.4717266  0.47179353 0.47197425 0.47220886
 0.47232836 0.47227955 0.47203287 0.4716232  0.47124216 0.4710206
 0.4709668  0.47114864 0.47151187 0.47187865 0.47217172 0.4722556
 0.47207156 0.47173145 0.47140235 0.4711466  0.47110805 0.47124088
 0.47144735 0.47160015 0.47164053 0.4715519  0.47134972 0.47113886
 0.4709439  0.4707509  0.47061256 0.47052622 0.47046238 0.47040483
 0.47037113 0.4703841  0.4704428  0.47052482 0.470584   0.4705737
 0.47048804 0.4704105  0.47035426 0.47027212 0.47020695 0.47012684
 0.47001544 0.4698701  0.46970588 0.46956095 0.46941265 0.469241
 0.4690435  0.46886408 0.46867737 0.46842733 0.46809226 0.46771312
 0.46734947 0.46699986 0.4667397  0.4665111  0.46615794 0.46578625
 0.46527317 0.46475688 0.46425578 0.46387926 0.4635726  0.4632369
 0.46282253 0.46228534 0.46171916 0.46118328 0.46088293 0.4609343
 0.46126392 0.46180388 0.46232378 0.4626497  0.46279338 0.4627541
 0.46277645 0.4629899  0.4634652  0.46423614 0.4650546  0.4658326
 0.46646014 0.46691912 0.4672104  0.467317   0.46733966 0.46730867
 0.46749046 0.4680331  0.46867493 0.46906853 0.46803257 0.4630988 ]
