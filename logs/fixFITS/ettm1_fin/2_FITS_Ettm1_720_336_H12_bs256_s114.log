Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  58885120.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.5533287525177
Epoch: 1, Steps: 65 | Train Loss: 0.5551950 Vali Loss: 1.2101598 Test Loss: 0.7131490
Validation loss decreased (inf --> 1.210160).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.576119422912598
Epoch: 2, Steps: 65 | Train Loss: 0.4167130 Vali Loss: 1.0632008 Test Loss: 0.6210344
Validation loss decreased (1.210160 --> 1.063201).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.405187845230103
Epoch: 3, Steps: 65 | Train Loss: 0.3481713 Vali Loss: 0.9791006 Test Loss: 0.5688757
Validation loss decreased (1.063201 --> 0.979101).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.093506574630737
Epoch: 4, Steps: 65 | Train Loss: 0.3060405 Vali Loss: 0.9215253 Test Loss: 0.5349813
Validation loss decreased (0.979101 --> 0.921525).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.140616178512573
Epoch: 5, Steps: 65 | Train Loss: 0.2762648 Vali Loss: 0.8820010 Test Loss: 0.5093054
Validation loss decreased (0.921525 --> 0.882001).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.016190528869629
Epoch: 6, Steps: 65 | Train Loss: 0.2540390 Vali Loss: 0.8558034 Test Loss: 0.4916939
Validation loss decreased (0.882001 --> 0.855803).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.462509632110596
Epoch: 7, Steps: 65 | Train Loss: 0.2367289 Vali Loss: 0.8299014 Test Loss: 0.4758957
Validation loss decreased (0.855803 --> 0.829901).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.312105655670166
Epoch: 8, Steps: 65 | Train Loss: 0.2227587 Vali Loss: 0.8123854 Test Loss: 0.4637993
Validation loss decreased (0.829901 --> 0.812385).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.904064178466797
Epoch: 9, Steps: 65 | Train Loss: 0.2112775 Vali Loss: 0.7947590 Test Loss: 0.4533346
Validation loss decreased (0.812385 --> 0.794759).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.48106861114502
Epoch: 10, Steps: 65 | Train Loss: 0.2019380 Vali Loss: 0.7829536 Test Loss: 0.4454708
Validation loss decreased (0.794759 --> 0.782954).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.9301118850708
Epoch: 11, Steps: 65 | Train Loss: 0.1940830 Vali Loss: 0.7733593 Test Loss: 0.4378814
Validation loss decreased (0.782954 --> 0.773359).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.397894382476807
Epoch: 12, Steps: 65 | Train Loss: 0.1873898 Vali Loss: 0.7619737 Test Loss: 0.4319074
Validation loss decreased (0.773359 --> 0.761974).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.9645676612854
Epoch: 13, Steps: 65 | Train Loss: 0.1816857 Vali Loss: 0.7553436 Test Loss: 0.4263391
Validation loss decreased (0.761974 --> 0.755344).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.997236251831055
Epoch: 14, Steps: 65 | Train Loss: 0.1767625 Vali Loss: 0.7511919 Test Loss: 0.4220354
Validation loss decreased (0.755344 --> 0.751192).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.121930360794067
Epoch: 15, Steps: 65 | Train Loss: 0.1724707 Vali Loss: 0.7429613 Test Loss: 0.4175784
Validation loss decreased (0.751192 --> 0.742961).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.776742696762085
Epoch: 16, Steps: 65 | Train Loss: 0.1686844 Vali Loss: 0.7377333 Test Loss: 0.4146975
Validation loss decreased (0.742961 --> 0.737733).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.421296119689941
Epoch: 17, Steps: 65 | Train Loss: 0.1654703 Vali Loss: 0.7363414 Test Loss: 0.4114056
Validation loss decreased (0.737733 --> 0.736341).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.423728227615356
Epoch: 18, Steps: 65 | Train Loss: 0.1625267 Vali Loss: 0.7303523 Test Loss: 0.4082898
Validation loss decreased (0.736341 --> 0.730352).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.21130084991455
Epoch: 19, Steps: 65 | Train Loss: 0.1599398 Vali Loss: 0.7282273 Test Loss: 0.4063190
Validation loss decreased (0.730352 --> 0.728227).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 11.335997104644775
Epoch: 20, Steps: 65 | Train Loss: 0.1575195 Vali Loss: 0.7252315 Test Loss: 0.4037676
Validation loss decreased (0.728227 --> 0.725231).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.334259271621704
Epoch: 21, Steps: 65 | Train Loss: 0.1554773 Vali Loss: 0.7219871 Test Loss: 0.4018683
Validation loss decreased (0.725231 --> 0.721987).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 11.215882301330566
Epoch: 22, Steps: 65 | Train Loss: 0.1535650 Vali Loss: 0.7169007 Test Loss: 0.3998525
Validation loss decreased (0.721987 --> 0.716901).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.947599172592163
Epoch: 23, Steps: 65 | Train Loss: 0.1518567 Vali Loss: 0.7120244 Test Loss: 0.3981385
Validation loss decreased (0.716901 --> 0.712024).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 10.882436990737915
Epoch: 24, Steps: 65 | Train Loss: 0.1502386 Vali Loss: 0.7149804 Test Loss: 0.3964227
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.540550708770752
Epoch: 25, Steps: 65 | Train Loss: 0.1488673 Vali Loss: 0.7120889 Test Loss: 0.3949577
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 11.204050302505493
Epoch: 26, Steps: 65 | Train Loss: 0.1476011 Vali Loss: 0.7125118 Test Loss: 0.3937981
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  58885120.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.817713975906372
Epoch: 1, Steps: 65 | Train Loss: 0.3550426 Vali Loss: 0.6739656 Test Loss: 0.3700504
Validation loss decreased (inf --> 0.673966).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 12.07581090927124
Epoch: 2, Steps: 65 | Train Loss: 0.3419948 Vali Loss: 0.6646295 Test Loss: 0.3660203
Validation loss decreased (0.673966 --> 0.664629).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.903672933578491
Epoch: 3, Steps: 65 | Train Loss: 0.3386709 Vali Loss: 0.6605283 Test Loss: 0.3662657
Validation loss decreased (0.664629 --> 0.660528).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 12.70793867111206
Epoch: 4, Steps: 65 | Train Loss: 0.3378346 Vali Loss: 0.6592473 Test Loss: 0.3669200
Validation loss decreased (0.660528 --> 0.659247).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 12.002987146377563
Epoch: 5, Steps: 65 | Train Loss: 0.3372764 Vali Loss: 0.6593723 Test Loss: 0.3668824
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.484593868255615
Epoch: 6, Steps: 65 | Train Loss: 0.3372812 Vali Loss: 0.6580427 Test Loss: 0.3671227
Validation loss decreased (0.659247 --> 0.658043).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.556436777114868
Epoch: 7, Steps: 65 | Train Loss: 0.3370605 Vali Loss: 0.6557970 Test Loss: 0.3671216
Validation loss decreased (0.658043 --> 0.655797).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.554452657699585
Epoch: 8, Steps: 65 | Train Loss: 0.3368742 Vali Loss: 0.6545204 Test Loss: 0.3670919
Validation loss decreased (0.655797 --> 0.654520).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.977304458618164
Epoch: 9, Steps: 65 | Train Loss: 0.3367303 Vali Loss: 0.6561522 Test Loss: 0.3668633
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.551178455352783
Epoch: 10, Steps: 65 | Train Loss: 0.3368072 Vali Loss: 0.6573006 Test Loss: 0.3669564
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.137029886245728
Epoch: 11, Steps: 65 | Train Loss: 0.3366403 Vali Loss: 0.6542016 Test Loss: 0.3668900
Validation loss decreased (0.654520 --> 0.654202).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.306884050369263
Epoch: 12, Steps: 65 | Train Loss: 0.3364557 Vali Loss: 0.6571593 Test Loss: 0.3670883
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.480618715286255
Epoch: 13, Steps: 65 | Train Loss: 0.3363716 Vali Loss: 0.6554503 Test Loss: 0.3670921
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.251179933547974
Epoch: 14, Steps: 65 | Train Loss: 0.3363420 Vali Loss: 0.6544876 Test Loss: 0.3669545
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3663600981235504, mae:0.38510996103286743, rse:0.575973391532898, corr:[0.5378045  0.5479423  0.55407834 0.5554565  0.555281   0.55582744
 0.55730045 0.558922   0.5598813  0.5600561  0.5600335  0.5601546
 0.5604003  0.5603939  0.5597891  0.5586396  0.55733854 0.55622005
 0.55524635 0.554273   0.5531628  0.5517875  0.55026186 0.54895663
 0.5478665  0.5470011  0.5462543  0.545543   0.54510444 0.5451636
 0.5457879  0.54679775 0.5476703  0.54811877 0.5479807  0.54768026
 0.54743284 0.5474062  0.54746026 0.547264   0.54681283 0.5461946
 0.5457214  0.54567665 0.5459066  0.5461025  0.54609627 0.54580593
 0.54532206 0.54487324 0.5447367  0.5448602  0.5450657  0.5450892
 0.54494053 0.54473376 0.5445896  0.5444747  0.5443531  0.54406285
 0.5436243  0.54320973 0.54299814 0.5430036  0.5432472  0.5435766
 0.54379994 0.54385644 0.5438225  0.54377675 0.5437939  0.54381984
 0.5437369  0.54350704 0.54321516 0.5429057  0.54259914 0.54233354
 0.54210114 0.541893   0.5416937  0.5415073  0.5413772  0.5412992
 0.5412986  0.54129565 0.541243   0.5410977  0.54093814 0.5409299
 0.54112315 0.5414671  0.5418219  0.54202515 0.5419498  0.5415727
 0.5410366  0.5405652  0.5401702  0.5399253  0.5398197  0.53979695
 0.5398016  0.53973454 0.53963035 0.53953683 0.5393753  0.53920186
 0.5389913  0.53879833 0.53864133 0.5384786  0.5382746  0.53802747
 0.53773004 0.5374314  0.5372007  0.5370442  0.53695625 0.5369205
 0.53683114 0.5366174  0.53626347 0.53595185 0.5357891  0.5357133
 0.535633   0.5355118  0.53531533 0.5351528  0.53506917 0.5351035
 0.5352667  0.5354204  0.5353843  0.53516823 0.534933   0.53468287
 0.5345602  0.53461266 0.5347166  0.53484714 0.53501225 0.53522056
 0.5354354  0.5355144  0.5354658  0.5353181  0.53522074 0.535212
 0.5353076  0.5354986  0.53563726 0.5355963  0.5353335  0.5350042
 0.53474694 0.53463584 0.5347303  0.53486764 0.5350286  0.5351459
 0.53522545 0.5352868  0.53539205 0.53556204 0.5357783  0.53601766
 0.53619736 0.53625757 0.5362907  0.5363336  0.5363207  0.53624684
 0.53613055 0.53601    0.53591347 0.53589535 0.53595257 0.5360967
 0.53625333 0.53636074 0.5363224  0.5362159  0.5361314  0.53614736
 0.53631276 0.53664505 0.53704494 0.53733665 0.5374169  0.5372922
 0.53701305 0.5367464  0.5364802  0.5361709  0.53577733 0.5352923
 0.5346994  0.53403497 0.5333227  0.5326104  0.5318905  0.53115815
 0.5304274  0.52972543 0.5290927  0.5286074  0.52825075 0.5279384
 0.52753836 0.52703255 0.5264188  0.52574646 0.5250083  0.52433074
 0.52378887 0.5233297  0.5229842  0.52274454 0.52266014 0.52280766
 0.52321297 0.52365315 0.5240304  0.52425504 0.5242641  0.5241242
 0.5239619  0.5238894  0.5239437  0.52404875 0.52415556 0.52423865
 0.5242641  0.52430505 0.52429837 0.52443844 0.5246773  0.5249902
 0.52522796 0.5251909  0.5249355  0.52464026 0.5245011  0.5245127
 0.5246063  0.5247018  0.52467316 0.52453274 0.52423805 0.5239496
 0.5236711  0.52354616 0.523515   0.5235178  0.523543   0.52358365
 0.5236289  0.5236841  0.52370167 0.52367854 0.5236526  0.52373624
 0.52384454 0.5239153  0.5239114  0.52387327 0.5238259  0.5237622
 0.5237458  0.52374136 0.52374023 0.52375793 0.5237817  0.5238445
 0.5239106  0.52401423 0.5240466  0.5240562  0.5240075  0.52394104
 0.52395743 0.52408206 0.5242229  0.5243127  0.5242399  0.52390355
 0.5233635  0.5228161  0.5223285  0.5218154  0.5213069  0.5208202
 0.5203928  0.5199348  0.5194947  0.5189755  0.51834327 0.51764506
 0.51694185 0.51638067 0.5160642  0.5159094  0.5157553  0.5154363
 0.5150637  0.51463455 0.5142634  0.5140125  0.51390207 0.5137585
 0.51353246 0.51333576 0.5131699  0.51310444 0.5131394  0.5132698
 0.5134171  0.5134374  0.5133274  0.51316494 0.5130162  0.5128963
 0.5127837  0.5128049  0.51290005 0.51302665 0.513052   0.5129424
 0.51292175 0.5131574  0.51368994 0.5141745  0.51310813 0.5071127 ]
