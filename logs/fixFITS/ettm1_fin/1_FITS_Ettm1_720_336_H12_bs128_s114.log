Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  29442560.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3950753
	speed: 0.2061s/iter; left time: 1318.9646s
Epoch: 1 cost time: 27.060438632965088
Epoch: 1, Steps: 130 | Train Loss: 0.4647890 Vali Loss: 0.7791585 Test Loss: 0.4006237
Validation loss decreased (inf --> 0.779158).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3610477
	speed: 0.5983s/iter; left time: 3751.6845s
Epoch: 2 cost time: 28.892483711242676
Epoch: 2, Steps: 130 | Train Loss: 0.3621989 Vali Loss: 0.7157113 Test Loss: 0.3713518
Validation loss decreased (0.779158 --> 0.715711).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3336476
	speed: 0.5335s/iter; left time: 3276.3362s
Epoch: 3 cost time: 24.385908603668213
Epoch: 3, Steps: 130 | Train Loss: 0.3479316 Vali Loss: 0.6915974 Test Loss: 0.3658501
Validation loss decreased (0.715711 --> 0.691597).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3454404
	speed: 0.4878s/iter; left time: 2932.2349s
Epoch: 4 cost time: 22.17068338394165
Epoch: 4, Steps: 130 | Train Loss: 0.3430449 Vali Loss: 0.6818200 Test Loss: 0.3649060
Validation loss decreased (0.691597 --> 0.681820).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3452051
	speed: 0.4339s/iter; left time: 2551.6309s
Epoch: 5 cost time: 19.78257942199707
Epoch: 5, Steps: 130 | Train Loss: 0.3407375 Vali Loss: 0.6743343 Test Loss: 0.3650187
Validation loss decreased (0.681820 --> 0.674334).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3253188
	speed: 0.4375s/iter; left time: 2515.9169s
Epoch: 6 cost time: 22.913811683654785
Epoch: 6, Steps: 130 | Train Loss: 0.3396736 Vali Loss: 0.6700950 Test Loss: 0.3656997
Validation loss decreased (0.674334 --> 0.670095).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3516392
	speed: 0.4594s/iter; left time: 2582.0689s
Epoch: 7 cost time: 22.09630846977234
Epoch: 7, Steps: 130 | Train Loss: 0.3387543 Vali Loss: 0.6678607 Test Loss: 0.3660332
Validation loss decreased (0.670095 --> 0.667861).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3301662
	speed: 0.4655s/iter; left time: 2556.0019s
Epoch: 8 cost time: 23.391443014144897
Epoch: 8, Steps: 130 | Train Loss: 0.3382906 Vali Loss: 0.6650149 Test Loss: 0.3661704
Validation loss decreased (0.667861 --> 0.665015).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3499924
	speed: 0.4517s/iter; left time: 2421.3792s
Epoch: 9 cost time: 20.35684871673584
Epoch: 9, Steps: 130 | Train Loss: 0.3380038 Vali Loss: 0.6640702 Test Loss: 0.3660947
Validation loss decreased (0.665015 --> 0.664070).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3470212
	speed: 0.4042s/iter; left time: 2114.3925s
Epoch: 10 cost time: 21.447306871414185
Epoch: 10, Steps: 130 | Train Loss: 0.3377367 Vali Loss: 0.6628630 Test Loss: 0.3663795
Validation loss decreased (0.664070 --> 0.662863).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3350839
	speed: 0.4567s/iter; left time: 2329.8016s
Epoch: 11 cost time: 21.804519414901733
Epoch: 11, Steps: 130 | Train Loss: 0.3374387 Vali Loss: 0.6621538 Test Loss: 0.3663376
Validation loss decreased (0.662863 --> 0.662154).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3461179
	speed: 0.4140s/iter; left time: 2058.0800s
Epoch: 12 cost time: 19.05442523956299
Epoch: 12, Steps: 130 | Train Loss: 0.3372985 Vali Loss: 0.6605268 Test Loss: 0.3660234
Validation loss decreased (0.662154 --> 0.660527).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3500364
	speed: 0.3584s/iter; left time: 1734.8273s
Epoch: 13 cost time: 14.210716724395752
Epoch: 13, Steps: 130 | Train Loss: 0.3373119 Vali Loss: 0.6614698 Test Loss: 0.3662822
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3376606
	speed: 0.3313s/iter; left time: 1560.9266s
Epoch: 14 cost time: 16.52204132080078
Epoch: 14, Steps: 130 | Train Loss: 0.3370587 Vali Loss: 0.6605056 Test Loss: 0.3664980
Validation loss decreased (0.660527 --> 0.660506).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3469692
	speed: 0.3632s/iter; left time: 1663.8503s
Epoch: 15 cost time: 18.46708083152771
Epoch: 15, Steps: 130 | Train Loss: 0.3369246 Vali Loss: 0.6596080 Test Loss: 0.3662814
Validation loss decreased (0.660506 --> 0.659608).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3481287
	speed: 0.4526s/iter; left time: 2014.3161s
Epoch: 16 cost time: 21.586825847625732
Epoch: 16, Steps: 130 | Train Loss: 0.3368502 Vali Loss: 0.6585894 Test Loss: 0.3666172
Validation loss decreased (0.659608 --> 0.658589).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3316195
	speed: 0.4450s/iter; left time: 1922.9477s
Epoch: 17 cost time: 23.60738778114319
Epoch: 17, Steps: 130 | Train Loss: 0.3367887 Vali Loss: 0.6570010 Test Loss: 0.3665221
Validation loss decreased (0.658589 --> 0.657001).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3019435
	speed: 0.4670s/iter; left time: 1957.0812s
Epoch: 18 cost time: 22.068086624145508
Epoch: 18, Steps: 130 | Train Loss: 0.3367083 Vali Loss: 0.6573260 Test Loss: 0.3664436
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3498637
	speed: 0.4385s/iter; left time: 1780.9185s
Epoch: 19 cost time: 21.31705594062805
Epoch: 19, Steps: 130 | Train Loss: 0.3365084 Vali Loss: 0.6574321 Test Loss: 0.3665447
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3287386
	speed: 0.4652s/iter; left time: 1828.6708s
Epoch: 20 cost time: 21.49869728088379
Epoch: 20, Steps: 130 | Train Loss: 0.3366005 Vali Loss: 0.6568914 Test Loss: 0.3665616
Validation loss decreased (0.657001 --> 0.656891).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3212578
	speed: 0.4429s/iter; left time: 1683.4333s
Epoch: 21 cost time: 21.490302085876465
Epoch: 21, Steps: 130 | Train Loss: 0.3364551 Vali Loss: 0.6583315 Test Loss: 0.3665687
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3251680
	speed: 0.4315s/iter; left time: 1583.9835s
Epoch: 22 cost time: 20.31519365310669
Epoch: 22, Steps: 130 | Train Loss: 0.3365790 Vali Loss: 0.6585201 Test Loss: 0.3664877
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3223179
	speed: 0.3754s/iter; left time: 1329.1598s
Epoch: 23 cost time: 16.386370182037354
Epoch: 23, Steps: 130 | Train Loss: 0.3363151 Vali Loss: 0.6571345 Test Loss: 0.3664064
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.36635321378707886, mae:0.3848212659358978, rse:0.5759679675102234, corr:[0.5363181  0.5457554  0.5511682  0.55270153 0.553156   0.55409807
 0.55548257 0.5567835  0.55766875 0.5581779  0.5586365  0.55900264
 0.5591312  0.5588663  0.55821747 0.5573838  0.5565811  0.55580205
 0.5547802  0.5534296  0.55190074 0.5503439  0.5489404  0.54790515
 0.5469805  0.5460379  0.5450121  0.54399973 0.5434517  0.54364586
 0.54451656 0.5457218  0.54666257 0.5471071  0.54700476 0.54684746
 0.5467868  0.54686815 0.5468953  0.5465781  0.5460265  0.5454075
 0.54502285 0.5450623  0.5452708  0.5453397  0.54519427 0.5448687
 0.54449916 0.5442428  0.54424125 0.54435104 0.5444051  0.544233
 0.5439444  0.54371876 0.5436623  0.54368585 0.54371315 0.54356223
 0.5432637  0.54298884 0.54287094 0.5428498  0.5429284  0.5430019
 0.5429614  0.5428275  0.5427293  0.5427322  0.5428434  0.5429573
 0.54296845 0.54288256 0.5428009  0.5427532  0.5427201  0.5426881
 0.54260576 0.54245454 0.5422344  0.5419592  0.541691   0.541451
 0.5412957  0.54118454 0.541095   0.5409866  0.54091793 0.54101163
 0.5413049  0.54176027 0.5422656  0.5426757  0.5428355  0.54266334
 0.5422271  0.54170823 0.54113615 0.5406456  0.54029506 0.5400651
 0.5399044  0.5397064  0.53950256 0.5393526  0.53919655 0.53908145
 0.53894746 0.5387931  0.5386132  0.53839475 0.5381739  0.5380058
 0.53787786 0.53776085 0.5375982  0.53730255 0.5368756  0.53643525
 0.5360593  0.5358105  0.53567284 0.53567296 0.53571385 0.5356286
 0.535384   0.5351076  0.534929   0.53497803 0.5351556  0.53529245
 0.53528035 0.53504395 0.5346191  0.5342454  0.5341825  0.5343568
 0.53466403 0.53492117 0.5349234  0.53473854 0.5345864  0.5346697
 0.53501445 0.53539526 0.5356571  0.53569835 0.5356051  0.5354703
 0.5354053  0.5354904  0.5356235  0.5356759  0.53556913 0.5354066
 0.53526807 0.53520733 0.5352893  0.5353818  0.53547996 0.53553826
 0.53559995 0.53571355 0.535943   0.53627247 0.5366299  0.5369339
 0.5370671  0.536985   0.5368375  0.536735   0.53667164 0.53664815
 0.5366378  0.5366099  0.5365509  0.53649896 0.53646594 0.5364896
 0.5365339  0.53656286 0.5365119  0.5364633  0.5365039  0.53669167
 0.537042   0.5375115  0.5379599  0.538207   0.5381911  0.5379617
 0.5376127  0.5373092  0.5370229  0.5366741  0.5361804  0.535525
 0.5347205  0.53387666 0.5331071  0.5325139  0.5320591  0.53163695
 0.5311591  0.53057975 0.52993375 0.5293425  0.52885836 0.52845
 0.5280022  0.5274608  0.5267895  0.5260354  0.52523243 0.52456635
 0.5241462  0.5239026  0.5237838  0.52365613 0.5234885  0.52336204
 0.52339673 0.5235135  0.5237197  0.5239652  0.5241352  0.52420896
 0.5242309  0.5242668  0.52435297 0.5244427  0.524527   0.5246064
 0.52465034 0.5247122  0.5247154  0.5248405  0.5250531  0.5253462
 0.52558595 0.525593   0.5254075  0.52516574 0.52501464 0.5249515
 0.5249422  0.52493876 0.52485096 0.52470344 0.5244736  0.5242987
 0.5241711  0.5241879  0.52425116 0.52426845 0.5242231  0.5241412
 0.52406514 0.52406454 0.52413327 0.5242475  0.52437824 0.5245628
 0.5246848  0.52470773 0.52465385 0.52461463 0.52462345 0.5246407
 0.5246837  0.52470315 0.5246836  0.52464443 0.5245828  0.5245353
 0.52448434 0.5245094  0.52454734 0.5246611  0.524791   0.5249045
 0.5250184  0.52509785 0.5250735  0.5249534  0.5247239  0.52436143
 0.5239368  0.523571   0.52321446 0.52270347 0.5220474  0.52132255
 0.5206793  0.5201485  0.51981443 0.5195269  0.51915276 0.5186251
 0.5179508  0.5172827  0.5167988  0.51650846 0.51630014 0.5160004
 0.51567715 0.51528776 0.5149161  0.5146303  0.51450384 0.5144181
 0.51430196 0.5141927  0.5140629  0.5139727  0.51393914 0.51395714
 0.51391774 0.51368964 0.51335275 0.5130765  0.51296747 0.5129934
 0.5129962  0.5129151  0.5126717  0.5124262  0.5123032  0.5123569
 0.51261044 0.51287663 0.5131235  0.5135063  0.51335645 0.51036865]
