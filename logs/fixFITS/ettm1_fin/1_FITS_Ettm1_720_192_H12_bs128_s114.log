Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=106, out_features=134, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  25453568.0
params:  14338.0
Trainable parameters:  14338
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3600454
	speed: 0.1404s/iter; left time: 905.9185s
Epoch: 1 cost time: 18.22766375541687
Epoch: 1, Steps: 131 | Train Loss: 0.4362822 Vali Loss: 0.6285880 Test Loss: 0.3741272
Validation loss decreased (inf --> 0.628588).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3331667
	speed: 0.4190s/iter; left time: 2648.0197s
Epoch: 2 cost time: 20.081480264663696
Epoch: 2, Steps: 131 | Train Loss: 0.3226029 Vali Loss: 0.5632611 Test Loss: 0.3448404
Validation loss decreased (0.628588 --> 0.563261).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3102773
	speed: 0.4079s/iter; left time: 2524.6621s
Epoch: 3 cost time: 18.34329390525818
Epoch: 3, Steps: 131 | Train Loss: 0.3084603 Vali Loss: 0.5432796 Test Loss: 0.3406579
Validation loss decreased (0.563261 --> 0.543280).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3088546
	speed: 0.3980s/iter; left time: 2411.2348s
Epoch: 4 cost time: 18.874353885650635
Epoch: 4, Steps: 131 | Train Loss: 0.3040504 Vali Loss: 0.5355476 Test Loss: 0.3394853
Validation loss decreased (0.543280 --> 0.535548).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3203940
	speed: 0.3831s/iter; left time: 2270.6226s
Epoch: 5 cost time: 17.848625659942627
Epoch: 5, Steps: 131 | Train Loss: 0.3019745 Vali Loss: 0.5287614 Test Loss: 0.3391353
Validation loss decreased (0.535548 --> 0.528761).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3029347
	speed: 0.3727s/iter; left time: 2159.9018s
Epoch: 6 cost time: 18.2721688747406
Epoch: 6, Steps: 131 | Train Loss: 0.3007691 Vali Loss: 0.5252693 Test Loss: 0.3389487
Validation loss decreased (0.528761 --> 0.525269).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2988314
	speed: 0.3381s/iter; left time: 1915.4730s
Epoch: 7 cost time: 15.376045942306519
Epoch: 7, Steps: 131 | Train Loss: 0.3000341 Vali Loss: 0.5238152 Test Loss: 0.3385144
Validation loss decreased (0.525269 --> 0.523815).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2888980
	speed: 0.3375s/iter; left time: 1867.4850s
Epoch: 8 cost time: 16.353903770446777
Epoch: 8, Steps: 131 | Train Loss: 0.2994795 Vali Loss: 0.5220589 Test Loss: 0.3386239
Validation loss decreased (0.523815 --> 0.522059).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2807004
	speed: 0.3329s/iter; left time: 1798.5327s
Epoch: 9 cost time: 15.951563119888306
Epoch: 9, Steps: 131 | Train Loss: 0.2989359 Vali Loss: 0.5194582 Test Loss: 0.3383728
Validation loss decreased (0.522059 --> 0.519458).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3121304
	speed: 0.3378s/iter; left time: 1780.7507s
Epoch: 10 cost time: 17.705771446228027
Epoch: 10, Steps: 131 | Train Loss: 0.2989386 Vali Loss: 0.5190518 Test Loss: 0.3381823
Validation loss decreased (0.519458 --> 0.519052).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3017839
	speed: 0.3776s/iter; left time: 1941.1810s
Epoch: 11 cost time: 17.52803874015808
Epoch: 11, Steps: 131 | Train Loss: 0.2987051 Vali Loss: 0.5182154 Test Loss: 0.3386847
Validation loss decreased (0.519052 --> 0.518215).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2943335
	speed: 0.3587s/iter; left time: 1797.0941s
Epoch: 12 cost time: 16.44235634803772
Epoch: 12, Steps: 131 | Train Loss: 0.2984626 Vali Loss: 0.5170831 Test Loss: 0.3384679
Validation loss decreased (0.518215 --> 0.517083).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3019559
	speed: 0.3279s/iter; left time: 1600.0172s
Epoch: 13 cost time: 13.493731021881104
Epoch: 13, Steps: 131 | Train Loss: 0.2982910 Vali Loss: 0.5165097 Test Loss: 0.3383198
Validation loss decreased (0.517083 --> 0.516510).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2957855
	speed: 0.4176s/iter; left time: 1982.9507s
Epoch: 14 cost time: 22.2225341796875
Epoch: 14, Steps: 131 | Train Loss: 0.2982431 Vali Loss: 0.5154178 Test Loss: 0.3383646
Validation loss decreased (0.516510 --> 0.515418).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2981281
	speed: 0.4845s/iter; left time: 2237.0525s
Epoch: 15 cost time: 22.807405948638916
Epoch: 15, Steps: 131 | Train Loss: 0.2980154 Vali Loss: 0.5158684 Test Loss: 0.3381791
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3001137
	speed: 0.4806s/iter; left time: 2155.9571s
Epoch: 16 cost time: 21.627039670944214
Epoch: 16, Steps: 131 | Train Loss: 0.2981025 Vali Loss: 0.5153587 Test Loss: 0.3383864
Validation loss decreased (0.515418 --> 0.515359).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2841990
	speed: 0.4446s/iter; left time: 1936.2354s
Epoch: 17 cost time: 20.76405620574951
Epoch: 17, Steps: 131 | Train Loss: 0.2979624 Vali Loss: 0.5154330 Test Loss: 0.3381991
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2972662
	speed: 0.4191s/iter; left time: 1770.0888s
Epoch: 18 cost time: 19.630433797836304
Epoch: 18, Steps: 131 | Train Loss: 0.2979279 Vali Loss: 0.5139560 Test Loss: 0.3384974
Validation loss decreased (0.515359 --> 0.513956).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2829027
	speed: 0.4440s/iter; left time: 1817.3852s
Epoch: 19 cost time: 21.01550030708313
Epoch: 19, Steps: 131 | Train Loss: 0.2977984 Vali Loss: 0.5149176 Test Loss: 0.3379029
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2973652
	speed: 0.4481s/iter; left time: 1775.4985s
Epoch: 20 cost time: 21.493180751800537
Epoch: 20, Steps: 131 | Train Loss: 0.2978000 Vali Loss: 0.5146235 Test Loss: 0.3381267
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2938638
	speed: 0.4580s/iter; left time: 1754.7079s
Epoch: 21 cost time: 22.535587787628174
Epoch: 21, Steps: 131 | Train Loss: 0.2976808 Vali Loss: 0.5136635 Test Loss: 0.3381517
Validation loss decreased (0.513956 --> 0.513664).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3055602
	speed: 0.4822s/iter; left time: 1784.0998s
Epoch: 22 cost time: 22.447015285491943
Epoch: 22, Steps: 131 | Train Loss: 0.2977268 Vali Loss: 0.5133786 Test Loss: 0.3383784
Validation loss decreased (0.513664 --> 0.513379).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3039702
	speed: 0.4794s/iter; left time: 1710.9408s
Epoch: 23 cost time: 22.99592137336731
Epoch: 23, Steps: 131 | Train Loss: 0.2976646 Vali Loss: 0.5137348 Test Loss: 0.3383335
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3064694
	speed: 0.4817s/iter; left time: 1656.1229s
Epoch: 24 cost time: 23.561569929122925
Epoch: 24, Steps: 131 | Train Loss: 0.2975950 Vali Loss: 0.5132120 Test Loss: 0.3381391
Validation loss decreased (0.513379 --> 0.513212).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3049752
	speed: 0.4710s/iter; left time: 1557.5119s
Epoch: 25 cost time: 22.428405284881592
Epoch: 25, Steps: 131 | Train Loss: 0.2975244 Vali Loss: 0.5142526 Test Loss: 0.3381456
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2886917
	speed: 0.4600s/iter; left time: 1460.9230s
Epoch: 26 cost time: 21.52607274055481
Epoch: 26, Steps: 131 | Train Loss: 0.2976472 Vali Loss: 0.5132030 Test Loss: 0.3381429
Validation loss decreased (0.513212 --> 0.513203).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2973472
	speed: 0.4521s/iter; left time: 1376.6308s
Epoch: 27 cost time: 22.01056718826294
Epoch: 27, Steps: 131 | Train Loss: 0.2975287 Vali Loss: 0.5136152 Test Loss: 0.3381644
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3087170
	speed: 0.4582s/iter; left time: 1335.2592s
Epoch: 28 cost time: 22.384632349014282
Epoch: 28, Steps: 131 | Train Loss: 0.2974884 Vali Loss: 0.5125045 Test Loss: 0.3381926
Validation loss decreased (0.513203 --> 0.512504).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2902054
	speed: 0.4676s/iter; left time: 1301.2252s
Epoch: 29 cost time: 22.819175004959106
Epoch: 29, Steps: 131 | Train Loss: 0.2972892 Vali Loss: 0.5132382 Test Loss: 0.3380561
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2936556
	speed: 0.4823s/iter; left time: 1279.1541s
Epoch: 30 cost time: 21.66375422477722
Epoch: 30, Steps: 131 | Train Loss: 0.2973625 Vali Loss: 0.5130769 Test Loss: 0.3381467
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2781728
	speed: 0.4483s/iter; left time: 1130.2815s
Epoch: 31 cost time: 20.8029944896698
Epoch: 31, Steps: 131 | Train Loss: 0.2973567 Vali Loss: 0.5122182 Test Loss: 0.3382458
Validation loss decreased (0.512504 --> 0.512218).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3113434
	speed: 0.4469s/iter; left time: 1068.0551s
Epoch: 32 cost time: 22.175798416137695
Epoch: 32, Steps: 131 | Train Loss: 0.2973880 Vali Loss: 0.5129421 Test Loss: 0.3384179
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2812432
	speed: 0.4428s/iter; left time: 1000.3433s
Epoch: 33 cost time: 20.651095390319824
Epoch: 33, Steps: 131 | Train Loss: 0.2973758 Vali Loss: 0.5127572 Test Loss: 0.3382817
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2831959
	speed: 0.4216s/iter; left time: 897.1662s
Epoch: 34 cost time: 19.61016607284546
Epoch: 34, Steps: 131 | Train Loss: 0.2971987 Vali Loss: 0.5127679 Test Loss: 0.3380976
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.3382168710231781, mae:0.3683535158634186, rse:0.5536038875579834, corr:[0.54177976 0.55071586 0.55538195 0.5568308  0.5576957  0.5592245
 0.56105256 0.56243503 0.5631248  0.56344104 0.5638983  0.5644925
 0.56499356 0.5650568  0.5645538  0.5636251  0.56253844 0.56145036
 0.5602817  0.55903924 0.55781645 0.5565962  0.5553771  0.5543034
 0.5531515  0.5519518  0.55075717 0.5496781  0.54911447 0.5492376
 0.54994756 0.5509572  0.551751   0.5521604  0.5521041  0.5520034
 0.55189687 0.55183274 0.55172193 0.55138755 0.5510074  0.55065304
 0.55042905 0.55038697 0.55028605 0.5499965  0.5496821  0.5494928
 0.5494539  0.5494939  0.549581   0.549559   0.54939884 0.54908097
 0.5488134  0.548757   0.54889363 0.5489705  0.54891026 0.5485815
 0.5481519  0.5478713  0.5478878  0.54805857 0.548332   0.54857725
 0.548699   0.5487226  0.54878044 0.54889494 0.5490536  0.54911816
 0.5489902  0.5487017  0.5484275  0.5482277  0.54811513 0.5480652
 0.5480144  0.5479175  0.5477613  0.54754233 0.54733366 0.54712504
 0.5469683  0.54682606 0.5467105  0.54659307 0.5465272  0.5466259
 0.54686564 0.5471558  0.54739594 0.54750955 0.5474493  0.54721934
 0.5469259  0.54670036 0.5464321  0.5461091  0.5457362  0.545368
 0.5450818  0.544859   0.5447668  0.54482424 0.5448498  0.54479885
 0.5445916  0.5442874  0.54396033 0.54365635 0.5434199  0.54327613
 0.54317266 0.5430677  0.5429224  0.5426638  0.54231733 0.5419849
 0.5417081  0.54150456 0.5413314  0.54122823 0.5411535  0.54098165
 0.5407068  0.5404596  0.540318   0.5403604  0.54045814 0.54048467
 0.5404371  0.54033524 0.5401821  0.5401118  0.5402466  0.5403826
 0.5404738  0.5404955  0.5403969  0.540333   0.5404477  0.54076236
 0.5411377  0.5413131  0.5412553  0.5410284  0.54084647 0.540781
 0.54082423 0.5409799  0.54109734 0.54105675 0.5408283  0.5405867
 0.5404188  0.5403024  0.5402761  0.54020035 0.5401643  0.540164
 0.54022646 0.5403009  0.54037946 0.5404629  0.54058367 0.5407482
 0.540878   0.54087377 0.5408298  0.54081064 0.54072773 0.54058987
 0.54040915 0.5401941  0.53992665 0.5396607  0.5394065  0.5392928
 0.5392918  0.53933054 0.53923875 0.53909636 0.5390178  0.5391258
 0.5393833  0.5398809  0.54058814 0.54128766 0.54131186 0.5395221 ]
