Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_96_FITS_ETTm1_ftM_sl720_ll48_pl96_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 11425
test 11425
Model(
  (freq_upsampler): Linear(in_features=90, out_features=102, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32901120.0
params:  9282.0
Trainable parameters:  9282
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.794393301010132
Epoch: 1, Steps: 65 | Train Loss: 0.5026646 Vali Loss: 0.8823798 Test Loss: 0.5886599
Validation loss decreased (inf --> 0.882380).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.748944759368896
Epoch: 2, Steps: 65 | Train Loss: 0.3760549 Vali Loss: 0.7506645 Test Loss: 0.5177078
Validation loss decreased (0.882380 --> 0.750664).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.212121486663818
Epoch: 3, Steps: 65 | Train Loss: 0.3086118 Vali Loss: 0.7064966 Test Loss: 0.5002044
Validation loss decreased (0.750664 --> 0.706497).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.52757716178894
Epoch: 4, Steps: 65 | Train Loss: 0.2667758 Vali Loss: 0.6819085 Test Loss: 0.4953799
Validation loss decreased (0.706497 --> 0.681909).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.659480094909668
Epoch: 5, Steps: 65 | Train Loss: 0.2379867 Vali Loss: 0.6682303 Test Loss: 0.4921693
Validation loss decreased (0.681909 --> 0.668230).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.498201131820679
Epoch: 6, Steps: 65 | Train Loss: 0.2168185 Vali Loss: 0.6572317 Test Loss: 0.4897230
Validation loss decreased (0.668230 --> 0.657232).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.172807931900024
Epoch: 7, Steps: 65 | Train Loss: 0.2001287 Vali Loss: 0.6452516 Test Loss: 0.4857986
Validation loss decreased (0.657232 --> 0.645252).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.9105224609375
Epoch: 8, Steps: 65 | Train Loss: 0.1868987 Vali Loss: 0.6354306 Test Loss: 0.4819372
Validation loss decreased (0.645252 --> 0.635431).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.231762409210205
Epoch: 9, Steps: 65 | Train Loss: 0.1758860 Vali Loss: 0.6292007 Test Loss: 0.4786774
Validation loss decreased (0.635431 --> 0.629201).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.00593376159668
Epoch: 10, Steps: 65 | Train Loss: 0.1665121 Vali Loss: 0.6191965 Test Loss: 0.4750255
Validation loss decreased (0.629201 --> 0.619196).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.720216035842896
Epoch: 11, Steps: 65 | Train Loss: 0.1585109 Vali Loss: 0.6124341 Test Loss: 0.4713885
Validation loss decreased (0.619196 --> 0.612434).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.916290521621704
Epoch: 12, Steps: 65 | Train Loss: 0.1513756 Vali Loss: 0.6057377 Test Loss: 0.4679186
Validation loss decreased (0.612434 --> 0.605738).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.176244020462036
Epoch: 13, Steps: 65 | Train Loss: 0.1450982 Vali Loss: 0.5996002 Test Loss: 0.4644744
Validation loss decreased (0.605738 --> 0.599600).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.94882321357727
Epoch: 14, Steps: 65 | Train Loss: 0.1394680 Vali Loss: 0.5944640 Test Loss: 0.4615334
Validation loss decreased (0.599600 --> 0.594464).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.096381664276123
Epoch: 15, Steps: 65 | Train Loss: 0.1344757 Vali Loss: 0.5876882 Test Loss: 0.4578659
Validation loss decreased (0.594464 --> 0.587688).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.481619834899902
Epoch: 16, Steps: 65 | Train Loss: 0.1299733 Vali Loss: 0.5825624 Test Loss: 0.4548468
Validation loss decreased (0.587688 --> 0.582562).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.060187578201294
Epoch: 17, Steps: 65 | Train Loss: 0.1258581 Vali Loss: 0.5775335 Test Loss: 0.4521753
Validation loss decreased (0.582562 --> 0.577533).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.59565544128418
Epoch: 18, Steps: 65 | Train Loss: 0.1221946 Vali Loss: 0.5739124 Test Loss: 0.4500314
Validation loss decreased (0.577533 --> 0.573912).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 12.057347297668457
Epoch: 19, Steps: 65 | Train Loss: 0.1187953 Vali Loss: 0.5696674 Test Loss: 0.4477698
Validation loss decreased (0.573912 --> 0.569667).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 11.105970621109009
Epoch: 20, Steps: 65 | Train Loss: 0.1157260 Vali Loss: 0.5657144 Test Loss: 0.4457685
Validation loss decreased (0.569667 --> 0.565714).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.459856510162354
Epoch: 21, Steps: 65 | Train Loss: 0.1129115 Vali Loss: 0.5627423 Test Loss: 0.4432687
Validation loss decreased (0.565714 --> 0.562742).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 11.457198143005371
Epoch: 22, Steps: 65 | Train Loss: 0.1102927 Vali Loss: 0.5591351 Test Loss: 0.4410578
Validation loss decreased (0.562742 --> 0.559135).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.898908376693726
Epoch: 23, Steps: 65 | Train Loss: 0.1079683 Vali Loss: 0.5560260 Test Loss: 0.4394686
Validation loss decreased (0.559135 --> 0.556026).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 10.29356837272644
Epoch: 24, Steps: 65 | Train Loss: 0.1057947 Vali Loss: 0.5537061 Test Loss: 0.4377455
Validation loss decreased (0.556026 --> 0.553706).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.493794202804565
Epoch: 25, Steps: 65 | Train Loss: 0.1037810 Vali Loss: 0.5513433 Test Loss: 0.4359620
Validation loss decreased (0.553706 --> 0.551343).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.70829725265503
Epoch: 26, Steps: 65 | Train Loss: 0.1018375 Vali Loss: 0.5488455 Test Loss: 0.4342818
Validation loss decreased (0.551343 --> 0.548846).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.769373178482056
Epoch: 27, Steps: 65 | Train Loss: 0.1001539 Vali Loss: 0.5460737 Test Loss: 0.4328647
Validation loss decreased (0.548846 --> 0.546074).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.67558765411377
Epoch: 28, Steps: 65 | Train Loss: 0.0985016 Vali Loss: 0.5438910 Test Loss: 0.4314055
Validation loss decreased (0.546074 --> 0.543891).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 10.024918794631958
Epoch: 29, Steps: 65 | Train Loss: 0.0969909 Vali Loss: 0.5424500 Test Loss: 0.4301963
Validation loss decreased (0.543891 --> 0.542450).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.478720903396606
Epoch: 30, Steps: 65 | Train Loss: 0.0956045 Vali Loss: 0.5413752 Test Loss: 0.4290884
Validation loss decreased (0.542450 --> 0.541375).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 9.485147953033447
Epoch: 31, Steps: 65 | Train Loss: 0.0943235 Vali Loss: 0.5389234 Test Loss: 0.4274953
Validation loss decreased (0.541375 --> 0.538923).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 10.431061029434204
Epoch: 32, Steps: 65 | Train Loss: 0.0931239 Vali Loss: 0.5362427 Test Loss: 0.4265137
Validation loss decreased (0.538923 --> 0.536243).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 10.730521202087402
Epoch: 33, Steps: 65 | Train Loss: 0.0920416 Vali Loss: 0.5345336 Test Loss: 0.4253466
Validation loss decreased (0.536243 --> 0.534534).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 10.407851934432983
Epoch: 34, Steps: 65 | Train Loss: 0.0909417 Vali Loss: 0.5336984 Test Loss: 0.4244083
Validation loss decreased (0.534534 --> 0.533698).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 10.928239107131958
Epoch: 35, Steps: 65 | Train Loss: 0.0899451 Vali Loss: 0.5314779 Test Loss: 0.4233147
Validation loss decreased (0.533698 --> 0.531478).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 11.24823260307312
Epoch: 36, Steps: 65 | Train Loss: 0.0890002 Vali Loss: 0.5304642 Test Loss: 0.4223865
Validation loss decreased (0.531478 --> 0.530464).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 11.43512487411499
Epoch: 37, Steps: 65 | Train Loss: 0.0882011 Vali Loss: 0.5293958 Test Loss: 0.4214210
Validation loss decreased (0.530464 --> 0.529396).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 11.348376035690308
Epoch: 38, Steps: 65 | Train Loss: 0.0873532 Vali Loss: 0.5283701 Test Loss: 0.4206362
Validation loss decreased (0.529396 --> 0.528370).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 11.018559455871582
Epoch: 39, Steps: 65 | Train Loss: 0.0865905 Vali Loss: 0.5273618 Test Loss: 0.4197250
Validation loss decreased (0.528370 --> 0.527362).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 10.816241025924683
Epoch: 40, Steps: 65 | Train Loss: 0.0858653 Vali Loss: 0.5257748 Test Loss: 0.4189083
Validation loss decreased (0.527362 --> 0.525775).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 10.135037899017334
Epoch: 41, Steps: 65 | Train Loss: 0.0851744 Vali Loss: 0.5245920 Test Loss: 0.4180274
Validation loss decreased (0.525775 --> 0.524592).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 10.911362886428833
Epoch: 42, Steps: 65 | Train Loss: 0.0845138 Vali Loss: 0.5238631 Test Loss: 0.4171817
Validation loss decreased (0.524592 --> 0.523863).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 8.964575290679932
Epoch: 43, Steps: 65 | Train Loss: 0.0839437 Vali Loss: 0.5220903 Test Loss: 0.4164236
Validation loss decreased (0.523863 --> 0.522090).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 10.472346067428589
Epoch: 44, Steps: 65 | Train Loss: 0.0833762 Vali Loss: 0.5220951 Test Loss: 0.4160829
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 9.891072988510132
Epoch: 45, Steps: 65 | Train Loss: 0.0828202 Vali Loss: 0.5209729 Test Loss: 0.4153500
Validation loss decreased (0.522090 --> 0.520973).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.904422998428345
Epoch: 46, Steps: 65 | Train Loss: 0.0822981 Vali Loss: 0.5205563 Test Loss: 0.4147486
Validation loss decreased (0.520973 --> 0.520556).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 10.190468311309814
Epoch: 47, Steps: 65 | Train Loss: 0.0818410 Vali Loss: 0.5193048 Test Loss: 0.4141499
Validation loss decreased (0.520556 --> 0.519305).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 10.739783525466919
Epoch: 48, Steps: 65 | Train Loss: 0.0813664 Vali Loss: 0.5185970 Test Loss: 0.4135382
Validation loss decreased (0.519305 --> 0.518597).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 10.8730788230896
Epoch: 49, Steps: 65 | Train Loss: 0.0809443 Vali Loss: 0.5167179 Test Loss: 0.4130634
Validation loss decreased (0.518597 --> 0.516718).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 10.386250257492065
Epoch: 50, Steps: 65 | Train Loss: 0.0805399 Vali Loss: 0.5169522 Test Loss: 0.4125235
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.0497355408796396e-05
train 33745
val 11425
test 11425
Model(
  (freq_upsampler): Linear(in_features=90, out_features=102, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32901120.0
params:  9282.0
Trainable parameters:  9282
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.113678693771362
Epoch: 1, Steps: 65 | Train Loss: 0.2842030 Vali Loss: 0.4294991 Test Loss: 0.3384444
Validation loss decreased (inf --> 0.429499).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.107886791229248
Epoch: 2, Steps: 65 | Train Loss: 0.2697410 Vali Loss: 0.4125452 Test Loss: 0.3212163
Validation loss decreased (0.429499 --> 0.412545).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.6113862991333
Epoch: 3, Steps: 65 | Train Loss: 0.2665627 Vali Loss: 0.4063394 Test Loss: 0.3146625
Validation loss decreased (0.412545 --> 0.406339).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.131071329116821
Epoch: 4, Steps: 65 | Train Loss: 0.2656423 Vali Loss: 0.4027012 Test Loss: 0.3129925
Validation loss decreased (0.406339 --> 0.402701).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.73522400856018
Epoch: 5, Steps: 65 | Train Loss: 0.2649600 Vali Loss: 0.4007934 Test Loss: 0.3121362
Validation loss decreased (0.402701 --> 0.400793).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.56700325012207
Epoch: 6, Steps: 65 | Train Loss: 0.2645690 Vali Loss: 0.3999832 Test Loss: 0.3120330
Validation loss decreased (0.400793 --> 0.399983).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.881949663162231
Epoch: 7, Steps: 65 | Train Loss: 0.2642161 Vali Loss: 0.4006110 Test Loss: 0.3111502
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.035755395889282
Epoch: 8, Steps: 65 | Train Loss: 0.2639986 Vali Loss: 0.3989463 Test Loss: 0.3116106
Validation loss decreased (0.399983 --> 0.398946).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.233182907104492
Epoch: 9, Steps: 65 | Train Loss: 0.2637090 Vali Loss: 0.3973719 Test Loss: 0.3112258
Validation loss decreased (0.398946 --> 0.397372).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.678285121917725
Epoch: 10, Steps: 65 | Train Loss: 0.2635359 Vali Loss: 0.3969364 Test Loss: 0.3108529
Validation loss decreased (0.397372 --> 0.396936).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.324353218078613
Epoch: 11, Steps: 65 | Train Loss: 0.2633658 Vali Loss: 0.3981477 Test Loss: 0.3105389
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.758509159088135
Epoch: 12, Steps: 65 | Train Loss: 0.2634344 Vali Loss: 0.3979670 Test Loss: 0.3106810
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.262512683868408
Epoch: 13, Steps: 65 | Train Loss: 0.2632914 Vali Loss: 0.3963367 Test Loss: 0.3106245
Validation loss decreased (0.396936 --> 0.396337).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.721990823745728
Epoch: 14, Steps: 65 | Train Loss: 0.2628023 Vali Loss: 0.3966511 Test Loss: 0.3104047
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 10.709518194198608
Epoch: 15, Steps: 65 | Train Loss: 0.2631160 Vali Loss: 0.3971641 Test Loss: 0.3104095
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 10.643242597579956
Epoch: 16, Steps: 65 | Train Loss: 0.2632138 Vali Loss: 0.3962678 Test Loss: 0.3107131
Validation loss decreased (0.396337 --> 0.396268).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.734413146972656
Epoch: 17, Steps: 65 | Train Loss: 0.2627735 Vali Loss: 0.3964230 Test Loss: 0.3105691
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.951146125793457
Epoch: 18, Steps: 65 | Train Loss: 0.2629288 Vali Loss: 0.3965996 Test Loss: 0.3106071
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.32779598236084
Epoch: 19, Steps: 65 | Train Loss: 0.2628344 Vali Loss: 0.3968765 Test Loss: 0.3106640
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_96_FITS_ETTm1_ftM_sl720_ll48_pl96_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
mse:0.3111005425453186, mae:0.353329062461853, rse:0.5307405591011047, corr:[0.54530704 0.5559076  0.5624652  0.5646084  0.56466776 0.5648575
 0.5658713  0.56745374 0.56897193 0.5698392  0.57003343 0.56981015
 0.56958956 0.5694626  0.5692367  0.5686549  0.5676273  0.56624675
 0.56467474 0.5631408  0.56182486 0.5606531  0.5594724  0.5582665
 0.5568334  0.5554013  0.5542405  0.55347496 0.55329263 0.5536027
 0.5541663  0.5547691  0.5551519  0.55537224 0.55541027 0.5555086
 0.5555355  0.5554914  0.55534005 0.55497575 0.5546029  0.5543022
 0.554146   0.5542073  0.55426425 0.55416137 0.55397004 0.55375373
 0.55354047 0.55336404 0.5533471  0.55343986 0.55359584 0.5536198
 0.55353993 0.5534427  0.5534067  0.55336857 0.5533626  0.5532173
 0.5529497  0.5526343  0.552402   0.55226934 0.5523811  0.55268574
 0.55296844 0.5530509  0.5529011  0.5525622  0.55223244 0.5520133
 0.5519041  0.5518145  0.55168676 0.5513534  0.5507552  0.5500104
 0.5493383  0.54893875 0.54888386 0.548999   0.5490525  0.54871404
 0.54796255 0.54696435 0.54610944 0.54564095 0.54566    0.54607767
 0.5465081  0.5467215  0.5469224  0.5475549  0.548903   0.55025584]
