Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13336064.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5078006
	speed: 0.1562s/iter; left time: 4022.5362s
	iters: 200, epoch: 1 | loss: 0.3965687
	speed: 0.1325s/iter; left time: 3399.4022s
	iters: 300, epoch: 1 | loss: 0.3272898
	speed: 0.1296s/iter; left time: 3311.9860s
	iters: 400, epoch: 1 | loss: 0.3069653
	speed: 0.1301s/iter; left time: 3311.2372s
	iters: 500, epoch: 1 | loss: 0.2557389
	speed: 0.1308s/iter; left time: 3315.4891s
Epoch: 1 cost time: 70.30877542495728
Epoch: 1, Steps: 517 | Train Loss: 0.3992296 Vali Loss: 1.0570859 Test Loss: 0.4897033
Validation loss decreased (inf --> 1.057086).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2595647
	speed: 0.8958s/iter; left time: 22605.0260s
	iters: 200, epoch: 2 | loss: 0.2458595
	speed: 0.1394s/iter; left time: 3502.8348s
	iters: 300, epoch: 2 | loss: 0.2333690
	speed: 0.1416s/iter; left time: 3545.6056s
	iters: 400, epoch: 2 | loss: 0.2143880
	speed: 0.1423s/iter; left time: 3547.3030s
	iters: 500, epoch: 2 | loss: 0.2176386
	speed: 0.1303s/iter; left time: 3236.8267s
Epoch: 2 cost time: 71.8721616268158
Epoch: 2, Steps: 517 | Train Loss: 0.2517207 Vali Loss: 0.9787654 Test Loss: 0.4405878
Validation loss decreased (1.057086 --> 0.978765).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2031495
	speed: 0.8478s/iter; left time: 20954.5287s
	iters: 200, epoch: 3 | loss: 0.2360077
	speed: 0.1324s/iter; left time: 3258.5090s
	iters: 300, epoch: 3 | loss: 0.2200380
	speed: 0.1345s/iter; left time: 3298.2744s
	iters: 400, epoch: 3 | loss: 0.2322904
	speed: 0.1317s/iter; left time: 3215.9230s
	iters: 500, epoch: 3 | loss: 0.2316838
	speed: 0.1420s/iter; left time: 3452.8273s
Epoch: 3 cost time: 67.55136442184448
Epoch: 3, Steps: 517 | Train Loss: 0.2237096 Vali Loss: 0.9537027 Test Loss: 0.4226595
Validation loss decreased (0.978765 --> 0.953703).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2101155
	speed: 0.9087s/iter; left time: 21989.7049s
	iters: 200, epoch: 4 | loss: 0.2126528
	speed: 0.1464s/iter; left time: 3528.5197s
	iters: 300, epoch: 4 | loss: 0.2226974
	speed: 0.1432s/iter; left time: 3436.0753s
	iters: 400, epoch: 4 | loss: 0.2063287
	speed: 0.1306s/iter; left time: 3122.0370s
	iters: 500, epoch: 4 | loss: 0.1854864
	speed: 0.1140s/iter; left time: 2712.2375s
Epoch: 4 cost time: 69.73568964004517
Epoch: 4, Steps: 517 | Train Loss: 0.2142424 Vali Loss: 0.9444445 Test Loss: 0.4162136
Validation loss decreased (0.953703 --> 0.944444).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2127166
	speed: 0.7897s/iter; left time: 18701.6119s
	iters: 200, epoch: 5 | loss: 0.2127690
	speed: 0.1326s/iter; left time: 3127.8970s
	iters: 300, epoch: 5 | loss: 0.1976922
	speed: 0.1423s/iter; left time: 3340.8940s
	iters: 400, epoch: 5 | loss: 0.1977779
	speed: 0.1430s/iter; left time: 3343.7478s
	iters: 500, epoch: 5 | loss: 0.2167576
	speed: 0.1259s/iter; left time: 2931.5605s
Epoch: 5 cost time: 70.55902051925659
Epoch: 5, Steps: 517 | Train Loss: 0.2105231 Vali Loss: 0.9403422 Test Loss: 0.4148995
Validation loss decreased (0.944444 --> 0.940342).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2342203
	speed: 0.8826s/iter; left time: 20447.1487s
	iters: 200, epoch: 6 | loss: 0.2200269
	speed: 0.1315s/iter; left time: 3033.1225s
	iters: 300, epoch: 6 | loss: 0.2157321
	speed: 0.1303s/iter; left time: 2992.3269s
	iters: 400, epoch: 6 | loss: 0.2304215
	speed: 0.1141s/iter; left time: 2609.8254s
	iters: 500, epoch: 6 | loss: 0.2108007
	speed: 0.1263s/iter; left time: 2875.3182s
Epoch: 6 cost time: 66.7791097164154
Epoch: 6, Steps: 517 | Train Loss: 0.2089683 Vali Loss: 0.9398150 Test Loss: 0.4149044
Validation loss decreased (0.940342 --> 0.939815).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2230060
	speed: 0.8954s/iter; left time: 20279.0169s
	iters: 200, epoch: 7 | loss: 0.2116933
	speed: 0.1384s/iter; left time: 3119.8243s
	iters: 300, epoch: 7 | loss: 0.1981604
	speed: 0.1438s/iter; left time: 3228.3195s
	iters: 400, epoch: 7 | loss: 0.1961999
	speed: 0.1328s/iter; left time: 2968.2756s
	iters: 500, epoch: 7 | loss: 0.2174509
	speed: 0.1310s/iter; left time: 2913.8223s
Epoch: 7 cost time: 71.08620142936707
Epoch: 7, Steps: 517 | Train Loss: 0.2083761 Vali Loss: 0.9387098 Test Loss: 0.4157829
Validation loss decreased (0.939815 --> 0.938710).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2039090
	speed: 0.8390s/iter; left time: 18569.2413s
	iters: 200, epoch: 8 | loss: 0.2239359
	speed: 0.0654s/iter; left time: 1441.0357s
	iters: 300, epoch: 8 | loss: 0.2043175
	speed: 0.0544s/iter; left time: 1192.5255s
	iters: 400, epoch: 8 | loss: 0.2200879
	speed: 0.0524s/iter; left time: 1144.9171s
	iters: 500, epoch: 8 | loss: 0.2037180
	speed: 0.0518s/iter; left time: 1126.0881s
Epoch: 8 cost time: 36.122650384902954
Epoch: 8, Steps: 517 | Train Loss: 0.2081396 Vali Loss: 0.9401647 Test Loss: 0.4163969
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1924522
	speed: 0.8894s/iter; left time: 19224.1003s
	iters: 200, epoch: 9 | loss: 0.2089015
	speed: 0.1381s/iter; left time: 2971.7906s
	iters: 300, epoch: 9 | loss: 0.2037501
	speed: 0.1391s/iter; left time: 2978.3656s
	iters: 400, epoch: 9 | loss: 0.2476670
	speed: 0.1391s/iter; left time: 2964.8163s
	iters: 500, epoch: 9 | loss: 0.2023821
	speed: 0.1412s/iter; left time: 2994.9391s
Epoch: 9 cost time: 71.96906924247742
Epoch: 9, Steps: 517 | Train Loss: 0.2080352 Vali Loss: 0.9398744 Test Loss: 0.4167154
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2206253
	speed: 0.8916s/iter; left time: 18812.0094s
	iters: 200, epoch: 10 | loss: 0.2191394
	speed: 0.1368s/iter; left time: 2872.5890s
	iters: 300, epoch: 10 | loss: 0.2179133
	speed: 0.1308s/iter; left time: 2732.4691s
	iters: 400, epoch: 10 | loss: 0.2140774
	speed: 0.1269s/iter; left time: 2638.4549s
	iters: 500, epoch: 10 | loss: 0.2215385
	speed: 0.1232s/iter; left time: 2550.5582s
Epoch: 10 cost time: 68.76983165740967
Epoch: 10, Steps: 517 | Train Loss: 0.2079806 Vali Loss: 0.9387994 Test Loss: 0.4180359
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13336064.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4066322
	speed: 0.1388s/iter; left time: 3575.3928s
	iters: 200, epoch: 1 | loss: 0.3535147
	speed: 0.1276s/iter; left time: 3273.8336s
	iters: 300, epoch: 1 | loss: 0.3727416
	speed: 0.1306s/iter; left time: 3335.8803s
	iters: 400, epoch: 1 | loss: 0.3686948
	speed: 0.1386s/iter; left time: 3528.7251s
	iters: 500, epoch: 1 | loss: 0.4234046
	speed: 0.1367s/iter; left time: 3465.4013s
Epoch: 1 cost time: 69.67047595977783
Epoch: 1, Steps: 517 | Train Loss: 0.3983472 Vali Loss: 0.9337825 Test Loss: 0.4160242
Validation loss decreased (inf --> 0.933782).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3794604
	speed: 0.9027s/iter; left time: 22778.3434s
	iters: 200, epoch: 2 | loss: 0.3621542
	speed: 0.1372s/iter; left time: 3447.2940s
	iters: 300, epoch: 2 | loss: 0.4144445
	speed: 0.1281s/iter; left time: 3207.0351s
	iters: 400, epoch: 2 | loss: 0.4154768
	speed: 0.1307s/iter; left time: 3258.9323s
	iters: 500, epoch: 2 | loss: 0.4088039
	speed: 0.1285s/iter; left time: 3191.6406s
Epoch: 2 cost time: 70.05685782432556
Epoch: 2, Steps: 517 | Train Loss: 0.3976272 Vali Loss: 0.9314865 Test Loss: 0.4164882
Validation loss decreased (0.933782 --> 0.931486).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3922468
	speed: 0.8952s/iter; left time: 22126.8759s
	iters: 200, epoch: 3 | loss: 0.3960983
	speed: 0.1306s/iter; left time: 3214.4756s
	iters: 300, epoch: 3 | loss: 0.4026310
	speed: 0.1370s/iter; left time: 3358.3538s
	iters: 400, epoch: 3 | loss: 0.3722726
	speed: 0.1345s/iter; left time: 3284.0640s
	iters: 500, epoch: 3 | loss: 0.4273072
	speed: 0.1310s/iter; left time: 3186.6408s
Epoch: 3 cost time: 69.72365403175354
Epoch: 3, Steps: 517 | Train Loss: 0.3973705 Vali Loss: 0.9315612 Test Loss: 0.4159490
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4482224
	speed: 0.8482s/iter; left time: 20525.6078s
	iters: 200, epoch: 4 | loss: 0.3721438
	speed: 0.1307s/iter; left time: 3149.1973s
	iters: 300, epoch: 4 | loss: 0.3449259
	speed: 0.1316s/iter; left time: 3159.1493s
	iters: 400, epoch: 4 | loss: 0.3859657
	speed: 0.1188s/iter; left time: 2839.1728s
	iters: 500, epoch: 4 | loss: 0.4024925
	speed: 0.1344s/iter; left time: 3198.2194s
Epoch: 4 cost time: 66.85355162620544
Epoch: 4, Steps: 517 | Train Loss: 0.3972128 Vali Loss: 0.9311228 Test Loss: 0.4154209
Validation loss decreased (0.931486 --> 0.931123).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3924569
	speed: 0.8973s/iter; left time: 21249.6026s
	iters: 200, epoch: 5 | loss: 0.3966125
	speed: 0.1289s/iter; left time: 3039.3238s
	iters: 300, epoch: 5 | loss: 0.3670862
	speed: 0.1412s/iter; left time: 3316.7209s
	iters: 400, epoch: 5 | loss: 0.4251483
	speed: 0.1394s/iter; left time: 3260.0966s
	iters: 500, epoch: 5 | loss: 0.4498381
	speed: 0.1286s/iter; left time: 2994.6340s
Epoch: 5 cost time: 70.27964782714844
Epoch: 5, Steps: 517 | Train Loss: 0.3971209 Vali Loss: 0.9308010 Test Loss: 0.4160618
Validation loss decreased (0.931123 --> 0.930801).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4179395
	speed: 0.7810s/iter; left time: 18091.7128s
	iters: 200, epoch: 6 | loss: 0.3597608
	speed: 0.1168s/iter; left time: 2693.6665s
	iters: 300, epoch: 6 | loss: 0.4038480
	speed: 0.1129s/iter; left time: 2592.0992s
	iters: 400, epoch: 6 | loss: 0.3486925
	speed: 0.1162s/iter; left time: 2658.0144s
	iters: 500, epoch: 6 | loss: 0.3767878
	speed: 0.1147s/iter; left time: 2610.5453s
Epoch: 6 cost time: 60.829933404922485
Epoch: 6, Steps: 517 | Train Loss: 0.3969987 Vali Loss: 0.9308112 Test Loss: 0.4151314
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4399533
	speed: 0.7259s/iter; left time: 16441.6420s
	iters: 200, epoch: 7 | loss: 0.3828278
	speed: 0.1382s/iter; left time: 3115.6593s
	iters: 300, epoch: 7 | loss: 0.3992532
	speed: 0.1345s/iter; left time: 3018.4388s
	iters: 400, epoch: 7 | loss: 0.3463135
	speed: 0.1440s/iter; left time: 3217.5959s
	iters: 500, epoch: 7 | loss: 0.4392176
	speed: 0.1441s/iter; left time: 3206.5686s
Epoch: 7 cost time: 72.51394605636597
Epoch: 7, Steps: 517 | Train Loss: 0.3969799 Vali Loss: 0.9318095 Test Loss: 0.4151906
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3418685
	speed: 0.9280s/iter; left time: 20538.7820s
	iters: 200, epoch: 8 | loss: 0.3524845
	speed: 0.1306s/iter; left time: 2876.8106s
	iters: 300, epoch: 8 | loss: 0.4123211
	speed: 0.1275s/iter; left time: 2797.0587s
	iters: 400, epoch: 8 | loss: 0.4131263
	speed: 0.1266s/iter; left time: 2764.2469s
	iters: 500, epoch: 8 | loss: 0.3392624
	speed: 0.1370s/iter; left time: 2976.7942s
Epoch: 8 cost time: 70.08620500564575
Epoch: 8, Steps: 517 | Train Loss: 0.3968564 Vali Loss: 0.9301852 Test Loss: 0.4164594
Validation loss decreased (0.930801 --> 0.930185).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4075026
	speed: 0.8329s/iter; left time: 18002.6783s
	iters: 200, epoch: 9 | loss: 0.4462247
	speed: 0.0993s/iter; left time: 2135.7081s
	iters: 300, epoch: 9 | loss: 0.3930523
	speed: 0.1273s/iter; left time: 2725.7627s
	iters: 400, epoch: 9 | loss: 0.4344022
	speed: 0.1357s/iter; left time: 2892.6791s
	iters: 500, epoch: 9 | loss: 0.3376654
	speed: 0.1369s/iter; left time: 2904.6891s
Epoch: 9 cost time: 62.258079290390015
Epoch: 9, Steps: 517 | Train Loss: 0.3968564 Vali Loss: 0.9297463 Test Loss: 0.4159873
Validation loss decreased (0.930185 --> 0.929746).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3880537
	speed: 0.9053s/iter; left time: 19100.3749s
	iters: 200, epoch: 10 | loss: 0.3941996
	speed: 0.1284s/iter; left time: 2695.4000s
	iters: 300, epoch: 10 | loss: 0.3962540
	speed: 0.1256s/iter; left time: 2624.1192s
	iters: 400, epoch: 10 | loss: 0.4138215
	speed: 0.1289s/iter; left time: 2681.5507s
	iters: 500, epoch: 10 | loss: 0.4614301
	speed: 0.1358s/iter; left time: 2810.7783s
Epoch: 10 cost time: 68.11580204963684
Epoch: 10, Steps: 517 | Train Loss: 0.3967996 Vali Loss: 0.9319163 Test Loss: 0.4157205
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3684517
	speed: 0.8559s/iter; left time: 17616.1869s
	iters: 200, epoch: 11 | loss: 0.3456323
	speed: 0.1313s/iter; left time: 2689.1833s
	iters: 300, epoch: 11 | loss: 0.4238315
	speed: 0.1384s/iter; left time: 2821.3959s
	iters: 400, epoch: 11 | loss: 0.4064771
	speed: 0.1271s/iter; left time: 2577.7515s
	iters: 500, epoch: 11 | loss: 0.4260782
	speed: 0.1252s/iter; left time: 2527.2175s
Epoch: 11 cost time: 68.20116424560547
Epoch: 11, Steps: 517 | Train Loss: 0.3967615 Vali Loss: 0.9304465 Test Loss: 0.4158296
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3930258
	speed: 0.8729s/iter; left time: 17513.2709s
	iters: 200, epoch: 12 | loss: 0.4495598
	speed: 0.1278s/iter; left time: 2550.6851s
	iters: 300, epoch: 12 | loss: 0.4664353
	speed: 0.1259s/iter; left time: 2500.1495s
	iters: 400, epoch: 12 | loss: 0.4222702
	speed: 0.1224s/iter; left time: 2418.6379s
	iters: 500, epoch: 12 | loss: 0.3917975
	speed: 0.1336s/iter; left time: 2627.2181s
Epoch: 12 cost time: 67.13871264457703
Epoch: 12, Steps: 517 | Train Loss: 0.3966327 Vali Loss: 0.9290412 Test Loss: 0.4157711
Validation loss decreased (0.929746 --> 0.929041).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3438739
	speed: 0.8352s/iter; left time: 16325.7903s
	iters: 200, epoch: 13 | loss: 0.4265189
	speed: 0.1460s/iter; left time: 2838.3752s
	iters: 300, epoch: 13 | loss: 0.3705613
	speed: 0.1425s/iter; left time: 2757.6198s
	iters: 400, epoch: 13 | loss: 0.3852152
	speed: 0.1336s/iter; left time: 2571.0929s
	iters: 500, epoch: 13 | loss: 0.4021242
	speed: 0.1288s/iter; left time: 2466.3349s
Epoch: 13 cost time: 72.39663934707642
Epoch: 13, Steps: 517 | Train Loss: 0.3966625 Vali Loss: 0.9298217 Test Loss: 0.4161865
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4101592
	speed: 0.8782s/iter; left time: 16712.9057s
	iters: 200, epoch: 14 | loss: 0.3715717
	speed: 0.1256s/iter; left time: 2378.2188s
	iters: 300, epoch: 14 | loss: 0.3865793
	speed: 0.1255s/iter; left time: 2363.2475s
	iters: 400, epoch: 14 | loss: 0.4069888
	speed: 0.1356s/iter; left time: 2539.0107s
	iters: 500, epoch: 14 | loss: 0.3510635
	speed: 0.1280s/iter; left time: 2385.3090s
Epoch: 14 cost time: 67.86633014678955
Epoch: 14, Steps: 517 | Train Loss: 0.3966092 Vali Loss: 0.9307369 Test Loss: 0.4156126
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3636987
	speed: 0.8488s/iter; left time: 15713.5278s
	iters: 200, epoch: 15 | loss: 0.3964696
	speed: 0.1377s/iter; left time: 2534.7678s
	iters: 300, epoch: 15 | loss: 0.4116195
	speed: 0.1312s/iter; left time: 2402.5206s
	iters: 400, epoch: 15 | loss: 0.4085864
	speed: 0.1268s/iter; left time: 2309.0378s
	iters: 500, epoch: 15 | loss: 0.4208948
	speed: 0.1306s/iter; left time: 2365.7794s
Epoch: 15 cost time: 68.94043755531311
Epoch: 15, Steps: 517 | Train Loss: 0.3965762 Vali Loss: 0.9301359 Test Loss: 0.4163206
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41558170318603516, mae:0.41203370690345764, rse:0.6133367419242859, corr:[0.5254707  0.5329128  0.5341944  0.53393346 0.53501534 0.53743947
 0.5394272  0.54015523 0.5403882  0.54089236 0.5417585  0.54234225
 0.54233766 0.54197496 0.54159915 0.54113185 0.5402678  0.53894156
 0.5373809  0.53601986 0.5350286  0.5340504  0.53264123 0.5309488
 0.5291411  0.5278167  0.5271936  0.5270026  0.5270458  0.52714294
 0.5273891  0.52800894 0.5287796  0.5294135  0.5294537  0.5292324
 0.5288728  0.5286786  0.5286877  0.52867514 0.52865213 0.52851593
 0.52829164 0.52804756 0.5277029  0.52739197 0.52747625 0.5280136
 0.5286713  0.5289986  0.52889496 0.528414   0.52794427 0.5276907
 0.52776957 0.5279069  0.5277817  0.5272825  0.52675    0.5264094
 0.5264236  0.52664083 0.5267812  0.5265801  0.52622217 0.52592516
 0.5257424  0.5256695  0.5257521  0.5259494  0.5262514  0.52644986
 0.526318   0.5257717  0.5250675  0.524494   0.52426785 0.52440405
 0.5246523  0.5247387  0.52454436 0.5241689  0.52384293 0.52364135
 0.5236196  0.5236362  0.52366686 0.5237137  0.5238773  0.5242357
 0.52462244 0.5248139  0.52473044 0.5244507  0.52419794 0.52409905
 0.52422404 0.5245184  0.5246537  0.52463174 0.52449554 0.5243432
 0.52424955 0.52414    0.52404135 0.52401096 0.52391934 0.5238001
 0.52356523 0.5232565  0.52290195 0.5225169  0.5221603  0.5219036
 0.5217171  0.521569   0.52140206 0.52112913 0.5207668  0.52044505
 0.52023244 0.52019936 0.5203142  0.52053046 0.52064776 0.52045697
 0.52001286 0.5196112  0.51942503 0.5194544  0.5193699  0.51895124
 0.5183056  0.51771396 0.51747066 0.5177794  0.51847416 0.51897424
 0.51909053 0.518977   0.5188653  0.51904655 0.5194807  0.51988214
 0.519996   0.51972115 0.519327   0.51906544 0.5190696  0.519169
 0.51923317 0.519331   0.5194421  0.5194875  0.5193306  0.5189925
 0.51850975 0.5180502  0.5178719  0.51790696 0.51806957 0.5181369
 0.5180402  0.51788914 0.51793826 0.51831716 0.5189723  0.51970804
 0.52025837 0.52044547 0.5203812  0.5201961  0.5199063  0.5196117
 0.5194336  0.5194337  0.5195904  0.519808   0.51994115 0.5199642
 0.51991653 0.5199369  0.5200582  0.52030206 0.5205361  0.5206221
 0.52055573 0.52055234 0.52079374 0.5212626  0.52177346 0.5220832
 0.5220484  0.52171695 0.5211026  0.52031845 0.51952875 0.5188787
 0.5183423  0.5178457  0.51723176 0.51644176 0.5154993  0.5145735
 0.5138025  0.51317894 0.5125985  0.51199514 0.5113168  0.51062614
 0.5100031  0.5095548  0.5092029  0.508814   0.50824845 0.5076602
 0.50726646 0.5071065  0.5071491  0.5071905  0.50711817 0.50699073
 0.5069742  0.5070333  0.50720114 0.50738645 0.5073892  0.5071779
 0.50687885 0.5066981  0.50675344 0.5069238  0.5070414  0.5069856
 0.5067211  0.5064289  0.5062632  0.5065767  0.50724447 0.50802183
 0.5085573  0.5086375  0.5084253  0.5082055  0.5082016  0.5083378
 0.5084634  0.5085048  0.5084432  0.5083809  0.50829244 0.50825024
 0.50811505 0.5079559  0.5077408  0.5075318  0.5074036  0.50735664
 0.50731707 0.5072475  0.5071647  0.50716853 0.50734586 0.50775075
 0.50814617 0.5083639  0.5083287  0.50811774 0.50784165 0.5075672
 0.50741524 0.50739706 0.5075107  0.5076424  0.5076304  0.50744575
 0.50711435 0.50687647 0.50681514 0.50705117 0.5074087  0.5076754
 0.5078191  0.5078027  0.50766534 0.5074847  0.507213   0.50678676
 0.506246   0.50575787 0.50539815 0.50512683 0.504938   0.50478214
 0.50459397 0.5042137  0.50366443 0.502941   0.5021235  0.50135535
 0.50067693 0.5001583  0.49983153 0.4996289  0.49948984 0.499351
 0.49927697 0.49906644 0.4986178  0.49800417 0.49752542 0.49738097
 0.49765876 0.49823555 0.4986442  0.49854922 0.49792537 0.497127
 0.4965697  0.49644586 0.4966898  0.49703074 0.4972379  0.49724486
 0.4971566  0.4971723  0.49727607 0.49739537 0.497378   0.49716857
 0.4969009  0.49663332 0.49640098 0.4962784  0.4962481  0.49636078
 0.4964585  0.49630454 0.4960723  0.49594134 0.49608684 0.49643195
 0.49672502 0.49688354 0.4968471  0.49671176 0.4966006  0.49659014
 0.4965396  0.49642238 0.49625477 0.496086   0.4960333  0.49607882
 0.4961481  0.49627197 0.49644288 0.4966495  0.49691692 0.4971607
 0.4972281  0.49715433 0.49698192 0.49680457 0.4967327  0.49677512
 0.4967533  0.49671078 0.4966768  0.49662814 0.49663422 0.49668893
 0.49672768 0.49668476 0.49652258 0.49637774 0.4963532  0.4965746
 0.49701884 0.49754047 0.4979851  0.49822113 0.49827152 0.49824864
 0.49819762 0.4981135  0.49795327 0.49762583 0.4970896  0.49652618
 0.49597475 0.49548003 0.4950198  0.49460152 0.49415547 0.4937307
 0.49335414 0.49305284 0.49280033 0.49252543 0.49214205 0.49175638
 0.4913375  0.490944   0.49049532 0.49007088 0.4897686  0.48973268
 0.48989356 0.49010313 0.49017942 0.49010018 0.4899631  0.48983437
 0.48991916 0.48997435 0.48992363 0.48963228 0.48929384 0.4891908
 0.48934013 0.48971903 0.49004996 0.49019435 0.49011484 0.4900433
 0.49006823 0.490225   0.49032962 0.49037695 0.49026707 0.4901906
 0.49014118 0.49006176 0.48999533 0.48997867 0.48996094 0.48991415
 0.48982838 0.4897606  0.48976564 0.48981288 0.48982608 0.48973265
 0.4895283  0.4893722  0.48936045 0.4894772  0.4896328  0.48966497
 0.48959693 0.48950967 0.48956612 0.4898017  0.49018186 0.490649
 0.4909037  0.4908301  0.49047336 0.49006885 0.48970848 0.48959193
 0.48969677 0.4899292  0.49002218 0.48995408 0.48979682 0.48964018
 0.4896231  0.48971006 0.4898825  0.49005795 0.4902011  0.49027354
 0.49024072 0.49019477 0.49017292 0.49010262 0.4899417  0.48969522
 0.48936027 0.48892733 0.4883722  0.48765773 0.48683774 0.48607504
 0.48549843 0.48497766 0.48446968 0.48395514 0.48337436 0.48277017
 0.48218188 0.48163703 0.48105106 0.48035336 0.47961253 0.47901
 0.4786326  0.4785485  0.47862032 0.4785386  0.4783887  0.4781599
 0.47798678 0.4779297  0.47794616 0.4780259  0.4780736  0.4781132
 0.47818    0.478226   0.4782496  0.4783332  0.47838095 0.47837648
 0.47833866 0.47845495 0.47878227 0.4791292  0.47935405 0.4794453
 0.47944096 0.47943333 0.4794691  0.4795114  0.4796535  0.47991934
 0.48020792 0.48041162 0.48046875 0.48049247 0.48050508 0.48043048
 0.48024663 0.48000363 0.47984096 0.4798513  0.48004687 0.48033077
 0.480497   0.48035136 0.4799094  0.4793496  0.47892126 0.4788944
 0.47928858 0.47993037 0.48051292 0.4807404  0.48060894 0.48039484
 0.4802166  0.48015568 0.4801828  0.48023242 0.48023322 0.4802599
 0.48033518 0.48046672 0.48058116 0.4806262  0.48060095 0.48054776
 0.4805031  0.48059472 0.48065212 0.48063776 0.4805422  0.48045778
 0.48047155 0.48059222 0.48079437 0.48088163 0.4806861  0.48016238
 0.47946075 0.47878912 0.47821376 0.4777058  0.4772333  0.47685093
 0.4764792  0.47607246 0.47555944 0.4749165  0.4742116  0.47351408
 0.4729946  0.47266608 0.47245488 0.47217643 0.47172728 0.47108296
 0.4703849  0.4698203  0.4695712  0.46961698 0.469769   0.46979296
 0.46964756 0.46937642 0.46918273 0.46918237 0.46937284 0.46952662
 0.46956164 0.4693551  0.46902213 0.46879053 0.4688199  0.46917656
 0.46965128 0.46990955 0.46992496 0.46978125 0.46972552 0.46992725
 0.4704258  0.47105074 0.47145727 0.47158518 0.47166374 0.47181693
 0.47196567 0.47209087 0.47213036 0.47208712 0.4721225  0.4723008
 0.47251183 0.47273982 0.47290596 0.47289175 0.47272983 0.47237933
 0.47186458 0.47132668 0.47086573 0.47041923 0.47012034 0.4700123
 0.47014487 0.47045714 0.47081575 0.4710468  0.47100332 0.4707547
 0.4704495  0.4702469  0.47031724 0.4706132  0.47092438 0.47103998
 0.47090673 0.47063723 0.4704102  0.47031116 0.47028005 0.4701912
 0.4700071  0.4698875  0.46994323 0.4701583  0.47051266 0.47085178
 0.4711212  0.47135374 0.47161037 0.47188717 0.47202045 0.47186497
 0.47138664 0.4707389  0.47007564 0.4695049  0.46910068 0.46884784
 0.4686288  0.46824554 0.4677359  0.4671464  0.4665047  0.4660678
 0.4657304  0.46549043 0.4651427  0.46472564 0.4643009  0.46398905
 0.46387622 0.46384752 0.4637926  0.4635431  0.46328983 0.463261
 0.4634837  0.4638986  0.46421367 0.4642864  0.46430779 0.4643772
 0.4646773  0.46504772 0.4652676  0.46530336 0.4651427  0.46513727
 0.46547252 0.46602848 0.4664794  0.46664518 0.46673495 0.46684673
 0.46710575 0.46734992 0.4674537  0.46811217 0.4691822  0.46736476]
