Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14515200.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5048286
	speed: 0.1324s/iter; left time: 1694.2277s
	iters: 200, epoch: 1 | loss: 0.3930351
	speed: 0.1073s/iter; left time: 1363.2253s
Epoch: 1 cost time: 31.53494143486023
Epoch: 1, Steps: 258 | Train Loss: 0.4839407 Vali Loss: 1.1853236 Test Loss: 0.5747862
Validation loss decreased (inf --> 1.185324).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3240566
	speed: 0.5899s/iter; left time: 7398.6186s
	iters: 200, epoch: 2 | loss: 0.2969452
	speed: 0.1343s/iter; left time: 1670.8318s
Epoch: 2 cost time: 35.372966051101685
Epoch: 2, Steps: 258 | Train Loss: 0.3119610 Vali Loss: 1.0561883 Test Loss: 0.4922231
Validation loss decreased (1.185324 --> 1.056188).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2613844
	speed: 0.6496s/iter; left time: 7980.2988s
	iters: 200, epoch: 3 | loss: 0.2611734
	speed: 0.1493s/iter; left time: 1819.2193s
Epoch: 3 cost time: 40.06414008140564
Epoch: 3, Steps: 258 | Train Loss: 0.2632476 Vali Loss: 1.0068797 Test Loss: 0.4602963
Validation loss decreased (1.056188 --> 1.006880).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2515627
	speed: 0.6821s/iter; left time: 8204.0888s
	iters: 200, epoch: 4 | loss: 0.2465040
	speed: 0.1532s/iter; left time: 1827.2082s
Epoch: 4 cost time: 40.764796018600464
Epoch: 4, Steps: 258 | Train Loss: 0.2416096 Vali Loss: 0.9818445 Test Loss: 0.4437095
Validation loss decreased (1.006880 --> 0.981844).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2399059
	speed: 0.6037s/iter; left time: 7105.5174s
	iters: 200, epoch: 5 | loss: 0.2489153
	speed: 0.0802s/iter; left time: 935.3250s
Epoch: 5 cost time: 26.299386739730835
Epoch: 5, Steps: 258 | Train Loss: 0.2301072 Vali Loss: 0.9658708 Test Loss: 0.4333239
Validation loss decreased (0.981844 --> 0.965871).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2163139
	speed: 0.4710s/iter; left time: 5422.0496s
	iters: 200, epoch: 6 | loss: 0.2299647
	speed: 0.1078s/iter; left time: 1229.5357s
Epoch: 6 cost time: 29.087300300598145
Epoch: 6, Steps: 258 | Train Loss: 0.2232034 Vali Loss: 0.9557182 Test Loss: 0.4268895
Validation loss decreased (0.965871 --> 0.955718).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2054118
	speed: 0.4958s/iter; left time: 5579.3179s
	iters: 200, epoch: 7 | loss: 0.2105693
	speed: 0.1185s/iter; left time: 1322.0295s
Epoch: 7 cost time: 29.798913955688477
Epoch: 7, Steps: 258 | Train Loss: 0.2189434 Vali Loss: 0.9498776 Test Loss: 0.4216576
Validation loss decreased (0.955718 --> 0.949878).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2080684
	speed: 0.4958s/iter; left time: 5451.7931s
	iters: 200, epoch: 8 | loss: 0.2034730
	speed: 0.1116s/iter; left time: 1215.9011s
Epoch: 8 cost time: 30.485490560531616
Epoch: 8, Steps: 258 | Train Loss: 0.2159732 Vali Loss: 0.9453720 Test Loss: 0.4194270
Validation loss decreased (0.949878 --> 0.945372).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2117739
	speed: 0.5304s/iter; left time: 5694.8614s
	iters: 200, epoch: 9 | loss: 0.2022267
	speed: 0.1102s/iter; left time: 1171.9365s
Epoch: 9 cost time: 31.719151496887207
Epoch: 9, Steps: 258 | Train Loss: 0.2141178 Vali Loss: 0.9428877 Test Loss: 0.4176768
Validation loss decreased (0.945372 --> 0.942888).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2031985
	speed: 0.5543s/iter; left time: 5808.4239s
	iters: 200, epoch: 10 | loss: 0.2133309
	speed: 0.1265s/iter; left time: 1313.3226s
Epoch: 10 cost time: 34.18934154510498
Epoch: 10, Steps: 258 | Train Loss: 0.2128542 Vali Loss: 0.9409119 Test Loss: 0.4164475
Validation loss decreased (0.942888 --> 0.940912).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2298575
	speed: 0.5575s/iter; left time: 5697.9707s
	iters: 200, epoch: 11 | loss: 0.2120288
	speed: 0.1208s/iter; left time: 1223.0326s
Epoch: 11 cost time: 32.1348021030426
Epoch: 11, Steps: 258 | Train Loss: 0.2118272 Vali Loss: 0.9394196 Test Loss: 0.4160179
Validation loss decreased (0.940912 --> 0.939420).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1964774
	speed: 0.5304s/iter; left time: 5284.1873s
	iters: 200, epoch: 12 | loss: 0.2028622
	speed: 0.1333s/iter; left time: 1314.6859s
Epoch: 12 cost time: 34.588801860809326
Epoch: 12, Steps: 258 | Train Loss: 0.2112752 Vali Loss: 0.9393928 Test Loss: 0.4157664
Validation loss decreased (0.939420 --> 0.939393).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2095964
	speed: 0.5239s/iter; left time: 5084.0092s
	iters: 200, epoch: 13 | loss: 0.2068392
	speed: 0.1030s/iter; left time: 989.5530s
Epoch: 13 cost time: 28.023683071136475
Epoch: 13, Steps: 258 | Train Loss: 0.2108441 Vali Loss: 0.9389941 Test Loss: 0.4161033
Validation loss decreased (0.939393 --> 0.938994).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2132904
	speed: 0.5264s/iter; left time: 4973.2971s
	iters: 200, epoch: 14 | loss: 0.2209102
	speed: 0.1203s/iter; left time: 1124.0333s
Epoch: 14 cost time: 32.27084994316101
Epoch: 14, Steps: 258 | Train Loss: 0.2105097 Vali Loss: 0.9396623 Test Loss: 0.4160871
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2077101
	speed: 0.5317s/iter; left time: 4886.0430s
	iters: 200, epoch: 15 | loss: 0.2152769
	speed: 0.1260s/iter; left time: 1145.1288s
Epoch: 15 cost time: 33.91863775253296
Epoch: 15, Steps: 258 | Train Loss: 0.2103898 Vali Loss: 0.9396070 Test Loss: 0.4164639
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2115426
	speed: 0.5762s/iter; left time: 5146.4735s
	iters: 200, epoch: 16 | loss: 0.2149202
	speed: 0.1193s/iter; left time: 1053.6161s
Epoch: 16 cost time: 32.70776844024658
Epoch: 16, Steps: 258 | Train Loss: 0.2102044 Vali Loss: 0.9393898 Test Loss: 0.4169309
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14515200.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3881123
	speed: 0.1471s/iter; left time: 1882.7523s
	iters: 200, epoch: 1 | loss: 0.4162235
	speed: 0.1356s/iter; left time: 1722.7657s
Epoch: 1 cost time: 36.09338068962097
Epoch: 1, Steps: 258 | Train Loss: 0.3987783 Vali Loss: 0.9342521 Test Loss: 0.4177166
Validation loss decreased (inf --> 0.934252).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4113927
	speed: 0.5802s/iter; left time: 7277.3743s
	iters: 200, epoch: 2 | loss: 0.3895319
	speed: 0.1222s/iter; left time: 1520.8552s
Epoch: 2 cost time: 33.53578019142151
Epoch: 2, Steps: 258 | Train Loss: 0.3979837 Vali Loss: 0.9329740 Test Loss: 0.4173169
Validation loss decreased (0.934252 --> 0.932974).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4303841
	speed: 0.5614s/iter; left time: 6896.6660s
	iters: 200, epoch: 3 | loss: 0.3960497
	speed: 0.1213s/iter; left time: 1477.5027s
Epoch: 3 cost time: 31.448501348495483
Epoch: 3, Steps: 258 | Train Loss: 0.3976261 Vali Loss: 0.9324197 Test Loss: 0.4168335
Validation loss decreased (0.932974 --> 0.932420).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3983454
	speed: 0.5143s/iter; left time: 6185.7451s
	iters: 200, epoch: 4 | loss: 0.3922381
	speed: 0.1095s/iter; left time: 1306.3028s
Epoch: 4 cost time: 30.162335872650146
Epoch: 4, Steps: 258 | Train Loss: 0.3975401 Vali Loss: 0.9331952 Test Loss: 0.4169737
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3983082
	speed: 0.5275s/iter; left time: 6208.3353s
	iters: 200, epoch: 5 | loss: 0.3486586
	speed: 0.1318s/iter; left time: 1537.8079s
Epoch: 5 cost time: 33.3378472328186
Epoch: 5, Steps: 258 | Train Loss: 0.3974705 Vali Loss: 0.9312544 Test Loss: 0.4171623
Validation loss decreased (0.932420 --> 0.931254).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4139211
	speed: 0.5724s/iter; left time: 6589.3991s
	iters: 200, epoch: 6 | loss: 0.3808816
	speed: 0.1281s/iter; left time: 1461.6603s
Epoch: 6 cost time: 35.49982476234436
Epoch: 6, Steps: 258 | Train Loss: 0.3973840 Vali Loss: 0.9316911 Test Loss: 0.4167310
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3991221
	speed: 0.5748s/iter; left time: 6468.1831s
	iters: 200, epoch: 7 | loss: 0.4152974
	speed: 0.1310s/iter; left time: 1461.5609s
Epoch: 7 cost time: 34.71874785423279
Epoch: 7, Steps: 258 | Train Loss: 0.3971433 Vali Loss: 0.9318075 Test Loss: 0.4170828
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3876126
	speed: 0.6540s/iter; left time: 7190.4267s
	iters: 200, epoch: 8 | loss: 0.4072436
	speed: 0.1601s/iter; left time: 1744.7820s
Epoch: 8 cost time: 43.084503412246704
Epoch: 8, Steps: 258 | Train Loss: 0.3972071 Vali Loss: 0.9320571 Test Loss: 0.4170890
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.416145920753479, mae:0.41203272342681885, rse:0.6137529015541077, corr:[0.521278   0.5308321  0.53755295 0.5401626  0.54027504 0.5401578
 0.5407856  0.54208547 0.5434933  0.5443149  0.5443226  0.5437212
 0.5430567  0.54262257 0.5423158  0.54185784 0.5410306  0.53980297
 0.53828824 0.53675884 0.5354519  0.5343814  0.5334061  0.5324653
 0.53130436 0.5300491  0.528926   0.5281673  0.52804375 0.52851593
 0.5292963  0.53006476 0.5304545  0.5304513  0.53010106 0.52977985
 0.5295627  0.52952474 0.52957344 0.52949876 0.5293369  0.52911884
 0.5290002  0.5291207  0.52936715 0.52955955 0.52961046 0.52944183
 0.5290568  0.5285886  0.5283149  0.5283298  0.5286007  0.5288171
 0.52880895 0.5285027  0.52799016 0.52742195 0.5270191  0.52679753
 0.5267364  0.5267471  0.5267142  0.52652985 0.52630544 0.52615976
 0.52612853 0.52618784 0.5262758  0.5262629  0.5261395  0.5259095
 0.5256285  0.5253905  0.52529776 0.52530706 0.5253226  0.5252956
 0.5252022  0.5250625  0.5248886  0.5247049  0.5245443  0.5243841
 0.52427614 0.5241808  0.5241518  0.52420723 0.5243875  0.5247341
 0.52519643 0.52566713 0.5260395  0.5262033  0.52615154 0.52589244
 0.5255586  0.5252929  0.52502126 0.52481836 0.5246485  0.5244815
 0.52432835 0.5241645  0.5240533  0.5240786  0.52415234 0.52425295
 0.5242621  0.52414125 0.5238685  0.52346253 0.52299654 0.5225599
 0.52218574 0.5218961  0.5216785  0.52149236 0.521322   0.5212073
 0.5211199  0.52101177 0.5208173  0.5206063  0.52040213 0.5201487
 0.5198216  0.5194772  0.5191263  0.51884514 0.5186307  0.51852024
 0.51857126 0.5187459  0.5189057  0.5190023  0.51904994 0.5189503
 0.5187812  0.51864046 0.51852113 0.5184802  0.51853526 0.5186748
 0.51885194 0.51896095 0.51901615 0.51901954 0.5190089  0.51895356
 0.5188432  0.5187574  0.51870316 0.51868504 0.51868325 0.51876247
 0.5188552  0.5189018  0.5189188  0.5188638  0.5188416  0.5188849
 0.51900834 0.5191793  0.5193634  0.5195217  0.51966953 0.51984835
 0.5200298  0.5201826  0.52036554 0.52057236 0.5207172  0.5207833
 0.5207977  0.5207947  0.5208096  0.520837   0.5208296  0.52076936
 0.52063954 0.52047825 0.52030927 0.52026486 0.5204146  0.52075523
 0.5212347  0.5217667  0.5222324  0.522506   0.52251256 0.52225363
 0.5217896  0.52126837 0.52069384 0.5200778  0.5194351  0.5187957
 0.51812273 0.5174354  0.51672906 0.5160666  0.51545924 0.5149015
 0.5143637  0.5137961  0.51316565 0.51250935 0.51184857 0.5112057
 0.51058877 0.51004994 0.5095951  0.5091877  0.5087171  0.5082033
 0.5076816  0.50713    0.50662285 0.5061908  0.5058967  0.5058165
 0.5059998  0.50627273 0.50654215 0.50673676 0.5067739  0.5066928
 0.5065758  0.50650686 0.506525   0.5065941  0.5067077  0.5068461
 0.5069843  0.50713456 0.50720596 0.5073156  0.50743586 0.5076332
 0.50787777 0.5080596  0.50817627 0.50825506 0.5083152  0.50830483
 0.5082087  0.50804496 0.50783044 0.50763553 0.50746757 0.5074143
 0.50741714 0.5075149  0.5076221  0.50766504 0.50762284 0.5075009
 0.5073351  0.50718457 0.5070657  0.506986   0.5069456  0.50700235
 0.5070516  0.507056   0.5069982  0.5069277  0.50686616 0.5067915
 0.50675005 0.50672466 0.5067235  0.5067401  0.5067587  0.5067906
 0.50681645 0.5069307  0.5070605  0.5072585  0.50746316 0.5076208
 0.50775856 0.5078045  0.5077367  0.5075777  0.50730336 0.5068938
 0.5063837  0.5058749  0.50539505 0.50490284 0.5044165  0.50396705
 0.5035877  0.5032298  0.50293005 0.5026197  0.5022439  0.50180846
 0.50130135 0.50078577 0.5003508  0.5000174  0.49976623 0.49953693
 0.49938408 0.49921045 0.49899718 0.4987533  0.498561   0.49842006
 0.4983521  0.49839273 0.49846587 0.49851832 0.49850526 0.4984479
 0.4983475  0.49821296 0.49810454 0.4980671  0.49809664 0.49814335
 0.49814647 0.4980936  0.49793306 0.49771515 0.4974618  0.49722746
 0.4971198  0.4971127  0.49713475 0.49716222 0.49716553 0.49722856
 0.4973144  0.49730375 0.49725777 0.49718252 0.49711964 0.49708107
 0.49704927 0.49706462 0.49710864 0.4971697  0.49721435 0.4972447
 0.49716693 0.49701983 0.49682176 0.4965923  0.49641165 0.49630457
 0.49625355 0.49626425 0.49630442 0.4963462  0.49642432 0.49654564
 0.49663025 0.49669805 0.4966845  0.4965626  0.49634925 0.49608633
 0.4957812  0.49557284 0.49552462 0.49558136 0.4957246  0.49589175
 0.49602753 0.49613443 0.49622884 0.49639615 0.49666205 0.49706882
 0.49758038 0.49810818 0.49855345 0.49879473 0.498781   0.4985546
 0.4981714  0.49772605 0.4973308  0.49698675 0.49662128 0.4962987
 0.4959496  0.4955493  0.4950986  0.49467364 0.4942754  0.49395242
 0.49370146 0.49348792 0.4932604  0.49295592 0.49250305 0.4919938
 0.49146637 0.49104133 0.4906934  0.4904605  0.49030906 0.490232
 0.49019137 0.49019736 0.49025598 0.49039847 0.4906216  0.49082696
 0.4910629  0.49116093 0.4911488  0.4909688  0.49072823 0.49056628
 0.4904946  0.49057397 0.49072435 0.49089193 0.4909833  0.49103817
 0.49103504 0.4910337  0.49103022 0.4911279  0.49127087 0.49152976
 0.49177948 0.49189276 0.4918854  0.4918153  0.491691   0.49154583
 0.49138916 0.49125132 0.49115783 0.49110785 0.49109623 0.49110848
 0.49106714 0.49098045 0.4908267  0.4906464  0.49049297 0.4903731
 0.49035057 0.49038655 0.49042833 0.49039862 0.49026862 0.49011943
 0.48990637 0.4897228  0.48960742 0.48961666 0.48966587 0.4897629
 0.48984304 0.48991352 0.48989448 0.48980352 0.48965037 0.48944926
 0.48926067 0.48909304 0.4890091  0.48904642 0.48921987 0.48950282
 0.48981485 0.49012816 0.4903651  0.49042717 0.49025783 0.48987478
 0.4893287  0.48871455 0.48811442 0.4875421  0.48699516 0.48651332
 0.48613033 0.48573208 0.48530626 0.48486033 0.48433447 0.48372793
 0.48306963 0.48243266 0.48180968 0.48115024 0.48045227 0.47978878
 0.47919795 0.4787861  0.4785718  0.4784102  0.4783601  0.47832194
 0.47828612 0.47822    0.47810096 0.47802684 0.4780236  0.47815436
 0.47840014 0.47863635 0.4787842  0.47888523 0.4789202  0.47891575
 0.47889525 0.4789693  0.47916746 0.4793781  0.4795334  0.47963893
 0.47969145 0.47972125 0.47978216 0.47988224 0.48012242 0.48052415
 0.48094454 0.48122105 0.48125383 0.48113868 0.48096555 0.48078552
 0.480665   0.48064154 0.4807265  0.48089573 0.48107982 0.48121145
 0.4812512  0.4811851  0.4810576  0.4809154  0.48079067 0.4807326
 0.4806937  0.48064184 0.48054808 0.4803756  0.48016825 0.48006254
 0.48002955 0.4800903  0.48024252 0.48046407 0.48067468 0.48085812
 0.48098305 0.4810726  0.48114765 0.48122624 0.48130748 0.4813544
 0.48133785 0.4813504  0.4813342  0.48134348 0.4813937  0.48148617
 0.48159847 0.4816809  0.4817233  0.4816627  0.4814535  0.48107305
 0.48057523 0.4800738  0.47960517 0.4791399  0.47864252 0.4781766
 0.47771013 0.4772601  0.47680202 0.476326   0.47583273 0.4752628
 0.4746587  0.474006   0.47331664 0.4725947  0.47193775 0.4713833
 0.4709586  0.47063902 0.4704256  0.47029716 0.47024018 0.47024906
 0.47035816 0.47050563 0.47068492 0.47085232 0.47097617 0.47096118
 0.47086462 0.47065544 0.47037473 0.4700882  0.4698597  0.46979305
 0.46990803 0.47012752 0.47044122 0.47076273 0.47103217 0.47119555
 0.47127807 0.47132808 0.47130436 0.47128242 0.47142604 0.47174662
 0.47211573 0.4724732  0.47272968 0.47283667 0.4728899  0.47293234
 0.47292206 0.4729322  0.47296473 0.4729315  0.4728259  0.4725755
 0.47215208 0.47167018 0.47127268 0.4709729  0.4708521  0.47083342
 0.47084653 0.47081175 0.4707202  0.47061148 0.47053432 0.47057915
 0.47070825 0.47081813 0.4709077  0.47095403 0.470932   0.47085497
 0.47079524 0.47083583 0.4710217  0.4713274  0.4716537  0.47188208
 0.47193384 0.4718593  0.4716909  0.4714701  0.47135776 0.47140846
 0.47161502 0.47190675 0.47218463 0.472374   0.47235864 0.47209454
 0.47162363 0.47110924 0.47062418 0.47016287 0.46971652 0.46928343
 0.46885526 0.46835285 0.4678094  0.46720725 0.46648803 0.46586174
 0.46528056 0.46489403 0.46466312 0.46461153 0.46460047 0.46449497
 0.46424913 0.46383905 0.46336207 0.46286577 0.46255022 0.46252716
 0.46272236 0.4630954  0.46345964 0.46371615 0.4639304  0.46410105
 0.46436685 0.46468928 0.4650088  0.46532783 0.46545276 0.4654265
 0.46535346 0.46542513 0.46570867 0.46606162 0.46638367 0.46654198
 0.46673617 0.4671696  0.46773916 0.46817744 0.46720088 0.46226156]
