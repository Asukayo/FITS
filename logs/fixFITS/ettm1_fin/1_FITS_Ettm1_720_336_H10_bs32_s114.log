Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5322240.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4054357
	speed: 0.1190s/iter; left time: 3099.7206s
	iters: 200, epoch: 1 | loss: 0.3611545
	speed: 0.1133s/iter; left time: 2939.4815s
	iters: 300, epoch: 1 | loss: 0.4268021
	speed: 0.1345s/iter; left time: 3475.9201s
	iters: 400, epoch: 1 | loss: 0.3006050
	speed: 0.1361s/iter; left time: 3504.4755s
	iters: 500, epoch: 1 | loss: 0.3226446
	speed: 0.1309s/iter; left time: 3357.6322s
Epoch: 1 cost time: 66.52729916572571
Epoch: 1, Steps: 523 | Train Loss: 0.3958574 Vali Loss: 0.6928207 Test Loss: 0.3676803
Validation loss decreased (inf --> 0.692821).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4074659
	speed: 0.8514s/iter; left time: 21735.2305s
	iters: 200, epoch: 2 | loss: 0.3376115
	speed: 0.1196s/iter; left time: 3041.7039s
	iters: 300, epoch: 2 | loss: 0.3678306
	speed: 0.1271s/iter; left time: 3218.8942s
	iters: 400, epoch: 2 | loss: 0.3735515
	speed: 0.0985s/iter; left time: 2484.5053s
	iters: 500, epoch: 2 | loss: 0.3636457
	speed: 0.0827s/iter; left time: 2077.7078s
Epoch: 2 cost time: 58.591102838516235
Epoch: 2, Steps: 523 | Train Loss: 0.3418507 Vali Loss: 0.6702804 Test Loss: 0.3670931
Validation loss decreased (0.692821 --> 0.670280).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3099739
	speed: 0.8181s/iter; left time: 20455.6803s
	iters: 200, epoch: 3 | loss: 0.2941627
	speed: 0.1229s/iter; left time: 3061.0100s
	iters: 300, epoch: 3 | loss: 0.3331798
	speed: 0.1197s/iter; left time: 2970.2982s
	iters: 400, epoch: 3 | loss: 0.3244337
	speed: 0.1172s/iter; left time: 2896.3768s
	iters: 500, epoch: 3 | loss: 0.3098577
	speed: 0.1159s/iter; left time: 2851.1506s
Epoch: 3 cost time: 64.00793385505676
Epoch: 3, Steps: 523 | Train Loss: 0.3391672 Vali Loss: 0.6644537 Test Loss: 0.3675415
Validation loss decreased (0.670280 --> 0.664454).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4030139
	speed: 0.8685s/iter; left time: 21262.1598s
	iters: 200, epoch: 4 | loss: 0.3389537
	speed: 0.1258s/iter; left time: 3067.2856s
	iters: 300, epoch: 4 | loss: 0.3372392
	speed: 0.1223s/iter; left time: 2970.5363s
	iters: 400, epoch: 4 | loss: 0.3326673
	speed: 0.1211s/iter; left time: 2927.9054s
	iters: 500, epoch: 4 | loss: 0.3659051
	speed: 0.1272s/iter; left time: 3062.3606s
Epoch: 4 cost time: 66.354243516922
Epoch: 4, Steps: 523 | Train Loss: 0.3383617 Vali Loss: 0.6586333 Test Loss: 0.3670605
Validation loss decreased (0.664454 --> 0.658633).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3282516
	speed: 0.5534s/iter; left time: 13259.5473s
	iters: 200, epoch: 5 | loss: 0.3172977
	speed: 0.0681s/iter; left time: 1625.2477s
	iters: 300, epoch: 5 | loss: 0.3571852
	speed: 0.0917s/iter; left time: 2178.8940s
	iters: 400, epoch: 5 | loss: 0.3441314
	speed: 0.0975s/iter; left time: 2307.8836s
	iters: 500, epoch: 5 | loss: 0.3534066
	speed: 0.1144s/iter; left time: 2694.8073s
Epoch: 5 cost time: 48.46753191947937
Epoch: 5, Steps: 523 | Train Loss: 0.3378788 Vali Loss: 0.6571380 Test Loss: 0.3670883
Validation loss decreased (0.658633 --> 0.657138).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3204401
	speed: 0.9134s/iter; left time: 21406.3286s
	iters: 200, epoch: 6 | loss: 0.2868526
	speed: 0.1448s/iter; left time: 3379.9512s
	iters: 300, epoch: 6 | loss: 0.3547309
	speed: 0.1535s/iter; left time: 3565.8095s
	iters: 400, epoch: 6 | loss: 0.3264568
	speed: 0.1458s/iter; left time: 3374.2725s
	iters: 500, epoch: 6 | loss: 0.3633029
	speed: 0.1334s/iter; left time: 3073.5896s
Epoch: 6 cost time: 75.32584142684937
Epoch: 6, Steps: 523 | Train Loss: 0.3376239 Vali Loss: 0.6559630 Test Loss: 0.3667288
Validation loss decreased (0.657138 --> 0.655963).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3330941
	speed: 0.7891s/iter; left time: 18080.5038s
	iters: 200, epoch: 7 | loss: 0.3526276
	speed: 0.1061s/iter; left time: 2421.4130s
	iters: 300, epoch: 7 | loss: 0.3554257
	speed: 0.1047s/iter; left time: 2377.9868s
	iters: 400, epoch: 7 | loss: 0.2839544
	speed: 0.1068s/iter; left time: 2415.4799s
	iters: 500, epoch: 7 | loss: 0.3187318
	speed: 0.1005s/iter; left time: 2262.2326s
Epoch: 7 cost time: 56.28560781478882
Epoch: 7, Steps: 523 | Train Loss: 0.3373934 Vali Loss: 0.6602578 Test Loss: 0.3666715
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2914188
	speed: 0.6868s/iter; left time: 15378.4364s
	iters: 200, epoch: 8 | loss: 0.3472523
	speed: 0.0984s/iter; left time: 2193.8613s
	iters: 300, epoch: 8 | loss: 0.3552184
	speed: 0.1036s/iter; left time: 2298.5783s
	iters: 400, epoch: 8 | loss: 0.3160453
	speed: 0.1023s/iter; left time: 2260.0688s
	iters: 500, epoch: 8 | loss: 0.3388817
	speed: 0.0994s/iter; left time: 2186.0434s
Epoch: 8 cost time: 53.67934560775757
Epoch: 8, Steps: 523 | Train Loss: 0.3373211 Vali Loss: 0.6582493 Test Loss: 0.3670632
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3323061
	speed: 0.6791s/iter; left time: 14849.7152s
	iters: 200, epoch: 9 | loss: 0.3451841
	speed: 0.1295s/iter; left time: 2819.3412s
	iters: 300, epoch: 9 | loss: 0.3391927
	speed: 0.1295s/iter; left time: 2806.1699s
	iters: 400, epoch: 9 | loss: 0.3241794
	speed: 0.1334s/iter; left time: 2876.3814s
	iters: 500, epoch: 9 | loss: 0.3466149
	speed: 0.1317s/iter; left time: 2828.1180s
Epoch: 9 cost time: 66.42912745475769
Epoch: 9, Steps: 523 | Train Loss: 0.3371374 Vali Loss: 0.6542704 Test Loss: 0.3674196
Validation loss decreased (0.655963 --> 0.654270).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3538397
	speed: 0.7357s/iter; left time: 15702.4527s
	iters: 200, epoch: 10 | loss: 0.3457008
	speed: 0.1004s/iter; left time: 2132.7418s
	iters: 300, epoch: 10 | loss: 0.3586082
	speed: 0.0970s/iter; left time: 2051.4133s
	iters: 400, epoch: 10 | loss: 0.3386482
	speed: 0.0814s/iter; left time: 1712.1010s
	iters: 500, epoch: 10 | loss: 0.3758734
	speed: 0.0901s/iter; left time: 1887.2696s
Epoch: 10 cost time: 48.90861678123474
Epoch: 10, Steps: 523 | Train Loss: 0.3370609 Vali Loss: 0.6557229 Test Loss: 0.3677458
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3834211
	speed: 0.5675s/iter; left time: 11816.3989s
	iters: 200, epoch: 11 | loss: 0.3239548
	speed: 0.0826s/iter; left time: 1712.0673s
	iters: 300, epoch: 11 | loss: 0.3279800
	speed: 0.0816s/iter; left time: 1683.0141s
	iters: 400, epoch: 11 | loss: 0.3238885
	speed: 0.0860s/iter; left time: 1765.0413s
	iters: 500, epoch: 11 | loss: 0.3397007
	speed: 0.0881s/iter; left time: 1798.2990s
Epoch: 11 cost time: 45.878228187561035
Epoch: 11, Steps: 523 | Train Loss: 0.3370439 Vali Loss: 0.6555992 Test Loss: 0.3673259
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3654743
	speed: 0.5925s/iter; left time: 12026.1347s
	iters: 200, epoch: 12 | loss: 0.3728032
	speed: 0.0853s/iter; left time: 1722.7740s
	iters: 300, epoch: 12 | loss: 0.3241367
	speed: 0.0879s/iter; left time: 1766.6123s
	iters: 400, epoch: 12 | loss: 0.3512524
	speed: 0.0916s/iter; left time: 1831.2910s
	iters: 500, epoch: 12 | loss: 0.3556298
	speed: 0.0870s/iter; left time: 1730.3593s
Epoch: 12 cost time: 47.65154051780701
Epoch: 12, Steps: 523 | Train Loss: 0.3369906 Vali Loss: 0.6553497 Test Loss: 0.3666444
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3673053979873657, mae:0.3855595290660858, rse:0.5767159461975098, corr:[0.5364694  0.54514897 0.5507681  0.55268097 0.55290926 0.5534104
 0.55471575 0.5565105  0.55817896 0.55916923 0.5594715  0.5593018
 0.5590715  0.5589004  0.55863434 0.55806774 0.5571376  0.55591196
 0.55445856 0.55290085 0.55136985 0.54987854 0.5484163  0.5470722
 0.5457021  0.5444761  0.54354066 0.54292685 0.54276067 0.54299325
 0.5434623  0.54403687 0.5444897  0.54483235 0.54496944 0.54501027
 0.54478735 0.54433155 0.54374236 0.54310584 0.5427427  0.5427689
 0.5431768  0.54380465 0.5442396  0.54421943 0.5438017  0.543156
 0.54251766 0.54211026 0.5421268  0.5424422  0.5428295  0.54295784
 0.54276264 0.5423343  0.54188085 0.5415649  0.5415199  0.54160637
 0.5416878  0.54169035 0.54160684 0.54146945 0.5415013  0.5418063
 0.5422659  0.54269713 0.54295814 0.5429706  0.5428061  0.5425644
 0.54233986 0.54221916 0.5422136  0.54219025 0.54202247 0.54171383
 0.5413465  0.54104406 0.54089826 0.54092675 0.541096   0.5412726
 0.5413791  0.5413268  0.541151   0.5409185  0.54078525 0.5409137
 0.5412992  0.54180866 0.54225194 0.5424689  0.5423655  0.5419509
 0.54136693 0.5408352  0.5403683  0.54005486 0.5399028  0.5398607
 0.53985095 0.5397393  0.5395313  0.5392835  0.53894997 0.5386257
 0.5383266  0.5381211  0.5380268  0.5379956  0.5379746  0.53793204
 0.53782135 0.5376368  0.5373901  0.5370885  0.5367861  0.53657734
 0.5364649  0.5364111  0.5363277  0.5362491  0.53618073 0.5360938
 0.5360202  0.5360236  0.53606594 0.5361075  0.5360198  0.5357635
 0.5354434  0.53515685 0.5349368  0.5348663  0.5349945  0.5351313
 0.535202   0.53514874 0.53489107 0.53453153 0.534251   0.5342075
 0.53443724 0.5347814  0.53512114 0.53531516 0.5353357  0.5351398
 0.5347899  0.5344636  0.5342449  0.5341708  0.5341881  0.53429574
 0.53439754 0.5344226  0.5344369  0.53442836 0.5345184  0.5347095
 0.53497773 0.5352404  0.5354644  0.53565377 0.53586125 0.5361585
 0.5365145  0.536858   0.5371952  0.5374701  0.5375556  0.5374153
 0.5370897  0.53667444 0.536275   0.535984   0.5358129  0.5357839
 0.53585136 0.53599733 0.5361538  0.5363777  0.5366891  0.53706175
 0.5374606  0.5378652  0.5382231  0.53844166 0.53846306 0.5383111
 0.53800625 0.537675   0.5372765  0.5367794  0.5361988  0.5355949
 0.53496975 0.53434145 0.5337034  0.5330685  0.5324115  0.5317062
 0.53096354 0.5301838  0.52939695 0.52869415 0.52811915 0.5276725
 0.5273041  0.5269924  0.5266778  0.5263158  0.5258227  0.5252778
 0.52478945 0.5243903  0.52415204 0.52405345 0.52408516 0.52425486
 0.52457285 0.52486265 0.52505606 0.52512246 0.5250277  0.52484256
 0.52466756 0.5245894  0.5246431  0.5247651  0.5249137  0.5250417
 0.5251121  0.525173   0.52514195 0.5251768  0.52526337 0.5254308
 0.52561444 0.5256582  0.52555853 0.52538055 0.52521735 0.5250692
 0.524954   0.5248977  0.5248641  0.524853   0.52477986 0.5246834
 0.5244949  0.524312   0.52413875 0.5240061  0.523968   0.52404124
 0.52421147 0.5244638  0.5246976  0.52483076 0.52481747 0.5247413
 0.5245749  0.5243568  0.5241622  0.524078   0.52411497 0.52419513
 0.5243075  0.52438927 0.52442074 0.5244123  0.5243581  0.5242835
 0.52416855 0.52409047 0.52400273 0.5239898  0.5240136  0.52404624
 0.5241176  0.52421325 0.5242808  0.5243234  0.5243049  0.52414036
 0.5238223  0.523436   0.5229649  0.52231085 0.5215179  0.52068096
 0.5199343  0.51929635 0.5188549  0.5185141  0.5181698  0.5177484
 0.51718557 0.5165254  0.51591736 0.5154311  0.515063   0.5147297
 0.5144936  0.5142406  0.5139639  0.5137231  0.5136738  0.5138193
 0.514115   0.5144742  0.51466244 0.51455796 0.51415443 0.51361096
 0.51308054 0.5126194  0.5122769  0.5120472  0.5118825  0.51178205
 0.5117883  0.51200736 0.5123611  0.5127822  0.5131024  0.5131921
 0.51309484 0.5129089  0.512822   0.5128705  0.51248455 0.5104678 ]
