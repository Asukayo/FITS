Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  77830144.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.777990579605103
Epoch: 1, Steps: 65 | Train Loss: 0.5576367 Vali Loss: 0.9271434 Test Loss: 0.5128818
Validation loss decreased (inf --> 0.927143).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.889210224151611
Epoch: 2, Steps: 65 | Train Loss: 0.4089122 Vali Loss: 0.7996540 Test Loss: 0.4266883
Validation loss decreased (0.927143 --> 0.799654).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.944892168045044
Epoch: 3, Steps: 65 | Train Loss: 0.3763320 Vali Loss: 0.7515594 Test Loss: 0.3980516
Validation loss decreased (0.799654 --> 0.751559).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.408989667892456
Epoch: 4, Steps: 65 | Train Loss: 0.3619236 Vali Loss: 0.7256083 Test Loss: 0.3834850
Validation loss decreased (0.751559 --> 0.725608).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 12.171114444732666
Epoch: 5, Steps: 65 | Train Loss: 0.3537705 Vali Loss: 0.7104715 Test Loss: 0.3752515
Validation loss decreased (0.725608 --> 0.710472).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.478202819824219
Epoch: 6, Steps: 65 | Train Loss: 0.3487866 Vali Loss: 0.6984283 Test Loss: 0.3708167
Validation loss decreased (0.710472 --> 0.698428).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.386367321014404
Epoch: 7, Steps: 65 | Train Loss: 0.3456909 Vali Loss: 0.6917893 Test Loss: 0.3683862
Validation loss decreased (0.698428 --> 0.691789).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.890159845352173
Epoch: 8, Steps: 65 | Train Loss: 0.3432674 Vali Loss: 0.6853762 Test Loss: 0.3671670
Validation loss decreased (0.691789 --> 0.685376).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.68080997467041
Epoch: 9, Steps: 65 | Train Loss: 0.3419739 Vali Loss: 0.6819398 Test Loss: 0.3664429
Validation loss decreased (0.685376 --> 0.681940).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.80513882637024
Epoch: 10, Steps: 65 | Train Loss: 0.3410040 Vali Loss: 0.6808509 Test Loss: 0.3659972
Validation loss decreased (0.681940 --> 0.680851).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.070234537124634
Epoch: 11, Steps: 65 | Train Loss: 0.3402257 Vali Loss: 0.6769930 Test Loss: 0.3659287
Validation loss decreased (0.680851 --> 0.676993).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.692092895507812
Epoch: 12, Steps: 65 | Train Loss: 0.3396989 Vali Loss: 0.6725078 Test Loss: 0.3658918
Validation loss decreased (0.676993 --> 0.672508).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.061094760894775
Epoch: 13, Steps: 65 | Train Loss: 0.3390431 Vali Loss: 0.6719335 Test Loss: 0.3660727
Validation loss decreased (0.672508 --> 0.671933).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.439477920532227
Epoch: 14, Steps: 65 | Train Loss: 0.3388416 Vali Loss: 0.6711339 Test Loss: 0.3659922
Validation loss decreased (0.671933 --> 0.671134).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.005495071411133
Epoch: 15, Steps: 65 | Train Loss: 0.3383235 Vali Loss: 0.6702059 Test Loss: 0.3660195
Validation loss decreased (0.671134 --> 0.670206).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 10.70886778831482
Epoch: 16, Steps: 65 | Train Loss: 0.3381813 Vali Loss: 0.6687825 Test Loss: 0.3661770
Validation loss decreased (0.670206 --> 0.668782).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.106203317642212
Epoch: 17, Steps: 65 | Train Loss: 0.3381322 Vali Loss: 0.6670632 Test Loss: 0.3661481
Validation loss decreased (0.668782 --> 0.667063).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 11.443540811538696
Epoch: 18, Steps: 65 | Train Loss: 0.3378848 Vali Loss: 0.6657947 Test Loss: 0.3662195
Validation loss decreased (0.667063 --> 0.665795).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 11.114868640899658
Epoch: 19, Steps: 65 | Train Loss: 0.3378414 Vali Loss: 0.6675170 Test Loss: 0.3663746
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.823866605758667
Epoch: 20, Steps: 65 | Train Loss: 0.3375972 Vali Loss: 0.6645313 Test Loss: 0.3662603
Validation loss decreased (0.665795 --> 0.664531).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.150266170501709
Epoch: 21, Steps: 65 | Train Loss: 0.3373920 Vali Loss: 0.6639035 Test Loss: 0.3663200
Validation loss decreased (0.664531 --> 0.663903).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.692851305007935
Epoch: 22, Steps: 65 | Train Loss: 0.3373378 Vali Loss: 0.6628918 Test Loss: 0.3663698
Validation loss decreased (0.663903 --> 0.662892).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.61713719367981
Epoch: 23, Steps: 65 | Train Loss: 0.3371618 Vali Loss: 0.6635267 Test Loss: 0.3664630
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.675636768341064
Epoch: 24, Steps: 65 | Train Loss: 0.3371510 Vali Loss: 0.6616709 Test Loss: 0.3664222
Validation loss decreased (0.662892 --> 0.661671).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.792909145355225
Epoch: 25, Steps: 65 | Train Loss: 0.3371867 Vali Loss: 0.6615142 Test Loss: 0.3665120
Validation loss decreased (0.661671 --> 0.661514).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.677484273910522
Epoch: 26, Steps: 65 | Train Loss: 0.3371761 Vali Loss: 0.6605445 Test Loss: 0.3664579
Validation loss decreased (0.661514 --> 0.660545).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.172613859176636
Epoch: 27, Steps: 65 | Train Loss: 0.3369829 Vali Loss: 0.6617119 Test Loss: 0.3665085
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.941121339797974
Epoch: 28, Steps: 65 | Train Loss: 0.3368973 Vali Loss: 0.6629106 Test Loss: 0.3664650
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 10.258872509002686
Epoch: 29, Steps: 65 | Train Loss: 0.3370399 Vali Loss: 0.6607624 Test Loss: 0.3664975
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3659422993659973, mae:0.3847576677799225, rse:0.575644850730896, corr:[0.53724056 0.5470577  0.5527888  0.5539001  0.5542379  0.5553852
 0.55699617 0.55813134 0.55852437 0.5586724  0.55894375 0.55924886
 0.5594262  0.5592819  0.55882716 0.55812836 0.5571391  0.55592865
 0.55462974 0.5534542  0.55253845 0.5516259  0.5504163  0.54902536
 0.547584   0.5465346  0.5460226  0.54567295 0.54525715 0.54482675
 0.544742   0.54540765 0.5466514  0.54787713 0.5483176  0.5480218
 0.54728246 0.5467313  0.54662323 0.5466084  0.54638463 0.5458256
 0.545272   0.54518896 0.545475   0.54571587 0.5456421  0.54523844
 0.54478097 0.5445896  0.5447569  0.54493326 0.5448167  0.5443153
 0.5437751  0.5435739  0.5437405  0.543944   0.5439218  0.54356897
 0.54317445 0.5430975  0.5433631  0.543666   0.5438659  0.54389775
 0.5438472  0.5438965  0.5441126  0.54438454 0.54459053 0.54461616
 0.5444526  0.54422575 0.54406834 0.54395986 0.54381406 0.5435948
 0.54333067 0.54313844 0.54309154 0.5431481  0.5431811  0.54305184
 0.54280317 0.5425221  0.5423438  0.5422793  0.5423102  0.5424099
 0.54254144 0.5427006  0.54287094 0.5429912  0.54294217 0.54266
 0.5422358  0.5418536  0.5415123  0.54126394 0.54105914 0.5408386
 0.54059315 0.54031426 0.5401191  0.54005516 0.53997105 0.5398261
 0.5395354  0.53918093 0.5388788  0.53868264 0.5385882  0.5385314
 0.53839725 0.538137   0.5377777  0.5373849  0.537085   0.5370001
 0.5370653  0.5371058  0.5369513  0.5366867  0.5364668  0.53636223
 0.5363617  0.53636485 0.536211   0.53594434 0.53566957 0.53555876
 0.53569233 0.5358602  0.53577363 0.5354022  0.53497314 0.53464836
 0.5346271  0.53484    0.53499997 0.5350179  0.5349835  0.5350565
 0.53530127 0.5355735  0.53576076 0.53576386 0.53562903 0.53544176
 0.53534687 0.5354479  0.5356436  0.53578365 0.535778   0.5357329
 0.53570896 0.5357165  0.535769   0.5357407  0.5356738  0.5356062
 0.53563446 0.53580266 0.5361073  0.5364504  0.5367083  0.53684396
 0.5368513  0.5368129  0.53687805 0.53699774 0.5369992  0.53684014
 0.5366122  0.5364524  0.5364208  0.53648084 0.53651386 0.5364815
 0.53638697 0.5362977  0.5362253  0.5362264  0.536301   0.5364414
 0.53668284 0.537064   0.53747964 0.5377415  0.53777134 0.5375925
 0.5373087  0.53707623 0.5368313  0.5364681  0.5359241  0.5352423
 0.5345038  0.53382844 0.5332512  0.5327377  0.5321879  0.53156567
 0.5309314  0.5303516  0.5298336  0.5293582  0.5288373  0.5282441
 0.5276285  0.5271256  0.5267176  0.526303   0.5257056  0.52498746
 0.52429795 0.52372783 0.5233921  0.5232386  0.5232081  0.5232466
 0.52334416 0.52338403 0.52346057 0.5236232  0.5237954  0.52390045
 0.52389055 0.5238087  0.5237519  0.5237504  0.52384293 0.5239882
 0.52407783 0.5241264  0.5240853  0.5242092  0.5244878  0.52487344
 0.52516794 0.52516854 0.52495515 0.52471465 0.52460605 0.5245925
 0.5246081  0.5246166  0.5245496  0.5244512  0.5243008  0.52421916
 0.5241711  0.5242648  0.52439827 0.5244748  0.52448595 0.524466
 0.52447134 0.52455544 0.52466345 0.5247268  0.5247195  0.52476674
 0.52481496 0.52485657 0.5248749  0.52487415 0.52480257 0.5246114
 0.52439195 0.52418286 0.52403635 0.5239921  0.52400595 0.5240469
 0.5240388  0.52406377 0.52406985 0.52415514 0.5242687  0.5243872
 0.5245321  0.52466685 0.52472305 0.52471125 0.5245997  0.5243149
 0.5238818  0.52340543 0.522891   0.5222734  0.5216954  0.5212364
 0.5208962  0.5204751  0.51993775 0.51922905 0.51849484 0.51792294
 0.5175167  0.51718575 0.51679665 0.51627076 0.51571524 0.51527077
 0.51514786 0.5151031  0.5148664  0.5143454  0.51379186 0.5133975
 0.5132677  0.5133161  0.5131663  0.5127295  0.5122127  0.5119791
 0.51211923 0.51227593 0.5121267  0.5116629  0.5111996  0.5110652
 0.5112759  0.5115607  0.5114578  0.5110522  0.51074237 0.51086646
 0.5114182  0.51179963 0.5117993  0.51226705 0.51282597 0.50951153]
