Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4477143
	speed: 0.1505s/iter; left time: 1926.4674s
	iters: 200, epoch: 1 | loss: 0.4287235
	speed: 0.1359s/iter; left time: 1726.3588s
Epoch: 1 cost time: 36.74120259284973
Epoch: 1, Steps: 258 | Train Loss: 0.5067373 Vali Loss: 1.0141231 Test Loss: 0.4416979
Validation loss decreased (inf --> 1.014123).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3928874
	speed: 0.6441s/iter; left time: 8079.0459s
	iters: 200, epoch: 2 | loss: 0.3963578
	speed: 0.1477s/iter; left time: 1837.3871s
Epoch: 2 cost time: 38.995824575424194
Epoch: 2, Steps: 258 | Train Loss: 0.4131293 Vali Loss: 0.9652311 Test Loss: 0.4177590
Validation loss decreased (1.014123 --> 0.965231).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3841108
	speed: 0.6289s/iter; left time: 7726.4677s
	iters: 200, epoch: 3 | loss: 0.3888155
	speed: 0.1398s/iter; left time: 1703.5056s
Epoch: 3 cost time: 36.79070258140564
Epoch: 3, Steps: 258 | Train Loss: 0.4026298 Vali Loss: 0.9498407 Test Loss: 0.4145276
Validation loss decreased (0.965231 --> 0.949841).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4062220
	speed: 0.5133s/iter; left time: 6173.7082s
	iters: 200, epoch: 4 | loss: 0.4580542
	speed: 0.1211s/iter; left time: 1443.8100s
Epoch: 4 cost time: 33.12898540496826
Epoch: 4, Steps: 258 | Train Loss: 0.3994790 Vali Loss: 0.9434583 Test Loss: 0.4150304
Validation loss decreased (0.949841 --> 0.943458).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4420487
	speed: 0.6071s/iter; left time: 7145.1453s
	iters: 200, epoch: 5 | loss: 0.4116120
	speed: 0.1536s/iter; left time: 1792.5022s
Epoch: 5 cost time: 40.17830538749695
Epoch: 5, Steps: 258 | Train Loss: 0.3983752 Vali Loss: 0.9391553 Test Loss: 0.4151621
Validation loss decreased (0.943458 --> 0.939155).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3986238
	speed: 0.6200s/iter; left time: 7136.3295s
	iters: 200, epoch: 6 | loss: 0.4199855
	speed: 0.1336s/iter; left time: 1524.7963s
Epoch: 6 cost time: 36.274232387542725
Epoch: 6, Steps: 258 | Train Loss: 0.3978785 Vali Loss: 0.9385544 Test Loss: 0.4153945
Validation loss decreased (0.939155 --> 0.938554).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4066201
	speed: 0.5663s/iter; left time: 6373.1100s
	iters: 200, epoch: 7 | loss: 0.4103454
	speed: 0.1339s/iter; left time: 1493.7883s
Epoch: 7 cost time: 35.334874391555786
Epoch: 7, Steps: 258 | Train Loss: 0.3974246 Vali Loss: 0.9359883 Test Loss: 0.4161712
Validation loss decreased (0.938554 --> 0.935988).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4382278
	speed: 0.5898s/iter; left time: 6485.1077s
	iters: 200, epoch: 8 | loss: 0.4317838
	speed: 0.1319s/iter; left time: 1437.4335s
Epoch: 8 cost time: 34.91357755661011
Epoch: 8, Steps: 258 | Train Loss: 0.3973270 Vali Loss: 0.9353138 Test Loss: 0.4158286
Validation loss decreased (0.935988 --> 0.935314).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3957881
	speed: 0.5876s/iter; left time: 6308.7339s
	iters: 200, epoch: 9 | loss: 0.4034412
	speed: 0.1203s/iter; left time: 1279.1522s
Epoch: 9 cost time: 33.74266290664673
Epoch: 9, Steps: 258 | Train Loss: 0.3971203 Vali Loss: 0.9334255 Test Loss: 0.4160736
Validation loss decreased (0.935314 --> 0.933426).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4185200
	speed: 0.5223s/iter; left time: 5473.1518s
	iters: 200, epoch: 10 | loss: 0.3754901
	speed: 0.1005s/iter; left time: 1042.7003s
Epoch: 10 cost time: 23.755155324935913
Epoch: 10, Steps: 258 | Train Loss: 0.3968780 Vali Loss: 0.9343039 Test Loss: 0.4162051
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3780868
	speed: 0.1943s/iter; left time: 1986.2389s
	iters: 200, epoch: 11 | loss: 0.3939108
	speed: 0.0652s/iter; left time: 660.2599s
Epoch: 11 cost time: 14.956689357757568
Epoch: 11, Steps: 258 | Train Loss: 0.3967993 Vali Loss: 0.9329076 Test Loss: 0.4160784
Validation loss decreased (0.933426 --> 0.932908).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4012182
	speed: 0.4768s/iter; left time: 4750.2339s
	iters: 200, epoch: 12 | loss: 0.4129649
	speed: 0.1167s/iter; left time: 1151.4933s
Epoch: 12 cost time: 31.205718994140625
Epoch: 12, Steps: 258 | Train Loss: 0.3968941 Vali Loss: 0.9324608 Test Loss: 0.4163824
Validation loss decreased (0.932908 --> 0.932461).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3642791
	speed: 0.5367s/iter; left time: 5209.1042s
	iters: 200, epoch: 13 | loss: 0.4125378
	speed: 0.1178s/iter; left time: 1131.2312s
Epoch: 13 cost time: 32.25867295265198
Epoch: 13, Steps: 258 | Train Loss: 0.3968457 Vali Loss: 0.9324865 Test Loss: 0.4162943
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3912449
	speed: 0.5421s/iter; left time: 5121.4468s
	iters: 200, epoch: 14 | loss: 0.3794019
	speed: 0.1251s/iter; left time: 1168.8532s
Epoch: 14 cost time: 33.685813188552856
Epoch: 14, Steps: 258 | Train Loss: 0.3967264 Vali Loss: 0.9322935 Test Loss: 0.4161349
Validation loss decreased (0.932461 --> 0.932293).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3797838
	speed: 0.4799s/iter; left time: 4410.0187s
	iters: 200, epoch: 15 | loss: 0.4117922
	speed: 0.1060s/iter; left time: 963.8876s
Epoch: 15 cost time: 29.39747905731201
Epoch: 15, Steps: 258 | Train Loss: 0.3967743 Vali Loss: 0.9322794 Test Loss: 0.4164195
Validation loss decreased (0.932293 --> 0.932279).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3737689
	speed: 0.5957s/iter; left time: 5320.1573s
	iters: 200, epoch: 16 | loss: 0.3741005
	speed: 0.1341s/iter; left time: 1184.4592s
Epoch: 16 cost time: 35.42916417121887
Epoch: 16, Steps: 258 | Train Loss: 0.3966416 Vali Loss: 0.9312431 Test Loss: 0.4157592
Validation loss decreased (0.932279 --> 0.931243).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3928449
	speed: 0.6142s/iter; left time: 5326.6069s
	iters: 200, epoch: 17 | loss: 0.3642943
	speed: 0.1335s/iter; left time: 1144.5432s
Epoch: 17 cost time: 35.66858458518982
Epoch: 17, Steps: 258 | Train Loss: 0.3966053 Vali Loss: 0.9324467 Test Loss: 0.4159833
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4133730
	speed: 0.6045s/iter; left time: 5087.0699s
	iters: 200, epoch: 18 | loss: 0.3939097
	speed: 0.1255s/iter; left time: 1043.1190s
Epoch: 18 cost time: 33.14910697937012
Epoch: 18, Steps: 258 | Train Loss: 0.3965566 Vali Loss: 0.9307193 Test Loss: 0.4160227
Validation loss decreased (0.931243 --> 0.930719).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4241287
	speed: 0.5294s/iter; left time: 4318.0669s
	iters: 200, epoch: 19 | loss: 0.3936009
	speed: 0.1219s/iter; left time: 982.5329s
Epoch: 19 cost time: 34.14324736595154
Epoch: 19, Steps: 258 | Train Loss: 0.3965602 Vali Loss: 0.9309354 Test Loss: 0.4161092
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3972679
	speed: 0.5830s/iter; left time: 4605.1922s
	iters: 200, epoch: 20 | loss: 0.3904997
	speed: 0.1449s/iter; left time: 1130.0711s
Epoch: 20 cost time: 37.086063861846924
Epoch: 20, Steps: 258 | Train Loss: 0.3964000 Vali Loss: 0.9321612 Test Loss: 0.4159058
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3829969
	speed: 0.5990s/iter; left time: 4576.5935s
	iters: 200, epoch: 21 | loss: 0.3849583
	speed: 0.1471s/iter; left time: 1109.1508s
Epoch: 21 cost time: 38.30059814453125
Epoch: 21, Steps: 258 | Train Loss: 0.3965087 Vali Loss: 0.9312366 Test Loss: 0.4163118
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41503968834877014, mae:0.411494642496109, rse:0.6129366159439087, corr:[0.5255204  0.53187096 0.5342884  0.53474903 0.5355476  0.5372265
 0.5389482  0.5399249  0.54029584 0.5405392  0.5409794  0.5414562
 0.54180765 0.54186547 0.5415073  0.5406649  0.5394596  0.53823644
 0.53719205 0.5362714  0.53518677 0.53367054 0.531849   0.53031254
 0.52916986 0.5284978  0.5279445  0.52714354 0.5263962  0.52613443
 0.5266532  0.5278705  0.52909964 0.52983665 0.5297716  0.5294984
 0.5292572  0.52913016 0.52885956 0.52821326 0.5275222  0.52714807
 0.52732575 0.5278174  0.52797127 0.5275591  0.5270138  0.5268523
 0.5271962  0.52762604 0.5277013  0.5271471  0.52627933 0.5255479
 0.5253829  0.52563083 0.5258257  0.52558    0.5250503  0.5245349
 0.5244066  0.5246561  0.5249872  0.5250434  0.5249526  0.5249743
 0.52519983 0.525514   0.5257688  0.52580136 0.52569336 0.5255582
 0.525514   0.5255551  0.5256556  0.5256346  0.5253558  0.524891
 0.5244238  0.52415454 0.52418137 0.524452   0.5248162  0.5250453
 0.5251323  0.5250428  0.5248726  0.5246919  0.52463204 0.52482915
 0.5252442  0.5257011  0.52600217 0.5259823  0.5256515  0.5251101
 0.52461815 0.5244015  0.52429223 0.52421623 0.52404207 0.52379566
 0.5236313  0.5236149  0.52378    0.52402437 0.524038   0.5237891
 0.52332765 0.5229187  0.52270406 0.5226067  0.52247435 0.5222172
 0.5218241  0.5214471  0.52122706 0.5211475  0.52108514 0.52096635
 0.5207307  0.5204507  0.52022874 0.5202236  0.5203397  0.52027565
 0.5198993  0.51939976 0.51905864 0.5191386  0.51950467 0.51982534
 0.5198688  0.51957756 0.51908964 0.51874155 0.51875937 0.51891124
 0.51900417 0.51894325 0.5187431  0.5186738  0.51891655 0.5194054
 0.51990724 0.52017236 0.52021426 0.52013    0.5200419  0.5199127
 0.51968354 0.5194182  0.51915646 0.5189611  0.5188481  0.5188618
 0.5188977  0.5188657  0.5187995  0.51868653 0.5186835  0.51883
 0.5191288  0.5195151  0.5199215  0.5202442  0.52042127 0.5204684
 0.5204315  0.5204217  0.5206128  0.52097195 0.521261   0.5213406
 0.5212063  0.52097195 0.5207948  0.5207555  0.5207865  0.5208093
 0.5207245  0.52054757 0.5203365  0.52028793 0.5204941  0.52091736
 0.5214364  0.52195776 0.5223763  0.52260864 0.5226436  0.52249575
 0.5222146  0.5218702  0.52139515 0.5207807  0.52008736 0.5194186
 0.5188     0.5182393  0.51764643 0.5169954  0.5162804  0.5155978
 0.51503146 0.51456773 0.51410127 0.51356965 0.51292545 0.51221395
 0.51149327 0.51087713 0.5103503  0.5098232  0.5091595  0.5084494
 0.507808   0.50723225 0.5067676  0.50640327 0.5062028  0.50626767
 0.5066412  0.50709116 0.50749487 0.5077925  0.5079395  0.5080125
 0.50807226 0.5081265  0.5081088  0.50791126 0.50759816 0.5073217
 0.5072136  0.507375   0.50766176 0.5080703  0.5084219  0.50866807
 0.50870526 0.5084521  0.5080448  0.5076779  0.50752157 0.50759315
 0.50782704 0.50810504 0.5082528  0.50818694 0.5078647  0.5074842
 0.5071798  0.5071738  0.5074081  0.5077253  0.50796145 0.5079979
 0.50785035 0.50766313 0.5075617  0.50762063 0.50783145 0.508182
 0.5084467  0.5085667  0.5085739  0.5085726  0.50858057 0.5085438
 0.50846726 0.508317   0.5081445  0.50801533 0.5079716  0.50802255
 0.5080895  0.5081917  0.50821877 0.50820595 0.5081385  0.50805044
 0.5080614  0.50814015 0.5082302  0.50825214 0.50808156 0.50768495
 0.50716007 0.50668186 0.50629324 0.5058844  0.50536954 0.5047177
 0.50401044 0.50333947 0.5028775  0.5025854  0.5023135  0.50191844
 0.50131935 0.5006832  0.5002459  0.5000843  0.50008935 0.5000236
 0.49980745 0.49934623 0.4987866  0.4983158  0.49809366 0.4980597
 0.49809328 0.49812168 0.4980543  0.49796942 0.49797812 0.4981454
 0.4983596  0.49842265 0.49827006 0.49798715 0.4977524  0.49768764
 0.49775565 0.49781895 0.49766746 0.49732795 0.49697515 0.4968393
 0.49707174 0.49747127 0.49770835 0.49764585 0.49734914 0.4971476
 0.49717674 0.49728888 0.4974285  0.49747252 0.49742427 0.49733722
 0.49726012 0.49726397 0.4972785  0.49723884 0.49713182 0.4970423
 0.49697334 0.49699777 0.49708456 0.49713838 0.49714243 0.49706176
 0.49688694 0.49669284 0.4965261  0.49643546 0.4964874  0.49665833
 0.4967928  0.49686626 0.4968242  0.49668878 0.49653876 0.49643245
 0.49632293 0.49624926 0.49618524 0.4960736  0.49599314 0.49600732
 0.49613467 0.49632162 0.49645212 0.49649483 0.49647242 0.4965476
 0.4968476  0.49737826 0.49796963 0.49842018 0.4985966  0.49851435
 0.4982653  0.49793643 0.49757326 0.4971142  0.49649677 0.49589053
 0.49539325 0.49509883 0.4949631  0.4948664  0.49458626 0.49406958
 0.49339816 0.49277598 0.492354   0.49208963 0.49178568 0.49140745
 0.4909217  0.49048433 0.4901138  0.48984617 0.489607   0.48938528
 0.48917827 0.4890878  0.48915625 0.4893845  0.48969018 0.4898849
 0.49001864 0.489985   0.48994526 0.48992518 0.49003437 0.4902974
 0.49053466 0.490704   0.49073336 0.49070403 0.49064887 0.490672
 0.49072292 0.49079397 0.4908248  0.490937   0.49109247 0.4913565
 0.49153894 0.49149412 0.49127367 0.49099764 0.49072897 0.49051178
 0.4903559  0.49026752 0.49025092 0.49028477 0.49034458 0.49039486
 0.4903709  0.49031085 0.49022928 0.49015766 0.49011737 0.49008077
 0.49009714 0.49013376 0.4901737  0.49017063 0.4901132  0.4900802
 0.49000973 0.48998618 0.4900268  0.49014807 0.49020872 0.49021578
 0.4901482  0.49011084 0.4900953  0.49013698 0.49018764 0.4901569
 0.49004677 0.48985502 0.48969236 0.48961884 0.4896427  0.4897204
 0.4897867  0.4898831  0.48998958 0.49001184 0.48986748 0.48952964
 0.48901033 0.4883878  0.48775542 0.48716766 0.48666462 0.48627812
 0.48597643 0.4855481  0.48495695 0.4842628  0.48350546 0.48279276
 0.4821964  0.48173407 0.48128864 0.48074532 0.48012093 0.4795422
 0.4790598  0.4787341  0.4784932  0.478151   0.47784662 0.4775957
 0.47747737 0.47744974 0.4774048  0.47736284 0.47732502 0.4773954
 0.47761112 0.47787035 0.4781178  0.47840458 0.47865894 0.47886077
 0.47898883 0.47914073 0.47933817 0.4794721  0.4795167  0.47953063
 0.47955233 0.4796383  0.47982714 0.48009372 0.48047125 0.48088136
 0.48110697 0.4810163  0.48064995 0.48026508 0.4800515  0.4800541
 0.48021308 0.4803699  0.48040867 0.4802727  0.47999233 0.47969577
 0.4795476  0.47962242 0.47987816 0.48014933 0.48026884 0.48021874
 0.48004332 0.47990808 0.47993705 0.4800915  0.48029163 0.48052043
 0.4806225  0.48063144 0.4806336  0.48068655 0.4807317  0.48075622
 0.4807244  0.48068053 0.48065397 0.4806616  0.48066154 0.48058662
 0.4804282  0.48034686 0.4802993  0.4803007  0.48029467 0.4802666
 0.48026568 0.48037258 0.48064122 0.48091632 0.48097178 0.4806549
 0.48001134 0.47926188 0.47857857 0.478036   0.47761515 0.47730106
 0.4769511  0.47652102 0.4760142  0.47549066 0.47500616 0.474509
 0.474022   0.47350898 0.47298884 0.47244966 0.47191662 0.47134662
 0.4707597  0.47024074 0.4699474  0.4699121  0.47002754 0.47011352
 0.47006556 0.4697932  0.46943665 0.46919537 0.46924654 0.46953905
 0.47001553 0.47040993 0.4705984  0.47060275 0.47051695 0.4704989
 0.47056875 0.47061664 0.47068796 0.47076854 0.4709106  0.47110683
 0.47135735 0.47161612 0.47174862 0.4718423  0.47210562 0.4725016
 0.47277534 0.4728189  0.47259602 0.47223112 0.47203502 0.4721499
 0.4724288  0.47277313 0.47301295 0.4729773  0.4727709  0.47245327
 0.47209892 0.47182634 0.47165272 0.4713902  0.47106814 0.47069284
 0.47037116 0.47016233 0.4700722  0.47006437 0.47006077 0.47009876
 0.47015807 0.4701728  0.47023195 0.47033152 0.47038966 0.47034878
 0.47023126 0.47009906 0.4699995  0.46993887 0.46990362 0.4698969
 0.46998104 0.47026995 0.47067606 0.4709878  0.47117403 0.47119787
 0.47116637 0.47121435 0.47140777 0.47167316 0.47178942 0.47163764
 0.47127765 0.47092193 0.47062534 0.47029692 0.46988827 0.46943042
 0.4690188  0.4686743  0.46849713 0.46838292 0.46811315 0.46784967
 0.4674396  0.4670302  0.4665441  0.46604404 0.46551287 0.46501094
 0.46469483 0.46454614 0.4645009  0.46427524 0.4639242  0.46362078
 0.4634826  0.46368033 0.46406558 0.46443114 0.46470922 0.4647574
 0.46481013 0.46494433 0.46518075 0.46550164 0.46555698 0.46541998
 0.46534446 0.46561226 0.4662317  0.46685213 0.46725184 0.46732166
 0.46747392 0.46792957 0.46841833 0.46894553 0.46882853 0.46504885]
