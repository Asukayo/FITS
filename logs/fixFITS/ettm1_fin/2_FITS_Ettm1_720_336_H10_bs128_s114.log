Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21288960.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4169092
	speed: 0.1256s/iter; left time: 803.6815s
Epoch: 1 cost time: 16.188143968582153
Epoch: 1, Steps: 130 | Train Loss: 0.5036576 Vali Loss: 1.1057664 Test Loss: 0.6584966
Validation loss decreased (inf --> 1.105766).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3099348
	speed: 0.3400s/iter; left time: 2132.3580s
Epoch: 2 cost time: 16.611987590789795
Epoch: 2, Steps: 130 | Train Loss: 0.3442776 Vali Loss: 0.9496670 Test Loss: 0.5600790
Validation loss decreased (1.105766 --> 0.949667).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2689530
	speed: 0.3436s/iter; left time: 2109.7588s
Epoch: 3 cost time: 16.40046262741089
Epoch: 3, Steps: 130 | Train Loss: 0.2752918 Vali Loss: 0.8692534 Test Loss: 0.5104404
Validation loss decreased (0.949667 --> 0.869253).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2256418
	speed: 0.3427s/iter; left time: 2060.1214s
Epoch: 4 cost time: 16.283310174942017
Epoch: 4, Steps: 130 | Train Loss: 0.2355845 Vali Loss: 0.8173790 Test Loss: 0.4770394
Validation loss decreased (0.869253 --> 0.817379).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2034134
	speed: 0.3602s/iter; left time: 2118.1496s
Epoch: 5 cost time: 17.42138934135437
Epoch: 5, Steps: 130 | Train Loss: 0.2096427 Vali Loss: 0.7860469 Test Loss: 0.4549551
Validation loss decreased (0.817379 --> 0.786047).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1818157
	speed: 0.3469s/iter; left time: 1994.7982s
Epoch: 6 cost time: 17.2056725025177
Epoch: 6, Steps: 130 | Train Loss: 0.1915781 Vali Loss: 0.7641302 Test Loss: 0.4397941
Validation loss decreased (0.786047 --> 0.764130).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1741236
	speed: 0.3458s/iter; left time: 1943.7625s
Epoch: 7 cost time: 16.980479955673218
Epoch: 7, Steps: 130 | Train Loss: 0.1785496 Vali Loss: 0.7486191 Test Loss: 0.4280237
Validation loss decreased (0.764130 --> 0.748619).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1679803
	speed: 0.3467s/iter; left time: 1903.6735s
Epoch: 8 cost time: 17.379149436950684
Epoch: 8, Steps: 130 | Train Loss: 0.1687751 Vali Loss: 0.7346773 Test Loss: 0.4187638
Validation loss decreased (0.748619 --> 0.734677).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1601579
	speed: 0.3473s/iter; left time: 1861.6937s
Epoch: 9 cost time: 16.592189073562622
Epoch: 9, Steps: 130 | Train Loss: 0.1613191 Vali Loss: 0.7265545 Test Loss: 0.4113310
Validation loss decreased (0.734677 --> 0.726555).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1569834
	speed: 0.3418s/iter; left time: 1787.9557s
Epoch: 10 cost time: 16.47985863685608
Epoch: 10, Steps: 130 | Train Loss: 0.1553734 Vali Loss: 0.7164247 Test Loss: 0.4055387
Validation loss decreased (0.726555 --> 0.716425).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1532230
	speed: 0.3360s/iter; left time: 1713.8905s
Epoch: 11 cost time: 16.311789512634277
Epoch: 11, Steps: 130 | Train Loss: 0.1505582 Vali Loss: 0.7097269 Test Loss: 0.3997216
Validation loss decreased (0.716425 --> 0.709727).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1383377
	speed: 0.3357s/iter; left time: 1669.0045s
Epoch: 12 cost time: 16.244863033294678
Epoch: 12, Steps: 130 | Train Loss: 0.1465915 Vali Loss: 0.7072741 Test Loss: 0.3956444
Validation loss decreased (0.709727 --> 0.707274).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1403602
	speed: 0.3417s/iter; left time: 1654.1664s
Epoch: 13 cost time: 16.405351161956787
Epoch: 13, Steps: 130 | Train Loss: 0.1433888 Vali Loss: 0.7011302 Test Loss: 0.3918733
Validation loss decreased (0.707274 --> 0.701130).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1338342
	speed: 0.3521s/iter; left time: 1658.8037s
Epoch: 14 cost time: 16.65080213546753
Epoch: 14, Steps: 130 | Train Loss: 0.1407631 Vali Loss: 0.6973829 Test Loss: 0.3891518
Validation loss decreased (0.701130 --> 0.697383).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1307460
	speed: 0.3353s/iter; left time: 1536.1318s
Epoch: 15 cost time: 16.43507170677185
Epoch: 15, Steps: 130 | Train Loss: 0.1384539 Vali Loss: 0.6943788 Test Loss: 0.3864617
Validation loss decreased (0.697383 --> 0.694379).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1334468
	speed: 0.3372s/iter; left time: 1500.8408s
Epoch: 16 cost time: 16.63791584968567
Epoch: 16, Steps: 130 | Train Loss: 0.1365698 Vali Loss: 0.6903490 Test Loss: 0.3836034
Validation loss decreased (0.694379 --> 0.690349).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1304192
	speed: 0.3460s/iter; left time: 1495.0954s
Epoch: 17 cost time: 17.003015995025635
Epoch: 17, Steps: 130 | Train Loss: 0.1349385 Vali Loss: 0.6874774 Test Loss: 0.3818171
Validation loss decreased (0.690349 --> 0.687477).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1361332
	speed: 0.3541s/iter; left time: 1484.1590s
Epoch: 18 cost time: 17.29574418067932
Epoch: 18, Steps: 130 | Train Loss: 0.1335563 Vali Loss: 0.6855847 Test Loss: 0.3805333
Validation loss decreased (0.687477 --> 0.685585).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1337089
	speed: 0.3470s/iter; left time: 1409.2684s
Epoch: 19 cost time: 16.27207040786743
Epoch: 19, Steps: 130 | Train Loss: 0.1323915 Vali Loss: 0.6857436 Test Loss: 0.3786776
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1332314
	speed: 0.3577s/iter; left time: 1405.9604s
Epoch: 20 cost time: 18.154765844345093
Epoch: 20, Steps: 130 | Train Loss: 0.1313455 Vali Loss: 0.6834456 Test Loss: 0.3776683
Validation loss decreased (0.685585 --> 0.683446).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1299995
	speed: 0.3577s/iter; left time: 1359.6617s
Epoch: 21 cost time: 17.468283891677856
Epoch: 21, Steps: 130 | Train Loss: 0.1304196 Vali Loss: 0.6812548 Test Loss: 0.3765197
Validation loss decreased (0.683446 --> 0.681255).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1280302
	speed: 0.3572s/iter; left time: 1311.2927s
Epoch: 22 cost time: 16.6873300075531
Epoch: 22, Steps: 130 | Train Loss: 0.1296659 Vali Loss: 0.6808247 Test Loss: 0.3756317
Validation loss decreased (0.681255 --> 0.680825).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1353014
	speed: 0.3483s/iter; left time: 1233.1888s
Epoch: 23 cost time: 17.58939266204834
Epoch: 23, Steps: 130 | Train Loss: 0.1289091 Vali Loss: 0.6796357 Test Loss: 0.3750979
Validation loss decreased (0.680825 --> 0.679636).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1314382
	speed: 0.3783s/iter; left time: 1290.5072s
Epoch: 24 cost time: 19.34494948387146
Epoch: 24, Steps: 130 | Train Loss: 0.1283297 Vali Loss: 0.6795501 Test Loss: 0.3741141
Validation loss decreased (0.679636 --> 0.679550).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1289740
	speed: 0.3841s/iter; left time: 1260.2743s
Epoch: 25 cost time: 18.537192344665527
Epoch: 25, Steps: 130 | Train Loss: 0.1277956 Vali Loss: 0.6796427 Test Loss: 0.3736384
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1302368
	speed: 0.3634s/iter; left time: 1144.9214s
Epoch: 26 cost time: 15.757454633712769
Epoch: 26, Steps: 130 | Train Loss: 0.1273642 Vali Loss: 0.6775938 Test Loss: 0.3729467
Validation loss decreased (0.679550 --> 0.677594).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1296619
	speed: 0.3082s/iter; left time: 930.9524s
Epoch: 27 cost time: 15.940560340881348
Epoch: 27, Steps: 130 | Train Loss: 0.1268428 Vali Loss: 0.6768582 Test Loss: 0.3727237
Validation loss decreased (0.677594 --> 0.676858).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1223836
	speed: 0.4139s/iter; left time: 1196.7012s
Epoch: 28 cost time: 19.525797367095947
Epoch: 28, Steps: 130 | Train Loss: 0.1265267 Vali Loss: 0.6771647 Test Loss: 0.3722831
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1294411
	speed: 0.4060s/iter; left time: 1120.8314s
Epoch: 29 cost time: 20.208073616027832
Epoch: 29, Steps: 130 | Train Loss: 0.1261695 Vali Loss: 0.6757563 Test Loss: 0.3719933
Validation loss decreased (0.676858 --> 0.675756).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1178222
	speed: 0.4503s/iter; left time: 1184.8179s
Epoch: 30 cost time: 21.725579261779785
Epoch: 30, Steps: 130 | Train Loss: 0.1258035 Vali Loss: 0.6775308 Test Loss: 0.3715388
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1342220
	speed: 0.4536s/iter; left time: 1134.4613s
Epoch: 31 cost time: 22.102932929992676
Epoch: 31, Steps: 130 | Train Loss: 0.1256144 Vali Loss: 0.6762259 Test Loss: 0.3713026
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1229459
	speed: 0.4476s/iter; left time: 1061.3122s
Epoch: 32 cost time: 21.884963035583496
Epoch: 32, Steps: 130 | Train Loss: 0.1252889 Vali Loss: 0.6768991 Test Loss: 0.3710852
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21288960.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3469712
	speed: 0.1778s/iter; left time: 1137.8483s
Epoch: 1 cost time: 22.684453010559082
Epoch: 1, Steps: 130 | Train Loss: 0.3407744 Vali Loss: 0.6620717 Test Loss: 0.3676495
Validation loss decreased (inf --> 0.662072).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3212260
	speed: 0.4401s/iter; left time: 2759.7532s
Epoch: 2 cost time: 21.97662878036499
Epoch: 2, Steps: 130 | Train Loss: 0.3384918 Vali Loss: 0.6611583 Test Loss: 0.3678220
Validation loss decreased (0.662072 --> 0.661158).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3254921
	speed: 0.4586s/iter; left time: 2816.2395s
Epoch: 3 cost time: 21.7137451171875
Epoch: 3, Steps: 130 | Train Loss: 0.3379899 Vali Loss: 0.6583535 Test Loss: 0.3677459
Validation loss decreased (0.661158 --> 0.658354).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3163407
	speed: 0.4481s/iter; left time: 2693.3632s
Epoch: 4 cost time: 20.878763914108276
Epoch: 4, Steps: 130 | Train Loss: 0.3376044 Vali Loss: 0.6577156 Test Loss: 0.3675318
Validation loss decreased (0.658354 --> 0.657716).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3340071
	speed: 0.4363s/iter; left time: 2565.8119s
Epoch: 5 cost time: 21.283039808273315
Epoch: 5, Steps: 130 | Train Loss: 0.3371632 Vali Loss: 0.6579666 Test Loss: 0.3673646
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3483692
	speed: 0.4219s/iter; left time: 2426.3804s
Epoch: 6 cost time: 20.062824964523315
Epoch: 6, Steps: 130 | Train Loss: 0.3370296 Vali Loss: 0.6574796 Test Loss: 0.3671689
Validation loss decreased (0.657716 --> 0.657480).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3446881
	speed: 0.4336s/iter; left time: 2437.4126s
Epoch: 7 cost time: 20.784121990203857
Epoch: 7, Steps: 130 | Train Loss: 0.3371852 Vali Loss: 0.6548572 Test Loss: 0.3672658
Validation loss decreased (0.657480 --> 0.654857).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3363803
	speed: 0.4460s/iter; left time: 2449.0250s
Epoch: 8 cost time: 20.990461587905884
Epoch: 8, Steps: 130 | Train Loss: 0.3369425 Vali Loss: 0.6561046 Test Loss: 0.3673533
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3474477
	speed: 0.4076s/iter; left time: 2184.9544s
Epoch: 9 cost time: 17.981642723083496
Epoch: 9, Steps: 130 | Train Loss: 0.3369757 Vali Loss: 0.6558645 Test Loss: 0.3674176
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3345011
	speed: 0.3346s/iter; left time: 1750.1529s
Epoch: 10 cost time: 18.3032443523407
Epoch: 10, Steps: 130 | Train Loss: 0.3369282 Vali Loss: 0.6548806 Test Loss: 0.3672177
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3670755624771118, mae:0.38558506965637207, rse:0.5765354633331299, corr:[0.5372413  0.5467502  0.55355895 0.5562635  0.556439   0.556246
 0.5566752  0.5578094  0.5592046  0.5602068  0.56054354 0.56031346
 0.5599131  0.5595053  0.5589802  0.5581658  0.5569977  0.5555665
 0.5540163  0.5525865  0.5514368  0.5504657  0.54947513 0.54839426
 0.54703027 0.54557365 0.5443     0.5434515  0.5432766  0.5437145
 0.54446954 0.54522395 0.54565376 0.54580474 0.5457907  0.5459511
 0.5462206  0.5465054  0.54662114 0.54634607 0.54576916 0.54504544
 0.5444921  0.5443979  0.54468167 0.5450867  0.54540795 0.54544586
 0.54512507 0.5445523  0.5440243  0.5436941  0.5435818  0.5434862
 0.54331386 0.54303783 0.54271793 0.5424393  0.542346   0.54237604
 0.5424433  0.5424632  0.5423712  0.5421345  0.54194874 0.54196274
 0.5421885  0.54253626 0.5428711  0.5430472  0.54304284 0.54288626
 0.54263794 0.54240155 0.5422582  0.54215646 0.5420035  0.54178745
 0.541542   0.54133034 0.5411757  0.5410505  0.54091495 0.54068124
 0.5403469  0.5399243  0.5395454  0.5393213  0.53936815 0.5397424
 0.54034615 0.54097843 0.54142725 0.5415426  0.5412796  0.540729
 0.5401004  0.53964704 0.5393552  0.5392437  0.539248   0.53927773
 0.53927803 0.5391715  0.539039   0.5389708  0.53890574 0.53886586
 0.53875923 0.5385864  0.5383607  0.5381102  0.53788674 0.5377453
 0.537684   0.53767514 0.53765666 0.53754807 0.5373318  0.53707874
 0.5368261  0.5366074  0.53640044 0.53626156 0.536154   0.5359652
 0.5356312  0.53520566 0.5347449  0.53438777 0.53416926 0.53412336
 0.5342803  0.53456086 0.5348043  0.53497285 0.5351305  0.5352098
 0.5353001  0.5354553  0.5355915  0.5356983  0.535764   0.53578836
 0.53577995 0.53570545 0.5356347  0.53558195 0.5355762  0.5355505
 0.5354558  0.5353544  0.5352555  0.5351828  0.5351339  0.53517395
 0.535257   0.53531563 0.53539014 0.5354368  0.5355667  0.5357905
 0.5360906  0.5363764  0.5365821  0.53668433 0.53672117 0.536772
 0.53684586 0.536918   0.53704107 0.5371876  0.53725064 0.5372106
 0.53711444 0.5370376  0.5370399  0.53714365 0.5372761  0.5373828
 0.5373757  0.53722405 0.5368982  0.5365383  0.5362821  0.5362322
 0.5364377  0.53689176 0.53749025 0.53806084 0.53844887 0.53859407
 0.53846097 0.5381655  0.5376941  0.5370596  0.53632873 0.53559154
 0.53486747 0.53417885 0.53352505 0.5329283  0.532382   0.5318642
 0.5313724  0.53087825 0.5303554  0.5298335  0.52929556 0.5287079
 0.5280248  0.52727365 0.5264758  0.52567655 0.52485985 0.5241516
 0.5236438  0.52331567 0.5231771  0.5231454  0.5231706  0.52324766
 0.5233923  0.52347404 0.52350163 0.52350014 0.5234552  0.5234142
 0.5234165  0.52348465 0.52359885 0.52366865 0.5236989  0.52374023
 0.5238303  0.524042   0.5242525  0.5245422  0.52480763 0.5250271
 0.5251377  0.52503026 0.52477604 0.5245002  0.5243236  0.52421665
 0.52415067 0.5241079  0.5240362  0.5239597  0.5238318  0.5237426
 0.5236553  0.5236626  0.52370435 0.5237303  0.52374107 0.52374715
 0.52376986 0.5238443  0.5239422  0.5240502  0.5241602  0.524342
 0.524482   0.5245157  0.5244316  0.5242881  0.52412724 0.52395874
 0.5238763  0.52388763 0.5239895  0.5241495  0.52428883 0.52436376
 0.5243167  0.5242388  0.5241416  0.5241639  0.5242913  0.5244757
 0.5246807  0.5248142  0.52478164 0.5245824  0.52423066 0.5237375
 0.5231972  0.52277744 0.5225024  0.5222296  0.5218914  0.5214402
 0.520895   0.52022797 0.51955384 0.5188646  0.5181738  0.5175245
 0.5169119  0.5163796  0.516012   0.515799   0.515671   0.51549053
 0.5152923  0.51495934 0.5145193  0.51406986 0.5137914  0.5137044
 0.5137865  0.5139776  0.51407415 0.51399064 0.5137543  0.5135305
 0.5134302  0.5134174  0.5134487  0.5134482  0.5133248  0.51308477
 0.5128334  0.51276916 0.5128779  0.5130973  0.51320106 0.5130159
 0.51267415 0.5124141  0.5123915  0.512317   0.51094353 0.50567865]
