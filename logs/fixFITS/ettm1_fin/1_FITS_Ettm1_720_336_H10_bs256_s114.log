Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  42577920.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.06193232536316
Epoch: 1, Steps: 65 | Train Loss: 0.5687424 Vali Loss: 0.9376200 Test Loss: 0.5168509
Validation loss decreased (inf --> 0.937620).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.563207149505615
Epoch: 2, Steps: 65 | Train Loss: 0.4141571 Vali Loss: 0.8029016 Test Loss: 0.4297175
Validation loss decreased (0.937620 --> 0.802902).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.631420612335205
Epoch: 3, Steps: 65 | Train Loss: 0.3792247 Vali Loss: 0.7550467 Test Loss: 0.4009998
Validation loss decreased (0.802902 --> 0.755047).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.289997100830078
Epoch: 4, Steps: 65 | Train Loss: 0.3644307 Vali Loss: 0.7281530 Test Loss: 0.3860829
Validation loss decreased (0.755047 --> 0.728153).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.222668647766113
Epoch: 5, Steps: 65 | Train Loss: 0.3556367 Vali Loss: 0.7116392 Test Loss: 0.3771468
Validation loss decreased (0.728153 --> 0.711639).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.056230545043945
Epoch: 6, Steps: 65 | Train Loss: 0.3503768 Vali Loss: 0.6992106 Test Loss: 0.3720382
Validation loss decreased (0.711639 --> 0.699211).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 6.99482274055481
Epoch: 7, Steps: 65 | Train Loss: 0.3468566 Vali Loss: 0.6923366 Test Loss: 0.3691305
Validation loss decreased (0.699211 --> 0.692337).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 7.35559606552124
Epoch: 8, Steps: 65 | Train Loss: 0.3445427 Vali Loss: 0.6866991 Test Loss: 0.3675461
Validation loss decreased (0.692337 --> 0.686699).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.257927894592285
Epoch: 9, Steps: 65 | Train Loss: 0.3429329 Vali Loss: 0.6794661 Test Loss: 0.3665608
Validation loss decreased (0.686699 --> 0.679466).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.869623899459839
Epoch: 10, Steps: 65 | Train Loss: 0.3416473 Vali Loss: 0.6791663 Test Loss: 0.3660859
Validation loss decreased (0.679466 --> 0.679166).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.988580703735352
Epoch: 11, Steps: 65 | Train Loss: 0.3412652 Vali Loss: 0.6753092 Test Loss: 0.3658390
Validation loss decreased (0.679166 --> 0.675309).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.501683712005615
Epoch: 12, Steps: 65 | Train Loss: 0.3404876 Vali Loss: 0.6747438 Test Loss: 0.3660040
Validation loss decreased (0.675309 --> 0.674744).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.338367462158203
Epoch: 13, Steps: 65 | Train Loss: 0.3401138 Vali Loss: 0.6719797 Test Loss: 0.3658919
Validation loss decreased (0.674744 --> 0.671980).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.177804470062256
Epoch: 14, Steps: 65 | Train Loss: 0.3394566 Vali Loss: 0.6715083 Test Loss: 0.3659920
Validation loss decreased (0.671980 --> 0.671508).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.624514818191528
Epoch: 15, Steps: 65 | Train Loss: 0.3390959 Vali Loss: 0.6710669 Test Loss: 0.3660502
Validation loss decreased (0.671508 --> 0.671067).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.461316108703613
Epoch: 16, Steps: 65 | Train Loss: 0.3390157 Vali Loss: 0.6682688 Test Loss: 0.3662557
Validation loss decreased (0.671067 --> 0.668269).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.084687948226929
Epoch: 17, Steps: 65 | Train Loss: 0.3389590 Vali Loss: 0.6667897 Test Loss: 0.3662718
Validation loss decreased (0.668269 --> 0.666790).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.657802820205688
Epoch: 18, Steps: 65 | Train Loss: 0.3386585 Vali Loss: 0.6655725 Test Loss: 0.3663871
Validation loss decreased (0.666790 --> 0.665572).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 8.376266241073608
Epoch: 19, Steps: 65 | Train Loss: 0.3384524 Vali Loss: 0.6676180 Test Loss: 0.3664095
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 8.288860321044922
Epoch: 20, Steps: 65 | Train Loss: 0.3384346 Vali Loss: 0.6671797 Test Loss: 0.3665915
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.606124877929688
Epoch: 21, Steps: 65 | Train Loss: 0.3380951 Vali Loss: 0.6633341 Test Loss: 0.3666458
Validation loss decreased (0.665572 --> 0.663334).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.805333852767944
Epoch: 22, Steps: 65 | Train Loss: 0.3380306 Vali Loss: 0.6651367 Test Loss: 0.3666723
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.507005453109741
Epoch: 23, Steps: 65 | Train Loss: 0.3379537 Vali Loss: 0.6627997 Test Loss: 0.3666313
Validation loss decreased (0.663334 --> 0.662800).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 10.822238445281982
Epoch: 24, Steps: 65 | Train Loss: 0.3377502 Vali Loss: 0.6640914 Test Loss: 0.3667361
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 11.126219511032104
Epoch: 25, Steps: 65 | Train Loss: 0.3379438 Vali Loss: 0.6617770 Test Loss: 0.3667102
Validation loss decreased (0.662800 --> 0.661777).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.428683996200562
Epoch: 26, Steps: 65 | Train Loss: 0.3376937 Vali Loss: 0.6628066 Test Loss: 0.3667352
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 9.655420303344727
Epoch: 27, Steps: 65 | Train Loss: 0.3378648 Vali Loss: 0.6605196 Test Loss: 0.3667887
Validation loss decreased (0.661777 --> 0.660520).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.064976930618286
Epoch: 28, Steps: 65 | Train Loss: 0.3378695 Vali Loss: 0.6622819 Test Loss: 0.3668336
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.218059778213501
Epoch: 29, Steps: 65 | Train Loss: 0.3375935 Vali Loss: 0.6614742 Test Loss: 0.3668598
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 8.198338747024536
Epoch: 30, Steps: 65 | Train Loss: 0.3375576 Vali Loss: 0.6605998 Test Loss: 0.3668743
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.36626407504081726, mae:0.384952187538147, rse:0.5758978724479675, corr:[0.5376312  0.5464243  0.5530187  0.5556485  0.5559566  0.5561659
 0.55696744 0.5581214  0.55915636 0.5596838  0.55972004 0.55952805
 0.55946106 0.5594872  0.559271   0.5585412  0.55730104 0.55577487
 0.55425715 0.5530534  0.5522824  0.5516755  0.55087006 0.54973847
 0.5481929  0.5465705  0.5453275  0.544791   0.5451246  0.54604894
 0.5470707  0.54780155 0.54797786 0.54774797 0.5473586  0.5472397
 0.5473871  0.5476622  0.54779977 0.54752755 0.5469078  0.54610896
 0.5454511  0.5452225  0.5453461  0.5455832  0.5457496  0.5456848
 0.5453606  0.5449133  0.54462516 0.544611   0.54481804 0.5449927
 0.5449952  0.54477894 0.54440576 0.54399544 0.5437382  0.5436545
 0.5436939  0.54377484 0.543797   0.54369146 0.5435824  0.54357755
 0.54371494 0.5439651  0.5442436  0.5444326  0.5445035  0.5444701
 0.5443588  0.54424065 0.5441717  0.5441232  0.54402775 0.5438745
 0.5436798  0.5434866  0.5433194  0.54317284 0.5430366  0.54288
 0.5427212  0.5425512  0.5424232  0.5423652  0.5424276  0.54264444
 0.5429623  0.5432749  0.5434737  0.5435003  0.5433311  0.54299366
 0.54258454 0.5422307  0.5418781  0.54154956 0.54124165 0.54095155
 0.5407003  0.5404637  0.5402866  0.5401968  0.54008675 0.53994286
 0.53970253 0.53939813 0.5390776  0.53877324 0.53852594 0.53836644
 0.5382671  0.53818536 0.5380683  0.5378659  0.5375998  0.53735566
 0.5371738  0.5370761  0.53701293 0.53698766 0.53694236 0.53679055
 0.53652436 0.5362359  0.5359925  0.5358871  0.53587776 0.5359069
 0.53593504 0.53588355 0.5356787  0.5353857  0.5351499  0.53498787
 0.53499615 0.5351824  0.53541374 0.53561544 0.53574574 0.5358018
 0.5358104  0.5357729  0.53576666 0.5358108  0.5359212  0.53602505
 0.53606313 0.5360548  0.53598714 0.5358866  0.5357773  0.5357526
 0.5357862  0.5358266  0.535885   0.535893   0.5359299  0.53600633
 0.53613776 0.5362926  0.53645134 0.53659695 0.5367294  0.5368664
 0.5369581  0.53696233 0.53695196 0.5369535  0.5369442  0.5369357
 0.536944   0.5369561  0.5369494  0.536903   0.53679323 0.53665715
 0.5365255  0.5364516  0.5364286  0.53650916 0.53668714 0.5369135
 0.5371754  0.5374628  0.5377494  0.537961   0.5380449  0.5379799
 0.53773785 0.53740335 0.53694326 0.5363611  0.535714   0.53508085
 0.53447556 0.53391147 0.5333644  0.5328274  0.53227    0.5316683
 0.53104615 0.53042734 0.52983844 0.52933025 0.5288917  0.52847165
 0.52799714 0.5274549  0.52683955 0.52617186 0.5254447  0.524776
 0.52427286 0.52393585 0.5237815  0.5237218  0.523712   0.5237496
 0.52387756 0.5239981  0.52412015 0.5242451  0.5243219  0.5243568
 0.52436346 0.52435476 0.5243249  0.5242318  0.5241225  0.52406484
 0.52410257 0.5242898  0.52449566 0.5247797  0.5250428  0.5252733
 0.52541566 0.5253748  0.5252189  0.52507424 0.5250375  0.5250658
 0.5251038  0.5251086  0.5250219  0.5248771  0.52466756 0.5245108
 0.524394   0.524407   0.5244781  0.5245319  0.5245552  0.52454203
 0.52450943 0.5244997  0.5245075  0.52453256 0.5245714  0.5246862
 0.52477336 0.52479225 0.52473825 0.52465975 0.5245827  0.52449006
 0.5244413  0.5244214  0.5244231  0.5244456  0.5244519  0.524434
 0.5243514  0.5242965  0.524239   0.52427936 0.52437615 0.5244941
 0.524636   0.52476317 0.5248052  0.52475584 0.52457917 0.5242159
 0.5236983  0.5231575  0.522627   0.5220368  0.52143425 0.5208801
 0.52045137 0.52009565 0.5198342  0.5195131  0.51902676 0.5183689
 0.51759213 0.5168595  0.51636237 0.5161348  0.51606333 0.5159221
 0.51566994 0.515165   0.51447266 0.51377606 0.51334786 0.51323825
 0.5133781  0.5136115  0.51362455 0.51330817 0.51275    0.5122541
 0.5120522  0.5121349  0.5123968  0.5126165  0.51255727 0.51216745
 0.5116168  0.51127434 0.511303   0.5117133  0.5121789  0.5123182
 0.5121134  0.5118265  0.5118883  0.5124406  0.5124915  0.50988746]
