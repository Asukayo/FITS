Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4562596
	speed: 0.1077s/iter; left time: 1378.3045s
	iters: 200, epoch: 1 | loss: 0.3890638
	speed: 0.1045s/iter; left time: 1326.6639s
Epoch: 1 cost time: 27.349740266799927
Epoch: 1, Steps: 258 | Train Loss: 0.4813778 Vali Loss: 1.1713361 Test Loss: 0.5656516
Validation loss decreased (inf --> 1.171336).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3041724
	speed: 0.4123s/iter; left time: 5171.8096s
	iters: 200, epoch: 2 | loss: 0.2926376
	speed: 0.0992s/iter; left time: 1234.8582s
Epoch: 2 cost time: 26.17841863632202
Epoch: 2, Steps: 258 | Train Loss: 0.3120744 Vali Loss: 1.0545866 Test Loss: 0.4896765
Validation loss decreased (1.171336 --> 1.054587).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2608463
	speed: 0.4540s/iter; left time: 5577.0108s
	iters: 200, epoch: 3 | loss: 0.2498273
	speed: 0.0994s/iter; left time: 1211.5205s
Epoch: 3 cost time: 26.61204743385315
Epoch: 3, Steps: 258 | Train Loss: 0.2634577 Vali Loss: 1.0059866 Test Loss: 0.4587839
Validation loss decreased (1.054587 --> 1.005987).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2453034
	speed: 0.4362s/iter; left time: 5245.7438s
	iters: 200, epoch: 4 | loss: 0.2652143
	speed: 0.1024s/iter; left time: 1220.8508s
Epoch: 4 cost time: 27.416354179382324
Epoch: 4, Steps: 258 | Train Loss: 0.2409136 Vali Loss: 0.9802986 Test Loss: 0.4420092
Validation loss decreased (1.005987 --> 0.980299).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2482164
	speed: 0.4464s/iter; left time: 5253.2771s
	iters: 200, epoch: 5 | loss: 0.2337150
	speed: 0.1054s/iter; left time: 1229.4542s
Epoch: 5 cost time: 27.5081307888031
Epoch: 5, Steps: 258 | Train Loss: 0.2286616 Vali Loss: 0.9643949 Test Loss: 0.4316329
Validation loss decreased (0.980299 --> 0.964395).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2229728
	speed: 0.4697s/iter; left time: 5407.0710s
	iters: 200, epoch: 6 | loss: 0.2315278
	speed: 0.1039s/iter; left time: 1185.3154s
Epoch: 6 cost time: 28.676844358444214
Epoch: 6, Steps: 258 | Train Loss: 0.2213788 Vali Loss: 0.9555538 Test Loss: 0.4247303
Validation loss decreased (0.964395 --> 0.955554).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2222626
	speed: 0.4087s/iter; left time: 4598.7654s
	iters: 200, epoch: 7 | loss: 0.2221963
	speed: 0.0785s/iter; left time: 875.6166s
Epoch: 7 cost time: 21.322298049926758
Epoch: 7, Steps: 258 | Train Loss: 0.2167579 Vali Loss: 0.9481914 Test Loss: 0.4207241
Validation loss decreased (0.955554 --> 0.948191).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2340588
	speed: 0.4310s/iter; left time: 4738.7641s
	iters: 200, epoch: 8 | loss: 0.2299697
	speed: 0.1067s/iter; left time: 1162.4362s
Epoch: 8 cost time: 28.043780088424683
Epoch: 8, Steps: 258 | Train Loss: 0.2138786 Vali Loss: 0.9444327 Test Loss: 0.4180208
Validation loss decreased (0.948191 --> 0.944433).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2110227
	speed: 0.4877s/iter; left time: 5236.1924s
	iters: 200, epoch: 9 | loss: 0.2141840
	speed: 0.1128s/iter; left time: 1200.3467s
Epoch: 9 cost time: 30.025461435317993
Epoch: 9, Steps: 258 | Train Loss: 0.2119171 Vali Loss: 0.9413223 Test Loss: 0.4165522
Validation loss decreased (0.944433 --> 0.941322).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2223875
	speed: 0.4425s/iter; left time: 4637.1494s
	iters: 200, epoch: 10 | loss: 0.1985987
	speed: 0.1125s/iter; left time: 1167.3758s
Epoch: 10 cost time: 29.93595790863037
Epoch: 10, Steps: 258 | Train Loss: 0.2105434 Vali Loss: 0.9411968 Test Loss: 0.4159902
Validation loss decreased (0.941322 --> 0.941197).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1996138
	speed: 0.4797s/iter; left time: 4902.8314s
	iters: 200, epoch: 11 | loss: 0.2077828
	speed: 0.1075s/iter; left time: 1088.4509s
Epoch: 11 cost time: 28.40048384666443
Epoch: 11, Steps: 258 | Train Loss: 0.2096523 Vali Loss: 0.9391923 Test Loss: 0.4155627
Validation loss decreased (0.941197 --> 0.939192).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2112553
	speed: 0.5037s/iter; left time: 5018.8537s
	iters: 200, epoch: 12 | loss: 0.2165666
	speed: 0.1144s/iter; left time: 1128.1978s
Epoch: 12 cost time: 31.203217267990112
Epoch: 12, Steps: 258 | Train Loss: 0.2091205 Vali Loss: 0.9386914 Test Loss: 0.4155642
Validation loss decreased (0.939192 --> 0.938691).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1930981
	speed: 0.5196s/iter; left time: 5042.7415s
	iters: 200, epoch: 13 | loss: 0.2165103
	speed: 0.1163s/iter; left time: 1117.1391s
Epoch: 13 cost time: 30.984776496887207
Epoch: 13, Steps: 258 | Train Loss: 0.2087043 Vali Loss: 0.9383304 Test Loss: 0.4154786
Validation loss decreased (0.938691 --> 0.938330).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2066566
	speed: 0.4910s/iter; left time: 4638.0942s
	iters: 200, epoch: 14 | loss: 0.1984549
	speed: 0.0899s/iter; left time: 840.6837s
Epoch: 14 cost time: 24.683704614639282
Epoch: 14, Steps: 258 | Train Loss: 0.2083827 Vali Loss: 0.9387514 Test Loss: 0.4156850
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1999466
	speed: 0.4806s/iter; left time: 4415.9388s
	iters: 200, epoch: 15 | loss: 0.2157337
	speed: 0.1195s/iter; left time: 1086.0400s
Epoch: 15 cost time: 31.854177713394165
Epoch: 15, Steps: 258 | Train Loss: 0.2082275 Vali Loss: 0.9387967 Test Loss: 0.4162592
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1972649
	speed: 0.5439s/iter; left time: 4857.9133s
	iters: 200, epoch: 16 | loss: 0.1966497
	speed: 0.1170s/iter; left time: 1033.1002s
Epoch: 16 cost time: 31.384993314743042
Epoch: 16, Steps: 258 | Train Loss: 0.2080453 Vali Loss: 0.9379546 Test Loss: 0.4159872
Validation loss decreased (0.938330 --> 0.937955).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2064511
	speed: 0.5221s/iter; left time: 4528.2253s
	iters: 200, epoch: 17 | loss: 0.1918213
	speed: 0.1139s/iter; left time: 976.4387s
Epoch: 17 cost time: 30.880314350128174
Epoch: 17, Steps: 258 | Train Loss: 0.2079552 Vali Loss: 0.9393027 Test Loss: 0.4164840
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2169303
	speed: 0.4272s/iter; left time: 3595.0232s
	iters: 200, epoch: 18 | loss: 0.2076568
	speed: 0.0839s/iter; left time: 697.7214s
Epoch: 18 cost time: 23.04641032218933
Epoch: 18, Steps: 258 | Train Loss: 0.2078831 Vali Loss: 0.9374942 Test Loss: 0.4166954
Validation loss decreased (0.937955 --> 0.937494).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2204648
	speed: 0.4664s/iter; left time: 3804.7849s
	iters: 200, epoch: 19 | loss: 0.2050431
	speed: 0.0983s/iter; left time: 792.3384s
Epoch: 19 cost time: 26.380255937576294
Epoch: 19, Steps: 258 | Train Loss: 0.2078542 Vali Loss: 0.9381359 Test Loss: 0.4170767
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2084783
	speed: 0.4015s/iter; left time: 3171.4122s
	iters: 200, epoch: 20 | loss: 0.2042582
	speed: 0.0856s/iter; left time: 667.9280s
Epoch: 20 cost time: 23.7696373462677
Epoch: 20, Steps: 258 | Train Loss: 0.2077595 Vali Loss: 0.9393439 Test Loss: 0.4169916
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2014866
	speed: 0.4002s/iter; left time: 3057.5862s
	iters: 200, epoch: 21 | loss: 0.2016940
	speed: 0.0918s/iter; left time: 692.6240s
Epoch: 21 cost time: 24.8112473487854
Epoch: 21, Steps: 258 | Train Loss: 0.2078052 Vali Loss: 0.9386477 Test Loss: 0.4174704
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4228503
	speed: 0.1034s/iter; left time: 1323.3015s
	iters: 200, epoch: 1 | loss: 0.3740223
	speed: 0.0931s/iter; left time: 1182.2316s
Epoch: 1 cost time: 24.81639003753662
Epoch: 1, Steps: 258 | Train Loss: 0.3981708 Vali Loss: 0.9335805 Test Loss: 0.4174381
Validation loss decreased (inf --> 0.933581).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3877930
	speed: 0.3777s/iter; left time: 4737.7766s
	iters: 200, epoch: 2 | loss: 0.3451276
	speed: 0.0835s/iter; left time: 1039.0021s
Epoch: 2 cost time: 23.169821977615356
Epoch: 2, Steps: 258 | Train Loss: 0.3974453 Vali Loss: 0.9328578 Test Loss: 0.4165626
Validation loss decreased (0.933581 --> 0.932858).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3791595
	speed: 0.3902s/iter; left time: 4793.8427s
	iters: 200, epoch: 3 | loss: 0.3938678
	speed: 0.0945s/iter; left time: 1151.1339s
Epoch: 3 cost time: 25.086817741394043
Epoch: 3, Steps: 258 | Train Loss: 0.3972276 Vali Loss: 0.9329162 Test Loss: 0.4159862
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3923186
	speed: 0.4512s/iter; left time: 5427.1577s
	iters: 200, epoch: 4 | loss: 0.3864813
	speed: 0.1076s/iter; left time: 1283.7667s
Epoch: 4 cost time: 28.664926528930664
Epoch: 4, Steps: 258 | Train Loss: 0.3970394 Vali Loss: 0.9327118 Test Loss: 0.4165292
Validation loss decreased (0.932858 --> 0.932712).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3846666
	speed: 0.4711s/iter; left time: 5544.7970s
	iters: 200, epoch: 5 | loss: 0.3804567
	speed: 0.1115s/iter; left time: 1301.4853s
Epoch: 5 cost time: 29.152252674102783
Epoch: 5, Steps: 258 | Train Loss: 0.3969282 Vali Loss: 0.9316635 Test Loss: 0.4168534
Validation loss decreased (0.932712 --> 0.931664).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4253006
	speed: 0.4762s/iter; left time: 5481.8728s
	iters: 200, epoch: 6 | loss: 0.4082317
	speed: 0.1090s/iter; left time: 1243.8588s
Epoch: 6 cost time: 28.866671323776245
Epoch: 6, Steps: 258 | Train Loss: 0.3967664 Vali Loss: 0.9314119 Test Loss: 0.4164421
Validation loss decreased (0.931664 --> 0.931412).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3926942
	speed: 0.4648s/iter; left time: 5229.8881s
	iters: 200, epoch: 7 | loss: 0.4114366
	speed: 0.1059s/iter; left time: 1180.7193s
Epoch: 7 cost time: 27.71861696243286
Epoch: 7, Steps: 258 | Train Loss: 0.3967767 Vali Loss: 0.9309708 Test Loss: 0.4168022
Validation loss decreased (0.931412 --> 0.930971).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3867888
	speed: 0.4429s/iter; left time: 4870.0856s
	iters: 200, epoch: 8 | loss: 0.3910033
	speed: 0.1058s/iter; left time: 1152.2523s
Epoch: 8 cost time: 28.432604789733887
Epoch: 8, Steps: 258 | Train Loss: 0.3967657 Vali Loss: 0.9310141 Test Loss: 0.4166985
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3720419
	speed: 0.4691s/iter; left time: 5037.1032s
	iters: 200, epoch: 9 | loss: 0.4155293
	speed: 0.1043s/iter; left time: 1109.2097s
Epoch: 9 cost time: 29.181753396987915
Epoch: 9, Steps: 258 | Train Loss: 0.3967023 Vali Loss: 0.9297007 Test Loss: 0.4167888
Validation loss decreased (0.930971 --> 0.929701).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3717964
	speed: 0.5015s/iter; left time: 5254.8508s
	iters: 200, epoch: 10 | loss: 0.3967582
	speed: 0.1186s/iter; left time: 1231.0153s
Epoch: 10 cost time: 30.784480810165405
Epoch: 10, Steps: 258 | Train Loss: 0.3966654 Vali Loss: 0.9323422 Test Loss: 0.4168390
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3714106
	speed: 0.5053s/iter; left time: 5164.2027s
	iters: 200, epoch: 11 | loss: 0.4024594
	speed: 0.1131s/iter; left time: 1144.8353s
Epoch: 11 cost time: 30.38979148864746
Epoch: 11, Steps: 258 | Train Loss: 0.3965991 Vali Loss: 0.9300491 Test Loss: 0.4165218
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4145168
	speed: 0.5012s/iter; left time: 4993.5357s
	iters: 200, epoch: 12 | loss: 0.4217287
	speed: 0.1049s/iter; left time: 1034.8077s
Epoch: 12 cost time: 28.56725835800171
Epoch: 12, Steps: 258 | Train Loss: 0.3966215 Vali Loss: 0.9302554 Test Loss: 0.4169548
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41571617126464844, mae:0.4118068814277649, rse:0.6134359240531921, corr:[0.52258486 0.5321678  0.53570926 0.5365927  0.5381957  0.54079443
 0.5426096  0.5427247  0.54211843 0.5420457  0.5428633  0.54374963
 0.5439195  0.5433295  0.54243976 0.54157305 0.5406841  0.5395895
 0.5381964  0.5366997  0.53527033 0.53380525 0.5321688  0.5305599
 0.52907354 0.52813333 0.5277529  0.5275327  0.5272997  0.52704674
 0.52702355 0.52755153 0.5284208  0.5292232  0.52938575 0.5291068
 0.52855825 0.52812815 0.5279265  0.52774996 0.5275702  0.52735937
 0.52730334 0.5275005  0.52767533 0.52760774 0.52737653 0.5271485
 0.52705574 0.52708584 0.52719855 0.52719176 0.5270673  0.5267999
 0.5265817  0.5264592  0.5263544  0.52616143 0.52601296 0.5259127
 0.52591634 0.5259323  0.5258324  0.5255229  0.5253135  0.5254484
 0.5258712  0.52634877 0.5266941  0.5267867  0.5267453  0.52662224
 0.52642184 0.5261254  0.52582926 0.5255681  0.52537453 0.52525765
 0.52512604 0.5248929  0.52454895 0.52420294 0.52404207 0.524102
 0.52437204 0.5246121  0.5246835  0.5245413  0.52434427 0.5243568
 0.52466106 0.52516174 0.5256897  0.52605236 0.5261615  0.52594197
 0.525459   0.524872   0.5242053  0.52371484 0.52349    0.5235324
 0.5237461  0.52391607 0.5239906  0.5240252  0.5239596  0.52388227
 0.52374786 0.5235969  0.52343965 0.52325964 0.52305263 0.5228135
 0.5224715  0.5220231  0.521534   0.5211002  0.5208423  0.52083606
 0.5209369  0.5209436  0.52069944 0.5203563  0.5200794  0.519903
 0.51980436 0.5197603  0.51969707 0.5196939  0.5197327  0.51979184
 0.51984584 0.51980245 0.51958585 0.51936084 0.51934594 0.5194397
 0.5195746  0.5196226  0.5194348  0.5191508  0.5190091  0.5191386
 0.5194674  0.5197416  0.5198565  0.5197725  0.5196058  0.51943123
 0.5193495  0.51949227 0.5197491  0.5199058  0.5197876  0.51950055
 0.5192018  0.5190578  0.5191697  0.51931375 0.519374   0.51925874
 0.51906514 0.51899034 0.51920193 0.5196474  0.5200867  0.5203008
 0.52020997 0.5199707  0.5199157  0.520122   0.5203394  0.5203738
 0.5201925  0.5199432  0.5198389  0.5199636  0.52016866 0.5202741
 0.52015316 0.5199037  0.51968354 0.5196568  0.5197796  0.519928
 0.5200459  0.5202646  0.52067995 0.52122396 0.52169085 0.5218353
 0.52157265 0.5210795  0.5205258  0.5200878  0.51979977 0.51952386
 0.5190116  0.518182   0.5171084  0.51604456 0.5151871  0.5146299
 0.51427174 0.5139002  0.5133728  0.5127573  0.5121744  0.5117175
 0.5113471  0.51100576 0.5105542  0.50994587 0.50918025 0.5084658
 0.50793254 0.5074754  0.50702053 0.5065051  0.50607854 0.5059881
 0.50636166 0.5069332  0.5074749  0.50777036 0.50770193 0.50742227
 0.5071985  0.5072245  0.50749725 0.50777435 0.50787866 0.50778836
 0.5076026  0.50754005 0.5076062  0.5078817  0.5081308  0.5082554
 0.50819147 0.50797546 0.5078485  0.5079386  0.5081821  0.5083509
 0.50833124 0.5081835  0.50801    0.5079294  0.5078631  0.5078107
 0.5076434  0.5074955  0.50740886 0.5074322  0.50756127 0.50770473
 0.5077588  0.5076968  0.50752926 0.5073414  0.5072304  0.50733453
 0.50751346 0.5076756  0.5078018  0.5079699  0.5081976  0.50841707
 0.50857687 0.5085162  0.5081853  0.5076705  0.50715786 0.50687957
 0.50691545 0.5073084  0.50779235 0.50819486 0.5083063  0.50809455
 0.5077791  0.50751823 0.5074148  0.50750726 0.5076601  0.5076785
 0.5074506  0.5070118  0.5064171  0.50571185 0.5050271  0.5044021
 0.5038087  0.5031464  0.5025086  0.50196284 0.50160176 0.501442
 0.501303   0.50103915 0.50061303 0.5000994  0.49967986 0.4994669
 0.4995664  0.4997321  0.49973264 0.4994404  0.498958   0.4984149
 0.49799955 0.49786648 0.49789286 0.4979438  0.49791643 0.49783456
 0.49772558 0.49760494 0.49752125 0.49749267 0.4975097  0.49754268
 0.49757376 0.49761873 0.4975781  0.49743685 0.49718988 0.4969037
 0.49674833 0.49670738 0.49668708 0.49665537 0.4966012  0.4966849
 0.49686697 0.49695432 0.49699283 0.49695805 0.49690643 0.4968731
 0.49687603 0.4969937  0.49712485 0.49717775 0.4970949  0.49690592
 0.4966266  0.4964304  0.49637347 0.4963755  0.49641275 0.4964295
 0.49639362 0.49638566 0.49642536 0.49648815 0.49658516 0.4966765
 0.49666572 0.49659723 0.49646342 0.49630168 0.4962089  0.49624854
 0.4963325  0.49647182 0.49657863 0.49653706 0.4964059  0.4962442
 0.49611723 0.4960932  0.49617058 0.49634466 0.49652255 0.4967089
 0.49693593 0.49724382 0.4976334  0.49798548 0.4981795  0.49817175
 0.49794987 0.4975632  0.4971308  0.49671203 0.49631763 0.49606943
 0.4958645  0.4956198  0.49529573 0.4949596  0.49457368 0.4941792
 0.49376985 0.4933495  0.49289954 0.49239767 0.49183768 0.49137837
 0.4910222  0.4908134  0.49063376 0.49048573 0.4903345  0.4901719
 0.48990566 0.489544   0.48915717 0.48892394 0.48900265 0.48932463
 0.48985907 0.49019888 0.49029222 0.49012473 0.4899594  0.4900119
 0.49017587 0.49037233 0.49040768 0.49031344 0.4901897  0.49027014
 0.49052447 0.49083504 0.4909527  0.49090338 0.49070895 0.49066415
 0.49079353 0.4909905  0.49117374 0.49124002 0.49108484 0.49075106
 0.49036926 0.49009302 0.48997226 0.48991442 0.48980913 0.48963645
 0.48946163 0.48946536 0.48967406 0.48995665 0.49012    0.48999065
 0.48969445 0.4894248  0.48939383 0.4895865  0.48984492 0.49002582
 0.4899412  0.48970118 0.48949876 0.48956457 0.48983398 0.49024528
 0.4905856  0.49078402 0.49076012 0.49063867 0.490527   0.4904516
 0.49045625 0.49043497 0.49038684 0.4903089  0.49025795 0.4902587
 0.49026766 0.49030367 0.4903242  0.49018672 0.48982483 0.4892925
 0.48870227 0.4881679  0.48772502 0.48729575 0.4868243  0.48633936
 0.48590228 0.48540056 0.48484102 0.48426357 0.48363146 0.4829745
 0.4823286  0.48176378 0.48126498 0.4807858  0.48033914 0.47996575
 0.4795977  0.47921947 0.4787787  0.4781734  0.47768176 0.477383
 0.47733518 0.4774     0.47739306 0.4773301  0.47725764 0.47732532
 0.47758994 0.47793797 0.4783034  0.47874412 0.47915426 0.47945136
 0.4795527  0.4795424  0.47946894 0.4793033  0.47914463 0.47914466
 0.47935095 0.47972965 0.4801712  0.4805219  0.48080984 0.48105133
 0.48118314 0.48117352 0.48104572 0.4809492  0.48092583 0.48091304
 0.4808787  0.480826   0.4808147  0.48083335 0.48081127 0.48069525
 0.4805263  0.48039696 0.48040655 0.4805203  0.48062068 0.48064768
 0.48055065 0.4804334  0.4804307  0.4805351  0.48068744 0.48086005
 0.48086146 0.48072815 0.48060486 0.48061156 0.48068973 0.48077455
 0.48077166 0.48072353 0.4807078  0.48079824 0.48096454 0.48108223
 0.48102787 0.48090202 0.48069996 0.4805974  0.4806787  0.48094183
 0.4812826  0.48154944 0.4816821  0.48159152 0.48128662 0.48086962
 0.4804903  0.4802038  0.47987437 0.4793554  0.47864062 0.47794527
 0.47737932 0.47699714 0.47670445 0.47637048 0.47591802 0.47531718
 0.4746997  0.47410646 0.47354105 0.47292566 0.47227424 0.4716119
 0.47104663 0.47064215 0.47040656 0.47022778 0.47001842 0.4697829
 0.4696535  0.46961895 0.46964934 0.4696482  0.4696015  0.46951538
 0.46957973 0.46974933 0.46993712 0.47004735 0.4700249  0.4699945
 0.47006416 0.4702161  0.47045863 0.4706583  0.47079676 0.47091937
 0.47115558 0.47153154 0.4718299  0.47194377 0.47197023 0.4719741
 0.47195208 0.47200957 0.4721067  0.47215903 0.47219458 0.4722248
 0.47222996 0.47235706 0.47260427 0.47278965 0.47285518 0.4726733
 0.47223854 0.47175393 0.47142875 0.4712342  0.47124663 0.47136882
 0.47149318 0.47151238 0.47144473 0.4713852  0.4713711  0.47148722
 0.47171557 0.471922   0.47202784 0.47192678 0.47163028 0.4713045
 0.47116703 0.47132608 0.47166556 0.47192177 0.4718867  0.47156256
 0.4711491  0.47092947 0.47092852 0.47096843 0.47098628 0.47095388
 0.47099254 0.47123125 0.47166246 0.4720785  0.4721841  0.47189304
 0.47134966 0.4708233  0.47040838 0.47002262 0.4695744  0.46905768
 0.46855235 0.46807972 0.46773273 0.46742883 0.46700782 0.46664786
 0.46629    0.4660622  0.46582925 0.46552005 0.46503243 0.46443605
 0.46397117 0.46376684 0.4637928  0.46372783 0.463528   0.46330118
 0.4631744  0.46331793 0.46357936 0.46373263 0.4637511  0.46363035
 0.4636533  0.46387085 0.46423763 0.4647437  0.46521574 0.46576723
 0.46640587 0.46703428 0.46749693 0.46773434 0.46795154 0.4681713
 0.46839136 0.46844798 0.46828398 0.46849373 0.46868747 0.46515876]
