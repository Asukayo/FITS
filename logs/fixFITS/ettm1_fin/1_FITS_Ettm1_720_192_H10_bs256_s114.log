Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=90, out_features=114, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  36771840.0
params:  10374.0
Trainable parameters:  10374
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.886380910873413
Epoch: 1, Steps: 65 | Train Loss: 0.4918526 Vali Loss: 0.7387146 Test Loss: 0.4171129
Validation loss decreased (inf --> 0.738715).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.441204309463501
Epoch: 2, Steps: 65 | Train Loss: 0.3511907 Vali Loss: 0.6191614 Test Loss: 0.3618607
Validation loss decreased (0.738715 --> 0.619161).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.033252954483032
Epoch: 3, Steps: 65 | Train Loss: 0.3255307 Vali Loss: 0.5818712 Test Loss: 0.3494691
Validation loss decreased (0.619161 --> 0.581871).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.860404014587402
Epoch: 4, Steps: 65 | Train Loss: 0.3158907 Vali Loss: 0.5650222 Test Loss: 0.3453166
Validation loss decreased (0.581871 --> 0.565022).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.81703233718872
Epoch: 5, Steps: 65 | Train Loss: 0.3108565 Vali Loss: 0.5545078 Test Loss: 0.3434904
Validation loss decreased (0.565022 --> 0.554508).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.830610036849976
Epoch: 6, Steps: 65 | Train Loss: 0.3078944 Vali Loss: 0.5473270 Test Loss: 0.3423382
Validation loss decreased (0.554508 --> 0.547327).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.083654642105103
Epoch: 7, Steps: 65 | Train Loss: 0.3057946 Vali Loss: 0.5425691 Test Loss: 0.3416395
Validation loss decreased (0.547327 --> 0.542569).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.438787460327148
Epoch: 8, Steps: 65 | Train Loss: 0.3043780 Vali Loss: 0.5389084 Test Loss: 0.3410651
Validation loss decreased (0.542569 --> 0.538908).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.16487717628479
Epoch: 9, Steps: 65 | Train Loss: 0.3034767 Vali Loss: 0.5360074 Test Loss: 0.3407780
Validation loss decreased (0.538908 --> 0.536007).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.2272047996521
Epoch: 10, Steps: 65 | Train Loss: 0.3025207 Vali Loss: 0.5327566 Test Loss: 0.3404436
Validation loss decreased (0.536007 --> 0.532757).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.497997999191284
Epoch: 11, Steps: 65 | Train Loss: 0.3020438 Vali Loss: 0.5314845 Test Loss: 0.3401296
Validation loss decreased (0.532757 --> 0.531484).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.371145248413086
Epoch: 12, Steps: 65 | Train Loss: 0.3013720 Vali Loss: 0.5286903 Test Loss: 0.3399393
Validation loss decreased (0.531484 --> 0.528690).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.95451021194458
Epoch: 13, Steps: 65 | Train Loss: 0.3010429 Vali Loss: 0.5280076 Test Loss: 0.3397024
Validation loss decreased (0.528690 --> 0.528008).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.039958477020264
Epoch: 14, Steps: 65 | Train Loss: 0.3007609 Vali Loss: 0.5275949 Test Loss: 0.3396764
Validation loss decreased (0.528008 --> 0.527595).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.673230171203613
Epoch: 15, Steps: 65 | Train Loss: 0.3006092 Vali Loss: 0.5255157 Test Loss: 0.3394434
Validation loss decreased (0.527595 --> 0.525516).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.476728439331055
Epoch: 16, Steps: 65 | Train Loss: 0.3002952 Vali Loss: 0.5244350 Test Loss: 0.3392896
Validation loss decreased (0.525516 --> 0.524435).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.028011560440063
Epoch: 17, Steps: 65 | Train Loss: 0.2999917 Vali Loss: 0.5236220 Test Loss: 0.3393405
Validation loss decreased (0.524435 --> 0.523622).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 7.658385992050171
Epoch: 18, Steps: 65 | Train Loss: 0.2998782 Vali Loss: 0.5237753 Test Loss: 0.3392359
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 8.132740259170532
Epoch: 19, Steps: 65 | Train Loss: 0.2995798 Vali Loss: 0.5225359 Test Loss: 0.3392206
Validation loss decreased (0.523622 --> 0.522536).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 8.912940740585327
Epoch: 20, Steps: 65 | Train Loss: 0.2996100 Vali Loss: 0.5221589 Test Loss: 0.3392420
Validation loss decreased (0.522536 --> 0.522159).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.006014347076416
Epoch: 21, Steps: 65 | Train Loss: 0.2993303 Vali Loss: 0.5220213 Test Loss: 0.3390810
Validation loss decreased (0.522159 --> 0.522021).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.292332887649536
Epoch: 22, Steps: 65 | Train Loss: 0.2991199 Vali Loss: 0.5214186 Test Loss: 0.3389799
Validation loss decreased (0.522021 --> 0.521419).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.478174209594727
Epoch: 23, Steps: 65 | Train Loss: 0.2991816 Vali Loss: 0.5211141 Test Loss: 0.3389818
Validation loss decreased (0.521419 --> 0.521114).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 10.255963802337646
Epoch: 24, Steps: 65 | Train Loss: 0.2991663 Vali Loss: 0.5207379 Test Loss: 0.3388284
Validation loss decreased (0.521114 --> 0.520738).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.997846126556396
Epoch: 25, Steps: 65 | Train Loss: 0.2992322 Vali Loss: 0.5205140 Test Loss: 0.3388095
Validation loss decreased (0.520738 --> 0.520514).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 9.786908626556396
Epoch: 26, Steps: 65 | Train Loss: 0.2987908 Vali Loss: 0.5197107 Test Loss: 0.3389058
Validation loss decreased (0.520514 --> 0.519711).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 9.100704431533813
Epoch: 27, Steps: 65 | Train Loss: 0.2991321 Vali Loss: 0.5196538 Test Loss: 0.3389466
Validation loss decreased (0.519711 --> 0.519654).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.457998752593994
Epoch: 28, Steps: 65 | Train Loss: 0.2989026 Vali Loss: 0.5187507 Test Loss: 0.3389202
Validation loss decreased (0.519654 --> 0.518751).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.642991065979004
Epoch: 29, Steps: 65 | Train Loss: 0.2986265 Vali Loss: 0.5195265 Test Loss: 0.3387955
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 8.647815704345703
Epoch: 30, Steps: 65 | Train Loss: 0.2988889 Vali Loss: 0.5190876 Test Loss: 0.3387874
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 8.483294010162354
Epoch: 31, Steps: 65 | Train Loss: 0.2989302 Vali Loss: 0.5186775 Test Loss: 0.3388463
Validation loss decreased (0.518751 --> 0.518677).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.5360107421875
Epoch: 32, Steps: 65 | Train Loss: 0.2987357 Vali Loss: 0.5185232 Test Loss: 0.3387930
Validation loss decreased (0.518677 --> 0.518523).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 7.191873788833618
Epoch: 33, Steps: 65 | Train Loss: 0.2986612 Vali Loss: 0.5177712 Test Loss: 0.3387178
Validation loss decreased (0.518523 --> 0.517771).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 9.20705509185791
Epoch: 34, Steps: 65 | Train Loss: 0.2987890 Vali Loss: 0.5176947 Test Loss: 0.3387448
Validation loss decreased (0.517771 --> 0.517695).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 5.863430976867676
Epoch: 35, Steps: 65 | Train Loss: 0.2984961 Vali Loss: 0.5175434 Test Loss: 0.3387765
Validation loss decreased (0.517695 --> 0.517543).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.96798586845398
Epoch: 36, Steps: 65 | Train Loss: 0.2984804 Vali Loss: 0.5181109 Test Loss: 0.3387187
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 8.764313220977783
Epoch: 37, Steps: 65 | Train Loss: 0.2986209 Vali Loss: 0.5175912 Test Loss: 0.3387702
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 8.682967901229858
Epoch: 38, Steps: 65 | Train Loss: 0.2984550 Vali Loss: 0.5176043 Test Loss: 0.3386986
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.33850952982902527, mae:0.36854761838912964, rse:0.5538433790206909, corr:[0.541379   0.5507267  0.5573299  0.55992067 0.56043416 0.56091845
 0.5619861  0.56344384 0.56480443 0.5656109  0.5658558  0.56576985
 0.565685   0.5656075  0.5653246  0.56463474 0.56352246 0.5621408
 0.5606912  0.55938864 0.5583416  0.55739295 0.55633175 0.55513334
 0.5536711  0.5522161  0.55109304 0.5504629  0.55046713 0.5509592
 0.55164754 0.55228674 0.55262643 0.55274606 0.552712   0.5527731
 0.5528112  0.552773   0.5525653  0.5520621  0.551454   0.55090034
 0.5505941  0.5506447  0.5508329  0.55091786 0.55085206 0.5506168
 0.5502352  0.5498368  0.5496337  0.5496447  0.54978615 0.54983956
 0.54974633 0.5495413  0.5493281  0.54915    0.5491099  0.5490917
 0.5490406  0.54892933 0.548768   0.5485553  0.5484744  0.54860926
 0.54889673 0.5492113  0.549447   0.54951495 0.5494622  0.54933417
 0.5491859  0.5490732  0.5490514  0.5490424  0.5489746  0.5488264
 0.5486116  0.5483807  0.54818827 0.54804105 0.5479477  0.5478427
 0.5477204  0.5475398  0.5473375  0.54713875 0.5470291  0.54711896
 0.54738957 0.5477342  0.5480366  0.54818517 0.5481079  0.54777867
 0.547303   0.5468506  0.54641044 0.54605764 0.5458303  0.545729
 0.54572225 0.5457014  0.5456597  0.54561406 0.54545355 0.54521745
 0.54491097 0.5446115  0.54435754 0.54413605 0.54392606 0.5437101
 0.5434528  0.5431437  0.54279804 0.5424281  0.5421099  0.5419458
 0.5419242  0.5419466  0.5418573  0.5416373  0.5412958  0.5408606
 0.54045194 0.5402351  0.5402268  0.54037833 0.5404867  0.5404234
 0.5402298  0.53998226 0.5397354  0.53963083 0.53977185 0.53997195
 0.54017717 0.5403361  0.5403533  0.54032594 0.54040027 0.5406536
 0.5410202  0.541287   0.5413803  0.54127324 0.54107124 0.54083806
 0.54065776 0.5406649  0.5407926  0.5409206  0.54092366 0.5408526
 0.54071796 0.5405401  0.5404638  0.540441   0.54054505 0.5406853
 0.54079264 0.54078496 0.5407023  0.5406348  0.54069203 0.54089683
 0.54113793 0.541257   0.54126227 0.54115534 0.5408636  0.5404611
 0.54007375 0.53981304 0.5396808  0.53960943 0.53940547 0.5390252
 0.5384809  0.5379394  0.5375061  0.5373726  0.5374919  0.5376111
 0.53743076 0.5371625  0.537272   0.53801686 0.5387799  0.5379472 ]
