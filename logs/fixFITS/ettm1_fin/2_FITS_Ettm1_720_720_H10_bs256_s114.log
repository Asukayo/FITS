Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  58060800.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 13.605829954147339
Epoch: 1, Steps: 64 | Train Loss: 0.6452867 Vali Loss: 1.5125324 Test Loss: 0.7873295
Validation loss decreased (inf --> 1.512532).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 14.552066326141357
Epoch: 2, Steps: 64 | Train Loss: 0.4920551 Vali Loss: 1.3386682 Test Loss: 0.6733871
Validation loss decreased (1.512532 --> 1.338668).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 14.082206726074219
Epoch: 3, Steps: 64 | Train Loss: 0.4182513 Vali Loss: 1.2453864 Test Loss: 0.6132791
Validation loss decreased (1.338668 --> 1.245386).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 15.62846064567566
Epoch: 4, Steps: 64 | Train Loss: 0.3749229 Vali Loss: 1.1850095 Test Loss: 0.5752353
Validation loss decreased (1.245386 --> 1.185009).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 20.543519973754883
Epoch: 5, Steps: 64 | Train Loss: 0.3461205 Vali Loss: 1.1445029 Test Loss: 0.5490400
Validation loss decreased (1.185009 --> 1.144503).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 18.281614780426025
Epoch: 6, Steps: 64 | Train Loss: 0.3252098 Vali Loss: 1.1138839 Test Loss: 0.5298108
Validation loss decreased (1.144503 --> 1.113884).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 20.700398921966553
Epoch: 7, Steps: 64 | Train Loss: 0.3095047 Vali Loss: 1.0896912 Test Loss: 0.5151506
Validation loss decreased (1.113884 --> 1.089691).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 20.191566944122314
Epoch: 8, Steps: 64 | Train Loss: 0.2970352 Vali Loss: 1.0718232 Test Loss: 0.5038129
Validation loss decreased (1.089691 --> 1.071823).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 18.554384231567383
Epoch: 9, Steps: 64 | Train Loss: 0.2869031 Vali Loss: 1.0585817 Test Loss: 0.4950809
Validation loss decreased (1.071823 --> 1.058582).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 18.163713932037354
Epoch: 10, Steps: 64 | Train Loss: 0.2786019 Vali Loss: 1.0453169 Test Loss: 0.4877177
Validation loss decreased (1.058582 --> 1.045317).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 17.65165638923645
Epoch: 11, Steps: 64 | Train Loss: 0.2718505 Vali Loss: 1.0340722 Test Loss: 0.4809796
Validation loss decreased (1.045317 --> 1.034072).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 15.868433237075806
Epoch: 12, Steps: 64 | Train Loss: 0.2660771 Vali Loss: 1.0268364 Test Loss: 0.4759588
Validation loss decreased (1.034072 --> 1.026836).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 14.89846134185791
Epoch: 13, Steps: 64 | Train Loss: 0.2611571 Vali Loss: 1.0198497 Test Loss: 0.4718943
Validation loss decreased (1.026836 --> 1.019850).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 17.034591913223267
Epoch: 14, Steps: 64 | Train Loss: 0.2570434 Vali Loss: 1.0121938 Test Loss: 0.4681282
Validation loss decreased (1.019850 --> 1.012194).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 17.11374855041504
Epoch: 15, Steps: 64 | Train Loss: 0.2533676 Vali Loss: 1.0075772 Test Loss: 0.4648933
Validation loss decreased (1.012194 --> 1.007577).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 16.54665231704712
Epoch: 16, Steps: 64 | Train Loss: 0.2503564 Vali Loss: 1.0040028 Test Loss: 0.4620489
Validation loss decreased (1.007577 --> 1.004003).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 16.712457180023193
Epoch: 17, Steps: 64 | Train Loss: 0.2475071 Vali Loss: 0.9999553 Test Loss: 0.4598032
Validation loss decreased (1.004003 --> 0.999955).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 16.16505217552185
Epoch: 18, Steps: 64 | Train Loss: 0.2452314 Vali Loss: 0.9961925 Test Loss: 0.4576037
Validation loss decreased (0.999955 --> 0.996192).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 14.605920314788818
Epoch: 19, Steps: 64 | Train Loss: 0.2431071 Vali Loss: 0.9930726 Test Loss: 0.4557197
Validation loss decreased (0.996192 --> 0.993073).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 15.106735706329346
Epoch: 20, Steps: 64 | Train Loss: 0.2412919 Vali Loss: 0.9899523 Test Loss: 0.4538047
Validation loss decreased (0.993073 --> 0.989952).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 15.705067157745361
Epoch: 21, Steps: 64 | Train Loss: 0.2394649 Vali Loss: 0.9875367 Test Loss: 0.4519985
Validation loss decreased (0.989952 --> 0.987537).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 16.95030975341797
Epoch: 22, Steps: 64 | Train Loss: 0.2379357 Vali Loss: 0.9853808 Test Loss: 0.4507442
Validation loss decreased (0.987537 --> 0.985381).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 18.004840850830078
Epoch: 23, Steps: 64 | Train Loss: 0.2364829 Vali Loss: 0.9833794 Test Loss: 0.4494803
Validation loss decreased (0.985381 --> 0.983379).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 18.405631065368652
Epoch: 24, Steps: 64 | Train Loss: 0.2353279 Vali Loss: 0.9798722 Test Loss: 0.4480498
Validation loss decreased (0.983379 --> 0.979872).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 16.4382746219635
Epoch: 25, Steps: 64 | Train Loss: 0.2341088 Vali Loss: 0.9780048 Test Loss: 0.4468884
Validation loss decreased (0.979872 --> 0.978005).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 17.79342269897461
Epoch: 26, Steps: 64 | Train Loss: 0.2330332 Vali Loss: 0.9775552 Test Loss: 0.4458655
Validation loss decreased (0.978005 --> 0.977555).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 15.84025239944458
Epoch: 27, Steps: 64 | Train Loss: 0.2321148 Vali Loss: 0.9750039 Test Loss: 0.4448057
Validation loss decreased (0.977555 --> 0.975004).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 15.812028169631958
Epoch: 28, Steps: 64 | Train Loss: 0.2313114 Vali Loss: 0.9737018 Test Loss: 0.4439254
Validation loss decreased (0.975004 --> 0.973702).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 15.02694296836853
Epoch: 29, Steps: 64 | Train Loss: 0.2305687 Vali Loss: 0.9724017 Test Loss: 0.4430458
Validation loss decreased (0.973702 --> 0.972402).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 14.847254037857056
Epoch: 30, Steps: 64 | Train Loss: 0.2298234 Vali Loss: 0.9712321 Test Loss: 0.4422395
Validation loss decreased (0.972402 --> 0.971232).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 16.52648639678955
Epoch: 31, Steps: 64 | Train Loss: 0.2289965 Vali Loss: 0.9700689 Test Loss: 0.4416324
Validation loss decreased (0.971232 --> 0.970069).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 15.863888502120972
Epoch: 32, Steps: 64 | Train Loss: 0.2283707 Vali Loss: 0.9693489 Test Loss: 0.4408099
Validation loss decreased (0.970069 --> 0.969349).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 15.569056272506714
Epoch: 33, Steps: 64 | Train Loss: 0.2278836 Vali Loss: 0.9676021 Test Loss: 0.4402142
Validation loss decreased (0.969349 --> 0.967602).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 15.54014253616333
Epoch: 34, Steps: 64 | Train Loss: 0.2272839 Vali Loss: 0.9661657 Test Loss: 0.4396271
Validation loss decreased (0.967602 --> 0.966166).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 15.181564331054688
Epoch: 35, Steps: 64 | Train Loss: 0.2267924 Vali Loss: 0.9659729 Test Loss: 0.4390069
Validation loss decreased (0.966166 --> 0.965973).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 15.808027267456055
Epoch: 36, Steps: 64 | Train Loss: 0.2262210 Vali Loss: 0.9646582 Test Loss: 0.4384773
Validation loss decreased (0.965973 --> 0.964658).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 16.704736471176147
Epoch: 37, Steps: 64 | Train Loss: 0.2256549 Vali Loss: 0.9648961 Test Loss: 0.4380511
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 17.042120695114136
Epoch: 38, Steps: 64 | Train Loss: 0.2252924 Vali Loss: 0.9635223 Test Loss: 0.4375720
Validation loss decreased (0.964658 --> 0.963522).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 21.096336364746094
Epoch: 39, Steps: 64 | Train Loss: 0.2249690 Vali Loss: 0.9624589 Test Loss: 0.4371443
Validation loss decreased (0.963522 --> 0.962459).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 24.60653567314148
Epoch: 40, Steps: 64 | Train Loss: 0.2245304 Vali Loss: 0.9623265 Test Loss: 0.4367237
Validation loss decreased (0.962459 --> 0.962327).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 27.662692308425903
Epoch: 41, Steps: 64 | Train Loss: 0.2241592 Vali Loss: 0.9618618 Test Loss: 0.4363536
Validation loss decreased (0.962327 --> 0.961862).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 23.260124921798706
Epoch: 42, Steps: 64 | Train Loss: 0.2240423 Vali Loss: 0.9600638 Test Loss: 0.4359901
Validation loss decreased (0.961862 --> 0.960064).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 21.796699047088623
Epoch: 43, Steps: 64 | Train Loss: 0.2236438 Vali Loss: 0.9605652 Test Loss: 0.4356731
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 17.75222134590149
Epoch: 44, Steps: 64 | Train Loss: 0.2233884 Vali Loss: 0.9606121 Test Loss: 0.4353549
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 17.437110900878906
Epoch: 45, Steps: 64 | Train Loss: 0.2231307 Vali Loss: 0.9601123 Test Loss: 0.4350688
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  58060800.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 16.899613618850708
Epoch: 1, Steps: 64 | Train Loss: 0.4081441 Vali Loss: 0.9439014 Test Loss: 0.4259319
Validation loss decreased (inf --> 0.943901).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 17.051452159881592
Epoch: 2, Steps: 64 | Train Loss: 0.4019517 Vali Loss: 0.9379835 Test Loss: 0.4219122
Validation loss decreased (0.943901 --> 0.937983).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 17.267837524414062
Epoch: 3, Steps: 64 | Train Loss: 0.3994057 Vali Loss: 0.9355552 Test Loss: 0.4209296
Validation loss decreased (0.937983 --> 0.935555).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 18.766494035720825
Epoch: 4, Steps: 64 | Train Loss: 0.3983595 Vali Loss: 0.9328339 Test Loss: 0.4204679
Validation loss decreased (0.935555 --> 0.932834).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 20.61540961265564
Epoch: 5, Steps: 64 | Train Loss: 0.3978488 Vali Loss: 0.9338252 Test Loss: 0.4205444
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 20.430321216583252
Epoch: 6, Steps: 64 | Train Loss: 0.3976960 Vali Loss: 0.9322879 Test Loss: 0.4207126
Validation loss decreased (0.932834 --> 0.932288).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 22.749464750289917
Epoch: 7, Steps: 64 | Train Loss: 0.3973591 Vali Loss: 0.9319326 Test Loss: 0.4207907
Validation loss decreased (0.932288 --> 0.931933).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 22.214242696762085
Epoch: 8, Steps: 64 | Train Loss: 0.3970966 Vali Loss: 0.9326262 Test Loss: 0.4212520
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 22.129777669906616
Epoch: 9, Steps: 64 | Train Loss: 0.3970912 Vali Loss: 0.9328591 Test Loss: 0.4208755
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 21.595732927322388
Epoch: 10, Steps: 64 | Train Loss: 0.3971396 Vali Loss: 0.9315205 Test Loss: 0.4209743
Validation loss decreased (0.931933 --> 0.931520).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 22.42275094985962
Epoch: 11, Steps: 64 | Train Loss: 0.3971833 Vali Loss: 0.9323226 Test Loss: 0.4212001
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 23.734954118728638
Epoch: 12, Steps: 64 | Train Loss: 0.3970823 Vali Loss: 0.9318364 Test Loss: 0.4210650
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 22.3664972782135
Epoch: 13, Steps: 64 | Train Loss: 0.3970237 Vali Loss: 0.9319904 Test Loss: 0.4212390
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4153805673122406, mae:0.41176316142082214, rse:0.6131882667541504, corr:[0.52391857 0.53264236 0.5393026  0.5420837  0.54213834 0.54172766
 0.54193926 0.542828   0.54398376 0.5447942  0.54497504 0.54457676
 0.5439727  0.5433638  0.542692   0.54181397 0.5406659  0.53930104
 0.537844   0.5365186  0.5354782  0.53463113 0.533751   0.5327461
 0.53140634 0.5299404  0.5286579  0.5278296  0.5277173  0.5282598
 0.529138   0.5300071  0.53050333 0.5306404  0.5305016  0.5304293
 0.5304129  0.53044456 0.530382   0.5300173  0.5294193  0.528734
 0.52826256 0.52824914 0.528572   0.52897084 0.529261   0.52928925
 0.5290123  0.5285683  0.52828205 0.5282841  0.52855325 0.52880013
 0.5288564  0.5286385  0.5282     0.52765405 0.5272098  0.5269129
 0.52678216 0.5267614  0.52675843 0.52668065 0.52663416 0.5267135
 0.5269248  0.5272077  0.5274807  0.5276049  0.5275728  0.52739036
 0.52711046 0.5268244  0.526635   0.5265041  0.5263533  0.5261492
 0.5258999  0.5256682  0.5255135  0.52547413 0.5255613  0.52569526
 0.52585024 0.5258942  0.5258005  0.5255587  0.5252756  0.52512354
 0.52519244 0.52545804 0.5258376  0.5261801  0.5263898  0.52635175
 0.52609426 0.5257252  0.5252202  0.52475554 0.5244037  0.5241999
 0.5241429  0.5241424  0.5241597  0.5241921  0.52411294 0.52392846
 0.52360827 0.5232318  0.5228603  0.5225225  0.5222489  0.5220563
 0.52191025 0.52178335 0.521641   0.52144086 0.5211903  0.5209726
 0.52081245 0.5207241  0.52066666 0.5206756  0.5207102  0.52066404
 0.52049375 0.52026194 0.5200062  0.5198243  0.5197042  0.5196322
 0.51960903 0.519572   0.51942116 0.51919764 0.5190273  0.51891136
 0.5189447  0.5191566  0.5194175  0.51965344 0.51981634 0.51990455
 0.5199428  0.5199261  0.51992667 0.51994956 0.5199863  0.51995355
 0.51979774 0.5195908  0.5193761  0.51921475 0.5191327  0.51919717
 0.5193329  0.51945806 0.51957417 0.5196357  0.51975185 0.5199563
 0.52024436 0.5205478  0.52078503 0.5208966  0.5208946  0.5208605
 0.5208176  0.5207789  0.5208177  0.52091354 0.5209661  0.520935
 0.5208365  0.5207185  0.5206504  0.5206718  0.520771   0.5209355
 0.52110845 0.5212582  0.5213175  0.521344   0.52138036 0.5214643
 0.5216297  0.5218792  0.52214336 0.5223153  0.52230877 0.52210426
 0.5217391  0.52133167 0.52088165 0.5203954  0.5198932  0.51939636
 0.51885194 0.5182413  0.5175243  0.51675117 0.5159612  0.5152151
 0.5145678  0.51403373 0.51358366 0.5132011  0.51281774 0.51236755
 0.5117972  0.51114994 0.51047665 0.5098186  0.50915    0.5085444
 0.5080599  0.5076624  0.50738066 0.5071851  0.5070757  0.5070961
 0.5072928  0.5075247  0.5077499  0.5079428  0.5080412  0.50806147
 0.5080376  0.50800633 0.50797516 0.5079031  0.5078283  0.50779897
 0.5078384  0.5079795  0.5081058  0.5082668  0.50836533 0.5084149
 0.5083828  0.50821656 0.50799775 0.5078269  0.5077712  0.50778794
 0.5078347  0.50787586 0.50786275 0.5078029  0.5076749  0.50756407
 0.50744057 0.507392   0.50738513 0.5074023  0.5074457  0.5075127
 0.5076028  0.50772405 0.5078504  0.5079723  0.5080866  0.5082677
 0.5084144  0.50850016 0.50848526 0.5083924  0.5082279  0.507997
 0.5077872  0.5076303  0.5075607  0.5075738  0.50763494 0.50772953
 0.5078063  0.50793236 0.5080344  0.50816345 0.50826913 0.5083135
 0.50834215 0.5083029  0.5081856  0.5080194  0.50778496 0.50746727
 0.5070878  0.5067124  0.5063325  0.5058885  0.5053822  0.504843
 0.50431424 0.50377524 0.50328696 0.50281006 0.5023185  0.50184506
 0.5013966  0.50102586 0.5007869  0.5006425  0.5005234  0.50033116
 0.5001005  0.49976125 0.49933228 0.49887943 0.49853304 0.4983206
 0.4982624  0.49836397 0.49851298 0.49863148 0.4986609  0.4986273
 0.49853855 0.49840483 0.49828863 0.49822488 0.49819872 0.49815664
 0.49805364 0.49789828 0.49766317 0.49740696 0.49716786 0.49700007
 0.49698985 0.49708548 0.49718028 0.4972263  0.4971769  0.49711576
 0.49703184 0.4968509  0.49669164 0.49658144 0.49654785 0.49656802
 0.49658638 0.49662554 0.49666774 0.49672017 0.49677348 0.49684855
 0.49685845 0.49683672 0.4967889  0.49671912 0.4966983  0.49673495
 0.49679205 0.49685785 0.4968773  0.49681774 0.49672422 0.4966404
 0.49654308 0.49650714 0.49649715 0.4964819  0.49643075 0.4963213
 0.49610096 0.49587375 0.49571368 0.49563587 0.49569902 0.49589995
 0.49618915 0.4965146  0.49679342 0.49700713 0.49713176 0.49723482
 0.49738288 0.49760288 0.49788174 0.49812773 0.4982659  0.49827337
 0.49811432 0.4977921  0.49737006 0.49686775 0.49629912 0.49582702
 0.49546453 0.49520087 0.4949795  0.49477565 0.49448875 0.49410895
 0.4936465  0.49315852 0.4927115  0.4923449  0.49201316 0.4917622
 0.49152973 0.49133185 0.49108344 0.49082634 0.4905772  0.49040458
 0.49030334 0.49028128 0.49031094 0.490384   0.49047893 0.49052024
 0.49060568 0.49063057 0.49066454 0.49064994 0.49065053 0.49073568
 0.49083468 0.49096698 0.49103913 0.49103943 0.49093604 0.49083355
 0.49075583 0.49075994 0.49080637 0.49095243 0.4910965  0.49128154
 0.4913855  0.49131265 0.49112856 0.49093348 0.4907551  0.49061278
 0.49048302 0.49035197 0.4902121  0.49006402 0.4899356  0.4898641
 0.4898249  0.48985064 0.48990148 0.4899704  0.49004912 0.4901089
 0.49019918 0.49030235 0.4904029  0.49046534 0.4904774  0.49050447
 0.49046817 0.49041912 0.4903686  0.49036857 0.4903483  0.49034086
 0.4903138  0.490308   0.49028832 0.49029654 0.4903441  0.49041563
 0.49051264 0.49057257 0.4905968  0.49058586 0.4905652  0.49056324
 0.4905734  0.49063852 0.49072802 0.4907431  0.49060607 0.49029854
 0.48982203 0.48924413 0.48863372 0.48800692 0.4873813  0.48681116
 0.48634398 0.4858735  0.48539042 0.48489675 0.48432612 0.48367372
 0.48298392 0.4823471  0.4817873  0.48126742 0.48077667 0.48034728
 0.47996023 0.47965962 0.47942644 0.47913104 0.47888094 0.4786614
 0.47853473 0.4784891  0.47847086 0.47850138 0.4785308  0.47857246
 0.47862497 0.47863033 0.47859508 0.4786428  0.47877195 0.47896174
 0.47914514 0.47933796 0.47952616 0.47961903 0.47962585 0.47964373
 0.47973815 0.47994694 0.48025003 0.48055452 0.4808746  0.48121294
 0.48147205 0.48156974 0.48146996 0.48128414 0.48110232 0.48095125
 0.48085907 0.48081815 0.4808184  0.4808384  0.48083445 0.48076937
 0.48063236 0.48042592 0.48020568 0.48002356 0.4799271  0.47998455
 0.4801549  0.48038512 0.48059794 0.4806974  0.48067442 0.48063898
 0.48056582 0.4805109  0.48051158 0.48057428 0.480638   0.48069426
 0.4807182  0.48072886 0.4807387  0.48076397 0.4808018  0.4808178
 0.48077372 0.48075157 0.4806733  0.48058754 0.4805181  0.48049837
 0.480535   0.48060307 0.48070556 0.48076296 0.48070097 0.4804552
 0.48003447 0.47951144 0.47891596 0.4782593  0.47757635 0.47700107
 0.4765387  0.4761877  0.4758626  0.47548696 0.47502667 0.47443807
 0.4738023  0.47316906 0.4725886  0.4720657  0.4716295  0.47123894
 0.47086486 0.4704767  0.4701224  0.46982846 0.46960172 0.4694413
 0.46937189 0.4693415  0.46935955 0.46941954 0.46952942 0.46963176
 0.46978122 0.4699274  0.47006676 0.47021243 0.4703634  0.4705685
 0.4708143  0.47101805 0.47119418 0.47129488 0.4713224  0.47128877
 0.4712739  0.4713556  0.47148377 0.4716762  0.47200865 0.4724067
 0.47269458 0.4728424  0.47283626 0.47269985 0.47260028 0.4726077
 0.47265413 0.4727642  0.47288594 0.47290957 0.4728414  0.47263435
 0.4722752  0.47187525 0.4715556  0.47130248 0.47118896 0.4711495
 0.47113714 0.4711077  0.47107628 0.47107238 0.4711042  0.47121578
 0.4713364  0.4713477  0.4712553  0.47105652 0.47076687 0.4704458
 0.4701875  0.47007424 0.47013828 0.47033638 0.470565   0.47070518
 0.47070226 0.47063726 0.4705671  0.4705139  0.47058955 0.47078523
 0.471054   0.47131625 0.4714886  0.47153053 0.4713718  0.4710094
 0.47051272 0.47003397 0.46960667 0.46917194 0.46869177 0.46819156
 0.46773434 0.46732175 0.46703276 0.46680942 0.4664925  0.4661795
 0.4657412  0.46531236 0.4649119  0.46467066 0.46454692 0.4644273
 0.4642302  0.46386918 0.4633954  0.4628285  0.46237674 0.4622017
 0.46229267 0.46265084 0.46306938 0.46339977 0.4636603  0.46381244
 0.46401888 0.4643125  0.4647158  0.46526432 0.46571776 0.46603715
 0.4662398  0.46646044 0.46677867 0.467156   0.46764922 0.46818903
 0.46891603 0.4698233  0.4705274  0.47053087 0.4681237  0.46001253]
