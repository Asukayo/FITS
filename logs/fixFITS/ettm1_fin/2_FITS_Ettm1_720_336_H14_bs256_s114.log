Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  77830144.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.95093297958374
Epoch: 1, Steps: 65 | Train Loss: 0.5711254 Vali Loss: 1.2488123 Test Loss: 0.7595453
Validation loss decreased (inf --> 1.248812).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.698896408081055
Epoch: 2, Steps: 65 | Train Loss: 0.4316593 Vali Loss: 1.0924917 Test Loss: 0.6526468
Validation loss decreased (1.248812 --> 1.092492).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 12.454084873199463
Epoch: 3, Steps: 65 | Train Loss: 0.3628663 Vali Loss: 1.0043585 Test Loss: 0.5962938
Validation loss decreased (1.092492 --> 1.004359).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 13.310329675674438
Epoch: 4, Steps: 65 | Train Loss: 0.3205297 Vali Loss: 0.9477859 Test Loss: 0.5609882
Validation loss decreased (1.004359 --> 0.947786).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 12.557957649230957
Epoch: 5, Steps: 65 | Train Loss: 0.2909505 Vali Loss: 0.9091501 Test Loss: 0.5358410
Validation loss decreased (0.947786 --> 0.909150).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 12.50659441947937
Epoch: 6, Steps: 65 | Train Loss: 0.2684888 Vali Loss: 0.8783155 Test Loss: 0.5158755
Validation loss decreased (0.909150 --> 0.878316).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.751638889312744
Epoch: 7, Steps: 65 | Train Loss: 0.2508395 Vali Loss: 0.8545499 Test Loss: 0.4993812
Validation loss decreased (0.878316 --> 0.854550).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 15.059486389160156
Epoch: 8, Steps: 65 | Train Loss: 0.2364498 Vali Loss: 0.8360977 Test Loss: 0.4875562
Validation loss decreased (0.854550 --> 0.836098).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 13.991263151168823
Epoch: 9, Steps: 65 | Train Loss: 0.2247069 Vali Loss: 0.8213825 Test Loss: 0.4763814
Validation loss decreased (0.836098 --> 0.821383).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 14.227598667144775
Epoch: 10, Steps: 65 | Train Loss: 0.2148473 Vali Loss: 0.8104819 Test Loss: 0.4673647
Validation loss decreased (0.821383 --> 0.810482).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 13.463217496871948
Epoch: 11, Steps: 65 | Train Loss: 0.2065711 Vali Loss: 0.7991112 Test Loss: 0.4600610
Validation loss decreased (0.810482 --> 0.799111).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.313154935836792
Epoch: 12, Steps: 65 | Train Loss: 0.1994713 Vali Loss: 0.7872968 Test Loss: 0.4533920
Validation loss decreased (0.799111 --> 0.787297).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 13.067517518997192
Epoch: 13, Steps: 65 | Train Loss: 0.1933021 Vali Loss: 0.7801122 Test Loss: 0.4471790
Validation loss decreased (0.787297 --> 0.780112).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 13.34484314918518
Epoch: 14, Steps: 65 | Train Loss: 0.1879838 Vali Loss: 0.7735561 Test Loss: 0.4422816
Validation loss decreased (0.780112 --> 0.773556).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 13.603184938430786
Epoch: 15, Steps: 65 | Train Loss: 0.1832581 Vali Loss: 0.7680516 Test Loss: 0.4377013
Validation loss decreased (0.773556 --> 0.768052).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 13.868085861206055
Epoch: 16, Steps: 65 | Train Loss: 0.1791598 Vali Loss: 0.7618076 Test Loss: 0.4334504
Validation loss decreased (0.768052 --> 0.761808).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 13.787814617156982
Epoch: 17, Steps: 65 | Train Loss: 0.1755403 Vali Loss: 0.7567836 Test Loss: 0.4303706
Validation loss decreased (0.761808 --> 0.756784).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 13.537927150726318
Epoch: 18, Steps: 65 | Train Loss: 0.1722597 Vali Loss: 0.7519898 Test Loss: 0.4269327
Validation loss decreased (0.756784 --> 0.751990).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 13.167024612426758
Epoch: 19, Steps: 65 | Train Loss: 0.1693354 Vali Loss: 0.7498423 Test Loss: 0.4240856
Validation loss decreased (0.751990 --> 0.749842).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 13.845860719680786
Epoch: 20, Steps: 65 | Train Loss: 0.1666752 Vali Loss: 0.7440351 Test Loss: 0.4213120
Validation loss decreased (0.749842 --> 0.744035).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 13.67759394645691
Epoch: 21, Steps: 65 | Train Loss: 0.1642102 Vali Loss: 0.7412248 Test Loss: 0.4189934
Validation loss decreased (0.744035 --> 0.741225).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 14.70622968673706
Epoch: 22, Steps: 65 | Train Loss: 0.1620970 Vali Loss: 0.7373828 Test Loss: 0.4166178
Validation loss decreased (0.741225 --> 0.737383).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 14.759799003601074
Epoch: 23, Steps: 65 | Train Loss: 0.1601019 Vali Loss: 0.7355454 Test Loss: 0.4145161
Validation loss decreased (0.737383 --> 0.735545).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 14.976410865783691
Epoch: 24, Steps: 65 | Train Loss: 0.1583191 Vali Loss: 0.7319357 Test Loss: 0.4128314
Validation loss decreased (0.735545 --> 0.731936).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 13.697847127914429
Epoch: 25, Steps: 65 | Train Loss: 0.1566947 Vali Loss: 0.7296682 Test Loss: 0.4110103
Validation loss decreased (0.731936 --> 0.729668).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 13.186310768127441
Epoch: 26, Steps: 65 | Train Loss: 0.1552168 Vali Loss: 0.7271343 Test Loss: 0.4093480
Validation loss decreased (0.729668 --> 0.727134).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 13.565362930297852
Epoch: 27, Steps: 65 | Train Loss: 0.1537786 Vali Loss: 0.7263948 Test Loss: 0.4078594
Validation loss decreased (0.727134 --> 0.726395).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 12.64055323600769
Epoch: 28, Steps: 65 | Train Loss: 0.1524774 Vali Loss: 0.7259622 Test Loss: 0.4063797
Validation loss decreased (0.726395 --> 0.725962).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 13.230031728744507
Epoch: 29, Steps: 65 | Train Loss: 0.1513543 Vali Loss: 0.7226288 Test Loss: 0.4053836
Validation loss decreased (0.725962 --> 0.722629).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 13.179856061935425
Epoch: 30, Steps: 65 | Train Loss: 0.1502083 Vali Loss: 0.7205075 Test Loss: 0.4037626
Validation loss decreased (0.722629 --> 0.720508).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 13.21843409538269
Epoch: 31, Steps: 65 | Train Loss: 0.1492428 Vali Loss: 0.7198455 Test Loss: 0.4025731
Validation loss decreased (0.720508 --> 0.719845).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 12.026301860809326
Epoch: 32, Steps: 65 | Train Loss: 0.1482230 Vali Loss: 0.7175261 Test Loss: 0.4017836
Validation loss decreased (0.719845 --> 0.717526).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 12.08497166633606
Epoch: 33, Steps: 65 | Train Loss: 0.1474209 Vali Loss: 0.7152057 Test Loss: 0.4008703
Validation loss decreased (0.717526 --> 0.715206).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 9.079764127731323
Epoch: 34, Steps: 65 | Train Loss: 0.1465452 Vali Loss: 0.7146682 Test Loss: 0.3998281
Validation loss decreased (0.715206 --> 0.714668).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 7.4337358474731445
Epoch: 35, Steps: 65 | Train Loss: 0.1457983 Vali Loss: 0.7172305 Test Loss: 0.3990006
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.424196720123291
Epoch: 36, Steps: 65 | Train Loss: 0.1450751 Vali Loss: 0.7147800 Test Loss: 0.3981240
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 10.99245810508728
Epoch: 37, Steps: 65 | Train Loss: 0.1444343 Vali Loss: 0.7115048 Test Loss: 0.3974259
Validation loss decreased (0.714668 --> 0.711505).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 5.971189260482788
Epoch: 38, Steps: 65 | Train Loss: 0.1438115 Vali Loss: 0.7107095 Test Loss: 0.3967158
Validation loss decreased (0.711505 --> 0.710709).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 6.227468729019165
Epoch: 39, Steps: 65 | Train Loss: 0.1431762 Vali Loss: 0.7078162 Test Loss: 0.3959444
Validation loss decreased (0.710709 --> 0.707816).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 12.76668405532837
Epoch: 40, Steps: 65 | Train Loss: 0.1426661 Vali Loss: 0.7111968 Test Loss: 0.3954495
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 12.62146282196045
Epoch: 41, Steps: 65 | Train Loss: 0.1422079 Vali Loss: 0.7075068 Test Loss: 0.3948946
Validation loss decreased (0.707816 --> 0.707507).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 13.017920017242432
Epoch: 42, Steps: 65 | Train Loss: 0.1416931 Vali Loss: 0.7083602 Test Loss: 0.3943199
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 12.788246393203735
Epoch: 43, Steps: 65 | Train Loss: 0.1412800 Vali Loss: 0.7070136 Test Loss: 0.3937815
Validation loss decreased (0.707507 --> 0.707014).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 12.588099718093872
Epoch: 44, Steps: 65 | Train Loss: 0.1408061 Vali Loss: 0.7046006 Test Loss: 0.3933481
Validation loss decreased (0.707014 --> 0.704601).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 12.419489860534668
Epoch: 45, Steps: 65 | Train Loss: 0.1404534 Vali Loss: 0.7065142 Test Loss: 0.3929766
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 11.938041925430298
Epoch: 46, Steps: 65 | Train Loss: 0.1400830 Vali Loss: 0.7053441 Test Loss: 0.3925191
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 12.346396446228027
Epoch: 47, Steps: 65 | Train Loss: 0.1396874 Vali Loss: 0.7047618 Test Loss: 0.3920756
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  77830144.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 12.330058813095093
Epoch: 1, Steps: 65 | Train Loss: 0.3513120 Vali Loss: 0.6719102 Test Loss: 0.3713179
Validation loss decreased (inf --> 0.671910).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 12.625905752182007
Epoch: 2, Steps: 65 | Train Loss: 0.3409634 Vali Loss: 0.6641223 Test Loss: 0.3676971
Validation loss decreased (0.671910 --> 0.664122).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.771257162094116
Epoch: 3, Steps: 65 | Train Loss: 0.3382837 Vali Loss: 0.6598672 Test Loss: 0.3669542
Validation loss decreased (0.664122 --> 0.659867).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 13.833938121795654
Epoch: 4, Steps: 65 | Train Loss: 0.3375239 Vali Loss: 0.6587797 Test Loss: 0.3671546
Validation loss decreased (0.659867 --> 0.658780).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 13.912517309188843
Epoch: 5, Steps: 65 | Train Loss: 0.3369016 Vali Loss: 0.6569353 Test Loss: 0.3670199
Validation loss decreased (0.658780 --> 0.656935).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 13.339493036270142
Epoch: 6, Steps: 65 | Train Loss: 0.3368680 Vali Loss: 0.6581508 Test Loss: 0.3670613
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.667746305465698
Epoch: 7, Steps: 65 | Train Loss: 0.3366286 Vali Loss: 0.6590785 Test Loss: 0.3670454
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.677294492721558
Epoch: 8, Steps: 65 | Train Loss: 0.3366138 Vali Loss: 0.6550012 Test Loss: 0.3669820
Validation loss decreased (0.656935 --> 0.655001).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.114835023880005
Epoch: 9, Steps: 65 | Train Loss: 0.3365183 Vali Loss: 0.6551431 Test Loss: 0.3668226
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.836295366287231
Epoch: 10, Steps: 65 | Train Loss: 0.3364368 Vali Loss: 0.6536716 Test Loss: 0.3669003
Validation loss decreased (0.655001 --> 0.653672).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.227808952331543
Epoch: 11, Steps: 65 | Train Loss: 0.3362306 Vali Loss: 0.6547979 Test Loss: 0.3668472
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 12.4291353225708
Epoch: 12, Steps: 65 | Train Loss: 0.3361697 Vali Loss: 0.6530480 Test Loss: 0.3670375
Validation loss decreased (0.653672 --> 0.653048).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 12.342432022094727
Epoch: 13, Steps: 65 | Train Loss: 0.3362651 Vali Loss: 0.6541583 Test Loss: 0.3667509
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.960687398910522
Epoch: 14, Steps: 65 | Train Loss: 0.3361040 Vali Loss: 0.6531420 Test Loss: 0.3669060
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.781277179718018
Epoch: 15, Steps: 65 | Train Loss: 0.3361809 Vali Loss: 0.6550740 Test Loss: 0.3669370
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.36648961901664734, mae:0.3850509524345398, rse:0.5760751366615295, corr:[0.53660786 0.54815614 0.55322945 0.5537035  0.5542924  0.55636406
 0.5586244  0.55960464 0.5594565  0.5592852  0.5597975  0.56061244
 0.5609753  0.560514   0.55949223 0.5583722  0.5573835  0.5564764
 0.55545384 0.55431855 0.55315113 0.55187035 0.5504209  0.5489909
 0.5476011  0.5465276  0.54579395 0.5451712  0.5446719  0.54438996
 0.54454076 0.5452853  0.54628956 0.5471046  0.54719526 0.546799
 0.5462217  0.54593295 0.54597116 0.5459595  0.5457309  0.5452389
 0.544794   0.54473823 0.54490775 0.54501    0.5449581  0.54478663
 0.54460377 0.5444779  0.5444421  0.54433495 0.5441235  0.5438236
 0.5436273  0.54357463 0.54354244 0.5433863  0.5432336  0.5431797
 0.5433376  0.543598   0.54368764 0.543385   0.5429459  0.5427102
 0.5428287  0.5431792  0.54350334 0.5435819  0.54345095 0.54325384
 0.5431183  0.5430324  0.5429288  0.54270023 0.5423364  0.5419742
 0.54173946 0.54167783 0.5417141  0.54169816 0.5415427  0.5412322
 0.5409135  0.5406566  0.54050905 0.5404187  0.54039305 0.5405194
 0.5407931  0.54113704 0.54143876 0.54161847 0.54160154 0.54137033
 0.5410065  0.54065233 0.54025334 0.53990203 0.5396502  0.53948843
 0.5393755  0.5391961  0.5390143  0.5389294  0.53889245 0.5389451
 0.53896177 0.53890675 0.5387625  0.5385235  0.5382592  0.53805107
 0.5378799  0.53769433 0.53743917 0.537116   0.5368468  0.5367619
 0.53679734 0.53679276 0.53659856 0.5363348  0.5361451  0.53604066
 0.5359667  0.53587055 0.5356897  0.53554344 0.5354904  0.53554994
 0.5356787  0.5356843  0.5354068  0.5349738  0.5346645  0.5344922
 0.5344895  0.53455967 0.53452444 0.534455   0.53449064 0.53467625
 0.5349243  0.53504354 0.535059   0.53505063 0.5351455  0.5352422
 0.5352204  0.5350953  0.53490365 0.5347799  0.53481305 0.5350335
 0.5352551  0.5352737  0.5351311  0.53488034 0.5347888  0.5349149
 0.5351883  0.5354565  0.5356661  0.5358446  0.53604627 0.5362874
 0.53647023 0.53653425 0.5365949  0.53670466 0.53676397 0.5367245
 0.53657526 0.536361   0.53615296 0.53605235 0.53605425 0.5361306
 0.5361666  0.53611016 0.5359537  0.5358726  0.5359878  0.53630286
 0.5367235  0.53716916 0.53754216 0.5377646  0.5378589  0.53787595
 0.5378184  0.53772074 0.53745025 0.53696376 0.53633326 0.5356845
 0.5350482  0.53444386 0.53384477 0.5332571  0.5326828  0.5321418
 0.53163856 0.5310945  0.53043395 0.52970064 0.528963   0.52831906
 0.52780956 0.5274524  0.52712655 0.52672136 0.526111   0.52543944
 0.52486664 0.5243936  0.5240605  0.5238314  0.5237397  0.52383965
 0.5241255  0.52435535 0.5245048  0.524599   0.5246404  0.52468437
 0.52473825 0.52476877 0.5247372  0.5245913  0.52440834 0.5243018
 0.5242898  0.5243922  0.52442163 0.52451414 0.5246331  0.52484643
 0.525088   0.5251646  0.5250771  0.52491677 0.5248233  0.52479327
 0.52481633 0.52487206 0.52482325 0.5246444  0.52429104 0.52398384
 0.5238075  0.5239297  0.5241408  0.52418697 0.5239902  0.52366275
 0.523435   0.5235135  0.5238489  0.5241984  0.5243383  0.52432245
 0.52419627 0.5241192  0.52418274 0.52433723 0.5244147  0.52429056
 0.5240912  0.52392924 0.523893   0.52396554 0.5240019  0.52393675
 0.52376497 0.5236967  0.5237514  0.52396613 0.52413756 0.524127
 0.5239911  0.52386624 0.5238333  0.5239281  0.5240057  0.52384186
 0.5233909  0.5228135  0.5222263  0.5216097  0.52104104 0.52052075
 0.5200547  0.51954776 0.5190723  0.51855403 0.51798695 0.51743156
 0.51692736 0.5165643  0.51634985 0.5161458  0.51581883 0.5153102
 0.51489145 0.51460284 0.5144329  0.51425374 0.5140496  0.5137858
 0.51358145 0.51357937 0.5136308  0.51359063 0.51340276 0.51325345
 0.5133201  0.5135348  0.5137617  0.5138285  0.51368713 0.5134711
 0.5133402  0.51347405 0.51373225 0.51398295 0.51405513 0.5139133
 0.5137343  0.5135955  0.51374215 0.5143238  0.51381165 0.5071075 ]
