Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_96_FITS_ETTm1_ftM_sl720_ll48_pl96_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 11425
test 11425
Model(
  (freq_upsampler): Linear(in_features=74, out_features=83, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2751616.0
params:  6225.0
Trainable parameters:  6225
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2935157
	speed: 0.1567s/iter; left time: 4113.6220s
	iters: 200, epoch: 1 | loss: 0.2727773
	speed: 0.2252s/iter; left time: 5889.5162s
	iters: 300, epoch: 1 | loss: 0.3221744
	speed: 0.2453s/iter; left time: 6389.3544s
	iters: 400, epoch: 1 | loss: 0.2689593
	speed: 0.2496s/iter; left time: 6478.0094s
	iters: 500, epoch: 1 | loss: 0.2590291
	speed: 0.2578s/iter; left time: 6664.7671s
Epoch: 1 cost time: 120.73733901977539
Epoch: 1, Steps: 527 | Train Loss: 0.3030134 Vali Loss: 0.4199844 Test Loss: 0.3133527
Validation loss decreased (inf --> 0.419984).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2842878
	speed: 1.7623s/iter; left time: 45333.1239s
	iters: 200, epoch: 2 | loss: 0.2384148
	speed: 0.2352s/iter; left time: 6026.9235s
	iters: 300, epoch: 2 | loss: 0.3168316
	speed: 0.2388s/iter; left time: 6094.8429s
	iters: 400, epoch: 2 | loss: 0.2480577
	speed: 0.2211s/iter; left time: 5621.2756s
	iters: 500, epoch: 2 | loss: 0.2812642
	speed: 0.2157s/iter; left time: 5462.6139s
Epoch: 2 cost time: 122.02038168907166
Epoch: 2, Steps: 527 | Train Loss: 0.2686862 Vali Loss: 0.4112580 Test Loss: 0.3133735
Validation loss decreased (0.419984 --> 0.411258).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2609670
	speed: 1.5860s/iter; left time: 39962.4808s
	iters: 200, epoch: 3 | loss: 0.2190269
	speed: 0.2038s/iter; left time: 5113.6851s
	iters: 300, epoch: 3 | loss: 0.2779392
	speed: 0.2013s/iter; left time: 5032.0499s
	iters: 400, epoch: 3 | loss: 0.2486748
	speed: 0.1996s/iter; left time: 4969.8587s
	iters: 500, epoch: 3 | loss: 0.2535249
	speed: 0.1971s/iter; left time: 4887.2564s
Epoch: 3 cost time: 107.65960717201233
Epoch: 3, Steps: 527 | Train Loss: 0.2664052 Vali Loss: 0.3999721 Test Loss: 0.3113879
Validation loss decreased (0.411258 --> 0.399972).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2292951
	speed: 1.2995s/iter; left time: 32058.7642s
	iters: 200, epoch: 4 | loss: 0.2579659
	speed: 0.1881s/iter; left time: 4622.1943s
	iters: 300, epoch: 4 | loss: 0.2466311
	speed: 0.1948s/iter; left time: 4767.7727s
	iters: 400, epoch: 4 | loss: 0.2952986
	speed: 0.2056s/iter; left time: 5011.5002s
	iters: 500, epoch: 4 | loss: 0.2460608
	speed: 0.1791s/iter; left time: 4347.0285s
Epoch: 4 cost time: 99.44089651107788
Epoch: 4, Steps: 527 | Train Loss: 0.2654858 Vali Loss: 0.3977379 Test Loss: 0.3115041
Validation loss decreased (0.399972 --> 0.397738).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2666762
	speed: 1.4201s/iter; left time: 34286.0457s
	iters: 200, epoch: 5 | loss: 0.2408687
	speed: 0.1989s/iter; left time: 4783.2012s
	iters: 300, epoch: 5 | loss: 0.2796693
	speed: 0.1890s/iter; left time: 4524.7642s
	iters: 400, epoch: 5 | loss: 0.2600883
	speed: 0.1977s/iter; left time: 4714.1337s
	iters: 500, epoch: 5 | loss: 0.2584306
	speed: 0.1935s/iter; left time: 4594.7793s
Epoch: 5 cost time: 103.34792017936707
Epoch: 5, Steps: 527 | Train Loss: 0.2648854 Vali Loss: 0.3989595 Test Loss: 0.3109620
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2467929
	speed: 1.4022s/iter; left time: 33113.4877s
	iters: 200, epoch: 6 | loss: 0.2952092
	speed: 0.2121s/iter; left time: 4987.2298s
	iters: 300, epoch: 6 | loss: 0.2850441
	speed: 0.2057s/iter; left time: 4815.9753s
	iters: 400, epoch: 6 | loss: 0.2240915
	speed: 0.2009s/iter; left time: 4683.0518s
	iters: 500, epoch: 6 | loss: 0.2495690
	speed: 0.2070s/iter; left time: 4804.6192s
Epoch: 6 cost time: 108.02838277816772
Epoch: 6, Steps: 527 | Train Loss: 0.2644792 Vali Loss: 0.4021264 Test Loss: 0.3117435
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2789531
	speed: 1.4321s/iter; left time: 33065.1507s
	iters: 200, epoch: 7 | loss: 0.2981963
	speed: 0.2085s/iter; left time: 4793.0568s
	iters: 300, epoch: 7 | loss: 0.3052021
	speed: 0.2022s/iter; left time: 4628.1625s
	iters: 400, epoch: 7 | loss: 0.2513547
	speed: 0.1937s/iter; left time: 4413.2884s
	iters: 500, epoch: 7 | loss: 0.2256173
	speed: 0.2111s/iter; left time: 4789.3394s
Epoch: 7 cost time: 110.13036131858826
Epoch: 7, Steps: 527 | Train Loss: 0.2642940 Vali Loss: 0.3999527 Test Loss: 0.3109362
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_96_FITS_ETTm1_ftM_sl720_ll48_pl96_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
mse:0.3120361566543579, mae:0.3540908992290497, rse:0.5315380692481995, corr:[0.54614186 0.5552984  0.5624721  0.5662506  0.5671253  0.56672096
 0.5662561  0.56634516 0.5671144  0.56827515 0.5693801  0.5699332
 0.56980443 0.56913286 0.56818336 0.567216   0.5663579  0.5655922
 0.56477904 0.56380713 0.5626508  0.5612255  0.5595595  0.5579069
 0.556368   0.55520225 0.5545181  0.5542467  0.55438316 0.55479056
 0.5552593  0.55566365 0.5558475  0.55587685 0.55572206 0.5555671
 0.5553425  0.5551034  0.554853   0.5544955  0.55413574 0.5537851
 0.55350596 0.55340385 0.5533524  0.5532211  0.55301476 0.55270314
 0.5522575  0.55172616 0.5513443  0.5512631  0.55155706 0.55202293
 0.55248094 0.55274016 0.5526892  0.5522689  0.5516617  0.5509912
 0.5505065  0.5503815  0.5506356  0.55107945 0.5516287  0.5521603
 0.5525433  0.55272406 0.55274105 0.55264306 0.552552   0.55250424
 0.5524543  0.55233335 0.55214316 0.551814   0.5513222  0.55071175
 0.5500547  0.54943866 0.54894215 0.54854786 0.5482659  0.5479782
 0.5476624  0.54724556 0.5467451  0.5461273  0.5454849  0.54503244
 0.544853   0.54493254 0.54520154 0.5454534  0.5453388  0.5442987 ]
