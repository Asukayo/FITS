Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14721280.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3785318
	speed: 0.1411s/iter; left time: 1827.6716s
	iters: 200, epoch: 1 | loss: 0.3549461
	speed: 0.1254s/iter; left time: 1612.1293s
Epoch: 1 cost time: 34.13162279129028
Epoch: 1, Steps: 261 | Train Loss: 0.4159174 Vali Loss: 0.7215721 Test Loss: 0.3727123
Validation loss decreased (inf --> 0.721572).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3475376
	speed: 0.5691s/iter; left time: 7221.5041s
	iters: 200, epoch: 2 | loss: 0.3542877
	speed: 0.1325s/iter; left time: 1668.3635s
Epoch: 2 cost time: 36.379570960998535
Epoch: 2, Steps: 261 | Train Loss: 0.3465940 Vali Loss: 0.6833259 Test Loss: 0.3654761
Validation loss decreased (0.721572 --> 0.683326).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3412400
	speed: 0.6227s/iter; left time: 7739.1225s
	iters: 200, epoch: 3 | loss: 0.3498040
	speed: 0.1227s/iter; left time: 1512.7702s
Epoch: 3 cost time: 34.076003074645996
Epoch: 3, Steps: 261 | Train Loss: 0.3408111 Vali Loss: 0.6717353 Test Loss: 0.3666883
Validation loss decreased (0.683326 --> 0.671735).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3267066
	speed: 0.5452s/iter; left time: 6634.4223s
	iters: 200, epoch: 4 | loss: 0.3167731
	speed: 0.1241s/iter; left time: 1497.2762s
Epoch: 4 cost time: 32.97667121887207
Epoch: 4, Steps: 261 | Train Loss: 0.3390109 Vali Loss: 0.6660781 Test Loss: 0.3671494
Validation loss decreased (0.671735 --> 0.666078).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3322897
	speed: 0.5446s/iter; left time: 6484.6553s
	iters: 200, epoch: 5 | loss: 0.3104663
	speed: 0.1202s/iter; left time: 1419.2126s
Epoch: 5 cost time: 32.18724322319031
Epoch: 5, Steps: 261 | Train Loss: 0.3381682 Vali Loss: 0.6631325 Test Loss: 0.3667869
Validation loss decreased (0.666078 --> 0.663132).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3018045
	speed: 0.5524s/iter; left time: 6433.1449s
	iters: 200, epoch: 6 | loss: 0.3366645
	speed: 0.1193s/iter; left time: 1377.7711s
Epoch: 6 cost time: 32.514976501464844
Epoch: 6, Steps: 261 | Train Loss: 0.3377172 Vali Loss: 0.6610025 Test Loss: 0.3668032
Validation loss decreased (0.663132 --> 0.661002).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3162297
	speed: 0.5569s/iter; left time: 6339.8830s
	iters: 200, epoch: 7 | loss: 0.3222197
	speed: 0.1357s/iter; left time: 1530.8975s
Epoch: 7 cost time: 35.00733256340027
Epoch: 7, Steps: 261 | Train Loss: 0.3373426 Vali Loss: 0.6591687 Test Loss: 0.3671832
Validation loss decreased (0.661002 --> 0.659169).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3688959
	speed: 0.6158s/iter; left time: 6849.7828s
	iters: 200, epoch: 8 | loss: 0.3427058
	speed: 0.1309s/iter; left time: 1442.6250s
Epoch: 8 cost time: 34.724647521972656
Epoch: 8, Steps: 261 | Train Loss: 0.3371100 Vali Loss: 0.6589900 Test Loss: 0.3666441
Validation loss decreased (0.659169 --> 0.658990).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3431845
	speed: 0.5475s/iter; left time: 5947.8244s
	iters: 200, epoch: 9 | loss: 0.2983027
	speed: 0.1211s/iter; left time: 1303.3768s
Epoch: 9 cost time: 32.48245286941528
Epoch: 9, Steps: 261 | Train Loss: 0.3369372 Vali Loss: 0.6590255 Test Loss: 0.3669736
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3344638
	speed: 0.5525s/iter; left time: 5857.9770s
	iters: 200, epoch: 10 | loss: 0.3134582
	speed: 0.1165s/iter; left time: 1223.5273s
Epoch: 10 cost time: 32.14666032791138
Epoch: 10, Steps: 261 | Train Loss: 0.3367516 Vali Loss: 0.6565899 Test Loss: 0.3670503
Validation loss decreased (0.658990 --> 0.656590).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3578082
	speed: 0.3888s/iter; left time: 4020.7910s
	iters: 200, epoch: 11 | loss: 0.3412856
	speed: 0.0572s/iter; left time: 585.6668s
Epoch: 11 cost time: 19.055464506149292
Epoch: 11, Steps: 261 | Train Loss: 0.3366621 Vali Loss: 0.6573393 Test Loss: 0.3671621
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3233471
	speed: 0.4774s/iter; left time: 4811.9653s
	iters: 200, epoch: 12 | loss: 0.3551520
	speed: 0.0890s/iter; left time: 887.8001s
Epoch: 12 cost time: 25.195432424545288
Epoch: 12, Steps: 261 | Train Loss: 0.3366302 Vali Loss: 0.6565083 Test Loss: 0.3669412
Validation loss decreased (0.656590 --> 0.656508).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3510223
	speed: 0.6017s/iter; left time: 5908.0778s
	iters: 200, epoch: 13 | loss: 0.3394769
	speed: 0.1411s/iter; left time: 1371.7635s
Epoch: 13 cost time: 37.8803026676178
Epoch: 13, Steps: 261 | Train Loss: 0.3366380 Vali Loss: 0.6557667 Test Loss: 0.3666153
Validation loss decreased (0.656508 --> 0.655767).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3437396
	speed: 0.6160s/iter; left time: 5887.8555s
	iters: 200, epoch: 14 | loss: 0.3436418
	speed: 0.1190s/iter; left time: 1125.2388s
Epoch: 14 cost time: 32.39159560203552
Epoch: 14, Steps: 261 | Train Loss: 0.3365495 Vali Loss: 0.6563317 Test Loss: 0.3669740
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3368164
	speed: 0.5478s/iter; left time: 5092.5321s
	iters: 200, epoch: 15 | loss: 0.3535130
	speed: 0.1230s/iter; left time: 1130.9625s
Epoch: 15 cost time: 33.32247042655945
Epoch: 15, Steps: 261 | Train Loss: 0.3365806 Vali Loss: 0.6560028 Test Loss: 0.3666449
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3414338
	speed: 0.5656s/iter; left time: 5110.7124s
	iters: 200, epoch: 16 | loss: 0.3499029
	speed: 0.1242s/iter; left time: 1109.5320s
Epoch: 16 cost time: 33.12592148780823
Epoch: 16, Steps: 261 | Train Loss: 0.3364276 Vali Loss: 0.6548197 Test Loss: 0.3666955
Validation loss decreased (0.655767 --> 0.654820).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3062559
	speed: 0.5594s/iter; left time: 4908.8012s
	iters: 200, epoch: 17 | loss: 0.3154223
	speed: 0.1185s/iter; left time: 1027.6280s
Epoch: 17 cost time: 32.42937135696411
Epoch: 17, Steps: 261 | Train Loss: 0.3365139 Vali Loss: 0.6561462 Test Loss: 0.3672155
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3455555
	speed: 0.5723s/iter; left time: 4872.6814s
	iters: 200, epoch: 18 | loss: 0.3604389
	speed: 0.1297s/iter; left time: 1091.5453s
Epoch: 18 cost time: 35.303874015808105
Epoch: 18, Steps: 261 | Train Loss: 0.3362260 Vali Loss: 0.6551656 Test Loss: 0.3669803
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3057138
	speed: 0.6000s/iter; left time: 4951.5397s
	iters: 200, epoch: 19 | loss: 0.3162546
	speed: 0.1215s/iter; left time: 990.4878s
Epoch: 19 cost time: 33.69637894630432
Epoch: 19, Steps: 261 | Train Loss: 0.3362885 Vali Loss: 0.6553100 Test Loss: 0.3668862
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3661976754665375, mae:0.3849017322063446, rse:0.5758457183837891, corr:[0.53708637 0.5458724  0.550219   0.5515856  0.5525091  0.55404663
 0.55574495 0.55692875 0.5574704  0.5577096  0.5581455  0.558738
 0.5592481  0.559345   0.5589144  0.5580866  0.55707633 0.5559767
 0.5546908  0.5533076  0.5520172  0.5508409  0.5497547  0.54884076
 0.54779756 0.5466245  0.5454048  0.5442957  0.5436838  0.54371315
 0.544293   0.54518104 0.5459135  0.5463813  0.54654485 0.54675317
 0.5469156  0.54699004 0.54689217 0.54652005 0.54612356 0.5458462
 0.5457888  0.54592204 0.54593176 0.5456558  0.545275   0.54499835
 0.54493195 0.54501337 0.5451583  0.54513663 0.5448654  0.5443171
 0.54375917 0.54345334 0.5434742  0.54363763 0.5438153  0.5438028
 0.5436111  0.5433551  0.54313165 0.54290795 0.5428112  0.5428924
 0.54312867 0.54348797 0.54391235 0.54423577 0.54432976 0.544116
 0.543655   0.54315656 0.5428591  0.542778   0.5427746  0.5427167
 0.5425144  0.542213   0.5419294  0.54176736 0.541778   0.54185444
 0.54191947 0.54183894 0.5416053  0.5412615  0.54096884 0.5409262
 0.54117185 0.54161596 0.54209423 0.54244715 0.5425412  0.5423221
 0.54186743 0.54135495 0.54078126 0.54025364 0.5398213  0.53947365
 0.5391702  0.5388117  0.5384725  0.5382684  0.5381681  0.5382107
 0.53827214 0.5382706  0.5381259  0.53780085 0.53737426 0.53700674
 0.53679955 0.5367903  0.53690314 0.536968   0.5368857  0.5366913
 0.53644335 0.53624505 0.5361373  0.53618526 0.53629684 0.5362763
 0.53606313 0.53577626 0.53552854 0.5354535  0.53547543 0.53547215
 0.53539723 0.5352143  0.5349392  0.53474796 0.5348073  0.53497714
 0.53518134 0.5353258  0.53528655 0.5351527  0.53508234 0.5351809
 0.53542465 0.53563225 0.5357587  0.535796   0.5358431  0.5358896
 0.53591645 0.53594846 0.53593105 0.5358626  0.53577477 0.5358058
 0.53593075 0.5360543  0.53615135 0.5361216  0.53607243 0.53605694
 0.5361181  0.53619504 0.53621984 0.53613925 0.5359815  0.53585577
 0.5358203  0.5358806  0.53608155 0.5363334  0.53644794 0.53635556
 0.5361137  0.53587157 0.53577757 0.5359229  0.5362384  0.5366087
 0.5368524  0.53688115 0.53667516 0.5364196  0.5362851  0.5363545
 0.53662443 0.53704387 0.5374787  0.5377602  0.5378107  0.5376492
 0.5373263  0.53700304 0.53668207 0.53634137 0.5359448  0.53545517
 0.5348123  0.53404325 0.5332181  0.53246313 0.5318262  0.5312895
 0.5308     0.5302692  0.52963877 0.528968   0.5283231  0.52776706
 0.5272884  0.52690005 0.52652484 0.5260842  0.52546865 0.52479035
 0.52419555 0.52372277 0.52342165 0.52321583 0.52307564 0.52303815
 0.52317464 0.5233755  0.5236449  0.5239226  0.5240829  0.5240957
 0.52400875 0.52391475 0.52390045 0.523958   0.5240976  0.5242925
 0.52445835 0.5245934  0.5245888  0.5246543  0.52479    0.5250311
 0.5252583  0.52526695 0.5250655  0.52475923 0.52449685 0.52430874
 0.5242183  0.52424115 0.5243032  0.5243868  0.5243747  0.5243214
 0.52417386 0.5240837  0.5240448  0.5240573  0.52413493 0.5242566
 0.5243692  0.5244344  0.52439594 0.52426034 0.5240778  0.52399296
 0.52396655 0.5239838  0.52403444 0.5241358  0.5242639  0.5243622
 0.5244628  0.5245209  0.5245218  0.52447367 0.5243612  0.52423203
 0.52409124 0.5240394  0.52400964 0.52403677 0.52402496 0.523958
 0.523917   0.52395636 0.5240472  0.5241739  0.52422947 0.52406585
 0.5236515  0.5231     0.5224702  0.5217398  0.52101994 0.52039254
 0.5199222  0.51953083 0.51924324 0.5189196  0.51849294 0.5179679
 0.5173605  0.5167647  0.5162803  0.51588804 0.5155105  0.5150639
 0.5147011  0.5144121  0.5142355  0.5141469  0.51412594 0.5140163
 0.5137619  0.513464   0.51318306 0.51304626 0.5130782  0.5132104
 0.5132259  0.5128982  0.5123137  0.51179373 0.51163054 0.5118614
 0.5122296  0.5124506  0.51228243 0.5119098  0.5116486  0.51175326
 0.51228803 0.5128591  0.51310486 0.5131994  0.5129788  0.5104912 ]
