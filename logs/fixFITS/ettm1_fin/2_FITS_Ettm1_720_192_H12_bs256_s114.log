Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=106, out_features=134, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50907136.0
params:  14338.0
Trainable parameters:  14338
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.844759702682495
Epoch: 1, Steps: 65 | Train Loss: 0.5153629 Vali Loss: 1.0764101 Test Loss: 0.6861120
Validation loss decreased (inf --> 1.076410).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.79781985282898
Epoch: 2, Steps: 65 | Train Loss: 0.3755971 Vali Loss: 0.8887060 Test Loss: 0.5690746
Validation loss decreased (1.076410 --> 0.888706).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.924828052520752
Epoch: 3, Steps: 65 | Train Loss: 0.3079494 Vali Loss: 0.8044376 Test Loss: 0.5226088
Validation loss decreased (0.888706 --> 0.804438).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.90385913848877
Epoch: 4, Steps: 65 | Train Loss: 0.2688059 Vali Loss: 0.7605693 Test Loss: 0.5008038
Validation loss decreased (0.804438 --> 0.760569).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 12.069172382354736
Epoch: 5, Steps: 65 | Train Loss: 0.2424945 Vali Loss: 0.7295043 Test Loss: 0.4859236
Validation loss decreased (0.760569 --> 0.729504).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.294747352600098
Epoch: 6, Steps: 65 | Train Loss: 0.2233482 Vali Loss: 0.7111197 Test Loss: 0.4778389
Validation loss decreased (0.729504 --> 0.711120).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.35718560218811
Epoch: 7, Steps: 65 | Train Loss: 0.2086167 Vali Loss: 0.6961313 Test Loss: 0.4700918
Validation loss decreased (0.711120 --> 0.696131).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.946784734725952
Epoch: 8, Steps: 65 | Train Loss: 0.1965759 Vali Loss: 0.6851803 Test Loss: 0.4650837
Validation loss decreased (0.696131 --> 0.685180).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.711382627487183
Epoch: 9, Steps: 65 | Train Loss: 0.1865093 Vali Loss: 0.6754977 Test Loss: 0.4601791
Validation loss decreased (0.685180 --> 0.675498).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.305387735366821
Epoch: 10, Steps: 65 | Train Loss: 0.1779543 Vali Loss: 0.6669992 Test Loss: 0.4562713
Validation loss decreased (0.675498 --> 0.666999).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.871741771697998
Epoch: 11, Steps: 65 | Train Loss: 0.1704326 Vali Loss: 0.6606839 Test Loss: 0.4521298
Validation loss decreased (0.666999 --> 0.660684).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.815261125564575
Epoch: 12, Steps: 65 | Train Loss: 0.1640134 Vali Loss: 0.6549030 Test Loss: 0.4486977
Validation loss decreased (0.660684 --> 0.654903).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.197950839996338
Epoch: 13, Steps: 65 | Train Loss: 0.1582223 Vali Loss: 0.6499594 Test Loss: 0.4452534
Validation loss decreased (0.654903 --> 0.649959).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.890231370925903
Epoch: 14, Steps: 65 | Train Loss: 0.1531693 Vali Loss: 0.6446648 Test Loss: 0.4426849
Validation loss decreased (0.649959 --> 0.644665).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.96866750717163
Epoch: 15, Steps: 65 | Train Loss: 0.1486015 Vali Loss: 0.6401042 Test Loss: 0.4399694
Validation loss decreased (0.644665 --> 0.640104).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.959328413009644
Epoch: 16, Steps: 65 | Train Loss: 0.1444895 Vali Loss: 0.6354479 Test Loss: 0.4371807
Validation loss decreased (0.640104 --> 0.635448).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.179136514663696
Epoch: 17, Steps: 65 | Train Loss: 0.1408446 Vali Loss: 0.6316795 Test Loss: 0.4347583
Validation loss decreased (0.635448 --> 0.631679).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.167426347732544
Epoch: 18, Steps: 65 | Train Loss: 0.1375284 Vali Loss: 0.6285450 Test Loss: 0.4325266
Validation loss decreased (0.631679 --> 0.628545).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.334374904632568
Epoch: 19, Steps: 65 | Train Loss: 0.1345610 Vali Loss: 0.6254014 Test Loss: 0.4304465
Validation loss decreased (0.628545 --> 0.625401).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.835900068283081
Epoch: 20, Steps: 65 | Train Loss: 0.1317646 Vali Loss: 0.6217737 Test Loss: 0.4280731
Validation loss decreased (0.625401 --> 0.621774).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 10.29628610610962
Epoch: 21, Steps: 65 | Train Loss: 0.1291854 Vali Loss: 0.6198816 Test Loss: 0.4262968
Validation loss decreased (0.621774 --> 0.619882).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.278898477554321
Epoch: 22, Steps: 65 | Train Loss: 0.1270033 Vali Loss: 0.6165547 Test Loss: 0.4250502
Validation loss decreased (0.619882 --> 0.616555).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.886333227157593
Epoch: 23, Steps: 65 | Train Loss: 0.1249410 Vali Loss: 0.6136342 Test Loss: 0.4229977
Validation loss decreased (0.616555 --> 0.613634).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.775919437408447
Epoch: 24, Steps: 65 | Train Loss: 0.1229819 Vali Loss: 0.6129782 Test Loss: 0.4218138
Validation loss decreased (0.613634 --> 0.612978).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 7.903032541275024
Epoch: 25, Steps: 65 | Train Loss: 0.1212580 Vali Loss: 0.6103514 Test Loss: 0.4200708
Validation loss decreased (0.612978 --> 0.610351).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.749827146530151
Epoch: 26, Steps: 65 | Train Loss: 0.1195999 Vali Loss: 0.6083333 Test Loss: 0.4188157
Validation loss decreased (0.610351 --> 0.608333).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 11.118152141571045
Epoch: 27, Steps: 65 | Train Loss: 0.1180701 Vali Loss: 0.6068045 Test Loss: 0.4176271
Validation loss decreased (0.608333 --> 0.606804).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 11.683679580688477
Epoch: 28, Steps: 65 | Train Loss: 0.1167157 Vali Loss: 0.6049345 Test Loss: 0.4162392
Validation loss decreased (0.606804 --> 0.604934).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 13.277744770050049
Epoch: 29, Steps: 65 | Train Loss: 0.1153509 Vali Loss: 0.6031126 Test Loss: 0.4151084
Validation loss decreased (0.604934 --> 0.603113).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 12.52208924293518
Epoch: 30, Steps: 65 | Train Loss: 0.1141500 Vali Loss: 0.6016762 Test Loss: 0.4140977
Validation loss decreased (0.603113 --> 0.601676).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 11.692604541778564
Epoch: 31, Steps: 65 | Train Loss: 0.1131695 Vali Loss: 0.6003339 Test Loss: 0.4129861
Validation loss decreased (0.601676 --> 0.600334).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 12.10451364517212
Epoch: 32, Steps: 65 | Train Loss: 0.1120195 Vali Loss: 0.5981103 Test Loss: 0.4120367
Validation loss decreased (0.600334 --> 0.598110).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 11.98082184791565
Epoch: 33, Steps: 65 | Train Loss: 0.1110864 Vali Loss: 0.5979746 Test Loss: 0.4110748
Validation loss decreased (0.598110 --> 0.597975).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 11.624982357025146
Epoch: 34, Steps: 65 | Train Loss: 0.1101969 Vali Loss: 0.5966120 Test Loss: 0.4102418
Validation loss decreased (0.597975 --> 0.596612).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 11.286366701126099
Epoch: 35, Steps: 65 | Train Loss: 0.1093554 Vali Loss: 0.5959464 Test Loss: 0.4094338
Validation loss decreased (0.596612 --> 0.595946).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 11.34101152420044
Epoch: 36, Steps: 65 | Train Loss: 0.1085788 Vali Loss: 0.5945943 Test Loss: 0.4086878
Validation loss decreased (0.595946 --> 0.594594).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 11.039884805679321
Epoch: 37, Steps: 65 | Train Loss: 0.1078435 Vali Loss: 0.5932600 Test Loss: 0.4080208
Validation loss decreased (0.594594 --> 0.593260).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 10.635537385940552
Epoch: 38, Steps: 65 | Train Loss: 0.1070843 Vali Loss: 0.5929269 Test Loss: 0.4072938
Validation loss decreased (0.593260 --> 0.592927).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 10.697396993637085
Epoch: 39, Steps: 65 | Train Loss: 0.1064651 Vali Loss: 0.5916644 Test Loss: 0.4066012
Validation loss decreased (0.592927 --> 0.591664).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 10.854930877685547
Epoch: 40, Steps: 65 | Train Loss: 0.1058917 Vali Loss: 0.5908663 Test Loss: 0.4059362
Validation loss decreased (0.591664 --> 0.590866).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 10.5960373878479
Epoch: 41, Steps: 65 | Train Loss: 0.1053417 Vali Loss: 0.5897688 Test Loss: 0.4053579
Validation loss decreased (0.590866 --> 0.589769).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 11.592044115066528
Epoch: 42, Steps: 65 | Train Loss: 0.1047982 Vali Loss: 0.5894621 Test Loss: 0.4047682
Validation loss decreased (0.589769 --> 0.589462).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 12.254799604415894
Epoch: 43, Steps: 65 | Train Loss: 0.1041570 Vali Loss: 0.5890555 Test Loss: 0.4042402
Validation loss decreased (0.589462 --> 0.589055).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 11.078654050827026
Epoch: 44, Steps: 65 | Train Loss: 0.1037901 Vali Loss: 0.5875788 Test Loss: 0.4037207
Validation loss decreased (0.589055 --> 0.587579).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 10.524178981781006
Epoch: 45, Steps: 65 | Train Loss: 0.1034025 Vali Loss: 0.5872316 Test Loss: 0.4032643
Validation loss decreased (0.587579 --> 0.587232).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 6.033704042434692
Epoch: 46, Steps: 65 | Train Loss: 0.1029221 Vali Loss: 0.5868003 Test Loss: 0.4027442
Validation loss decreased (0.587232 --> 0.586800).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 10.39023470878601
Epoch: 47, Steps: 65 | Train Loss: 0.1024929 Vali Loss: 0.5856733 Test Loss: 0.4023467
Validation loss decreased (0.586800 --> 0.585673).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 11.433621406555176
Epoch: 48, Steps: 65 | Train Loss: 0.1022017 Vali Loss: 0.5853427 Test Loss: 0.4018868
Validation loss decreased (0.585673 --> 0.585343).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 11.67069387435913
Epoch: 49, Steps: 65 | Train Loss: 0.1017211 Vali Loss: 0.5853598 Test Loss: 0.4015200
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 10.119274139404297
Epoch: 50, Steps: 65 | Train Loss: 0.1014118 Vali Loss: 0.5847887 Test Loss: 0.4011746
Validation loss decreased (0.585343 --> 0.584789).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=106, out_features=134, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50907136.0
params:  14338.0
Trainable parameters:  14338
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.686371088027954
Epoch: 1, Steps: 65 | Train Loss: 0.3144401 Vali Loss: 0.5362504 Test Loss: 0.3592311
Validation loss decreased (inf --> 0.536250).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.666849613189697
Epoch: 2, Steps: 65 | Train Loss: 0.3030599 Vali Loss: 0.5232220 Test Loss: 0.3475752
Validation loss decreased (0.536250 --> 0.523222).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.370131015777588
Epoch: 3, Steps: 65 | Train Loss: 0.3003365 Vali Loss: 0.5199459 Test Loss: 0.3432539
Validation loss decreased (0.523222 --> 0.519946).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.552742958068848
Epoch: 4, Steps: 65 | Train Loss: 0.2991383 Vali Loss: 0.5171623 Test Loss: 0.3411340
Validation loss decreased (0.519946 --> 0.517162).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.05333137512207
Epoch: 5, Steps: 65 | Train Loss: 0.2989534 Vali Loss: 0.5165425 Test Loss: 0.3400328
Validation loss decreased (0.517162 --> 0.516542).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.691176414489746
Epoch: 6, Steps: 65 | Train Loss: 0.2984777 Vali Loss: 0.5149546 Test Loss: 0.3394585
Validation loss decreased (0.516542 --> 0.514955).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.862216711044312
Epoch: 7, Steps: 65 | Train Loss: 0.2984480 Vali Loss: 0.5146664 Test Loss: 0.3390604
Validation loss decreased (0.514955 --> 0.514666).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.251789808273315
Epoch: 8, Steps: 65 | Train Loss: 0.2982244 Vali Loss: 0.5141968 Test Loss: 0.3395382
Validation loss decreased (0.514666 --> 0.514197).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.322940826416016
Epoch: 9, Steps: 65 | Train Loss: 0.2982066 Vali Loss: 0.5141293 Test Loss: 0.3389339
Validation loss decreased (0.514197 --> 0.514129).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.218964576721191
Epoch: 10, Steps: 65 | Train Loss: 0.2980101 Vali Loss: 0.5128014 Test Loss: 0.3388903
Validation loss decreased (0.514129 --> 0.512801).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.930915832519531
Epoch: 11, Steps: 65 | Train Loss: 0.2981619 Vali Loss: 0.5124433 Test Loss: 0.3388163
Validation loss decreased (0.512801 --> 0.512443).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.54762053489685
Epoch: 12, Steps: 65 | Train Loss: 0.2977688 Vali Loss: 0.5135569 Test Loss: 0.3388111
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.572736740112305
Epoch: 13, Steps: 65 | Train Loss: 0.2976219 Vali Loss: 0.5124369 Test Loss: 0.3387590
Validation loss decreased (0.512443 --> 0.512437).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.36692762374878
Epoch: 14, Steps: 65 | Train Loss: 0.2976455 Vali Loss: 0.5117040 Test Loss: 0.3388511
Validation loss decreased (0.512437 --> 0.511704).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.557899713516235
Epoch: 15, Steps: 65 | Train Loss: 0.2973906 Vali Loss: 0.5126316 Test Loss: 0.3388231
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.044307470321655
Epoch: 16, Steps: 65 | Train Loss: 0.2974566 Vali Loss: 0.5121332 Test Loss: 0.3383734
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.931416749954224
Epoch: 17, Steps: 65 | Train Loss: 0.2974599 Vali Loss: 0.5129294 Test Loss: 0.3387139
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.33870723843574524, mae:0.3687650263309479, rse:0.5540050864219666, corr:[0.54180396 0.5528189  0.5587019  0.55986226 0.5600353  0.56127965
 0.56335866 0.5651508  0.56588066 0.56574804 0.5656002  0.56581336
 0.56623304 0.566317   0.56567043 0.56439495 0.5629809  0.5618209
 0.5608539  0.55986536 0.55868655 0.55718607 0.5555002  0.5540659
 0.552914   0.5520556  0.55132294 0.5505538  0.54999673 0.5499105
 0.55041355 0.55138475 0.5523067  0.55284435 0.55274117 0.55237925
 0.5519483  0.5516661  0.55150056 0.5512174  0.5508597  0.5504463
 0.55015355 0.5501458  0.55021733 0.55016327 0.5500109  0.54982686
 0.54967165 0.5496181  0.5497383  0.5498664  0.5498608  0.549582
 0.5491793  0.54889727 0.5488902  0.5490104  0.54914856 0.5490515
 0.5487516  0.5484241  0.54826546 0.54825807 0.54844034 0.54868454
 0.5488248  0.5488226  0.54879105 0.54880345 0.5489124  0.5490119
 0.5489863  0.54879946 0.5485428  0.5482258  0.54788655 0.5475941
 0.54739285 0.54731745 0.5473521  0.5473999  0.54740334 0.5472678
 0.5470431  0.54674935 0.5464728  0.54622537 0.5460659  0.5460882
 0.54625005 0.54645246 0.54661584 0.5466988  0.5466817  0.5465441
 0.5463471  0.5461651  0.5458547  0.54544526 0.5450235  0.5447277
 0.5446482  0.5446868  0.54477715 0.544842   0.5446956  0.54440755
 0.54405934 0.54383045 0.54378223 0.5438094  0.5437676  0.5435585
 0.54314554 0.542629   0.54215974 0.5417967  0.5415729  0.54147816
 0.54139525 0.5412441  0.5410076  0.5408276  0.5407321  0.5405903
 0.5403401  0.54007226 0.5398851  0.5399294  0.5401223  0.5402856
 0.54030675 0.5400926  0.5396212  0.539144   0.53896827 0.53899777
 0.5391822  0.53939325 0.53945434 0.5394901  0.5397024  0.54016453
 0.54072106 0.54102325 0.5409783  0.5406447  0.5403373  0.5402474
 0.5404447  0.5408908  0.5412884  0.541375   0.54104316 0.54052055
 0.5400651  0.53980345 0.5397948  0.53978854 0.5397705  0.53967166
 0.5395682  0.5395202  0.53960663 0.53981495 0.5400843  0.5403087
 0.5403664  0.54021496 0.54007626 0.54011196 0.54020345 0.5402553
 0.5401511  0.53985983 0.5394525  0.53916204 0.5391141  0.53936285
 0.53960866 0.5395498  0.53903013 0.53840876 0.5381666  0.5386397
 0.539592   0.5406425  0.5414341  0.5418445  0.5418108  0.54122967]
