Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9728768.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4298798
	speed: 0.1493s/iter; left time: 3890.6654s
	iters: 200, epoch: 1 | loss: 0.3465030
	speed: 0.1337s/iter; left time: 3469.3209s
	iters: 300, epoch: 1 | loss: 0.2715795
	speed: 0.1365s/iter; left time: 3529.7916s
	iters: 400, epoch: 1 | loss: 0.2495411
	speed: 0.1332s/iter; left time: 3429.4672s
	iters: 500, epoch: 1 | loss: 0.2297404
	speed: 0.1332s/iter; left time: 3417.3715s
Epoch: 1 cost time: 71.62297773361206
Epoch: 1, Steps: 523 | Train Loss: 0.3402379 Vali Loss: 0.8216779 Test Loss: 0.4753892
Validation loss decreased (inf --> 0.821678).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1970435
	speed: 0.9186s/iter; left time: 23450.7912s
	iters: 200, epoch: 2 | loss: 0.1834427
	speed: 0.1166s/iter; left time: 2966.1126s
	iters: 300, epoch: 2 | loss: 0.1620351
	speed: 0.1199s/iter; left time: 3037.5847s
	iters: 400, epoch: 2 | loss: 0.1659755
	speed: 0.1087s/iter; left time: 2742.7050s
	iters: 500, epoch: 2 | loss: 0.1660211
	speed: 0.1073s/iter; left time: 2697.4904s
Epoch: 2 cost time: 61.76957988739014
Epoch: 2, Steps: 523 | Train Loss: 0.1804083 Vali Loss: 0.7313911 Test Loss: 0.4089869
Validation loss decreased (0.821678 --> 0.731391).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1634731
	speed: 0.9846s/iter; left time: 24619.4039s
	iters: 200, epoch: 3 | loss: 0.1514363
	speed: 0.1397s/iter; left time: 3478.0484s
	iters: 300, epoch: 3 | loss: 0.1396068
	speed: 0.1345s/iter; left time: 3336.3996s
	iters: 400, epoch: 3 | loss: 0.1429489
	speed: 0.1393s/iter; left time: 3441.4086s
	iters: 500, epoch: 3 | loss: 0.1292802
	speed: 0.1503s/iter; left time: 3697.0273s
Epoch: 3 cost time: 74.67813205718994
Epoch: 3, Steps: 523 | Train Loss: 0.1431705 Vali Loss: 0.6951684 Test Loss: 0.3836071
Validation loss decreased (0.731391 --> 0.695168).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1457712
	speed: 0.9939s/iter; left time: 24332.4437s
	iters: 200, epoch: 4 | loss: 0.1546190
	speed: 0.1441s/iter; left time: 3514.2612s
	iters: 300, epoch: 4 | loss: 0.1347772
	speed: 0.1401s/iter; left time: 3401.3525s
	iters: 400, epoch: 4 | loss: 0.1099024
	speed: 0.1291s/iter; left time: 3120.8957s
	iters: 500, epoch: 4 | loss: 0.1196479
	speed: 0.1279s/iter; left time: 3080.6470s
Epoch: 4 cost time: 73.36055541038513
Epoch: 4, Steps: 523 | Train Loss: 0.1294133 Vali Loss: 0.6801537 Test Loss: 0.3736998
Validation loss decreased (0.695168 --> 0.680154).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1379142
	speed: 0.7558s/iter; left time: 18107.6574s
	iters: 200, epoch: 5 | loss: 0.1210905
	speed: 0.1173s/iter; left time: 2798.7078s
	iters: 300, epoch: 5 | loss: 0.1212307
	speed: 0.1292s/iter; left time: 3068.7022s
	iters: 400, epoch: 5 | loss: 0.1122693
	speed: 0.1365s/iter; left time: 3229.0000s
	iters: 500, epoch: 5 | loss: 0.1320477
	speed: 0.1009s/iter; left time: 2377.0888s
Epoch: 5 cost time: 64.35216355323792
Epoch: 5, Steps: 523 | Train Loss: 0.1238620 Vali Loss: 0.6757984 Test Loss: 0.3689731
Validation loss decreased (0.680154 --> 0.675798).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1299903
	speed: 0.8544s/iter; left time: 20023.9118s
	iters: 200, epoch: 6 | loss: 0.1308848
	speed: 0.1318s/iter; left time: 3075.5275s
	iters: 300, epoch: 6 | loss: 0.1064238
	speed: 0.1261s/iter; left time: 2931.1397s
	iters: 400, epoch: 6 | loss: 0.1228103
	speed: 0.1281s/iter; left time: 2963.5581s
	iters: 500, epoch: 6 | loss: 0.1170911
	speed: 0.1227s/iter; left time: 2826.3883s
Epoch: 6 cost time: 68.44992184638977
Epoch: 6, Steps: 523 | Train Loss: 0.1214970 Vali Loss: 0.6735224 Test Loss: 0.3691255
Validation loss decreased (0.675798 --> 0.673522).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1254041
	speed: 0.8892s/iter; left time: 20374.9708s
	iters: 200, epoch: 7 | loss: 0.1214221
	speed: 0.1256s/iter; left time: 2864.3643s
	iters: 300, epoch: 7 | loss: 0.1273798
	speed: 0.1371s/iter; left time: 3113.5171s
	iters: 400, epoch: 7 | loss: 0.1279895
	speed: 0.1373s/iter; left time: 3104.8128s
	iters: 500, epoch: 7 | loss: 0.1218791
	speed: 0.1378s/iter; left time: 3101.1863s
Epoch: 7 cost time: 70.22962760925293
Epoch: 7, Steps: 523 | Train Loss: 0.1205573 Vali Loss: 0.6770529 Test Loss: 0.3697476
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1215913
	speed: 0.9223s/iter; left time: 20650.0882s
	iters: 200, epoch: 8 | loss: 0.1261127
	speed: 0.1289s/iter; left time: 2872.9765s
	iters: 300, epoch: 8 | loss: 0.1309788
	speed: 0.1272s/iter; left time: 2822.3649s
	iters: 400, epoch: 8 | loss: 0.1167714
	speed: 0.1293s/iter; left time: 2856.2730s
	iters: 500, epoch: 8 | loss: 0.1137340
	speed: 0.1304s/iter; left time: 2866.7517s
Epoch: 8 cost time: 69.21684741973877
Epoch: 8, Steps: 523 | Train Loss: 0.1201642 Vali Loss: 0.6707762 Test Loss: 0.3709725
Validation loss decreased (0.673522 --> 0.670776).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1239908
	speed: 0.9000s/iter; left time: 19680.2972s
	iters: 200, epoch: 9 | loss: 0.1267861
	speed: 0.1374s/iter; left time: 2989.8129s
	iters: 300, epoch: 9 | loss: 0.1184960
	speed: 0.1395s/iter; left time: 3021.9203s
	iters: 400, epoch: 9 | loss: 0.1210939
	speed: 0.1403s/iter; left time: 3025.5746s
	iters: 500, epoch: 9 | loss: 0.1196839
	speed: 0.1289s/iter; left time: 2767.4357s
Epoch: 9 cost time: 71.12438106536865
Epoch: 9, Steps: 523 | Train Loss: 0.1200616 Vali Loss: 0.6748537 Test Loss: 0.3712772
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1368926
	speed: 0.9187s/iter; left time: 19609.6078s
	iters: 200, epoch: 10 | loss: 0.1239076
	speed: 0.1294s/iter; left time: 2749.7258s
	iters: 300, epoch: 10 | loss: 0.1232713
	speed: 0.1321s/iter; left time: 2792.2344s
	iters: 400, epoch: 10 | loss: 0.1102851
	speed: 0.1322s/iter; left time: 2783.0640s
	iters: 500, epoch: 10 | loss: 0.1199974
	speed: 0.1442s/iter; left time: 3020.1198s
Epoch: 10 cost time: 70.8574767112732
Epoch: 10, Steps: 523 | Train Loss: 0.1199874 Vali Loss: 0.6735073 Test Loss: 0.3708161
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1177016
	speed: 0.9227s/iter; left time: 19211.3467s
	iters: 200, epoch: 11 | loss: 0.1220053
	speed: 0.1365s/iter; left time: 2827.5313s
	iters: 300, epoch: 11 | loss: 0.1222616
	speed: 0.1382s/iter; left time: 2849.5028s
	iters: 400, epoch: 11 | loss: 0.1213870
	speed: 0.1303s/iter; left time: 2673.4151s
	iters: 500, epoch: 11 | loss: 0.1263044
	speed: 0.1285s/iter; left time: 2625.1194s
Epoch: 11 cost time: 70.89994931221008
Epoch: 11, Steps: 523 | Train Loss: 0.1199914 Vali Loss: 0.6741929 Test Loss: 0.3716617
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=122, out_features=178, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9728768.0
params:  21894.0
Trainable parameters:  21894
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3846055
	speed: 0.1372s/iter; left time: 3573.0092s
	iters: 200, epoch: 1 | loss: 0.3278130
	speed: 0.1305s/iter; left time: 3386.8714s
	iters: 300, epoch: 1 | loss: 0.2994172
	speed: 0.1257s/iter; left time: 3248.2473s
	iters: 400, epoch: 1 | loss: 0.3069204
	speed: 0.1363s/iter; left time: 3509.0460s
	iters: 500, epoch: 1 | loss: 0.3547653
	speed: 0.1187s/iter; left time: 3044.0584s
Epoch: 1 cost time: 67.42890429496765
Epoch: 1, Steps: 523 | Train Loss: 0.3390382 Vali Loss: 0.6598894 Test Loss: 0.3678774
Validation loss decreased (inf --> 0.659889).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3874071
	speed: 0.7428s/iter; left time: 18961.6632s
	iters: 200, epoch: 2 | loss: 0.3403540
	speed: 0.1147s/iter; left time: 2917.3189s
	iters: 300, epoch: 2 | loss: 0.3701591
	speed: 0.1066s/iter; left time: 2700.3196s
	iters: 400, epoch: 2 | loss: 0.3107435
	speed: 0.1370s/iter; left time: 3456.6824s
	iters: 500, epoch: 2 | loss: 0.2878446
	speed: 0.1359s/iter; left time: 3414.7650s
Epoch: 2 cost time: 65.23811316490173
Epoch: 2, Steps: 523 | Train Loss: 0.3375794 Vali Loss: 0.6546330 Test Loss: 0.3675158
Validation loss decreased (0.659889 --> 0.654633).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2881712
	speed: 0.9721s/iter; left time: 24306.3495s
	iters: 200, epoch: 3 | loss: 0.3254822
	speed: 0.1313s/iter; left time: 3271.1671s
	iters: 300, epoch: 3 | loss: 0.3153422
	speed: 0.1448s/iter; left time: 3592.2409s
	iters: 400, epoch: 3 | loss: 0.3348941
	speed: 0.1407s/iter; left time: 3476.0628s
	iters: 500, epoch: 3 | loss: 0.3145987
	speed: 0.1414s/iter; left time: 3479.1674s
Epoch: 3 cost time: 73.0212836265564
Epoch: 3, Steps: 523 | Train Loss: 0.3372179 Vali Loss: 0.6576223 Test Loss: 0.3678594
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3577108
	speed: 0.9237s/iter; left time: 22613.6612s
	iters: 200, epoch: 4 | loss: 0.3302057
	speed: 0.1231s/iter; left time: 3002.6301s
	iters: 300, epoch: 4 | loss: 0.3811989
	speed: 0.1266s/iter; left time: 3073.5700s
	iters: 400, epoch: 4 | loss: 0.3287511
	speed: 0.1247s/iter; left time: 3016.3695s
	iters: 500, epoch: 4 | loss: 0.3676429
	speed: 0.1264s/iter; left time: 3044.1884s
Epoch: 4 cost time: 67.84349799156189
Epoch: 4, Steps: 523 | Train Loss: 0.3369601 Vali Loss: 0.6536425 Test Loss: 0.3667683
Validation loss decreased (0.654633 --> 0.653642).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3555274
	speed: 0.9228s/iter; left time: 22109.0231s
	iters: 200, epoch: 5 | loss: 0.3308047
	speed: 0.1096s/iter; left time: 2615.1278s
	iters: 300, epoch: 5 | loss: 0.3455323
	speed: 0.1095s/iter; left time: 2601.5663s
	iters: 400, epoch: 5 | loss: 0.3732970
	speed: 0.1200s/iter; left time: 2838.4095s
	iters: 500, epoch: 5 | loss: 0.3194668
	speed: 0.1270s/iter; left time: 2991.1456s
Epoch: 5 cost time: 63.352967977523804
Epoch: 5, Steps: 523 | Train Loss: 0.3368233 Vali Loss: 0.6554870 Test Loss: 0.3666233
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2947946
	speed: 0.9445s/iter; left time: 22135.6386s
	iters: 200, epoch: 6 | loss: 0.3553736
	speed: 0.1299s/iter; left time: 3031.6856s
	iters: 300, epoch: 6 | loss: 0.3456022
	speed: 0.1318s/iter; left time: 3062.1368s
	iters: 400, epoch: 6 | loss: 0.3217567
	speed: 0.1366s/iter; left time: 3160.6000s
	iters: 500, epoch: 6 | loss: 0.3484347
	speed: 0.1443s/iter; left time: 3324.5251s
Epoch: 6 cost time: 72.0262291431427
Epoch: 6, Steps: 523 | Train Loss: 0.3367094 Vali Loss: 0.6560002 Test Loss: 0.3672352
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3134995
	speed: 0.9555s/iter; left time: 21894.0653s
	iters: 200, epoch: 7 | loss: 0.3477204
	speed: 0.1409s/iter; left time: 3214.5932s
	iters: 300, epoch: 7 | loss: 0.3408571
	speed: 0.1337s/iter; left time: 3035.7698s
	iters: 400, epoch: 7 | loss: 0.3771691
	speed: 0.1346s/iter; left time: 3044.1601s
	iters: 500, epoch: 7 | loss: 0.3297405
	speed: 0.1324s/iter; left time: 2980.0298s
Epoch: 7 cost time: 73.10508012771606
Epoch: 7, Steps: 523 | Train Loss: 0.3365518 Vali Loss: 0.6562936 Test Loss: 0.3664693
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.36666491627693176, mae:0.3854444622993469, rse:0.5762129426002502, corr:[0.5367027  0.5486657  0.55320966 0.55359304 0.554524   0.5569814
 0.55930245 0.5599733  0.5593398  0.5587197  0.5589783  0.55974734
 0.5601718  0.5597553  0.5587104  0.5575319  0.5565015  0.55558354
 0.55455613 0.55333954 0.55195034 0.5503723  0.54868585 0.5471429
 0.5457964  0.5448281  0.54411083 0.543398   0.54286003 0.5427107
 0.5431326  0.544134   0.5452312  0.54598004 0.54604185 0.5458502
 0.54573274 0.5459179  0.54616666 0.54602444 0.5454946  0.5447799
 0.5443052  0.5442861  0.5443197  0.54406184 0.5436511  0.54343224
 0.5436556  0.5441234  0.5444146  0.54407704 0.5432248  0.5424227
 0.54238415 0.5431406  0.54406476 0.5444818  0.5442722  0.5436891
 0.5433126  0.5434274  0.5437702  0.5438302  0.54351383 0.5430356
 0.5427831  0.5430131  0.5435957  0.54415536 0.54442936 0.54430276
 0.5438541  0.5432798  0.54278696 0.54239845 0.5420239  0.54162467
 0.5412214  0.5409294  0.54086065 0.5410331  0.5413307  0.54149187
 0.5414197  0.5411346  0.54084677 0.54074585 0.54095376 0.5414324
 0.54195213 0.5423386  0.5425736  0.54274315 0.54288536 0.5429113
 0.5426992  0.54222435 0.54147035 0.540747   0.5403311  0.5402521
 0.5402792  0.54004294 0.5395287  0.5389631  0.5385223  0.53837407
 0.5383306  0.538182   0.53781587 0.5373197  0.53695214 0.53690284
 0.5370261  0.536955   0.5364175  0.5354768  0.5345515  0.5341007
 0.53427887 0.53487825 0.53541034 0.5355966  0.53544    0.5351112
 0.53487    0.5348863  0.5350539  0.53522    0.5351911  0.53499043
 0.53485435 0.5349219  0.5351312  0.5354189  0.5357148  0.5358085
 0.53578365 0.53576916 0.5357819  0.5358713  0.5359618  0.5359034
 0.53560036 0.5350934  0.53471005 0.53468627 0.53502846 0.53543854
 0.5356669  0.5357016  0.53562486 0.5355979  0.5356664  0.53580743
 0.5358728  0.5357874  0.5357541  0.5358749  0.5361904  0.5364136
 0.536258   0.535661   0.534976   0.5346887  0.5350127  0.53574467
 0.53640145 0.536597   0.5363769  0.5360154  0.53574145 0.53576016
 0.5361182  0.5366298  0.5369915  0.5371075  0.53706086 0.5370224
 0.5370422  0.53711206 0.5371081  0.5370557  0.5370163  0.5370207
 0.5370513  0.53714573 0.5372888  0.53740335 0.5374811  0.5375276
 0.5374934  0.53739876 0.5371521  0.5367749  0.53636724 0.5360025
 0.5355862  0.5350126  0.5342109  0.53326374 0.5323374  0.53161275
 0.53115547 0.53086114 0.5305481  0.5301783  0.52976686 0.5293712
 0.5289703  0.5284909  0.52777195 0.5267923  0.52563775 0.5246723
 0.5241251  0.5238459  0.5235979  0.52311254 0.5224445  0.52195686
 0.52201974 0.5225486  0.5233045  0.52389944 0.52402246 0.523689
 0.5231427  0.52269876 0.5225777  0.52273846 0.52306885 0.5234565
 0.5238323  0.5242534  0.52459246 0.5249548  0.52521366 0.525344
 0.52524287 0.5248408  0.5243113  0.5238189  0.52349263 0.5233382
 0.52338135 0.5236595  0.5240552  0.5244102  0.5244784  0.52432066
 0.5240125  0.5238737  0.5239279  0.5240251  0.52400947 0.52383995
 0.523609   0.5234991  0.5235437  0.5236764  0.52383095 0.52408415
 0.524347   0.52457714 0.52469593 0.52458733 0.5241711  0.5235446
 0.52303094 0.5228012  0.5229007  0.52321064 0.5234615  0.5234882
 0.5232502  0.52300763 0.5228867  0.5230157  0.5232227  0.5233444
 0.5233906  0.5234264  0.5235151  0.5237191  0.52390164 0.5238168
 0.5233996  0.5228351  0.52224904 0.5216195  0.52099735 0.5203117
 0.5195279  0.51862174 0.5178277  0.5172008  0.5167219  0.5163119
 0.5158695  0.51548076 0.51530826 0.51533324 0.5153581  0.515143
 0.514796   0.51437634 0.51410264 0.5141077  0.5143778  0.51462185
 0.5146397  0.51440954 0.51393133 0.5134078  0.5130388  0.51302606
 0.5134128  0.5139518  0.5143943  0.51455796 0.5144194  0.5141044
 0.5137457  0.5134646  0.5132051  0.51309955 0.5131959  0.51337254
 0.5134796  0.51325077 0.51281625 0.5128076  0.5127164  0.50862664]
