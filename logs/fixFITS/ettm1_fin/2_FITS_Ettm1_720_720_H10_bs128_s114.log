Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  29030400.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4914834
	speed: 0.1698s/iter; left time: 1078.6479s
Epoch: 1 cost time: 22.09495258331299
Epoch: 1, Steps: 129 | Train Loss: 0.5699670 Vali Loss: 1.3422196 Test Loss: 0.6763324
Validation loss decreased (inf --> 1.342220).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3781369
	speed: 0.4490s/iter; left time: 2793.8704s
Epoch: 2 cost time: 22.041945934295654
Epoch: 2, Steps: 129 | Train Loss: 0.3951777 Vali Loss: 1.1795883 Test Loss: 0.5725201
Validation loss decreased (1.342220 --> 1.179588).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3196481
	speed: 0.4516s/iter; left time: 2751.6410s
Epoch: 3 cost time: 20.976749658584595
Epoch: 3, Steps: 129 | Train Loss: 0.3303865 Vali Loss: 1.1031568 Test Loss: 0.5234985
Validation loss decreased (1.179588 --> 1.103157).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2835979
	speed: 0.4209s/iter; left time: 2510.3993s
Epoch: 4 cost time: 19.987870454788208
Epoch: 4, Steps: 129 | Train Loss: 0.2956559 Vali Loss: 1.0604783 Test Loss: 0.4958730
Validation loss decreased (1.103157 --> 1.060478).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2734825
	speed: 0.4091s/iter; left time: 2386.9098s
Epoch: 5 cost time: 20.229083776474
Epoch: 5, Steps: 129 | Train Loss: 0.2739031 Vali Loss: 1.0309019 Test Loss: 0.4776213
Validation loss decreased (1.060478 --> 1.030902).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2650331
	speed: 0.4483s/iter; left time: 2557.9088s
Epoch: 6 cost time: 21.97647190093994
Epoch: 6, Steps: 129 | Train Loss: 0.2593243 Vali Loss: 1.0130907 Test Loss: 0.4654174
Validation loss decreased (1.030902 --> 1.013091).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2594414
	speed: 0.4487s/iter; left time: 2502.2586s
Epoch: 7 cost time: 22.529557466506958
Epoch: 7, Steps: 129 | Train Loss: 0.2492770 Vali Loss: 0.9984807 Test Loss: 0.4559019
Validation loss decreased (1.013091 --> 0.998481).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2394029
	speed: 0.4321s/iter; left time: 2353.9637s
Epoch: 8 cost time: 20.299699068069458
Epoch: 8, Steps: 129 | Train Loss: 0.2417991 Vali Loss: 0.9885375 Test Loss: 0.4495555
Validation loss decreased (0.998481 --> 0.988537).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2424443
	speed: 0.4258s/iter; left time: 2264.5837s
Epoch: 9 cost time: 20.332841634750366
Epoch: 9, Steps: 129 | Train Loss: 0.2361880 Vali Loss: 0.9793429 Test Loss: 0.4440426
Validation loss decreased (0.988537 --> 0.979343).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2253912
	speed: 0.4124s/iter; left time: 2140.1654s
Epoch: 10 cost time: 19.61812448501587
Epoch: 10, Steps: 129 | Train Loss: 0.2318179 Vali Loss: 0.9729338 Test Loss: 0.4399924
Validation loss decreased (0.979343 --> 0.972934).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2228137
	speed: 0.4061s/iter; left time: 2055.4824s
Epoch: 11 cost time: 20.072641372680664
Epoch: 11, Steps: 129 | Train Loss: 0.2284402 Vali Loss: 0.9681275 Test Loss: 0.4361224
Validation loss decreased (0.972934 --> 0.968127).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2252676
	speed: 0.4134s/iter; left time: 2039.0329s
Epoch: 12 cost time: 19.840933799743652
Epoch: 12, Steps: 129 | Train Loss: 0.2256182 Vali Loss: 0.9624912 Test Loss: 0.4330125
Validation loss decreased (0.968127 --> 0.962491).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2130180
	speed: 0.4088s/iter; left time: 1963.3663s
Epoch: 13 cost time: 19.905393362045288
Epoch: 13, Steps: 129 | Train Loss: 0.2233912 Vali Loss: 0.9589505 Test Loss: 0.4302997
Validation loss decreased (0.962491 --> 0.958951).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2208535
	speed: 0.4261s/iter; left time: 1991.7747s
Epoch: 14 cost time: 21.792909145355225
Epoch: 14, Steps: 129 | Train Loss: 0.2215682 Vali Loss: 0.9557509 Test Loss: 0.4284318
Validation loss decreased (0.958951 --> 0.955751).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2128326
	speed: 0.4793s/iter; left time: 2178.4695s
Epoch: 15 cost time: 23.3255774974823
Epoch: 15, Steps: 129 | Train Loss: 0.2200514 Vali Loss: 0.9518122 Test Loss: 0.4265263
Validation loss decreased (0.955751 --> 0.951812).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2053433
	speed: 0.4816s/iter; left time: 2126.7537s
Epoch: 16 cost time: 23.84785795211792
Epoch: 16, Steps: 129 | Train Loss: 0.2187252 Vali Loss: 0.9506997 Test Loss: 0.4249497
Validation loss decreased (0.951812 --> 0.950700).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2285981
	speed: 0.4744s/iter; left time: 2033.5558s
Epoch: 17 cost time: 22.52192735671997
Epoch: 17, Steps: 129 | Train Loss: 0.2176331 Vali Loss: 0.9495313 Test Loss: 0.4238895
Validation loss decreased (0.950700 --> 0.949531).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2172575
	speed: 0.4545s/iter; left time: 1889.8789s
Epoch: 18 cost time: 21.57585644721985
Epoch: 18, Steps: 129 | Train Loss: 0.2167049 Vali Loss: 0.9479936 Test Loss: 0.4227932
Validation loss decreased (0.949531 --> 0.947994).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2089417
	speed: 0.4425s/iter; left time: 1782.9969s
Epoch: 19 cost time: 21.548719882965088
Epoch: 19, Steps: 129 | Train Loss: 0.2158562 Vali Loss: 0.9462445 Test Loss: 0.4218030
Validation loss decreased (0.947994 --> 0.946244).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2131332
	speed: 0.4571s/iter; left time: 1782.6952s
Epoch: 20 cost time: 23.225771188735962
Epoch: 20, Steps: 129 | Train Loss: 0.2151960 Vali Loss: 0.9459493 Test Loss: 0.4210349
Validation loss decreased (0.946244 --> 0.945949).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2057709
	speed: 0.4724s/iter; left time: 1781.4264s
Epoch: 21 cost time: 21.33599328994751
Epoch: 21, Steps: 129 | Train Loss: 0.2146079 Vali Loss: 0.9441580 Test Loss: 0.4203967
Validation loss decreased (0.945949 --> 0.944158).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2141558
	speed: 0.4531s/iter; left time: 1650.1703s
Epoch: 22 cost time: 22.56800866127014
Epoch: 22, Steps: 129 | Train Loss: 0.2140812 Vali Loss: 0.9432576 Test Loss: 0.4198425
Validation loss decreased (0.944158 --> 0.943258).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2117315
	speed: 0.4213s/iter; left time: 1479.8526s
Epoch: 23 cost time: 20.520954370498657
Epoch: 23, Steps: 129 | Train Loss: 0.2135433 Vali Loss: 0.9427293 Test Loss: 0.4194653
Validation loss decreased (0.943258 --> 0.942729).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2116626
	speed: 0.4511s/iter; left time: 1526.6441s
Epoch: 24 cost time: 21.748842239379883
Epoch: 24, Steps: 129 | Train Loss: 0.2131912 Vali Loss: 0.9425156 Test Loss: 0.4191722
Validation loss decreased (0.942729 --> 0.942516).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1995146
	speed: 0.4410s/iter; left time: 1435.4910s
Epoch: 25 cost time: 22.159525632858276
Epoch: 25, Steps: 129 | Train Loss: 0.2128339 Vali Loss: 0.9414172 Test Loss: 0.4188646
Validation loss decreased (0.942516 --> 0.941417).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1963189
	speed: 0.4565s/iter; left time: 1427.1463s
Epoch: 26 cost time: 22.18666362762451
Epoch: 26, Steps: 129 | Train Loss: 0.2125102 Vali Loss: 0.9412693 Test Loss: 0.4186505
Validation loss decreased (0.941417 --> 0.941269).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2214861
	speed: 0.4544s/iter; left time: 1361.8189s
Epoch: 27 cost time: 23.60921597480774
Epoch: 27, Steps: 129 | Train Loss: 0.2121855 Vali Loss: 0.9405643 Test Loss: 0.4183773
Validation loss decreased (0.941269 --> 0.940564).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2011860
	speed: 0.4661s/iter; left time: 1336.8996s
Epoch: 28 cost time: 22.727850675582886
Epoch: 28, Steps: 129 | Train Loss: 0.2120065 Vali Loss: 0.9406480 Test Loss: 0.4182240
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2107991
	speed: 0.4501s/iter; left time: 1232.7284s
Epoch: 29 cost time: 22.042663097381592
Epoch: 29, Steps: 129 | Train Loss: 0.2118052 Vali Loss: 0.9400972 Test Loss: 0.4181390
Validation loss decreased (0.940564 --> 0.940097).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2212705
	speed: 0.4403s/iter; left time: 1149.1870s
Epoch: 30 cost time: 20.902803421020508
Epoch: 30, Steps: 129 | Train Loss: 0.2116379 Vali Loss: 0.9402527 Test Loss: 0.4179245
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2098042
	speed: 0.4229s/iter; left time: 1049.2395s
Epoch: 31 cost time: 20.659213542938232
Epoch: 31, Steps: 129 | Train Loss: 0.2113803 Vali Loss: 0.9405224 Test Loss: 0.4177936
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2130266
	speed: 0.4099s/iter; left time: 964.0480s
Epoch: 32 cost time: 20.62532353401184
Epoch: 32, Steps: 129 | Train Loss: 0.2112633 Vali Loss: 0.9397170 Test Loss: 0.4176941
Validation loss decreased (0.940097 --> 0.939717).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2136085
	speed: 0.4363s/iter; left time: 969.9381s
Epoch: 33 cost time: 21.35472011566162
Epoch: 33, Steps: 129 | Train Loss: 0.2111311 Vali Loss: 0.9391342 Test Loss: 0.4177163
Validation loss decreased (0.939717 --> 0.939134).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2170529
	speed: 0.4480s/iter; left time: 938.0130s
Epoch: 34 cost time: 22.106294870376587
Epoch: 34, Steps: 129 | Train Loss: 0.2109678 Vali Loss: 0.9398431 Test Loss: 0.4177409
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2226351
	speed: 0.4243s/iter; left time: 833.8018s
Epoch: 35 cost time: 21.209965467453003
Epoch: 35, Steps: 129 | Train Loss: 0.2108892 Vali Loss: 0.9398764 Test Loss: 0.4176803
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1982348
	speed: 0.4610s/iter; left time: 846.3097s
Epoch: 36 cost time: 21.917554140090942
Epoch: 36, Steps: 129 | Train Loss: 0.2107707 Vali Loss: 0.9394611 Test Loss: 0.4177670
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  29030400.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3941841
	speed: 0.1815s/iter; left time: 1152.6157s
Epoch: 1 cost time: 23.519214868545532
Epoch: 1, Steps: 129 | Train Loss: 0.3988028 Vali Loss: 0.9352174 Test Loss: 0.4179661
Validation loss decreased (inf --> 0.935217).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3841874
	speed: 0.5299s/iter; left time: 3297.0568s
Epoch: 2 cost time: 24.102664709091187
Epoch: 2, Steps: 129 | Train Loss: 0.3979268 Vali Loss: 0.9322591 Test Loss: 0.4188054
Validation loss decreased (0.935217 --> 0.932259).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4045713
	speed: 0.4913s/iter; left time: 2993.5112s
Epoch: 3 cost time: 25.38747787475586
Epoch: 3, Steps: 129 | Train Loss: 0.3977723 Vali Loss: 0.9325164 Test Loss: 0.4184179
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3885904
	speed: 0.5215s/iter; left time: 3110.0457s
Epoch: 4 cost time: 24.347729682922363
Epoch: 4, Steps: 129 | Train Loss: 0.3973936 Vali Loss: 0.9328039 Test Loss: 0.4179164
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3822006
	speed: 0.5089s/iter; left time: 2969.6905s
Epoch: 5 cost time: 25.16003131866455
Epoch: 5, Steps: 129 | Train Loss: 0.3972206 Vali Loss: 0.9325103 Test Loss: 0.4182083
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41597336530685425, mae:0.4120465815067291, rse:0.6136256456375122, corr:[0.52646935 0.53556806 0.54222476 0.5451907  0.5452329  0.5443283
 0.5438437  0.544188   0.5451574  0.5461396  0.5465655  0.5462415
 0.54542977 0.5444709  0.54352415 0.5425704  0.5414949  0.54021424
 0.53871083 0.5371146  0.5356314  0.53434366 0.5332178  0.53222203
 0.5311527  0.5300402  0.5289892  0.5281669  0.5278538  0.5281242
 0.528794   0.529558   0.5300231  0.53006834 0.52969563 0.5292453
 0.5289112  0.528858   0.5290252  0.52916646 0.5291602  0.5289135
 0.52852863 0.5282395  0.52815104 0.5282793  0.52856106 0.52880555
 0.5288209  0.5285529  0.5281792  0.5278786  0.5278118  0.5279004
 0.5280535  0.5281444  0.52810585 0.5279327  0.5277401  0.527593
 0.52756757 0.52766776 0.52778745 0.5277901  0.527703   0.5275909
 0.52751464 0.52753854 0.5277008  0.52793294 0.52816767 0.5282985
 0.5282583  0.52806985 0.5278294  0.5275663  0.527282   0.52698416
 0.5266606  0.52632415 0.5259902  0.5256921  0.525477   0.5253608
 0.52539665 0.5255279  0.5257383  0.5259717  0.5262198  0.52650595
 0.5268376  0.5271876  0.52753085 0.5277941  0.52794313 0.5279001
 0.52768517 0.5273761  0.526927   0.52649224 0.5261234  0.5258468
 0.5256649  0.5255054  0.5253383  0.5251745  0.5249282  0.5246437
 0.52431756 0.52400076 0.52369994 0.5233856  0.52304476 0.5226797
 0.52229464 0.5219234  0.5215986  0.5213388  0.5211557  0.5210852
 0.5210795  0.52108663 0.5210486  0.52102005 0.52101624 0.5209837
 0.52089405 0.52078295 0.52064395 0.52052563 0.5204184  0.52034384
 0.52033746 0.5203568  0.520287   0.52010834 0.51987815 0.51959217
 0.5193811  0.5193615  0.5194952  0.5197308  0.51997083 0.52012295
 0.5201206  0.5199346  0.5196892  0.5195035  0.51948065 0.5195871
 0.51975125 0.5199334  0.52005285 0.52006704 0.51997596 0.5199078
 0.51988435 0.51991594 0.52001625 0.520111   0.5202358  0.52036244
 0.5204691  0.5205378  0.52056944 0.520575   0.5206004  0.52070534
 0.5208545  0.52098864 0.5211214  0.5212177  0.52120996 0.52110475
 0.5209551  0.52081794 0.5207483  0.52075326 0.52079964 0.52087235
 0.52094895 0.52103925 0.52112037 0.5212467  0.52142584 0.5216372
 0.52188474 0.52215785 0.5224063  0.52255917 0.5225543  0.5223767
 0.52206695 0.5217312  0.5213452  0.52088666 0.5203383  0.5196967
 0.5189184  0.51801777 0.51700497 0.5159901  0.51506    0.5142954
 0.51373196 0.5133368  0.5130279  0.51274234 0.5123981  0.51195353
 0.5113963  0.51079476 0.51020443 0.5096467  0.50907254 0.5085186
 0.50801176 0.5075291  0.50713223 0.5068497  0.5067305  0.5068109
 0.50709355 0.50738823 0.5075989  0.5076914  0.50763935 0.5075224
 0.5074336  0.50743717 0.50752354 0.5076058  0.5076734  0.5077339
 0.50781953 0.5079838  0.5081689  0.5084426  0.5087074  0.5089408
 0.50906044 0.50898343 0.508772   0.5085355  0.5083539  0.5082271
 0.50814354 0.5080854  0.50802106 0.50796074 0.5078756  0.5078277
 0.50776124 0.50773376 0.50769734 0.5076239  0.50752413 0.5074189
 0.5073452  0.50734365 0.5074137  0.5075478  0.5077428  0.50805074
 0.5083539  0.50860006 0.5087361  0.5087615  0.5086691  0.50846624
 0.5082435  0.50805503 0.5079557  0.5079486  0.50799847 0.50807256
 0.50810784 0.5081682  0.5081938  0.508253   0.50831896 0.5083653
 0.50842994 0.50844955 0.50840086 0.5082885  0.50808495 0.5077695
 0.50735617 0.50690114 0.506397   0.50580865 0.50516737 0.50454384
 0.5040223  0.5036062  0.50333893 0.50313497 0.5028908  0.5025588
 0.50209194 0.5015375  0.5010049  0.5005566  0.50022495 0.49997923
 0.49987358 0.49980807 0.49974033 0.49964634 0.49956664 0.4994717
 0.49935904 0.49927223 0.49917656 0.49908516 0.49900696 0.4989687
 0.498933   0.49883425 0.49865294 0.498404   0.4981248  0.49786627
 0.4976812  0.49761623 0.49760866 0.49761817 0.4975626  0.4974212
 0.4972844  0.49717963 0.49712768 0.49716327 0.49726126 0.497449
 0.49761924 0.49761587 0.49751574 0.4973536  0.49718365 0.49703273
 0.4968638  0.49671447 0.49656948 0.4964547  0.49638617 0.49640876
 0.49645865 0.49656165 0.49667653 0.49674547 0.49677348 0.4967405
 0.49662757 0.49647784 0.49631533 0.4961667  0.49608833 0.49608615
 0.49606177 0.49603373 0.49595642 0.49583393 0.4956903  0.49555328
 0.49540508 0.49533328 0.49535745 0.49541405 0.49548885 0.49554545
 0.49555573 0.4955409  0.49552295 0.49556932 0.49569526 0.49594292
 0.49631044 0.49675375 0.49720478 0.49756446 0.49777672 0.49784088
 0.49774542 0.49750853 0.49718982 0.49680096 0.4963368  0.49594176
 0.49563047 0.4954066  0.49523172 0.4950855  0.49485743 0.49452013
 0.49406683 0.4935398  0.4930119  0.4925433  0.49211875 0.49181026
 0.49156195 0.4913818  0.4911572  0.49089682 0.49060637 0.4903546
 0.49016565 0.49007282 0.49006903 0.4901615  0.49033192 0.4904854
 0.4906814  0.49076593 0.49076653 0.4906058  0.49036184 0.49014655
 0.489956   0.48985925 0.4898006  0.48976585 0.48970217 0.48968688
 0.4897257  0.48987177 0.49008822 0.49042132 0.49074855 0.49107882
 0.4912682  0.49122217 0.49100786 0.49073756 0.49047038 0.49025705
 0.4900882  0.48993373 0.4897573  0.48953438 0.48928046 0.48905265
 0.4888705  0.4888054  0.48885256 0.48898637 0.48915923 0.4892807
 0.4893595  0.4893713  0.4893339  0.48927638 0.48925418 0.4893645
 0.48950744 0.48966274 0.48976257 0.48979682 0.48967928 0.48947352
 0.48921496 0.48902953 0.4889373  0.48899168 0.48915625 0.4893499
 0.48952514 0.4896181  0.48966032 0.4896885  0.48974872 0.4898549
 0.4899633  0.49006793 0.49010321 0.4899782  0.48965698 0.4891726
 0.48858938 0.48800975 0.4875125  0.48707703 0.48665375 0.48622915
 0.48581412 0.48531213 0.48475432 0.48419365 0.4836023  0.4829868
 0.48236364 0.48178545 0.4812399  0.48068115 0.48011717 0.47961086
 0.47917828 0.47888175 0.47870696 0.47850028 0.47831824 0.47809455
 0.47785863 0.4776172  0.47737905 0.47725627 0.4772835  0.47750935
 0.47789368 0.47828755 0.47858828 0.4788312  0.47899497 0.47910345
 0.47917205 0.4793099  0.47954032 0.4797608  0.47991392 0.4800169
 0.4800875  0.48015758 0.4802616  0.48038578 0.48061046 0.48096567
 0.4813425  0.4816128  0.48169893 0.4816769  0.48160252 0.4814804
 0.48132333 0.48113886 0.48094687 0.48077387 0.48062217 0.48048487
 0.48035458 0.4802182  0.48008808 0.47997162 0.47989187 0.4799113
 0.4800263  0.48023137 0.48050085 0.48076808 0.4810177  0.48130786
 0.48152953 0.48166022 0.48169032 0.48163584 0.48148304 0.48128745
 0.4810805  0.48092017 0.48083076 0.48081324 0.48082712 0.4808125
 0.48072517 0.48066688 0.4806087  0.4806336  0.48077166 0.48101202
 0.48129413 0.48152286 0.48165125 0.4815992  0.48133418 0.48085865
 0.48025784 0.4796726  0.47914374 0.47863334 0.47809124 0.47757953
 0.4770696  0.4765872  0.47611362 0.47564444 0.47517544 0.4746485
 0.4740929  0.47349992 0.47288817 0.47227773 0.471747   0.47130808
 0.47096765 0.47068438 0.47044572 0.4702096  0.46993884 0.46963522
 0.46937737 0.46920013 0.46918696 0.46936756 0.4697113  0.4700701
 0.4703943  0.47056383 0.4705775  0.47050563 0.4704393  0.4705044
 0.47072867 0.47100532 0.47128505 0.4714578  0.47149524 0.47142142
 0.4713641  0.4714469  0.47164318 0.47195414 0.4723978  0.47284734
 0.47311437 0.47318164 0.47308165 0.4728888  0.4727914  0.4728427
 0.47293603 0.47302642 0.47302732 0.47284278 0.4725187  0.472092
 0.47161976 0.47123957 0.4710301  0.47091055 0.4708755  0.47081724
 0.47069517 0.47050798 0.47033146 0.47024742 0.47028434 0.47047415
 0.47071925 0.47087947 0.47094736 0.4709087  0.47076172 0.47053346
 0.47028193 0.47008118 0.4699957  0.4700455  0.470177   0.47030738
 0.47039086 0.47049224 0.47063053 0.4707775  0.4709875  0.4712213
 0.47142845 0.47156516 0.4716022  0.4715371  0.47131407 0.47092417
 0.47043133 0.46998677 0.46962303 0.46926397 0.46885234 0.468391
 0.4679325  0.46749046 0.46718302 0.46699402 0.46678722 0.46665493
 0.46641734 0.46613166 0.46575138 0.46539074 0.4650727  0.46479633
 0.4645766  0.46435207 0.46412235 0.46378934 0.4634381  0.46318167
 0.4630746  0.46323887 0.4636013  0.46406922 0.46460935 0.4650619
 0.46545872 0.46575826 0.46599498 0.46628878 0.4665473  0.46686766
 0.46727118 0.46774486 0.46819448 0.46848568 0.46868366 0.46877196
 0.4690156  0.46949363 0.46973655 0.46912524 0.46634498 0.45941505]
