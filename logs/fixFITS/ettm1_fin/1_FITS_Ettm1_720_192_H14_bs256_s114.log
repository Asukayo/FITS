Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33649
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=122, out_features=154, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  67336192.0
params:  18942.0
Trainable parameters:  18942
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.635216474533081
Epoch: 1, Steps: 65 | Train Loss: 0.4771424 Vali Loss: 0.7230564 Test Loss: 0.4160279
Validation loss decreased (inf --> 0.723056).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.757671117782593
Epoch: 2, Steps: 65 | Train Loss: 0.3505500 Vali Loss: 0.6217408 Test Loss: 0.3657847
Validation loss decreased (0.723056 --> 0.621741).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.177587747573853
Epoch: 3, Steps: 65 | Train Loss: 0.3251888 Vali Loss: 0.5849370 Test Loss: 0.3515829
Validation loss decreased (0.621741 --> 0.584937).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.465426921844482
Epoch: 4, Steps: 65 | Train Loss: 0.3149248 Vali Loss: 0.5659337 Test Loss: 0.3460175
Validation loss decreased (0.584937 --> 0.565934).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.692533016204834
Epoch: 5, Steps: 65 | Train Loss: 0.3096089 Vali Loss: 0.5559211 Test Loss: 0.3434514
Validation loss decreased (0.565934 --> 0.555921).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.981570959091187
Epoch: 6, Steps: 65 | Train Loss: 0.3067901 Vali Loss: 0.5486867 Test Loss: 0.3417656
Validation loss decreased (0.555921 --> 0.548687).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.791977643966675
Epoch: 7, Steps: 65 | Train Loss: 0.3049937 Vali Loss: 0.5424038 Test Loss: 0.3408954
Validation loss decreased (0.548687 --> 0.542404).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 11.767558813095093
Epoch: 8, Steps: 65 | Train Loss: 0.3033660 Vali Loss: 0.5382950 Test Loss: 0.3401590
Validation loss decreased (0.542404 --> 0.538295).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.350017786026001
Epoch: 9, Steps: 65 | Train Loss: 0.3021897 Vali Loss: 0.5352727 Test Loss: 0.3397272
Validation loss decreased (0.538295 --> 0.535273).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.618768215179443
Epoch: 10, Steps: 65 | Train Loss: 0.3017103 Vali Loss: 0.5323480 Test Loss: 0.3394139
Validation loss decreased (0.535273 --> 0.532348).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.051321506500244
Epoch: 11, Steps: 65 | Train Loss: 0.3008585 Vali Loss: 0.5305547 Test Loss: 0.3390630
Validation loss decreased (0.532348 --> 0.530555).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.333852291107178
Epoch: 12, Steps: 65 | Train Loss: 0.3005701 Vali Loss: 0.5276080 Test Loss: 0.3389003
Validation loss decreased (0.530555 --> 0.527608).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.425470352172852
Epoch: 13, Steps: 65 | Train Loss: 0.3001140 Vali Loss: 0.5272098 Test Loss: 0.3388530
Validation loss decreased (0.527608 --> 0.527210).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.262187480926514
Epoch: 14, Steps: 65 | Train Loss: 0.3000331 Vali Loss: 0.5251473 Test Loss: 0.3385786
Validation loss decreased (0.527210 --> 0.525147).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.180489540100098
Epoch: 15, Steps: 65 | Train Loss: 0.2998216 Vali Loss: 0.5257017 Test Loss: 0.3384166
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.595126867294312
Epoch: 16, Steps: 65 | Train Loss: 0.2993758 Vali Loss: 0.5234395 Test Loss: 0.3384287
Validation loss decreased (0.525147 --> 0.523439).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 11.339977025985718
Epoch: 17, Steps: 65 | Train Loss: 0.2992042 Vali Loss: 0.5232865 Test Loss: 0.3383790
Validation loss decreased (0.523439 --> 0.523286).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.568186283111572
Epoch: 18, Steps: 65 | Train Loss: 0.2989776 Vali Loss: 0.5222936 Test Loss: 0.3382787
Validation loss decreased (0.523286 --> 0.522294).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.072184562683105
Epoch: 19, Steps: 65 | Train Loss: 0.2989492 Vali Loss: 0.5223081 Test Loss: 0.3383065
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 11.82995891571045
Epoch: 20, Steps: 65 | Train Loss: 0.2987417 Vali Loss: 0.5216181 Test Loss: 0.3382581
Validation loss decreased (0.522294 --> 0.521618).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.081986427307129
Epoch: 21, Steps: 65 | Train Loss: 0.2985445 Vali Loss: 0.5202259 Test Loss: 0.3383879
Validation loss decreased (0.521618 --> 0.520226).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.92153811454773
Epoch: 22, Steps: 65 | Train Loss: 0.2983133 Vali Loss: 0.5204255 Test Loss: 0.3382134
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 11.301536083221436
Epoch: 23, Steps: 65 | Train Loss: 0.2985653 Vali Loss: 0.5202854 Test Loss: 0.3381515
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 11.129823207855225
Epoch: 24, Steps: 65 | Train Loss: 0.2984354 Vali Loss: 0.5190869 Test Loss: 0.3382335
Validation loss decreased (0.520226 --> 0.519087).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.935343027114868
Epoch: 25, Steps: 65 | Train Loss: 0.2983563 Vali Loss: 0.5194456 Test Loss: 0.3382180
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 11.177711725234985
Epoch: 26, Steps: 65 | Train Loss: 0.2982764 Vali Loss: 0.5180221 Test Loss: 0.3381377
Validation loss decreased (0.519087 --> 0.518022).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 11.146775484085083
Epoch: 27, Steps: 65 | Train Loss: 0.2981271 Vali Loss: 0.5178214 Test Loss: 0.3381455
Validation loss decreased (0.518022 --> 0.517821).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 11.203993320465088
Epoch: 28, Steps: 65 | Train Loss: 0.2980575 Vali Loss: 0.5179129 Test Loss: 0.3381510
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 11.231525659561157
Epoch: 29, Steps: 65 | Train Loss: 0.2980821 Vali Loss: 0.5179274 Test Loss: 0.3380905
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.950647592544556
Epoch: 30, Steps: 65 | Train Loss: 0.2981523 Vali Loss: 0.5179718 Test Loss: 0.3382391
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_192_FITS_ETTm1_ftM_sl720_ll48_pl192_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.3379283547401428, mae:0.36814096570014954, rse:0.5533677339553833, corr:[0.53904384 0.5519765  0.55742955 0.5579262  0.55866224 0.5609347
 0.5633661  0.5644612  0.56448144 0.5645335  0.5651397  0.56586754
 0.56611913 0.5656993  0.56492805 0.5641108  0.56325555 0.5622561
 0.56103    0.5597456  0.5586049  0.55751497 0.55630565 0.5550329
 0.55362844 0.55241066 0.5515409  0.55088925 0.55051583 0.5504423
 0.5507733  0.55158514 0.5524915  0.55307376 0.55292106 0.55237424
 0.5517909  0.5516213  0.551798   0.55181795 0.55150294 0.5508924
 0.55042297 0.5504944  0.5508316  0.55098146 0.5508213  0.5504729
 0.5501827  0.55011505 0.55020654 0.55011505 0.5497016  0.54906577
 0.5486382  0.548666   0.5489748  0.54912865 0.5490142  0.5486552
 0.5484048  0.548477   0.5487506  0.5488688  0.54882604 0.5487502
 0.5488108  0.5490648  0.54940337 0.5496177  0.54966635 0.5495834
 0.5494676  0.54939216 0.549392   0.5493491  0.549183   0.5489257
 0.54865307 0.54843086 0.5482617  0.5480835  0.54788035 0.54762334
 0.5474094  0.5472406  0.5471148  0.54694945 0.546771   0.54672784
 0.54688984 0.547215   0.5475751  0.5478084  0.5477969  0.54753363
 0.5471585  0.5468192  0.5464276  0.54601747 0.5456504  0.545407
 0.5453251  0.5452768  0.54521734 0.54512006 0.5448951  0.5446705
 0.5444634  0.5443196  0.5441837  0.54393715 0.5435777  0.54320705
 0.54290044 0.54268086 0.54248047 0.5422047  0.54191506 0.5417596
 0.54172933 0.54168814 0.541463   0.5411003  0.5407518  0.54049915
 0.54035866 0.54026425 0.5400901  0.53988254 0.53972    0.5397218
 0.53992164 0.5400903  0.5399576  0.5396006  0.5393573  0.53933007
 0.5395922  0.5399508  0.54007244 0.5399847  0.53994745 0.54016995
 0.5405863  0.5408657  0.540872   0.54064304 0.54046494 0.54048157
 0.5406509  0.54086316 0.5408842  0.54065174 0.540317   0.54018223
 0.5402774  0.54038495 0.54039973 0.5402     0.54008156 0.5401806
 0.5404676  0.5406919  0.5406948  0.540519   0.54041815 0.5405707
 0.5408816  0.5410676  0.541022   0.54077256 0.5403922  0.5401172
 0.5400503  0.5400659  0.5398897  0.53947437 0.5389529  0.5386533
 0.53857416 0.5384748  0.538038   0.5375019  0.5372714  0.53747433
 0.5375945  0.5374096  0.5373536  0.53821546 0.5394974  0.5372649 ]
