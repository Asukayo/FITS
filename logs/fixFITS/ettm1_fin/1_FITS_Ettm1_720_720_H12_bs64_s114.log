Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20134912.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4710283
	speed: 0.1353s/iter; left time: 1731.7757s
	iters: 200, epoch: 1 | loss: 0.4180408
	speed: 0.1189s/iter; left time: 1509.8615s
Epoch: 1 cost time: 31.512197256088257
Epoch: 1, Steps: 258 | Train Loss: 0.5123190 Vali Loss: 1.0186989 Test Loss: 0.4505638
Validation loss decreased (inf --> 1.018699).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3831649
	speed: 0.5261s/iter; left time: 6599.2203s
	iters: 200, epoch: 2 | loss: 0.4190007
	speed: 0.1281s/iter; left time: 1593.8546s
Epoch: 2 cost time: 33.89818787574768
Epoch: 2, Steps: 258 | Train Loss: 0.4155705 Vali Loss: 0.9665688 Test Loss: 0.4207307
Validation loss decreased (1.018699 --> 0.966569).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3833669
	speed: 0.5630s/iter; left time: 6916.9607s
	iters: 200, epoch: 3 | loss: 0.4313729
	speed: 0.1253s/iter; left time: 1527.1370s
Epoch: 3 cost time: 33.8410382270813
Epoch: 3, Steps: 258 | Train Loss: 0.4035045 Vali Loss: 0.9506029 Test Loss: 0.4150716
Validation loss decreased (0.966569 --> 0.950603).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3909288
	speed: 0.6266s/iter; left time: 7536.3510s
	iters: 200, epoch: 4 | loss: 0.3848750
	speed: 0.1385s/iter; left time: 1651.9269s
Epoch: 4 cost time: 37.11526846885681
Epoch: 4, Steps: 258 | Train Loss: 0.3996968 Vali Loss: 0.9431298 Test Loss: 0.4146071
Validation loss decreased (0.950603 --> 0.943130).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3628984
	speed: 0.5771s/iter; left time: 6791.9319s
	iters: 200, epoch: 5 | loss: 0.3737473
	speed: 0.1256s/iter; left time: 1465.3758s
Epoch: 5 cost time: 33.15770506858826
Epoch: 5, Steps: 258 | Train Loss: 0.3984989 Vali Loss: 0.9407132 Test Loss: 0.4149322
Validation loss decreased (0.943130 --> 0.940713).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4311272
	speed: 0.5614s/iter; left time: 6461.9440s
	iters: 200, epoch: 6 | loss: 0.3697801
	speed: 0.1277s/iter; left time: 1456.8257s
Epoch: 6 cost time: 33.9811909198761
Epoch: 6, Steps: 258 | Train Loss: 0.3980554 Vali Loss: 0.9369753 Test Loss: 0.4154727
Validation loss decreased (0.940713 --> 0.936975).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3657526
	speed: 0.5633s/iter; left time: 6338.3352s
	iters: 200, epoch: 7 | loss: 0.4136329
	speed: 0.1264s/iter; left time: 1409.3527s
Epoch: 7 cost time: 34.01547026634216
Epoch: 7, Steps: 258 | Train Loss: 0.3977129 Vali Loss: 0.9364614 Test Loss: 0.4159396
Validation loss decreased (0.936975 --> 0.936461).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4272774
	speed: 0.5482s/iter; left time: 6027.8208s
	iters: 200, epoch: 8 | loss: 0.4154705
	speed: 0.1246s/iter; left time: 1357.1635s
Epoch: 8 cost time: 32.952656745910645
Epoch: 8, Steps: 258 | Train Loss: 0.3974614 Vali Loss: 0.9358404 Test Loss: 0.4161964
Validation loss decreased (0.936461 --> 0.935840).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4054029
	speed: 0.6077s/iter; left time: 6524.6897s
	iters: 200, epoch: 9 | loss: 0.3707682
	speed: 0.1393s/iter; left time: 1482.2072s
Epoch: 9 cost time: 37.16028308868408
Epoch: 9, Steps: 258 | Train Loss: 0.3972299 Vali Loss: 0.9336118 Test Loss: 0.4161788
Validation loss decreased (0.935840 --> 0.933612).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3855235
	speed: 0.5447s/iter; left time: 5707.5350s
	iters: 200, epoch: 10 | loss: 0.3806182
	speed: 0.1263s/iter; left time: 1310.7948s
Epoch: 10 cost time: 33.35228610038757
Epoch: 10, Steps: 258 | Train Loss: 0.3971973 Vali Loss: 0.9345308 Test Loss: 0.4161839
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3897054
	speed: 0.5524s/iter; left time: 5645.7821s
	iters: 200, epoch: 11 | loss: 0.3867984
	speed: 0.1245s/iter; left time: 1260.3535s
Epoch: 11 cost time: 33.81072735786438
Epoch: 11, Steps: 258 | Train Loss: 0.3971945 Vali Loss: 0.9337704 Test Loss: 0.4159133
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3685112
	speed: 0.5568s/iter; left time: 5547.2559s
	iters: 200, epoch: 12 | loss: 0.3945797
	speed: 0.1242s/iter; left time: 1224.8159s
Epoch: 12 cost time: 32.12918043136597
Epoch: 12, Steps: 258 | Train Loss: 0.3970706 Vali Loss: 0.9332222 Test Loss: 0.4163915
Validation loss decreased (0.933612 --> 0.933222).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3818312
	speed: 0.5020s/iter; left time: 4871.6680s
	iters: 200, epoch: 13 | loss: 0.3697174
	speed: 0.1242s/iter; left time: 1193.3385s
Epoch: 13 cost time: 32.4855272769928
Epoch: 13, Steps: 258 | Train Loss: 0.3969306 Vali Loss: 0.9329593 Test Loss: 0.4162549
Validation loss decreased (0.933222 --> 0.932959).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4224203
	speed: 0.5917s/iter; left time: 5589.8003s
	iters: 200, epoch: 14 | loss: 0.3857760
	speed: 0.1382s/iter; left time: 1291.7140s
Epoch: 14 cost time: 37.28459668159485
Epoch: 14, Steps: 258 | Train Loss: 0.3969515 Vali Loss: 0.9324374 Test Loss: 0.4161192
Validation loss decreased (0.932959 --> 0.932437).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3772309
	speed: 0.6059s/iter; left time: 5567.9055s
	iters: 200, epoch: 15 | loss: 0.4073808
	speed: 0.0868s/iter; left time: 789.1434s
Epoch: 15 cost time: 26.830159902572632
Epoch: 15, Steps: 258 | Train Loss: 0.3968741 Vali Loss: 0.9327228 Test Loss: 0.4163488
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3887330
	speed: 0.4610s/iter; left time: 4117.5171s
	iters: 200, epoch: 16 | loss: 0.4105164
	speed: 0.1017s/iter; left time: 897.8610s
Epoch: 16 cost time: 27.147826433181763
Epoch: 16, Steps: 258 | Train Loss: 0.3969159 Vali Loss: 0.9322006 Test Loss: 0.4161949
Validation loss decreased (0.932437 --> 0.932201).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4201859
	speed: 0.5331s/iter; left time: 4623.4386s
	iters: 200, epoch: 17 | loss: 0.4029607
	speed: 0.1445s/iter; left time: 1238.7685s
Epoch: 17 cost time: 37.04158687591553
Epoch: 17, Steps: 258 | Train Loss: 0.3968834 Vali Loss: 0.9312817 Test Loss: 0.4162992
Validation loss decreased (0.932201 --> 0.931282).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3905568
	speed: 0.6065s/iter; left time: 5103.9344s
	iters: 200, epoch: 18 | loss: 0.3803021
	speed: 0.1368s/iter; left time: 1137.3462s
Epoch: 18 cost time: 36.205031394958496
Epoch: 18, Steps: 258 | Train Loss: 0.3967055 Vali Loss: 0.9319463 Test Loss: 0.4163101
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3803118
	speed: 0.6183s/iter; left time: 5043.4723s
	iters: 200, epoch: 19 | loss: 0.4068508
	speed: 0.1573s/iter; left time: 1267.2852s
Epoch: 19 cost time: 40.20750880241394
Epoch: 19, Steps: 258 | Train Loss: 0.3967522 Vali Loss: 0.9311118 Test Loss: 0.4162257
Validation loss decreased (0.931282 --> 0.931112).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3946872
	speed: 0.6719s/iter; left time: 5307.7057s
	iters: 200, epoch: 20 | loss: 0.4049915
	speed: 0.1168s/iter; left time: 910.7428s
Epoch: 20 cost time: 34.09123158454895
Epoch: 20, Steps: 258 | Train Loss: 0.3967301 Vali Loss: 0.9314365 Test Loss: 0.4161829
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4364718
	speed: 0.4868s/iter; left time: 3719.9798s
	iters: 200, epoch: 21 | loss: 0.4279598
	speed: 0.1078s/iter; left time: 812.9215s
Epoch: 21 cost time: 30.857764959335327
Epoch: 21, Steps: 258 | Train Loss: 0.3965844 Vali Loss: 0.9312902 Test Loss: 0.4163567
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3884329
	speed: 0.5769s/iter; left time: 4259.4365s
	iters: 200, epoch: 22 | loss: 0.3592838
	speed: 0.1369s/iter; left time: 996.9630s
Epoch: 22 cost time: 35.172640323638916
Epoch: 22, Steps: 258 | Train Loss: 0.3965740 Vali Loss: 0.9310113 Test Loss: 0.4163444
Validation loss decreased (0.931112 --> 0.931011).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3871292
	speed: 0.5952s/iter; left time: 4240.9335s
	iters: 200, epoch: 23 | loss: 0.4349445
	speed: 0.1434s/iter; left time: 1007.5987s
Epoch: 23 cost time: 36.321088552474976
Epoch: 23, Steps: 258 | Train Loss: 0.3965737 Vali Loss: 0.9309167 Test Loss: 0.4162300
Validation loss decreased (0.931011 --> 0.930917).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4043674
	speed: 0.6210s/iter; left time: 4264.5190s
	iters: 200, epoch: 24 | loss: 0.3608876
	speed: 0.1388s/iter; left time: 939.5779s
Epoch: 24 cost time: 37.32632088661194
Epoch: 24, Steps: 258 | Train Loss: 0.3966848 Vali Loss: 0.9315515 Test Loss: 0.4160711
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3637599
	speed: 0.6452s/iter; left time: 4264.2871s
	iters: 200, epoch: 25 | loss: 0.4248860
	speed: 0.1499s/iter; left time: 975.9842s
Epoch: 25 cost time: 39.81989860534668
Epoch: 25, Steps: 258 | Train Loss: 0.3965856 Vali Loss: 0.9315936 Test Loss: 0.4163273
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4197252
	speed: 0.6223s/iter; left time: 3952.3589s
	iters: 200, epoch: 26 | loss: 0.4021019
	speed: 0.1289s/iter; left time: 805.7497s
Epoch: 26 cost time: 35.530760765075684
Epoch: 26, Steps: 258 | Train Loss: 0.3966050 Vali Loss: 0.9314814 Test Loss: 0.4162332
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41523393988609314, mae:0.4115874171257019, rse:0.6130800247192383, corr:[0.52617514 0.5306668  0.5330692  0.5341229  0.5350604  0.536436
 0.5378824  0.5390039  0.5397629  0.54035    0.54105526 0.5416672
 0.5419918  0.5418913  0.5413857  0.540622   0.5397809  0.5388574
 0.5376979  0.536404   0.53516275 0.533971   0.53277224 0.5316833
 0.5304534  0.52919865 0.5280642  0.527196   0.5269003  0.52720505
 0.5279016  0.52872765 0.5292749  0.52952546 0.5294498  0.5294338
 0.5294028  0.52934134 0.529155   0.5287435  0.5283613  0.52810484
 0.528035   0.52810663 0.52805024 0.52777344 0.52748406 0.5273395
 0.5273479  0.52738416 0.52741235 0.5272979  0.5270891  0.526774
 0.52653223 0.5264274  0.5263828  0.52619404 0.52586406 0.52538306
 0.52498925 0.5249008  0.52514863 0.5254882  0.5258541  0.5261806
 0.5264225  0.5266229  0.52688515 0.5271458  0.52735615 0.5273992
 0.52722406 0.5268697  0.5265096  0.5262045  0.5259583  0.52577025
 0.52560616 0.52544194 0.5252563  0.5250712  0.52496034 0.5249077
 0.5249753  0.5250572  0.52508926 0.524987   0.5247996  0.52471983
 0.5248492  0.52520424 0.52571505 0.5261956  0.526468   0.52636904
 0.52595794 0.52544796 0.5249149  0.5245895  0.52447563 0.5244573
 0.5243971  0.52413505 0.52372456 0.5233814  0.52317667 0.52321595
 0.523388   0.5235776  0.5236044  0.52336186 0.52292275 0.522471
 0.5221225  0.5219285  0.521824   0.52164483 0.5212952  0.52085423
 0.5204071  0.52006227 0.51985687 0.51984715 0.51991767 0.5198497
 0.51958436 0.5192757  0.5190745  0.51910347 0.51923484 0.5192725
 0.51911706 0.5187528  0.51827484 0.51796806 0.51807743 0.51844037
 0.5188732  0.5191784  0.5191891  0.51902986 0.5189416  0.51909596
 0.51945794 0.5197626  0.51986295 0.51971346 0.51946044 0.51921964
 0.5190988  0.5191732  0.51934403 0.5194889  0.5195344  0.5195724
 0.5196132  0.51964974 0.51970655 0.5196902  0.5196615  0.51963264
 0.5196387  0.51969016 0.5198109  0.51999754 0.5202493  0.5205576
 0.5208322  0.52098674 0.52107227 0.5211089  0.5210429  0.52089876
 0.5207201  0.52057135 0.5205127  0.5205672  0.5206787  0.52081907
 0.5209125  0.520932   0.5208487  0.52078557 0.5208384  0.52104485
 0.52138066 0.52179086 0.52215505 0.52235204 0.5223268  0.52209526
 0.52173066 0.5213695  0.52098817 0.5205337  0.5199462  0.5192165
 0.51834255 0.5174353  0.5165966  0.51595336 0.5154895  0.51512897
 0.51474375 0.514215   0.5135118  0.5127423  0.51202124 0.5114201
 0.5108999  0.5104374  0.5099437  0.5093562  0.5086167  0.5078779
 0.50729555 0.50688916 0.50667554 0.5065541  0.5064704  0.5064524
 0.5065734  0.5067211  0.5068899  0.50704587 0.5070902  0.5070236
 0.50691855 0.50687444 0.5069556  0.5071214  0.50733477 0.5075365
 0.5076615  0.5077301  0.5077133  0.50781155 0.5080303  0.5083928
 0.5087573  0.508929   0.5089044  0.50876737 0.5086257  0.5084954
 0.50838226 0.50826347 0.5080855  0.5078682  0.5076028  0.5074216
 0.5073185  0.50738037 0.5075181  0.5076334  0.50766027 0.5075848
 0.50744534 0.50733143 0.5073003  0.5073768  0.5075411  0.5078158
 0.50803995 0.50815564 0.5081567  0.50811785 0.508079   0.5080371
 0.50804055 0.50806314 0.50810176 0.5081507  0.50819397 0.5082448
 0.5082598  0.50829995 0.5082779  0.5082442  0.5081713  0.5080809
 0.5080854  0.50816274 0.50827557 0.50836843 0.50832695 0.5080671
 0.5075956  0.5070174  0.50638413 0.50570405 0.5050385  0.5044457
 0.50396425 0.5035432  0.50320685 0.502871   0.5024663  0.50199443
 0.5014588  0.5009421  0.5005403  0.50026375 0.50008583 0.49993068
 0.49983793 0.49970853 0.49951038 0.49924347 0.4989906  0.49876037
 0.49858683 0.49852642 0.49851805 0.4985308  0.4985385  0.49856472
 0.4985871  0.49855918 0.49848258 0.49837238 0.49823496 0.49806547
 0.4978762  0.49772215 0.497573   0.4974283  0.49724284 0.49700958
 0.49682742 0.4967119  0.4966604  0.4967116  0.49683326 0.49705827
 0.49726257 0.49726126 0.49717036 0.49705568 0.49700072 0.49699897
 0.4969898  0.49700975 0.49704662 0.4971265  0.4972596  0.49744716
 0.4975601  0.49758932 0.4975115  0.49732217 0.49711943 0.496962
 0.4968693  0.4968732  0.49693197 0.49699268 0.4970513  0.49707952
 0.49699688 0.49686208 0.4966769  0.49647617 0.49631655 0.49624652
 0.49621868 0.4962859  0.4963984  0.49644083 0.4964114  0.4963268
 0.4962333  0.49619833 0.49624977 0.49641746 0.4966488  0.49693507
 0.49725702 0.49760306 0.49794573 0.49823385 0.49842092 0.49848723
 0.4983905  0.49811676 0.49770454 0.49715942 0.49650154 0.4959116
 0.4954281  0.49507898 0.49483684 0.4946924  0.49453926 0.4943473
 0.49410215 0.493814   0.49348116 0.49307883 0.49255213 0.49201924
 0.4915148  0.49115774 0.49091348 0.49080202 0.49075297 0.49072912
 0.49064732 0.49051532 0.49037066 0.49031138 0.49039826 0.4905673
 0.49087775 0.49113187 0.49132746 0.49137047 0.49133536 0.4913302
 0.4913183  0.49135783 0.49137175 0.49135768 0.49127173 0.491187
 0.49108356 0.49100298 0.4909193  0.49094844 0.49105304 0.4912987
 0.49151924 0.4915573  0.4914204  0.49119627 0.49095622 0.4907807
 0.49069405 0.49067587 0.49068207 0.49064517 0.49055347 0.49043754
 0.4903198  0.49028414 0.49032432 0.4904118  0.4904896  0.49048644
 0.49045813 0.4904359  0.49047247 0.49055412 0.49064645 0.49075863
 0.49074322 0.49063292 0.49046928 0.49035528 0.4902685  0.49025854
 0.49026632 0.4902923  0.49024966 0.49019256 0.4901649  0.4902005
 0.4903325  0.49048957 0.49063718 0.4907299  0.49075988 0.49074602
 0.49069208 0.49066734 0.49064672 0.49052456 0.49023187 0.4897775
 0.48921236 0.4886265  0.48807883 0.48754644 0.48699832 0.48646617
 0.48599803 0.4854946  0.4849726  0.48444942 0.48384503 0.48314983
 0.48240545 0.48172578 0.48114783 0.4806409  0.48019826 0.4798376
 0.47951135 0.47924465 0.47901943 0.4786989  0.4784223  0.47817194
 0.47801033 0.47793308 0.47790018 0.4779456  0.47801962 0.47813696
 0.478288   0.478421   0.47856495 0.47884703 0.47919413 0.4794924
 0.47961223 0.47960815 0.4795682  0.4795325  0.4795936  0.47981527
 0.48010916 0.480344   0.4804207  0.4803196  0.48024726 0.48038134
 0.4806654  0.48089844 0.48089603 0.48070323 0.48043618 0.48020372
 0.48011455 0.48016384 0.48028216 0.4803545  0.48028788 0.48007822
 0.4798332  0.47966796 0.47966224 0.4797666  0.47987133 0.4799386
 0.4799306  0.47991705 0.47999045 0.48014313 0.48033756 0.48056307
 0.4806482  0.48058802 0.4804705  0.4804229  0.48047426 0.48064363
 0.48082653 0.4809228  0.48085144 0.48065302 0.48044494 0.48032415
 0.48030803 0.4804493  0.48054177 0.4805268  0.48039305 0.48023948
 0.4801858  0.48029166 0.48054507 0.48074138 0.480685   0.4803015
 0.4797039  0.47909364 0.47855693 0.478053   0.47751006 0.47695792
 0.47635967 0.47577465 0.47524443 0.4748142  0.4744933  0.47417548
 0.47382376 0.47336638 0.4728137  0.47221285 0.47168943 0.47127903
 0.47097927 0.47073108 0.47052965 0.47034413 0.470142   0.46991435
 0.46972537 0.46956137 0.46946552 0.46943578 0.46945983 0.46945634
 0.46952045 0.469649   0.46987116 0.47016692 0.47045156 0.47069815
 0.47086462 0.47090515 0.47095707 0.47107247 0.4712605  0.47142947
 0.471538   0.47158632 0.4715237  0.47149134 0.47171417 0.4721577
 0.47254854 0.4727476  0.47268474 0.47243336 0.47222987 0.47219032
 0.47221157 0.47227538 0.4723106  0.47221446 0.4720715  0.47187468
 0.47160926 0.47132438 0.47105387 0.47069353 0.4703562  0.4700904
 0.46998346 0.47004482 0.4702151  0.47040233 0.47049242 0.47049502
 0.4703847  0.47013897 0.4699067  0.4697774  0.4697739  0.4698651
 0.4700043  0.47013387 0.47023916 0.47033283 0.47039652 0.47038096
 0.47025672 0.47008967 0.469883   0.4696131  0.46943897 0.4694085
 0.46955118 0.46982726 0.47014564 0.47042826 0.47054225 0.47047466
 0.47030818 0.47016853 0.46998373 0.46960607 0.46900728 0.46830913
 0.46770778 0.46727788 0.46711785 0.46706036 0.4668263  0.46652797
 0.46606836 0.46568573 0.46539763 0.46525607 0.46507633 0.46469063
 0.46413878 0.46355346 0.46318915 0.46303192 0.46310484 0.46328953
 0.4633636  0.4633387  0.46323925 0.4631968  0.4634076  0.4637591
 0.46421507 0.4645709  0.46475118 0.46489725 0.46498236 0.4651823
 0.46551397 0.46590614 0.466227   0.46641746 0.46672985 0.46722734
 0.46795347 0.46858653 0.46867675 0.4685458  0.4680207  0.46497387]
