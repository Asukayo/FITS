Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  53344256.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4735604
	speed: 0.1356s/iter; left time: 860.9949s
Epoch: 1 cost time: 17.513472318649292
Epoch: 1, Steps: 129 | Train Loss: 0.5644020 Vali Loss: 1.3134005 Test Loss: 0.6614579
Validation loss decreased (inf --> 1.313401).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3709300
	speed: 0.3483s/iter; left time: 2167.3462s
Epoch: 2 cost time: 17.46967577934265
Epoch: 2, Steps: 129 | Train Loss: 0.3898606 Vali Loss: 1.1650399 Test Loss: 0.5617243
Validation loss decreased (1.313401 --> 1.165040).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3275644
	speed: 0.3637s/iter; left time: 2215.8874s
Epoch: 3 cost time: 17.36030650138855
Epoch: 3, Steps: 129 | Train Loss: 0.3290172 Vali Loss: 1.0953573 Test Loss: 0.5168104
Validation loss decreased (1.165040 --> 1.095357).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2918976
	speed: 0.3592s/iter; left time: 2142.2931s
Epoch: 4 cost time: 17.267390489578247
Epoch: 4, Steps: 129 | Train Loss: 0.2953021 Vali Loss: 1.0559716 Test Loss: 0.4916137
Validation loss decreased (1.095357 --> 1.055972).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2722361
	speed: 0.3469s/iter; left time: 2024.0307s
Epoch: 5 cost time: 15.698887825012207
Epoch: 5, Steps: 129 | Train Loss: 0.2737967 Vali Loss: 1.0283709 Test Loss: 0.4745864
Validation loss decreased (1.055972 --> 1.028371).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2615058
	speed: 0.3108s/iter; left time: 1773.5531s
Epoch: 6 cost time: 15.564931154251099
Epoch: 6, Steps: 129 | Train Loss: 0.2591359 Vali Loss: 1.0097837 Test Loss: 0.4626515
Validation loss decreased (1.028371 --> 1.009784).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2459146
	speed: 0.4110s/iter; left time: 2291.9173s
Epoch: 7 cost time: 19.346181631088257
Epoch: 7, Steps: 129 | Train Loss: 0.2486456 Vali Loss: 0.9975518 Test Loss: 0.4544215
Validation loss decreased (1.009784 --> 0.997552).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2316206
	speed: 0.3836s/iter; left time: 2089.9073s
Epoch: 8 cost time: 17.14008641242981
Epoch: 8, Steps: 129 | Train Loss: 0.2409353 Vali Loss: 0.9860597 Test Loss: 0.4476188
Validation loss decreased (0.997552 --> 0.986060).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2364198
	speed: 0.3355s/iter; left time: 1784.4149s
Epoch: 9 cost time: 16.286927461624146
Epoch: 9, Steps: 129 | Train Loss: 0.2349981 Vali Loss: 0.9780295 Test Loss: 0.4422618
Validation loss decreased (0.986060 --> 0.978030).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2308106
	speed: 0.3593s/iter; left time: 1864.6506s
Epoch: 10 cost time: 17.201277017593384
Epoch: 10, Steps: 129 | Train Loss: 0.2303412 Vali Loss: 0.9709239 Test Loss: 0.4377910
Validation loss decreased (0.978030 --> 0.970924).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2276553
	speed: 0.3593s/iter; left time: 1818.2794s
Epoch: 11 cost time: 17.171908378601074
Epoch: 11, Steps: 129 | Train Loss: 0.2267604 Vali Loss: 0.9650926 Test Loss: 0.4342356
Validation loss decreased (0.970924 --> 0.965093).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2239394
	speed: 0.3538s/iter; left time: 1744.9690s
Epoch: 12 cost time: 17.123544931411743
Epoch: 12, Steps: 129 | Train Loss: 0.2237449 Vali Loss: 0.9605290 Test Loss: 0.4311452
Validation loss decreased (0.965093 --> 0.960529).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2160062
	speed: 0.3523s/iter; left time: 1692.2768s
Epoch: 13 cost time: 16.742505311965942
Epoch: 13, Steps: 129 | Train Loss: 0.2214364 Vali Loss: 0.9573602 Test Loss: 0.4288230
Validation loss decreased (0.960529 --> 0.957360).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2243595
	speed: 0.3207s/iter; left time: 1498.9899s
Epoch: 14 cost time: 14.190342426300049
Epoch: 14, Steps: 129 | Train Loss: 0.2194584 Vali Loss: 0.9539670 Test Loss: 0.4267625
Validation loss decreased (0.957360 --> 0.953967).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2202014
	speed: 0.3238s/iter; left time: 1471.8110s
Epoch: 15 cost time: 17.765140295028687
Epoch: 15, Steps: 129 | Train Loss: 0.2178263 Vali Loss: 0.9514370 Test Loss: 0.4250252
Validation loss decreased (0.953967 --> 0.951437).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2201675
	speed: 0.2977s/iter; left time: 1314.8390s
Epoch: 16 cost time: 15.50105881690979
Epoch: 16, Steps: 129 | Train Loss: 0.2165527 Vali Loss: 0.9498457 Test Loss: 0.4236995
Validation loss decreased (0.951437 --> 0.949846).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2255748
	speed: 0.3726s/iter; left time: 1597.4002s
Epoch: 17 cost time: 17.11182451248169
Epoch: 17, Steps: 129 | Train Loss: 0.2152472 Vali Loss: 0.9471546 Test Loss: 0.4226200
Validation loss decreased (0.949846 --> 0.947155).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2030066
	speed: 0.3889s/iter; left time: 1617.2407s
Epoch: 18 cost time: 20.810312271118164
Epoch: 18, Steps: 129 | Train Loss: 0.2143580 Vali Loss: 0.9465443 Test Loss: 0.4216077
Validation loss decreased (0.947155 --> 0.946544).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2323002
	speed: 0.3903s/iter; left time: 1572.4906s
Epoch: 19 cost time: 17.79526948928833
Epoch: 19, Steps: 129 | Train Loss: 0.2135311 Vali Loss: 0.9444271 Test Loss: 0.4207491
Validation loss decreased (0.946544 --> 0.944427).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2133516
	speed: 0.4011s/iter; left time: 1564.3104s
Epoch: 20 cost time: 21.860673904418945
Epoch: 20, Steps: 129 | Train Loss: 0.2128948 Vali Loss: 0.9438602 Test Loss: 0.4200298
Validation loss decreased (0.944427 --> 0.943860).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2066255
	speed: 0.4466s/iter; left time: 1684.1594s
Epoch: 21 cost time: 21.006297826766968
Epoch: 21, Steps: 129 | Train Loss: 0.2123608 Vali Loss: 0.9426132 Test Loss: 0.4193154
Validation loss decreased (0.943860 --> 0.942613).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2094294
	speed: 0.4465s/iter; left time: 1626.2403s
Epoch: 22 cost time: 22.069748163223267
Epoch: 22, Steps: 129 | Train Loss: 0.2117914 Vali Loss: 0.9425690 Test Loss: 0.4189452
Validation loss decreased (0.942613 --> 0.942569).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1986166
	speed: 0.4275s/iter; left time: 1501.8556s
Epoch: 23 cost time: 19.082914113998413
Epoch: 23, Steps: 129 | Train Loss: 0.2113200 Vali Loss: 0.9408804 Test Loss: 0.4186049
Validation loss decreased (0.942569 --> 0.940880).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2026147
	speed: 0.4514s/iter; left time: 1527.5393s
Epoch: 24 cost time: 22.921252250671387
Epoch: 24, Steps: 129 | Train Loss: 0.2109416 Vali Loss: 0.9414118 Test Loss: 0.4183067
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2143068
	speed: 0.4518s/iter; left time: 1470.4541s
Epoch: 25 cost time: 22.81801962852478
Epoch: 25, Steps: 129 | Train Loss: 0.2105843 Vali Loss: 0.9413190 Test Loss: 0.4180554
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2079948
	speed: 0.5195s/iter; left time: 1623.9450s
Epoch: 26 cost time: 25.307814598083496
Epoch: 26, Steps: 129 | Train Loss: 0.2102803 Vali Loss: 0.9395930 Test Loss: 0.4178191
Validation loss decreased (0.940880 --> 0.939593).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2119312
	speed: 0.5161s/iter; left time: 1546.8972s
Epoch: 27 cost time: 23.604938507080078
Epoch: 27, Steps: 129 | Train Loss: 0.2099672 Vali Loss: 0.9398488 Test Loss: 0.4177079
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2094104
	speed: 0.4645s/iter; left time: 1332.2500s
Epoch: 28 cost time: 22.978461027145386
Epoch: 28, Steps: 129 | Train Loss: 0.2097720 Vali Loss: 0.9393817 Test Loss: 0.4175791
Validation loss decreased (0.939593 --> 0.939382).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2051080
	speed: 0.4718s/iter; left time: 1292.2103s
Epoch: 29 cost time: 22.914785146713257
Epoch: 29, Steps: 129 | Train Loss: 0.2095833 Vali Loss: 0.9396966 Test Loss: 0.4173005
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2159952
	speed: 0.4712s/iter; left time: 1229.9121s
Epoch: 30 cost time: 23.01015591621399
Epoch: 30, Steps: 129 | Train Loss: 0.2093821 Vali Loss: 0.9378503 Test Loss: 0.4173668
Validation loss decreased (0.939382 --> 0.937850).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2131100
	speed: 0.4743s/iter; left time: 1176.6462s
Epoch: 31 cost time: 22.943519592285156
Epoch: 31, Steps: 129 | Train Loss: 0.2092083 Vali Loss: 0.9401004 Test Loss: 0.4172993
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2145918
	speed: 0.4719s/iter; left time: 1109.8470s
Epoch: 32 cost time: 23.117730617523193
Epoch: 32, Steps: 129 | Train Loss: 0.2090476 Vali Loss: 0.9385970 Test Loss: 0.4172678
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2215172
	speed: 0.4879s/iter; left time: 1084.6567s
Epoch: 33 cost time: 24.53097891807556
Epoch: 33, Steps: 129 | Train Loss: 0.2089410 Vali Loss: 0.9386916 Test Loss: 0.4172836
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  53344256.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4012940
	speed: 0.1959s/iter; left time: 1244.0392s
Epoch: 1 cost time: 25.042680501937866
Epoch: 1, Steps: 129 | Train Loss: 0.3985719 Vali Loss: 0.9336446 Test Loss: 0.4174008
Validation loss decreased (inf --> 0.933645).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3972930
	speed: 0.5130s/iter; left time: 3191.8753s
Epoch: 2 cost time: 23.257139444351196
Epoch: 2, Steps: 129 | Train Loss: 0.3975021 Vali Loss: 0.9316684 Test Loss: 0.4177034
Validation loss decreased (0.933645 --> 0.931668).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4098696
	speed: 0.4653s/iter; left time: 2835.2556s
Epoch: 3 cost time: 22.389495611190796
Epoch: 3, Steps: 129 | Train Loss: 0.3972302 Vali Loss: 0.9321473 Test Loss: 0.4175686
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4099672
	speed: 0.4618s/iter; left time: 2754.2488s
Epoch: 4 cost time: 22.96649169921875
Epoch: 4, Steps: 129 | Train Loss: 0.3968894 Vali Loss: 0.9317998 Test Loss: 0.4185996
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3853176
	speed: 0.4652s/iter; left time: 2714.6280s
Epoch: 5 cost time: 22.18225932121277
Epoch: 5, Steps: 129 | Train Loss: 0.3967144 Vali Loss: 0.9313409 Test Loss: 0.4183928
Validation loss decreased (0.931668 --> 0.931341).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4087651
	speed: 0.4521s/iter; left time: 2579.4247s
Epoch: 6 cost time: 22.053932905197144
Epoch: 6, Steps: 129 | Train Loss: 0.3966819 Vali Loss: 0.9320095 Test Loss: 0.4183817
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3918850
	speed: 0.4630s/iter; left time: 2581.9475s
Epoch: 7 cost time: 22.669097661972046
Epoch: 7, Steps: 129 | Train Loss: 0.3966860 Vali Loss: 0.9318019 Test Loss: 0.4181637
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3663820
	speed: 0.4823s/iter; left time: 2627.7923s
Epoch: 8 cost time: 24.771740913391113
Epoch: 8, Steps: 129 | Train Loss: 0.3965889 Vali Loss: 0.9308245 Test Loss: 0.4181180
Validation loss decreased (0.931341 --> 0.930824).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4004485
	speed: 0.5018s/iter; left time: 2668.9673s
Epoch: 9 cost time: 24.216174840927124
Epoch: 9, Steps: 129 | Train Loss: 0.3965840 Vali Loss: 0.9306740 Test Loss: 0.4179254
Validation loss decreased (0.930824 --> 0.930674).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3559987
	speed: 0.4914s/iter; left time: 2550.5697s
Epoch: 10 cost time: 22.926511526107788
Epoch: 10, Steps: 129 | Train Loss: 0.3966145 Vali Loss: 0.9320141 Test Loss: 0.4181411
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3935404
	speed: 0.4555s/iter; left time: 2305.3874s
Epoch: 11 cost time: 21.936365604400635
Epoch: 11, Steps: 129 | Train Loss: 0.3965579 Vali Loss: 0.9307424 Test Loss: 0.4179496
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3837803
	speed: 0.4589s/iter; left time: 2263.1599s
Epoch: 12 cost time: 22.152122735977173
Epoch: 12, Steps: 129 | Train Loss: 0.3964403 Vali Loss: 0.9310592 Test Loss: 0.4175941
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4152737855911255, mae:0.41158968210220337, rse:0.6131094694137573, corr:[0.5213154  0.5324459  0.5367733  0.5369858  0.53754455 0.53968877
 0.54203767 0.5430768  0.5430577  0.54313797 0.5439032  0.54479504
 0.5450475  0.544446   0.54338    0.5422953  0.54127973 0.5401487
 0.53874815 0.5372188  0.5357482  0.53429747 0.5328033  0.53143364
 0.5301407  0.5291422  0.5284205  0.52770376 0.5269723  0.52638376
 0.526268   0.526941   0.52811027 0.5291809  0.52947086 0.5291749
 0.5286306  0.5283975  0.5285702  0.528729   0.52864283 0.52828604
 0.5280263  0.528212   0.52862245 0.5288348  0.5286793  0.52825457
 0.5278779  0.5278044  0.52804357 0.5282062  0.5280681  0.5276086
 0.5271854  0.5270621  0.527208   0.52732635 0.5272979  0.52706456
 0.52683115 0.5267428  0.52676165 0.526682   0.5265848  0.5265963
 0.5267561  0.5270193  0.52729774 0.52743775 0.52746284 0.52739036
 0.5272464  0.52704114 0.52685094 0.5266691  0.52647716 0.5262939
 0.52611077 0.52593386 0.52576417 0.5256207  0.5255132  0.5253497
 0.5251348  0.5248161  0.52449685 0.5243145  0.52442396 0.5248606
 0.5254107  0.5258087  0.52592605 0.5258109  0.5256687  0.5255887
 0.5255808  0.52555656 0.5252776  0.5248902  0.5245957  0.5245495
 0.52472967 0.5249024  0.52493995 0.52485377 0.52462465 0.5244079
 0.5241871  0.523953   0.52362823 0.52318215 0.52271307 0.52235556
 0.52211744 0.52193266 0.52169174 0.521336   0.52094406 0.5206811
 0.52054936 0.52047825 0.5203758  0.52033824 0.52040476 0.5204519
 0.5203718  0.5201785  0.5199156  0.5197628  0.5197342  0.5197663
 0.51976705 0.5196071  0.5192445  0.5189059  0.5188565  0.5189673
 0.51906157 0.5189684  0.5186142  0.5183006  0.51836354 0.5188474
 0.5194607  0.51978886 0.51973575 0.5194285  0.519178   0.5191192
 0.51921636 0.51939434 0.5195154  0.5195469  0.519526   0.5195834
 0.51967835 0.5197178  0.5196831  0.51950866 0.51934505 0.51926905
 0.5193162  0.5194748  0.51974076 0.5200818  0.520445   0.5207522
 0.5208676  0.5207253  0.520471   0.5202425  0.52007145 0.5200036
 0.52005196 0.52015895 0.5202618  0.5203255  0.5203391  0.5203665
 0.52040803 0.52044433 0.5203721  0.5202395  0.5201281  0.5201371
 0.52034235 0.5207918  0.5213541  0.5218118  0.52204555 0.5220184
 0.52180094 0.5215305  0.5211868  0.52072984 0.5201246  0.51938045
 0.5185148  0.51763064 0.5167865  0.51606214 0.5154352  0.51489246
 0.51439965 0.51391596 0.5134     0.5128775  0.51233697 0.51177937
 0.5112061  0.5107135  0.51028496 0.509833   0.5092035  0.50848347
 0.5078112  0.5072418  0.5068757  0.50668335 0.5066376  0.50673604
 0.5069866  0.50721925 0.5074369  0.5076474  0.50777256 0.50780046
 0.50773823 0.50763494 0.507565   0.50754887 0.50764495 0.50784993
 0.50806004 0.50821763 0.5081816  0.5081451  0.5081518  0.50830215
 0.5084911  0.5085186  0.5083592  0.50807106 0.50778127 0.507558
 0.50747526 0.5075277  0.50758666 0.50754076 0.5072784  0.5069351
 0.5066016  0.50651115 0.50664294 0.5068761  0.5070626  0.5071216
 0.5070754  0.50703967 0.507098   0.5072819  0.50755167 0.507912
 0.5082016  0.5083809  0.5084607  0.5085142  0.5085434  0.5084952
 0.5083837  0.50820154 0.5080197  0.5079134  0.5079015  0.5079691
 0.5080074  0.5080231  0.50790656 0.5077699  0.5076799  0.5077088
 0.50792295 0.5081673  0.50827074 0.5081604  0.50783926 0.50742435
 0.5070817  0.5068799  0.5066851  0.5062821  0.5056433  0.5048931
 0.5042303  0.5037018  0.50330627 0.50286263 0.50224394 0.50150186
 0.5007598  0.5002143  0.49995264 0.49986666 0.4997976  0.49960604
 0.49939165 0.4991322  0.49886394 0.49860084 0.49841338 0.49826956
 0.4981733  0.49816397 0.4981412  0.49805894 0.49791038 0.4977759
 0.49771884 0.49776343 0.49794742 0.4982358  0.49852172 0.49867514
 0.49861997 0.49839285 0.49797824 0.49749565 0.49707386 0.49684152
 0.49691072 0.49715278 0.49738422 0.49753988 0.49758902 0.49766216
 0.4977157  0.49759868 0.49740878 0.49716944 0.4969361  0.49672556
 0.4965178  0.4964099  0.49638325 0.4964274  0.49649882 0.4965765
 0.4965478  0.49643835 0.4962659  0.49603832 0.49586084 0.49577108
 0.49576145 0.49585003 0.495984   0.49609488 0.49617556 0.49620414
 0.4961311  0.49605614 0.4959936  0.49592242 0.4958201  0.49566522
 0.4954307  0.49529505 0.49533483 0.4954629  0.49559882 0.4956107
 0.4954671  0.49528423 0.49520046 0.49535853 0.49571696 0.49618366
 0.4966168  0.49692816 0.49712282 0.49723747 0.49732646 0.49743366
 0.49751577 0.4974911  0.4973064  0.4969163  0.49634486 0.49579802
 0.49529353 0.49483213 0.49439055 0.49402857 0.4937405  0.49359348
 0.4935471  0.49348414 0.49325848 0.49279752 0.49213874 0.49155676
 0.49115223 0.49098387 0.49083918 0.4906266  0.49031863 0.49004617
 0.48987383 0.4898322  0.48983935 0.48985118 0.48987082 0.48985827
 0.48996434 0.49001396 0.49003398 0.48993266 0.48984364 0.4899188
 0.49009418 0.49036044 0.49054164 0.49059725 0.49052143 0.49047574
 0.49047723 0.49054807 0.49059105 0.49067524 0.49074095 0.49090093
 0.49104682 0.49107674 0.49103457 0.49096137 0.49080604 0.49054176
 0.49018705 0.48984227 0.48963603 0.48962924 0.48980474 0.49004614
 0.49018428 0.4901944  0.49009213 0.48995775 0.48987406 0.48982814
 0.4898623  0.48992282 0.48998004 0.4900044  0.49001852 0.4901264
 0.49024802 0.49042937 0.49064064 0.49085626 0.49091974 0.4908685
 0.49072897 0.4906681  0.4907102  0.49086007 0.49098682 0.4909434
 0.49074653 0.4904445  0.4902148  0.49013862 0.49020717 0.49033627
 0.49042764 0.49052814 0.4906542  0.4907138  0.49062175 0.490351
 0.48990926 0.4893698  0.4888214  0.48829597 0.48779234 0.48732334
 0.48688295 0.4863278  0.48569378 0.48506224 0.4844213  0.48379615
 0.48321208 0.4827121  0.48223796 0.4817047  0.48111987 0.48056346
 0.48004076 0.47963622 0.47934568 0.47902593 0.47882628 0.47868735
 0.47860542 0.47849545 0.4782886  0.47808695 0.47795168 0.4779853
 0.47818255 0.4783985  0.47856677 0.47879294 0.47905302 0.47932318
 0.47951975 0.47967532 0.47979483 0.47980615 0.47974905 0.47971952
 0.4797562  0.47986567 0.4800214  0.4801489  0.48029196 0.48044547
 0.4805045  0.48042926 0.4802888  0.48029166 0.4804849  0.4807211
 0.48084497 0.48077777 0.48060423 0.48045978 0.48042706 0.48049292
 0.48058254 0.48059142 0.4804962  0.48033497 0.48020682 0.48024833
 0.48044628 0.48073313 0.480985   0.48104572 0.48091808 0.48078313
 0.48067623 0.4806838  0.48080066 0.48094383 0.480955   0.4808277
 0.4806261  0.4805202  0.48059022 0.48078066 0.48095042 0.4809954
 0.48092556 0.4809414  0.48100516 0.4811152  0.48117423 0.4811441
 0.48107493 0.48107475 0.48123717 0.48143634 0.4814929  0.48129708
 0.48087963 0.48036826 0.4798316  0.4792901  0.4787557  0.47831824
 0.47792888 0.4775659  0.47717762 0.4767338  0.47623816 0.47565162
 0.47506532 0.47451556 0.47404507 0.47361237 0.47318035 0.47263885
 0.4719633  0.47120875 0.47054896 0.47008693 0.46984076 0.46974504
 0.46975425 0.46974292 0.46973407 0.46978262 0.46995455 0.4701769
 0.47045928 0.47066396 0.47075343 0.4707774  0.470809   0.4709489
 0.4711525  0.4712574  0.47128052 0.47123078 0.47123528 0.47137672
 0.47168943 0.47207773 0.47228476 0.47229266 0.4723179  0.47246614
 0.47266042 0.472865   0.47296947 0.47292504 0.47285792 0.47284135
 0.47280237 0.47275877 0.4726474  0.4723654  0.47205713 0.47182474
 0.47175577 0.4718921  0.47211635 0.47214144 0.471939   0.47151476
 0.47104105 0.4706848  0.47053736 0.47056583 0.4706613  0.470822
 0.4709878  0.4710639  0.47108433 0.4710333  0.47088918 0.47069877
 0.47056925 0.47056416 0.47064906 0.47072956 0.47072592 0.47063732
 0.47054443 0.47057402 0.47065935 0.47064584 0.47056028 0.47046867
 0.4705099  0.4707391  0.47105154 0.47124147 0.4711047  0.47065702
 0.4701182  0.4697815  0.46967852 0.46961418 0.46938384 0.4689254
 0.4683387  0.46774012 0.46731022 0.46700037 0.46662283 0.46628466
 0.46586692 0.46552286 0.4651998  0.4649253  0.46462652 0.464287
 0.46397632 0.4637102  0.46353096 0.46329966 0.463095   0.46299532
 0.46295696 0.46300822 0.46302244 0.46298456 0.46308887 0.4633269
 0.4637454  0.4641883  0.46460468 0.4651299  0.46566957 0.4662194
 0.4666393  0.4668665  0.4670013  0.46727958 0.4678963  0.46843868
 0.46851107 0.46801993 0.46745494 0.4679386  0.46846533 0.46374115]
