Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10644480.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4183762
	speed: 0.1508s/iter; left time: 1952.7858s
	iters: 200, epoch: 1 | loss: 0.3397096
	speed: 0.1398s/iter; left time: 1795.9558s
Epoch: 1 cost time: 37.408920764923096
Epoch: 1, Steps: 261 | Train Loss: 0.4257456 Vali Loss: 0.9484317 Test Loss: 0.5593856
Validation loss decreased (inf --> 0.948432).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2603096
	speed: 0.6010s/iter; left time: 7626.0661s
	iters: 200, epoch: 2 | loss: 0.2345699
	speed: 0.1173s/iter; left time: 1477.0981s
Epoch: 2 cost time: 31.865190267562866
Epoch: 2, Steps: 261 | Train Loss: 0.2540394 Vali Loss: 0.8137708 Test Loss: 0.4741741
Validation loss decreased (0.948432 --> 0.813771).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1896826
	speed: 0.5323s/iter; left time: 6616.5511s
	iters: 200, epoch: 3 | loss: 0.1916033
	speed: 0.1163s/iter; left time: 1434.3886s
Epoch: 3 cost time: 31.357826471328735
Epoch: 3, Steps: 261 | Train Loss: 0.1961693 Vali Loss: 0.7571161 Test Loss: 0.4338191
Validation loss decreased (0.813771 --> 0.757116).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1697671
	speed: 0.5335s/iter; left time: 6491.0525s
	iters: 200, epoch: 4 | loss: 0.1582221
	speed: 0.1180s/iter; left time: 1424.5687s
Epoch: 4 cost time: 31.583524703979492
Epoch: 4, Steps: 261 | Train Loss: 0.1682219 Vali Loss: 0.7272325 Test Loss: 0.4125431
Validation loss decreased (0.757116 --> 0.727233).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1530772
	speed: 0.5289s/iter; left time: 6297.1672s
	iters: 200, epoch: 5 | loss: 0.1436922
	speed: 0.1078s/iter; left time: 1273.0848s
Epoch: 5 cost time: 31.28278398513794
Epoch: 5, Steps: 261 | Train Loss: 0.1525064 Vali Loss: 0.7103003 Test Loss: 0.3981883
Validation loss decreased (0.727233 --> 0.710300).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1562704
	speed: 0.5120s/iter; left time: 5962.5246s
	iters: 200, epoch: 6 | loss: 0.1359834
	speed: 0.1153s/iter; left time: 1331.2188s
Epoch: 6 cost time: 31.67840552330017
Epoch: 6, Steps: 261 | Train Loss: 0.1427931 Vali Loss: 0.6972016 Test Loss: 0.3876038
Validation loss decreased (0.710300 --> 0.697202).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1385564
	speed: 0.5235s/iter; left time: 5959.7026s
	iters: 200, epoch: 7 | loss: 0.1310717
	speed: 0.1218s/iter; left time: 1374.1967s
Epoch: 7 cost time: 31.609071731567383
Epoch: 7, Steps: 261 | Train Loss: 0.1365454 Vali Loss: 0.6897520 Test Loss: 0.3818723
Validation loss decreased (0.697202 --> 0.689752).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1262835
	speed: 0.5532s/iter; left time: 6153.8327s
	iters: 200, epoch: 8 | loss: 0.1357465
	speed: 0.1359s/iter; left time: 1497.8833s
Epoch: 8 cost time: 35.62266206741333
Epoch: 8, Steps: 261 | Train Loss: 0.1322813 Vali Loss: 0.6833581 Test Loss: 0.3772418
Validation loss decreased (0.689752 --> 0.683358).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1258595
	speed: 0.6568s/iter; left time: 7134.8380s
	iters: 200, epoch: 9 | loss: 0.1333041
	speed: 0.1083s/iter; left time: 1165.4170s
Epoch: 9 cost time: 34.70736050605774
Epoch: 9, Steps: 261 | Train Loss: 0.1294185 Vali Loss: 0.6797547 Test Loss: 0.3744968
Validation loss decreased (0.683358 --> 0.679755).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1264118
	speed: 0.6596s/iter; left time: 6992.8428s
	iters: 200, epoch: 10 | loss: 0.1221841
	speed: 0.1273s/iter; left time: 1336.5314s
Epoch: 10 cost time: 35.00486445426941
Epoch: 10, Steps: 261 | Train Loss: 0.1273618 Vali Loss: 0.6773946 Test Loss: 0.3716117
Validation loss decreased (0.679755 --> 0.677395).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1374400
	speed: 0.5838s/iter; left time: 6037.0718s
	iters: 200, epoch: 11 | loss: 0.1282863
	speed: 0.1305s/iter; left time: 1336.7085s
Epoch: 11 cost time: 34.17637252807617
Epoch: 11, Steps: 261 | Train Loss: 0.1259850 Vali Loss: 0.6757482 Test Loss: 0.3705913
Validation loss decreased (0.677395 --> 0.675748).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1310863
	speed: 0.5735s/iter; left time: 5781.3340s
	iters: 200, epoch: 12 | loss: 0.1201161
	speed: 0.1365s/iter; left time: 1362.5708s
Epoch: 12 cost time: 35.432286977767944
Epoch: 12, Steps: 261 | Train Loss: 0.1249585 Vali Loss: 0.6742307 Test Loss: 0.3701120
Validation loss decreased (0.675748 --> 0.674231).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1151880
	speed: 0.6020s/iter; left time: 5911.5044s
	iters: 200, epoch: 13 | loss: 0.1262809
	speed: 0.1370s/iter; left time: 1331.6711s
Epoch: 13 cost time: 36.27293848991394
Epoch: 13, Steps: 261 | Train Loss: 0.1242226 Vali Loss: 0.6745594 Test Loss: 0.3700263
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1251664
	speed: 0.6006s/iter; left time: 5740.8240s
	iters: 200, epoch: 14 | loss: 0.1226373
	speed: 0.1028s/iter; left time: 971.8785s
Epoch: 14 cost time: 28.73950433731079
Epoch: 14, Steps: 261 | Train Loss: 0.1236689 Vali Loss: 0.6741203 Test Loss: 0.3694722
Validation loss decreased (0.674231 --> 0.674120).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1250534
	speed: 0.5486s/iter; left time: 5100.6452s
	iters: 200, epoch: 15 | loss: 0.1302675
	speed: 0.1236s/iter; left time: 1136.6706s
Epoch: 15 cost time: 31.59524130821228
Epoch: 15, Steps: 261 | Train Loss: 0.1233471 Vali Loss: 0.6735478 Test Loss: 0.3697049
Validation loss decreased (0.674120 --> 0.673548).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1354622
	speed: 0.5313s/iter; left time: 4800.8088s
	iters: 200, epoch: 16 | loss: 0.1157276
	speed: 0.1179s/iter; left time: 1053.4194s
Epoch: 16 cost time: 31.061646223068237
Epoch: 16, Steps: 261 | Train Loss: 0.1230981 Vali Loss: 0.6745432 Test Loss: 0.3700446
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1226736
	speed: 0.5354s/iter; left time: 4698.0590s
	iters: 200, epoch: 17 | loss: 0.1204632
	speed: 0.1383s/iter; left time: 1200.1139s
Epoch: 17 cost time: 36.335166454315186
Epoch: 17, Steps: 261 | Train Loss: 0.1229425 Vali Loss: 0.6735774 Test Loss: 0.3700478
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1222880
	speed: 0.6759s/iter; left time: 5754.8714s
	iters: 200, epoch: 18 | loss: 0.1259395
	speed: 0.1513s/iter; left time: 1272.9617s
Epoch: 18 cost time: 41.07272291183472
Epoch: 18, Steps: 261 | Train Loss: 0.1227868 Vali Loss: 0.6738514 Test Loss: 0.3702343
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10644480.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3007474
	speed: 0.1535s/iter; left time: 1987.9340s
	iters: 200, epoch: 1 | loss: 0.3352568
	speed: 0.1434s/iter; left time: 1842.2373s
Epoch: 1 cost time: 38.778972148895264
Epoch: 1, Steps: 261 | Train Loss: 0.3398074 Vali Loss: 0.6612816 Test Loss: 0.3683339
Validation loss decreased (inf --> 0.661282).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2888830
	speed: 0.6399s/iter; left time: 8120.7017s
	iters: 200, epoch: 2 | loss: 0.3340770
	speed: 0.1306s/iter; left time: 1644.0476s
Epoch: 2 cost time: 35.0390989780426
Epoch: 2, Steps: 261 | Train Loss: 0.3381938 Vali Loss: 0.6592409 Test Loss: 0.3682660
Validation loss decreased (0.661282 --> 0.659241).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3626115
	speed: 0.6009s/iter; left time: 7469.0039s
	iters: 200, epoch: 3 | loss: 0.3474367
	speed: 0.1248s/iter; left time: 1538.9483s
Epoch: 3 cost time: 34.79993152618408
Epoch: 3, Steps: 261 | Train Loss: 0.3377912 Vali Loss: 0.6554282 Test Loss: 0.3681690
Validation loss decreased (0.659241 --> 0.655428).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3256749
	speed: 0.5700s/iter; left time: 6936.0556s
	iters: 200, epoch: 4 | loss: 0.3128441
	speed: 0.1267s/iter; left time: 1528.8761s
Epoch: 4 cost time: 33.67048501968384
Epoch: 4, Steps: 261 | Train Loss: 0.3375588 Vali Loss: 0.6559471 Test Loss: 0.3684193
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3191622
	speed: 0.5895s/iter; left time: 7018.7326s
	iters: 200, epoch: 5 | loss: 0.3338221
	speed: 0.1369s/iter; left time: 1616.7711s
Epoch: 5 cost time: 36.367430210113525
Epoch: 5, Steps: 261 | Train Loss: 0.3372161 Vali Loss: 0.6562591 Test Loss: 0.3680355
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3363612
	speed: 0.5805s/iter; left time: 6760.3842s
	iters: 200, epoch: 6 | loss: 0.3376122
	speed: 0.1223s/iter; left time: 1411.8195s
Epoch: 6 cost time: 33.441516637802124
Epoch: 6, Steps: 261 | Train Loss: 0.3370841 Vali Loss: 0.6561525 Test Loss: 0.3676959
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.36768078804016113, mae:0.3861030638217926, rse:0.5770105719566345, corr:[0.5408164  0.550646   0.5572394  0.5594936  0.55912864 0.55844593
 0.55858254 0.5596594  0.5611475  0.56223106 0.562459   0.5618782
 0.56097066 0.5601068  0.55935025 0.5585628  0.5575749  0.55632114
 0.5548399  0.55333227 0.5520324  0.55096567 0.55003506 0.54917246
 0.5481412  0.54699284 0.54587454 0.54496014 0.54453725 0.5446821
 0.5452381  0.54595894 0.546492   0.5467547  0.5467123  0.54661745
 0.5464826  0.5463593  0.5461959  0.54584473 0.54536366 0.54480284
 0.5443266  0.54411787 0.5441085  0.544154   0.54419065 0.5441376
 0.54397607 0.5437789  0.5437337  0.54384845 0.5440467  0.5441062
 0.5439553  0.5436209  0.5432389  0.5429762  0.543014   0.5432994
 0.5436967  0.544034   0.5441368  0.54389346 0.5434508  0.54299855
 0.5426646  0.542518   0.54252243 0.5425485  0.54251    0.54234314
 0.5420384  0.54167485 0.5413608  0.5411145  0.5408987  0.5407159
 0.5405603  0.5404543  0.5403948  0.5403518  0.54028946 0.54015267
 0.53995687 0.5397013  0.53945035 0.5392474  0.5391819  0.5393442
 0.53971153 0.5401703  0.54057825 0.5408335  0.54087883 0.54071766
 0.54042923 0.5401365  0.5397804  0.53940004 0.5390158  0.5386547
 0.53837657 0.53818333 0.53816015 0.53834355 0.53858316 0.5387991
 0.5388389  0.5386688  0.53831154 0.5378198  0.53730065 0.53688186
 0.53663415 0.53657913 0.53666985 0.5367922  0.53686005 0.5368463
 0.5367129  0.53646946 0.53613865 0.5358527  0.53568494 0.5356208
 0.5356298  0.5357096  0.5357915  0.5358569  0.5358212  0.5356728
 0.53548187 0.53527623 0.5350384  0.53483754 0.53477347 0.5347731
 0.534869   0.5350463  0.53518575 0.5352619  0.5352785  0.53526723
 0.5352738  0.5352854  0.53535765 0.53546697 0.53560334 0.53566283
 0.53557974 0.5353939  0.53513    0.53483874 0.53455967 0.5344027
 0.53433937 0.5343189  0.53436196 0.5343972  0.5344995  0.5346457
 0.5348102  0.5349077  0.5349033  0.53480977 0.53470147 0.5346786
 0.5347496  0.53487587 0.5350838  0.53531367 0.5354215  0.5353556
 0.5351445  0.53488606 0.53469604 0.53466487 0.5347763  0.5349842
 0.53517747 0.5352892  0.53525156 0.53517395 0.53516084 0.53528315
 0.5355777  0.53602594 0.5365265  0.5369175  0.53707576 0.5369694
 0.5366064  0.5361557  0.53565186 0.5351355  0.53464985 0.53422016
 0.5337745  0.53327954 0.53269804 0.53205884 0.5313845  0.5307128
 0.53010666 0.5295894  0.5291582  0.52883357 0.528565   0.5282783
 0.5278892  0.5273999  0.5268164  0.52617055 0.5254466  0.52476496
 0.5242351  0.5238509  0.523619   0.5234469  0.5232918  0.5231745
 0.52316153 0.5231785  0.52326065 0.5234136  0.5235692  0.5237268
 0.5239011  0.5241087  0.52434444 0.52454895 0.52473736 0.5249444
 0.52517515 0.5254612  0.52565575 0.5258292  0.5259039  0.5258997
 0.52579755 0.52551705 0.5251345  0.52476925 0.5245363  0.52441436
 0.5243749  0.52437836 0.5243148  0.52415043 0.52381825 0.5234333
 0.5230056  0.5227128  0.52256674 0.52254844 0.52261895 0.5227041
 0.52275383 0.52278656 0.5228017  0.52283174 0.52292114 0.5231829
 0.5235225  0.52383703 0.52404255 0.5241192  0.5240567  0.5238544
 0.5236363  0.52348053 0.5234602  0.5235996  0.5238347  0.5240955
 0.5242733  0.5244033  0.5244303  0.5244691  0.52451926 0.5245796
 0.5246753  0.5247605  0.5247465  0.5246119  0.5243356  0.5239098
 0.52341014 0.52299225 0.52268946 0.52236176 0.5219369  0.5213598
 0.520653   0.519818   0.51902056 0.5182928  0.5176773  0.517203
 0.5168133  0.5164756  0.51620674 0.5159744  0.51574033 0.5154398
 0.5151932  0.51491874 0.5146132  0.5142869  0.5140328  0.513827
 0.5136732  0.51358867 0.51347435 0.5133116  0.51313686 0.51307756
 0.5132     0.5133976  0.5135663  0.5135812  0.51336133 0.5129264
 0.51240385 0.5120452  0.51192313 0.5120443  0.5121947  0.5121601
 0.5120175  0.5118776  0.5118009  0.511547   0.5099564  0.504831  ]
