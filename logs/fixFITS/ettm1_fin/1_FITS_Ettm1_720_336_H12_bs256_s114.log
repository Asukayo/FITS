Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  58885120.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 11.966404914855957
Epoch: 1, Steps: 65 | Train Loss: 0.5307036 Vali Loss: 0.8868479 Test Loss: 0.4644308
Validation loss decreased (inf --> 0.886848).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 12.17558479309082
Epoch: 2, Steps: 65 | Train Loss: 0.3944558 Vali Loss: 0.7755970 Test Loss: 0.3994817
Validation loss decreased (0.886848 --> 0.775597).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 13.680218935012817
Epoch: 3, Steps: 65 | Train Loss: 0.3669777 Vali Loss: 0.7362508 Test Loss: 0.3804465
Validation loss decreased (0.775597 --> 0.736251).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 14.037678241729736
Epoch: 4, Steps: 65 | Train Loss: 0.3558800 Vali Loss: 0.7133179 Test Loss: 0.3720504
Validation loss decreased (0.736251 --> 0.713318).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 14.058136224746704
Epoch: 5, Steps: 65 | Train Loss: 0.3496260 Vali Loss: 0.7017554 Test Loss: 0.3678175
Validation loss decreased (0.713318 --> 0.701755).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 14.174980640411377
Epoch: 6, Steps: 65 | Train Loss: 0.3460363 Vali Loss: 0.6956015 Test Loss: 0.3660320
Validation loss decreased (0.701755 --> 0.695601).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.972183227539062
Epoch: 7, Steps: 65 | Train Loss: 0.3439533 Vali Loss: 0.6873897 Test Loss: 0.3652196
Validation loss decreased (0.695601 --> 0.687390).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.387170314788818
Epoch: 8, Steps: 65 | Train Loss: 0.3423235 Vali Loss: 0.6834878 Test Loss: 0.3648707
Validation loss decreased (0.687390 --> 0.683488).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.340881109237671
Epoch: 9, Steps: 65 | Train Loss: 0.3410446 Vali Loss: 0.6784935 Test Loss: 0.3651391
Validation loss decreased (0.683488 --> 0.678493).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 11.912482976913452
Epoch: 10, Steps: 65 | Train Loss: 0.3404140 Vali Loss: 0.6756999 Test Loss: 0.3650870
Validation loss decreased (0.678493 --> 0.675700).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.179874181747437
Epoch: 11, Steps: 65 | Train Loss: 0.3399262 Vali Loss: 0.6751364 Test Loss: 0.3653037
Validation loss decreased (0.675700 --> 0.675136).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.861887693405151
Epoch: 12, Steps: 65 | Train Loss: 0.3393794 Vali Loss: 0.6700521 Test Loss: 0.3653908
Validation loss decreased (0.675136 --> 0.670052).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 11.964401245117188
Epoch: 13, Steps: 65 | Train Loss: 0.3390582 Vali Loss: 0.6699280 Test Loss: 0.3656017
Validation loss decreased (0.670052 --> 0.669928).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.295257329940796
Epoch: 14, Steps: 65 | Train Loss: 0.3387644 Vali Loss: 0.6706426 Test Loss: 0.3658093
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.183040380477905
Epoch: 15, Steps: 65 | Train Loss: 0.3384844 Vali Loss: 0.6677230 Test Loss: 0.3659448
Validation loss decreased (0.669928 --> 0.667723).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 11.375947952270508
Epoch: 16, Steps: 65 | Train Loss: 0.3381869 Vali Loss: 0.6658557 Test Loss: 0.3660116
Validation loss decreased (0.667723 --> 0.665856).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 12.268205642700195
Epoch: 17, Steps: 65 | Train Loss: 0.3381729 Vali Loss: 0.6681662 Test Loss: 0.3661107
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 13.288838386535645
Epoch: 18, Steps: 65 | Train Loss: 0.3379865 Vali Loss: 0.6657135 Test Loss: 0.3661817
Validation loss decreased (0.665856 --> 0.665714).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 12.400786876678467
Epoch: 19, Steps: 65 | Train Loss: 0.3379953 Vali Loss: 0.6661139 Test Loss: 0.3661658
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 12.793987035751343
Epoch: 20, Steps: 65 | Train Loss: 0.3375502 Vali Loss: 0.6657083 Test Loss: 0.3662601
Validation loss decreased (0.665714 --> 0.665708).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 12.573364019393921
Epoch: 21, Steps: 65 | Train Loss: 0.3376297 Vali Loss: 0.6649411 Test Loss: 0.3664495
Validation loss decreased (0.665708 --> 0.664941).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 12.803321599960327
Epoch: 22, Steps: 65 | Train Loss: 0.3375476 Vali Loss: 0.6620232 Test Loss: 0.3664880
Validation loss decreased (0.664941 --> 0.662023).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 13.30388355255127
Epoch: 23, Steps: 65 | Train Loss: 0.3374565 Vali Loss: 0.6590600 Test Loss: 0.3664614
Validation loss decreased (0.662023 --> 0.659060).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 13.678356409072876
Epoch: 24, Steps: 65 | Train Loss: 0.3371663 Vali Loss: 0.6639904 Test Loss: 0.3665273
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 11.714285135269165
Epoch: 25, Steps: 65 | Train Loss: 0.3372911 Vali Loss: 0.6625479 Test Loss: 0.3664356
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 11.962010145187378
Epoch: 26, Steps: 65 | Train Loss: 0.3373237 Vali Loss: 0.6644704 Test Loss: 0.3665406
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3659350275993347, mae:0.3846631348133087, rse:0.5756391286849976, corr:[0.536496   0.5463577  0.55268574 0.5544416  0.55480474 0.5557094
 0.55707455 0.55827993 0.5589246  0.5591158  0.5593761  0.5597881
 0.56003034 0.55972517 0.5588099  0.557604   0.55657226 0.5558977
 0.55524445 0.5542503  0.55284375 0.5511525  0.5495411  0.5484885
 0.5478916  0.54745024 0.5468003  0.5458254  0.5449341  0.5446552
 0.54526085 0.5464951  0.5475954  0.5480583  0.5477164  0.5471202
 0.5467342  0.5468158  0.5471153  0.5471163  0.5466558  0.5458191
 0.5450641  0.5448621  0.54514146 0.5454817  0.5455479  0.54518497
 0.5445537  0.54402536 0.5439535  0.5442375  0.5445382  0.5445146
 0.5441827  0.5437844  0.5435983  0.54368186 0.54390985 0.54398847
 0.5438107  0.54350525 0.5432888  0.5432889  0.5435652  0.54392934
 0.544142   0.5441333  0.5440298  0.5439987  0.54414904 0.5443919
 0.54453534 0.5445     0.544328   0.5440743  0.54380846 0.5436131
 0.5434855  0.5434051  0.54332316 0.54318917 0.5430006  0.5427841
 0.5426206  0.54249364 0.5423938  0.54228425 0.5422123  0.5422693
 0.5424957  0.5428447  0.5431979  0.5434187  0.5433892  0.54308045
 0.5426034  0.5421422  0.5417078  0.5413709  0.5411334  0.5409565
 0.540806   0.54062986 0.5404732  0.5403596  0.5401968  0.5400088
 0.5397691  0.53953236 0.5393337  0.5391513  0.5389506  0.5387185
 0.5384277  0.5381039  0.5378114  0.53760374 0.5375059  0.53749645
 0.53745365 0.53728324 0.5369681  0.5366916  0.5365809  0.53659016
 0.53660065 0.53652704 0.53632134 0.5360962  0.5359447  0.5359301
 0.5360363  0.5360986  0.5359513  0.5356302  0.53533715 0.5351532
 0.53518593 0.5353613  0.53545755 0.53541005 0.5352907  0.53525776
 0.5354017  0.5356125  0.53576344 0.53573936 0.53557825 0.5353741
 0.53528345 0.53542024 0.5356862  0.53590393 0.5359266  0.5357926
 0.5355677  0.53537834 0.5353679  0.53545374 0.5355915  0.5356763
 0.5357043  0.5357385  0.5358909  0.5361927  0.53657496 0.5369273
 0.5370948  0.5370251  0.5368772  0.53678375 0.5367686  0.5368237
 0.53689533 0.5369046  0.53680325 0.53663325 0.5364741  0.5364355
 0.53651905 0.53667206 0.53675836 0.53676087 0.53672636 0.53675103
 0.5369534  0.53738046 0.5379175  0.5383404  0.5385056  0.5383838
 0.5380326  0.53762764 0.53720826 0.53676784 0.53626007 0.53564805
 0.5349083  0.5341129  0.5333533  0.5327314  0.5322433  0.53180194
 0.5313235  0.53075737 0.5301044  0.52946246 0.52889067 0.5283877
 0.5278735  0.52731615 0.5266939  0.52604383 0.5253755  0.5248245
 0.5244571  0.5241974  0.5240207  0.52384275 0.5236515  0.52351516
 0.52352846 0.52360463 0.5237627  0.52398187 0.5241571  0.5242402
 0.52423775 0.52419794 0.5241684  0.5241573  0.5242052  0.52430695
 0.52439415 0.52446437 0.52442986 0.52448785 0.52465993 0.52496666
 0.52525675 0.5253134  0.5251498  0.5249117  0.52475494 0.52470547
 0.5247371  0.5247848  0.52473825 0.5246178  0.52440906 0.5242514
 0.52415013 0.52420807 0.5243258  0.5244094  0.5244488  0.52445275
 0.52445614 0.52450067 0.5245651  0.5246308  0.5246977  0.52484244
 0.5249694  0.52503395 0.5250245  0.5249823  0.52492374 0.5248312
 0.524751   0.5246523  0.5245412  0.5244522  0.5243896  0.5243942
 0.5244146  0.52447534 0.5244635  0.5244504  0.5244128  0.5243838
 0.5244512  0.5245941  0.52468354 0.524653   0.52444845 0.5240484
 0.5235647  0.523169   0.52283233 0.522383   0.52182347 0.52122796
 0.52071923 0.52027565 0.519942   0.51955354 0.5190237  0.5183872
 0.5177266  0.5171861  0.5168499  0.5166101  0.5162896  0.51578027
 0.51529866 0.51491755 0.5147131  0.5146007  0.5144703  0.5141311
 0.5136457  0.51327825 0.5131191  0.5131656  0.513229   0.5131426
 0.51286167 0.5124774  0.51221097 0.51219    0.51231146 0.51229155
 0.51193166 0.5114724  0.5111708  0.51122826 0.51149195 0.5115858
 0.5113279  0.5109721  0.5111994  0.5122751  0.51280993 0.50909984]
