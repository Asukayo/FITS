Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21288960.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4090021
	speed: 0.1443s/iter; left time: 923.9371s
Epoch: 1 cost time: 18.60556674003601
Epoch: 1, Steps: 130 | Train Loss: 0.4939481 Vali Loss: 0.8087023 Test Loss: 0.4310621
Validation loss decreased (inf --> 0.808702).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3626441
	speed: 0.3550s/iter; left time: 2226.0359s
Epoch: 2 cost time: 16.817867040634155
Epoch: 2, Steps: 130 | Train Loss: 0.3726299 Vali Loss: 0.7296501 Test Loss: 0.3845622
Validation loss decreased (0.808702 --> 0.729650).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3426889
	speed: 0.3456s/iter; left time: 2122.4889s
Epoch: 3 cost time: 16.717551946640015
Epoch: 3, Steps: 130 | Train Loss: 0.3526563 Vali Loss: 0.6998935 Test Loss: 0.3706577
Validation loss decreased (0.729650 --> 0.699893).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3340446
	speed: 0.3328s/iter; left time: 2000.6754s
Epoch: 4 cost time: 15.848628997802734
Epoch: 4, Steps: 130 | Train Loss: 0.3455368 Vali Loss: 0.6848529 Test Loss: 0.3667175
Validation loss decreased (0.699893 --> 0.684853).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3369200
	speed: 0.3320s/iter; left time: 1952.6141s
Epoch: 5 cost time: 16.513973712921143
Epoch: 5, Steps: 130 | Train Loss: 0.3422937 Vali Loss: 0.6782647 Test Loss: 0.3657621
Validation loss decreased (0.684853 --> 0.678265).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3273381
	speed: 0.3451s/iter; left time: 1984.5362s
Epoch: 6 cost time: 17.159736156463623
Epoch: 6, Steps: 130 | Train Loss: 0.3405628 Vali Loss: 0.6734142 Test Loss: 0.3658840
Validation loss decreased (0.678265 --> 0.673414).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3396845
	speed: 0.3425s/iter; left time: 1925.4626s
Epoch: 7 cost time: 16.78090739250183
Epoch: 7, Steps: 130 | Train Loss: 0.3394976 Vali Loss: 0.6705368 Test Loss: 0.3661684
Validation loss decreased (0.673414 --> 0.670537).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3411645
	speed: 0.3506s/iter; left time: 1925.0070s
Epoch: 8 cost time: 16.647466897964478
Epoch: 8, Steps: 130 | Train Loss: 0.3388793 Vali Loss: 0.6664196 Test Loss: 0.3663022
Validation loss decreased (0.670537 --> 0.666420).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3457770
	speed: 0.3561s/iter; left time: 1909.0581s
Epoch: 9 cost time: 17.063955068588257
Epoch: 9, Steps: 130 | Train Loss: 0.3386521 Vali Loss: 0.6671904 Test Loss: 0.3664960
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3497319
	speed: 0.3807s/iter; left time: 1991.2937s
Epoch: 10 cost time: 18.126818656921387
Epoch: 10, Steps: 130 | Train Loss: 0.3383906 Vali Loss: 0.6620748 Test Loss: 0.3666123
Validation loss decreased (0.666420 --> 0.662075).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3538440
	speed: 0.3649s/iter; left time: 1861.5871s
Epoch: 11 cost time: 17.91232991218567
Epoch: 11, Steps: 130 | Train Loss: 0.3381025 Vali Loss: 0.6620378 Test Loss: 0.3668346
Validation loss decreased (0.662075 --> 0.662038).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3149780
	speed: 0.3603s/iter; left time: 1790.8705s
Epoch: 12 cost time: 16.1290123462677
Epoch: 12, Steps: 130 | Train Loss: 0.3377693 Vali Loss: 0.6635537 Test Loss: 0.3665710
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3289144
	speed: 0.2761s/iter; left time: 1336.6014s
Epoch: 13 cost time: 12.747987270355225
Epoch: 13, Steps: 130 | Train Loss: 0.3376433 Vali Loss: 0.6612186 Test Loss: 0.3668883
Validation loss decreased (0.662038 --> 0.661219).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3179331
	speed: 0.2646s/iter; left time: 1246.6070s
Epoch: 14 cost time: 9.259897470474243
Epoch: 14, Steps: 130 | Train Loss: 0.3376132 Vali Loss: 0.6603524 Test Loss: 0.3669647
Validation loss decreased (0.661219 --> 0.660352).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3181633
	speed: 0.2979s/iter; left time: 1364.8513s
Epoch: 15 cost time: 15.904072046279907
Epoch: 15, Steps: 130 | Train Loss: 0.3373411 Vali Loss: 0.6599166 Test Loss: 0.3665975
Validation loss decreased (0.660352 --> 0.659917).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3315066
	speed: 0.3198s/iter; left time: 1423.3648s
Epoch: 16 cost time: 15.733020544052124
Epoch: 16, Steps: 130 | Train Loss: 0.3373321 Vali Loss: 0.6586119 Test Loss: 0.3666534
Validation loss decreased (0.659917 --> 0.658612).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3241690
	speed: 0.3530s/iter; left time: 1525.2408s
Epoch: 17 cost time: 17.765809297561646
Epoch: 17, Steps: 130 | Train Loss: 0.3371478 Vali Loss: 0.6573270 Test Loss: 0.3667505
Validation loss decreased (0.658612 --> 0.657327).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3444667
	speed: 0.3290s/iter; left time: 1378.8700s
Epoch: 18 cost time: 16.214574575424194
Epoch: 18, Steps: 130 | Train Loss: 0.3371546 Vali Loss: 0.6568313 Test Loss: 0.3668339
Validation loss decreased (0.657327 --> 0.656831).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3447540
	speed: 0.3539s/iter; left time: 1437.1699s
Epoch: 19 cost time: 16.942213535308838
Epoch: 19, Steps: 130 | Train Loss: 0.3371776 Vali Loss: 0.6590160 Test Loss: 0.3668313
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3434112
	speed: 0.3428s/iter; left time: 1347.4315s
Epoch: 20 cost time: 16.319260835647583
Epoch: 20, Steps: 130 | Train Loss: 0.3370556 Vali Loss: 0.6577140 Test Loss: 0.3668631
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3370915
	speed: 0.3297s/iter; left time: 1253.0078s
Epoch: 21 cost time: 16.070300817489624
Epoch: 21, Steps: 130 | Train Loss: 0.3369524 Vali Loss: 0.6564906 Test Loss: 0.3667722
Validation loss decreased (0.656831 --> 0.656491).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3336311
	speed: 0.3207s/iter; left time: 1177.3065s
Epoch: 22 cost time: 15.89641547203064
Epoch: 22, Steps: 130 | Train Loss: 0.3370406 Vali Loss: 0.6572871 Test Loss: 0.3668873
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3575512
	speed: 0.3150s/iter; left time: 1115.3622s
Epoch: 23 cost time: 16.1409752368927
Epoch: 23, Steps: 130 | Train Loss: 0.3367898 Vali Loss: 0.6563942 Test Loss: 0.3667131
Validation loss decreased (0.656491 --> 0.656394).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3435545
	speed: 0.3220s/iter; left time: 1098.2553s
Epoch: 24 cost time: 15.916807413101196
Epoch: 24, Steps: 130 | Train Loss: 0.3368576 Vali Loss: 0.6571240 Test Loss: 0.3666961
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3405191
	speed: 0.3163s/iter; left time: 1037.6408s
Epoch: 25 cost time: 15.23701786994934
Epoch: 25, Steps: 130 | Train Loss: 0.3368423 Vali Loss: 0.6580198 Test Loss: 0.3668798
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3426357
	speed: 0.3035s/iter; left time: 956.4773s
Epoch: 26 cost time: 12.709651231765747
Epoch: 26, Steps: 130 | Train Loss: 0.3369922 Vali Loss: 0.6563970 Test Loss: 0.3668237
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3665159046649933, mae:0.3851238489151001, rse:0.5760958790779114, corr:[0.53591967 0.5449402  0.5510926  0.5532571  0.5534949  0.5538992
 0.55506486 0.55671257 0.55824465 0.5591276  0.55938566 0.55926883
 0.55913645 0.55900997 0.5586842  0.557971   0.55687535 0.5555759
 0.55426633 0.55312675 0.5521989  0.55127877 0.55016106 0.548872
 0.5473598  0.5459794  0.5450701  0.5447296  0.54499215 0.54562527
 0.5463041  0.54682064 0.5469991  0.5470299  0.54703605 0.54725915
 0.5474988  0.5476125  0.54745764 0.5469104  0.54620147 0.5455662
 0.54525113 0.54536134 0.5456339  0.5457796  0.5457157  0.54543513
 0.5450287  0.54467607 0.5446057  0.54478675 0.5450619  0.5451453
 0.544958   0.54455316 0.54410076 0.54375404 0.54365295 0.5436806
 0.5437002  0.5436217  0.5434079  0.5430806  0.5428826  0.54297227
 0.5433202  0.5437765  0.54416686 0.5443371  0.5442768  0.544036
 0.54370767 0.5434247  0.5432837  0.54323125 0.54316276 0.54303604
 0.54283875 0.542613   0.54240793 0.5422464  0.5421344  0.5420031
 0.54184824 0.5416385  0.54142904 0.5412604  0.5412191  0.5413898
 0.541736   0.54212815 0.5424183  0.54249716 0.54233    0.5419532
 0.5415032  0.5411494  0.54083824 0.5405871  0.54037833 0.5401907
 0.5400297  0.5398613  0.53975093 0.53973883 0.5397067  0.5396401
 0.5394708  0.53922474 0.5389411  0.5386445  0.53836733 0.53813684
 0.5379228  0.53769565 0.5374269  0.53709066 0.5367395  0.536504
 0.53643453 0.5365156  0.5366285  0.536731   0.53674996 0.53660744
 0.5363399  0.5360922  0.53595144 0.53597826 0.53606373 0.5360957
 0.5360472  0.5358915  0.5356104  0.5353222  0.5351875  0.5351562
 0.53524077 0.5353899  0.5354545  0.535419   0.5353431  0.53532326
 0.5354282  0.53560615 0.535839   0.53605026 0.5362136  0.5362501
 0.5361433  0.5359859  0.5358229  0.53569764 0.5356024  0.5355967
 0.53562844 0.5356342  0.5356515  0.53563476 0.53567487 0.535766
 0.5358971  0.53600985 0.53609174 0.5361505  0.53623295 0.53639555
 0.53659666 0.5367545  0.53688335 0.5369585  0.5369167  0.5367812
 0.53661895 0.53650445 0.53647846 0.53654015 0.536626   0.536691
 0.5366825  0.5366085  0.53646374 0.53637725 0.53643084 0.5366336
 0.5369602  0.5373524  0.53771186 0.5379156  0.53789926 0.5376836
 0.53730756 0.53692114 0.53651434 0.53606933 0.5355882  0.5350827
 0.5345158  0.53389597 0.53323644 0.5325829  0.5319537  0.5313505
 0.53078693 0.5302408  0.52968836 0.5291605  0.5286514  0.5281368
 0.52758163 0.5270077  0.5264024  0.5257551  0.52501076 0.52426434
 0.5236273  0.52312046 0.5228037  0.52263975 0.5226163  0.5227417
 0.5230336  0.5233338  0.523602   0.52379495 0.5238676  0.52386844
 0.52386814 0.52392375 0.5240391  0.5241393  0.5242089  0.52426004
 0.5243088  0.52443296 0.52453893 0.5247474  0.52498585 0.525245
 0.52543455 0.5254146  0.52523375 0.5250206  0.52489656 0.52484375
 0.52483433 0.5248406  0.52478975 0.52469087 0.52450436 0.52433693
 0.5241669  0.52410096 0.52409357 0.5240996  0.5241152  0.5241429
 0.52419335 0.5242987  0.52442044 0.5245298  0.52459705 0.5246918
 0.5247309  0.52470773 0.5246545  0.5246289  0.52462757 0.5245919
 0.52454585 0.5244706  0.52437675 0.52429223 0.5242018  0.52411306
 0.5239859  0.52390647 0.5238458  0.52389604 0.5240128  0.52415806
 0.52432466 0.52445996 0.52448094 0.524393   0.52418506 0.52381164
 0.52331734 0.5228439  0.5224127  0.5219217  0.5213699  0.5208038
 0.52032197 0.5199031  0.519597   0.51926816 0.5188022  0.5181598
 0.51735437 0.51653785 0.5159367  0.5156426  0.51560324 0.5156171
 0.51561123 0.5153616  0.51484555 0.51420695 0.513738   0.5135406
 0.51360685 0.51382756 0.513905   0.513689   0.51320964 0.51272404
 0.51244855 0.51240534 0.51254475 0.5127112  0.51270694 0.5124653
 0.512096   0.5118686  0.5118585  0.5120585  0.5122451  0.51220345
 0.512034   0.51202464 0.51246613 0.5132064  0.51313686 0.50990844]
