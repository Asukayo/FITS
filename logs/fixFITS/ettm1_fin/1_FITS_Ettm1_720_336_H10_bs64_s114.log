Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=90, out_features=132, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10644480.0
params:  12012.0
Trainable parameters:  12012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3997598
	speed: 0.1411s/iter; left time: 1827.5747s
	iters: 200, epoch: 1 | loss: 0.4264095
	speed: 0.1237s/iter; left time: 1590.2343s
Epoch: 1 cost time: 34.75399160385132
Epoch: 1, Steps: 261 | Train Loss: 0.4356930 Vali Loss: 0.7316959 Test Loss: 0.3863291
Validation loss decreased (inf --> 0.731696).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3499643
	speed: 0.5994s/iter; left time: 7606.5399s
	iters: 200, epoch: 2 | loss: 0.3656686
	speed: 0.1350s/iter; left time: 1700.1170s
Epoch: 2 cost time: 36.83560085296631
Epoch: 2, Steps: 261 | Train Loss: 0.3498434 Vali Loss: 0.6869864 Test Loss: 0.3670966
Validation loss decreased (0.731696 --> 0.686986).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3158378
	speed: 0.6135s/iter; left time: 7625.1471s
	iters: 200, epoch: 3 | loss: 0.3431299
	speed: 0.1344s/iter; left time: 1657.2577s
Epoch: 3 cost time: 36.222766160964966
Epoch: 3, Steps: 261 | Train Loss: 0.3418752 Vali Loss: 0.6753441 Test Loss: 0.3665044
Validation loss decreased (0.686986 --> 0.675344).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3396544
	speed: 0.6334s/iter; left time: 7707.3027s
	iters: 200, epoch: 4 | loss: 0.3258844
	speed: 0.1368s/iter; left time: 1650.4485s
Epoch: 4 cost time: 37.36394119262695
Epoch: 4, Steps: 261 | Train Loss: 0.3396691 Vali Loss: 0.6677686 Test Loss: 0.3669283
Validation loss decreased (0.675344 --> 0.667769).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3380521
	speed: 0.6178s/iter; left time: 7356.4122s
	iters: 200, epoch: 5 | loss: 0.3240921
	speed: 0.1407s/iter; left time: 1661.5611s
Epoch: 5 cost time: 35.5152473449707
Epoch: 5, Steps: 261 | Train Loss: 0.3387792 Vali Loss: 0.6660561 Test Loss: 0.3673131
Validation loss decreased (0.667769 --> 0.666056).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3692735
	speed: 0.5346s/iter; left time: 6226.2778s
	iters: 200, epoch: 6 | loss: 0.3221997
	speed: 0.1258s/iter; left time: 1452.7498s
Epoch: 6 cost time: 33.00360345840454
Epoch: 6, Steps: 261 | Train Loss: 0.3381792 Vali Loss: 0.6630241 Test Loss: 0.3672024
Validation loss decreased (0.666056 --> 0.663024).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3440205
	speed: 0.5615s/iter; left time: 6392.8109s
	iters: 200, epoch: 7 | loss: 0.3282187
	speed: 0.1401s/iter; left time: 1581.1973s
Epoch: 7 cost time: 36.52333402633667
Epoch: 7, Steps: 261 | Train Loss: 0.3379415 Vali Loss: 0.6607148 Test Loss: 0.3671075
Validation loss decreased (0.663024 --> 0.660715).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3124785
	speed: 0.6019s/iter; left time: 6695.1439s
	iters: 200, epoch: 8 | loss: 0.3553853
	speed: 0.1255s/iter; left time: 1383.8475s
Epoch: 8 cost time: 34.23874306678772
Epoch: 8, Steps: 261 | Train Loss: 0.3375469 Vali Loss: 0.6590216 Test Loss: 0.3672790
Validation loss decreased (0.660715 --> 0.659022).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3232670
	speed: 0.5788s/iter; left time: 6287.6421s
	iters: 200, epoch: 9 | loss: 0.3489716
	speed: 0.1362s/iter; left time: 1465.4055s
Epoch: 9 cost time: 35.42637515068054
Epoch: 9, Steps: 261 | Train Loss: 0.3374471 Vali Loss: 0.6576831 Test Loss: 0.3673202
Validation loss decreased (0.659022 --> 0.657683).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3376585
	speed: 0.5600s/iter; left time: 5937.5059s
	iters: 200, epoch: 10 | loss: 0.3218009
	speed: 0.1305s/iter; left time: 1370.3087s
Epoch: 10 cost time: 31.546087503433228
Epoch: 10, Steps: 261 | Train Loss: 0.3371791 Vali Loss: 0.6580113 Test Loss: 0.3669146
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3694895
	speed: 0.3902s/iter; left time: 4034.5573s
	iters: 200, epoch: 11 | loss: 0.3429027
	speed: 0.0673s/iter; left time: 689.0871s
Epoch: 11 cost time: 21.081520318984985
Epoch: 11, Steps: 261 | Train Loss: 0.3372541 Vali Loss: 0.6575335 Test Loss: 0.3672732
Validation loss decreased (0.657683 --> 0.657534).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3536759
	speed: 0.5621s/iter; left time: 5666.3181s
	iters: 200, epoch: 12 | loss: 0.3249513
	speed: 0.1253s/iter; left time: 1250.9084s
Epoch: 12 cost time: 33.54786729812622
Epoch: 12, Steps: 261 | Train Loss: 0.3371314 Vali Loss: 0.6561733 Test Loss: 0.3671923
Validation loss decreased (0.657534 --> 0.656173).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3105726
	speed: 0.5439s/iter; left time: 5340.7989s
	iters: 200, epoch: 13 | loss: 0.3470247
	speed: 0.1314s/iter; left time: 1276.9066s
Epoch: 13 cost time: 33.57133960723877
Epoch: 13, Steps: 261 | Train Loss: 0.3370489 Vali Loss: 0.6569663 Test Loss: 0.3672459
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3449059
	speed: 0.5958s/iter; left time: 5694.3928s
	iters: 200, epoch: 14 | loss: 0.3358499
	speed: 0.0878s/iter; left time: 829.9464s
Epoch: 14 cost time: 30.706774473190308
Epoch: 14, Steps: 261 | Train Loss: 0.3368409 Vali Loss: 0.6567896 Test Loss: 0.3670090
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3416530
	speed: 0.4856s/iter; left time: 4514.3214s
	iters: 200, epoch: 15 | loss: 0.3536575
	speed: 0.1284s/iter; left time: 1180.6956s
Epoch: 15 cost time: 28.818203926086426
Epoch: 15, Steps: 261 | Train Loss: 0.3369148 Vali Loss: 0.6562234 Test Loss: 0.3672912
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.36667487025260925, mae:0.38530367612838745, rse:0.5762207508087158, corr:[0.5367509  0.5462799  0.5526702  0.55479485 0.5548113  0.5548771
 0.5556547  0.55697244 0.5583488  0.5592864  0.5597402  0.55986255
 0.5599392  0.55993676 0.55959374 0.55870736 0.55733645 0.55575913
 0.5542513  0.5530263  0.55212075 0.5512998  0.5503141  0.5491333
 0.5476413  0.54613525 0.5449508  0.5442644  0.5442261  0.5446909
 0.5453712  0.5460376  0.5464555  0.5467138  0.5468242  0.54697686
 0.5469987  0.5468319  0.54643077 0.5457554  0.54509145 0.5446692
 0.5446743  0.5451127  0.5456225  0.5458596  0.5457382  0.5452708
 0.5445981  0.5439551  0.54362094 0.5435962  0.54372746 0.5437121
 0.543455   0.5430227  0.54260445 0.5423619  0.5424288  0.5426692
 0.54291534 0.54303586 0.5429514  0.54264545 0.54234576 0.54222566
 0.54229695 0.5424768  0.5426624  0.54276    0.54278415 0.54276764
 0.5427469  0.5427657  0.54282653 0.5428304  0.54269016 0.54243374
 0.5421325  0.54187334 0.54170924 0.54164547 0.54165715 0.5416433
 0.54155695 0.54134935 0.54109156 0.5408644  0.5407904  0.54098237
 0.5414004  0.541892   0.54227054 0.54239744 0.5422187  0.54178137
 0.541246   0.5408087  0.54043126 0.5401409  0.539925   0.53975457
 0.5396154  0.5394396  0.53926826 0.53913784 0.5389374  0.53868
 0.5383132  0.5378956  0.5374862  0.5371262  0.53685224 0.5366882
 0.5365891  0.53650117 0.5363795  0.53619754 0.5360022  0.53589433
 0.5358884  0.5359394  0.5359371  0.53589207 0.5357985  0.5356481
 0.5354998  0.53546786 0.5355554  0.53572816 0.53582674 0.5357321
 0.53544843 0.53501064 0.5344765  0.5340283  0.5338486  0.5338616
 0.53403634 0.5342917  0.5344633  0.53453785 0.5345835  0.53469795
 0.53493917 0.53522545 0.5355173  0.5357193  0.5358005  0.5356931
 0.5354214  0.5351345  0.5349193  0.5348301  0.53483796 0.53496116
 0.5350945  0.5351336  0.5351063  0.5349811  0.53490543 0.534931
 0.5350921  0.53533804 0.5356303  0.53592324 0.5362038  0.53647953
 0.5366855  0.5367539  0.536738   0.53665584 0.5364723  0.536242
 0.53605217 0.5359768  0.5360331  0.5361851  0.5363273  0.5364057
 0.536365   0.53623223 0.5360381  0.5359388  0.53602475 0.5362926
 0.53669804 0.5371649  0.53758794 0.5378524  0.5378989  0.53775626
 0.5374559  0.5371381  0.5367681  0.5363     0.53572315 0.5350686
 0.53434026 0.53359807 0.53290164 0.5323245  0.5318789  0.531518
 0.5311828  0.5307843  0.5302661  0.52966607 0.5290125  0.5283384
 0.52766913 0.52707607 0.5265601  0.52608913 0.5255637  0.525042
 0.52460146 0.5242414  0.52401066 0.52386844 0.5237996  0.5238177
 0.5239452  0.5240541  0.5241285  0.5241565  0.5241003  0.5240064
 0.52392733 0.5238968  0.52389187 0.52383757 0.52373797 0.5236281
 0.52353555 0.523544   0.523566   0.5237229  0.5239511  0.5242443
 0.52452123 0.5246297  0.524581   0.5244485  0.5243149  0.524148
 0.523946   0.52374864 0.5235499  0.52340275 0.5232706  0.5232371
 0.52323395 0.52332073 0.5233951  0.5233835  0.5232929  0.5231618
 0.52305377 0.5230404  0.5231097  0.5232291  0.5233516  0.5235118
 0.5236049  0.5235962  0.5235149  0.52344567 0.52344203 0.5234963
 0.52367413 0.5239411  0.5242458  0.524519   0.5246545  0.5246057
 0.52434003 0.5240091  0.52367663 0.5235414  0.5236123  0.5238403
 0.5241505  0.5244073  0.5244723  0.52433354 0.52400994 0.52351713
 0.52297056 0.5225393  0.5222181  0.52184314 0.5213609  0.5207816
 0.52020055 0.5196461  0.5192589  0.5189813  0.51871836 0.5183844
 0.51790637 0.51733196 0.5167987  0.51637566 0.51607734 0.5158346
 0.51569414 0.51545703 0.5150661  0.51459694 0.5142669  0.51414007
 0.51421154 0.51440454 0.51447374 0.51429147 0.5138877  0.51347977
 0.5132324  0.5131467  0.51319337 0.51329094 0.5133135  0.5132058
 0.513016   0.5129274  0.5129347  0.51300496 0.5129701  0.5127342
 0.51248604 0.51249504 0.51294667 0.51364493 0.5136231  0.5109183 ]
