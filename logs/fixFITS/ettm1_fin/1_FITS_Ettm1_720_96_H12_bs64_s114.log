Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_96_FITS_ETTm1_ftM_sl720_ll48_pl96_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 11425
test 11425
Model(
  (freq_upsampler): Linear(in_features=106, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11397120.0
params:  12840.0
Trainable parameters:  12840
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2937787
	speed: 0.1260s/iter; left time: 1644.1499s
	iters: 200, epoch: 1 | loss: 0.2953394
	speed: 0.1212s/iter; left time: 1569.0979s
Epoch: 1 cost time: 31.94426679611206
Epoch: 1, Steps: 263 | Train Loss: 0.3207129 Vali Loss: 0.4365470 Test Loss: 0.3129485
Validation loss decreased (inf --> 0.436547).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2685033
	speed: 0.5282s/iter; left time: 6755.2532s
	iters: 200, epoch: 2 | loss: 0.2897082
	speed: 0.1187s/iter; left time: 1505.7444s
Epoch: 2 cost time: 31.327203035354614
Epoch: 2, Steps: 263 | Train Loss: 0.2704149 Vali Loss: 0.4160782 Test Loss: 0.3095680
Validation loss decreased (0.436547 --> 0.416078).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2588059
	speed: 0.5164s/iter; left time: 6468.3721s
	iters: 200, epoch: 3 | loss: 0.2586372
	speed: 0.1133s/iter; left time: 1407.6619s
Epoch: 3 cost time: 30.65696430206299
Epoch: 3, Steps: 263 | Train Loss: 0.2663064 Vali Loss: 0.4057592 Test Loss: 0.3091342
Validation loss decreased (0.416078 --> 0.405759).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2703988
	speed: 0.5043s/iter; left time: 6183.8141s
	iters: 200, epoch: 4 | loss: 0.2518893
	speed: 0.1053s/iter; left time: 1280.6500s
Epoch: 4 cost time: 27.99584984779358
Epoch: 4, Steps: 263 | Train Loss: 0.2647057 Vali Loss: 0.4013827 Test Loss: 0.3096274
Validation loss decreased (0.405759 --> 0.401383).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2591863
	speed: 0.4620s/iter; left time: 5543.5157s
	iters: 200, epoch: 5 | loss: 0.2815532
	speed: 0.0902s/iter; left time: 1073.5091s
Epoch: 5 cost time: 24.70207166671753
Epoch: 5, Steps: 263 | Train Loss: 0.2640091 Vali Loss: 0.4000608 Test Loss: 0.3087319
Validation loss decreased (0.401383 --> 0.400061).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2385479
	speed: 0.5461s/iter; left time: 6408.8515s
	iters: 200, epoch: 6 | loss: 0.2847314
	speed: 0.0985s/iter; left time: 1145.9805s
Epoch: 6 cost time: 29.310755968093872
Epoch: 6, Steps: 263 | Train Loss: 0.2635249 Vali Loss: 0.3973704 Test Loss: 0.3091649
Validation loss decreased (0.400061 --> 0.397370).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2701338
	speed: 0.5478s/iter; left time: 6285.3177s
	iters: 200, epoch: 7 | loss: 0.2513798
	speed: 0.1188s/iter; left time: 1350.7880s
Epoch: 7 cost time: 31.29048728942871
Epoch: 7, Steps: 263 | Train Loss: 0.2630721 Vali Loss: 0.3995317 Test Loss: 0.3082823
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2496963
	speed: 0.4267s/iter; left time: 4782.7472s
	iters: 200, epoch: 8 | loss: 0.2700597
	speed: 0.0978s/iter; left time: 1087.0911s
Epoch: 8 cost time: 26.322903394699097
Epoch: 8, Steps: 263 | Train Loss: 0.2629037 Vali Loss: 0.3994294 Test Loss: 0.3082642
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2870777
	speed: 0.4753s/iter; left time: 5202.7444s
	iters: 200, epoch: 9 | loss: 0.2589847
	speed: 0.1100s/iter; left time: 1192.8869s
Epoch: 9 cost time: 29.200318574905396
Epoch: 9, Steps: 263 | Train Loss: 0.2628319 Vali Loss: 0.3985592 Test Loss: 0.3080580
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_96_FITS_ETTm1_ftM_sl720_ll48_pl96_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
mse:0.3107297122478485, mae:0.3530089855194092, rse:0.530424177646637, corr:[0.54359925 0.55580235 0.56208366 0.5635261  0.5639199  0.5650759
 0.5667643  0.5681737  0.56892174 0.56923634 0.56953055 0.5697574
 0.5697364  0.5692855  0.56848    0.5676663  0.56709635 0.566615
 0.5657837  0.56444573 0.56277406 0.5610527  0.559629   0.55872804
 0.5579288  0.5569166  0.5555403  0.5539741  0.55289906 0.55279106
 0.55362093 0.5549113  0.5559141  0.5563376  0.5562107  0.5561033
 0.55606925 0.5560064  0.5556615  0.5547939  0.5537139  0.55285054
 0.5525981  0.55298305 0.5534223  0.55347955 0.5531826  0.5527909
 0.55259466 0.55270016 0.5530414  0.5532946  0.55328053 0.5529123
 0.55248684 0.5523075  0.5524269  0.55258536 0.5526952  0.5525715
 0.5523431  0.55222905 0.5523553  0.55254173 0.5527596  0.55293506
 0.553      0.55297846 0.5529552  0.55286926 0.5526795  0.5523031
 0.5517515  0.55114067 0.5506657  0.55026037 0.54981863 0.5493348
 0.5488559  0.54843754 0.5481536  0.5479868  0.54792637 0.54774874
 0.54738975 0.5468399  0.546307   0.54591906 0.5457557  0.5458035
 0.5457432  0.5455371  0.5456771  0.5465753  0.5477566  0.5468678 ]
