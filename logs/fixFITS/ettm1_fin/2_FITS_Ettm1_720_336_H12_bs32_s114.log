Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7360640.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4210579
	speed: 0.1280s/iter; left time: 3335.4044s
	iters: 200, epoch: 1 | loss: 0.3166797
	speed: 0.1093s/iter; left time: 2836.0306s
	iters: 300, epoch: 1 | loss: 0.2874772
	speed: 0.1187s/iter; left time: 3068.8697s
	iters: 400, epoch: 1 | loss: 0.2436062
	speed: 0.1232s/iter; left time: 3171.2424s
	iters: 500, epoch: 1 | loss: 0.2104736
	speed: 0.1217s/iter; left time: 3121.2647s
Epoch: 1 cost time: 63.34251379966736
Epoch: 1, Steps: 523 | Train Loss: 0.3272412 Vali Loss: 0.7965745 Test Loss: 0.4531435
Validation loss decreased (inf --> 0.796574).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1849246
	speed: 0.9331s/iter; left time: 23820.0589s
	iters: 200, epoch: 2 | loss: 0.1645894
	speed: 0.1362s/iter; left time: 3464.3027s
	iters: 300, epoch: 2 | loss: 0.1618454
	speed: 0.1229s/iter; left time: 3111.9635s
	iters: 400, epoch: 2 | loss: 0.1709009
	speed: 0.0803s/iter; left time: 2026.9808s
	iters: 500, epoch: 2 | loss: 0.1370936
	speed: 0.0807s/iter; left time: 2028.6442s
Epoch: 2 cost time: 60.039140701293945
Epoch: 2, Steps: 523 | Train Loss: 0.1708099 Vali Loss: 0.7118330 Test Loss: 0.3939302
Validation loss decreased (0.796574 --> 0.711833).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.1457922
	speed: 0.7806s/iter; left time: 19517.7552s
	iters: 200, epoch: 3 | loss: 0.1335332
	speed: 0.1258s/iter; left time: 3132.6213s
	iters: 300, epoch: 3 | loss: 0.1269331
	speed: 0.1287s/iter; left time: 3193.1534s
	iters: 400, epoch: 3 | loss: 0.1174267
	speed: 0.1162s/iter; left time: 2871.4871s
	iters: 500, epoch: 3 | loss: 0.1481410
	speed: 0.1162s/iter; left time: 2857.9258s
Epoch: 3 cost time: 64.81390523910522
Epoch: 3, Steps: 523 | Train Loss: 0.1382454 Vali Loss: 0.6881377 Test Loss: 0.3758828
Validation loss decreased (0.711833 --> 0.688138).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.1256408
	speed: 0.6842s/iter; left time: 16751.4490s
	iters: 200, epoch: 4 | loss: 0.1622524
	speed: 0.0958s/iter; left time: 2336.6669s
	iters: 300, epoch: 4 | loss: 0.1183273
	speed: 0.1180s/iter; left time: 2865.9660s
	iters: 400, epoch: 4 | loss: 0.1234068
	speed: 0.1124s/iter; left time: 2719.2630s
	iters: 500, epoch: 4 | loss: 0.1194662
	speed: 0.1162s/iter; left time: 2798.7605s
Epoch: 4 cost time: 56.96501111984253
Epoch: 4, Steps: 523 | Train Loss: 0.1275511 Vali Loss: 0.6771924 Test Loss: 0.3687918
Validation loss decreased (0.688138 --> 0.677192).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1134961
	speed: 0.7990s/iter; left time: 19142.5552s
	iters: 200, epoch: 5 | loss: 0.1318377
	speed: 0.1234s/iter; left time: 2943.0807s
	iters: 300, epoch: 5 | loss: 0.1268689
	speed: 0.1222s/iter; left time: 2902.4624s
	iters: 400, epoch: 5 | loss: 0.1231985
	speed: 0.1281s/iter; left time: 3030.4457s
	iters: 500, epoch: 5 | loss: 0.1286512
	speed: 0.1211s/iter; left time: 2853.9883s
Epoch: 5 cost time: 65.57635855674744
Epoch: 5, Steps: 523 | Train Loss: 0.1236360 Vali Loss: 0.6748101 Test Loss: 0.3682609
Validation loss decreased (0.677192 --> 0.674810).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1180386
	speed: 0.7472s/iter; left time: 17511.0668s
	iters: 200, epoch: 6 | loss: 0.1161906
	speed: 0.0875s/iter; left time: 2041.3904s
	iters: 300, epoch: 6 | loss: 0.1177351
	speed: 0.0983s/iter; left time: 2283.9308s
	iters: 400, epoch: 6 | loss: 0.1306887
	speed: 0.0844s/iter; left time: 1952.0076s
	iters: 500, epoch: 6 | loss: 0.1122480
	speed: 0.1150s/iter; left time: 2648.1789s
Epoch: 6 cost time: 51.296024799346924
Epoch: 6, Steps: 523 | Train Loss: 0.1221376 Vali Loss: 0.6734057 Test Loss: 0.3684961
Validation loss decreased (0.674810 --> 0.673406).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1241641
	speed: 0.7095s/iter; left time: 16255.8167s
	iters: 200, epoch: 7 | loss: 0.1133282
	speed: 0.1177s/iter; left time: 2684.9967s
	iters: 300, epoch: 7 | loss: 0.1207403
	speed: 0.1089s/iter; left time: 2473.0168s
	iters: 400, epoch: 7 | loss: 0.1173370
	speed: 0.1065s/iter; left time: 2407.4643s
	iters: 500, epoch: 7 | loss: 0.1362792
	speed: 0.1050s/iter; left time: 2363.0278s
Epoch: 7 cost time: 58.53231143951416
Epoch: 7, Steps: 523 | Train Loss: 0.1215256 Vali Loss: 0.6740401 Test Loss: 0.3700629
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1206747
	speed: 0.7506s/iter; left time: 16806.1546s
	iters: 200, epoch: 8 | loss: 0.1308583
	speed: 0.1253s/iter; left time: 2792.2877s
	iters: 300, epoch: 8 | loss: 0.1360475
	speed: 0.1208s/iter; left time: 2680.4424s
	iters: 400, epoch: 8 | loss: 0.1222830
	speed: 0.1268s/iter; left time: 2800.2772s
	iters: 500, epoch: 8 | loss: 0.1284801
	speed: 0.1152s/iter; left time: 2533.5157s
Epoch: 8 cost time: 64.89962410926819
Epoch: 8, Steps: 523 | Train Loss: 0.1212936 Vali Loss: 0.6738612 Test Loss: 0.3703243
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1125643
	speed: 0.7935s/iter; left time: 17351.6716s
	iters: 200, epoch: 9 | loss: 0.1270841
	speed: 0.1250s/iter; left time: 2721.9563s
	iters: 300, epoch: 9 | loss: 0.1082906
	speed: 0.1244s/iter; left time: 2694.8009s
	iters: 400, epoch: 9 | loss: 0.1213812
	speed: 0.1255s/iter; left time: 2706.3398s
	iters: 500, epoch: 9 | loss: 0.1139509
	speed: 0.1270s/iter; left time: 2725.7620s
Epoch: 9 cost time: 66.56022953987122
Epoch: 9, Steps: 523 | Train Loss: 0.1212057 Vali Loss: 0.6721134 Test Loss: 0.3713666
Validation loss decreased (0.673406 --> 0.672113).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1155480
	speed: 0.6887s/iter; left time: 14698.7458s
	iters: 200, epoch: 10 | loss: 0.1470172
	speed: 0.1053s/iter; left time: 2237.3110s
	iters: 300, epoch: 10 | loss: 0.1280447
	speed: 0.1019s/iter; left time: 2154.2067s
	iters: 400, epoch: 10 | loss: 0.1008410
	speed: 0.0986s/iter; left time: 2075.4319s
	iters: 500, epoch: 10 | loss: 0.1239135
	speed: 0.1137s/iter; left time: 2381.9348s
Epoch: 10 cost time: 57.1447970867157
Epoch: 10, Steps: 523 | Train Loss: 0.1211496 Vali Loss: 0.6723809 Test Loss: 0.3711270
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1348774
	speed: 0.7161s/iter; left time: 14909.6656s
	iters: 200, epoch: 11 | loss: 0.1226863
	speed: 0.1032s/iter; left time: 2138.7824s
	iters: 300, epoch: 11 | loss: 0.1280386
	speed: 0.1051s/iter; left time: 2167.4650s
	iters: 400, epoch: 11 | loss: 0.1231260
	speed: 0.1128s/iter; left time: 2315.6247s
	iters: 500, epoch: 11 | loss: 0.1127355
	speed: 0.1057s/iter; left time: 2158.4257s
Epoch: 11 cost time: 55.96864628791809
Epoch: 11, Steps: 523 | Train Loss: 0.1211468 Vali Loss: 0.6732907 Test Loss: 0.3715979
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.0971801
	speed: 0.7374s/iter; left time: 14968.0244s
	iters: 200, epoch: 12 | loss: 0.1064958
	speed: 0.1037s/iter; left time: 2094.0894s
	iters: 300, epoch: 12 | loss: 0.1191912
	speed: 0.1056s/iter; left time: 2122.3408s
	iters: 400, epoch: 12 | loss: 0.1187477
	speed: 0.1052s/iter; left time: 2103.5833s
	iters: 500, epoch: 12 | loss: 0.1276365
	speed: 0.1117s/iter; left time: 2222.0072s
Epoch: 12 cost time: 56.80106592178345
Epoch: 12, Steps: 523 | Train Loss: 0.1211286 Vali Loss: 0.6713983 Test Loss: 0.3720643
Validation loss decreased (0.672113 --> 0.671398).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1120902
	speed: 0.8701s/iter; left time: 17207.1712s
	iters: 200, epoch: 13 | loss: 0.1130038
	speed: 0.1237s/iter; left time: 2434.7008s
	iters: 300, epoch: 13 | loss: 0.1164687
	speed: 0.1262s/iter; left time: 2470.8229s
	iters: 400, epoch: 13 | loss: 0.1113070
	speed: 0.1188s/iter; left time: 2313.2262s
	iters: 500, epoch: 13 | loss: 0.1224439
	speed: 0.1222s/iter; left time: 2368.3286s
Epoch: 13 cost time: 66.50822591781616
Epoch: 13, Steps: 523 | Train Loss: 0.1211300 Vali Loss: 0.6736887 Test Loss: 0.3708771
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1275911
	speed: 0.8556s/iter; left time: 16471.6152s
	iters: 200, epoch: 14 | loss: 0.1201857
	speed: 0.1359s/iter; left time: 2603.3224s
	iters: 300, epoch: 14 | loss: 0.1205862
	speed: 0.1320s/iter; left time: 2515.1314s
	iters: 400, epoch: 14 | loss: 0.1297228
	speed: 0.1358s/iter; left time: 2573.7738s
	iters: 500, epoch: 14 | loss: 0.1215555
	speed: 0.1155s/iter; left time: 2177.9670s
Epoch: 14 cost time: 68.83826994895935
Epoch: 14, Steps: 523 | Train Loss: 0.1211151 Vali Loss: 0.6741871 Test Loss: 0.3714355
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1038792
	speed: 0.7976s/iter; left time: 14938.6003s
	iters: 200, epoch: 15 | loss: 0.1263980
	speed: 0.1208s/iter; left time: 2250.5713s
	iters: 300, epoch: 15 | loss: 0.1436947
	speed: 0.1150s/iter; left time: 2130.8562s
	iters: 400, epoch: 15 | loss: 0.1235075
	speed: 0.1166s/iter; left time: 2149.7210s
	iters: 500, epoch: 15 | loss: 0.1084914
	speed: 0.1068s/iter; left time: 1957.6395s
Epoch: 15 cost time: 61.21766757965088
Epoch: 15, Steps: 523 | Train Loss: 0.1210834 Vali Loss: 0.6729641 Test Loss: 0.3712707
EarlyStopping counter: 3 out of 3
Early stopping
train 33505
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=106, out_features=155, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7360640.0
params:  16585.0
Trainable parameters:  16585
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3218652
	speed: 0.1386s/iter; left time: 3610.6445s
	iters: 200, epoch: 1 | loss: 0.3301395
	speed: 0.1389s/iter; left time: 3604.7813s
	iters: 300, epoch: 1 | loss: 0.3706914
	speed: 0.1436s/iter; left time: 3713.4576s
	iters: 400, epoch: 1 | loss: 0.3482875
	speed: 0.1494s/iter; left time: 3847.2938s
	iters: 500, epoch: 1 | loss: 0.3110647
	speed: 0.1370s/iter; left time: 3515.0881s
Epoch: 1 cost time: 74.08838415145874
Epoch: 1, Steps: 523 | Train Loss: 0.3391001 Vali Loss: 0.6577449 Test Loss: 0.3683147
Validation loss decreased (inf --> 0.657745).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3413877
	speed: 0.8245s/iter; left time: 21047.8069s
	iters: 200, epoch: 2 | loss: 0.3004625
	speed: 0.1051s/iter; left time: 2673.1904s
	iters: 300, epoch: 2 | loss: 0.3588482
	speed: 0.1047s/iter; left time: 2650.5891s
	iters: 400, epoch: 2 | loss: 0.3155154
	speed: 0.1149s/iter; left time: 2899.6154s
	iters: 500, epoch: 2 | loss: 0.3671333
	speed: 0.1213s/iter; left time: 3048.6053s
Epoch: 2 cost time: 58.62376570701599
Epoch: 2, Steps: 523 | Train Loss: 0.3378471 Vali Loss: 0.6582405 Test Loss: 0.3677439
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2873647
	speed: 0.7873s/iter; left time: 19686.8719s
	iters: 200, epoch: 3 | loss: 0.3111675
	speed: 0.1081s/iter; left time: 2691.1193s
	iters: 300, epoch: 3 | loss: 0.3358565
	speed: 0.1190s/iter; left time: 2952.3186s
	iters: 400, epoch: 3 | loss: 0.3609773
	speed: 0.1181s/iter; left time: 2918.1900s
	iters: 500, epoch: 3 | loss: 0.3154306
	speed: 0.1082s/iter; left time: 2663.2996s
Epoch: 3 cost time: 60.7603178024292
Epoch: 3, Steps: 523 | Train Loss: 0.3372901 Vali Loss: 0.6553005 Test Loss: 0.3668502
Validation loss decreased (0.657745 --> 0.655300).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3060776
	speed: 0.7191s/iter; left time: 17605.1706s
	iters: 200, epoch: 4 | loss: 0.2830999
	speed: 0.1088s/iter; left time: 2651.7975s
	iters: 300, epoch: 4 | loss: 0.3266465
	speed: 0.1243s/iter; left time: 3017.8738s
	iters: 400, epoch: 4 | loss: 0.3272958
	speed: 0.1320s/iter; left time: 3191.5536s
	iters: 500, epoch: 4 | loss: 0.3973509
	speed: 0.1305s/iter; left time: 3142.5086s
Epoch: 4 cost time: 64.51120471954346
Epoch: 4, Steps: 523 | Train Loss: 0.3371181 Vali Loss: 0.6563898 Test Loss: 0.3663522
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3425289
	speed: 0.8970s/iter; left time: 21491.7623s
	iters: 200, epoch: 5 | loss: 0.3155906
	speed: 0.1282s/iter; left time: 3058.8512s
	iters: 300, epoch: 5 | loss: 0.3652126
	speed: 0.1277s/iter; left time: 3034.4356s
	iters: 400, epoch: 5 | loss: 0.3016234
	speed: 0.1079s/iter; left time: 2553.8431s
	iters: 500, epoch: 5 | loss: 0.3081030
	speed: 0.1016s/iter; left time: 2394.5212s
Epoch: 5 cost time: 63.06995153427124
Epoch: 5, Steps: 523 | Train Loss: 0.3369987 Vali Loss: 0.6561950 Test Loss: 0.3667171
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4029466
	speed: 0.7019s/iter; left time: 16450.7959s
	iters: 200, epoch: 6 | loss: 0.2714057
	speed: 0.1059s/iter; left time: 2470.9857s
	iters: 300, epoch: 6 | loss: 0.3086263
	speed: 0.1107s/iter; left time: 2572.8149s
	iters: 400, epoch: 6 | loss: 0.3088028
	speed: 0.0735s/iter; left time: 1699.8580s
	iters: 500, epoch: 6 | loss: 0.3224832
	speed: 0.0856s/iter; left time: 1972.9703s
Epoch: 6 cost time: 51.55423283576965
Epoch: 6, Steps: 523 | Train Loss: 0.3368230 Vali Loss: 0.6576042 Test Loss: 0.3666306
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_336_FITS_ETTm1_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3667696416378021, mae:0.38556036353111267, rse:0.5762951970100403, corr:[0.5387635  0.54899156 0.55482644 0.5562793  0.55637705 0.55711216
 0.55855143 0.5599046  0.56053954 0.5605082  0.5603873  0.56047183
 0.5606693  0.56054616 0.559786   0.5585523  0.5573135  0.5563821
 0.5556196  0.5547301  0.553501   0.55186665 0.5500566  0.54855263
 0.5474518  0.54674065 0.5461439  0.5453785  0.5445809  0.5440651
 0.5441541  0.5449433  0.54600686 0.54692525 0.5473026  0.54728025
 0.54693663 0.546509   0.54610866 0.5456385  0.54517007 0.5447067
 0.5443649  0.5443017  0.5444433  0.5446695  0.5449179  0.54506487
 0.5450242  0.54479355 0.5445187  0.54422855 0.5439783  0.5437555
 0.5437265  0.54398674 0.5444473  0.5448077  0.544857   0.54442555
 0.5436613  0.5429317  0.542548   0.54254824 0.54287905 0.5433039
 0.54359895 0.54372704 0.543808   0.54390085 0.54396856 0.5438261
 0.5433176  0.54250365 0.5416801  0.5411098  0.54092145 0.5410763
 0.5413407  0.5414814  0.54138815 0.54111546 0.5408535  0.5407236
 0.540786   0.540891   0.54087824 0.5406266  0.5402472  0.5400312
 0.5401745  0.5406867  0.5413756  0.5419516  0.54216456 0.54193974
 0.5414429  0.54096067 0.54053426 0.54021996 0.5399641  0.53968716
 0.5393649  0.5389765  0.5386576  0.5385046  0.53840697 0.53832495
 0.5381442  0.53790563 0.53769505 0.5375741  0.537576   0.5376667
 0.53771055 0.53759164 0.5372581  0.53674054 0.5361842  0.5357966
 0.5356562  0.5357292  0.535878   0.5360464  0.5361253  0.5359749
 0.5355829  0.5350881  0.53462857 0.53439355 0.5343878  0.5345441
 0.534761   0.5348554  0.53469396 0.53438485 0.53419495 0.53419304
 0.53447366 0.53496116 0.5353876  0.5356203  0.5356252  0.5354871
 0.535305   0.5350658  0.53482753 0.53461754 0.5345486  0.53461283
 0.5347862  0.53504604 0.5352238  0.53519857 0.53495187 0.53467447
 0.53452736 0.53460884 0.5349998  0.53550375 0.5359986  0.5363018
 0.5363555  0.53619283 0.5359867  0.53589594 0.53599155 0.53622735
 0.5364237  0.53641814 0.5362619  0.5360311  0.53577036 0.5355764
 0.53553766 0.5356753  0.5358826  0.5360106  0.5359386  0.53572553
 0.5354721  0.53531235 0.53529584 0.5355015  0.53588116 0.5363089
 0.53670347 0.5370727  0.5374169  0.5376916  0.5378801  0.53798574
 0.5379577  0.5378189  0.5374493  0.5367983  0.5359122  0.534914
 0.53388727 0.5329531  0.5321827  0.53160185 0.5311188  0.5306348
 0.5301216  0.5296067  0.529168   0.5289077  0.52879506 0.52868825
 0.52838117 0.5277863  0.52690804 0.5258984  0.524911   0.5242388
 0.5240124  0.52406925 0.5242353  0.52428293 0.5241699  0.5240237
 0.5240243  0.5241274  0.5243415  0.5245764  0.5246732  0.5245919
 0.52437824 0.52411497 0.52385956 0.523613   0.5234323  0.5233698
 0.5233966  0.52353305 0.52362496 0.52378887 0.5239413  0.5240977
 0.52419704 0.524148   0.5240512  0.5240304  0.5241776  0.52439797
 0.52458626 0.52466786 0.52458555 0.5244326  0.524224   0.5240853
 0.52396756 0.52395177 0.5239417  0.5238568  0.5237081  0.52355313
 0.52345306 0.5234578  0.52351004 0.52353597 0.52350134 0.52350736
 0.5235184  0.5235203  0.5235007  0.5234581  0.5233281  0.5230393
 0.52271044 0.5224254  0.5222765  0.5223144  0.52248836 0.52275604
 0.522988   0.5231921  0.5232398  0.52322084 0.52315164 0.5231112
 0.5232025  0.5234012  0.523559   0.5235755  0.5233557  0.5228515
 0.52222687 0.52176106 0.521501   0.5212339  0.52079624 0.52007234
 0.5190915  0.5179406  0.5169465  0.5162586  0.51589936 0.5157556
 0.5156082  0.5153361  0.5149564  0.5145342  0.51416135 0.51385766
 0.51373804 0.5136585  0.51353174 0.51332563 0.513157   0.5130536
 0.5130791  0.51328945 0.5135245  0.51368517 0.513711   0.51367134
 0.5136492  0.5136592  0.51373863 0.5138511  0.513931   0.51390874
 0.5137281  0.5135251  0.5133594  0.51328164 0.5131323  0.51272726
 0.5121672  0.51178247 0.5120585  0.51287454 0.5126502  0.5077427 ]
