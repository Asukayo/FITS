Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  29030400.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4922891
	speed: 0.1224s/iter; left time: 777.1693s
Epoch: 1 cost time: 15.635477542877197
Epoch: 1, Steps: 129 | Train Loss: 0.5827472 Vali Loss: 1.1219116 Test Loss: 0.5046539
Validation loss decreased (inf --> 1.121912).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4534492
	speed: 0.3222s/iter; left time: 2004.4532s
Epoch: 2 cost time: 15.608247756958008
Epoch: 2, Steps: 129 | Train Loss: 0.4429049 Vali Loss: 1.0178585 Test Loss: 0.4450601
Validation loss decreased (1.121912 --> 1.017859).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4141940
	speed: 0.3218s/iter; left time: 1960.7264s
Epoch: 3 cost time: 15.54194974899292
Epoch: 3, Steps: 129 | Train Loss: 0.4189537 Vali Loss: 0.9837326 Test Loss: 0.4280489
Validation loss decreased (1.017859 --> 0.983733).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4034825
	speed: 0.3191s/iter; left time: 1903.3213s
Epoch: 4 cost time: 16.33050799369812
Epoch: 4, Steps: 129 | Train Loss: 0.4093286 Vali Loss: 0.9668061 Test Loss: 0.4204400
Validation loss decreased (0.983733 --> 0.966806).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4143868
	speed: 0.3216s/iter; left time: 1876.6439s
Epoch: 5 cost time: 15.651325225830078
Epoch: 5, Steps: 129 | Train Loss: 0.4044258 Vali Loss: 0.9554506 Test Loss: 0.4169822
Validation loss decreased (0.966806 --> 0.955451).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4163035
	speed: 0.3329s/iter; left time: 1899.5458s
Epoch: 6 cost time: 16.406951427459717
Epoch: 6, Steps: 129 | Train Loss: 0.4016352 Vali Loss: 0.9510260 Test Loss: 0.4159561
Validation loss decreased (0.955451 --> 0.951026).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4212413
	speed: 0.3299s/iter; left time: 1840.1233s
Epoch: 7 cost time: 16.073863983154297
Epoch: 7, Steps: 129 | Train Loss: 0.4003426 Vali Loss: 0.9466494 Test Loss: 0.4157026
Validation loss decreased (0.951026 --> 0.946649).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3970316
	speed: 0.3298s/iter; left time: 1796.7523s
Epoch: 8 cost time: 16.387892484664917
Epoch: 8, Steps: 129 | Train Loss: 0.3993131 Vali Loss: 0.9441345 Test Loss: 0.4160020
Validation loss decreased (0.946649 --> 0.944134).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4121947
	speed: 0.3508s/iter; left time: 1865.8649s
Epoch: 9 cost time: 15.640921592712402
Epoch: 9, Steps: 129 | Train Loss: 0.3986651 Vali Loss: 0.9415768 Test Loss: 0.4162439
Validation loss decreased (0.944134 --> 0.941577).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3891677
	speed: 0.3448s/iter; left time: 1789.3209s
Epoch: 10 cost time: 15.943228006362915
Epoch: 10, Steps: 129 | Train Loss: 0.3982244 Vali Loss: 0.9401557 Test Loss: 0.4166488
Validation loss decreased (0.941577 --> 0.940156).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3867313
	speed: 0.3414s/iter; left time: 1727.6112s
Epoch: 11 cost time: 17.66369867324829
Epoch: 11, Steps: 129 | Train Loss: 0.3980574 Vali Loss: 0.9397014 Test Loss: 0.4167086
Validation loss decreased (0.940156 --> 0.939701).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3973207
	speed: 0.3701s/iter; left time: 1825.4452s
Epoch: 12 cost time: 19.5986909866333
Epoch: 12, Steps: 129 | Train Loss: 0.3977641 Vali Loss: 0.9375314 Test Loss: 0.4168898
Validation loss decreased (0.939701 --> 0.937531).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3764542
	speed: 0.3912s/iter; left time: 1878.8829s
Epoch: 13 cost time: 18.71001386642456
Epoch: 13, Steps: 129 | Train Loss: 0.3976683 Vali Loss: 0.9371284 Test Loss: 0.4170435
Validation loss decreased (0.937531 --> 0.937128).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3961841
	speed: 0.3843s/iter; left time: 1796.2537s
Epoch: 14 cost time: 18.203944444656372
Epoch: 14, Steps: 129 | Train Loss: 0.3976229 Vali Loss: 0.9362445 Test Loss: 0.4172463
Validation loss decreased (0.937128 --> 0.936244).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3826411
	speed: 0.3774s/iter; left time: 1715.2477s
Epoch: 15 cost time: 18.866446495056152
Epoch: 15, Steps: 129 | Train Loss: 0.3976001 Vali Loss: 0.9345479 Test Loss: 0.4173547
Validation loss decreased (0.936244 --> 0.934548).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3716509
	speed: 0.3571s/iter; left time: 1577.1722s
Epoch: 16 cost time: 17.443297863006592
Epoch: 16, Steps: 129 | Train Loss: 0.3974858 Vali Loss: 0.9353903 Test Loss: 0.4175718
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4195289
	speed: 0.3203s/iter; left time: 1372.9335s
Epoch: 17 cost time: 15.95695161819458
Epoch: 17, Steps: 129 | Train Loss: 0.3974446 Vali Loss: 0.9355391 Test Loss: 0.4176745
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3985546
	speed: 0.3237s/iter; left time: 1345.9280s
Epoch: 18 cost time: 15.956118822097778
Epoch: 18, Steps: 129 | Train Loss: 0.3974107 Vali Loss: 0.9353241 Test Loss: 0.4177054
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4147503674030304, mae:0.4113670885562897, rse:0.6127229332923889, corr:[0.52317923 0.52997875 0.5351334  0.5373798  0.5377915  0.5380115
 0.5385595  0.5392768  0.53995126 0.5404359  0.5408123  0.54112893
 0.5414575  0.5416065  0.5412528  0.5402416  0.5387553  0.5371768
 0.5358265  0.5348976  0.53434324 0.5338322  0.5330091  0.5318119
 0.53020495 0.52863735 0.5275806  0.5272455  0.5276657  0.5285185
 0.5293656  0.5299377  0.53004587 0.52990943 0.5297508  0.5299181
 0.5302713  0.5306283  0.53074473 0.53039426 0.5297019  0.5288857
 0.5283015  0.52821904 0.52849615 0.5288382  0.52904326 0.52895653
 0.5285584  0.52799857 0.527598   0.52747905 0.5276041  0.5277084
 0.52764827 0.52737206 0.5269589  0.5265319  0.52628976 0.5262237
 0.5262851  0.52638465 0.5264425  0.5263986  0.5263954  0.52654636
 0.5268508  0.5272155  0.52751225 0.52758026 0.52743757 0.52715266
 0.52684575 0.5266514  0.5266689  0.5268238  0.5269644  0.5269782
 0.52681905 0.5265417  0.52623653 0.52599496 0.52588886 0.52589697
 0.5260187  0.52613485 0.5262034  0.5261887  0.5261432  0.5261721
 0.5263103  0.52651685 0.5267295  0.52685094 0.52685    0.5266748
 0.5263914  0.5261015  0.5257478  0.52544683 0.525209   0.5250382
 0.524935   0.5248568  0.5248016  0.52479553 0.52473134 0.5246094
 0.5243882  0.5241237  0.5238535  0.5235987  0.52337927 0.523206
 0.5230445  0.52287114 0.5226528  0.52236897 0.5220499  0.52179605
 0.5216372  0.52156377 0.5214989  0.52143866 0.52132744 0.52107453
 0.5206695  0.52021885 0.51981103 0.51956254 0.519455   0.5194478
 0.5195068  0.51955867 0.5195101  0.5194034  0.5193568  0.5193652
 0.5194886  0.51972735 0.5199528  0.5201004  0.52014196 0.5201039
 0.5200311  0.51994973 0.5199391  0.5200055  0.52014095 0.52025694
 0.5202832  0.52024716 0.52015734 0.52005154 0.5199532  0.51994514
 0.51998    0.520021   0.52008015 0.52012056 0.5202285  0.5204205
 0.52067816 0.52093655 0.5211365  0.5212342  0.5212503  0.5212551
 0.52125335 0.521244   0.52128965 0.5213727  0.5214139  0.52137697
 0.52127093 0.5211185  0.5209748  0.52088004 0.5208507  0.52090687
 0.5210342  0.52121794 0.5213909  0.52156126 0.5217148  0.5218452
 0.52199095 0.5221699  0.5223575  0.5224997  0.5225459  0.5224575
 0.52222574 0.5218984  0.5214369  0.5208457  0.5201687  0.5194774
 0.5187868  0.51811767 0.51743317 0.51674896 0.5160522  0.51535654
 0.51467985 0.51404184 0.5134484  0.5129251  0.5124399  0.5119452
 0.51138693 0.5107746  0.5101321  0.5094901  0.5088427  0.50828004
 0.50786537 0.5075705  0.50739855 0.5072877  0.50720596 0.5071578
 0.507185   0.5071889  0.5071841  0.507208   0.5072406  0.507305
 0.5074096  0.507542   0.50764716 0.50764364 0.5075394  0.5073889
 0.50727165 0.5072773  0.507374   0.5076317  0.5079532  0.5083007
 0.50856936 0.5086533  0.50858575 0.5084534  0.5083288  0.5082302
 0.5081754  0.50816244 0.5081604  0.5081516  0.5080902  0.50801176
 0.5078705  0.50773686 0.50762206 0.5075596  0.50758994 0.50772023
 0.5079195  0.50815344 0.50836384 0.50851405 0.5086007  0.5087128
 0.5087911  0.5088642  0.5089239  0.50898165 0.5090066  0.50895447
 0.50884545 0.50868195 0.5084991  0.50831914 0.50815773 0.5080436
 0.50796133 0.50799024 0.508064   0.50821215 0.5083621  0.50846106
 0.50853425 0.5085248  0.5084292  0.50825864 0.50798696 0.5075998
 0.50710946 0.5065781  0.5060131  0.50539327 0.50474894 0.5041419
 0.5036251  0.5031814  0.50283414 0.50250643 0.5021282  0.5017043
 0.5012402  0.5008105  0.5005015  0.5003226  0.50022835 0.50012875
 0.500033   0.49984375 0.49955574 0.4992216  0.4989578  0.49879828
 0.4987614  0.4988511  0.49895906 0.49901867 0.49898773 0.49890354
 0.49878588 0.49865687 0.49856487 0.49852592 0.4985134  0.49847052
 0.4983604  0.49819568 0.49797085 0.4977475  0.49756    0.49744433
 0.4974661  0.49757066 0.49767616 0.49774554 0.4977457  0.4977544
 0.49775076 0.49765444 0.49756628 0.49750525 0.49748453 0.49747798
 0.49743748 0.497377   0.4973058  0.49726126 0.49725804 0.49732494
 0.49738392 0.49744144 0.49746117 0.49742207 0.49736887 0.49731773
 0.49727297 0.49725574 0.49724668 0.49723142 0.4972292  0.49724922
 0.4972174  0.49717933 0.49710414 0.49700174 0.4968749  0.49672726
 0.49654013 0.49639198 0.49632567 0.49632773 0.4964217  0.49658558
 0.4967802  0.49698034 0.49715766 0.49732378 0.49747494 0.49764568
 0.49785364 0.4980866  0.49827522 0.49835894 0.49828777 0.49807033
 0.49772266 0.4972953  0.49687812 0.49649817 0.4961385  0.49588126
 0.49567112 0.49545658 0.4951981  0.49490103 0.49452817 0.4941238
 0.49373376 0.49339947 0.49314246 0.4929433  0.49271426 0.49247107
 0.49215794 0.4918057  0.49136952 0.49092293 0.49050316 0.4901776
 0.48995122 0.4898339  0.48980346 0.48985168 0.4899587  0.49005327
 0.49021965 0.49034575 0.4904786  0.4905444  0.49058655 0.4906599
 0.49071252 0.49077758 0.49079955 0.49078116 0.49070486 0.49065235
 0.49063045 0.49066737 0.49072567 0.49086088 0.49100485 0.4912084
 0.49136564 0.49139342 0.49132478 0.49122223 0.49108797 0.4909434
 0.4907864  0.49063724 0.49053037 0.49048388 0.49051258 0.4906104
 0.49070102 0.49076372 0.49074098 0.49064595 0.49052146 0.49040952
 0.49039257 0.49046576 0.4905909  0.49069914 0.4907412  0.4907553
 0.49065757 0.49051386 0.4903617  0.49027324 0.49020514 0.4901965
 0.4902238  0.49030393 0.49038458 0.4904719  0.49055237 0.49060923
 0.49064597 0.49063417 0.4905874  0.49052376 0.4904605  0.49041653
 0.49037614 0.49036476 0.4903379  0.49021918 0.48996094 0.4895696
 0.48906866 0.48851588 0.4879604  0.48739338 0.48680237 0.48622644
 0.48571405 0.48519313 0.48467985 0.48417944 0.48362812 0.4830105
 0.4823496  0.48171902 0.4811417  0.4806012  0.4801073  0.47969794
 0.47935474 0.47909296 0.47887668 0.47858068 0.4783087  0.47805822
 0.47789118 0.47781426 0.47779018 0.47784808 0.4779433  0.47808933
 0.47827575 0.47842586 0.47852358 0.47864845 0.47878203 0.47891214
 0.47900397 0.479112   0.47924885 0.47934595 0.47939113 0.4794413
 0.47952017 0.4796392  0.4798004  0.47995052 0.4801379  0.4803968
 0.48064235 0.48078606 0.48077694 0.48069885 0.48060974 0.48052225
 0.4804708  0.48046038 0.48049486 0.4805682  0.48064294 0.48068213
 0.4806661  0.48058414 0.48046997 0.48035613 0.48027438 0.4802722
 0.48031822 0.4803791  0.48041257 0.48036033 0.48024505 0.48018423
 0.48013952 0.48013324 0.48015535 0.4801763  0.48011965 0.47998554
 0.4797894  0.47961384 0.47953144 0.47959656 0.47978887 0.4800268
 0.48020315 0.4803372  0.480342   0.480285   0.48023364 0.4802531
 0.480368   0.48054236 0.48073298 0.4808152  0.48069942 0.48033988
 0.47979277 0.47918802 0.47859892 0.47804868 0.47754017 0.47715244
 0.47683924 0.47657007 0.47626674 0.47587985 0.4754026  0.47480884
 0.47417995 0.47355682 0.4729799  0.4724564  0.4720239  0.47165397
 0.47131872 0.4709804  0.47066352 0.47038358 0.4701423  0.46995464
 0.46986634 0.4698339  0.4698544  0.4698905  0.46992728 0.46990722
 0.4699173  0.46994758 0.47003043 0.4701801  0.47037178 0.47060263
 0.47081524 0.47091255 0.47093317 0.47089416 0.47086054 0.4708844
 0.4710386  0.47134495 0.4716747  0.47199214 0.47235382 0.47271565
 0.4729533  0.4730785  0.47309053 0.472991   0.4728805  0.47276706
 0.47256395 0.4723278  0.47209743 0.4718647  0.4717092  0.4715973
 0.47147164 0.47135973 0.47128505 0.47115648 0.4710357  0.47088885
 0.4707273  0.47055224 0.47039297 0.4702736  0.47018918 0.47019008
 0.47022527 0.4702079  0.47017494 0.4701232  0.47003168 0.46989763
 0.4697623  0.46968248 0.46969974 0.46981445 0.46996528 0.47005805
 0.4700436  0.46999666 0.4699537  0.4699101  0.46998653 0.47018474
 0.47047943 0.4707908  0.4710352  0.47116396 0.4710754  0.47075263
 0.47027823 0.46983603 0.46949106 0.46919045 0.46888584 0.4685725
 0.46827197 0.46793997 0.46765217 0.4673653  0.46694055 0.46653265
 0.46604216 0.46564052 0.46533212 0.46521398 0.4651974  0.46513313
 0.46493927 0.46455497 0.4640852  0.4635817  0.46326    0.4632317
 0.46340546 0.46370235 0.46387857 0.4638047  0.46359676 0.46333128
 0.46330076 0.46357754 0.464168   0.46499696 0.46567565 0.46605632
 0.46612698 0.46609324 0.46619758 0.46656284 0.46729055 0.46815702
 0.46904418 0.46977356 0.47004324 0.4697637  0.46798998 0.46262735]
