Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20134912.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4775875
	speed: 0.1423s/iter; left time: 1821.9451s
	iters: 200, epoch: 1 | loss: 0.3946110
	speed: 0.1337s/iter; left time: 1698.1478s
Epoch: 1 cost time: 35.86729145050049
Epoch: 1, Steps: 258 | Train Loss: 0.4944156 Vali Loss: 1.1861022 Test Loss: 0.5847777
Validation loss decreased (inf --> 1.186102).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3151536
	speed: 0.5854s/iter; left time: 7343.1536s
	iters: 200, epoch: 2 | loss: 0.3074704
	speed: 0.1263s/iter; left time: 1572.0230s
Epoch: 2 cost time: 34.892902851104736
Epoch: 2, Steps: 258 | Train Loss: 0.3199942 Vali Loss: 1.0645040 Test Loss: 0.5029512
Validation loss decreased (1.186102 --> 1.064504).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2632964
	speed: 0.5629s/iter; left time: 6915.2522s
	iters: 200, epoch: 3 | loss: 0.2762060
	speed: 0.1308s/iter; left time: 1594.2721s
Epoch: 3 cost time: 33.91114091873169
Epoch: 3, Steps: 258 | Train Loss: 0.2696378 Vali Loss: 1.0140002 Test Loss: 0.4687172
Validation loss decreased (1.064504 --> 1.014000).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2420076
	speed: 0.5541s/iter; left time: 6663.7381s
	iters: 200, epoch: 4 | loss: 0.2318859
	speed: 0.1236s/iter; left time: 1474.4721s
Epoch: 4 cost time: 33.092921018600464
Epoch: 4, Steps: 258 | Train Loss: 0.2459524 Vali Loss: 0.9869374 Test Loss: 0.4508940
Validation loss decreased (1.014000 --> 0.986937).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2143624
	speed: 0.6255s/iter; left time: 7361.6434s
	iters: 200, epoch: 5 | loss: 0.2164769
	speed: 0.1458s/iter; left time: 1701.6015s
Epoch: 5 cost time: 39.85282778739929
Epoch: 5, Steps: 258 | Train Loss: 0.2328594 Vali Loss: 0.9713478 Test Loss: 0.4386677
Validation loss decreased (0.986937 --> 0.971348).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2396297
	speed: 0.6587s/iter; left time: 7581.9463s
	iters: 200, epoch: 6 | loss: 0.2075291
	speed: 0.1564s/iter; left time: 1784.1481s
Epoch: 6 cost time: 40.94832420349121
Epoch: 6, Steps: 258 | Train Loss: 0.2248978 Vali Loss: 0.9588810 Test Loss: 0.4306059
Validation loss decreased (0.971348 --> 0.958881).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2031832
	speed: 0.6717s/iter; left time: 7558.9387s
	iters: 200, epoch: 7 | loss: 0.2275121
	speed: 0.1318s/iter; left time: 1469.9634s
Epoch: 7 cost time: 36.23238968849182
Epoch: 7, Steps: 258 | Train Loss: 0.2197396 Vali Loss: 0.9519437 Test Loss: 0.4252274
Validation loss decreased (0.958881 --> 0.951944).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2313638
	speed: 0.5950s/iter; left time: 6542.0540s
	iters: 200, epoch: 8 | loss: 0.2239211
	speed: 0.1253s/iter; left time: 1364.7116s
Epoch: 8 cost time: 32.43718361854553
Epoch: 8, Steps: 258 | Train Loss: 0.2162959 Vali Loss: 0.9471354 Test Loss: 0.4212267
Validation loss decreased (0.951944 --> 0.947135).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2191861
	speed: 0.5381s/iter; left time: 5777.9726s
	iters: 200, epoch: 9 | loss: 0.1995327
	speed: 0.1325s/iter; left time: 1409.5870s
Epoch: 9 cost time: 33.69908809661865
Epoch: 9, Steps: 258 | Train Loss: 0.2139348 Vali Loss: 0.9430757 Test Loss: 0.4189470
Validation loss decreased (0.947135 --> 0.943076).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2061763
	speed: 0.5510s/iter; left time: 5773.7660s
	iters: 200, epoch: 10 | loss: 0.2049591
	speed: 0.1237s/iter; left time: 1283.7729s
Epoch: 10 cost time: 32.81376385688782
Epoch: 10, Steps: 258 | Train Loss: 0.2123628 Vali Loss: 0.9425359 Test Loss: 0.4173717
Validation loss decreased (0.943076 --> 0.942536).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2070379
	speed: 0.5260s/iter; left time: 5376.4194s
	iters: 200, epoch: 11 | loss: 0.2059679
	speed: 0.1323s/iter; left time: 1338.8088s
Epoch: 11 cost time: 32.22968578338623
Epoch: 11, Steps: 258 | Train Loss: 0.2112703 Vali Loss: 0.9407607 Test Loss: 0.4164060
Validation loss decreased (0.942536 --> 0.940761).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1973599
	speed: 0.6366s/iter; left time: 6342.6686s
	iters: 200, epoch: 12 | loss: 0.2106906
	speed: 0.1433s/iter; left time: 1413.3899s
Epoch: 12 cost time: 37.260934829711914
Epoch: 12, Steps: 258 | Train Loss: 0.2104412 Vali Loss: 0.9400086 Test Loss: 0.4162779
Validation loss decreased (0.940761 --> 0.940009).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2025477
	speed: 0.6021s/iter; left time: 5843.1327s
	iters: 200, epoch: 13 | loss: 0.1965127
	speed: 0.1382s/iter; left time: 1327.5195s
Epoch: 13 cost time: 35.87367796897888
Epoch: 13, Steps: 258 | Train Loss: 0.2098400 Vali Loss: 0.9394041 Test Loss: 0.4159896
Validation loss decreased (0.940009 --> 0.939404).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2219154
	speed: 0.5316s/iter; left time: 5021.7331s
	iters: 200, epoch: 14 | loss: 0.2030340
	speed: 0.1224s/iter; left time: 1144.0447s
Epoch: 14 cost time: 31.203567504882812
Epoch: 14, Steps: 258 | Train Loss: 0.2094781 Vali Loss: 0.9389804 Test Loss: 0.4160752
Validation loss decreased (0.939404 --> 0.938980).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1999514
	speed: 0.5331s/iter; left time: 4898.6100s
	iters: 200, epoch: 15 | loss: 0.2136512
	speed: 0.1142s/iter; left time: 1038.3431s
Epoch: 15 cost time: 31.578548431396484
Epoch: 15, Steps: 258 | Train Loss: 0.2091917 Vali Loss: 0.9390821 Test Loss: 0.4162151
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2050111
	speed: 0.5197s/iter; left time: 4641.1749s
	iters: 200, epoch: 16 | loss: 0.2149524
	speed: 0.1284s/iter; left time: 1133.7150s
Epoch: 16 cost time: 31.45027995109558
Epoch: 16, Steps: 258 | Train Loss: 0.2090480 Vali Loss: 0.9389106 Test Loss: 0.4164637
Validation loss decreased (0.938980 --> 0.938911).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2200879
	speed: 0.5614s/iter; left time: 4868.6763s
	iters: 200, epoch: 17 | loss: 0.2124819
	speed: 0.1313s/iter; left time: 1125.2982s
Epoch: 17 cost time: 33.84567904472351
Epoch: 17, Steps: 258 | Train Loss: 0.2089255 Vali Loss: 0.9380663 Test Loss: 0.4166559
Validation loss decreased (0.938911 --> 0.938066).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2066381
	speed: 0.5573s/iter; left time: 4689.6278s
	iters: 200, epoch: 18 | loss: 0.2015360
	speed: 0.1300s/iter; left time: 1080.9048s
Epoch: 18 cost time: 34.17766213417053
Epoch: 18, Steps: 258 | Train Loss: 0.2087719 Vali Loss: 0.9389331 Test Loss: 0.4169708
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1998508
	speed: 0.5717s/iter; left time: 4663.4142s
	iters: 200, epoch: 19 | loss: 0.2142940
	speed: 0.1291s/iter; left time: 1040.4862s
Epoch: 19 cost time: 34.20927047729492
Epoch: 19, Steps: 258 | Train Loss: 0.2087500 Vali Loss: 0.9380443 Test Loss: 0.4169431
Validation loss decreased (0.938066 --> 0.938044).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2080835
	speed: 0.5708s/iter; left time: 4508.4586s
	iters: 200, epoch: 20 | loss: 0.2133667
	speed: 0.1385s/iter; left time: 1080.4341s
Epoch: 20 cost time: 37.293745279312134
Epoch: 20, Steps: 258 | Train Loss: 0.2087171 Vali Loss: 0.9386560 Test Loss: 0.4171991
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2290298
	speed: 0.5872s/iter; left time: 4486.9388s
	iters: 200, epoch: 21 | loss: 0.2241163
	speed: 0.0818s/iter; left time: 616.7652s
Epoch: 21 cost time: 27.58617401123047
Epoch: 21, Steps: 258 | Train Loss: 0.2086295 Vali Loss: 0.9386398 Test Loss: 0.4175586
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2052090
	speed: 0.5201s/iter; left time: 3839.6947s
	iters: 200, epoch: 22 | loss: 0.1910381
	speed: 0.1301s/iter; left time: 947.5040s
Epoch: 22 cost time: 33.471930503845215
Epoch: 22, Steps: 258 | Train Loss: 0.2086167 Vali Loss: 0.9384947 Test Loss: 0.4176585
EarlyStopping counter: 3 out of 3
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20134912.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4039181
	speed: 0.1257s/iter; left time: 1609.6274s
	iters: 200, epoch: 1 | loss: 0.4233851
	speed: 0.1095s/iter; left time: 1390.8348s
Epoch: 1 cost time: 29.496975421905518
Epoch: 1, Steps: 258 | Train Loss: 0.3982375 Vali Loss: 0.9341238 Test Loss: 0.4169287
Validation loss decreased (inf --> 0.934124).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3544542
	speed: 0.5300s/iter; left time: 6647.7137s
	iters: 200, epoch: 2 | loss: 0.4329402
	speed: 0.1413s/iter; left time: 1758.5252s
Epoch: 2 cost time: 36.09489107131958
Epoch: 2, Steps: 258 | Train Loss: 0.3975642 Vali Loss: 0.9327495 Test Loss: 0.4170643
Validation loss decreased (0.934124 --> 0.932750).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3830018
	speed: 0.5803s/iter; left time: 7128.8873s
	iters: 200, epoch: 3 | loss: 0.3855268
	speed: 0.1307s/iter; left time: 1592.6022s
Epoch: 3 cost time: 35.51393103599548
Epoch: 3, Steps: 258 | Train Loss: 0.3973296 Vali Loss: 0.9315158 Test Loss: 0.4168589
Validation loss decreased (0.932750 --> 0.931516).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4130740
	speed: 0.6351s/iter; left time: 7638.3496s
	iters: 200, epoch: 4 | loss: 0.3817484
	speed: 0.1294s/iter; left time: 1543.7372s
Epoch: 4 cost time: 34.757731199264526
Epoch: 4, Steps: 258 | Train Loss: 0.3971676 Vali Loss: 0.9323778 Test Loss: 0.4169457
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4064716
	speed: 0.5392s/iter; left time: 6345.5040s
	iters: 200, epoch: 5 | loss: 0.4238875
	speed: 0.1302s/iter; left time: 1519.4265s
Epoch: 5 cost time: 33.944437980651855
Epoch: 5, Steps: 258 | Train Loss: 0.3970748 Vali Loss: 0.9312611 Test Loss: 0.4163967
Validation loss decreased (0.931516 --> 0.931261).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3983392
	speed: 0.5497s/iter; left time: 6328.0722s
	iters: 200, epoch: 6 | loss: 0.4000559
	speed: 0.1211s/iter; left time: 1381.7639s
Epoch: 6 cost time: 31.654995679855347
Epoch: 6, Steps: 258 | Train Loss: 0.3969014 Vali Loss: 0.9319675 Test Loss: 0.4163054
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4258368
	speed: 0.5309s/iter; left time: 5974.3315s
	iters: 200, epoch: 7 | loss: 0.3859690
	speed: 0.1132s/iter; left time: 1262.4649s
Epoch: 7 cost time: 30.955787658691406
Epoch: 7, Steps: 258 | Train Loss: 0.3968967 Vali Loss: 0.9312785 Test Loss: 0.4167258
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4034109
	speed: 0.5343s/iter; left time: 5874.7819s
	iters: 200, epoch: 8 | loss: 0.3939048
	speed: 0.1083s/iter; left time: 1180.1481s
Epoch: 8 cost time: 28.186406135559082
Epoch: 8, Steps: 258 | Train Loss: 0.3967501 Vali Loss: 0.9315885 Test Loss: 0.4173147
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41541337966918945, mae:0.41179633140563965, rse:0.6132124662399292, corr:[0.52211905 0.5326793  0.5382126  0.5389518  0.5383958  0.53885376
 0.54049015 0.54235417 0.5435172  0.5437981  0.5437791  0.5438699
 0.5440915  0.5440496  0.54338956 0.5422057  0.5409029  0.5397617
 0.5387085  0.5375495  0.53612584 0.5343729  0.532496   0.5309578
 0.52980316 0.5290222  0.52840525 0.52777654 0.52731156 0.52721745
 0.5276049  0.5284068  0.52915806 0.52955    0.5293711  0.5290185
 0.52869725 0.5285906  0.52860177 0.52849036 0.5282992  0.5280911
 0.5280793  0.5284206  0.5289034  0.5292217  0.5292433  0.52896905
 0.5285604  0.5282414  0.5282025  0.52830654 0.52835995 0.5281103
 0.5276459  0.52719045 0.5269299  0.5268078  0.5267229  0.5264763
 0.5261279  0.52588236 0.5259251  0.52621037 0.52666837 0.52707994
 0.52721167 0.5270256  0.52671474 0.52646214 0.5264076  0.5264587
 0.5264466  0.52626544 0.52597564 0.52561945 0.525265   0.5249973
 0.5248275  0.5247394  0.52469295 0.5246424  0.5245922  0.52451074
 0.5244749  0.5244312  0.5243834  0.5243042  0.52422714 0.52424735
 0.524386   0.5246207  0.52492493 0.5252512  0.5255816  0.5258266
 0.525955   0.52594876 0.52564305 0.5251448  0.52457047 0.524094
 0.52387714 0.52391285 0.52414465 0.5244451  0.5245321  0.5243364
 0.52386963 0.52333254 0.5228792  0.52256006 0.5223498  0.52219826
 0.5220104  0.52176327 0.52148676 0.5212374  0.5210752  0.52104455
 0.52103853 0.5209352  0.5206619  0.5203722  0.5201803  0.52006245
 0.5199484  0.5197949  0.5195214  0.5191833  0.5188225  0.5185642
 0.5185351  0.5186968  0.518874   0.5189833  0.5190431  0.5189575
 0.5188229  0.5187544  0.51874524 0.5188505  0.5190946  0.5194439
 0.5197878  0.5199508  0.51990604 0.51968753 0.519439   0.5192577
 0.519217   0.5193628  0.5195657  0.51966804 0.5195624  0.51932675
 0.5190249  0.51876706 0.51869065 0.5187574  0.51898164 0.5192611
 0.51949596 0.5196006  0.51957726 0.5194796  0.51941293 0.5194898
 0.5196958  0.5199551  0.52025974 0.52052486 0.5206116  0.5205201
 0.52035135 0.5202422  0.5203027  0.52054566 0.52084786 0.52109456
 0.52116233 0.5210406  0.52076334 0.52053094 0.5204776  0.5206286
 0.5209125  0.5212612  0.52156603 0.5217652  0.521879   0.52194154
 0.52196294 0.5219649  0.5218113  0.5214188  0.5207912  0.52003396
 0.51925725 0.5185926  0.5180464  0.5175913  0.51711243 0.51652986
 0.5158108  0.5149862  0.5141245  0.51334625 0.5126775  0.5120767
 0.5114447  0.51079744 0.5101712  0.50961953 0.50911367 0.5087159
 0.5084138  0.5080644  0.5076175  0.50704765 0.50647855 0.506128
 0.5061773  0.5064753  0.50691277 0.50732523 0.50753236 0.5075411
 0.50746006 0.50741154 0.50745815 0.5075614  0.5076664  0.5076893
 0.5075663  0.5073544  0.50706536 0.5069791  0.50716627 0.5076418
 0.5081783  0.5084744  0.5084713  0.5082716  0.50806683 0.5079428
 0.5079406  0.50801796 0.50801843 0.50785923 0.50747436 0.5070459
 0.5066772  0.50656617 0.50666517 0.5068724  0.50709164 0.50722384
 0.50721437 0.5071135  0.50700784 0.50697964 0.5070624  0.5073006
 0.5075318  0.5076733  0.50769806 0.507681   0.5076572  0.5076098
 0.507564   0.5074763  0.5073539  0.50724155 0.50720483 0.5073333
 0.5075998  0.5080022  0.5083184  0.5084738  0.50838935 0.5081286
 0.5079174  0.5078564  0.5079561  0.50812995 0.50818086 0.507963
 0.50748616 0.50690925 0.50634784 0.5058219  0.5053298  0.50481415
 0.50421613 0.5034693  0.5027177  0.50204015 0.50149107 0.50110346
 0.500821   0.5006043  0.500409   0.50017005 0.49987525 0.49953064
 0.49928904 0.49912864 0.4990353  0.49895895 0.49890512 0.49879488
 0.49861395 0.4984574  0.4983485  0.4983118  0.49827707 0.49818334
 0.49796355 0.49762538 0.4973278  0.49725613 0.49746805 0.49784204
 0.49816608 0.49830574 0.49815497 0.49779284 0.49734443 0.49697366
 0.49687523 0.49702156 0.4972565  0.4974612  0.4975165  0.49747965
 0.4973148  0.49695215 0.49662226 0.49645084 0.49649206 0.49664733
 0.49674404 0.4967802  0.49675173 0.4967353  0.49679384 0.49695623
 0.4970904  0.49714747 0.4970678  0.49683863 0.49657884 0.49638298
 0.49628016 0.4962812  0.49628723 0.4962055  0.49604976 0.49586222
 0.49564722 0.4955308  0.49552146 0.49558648 0.49568275 0.495768
 0.49576133 0.49571556 0.49563885 0.49550384 0.49540567 0.49541235
 0.49555507 0.49583742 0.49618632 0.4965468  0.49683347 0.4970539
 0.4972363  0.49740237 0.49756873 0.49768618 0.4977169  0.49766314
 0.4975171  0.49730977 0.49710864 0.49689415 0.49661282 0.49632898
 0.49596977 0.4955028  0.4949247  0.49433172 0.49375606 0.49329653
 0.49299875 0.4928488  0.49276996 0.4926333  0.49231467 0.49190712
 0.49144563 0.4910569  0.4906964  0.4903918  0.49012202 0.4899429
 0.48986357 0.48994666 0.49019894 0.49059954 0.49103937 0.49130145
 0.49142122 0.491281   0.49103576 0.4907179  0.49048218 0.4904373
 0.4904769  0.49057132 0.49058592 0.49052256 0.49037492 0.4902741
 0.49022526 0.49025753 0.4902859  0.49037096 0.49042937 0.4905663
 0.49067464 0.49065378 0.49055406 0.49045625 0.49036658 0.49028635
 0.4902073  0.4901438  0.490119   0.49011615 0.4901196  0.49009234
 0.4899641  0.4897559  0.48948905 0.48922098 0.48901927 0.48890162
 0.48894924 0.48911083 0.48931584 0.4894833  0.48959318 0.4897235
 0.48981163 0.48993522 0.49009714 0.49028197 0.4903487  0.49030185
 0.49011692 0.4898822  0.48960224 0.4893857  0.48925686 0.48916233
 0.48909613 0.48901632 0.48899025 0.48906717 0.48928323 0.48959863
 0.4898986  0.4901583  0.49033442 0.49035922 0.49020204 0.48988685
 0.48942715 0.48884085 0.48814216 0.48733282 0.48647732 0.48572412
 0.48517984 0.48471126 0.48426348 0.48379415 0.48322955 0.48261034
 0.4820308  0.48158947 0.4812222  0.4807733  0.48016322 0.47946543
 0.47877982 0.4783045  0.4781015  0.47800052 0.47802955 0.4780308
 0.47797924 0.47785997 0.47769374 0.47761595 0.47763622 0.47777796
 0.47798923 0.47815073 0.4782421  0.47840902 0.47864246 0.47891575
 0.47912273 0.47927094 0.47934014 0.47926167 0.47912192 0.47908625
 0.47922972 0.4795159  0.4798219  0.4800046  0.48015758 0.48041168
 0.4807742  0.48115417 0.48139238 0.4814577  0.48133668 0.48103023
 0.48063296 0.48029304 0.48016712 0.4802892  0.4805387  0.4807587
 0.48085213 0.48079184 0.4806482  0.48049042 0.48034853 0.480241
 0.48009482 0.47990233 0.47971666 0.47959146 0.4796223  0.47988176
 0.4801789  0.4804059  0.48052666 0.4805828  0.48059508 0.48064098
 0.480691   0.480697   0.4806187  0.4804843  0.48035023 0.48024565
 0.48017585 0.48024333 0.48030207 0.48033598 0.4803019  0.48021597
 0.4801159  0.48004064 0.4800531  0.4801134  0.48020574 0.4802932
 0.4803177  0.48019865 0.479827   0.47914723 0.4782298  0.4773229
 0.4765702  0.47607675 0.47577208 0.47551298 0.47517437 0.47465587
 0.47405082 0.47344002 0.47289976 0.4724241  0.4720059  0.47156993
 0.47109348 0.47060564 0.47022447 0.47000906 0.46993655 0.46991655
 0.469888   0.46975368 0.4695627  0.46940958 0.46940345 0.46951476
 0.46976557 0.46998504 0.47005793 0.4699465  0.4696944  0.46947655
 0.46942106 0.46953517 0.46988434 0.47038025 0.47092754 0.47139215
 0.4717097  0.4718518  0.47173488 0.4714797  0.47137132 0.47153875
 0.47188377 0.47235206 0.47276556 0.47299063 0.47308186 0.47308275
 0.47299924 0.47296667 0.4730368  0.47312793 0.4732297  0.47321543
 0.47299686 0.47265643 0.47231513 0.47196087 0.47169495 0.4714811
 0.47132757 0.47123522 0.47122964 0.47129783 0.4713397  0.47130206
 0.4711059  0.47073582 0.47040397 0.47028545 0.47042736 0.47073787
 0.47105688 0.47122714 0.47117972 0.47095698 0.47067627 0.47046012
 0.4703751  0.47045818 0.47059485 0.47063243 0.47059944 0.47051424
 0.47045848 0.47050014 0.47064582 0.47081178 0.47083068 0.47060812
 0.47018132 0.4697135  0.46928996 0.46889994 0.46851328 0.4681055
 0.46767193 0.46718645 0.4667707  0.46644968 0.46614537 0.4660103
 0.46587497 0.46573606 0.46543825 0.46502006 0.46449465 0.4639539
 0.4635675  0.46340576 0.4634913  0.46361536 0.46369928 0.46370515
 0.46359038 0.463513   0.46352893 0.4636582  0.46394968 0.46425056
 0.46458742 0.46492413 0.46530327 0.465789   0.46615967 0.46636406
 0.46641168 0.46649355 0.46681765 0.4673949  0.46810064 0.46852377
 0.46855685 0.468351   0.46809968 0.46799967 0.46690935 0.46143183]
