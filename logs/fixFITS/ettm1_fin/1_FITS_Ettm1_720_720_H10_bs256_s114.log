Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  58060800.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.646626234054565
Epoch: 1, Steps: 64 | Train Loss: 0.6624345 Vali Loss: 1.2828840 Test Loss: 0.6123764
Validation loss decreased (inf --> 1.282884).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.710126876831055
Epoch: 2, Steps: 64 | Train Loss: 0.4984214 Vali Loss: 1.1161764 Test Loss: 0.5021308
Validation loss decreased (1.282884 --> 1.116176).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.947591543197632
Epoch: 3, Steps: 64 | Train Loss: 0.4517441 Vali Loss: 1.0523585 Test Loss: 0.4648049
Validation loss decreased (1.116176 --> 1.052359).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.539219617843628
Epoch: 4, Steps: 64 | Train Loss: 0.4330132 Vali Loss: 1.0192150 Test Loss: 0.4480517
Validation loss decreased (1.052359 --> 1.019215).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.850338459014893
Epoch: 5, Steps: 64 | Train Loss: 0.4228510 Vali Loss: 1.0003397 Test Loss: 0.4383721
Validation loss decreased (1.019215 --> 1.000340).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.210155725479126
Epoch: 6, Steps: 64 | Train Loss: 0.4162189 Vali Loss: 0.9870400 Test Loss: 0.4323054
Validation loss decreased (1.000340 --> 0.987040).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.222538232803345
Epoch: 7, Steps: 64 | Train Loss: 0.4120250 Vali Loss: 0.9763823 Test Loss: 0.4277286
Validation loss decreased (0.987040 --> 0.976382).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.332091093063354
Epoch: 8, Steps: 64 | Train Loss: 0.4088790 Vali Loss: 0.9694207 Test Loss: 0.4247723
Validation loss decreased (0.976382 --> 0.969421).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.031348705291748
Epoch: 9, Steps: 64 | Train Loss: 0.4064096 Vali Loss: 0.9647735 Test Loss: 0.4226016
Validation loss decreased (0.969421 --> 0.964774).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.352616548538208
Epoch: 10, Steps: 64 | Train Loss: 0.4044956 Vali Loss: 0.9589890 Test Loss: 0.4212116
Validation loss decreased (0.964774 --> 0.958989).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.538040161132812
Epoch: 11, Steps: 64 | Train Loss: 0.4033024 Vali Loss: 0.9552794 Test Loss: 0.4203436
Validation loss decreased (0.958989 --> 0.955279).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.76248836517334
Epoch: 12, Steps: 64 | Train Loss: 0.4021920 Vali Loss: 0.9533135 Test Loss: 0.4196977
Validation loss decreased (0.955279 --> 0.953313).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.540479898452759
Epoch: 13, Steps: 64 | Train Loss: 0.4013001 Vali Loss: 0.9508680 Test Loss: 0.4193251
Validation loss decreased (0.953313 --> 0.950868).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.160536289215088
Epoch: 14, Steps: 64 | Train Loss: 0.4007676 Vali Loss: 0.9475946 Test Loss: 0.4191527
Validation loss decreased (0.950868 --> 0.947595).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.957005500793457
Epoch: 15, Steps: 64 | Train Loss: 0.4000472 Vali Loss: 0.9466783 Test Loss: 0.4189170
Validation loss decreased (0.947595 --> 0.946678).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.519082307815552
Epoch: 16, Steps: 64 | Train Loss: 0.3998253 Vali Loss: 0.9465613 Test Loss: 0.4188815
Validation loss decreased (0.946678 --> 0.946561).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.999737024307251
Epoch: 17, Steps: 64 | Train Loss: 0.3992315 Vali Loss: 0.9454033 Test Loss: 0.4190603
Validation loss decreased (0.946561 --> 0.945403).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.725554943084717
Epoch: 18, Steps: 64 | Train Loss: 0.3992784 Vali Loss: 0.9439864 Test Loss: 0.4189642
Validation loss decreased (0.945403 --> 0.943986).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.032858610153198
Epoch: 19, Steps: 64 | Train Loss: 0.3990174 Vali Loss: 0.9434906 Test Loss: 0.4191158
Validation loss decreased (0.943986 --> 0.943491).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.028666973114014
Epoch: 20, Steps: 64 | Train Loss: 0.3989864 Vali Loss: 0.9426442 Test Loss: 0.4191161
Validation loss decreased (0.943491 --> 0.942644).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.033241748809814
Epoch: 21, Steps: 64 | Train Loss: 0.3985320 Vali Loss: 0.9424589 Test Loss: 0.4190895
Validation loss decreased (0.942644 --> 0.942459).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.172499895095825
Epoch: 22, Steps: 64 | Train Loss: 0.3984257 Vali Loss: 0.9419436 Test Loss: 0.4193588
Validation loss decreased (0.942459 --> 0.941944).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.910514831542969
Epoch: 23, Steps: 64 | Train Loss: 0.3982128 Vali Loss: 0.9415503 Test Loss: 0.4193478
Validation loss decreased (0.941944 --> 0.941550).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 10.478973627090454
Epoch: 24, Steps: 64 | Train Loss: 0.3983098 Vali Loss: 0.9399403 Test Loss: 0.4194488
Validation loss decreased (0.941550 --> 0.939940).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 11.304505109786987
Epoch: 25, Steps: 64 | Train Loss: 0.3980586 Vali Loss: 0.9394019 Test Loss: 0.4194837
Validation loss decreased (0.939940 --> 0.939402).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.951107025146484
Epoch: 26, Steps: 64 | Train Loss: 0.3978792 Vali Loss: 0.9402825 Test Loss: 0.4195774
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.84116530418396
Epoch: 27, Steps: 64 | Train Loss: 0.3979038 Vali Loss: 0.9390256 Test Loss: 0.4195528
Validation loss decreased (0.939402 --> 0.939026).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.844064950942993
Epoch: 28, Steps: 64 | Train Loss: 0.3979873 Vali Loss: 0.9388344 Test Loss: 0.4196864
Validation loss decreased (0.939026 --> 0.938834).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.716778993606567
Epoch: 29, Steps: 64 | Train Loss: 0.3980708 Vali Loss: 0.9386533 Test Loss: 0.4197841
Validation loss decreased (0.938834 --> 0.938653).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 10.305609703063965
Epoch: 30, Steps: 64 | Train Loss: 0.3979735 Vali Loss: 0.9384558 Test Loss: 0.4197906
Validation loss decreased (0.938653 --> 0.938456).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 10.035829544067383
Epoch: 31, Steps: 64 | Train Loss: 0.3976949 Vali Loss: 0.9383457 Test Loss: 0.4199129
Validation loss decreased (0.938456 --> 0.938346).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 10.509764432907104
Epoch: 32, Steps: 64 | Train Loss: 0.3976424 Vali Loss: 0.9385060 Test Loss: 0.4198211
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.637860536575317
Epoch: 33, Steps: 64 | Train Loss: 0.3978451 Vali Loss: 0.9375409 Test Loss: 0.4199335
Validation loss decreased (0.938346 --> 0.937541).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 10.394206047058105
Epoch: 34, Steps: 64 | Train Loss: 0.3976832 Vali Loss: 0.9369315 Test Loss: 0.4200078
Validation loss decreased (0.937541 --> 0.936931).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 10.170018672943115
Epoch: 35, Steps: 64 | Train Loss: 0.3976703 Vali Loss: 0.9373290 Test Loss: 0.4200014
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 10.113069534301758
Epoch: 36, Steps: 64 | Train Loss: 0.3974954 Vali Loss: 0.9367641 Test Loss: 0.4200381
Validation loss decreased (0.936931 --> 0.936764).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 10.215477705001831
Epoch: 37, Steps: 64 | Train Loss: 0.3972200 Vali Loss: 0.9376581 Test Loss: 0.4201161
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 10.89743685722351
Epoch: 38, Steps: 64 | Train Loss: 0.3973025 Vali Loss: 0.9368413 Test Loss: 0.4201147
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 11.109432697296143
Epoch: 39, Steps: 64 | Train Loss: 0.3974262 Vali Loss: 0.9363388 Test Loss: 0.4201719
Validation loss decreased (0.936764 --> 0.936339).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 10.47667908668518
Epoch: 40, Steps: 64 | Train Loss: 0.3972459 Vali Loss: 0.9368004 Test Loss: 0.4202099
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 10.572093725204468
Epoch: 41, Steps: 64 | Train Loss: 0.3971522 Vali Loss: 0.9368546 Test Loss: 0.4202383
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 10.290555477142334
Epoch: 42, Steps: 64 | Train Loss: 0.3975514 Vali Loss: 0.9354919 Test Loss: 0.4202454
Validation loss decreased (0.936339 --> 0.935492).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 10.33116364479065
Epoch: 43, Steps: 64 | Train Loss: 0.3973260 Vali Loss: 0.9364207 Test Loss: 0.4202890
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 10.735202550888062
Epoch: 44, Steps: 64 | Train Loss: 0.3973608 Vali Loss: 0.9368660 Test Loss: 0.4203131
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 10.510770559310913
Epoch: 45, Steps: 64 | Train Loss: 0.3973661 Vali Loss: 0.9367028 Test Loss: 0.4203354
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41453981399536133, mae:0.4112147390842438, rse:0.6125673651695251, corr:[0.5239715  0.53060573 0.5355364  0.53772706 0.5382502  0.5386411
 0.53931457 0.5400445  0.5406281  0.5409783  0.54124445 0.54148865
 0.54175967 0.54184043 0.54140294 0.54030377 0.5387442  0.5371122
 0.53574425 0.5348442  0.5343622  0.53393584 0.53317106 0.5320062
 0.5304046  0.52881736 0.52772206 0.52735037 0.52776265 0.5286522
 0.5295705  0.53021514 0.53035724 0.5301881  0.5299314  0.52997506
 0.5302159  0.53050303 0.53060585 0.53029114 0.52967227 0.52893245
 0.5283859  0.52826834 0.52844477 0.5286571  0.52874875 0.528597
 0.52819306 0.5276775  0.52734065 0.52727383 0.5274242  0.52751774
 0.5274282  0.5271337  0.52673906 0.52637464 0.5262143  0.52620816
 0.52628076 0.52632904 0.52627623 0.52608013 0.5259207  0.5259568
 0.52622604 0.52664936 0.52708566 0.52734566 0.52739537 0.52724284
 0.5269683  0.5266994  0.5265771  0.5265836  0.52662975 0.52664447
 0.5265839  0.52646613 0.52631956 0.5261823  0.52609867 0.5260598
 0.5260996  0.52614266 0.5261821  0.52619207 0.5262128  0.52632684
 0.52654713 0.5268214  0.527084   0.5272475  0.527286   0.5271459
 0.5268783  0.5265677  0.52614146 0.52572113 0.5253414  0.52504665
 0.52487254 0.5247868  0.5247737  0.52482903 0.5248041  0.5246733
 0.5243935  0.5240421  0.5236957  0.5234069  0.5232142  0.5231226
 0.52306217 0.5229679  0.5227764  0.52245516 0.52204597 0.521681
 0.52142906 0.5213158  0.52127683 0.5212948  0.521286   0.5211309
 0.5208038  0.5204074  0.52003884 0.5198334  0.5197861  0.51985925
 0.5199996  0.52009803 0.5200314  0.51982504 0.5195991  0.5193856
 0.5192985  0.5193889  0.5195599  0.51974255 0.51987565 0.5199414
 0.51994306 0.51987964 0.51983255 0.51983494 0.5199061  0.51998293
 0.5200048  0.5199972  0.51995623 0.5198983  0.5198378  0.5198532
 0.5198985  0.51993704 0.51998115 0.5199939  0.52005875 0.5201951
 0.52039623 0.52061373 0.5208041  0.5209369  0.52103174 0.52114505
 0.5212592  0.5213506  0.5214739  0.5216123  0.52169466 0.5216974
 0.52163637 0.52153283 0.5214376  0.52137697 0.5213556  0.5213864
 0.5214506  0.52153957 0.5215994  0.5216602  0.5217304  0.5218265
 0.5219893  0.5222324  0.52251863 0.5227709  0.52292    0.5229006
 0.5226812  0.522297   0.52171427 0.52096677 0.5201372  0.5193429
 0.5186201  0.5179943  0.51740116 0.5168169  0.51618737 0.51549774
 0.5147763  0.51406944 0.5134247  0.51290506 0.51248795 0.51211494
 0.5116938  0.5111918  0.51060057 0.5099378  0.5092134  0.50855386
 0.50805944 0.50772935 0.50756663 0.5074919  0.50744885 0.5074325
 0.50748307 0.5075118  0.5075375  0.5075933  0.5076486  0.50771755
 0.5077954  0.5078728  0.50791276 0.50786096 0.5077535  0.5076501
 0.5076065  0.50767696 0.5077884  0.50798815 0.5081916  0.5083887
 0.5085174  0.50849795 0.50837857 0.50824624 0.50816274 0.50812113
 0.5081156  0.5081282  0.50812596 0.5081092  0.5080469  0.5079986
 0.5079327  0.50791556 0.50793433 0.50797695 0.5080517  0.50814945
 0.50825274 0.50835544 0.5084313  0.50847787 0.50851274 0.50862485
 0.50873685 0.50884336 0.5089121  0.5089449  0.5089252  0.50883466
 0.5087188  0.5085967  0.5085007  0.5084324  0.5083826  0.5083529
 0.50830907 0.50832    0.5083265  0.50837463 0.5084252  0.5084442
 0.5084715  0.50845456 0.50838274 0.5082622  0.50805414 0.5077362
 0.5073104  0.50683063 0.50630605 0.5057182  0.5050992  0.50451225
 0.50400835 0.5035674  0.503216   0.50287867 0.5024875  0.50204027
 0.50153965 0.501058   0.5006855  0.500438   0.50028557 0.5001494
 0.5000425  0.49986625 0.4996014  0.49928546 0.4990212  0.4988406
 0.49876925 0.49882692 0.49891844 0.49897727 0.49894923 0.49885246
 0.49869564 0.498503   0.4983401  0.49825343 0.49823886 0.4982471
 0.49821603 0.49812984 0.4979466  0.49771273 0.49746588 0.4972695
 0.49722132 0.49729052 0.49740532 0.4975188  0.4975774  0.4976307
 0.4976469  0.49754813 0.4974469  0.49737555 0.49735972 0.4973874
 0.49740028 0.49740404 0.4973856  0.49735954 0.49733192 0.49733126
 0.49729174 0.49724448 0.49718273 0.4971054  0.49706888 0.49707904
 0.4971118  0.49716166 0.49718344 0.4971514  0.4970908  0.49704075
 0.4969585  0.49691504 0.4968942  0.4968917  0.49688342 0.49683967
 0.49670628 0.49654424 0.4963972  0.49627376 0.49622715 0.49627006
 0.49637896 0.4965353  0.49669918 0.4968716  0.497039   0.49723965
 0.49750143 0.497824   0.49814653 0.4983985  0.49850458 0.49844995
 0.49821255 0.4978213  0.49735358 0.49685302 0.49633342 0.49593058
 0.4956348  0.49542147 0.49523395 0.49504435 0.4947727  0.49442312
 0.49401996 0.4936123  0.49324694 0.4929418  0.49264282 0.49238738
 0.4921227  0.4918722  0.49156913 0.49126738 0.49099407 0.49081534
 0.49072826 0.49073818 0.49080494 0.4909066  0.49101    0.49105132
 0.49112597 0.4911512  0.49119627 0.49120155 0.49121153 0.4912681
 0.4913047  0.4913435  0.4913277  0.49126932 0.4911642  0.49110332
 0.4910986  0.49117255 0.49126685 0.49141115 0.49152336 0.49164584
 0.49169222 0.4916067  0.49144703 0.49129158 0.4911572  0.49106097
 0.49097967 0.4909018  0.4908292  0.49075958 0.49070665 0.49068564
 0.49065435 0.49062994 0.4905794  0.49052218 0.49047974 0.4904563
 0.49049747 0.49057683 0.4906597  0.49069998 0.49067873 0.4906689
 0.4906033  0.49055243 0.49052966 0.4905753  0.49061164 0.49063894
 0.49061558 0.4905614  0.4904568  0.49035257 0.49027538 0.4902337
 0.49023092 0.4902271  0.490217   0.490201   0.49019277 0.49020892
 0.49024418 0.49032554 0.49039862 0.49037254 0.49018177 0.4898174
 0.48929515 0.48868376 0.4880568  0.48744014 0.48684582 0.48631933
 0.4858859  0.4854384  0.4849585  0.48444125 0.4838375  0.48315814
 0.48246357 0.48184794 0.4813296  0.48086452 0.4804235  0.48001897
 0.47962424 0.47927693 0.47897175 0.47860965 0.47831297 0.47807702
 0.4779578  0.47793952 0.47796366 0.47804588 0.47814032 0.47826752
 0.47843605 0.4785895  0.47872496 0.47891712 0.4791222  0.47929892
 0.47938162 0.47941834 0.47943705 0.47940683 0.47936648 0.4794052
 0.47955015 0.47978505 0.48005378 0.48025623 0.48041475 0.48057133
 0.48068717 0.48072973 0.480684   0.48063818 0.48062807 0.4806262
 0.4806189  0.48058757 0.48053646 0.48048967 0.4804478  0.48041138
 0.480377   0.48033008 0.48028147 0.48024046 0.48021978 0.48026398
 0.48034686 0.48043782 0.48049617 0.48046237 0.48035717 0.48029155
 0.48023435 0.4802261  0.48027474 0.48036402 0.48042002 0.48042378
 0.4803631  0.4802857  0.48023322 0.48024115 0.480301   0.4803695
 0.4803857  0.48040655 0.48036125 0.48030847 0.48029548 0.4803599
 0.48050433 0.48068172 0.4808541  0.48091525 0.4807883  0.48043367
 0.47990477 0.47931632 0.47872165 0.47812584 0.47753212 0.47703674
 0.47661337 0.47626308 0.47592136 0.47553745 0.47509533 0.47454718
 0.4739559  0.47334597 0.47275817 0.47220585 0.47174168 0.47134635
 0.47099823 0.47065762 0.47034365 0.47006834 0.469833   0.46965262
 0.46957648 0.46956638 0.46962366 0.4697186  0.46983436 0.4699026
 0.46999782 0.47009313 0.47020864 0.4703573  0.470519   0.4707059
 0.47088426 0.47096995 0.47100767 0.4709987  0.47098875 0.47100624
 0.47110566 0.47130308 0.471481   0.4716266  0.47182786 0.4720741
 0.47225738 0.4723977  0.4724768  0.4724815  0.47251374 0.47257343
 0.4725642  0.47252902 0.47248107 0.47238013 0.47228515 0.47215176
 0.4719296  0.47166955 0.47144008 0.4711962  0.471034   0.4709286
 0.47087166 0.47082496 0.47078133 0.4707319  0.47065863 0.47061622
 0.47057956 0.47049114 0.47041568 0.47036675 0.47031584 0.4702338
 0.47013012 0.47003338 0.469982   0.4699917  0.47004    0.47007275
 0.47006312 0.4700925  0.4701703  0.47025415 0.470424   0.47066343
 0.47095257 0.4712374  0.47146022 0.47159183 0.4715345  0.47126037
 0.47082937 0.47039434 0.47000396 0.4696074  0.4691862  0.46877173
 0.46842074 0.46810493 0.46789196 0.4677096  0.46738192 0.4670347
 0.46654978 0.46609965 0.4657081  0.46549675 0.4654008  0.46528056
 0.46506202 0.46467894 0.4642307  0.46375567 0.46346    0.46345904
 0.46366    0.46398708 0.46420112 0.4641849  0.46404845 0.46385399
 0.46387026 0.46415505 0.464698   0.46541572 0.4659204  0.46610466
 0.4660168  0.465919   0.46607643 0.46658483 0.46749198 0.46849242
 0.4694277  0.470136   0.47037205 0.47007468 0.4682126  0.46246645]
