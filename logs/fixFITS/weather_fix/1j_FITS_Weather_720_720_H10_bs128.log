Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=70, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=140, bias=True)
    (1): Linear(in_features=70, out_features=140, bias=True)
    (2): Linear(in_features=70, out_features=140, bias=True)
    (3): Linear(in_features=70, out_features=140, bias=True)
    (4): Linear(in_features=70, out_features=140, bias=True)
    (5): Linear(in_features=70, out_features=140, bias=True)
    (6): Linear(in_features=70, out_features=140, bias=True)
    (7): Linear(in_features=70, out_features=140, bias=True)
    (8): Linear(in_features=70, out_features=140, bias=True)
    (9): Linear(in_features=70, out_features=140, bias=True)
    (10): Linear(in_features=70, out_features=140, bias=True)
    (11): Linear(in_features=70, out_features=140, bias=True)
    (12): Linear(in_features=70, out_features=140, bias=True)
    (13): Linear(in_features=70, out_features=140, bias=True)
    (14): Linear(in_features=70, out_features=140, bias=True)
    (15): Linear(in_features=70, out_features=140, bias=True)
    (16): Linear(in_features=70, out_features=140, bias=True)
    (17): Linear(in_features=70, out_features=140, bias=True)
    (18): Linear(in_features=70, out_features=140, bias=True)
    (19): Linear(in_features=70, out_features=140, bias=True)
    (20): Linear(in_features=70, out_features=140, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  52684800.0
params:  208740.0
Trainable parameters:  208740
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7426547
	speed: 0.2549s/iter; left time: 1733.8270s
Epoch: 1 cost time: 35.04981517791748
Epoch: 1, Steps: 138 | Train Loss: 0.8257280 Vali Loss: 0.7021774 Test Loss: 0.3492236
Validation loss decreased (inf --> 0.702177).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6459202
	speed: 0.5547s/iter; left time: 3696.1071s
Epoch: 2 cost time: 35.50891733169556
Epoch: 2, Steps: 138 | Train Loss: 0.6624565 Vali Loss: 0.6521152 Test Loss: 0.3337084
Validation loss decreased (0.702177 --> 0.652115).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6257237
	speed: 0.5882s/iter; left time: 3838.2429s
Epoch: 3 cost time: 37.3596293926239
Epoch: 3, Steps: 138 | Train Loss: 0.6193790 Vali Loss: 0.6270852 Test Loss: 0.3272257
Validation loss decreased (0.652115 --> 0.627085).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5525414
	speed: 0.6110s/iter; left time: 3902.1456s
Epoch: 4 cost time: 38.57099652290344
Epoch: 4, Steps: 138 | Train Loss: 0.5958018 Vali Loss: 0.6160370 Test Loss: 0.3233958
Validation loss decreased (0.627085 --> 0.616037).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5425063
	speed: 0.6221s/iter; left time: 3887.7980s
Epoch: 5 cost time: 37.969780683517456
Epoch: 5, Steps: 138 | Train Loss: 0.5827992 Vali Loss: 0.6109808 Test Loss: 0.3210278
Validation loss decreased (0.616037 --> 0.610981).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6344922
	speed: 0.5677s/iter; left time: 3469.3825s
Epoch: 6 cost time: 35.09938716888428
Epoch: 6, Steps: 138 | Train Loss: 0.5750995 Vali Loss: 0.6030099 Test Loss: 0.3195325
Validation loss decreased (0.610981 --> 0.603010).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5498897
	speed: 0.5799s/iter; left time: 3463.4900s
Epoch: 7 cost time: 36.20926594734192
Epoch: 7, Steps: 138 | Train Loss: 0.5704684 Vali Loss: 0.6028835 Test Loss: 0.3185882
Validation loss decreased (0.603010 --> 0.602884).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5628312
	speed: 0.5607s/iter; left time: 3271.3996s
Epoch: 8 cost time: 34.79802131652832
Epoch: 8, Steps: 138 | Train Loss: 0.5680390 Vali Loss: 0.6028957 Test Loss: 0.3178163
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5428503
	speed: 0.5660s/iter; left time: 3224.4242s
Epoch: 9 cost time: 35.60864782333374
Epoch: 9, Steps: 138 | Train Loss: 0.5666993 Vali Loss: 0.6000398 Test Loss: 0.3172626
Validation loss decreased (0.602884 --> 0.600040).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4909446
	speed: 0.5746s/iter; left time: 3194.3919s
Epoch: 10 cost time: 35.79175901412964
Epoch: 10, Steps: 138 | Train Loss: 0.5656241 Vali Loss: 0.6031752 Test Loss: 0.3168520
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6279100
	speed: 0.5843s/iter; left time: 3167.2804s
Epoch: 11 cost time: 36.46541476249695
Epoch: 11, Steps: 138 | Train Loss: 0.5648018 Vali Loss: 0.6012505 Test Loss: 0.3164461
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5652783
	speed: 0.5719s/iter; left time: 3021.5225s
Epoch: 12 cost time: 34.56308603286743
Epoch: 12, Steps: 138 | Train Loss: 0.5644354 Vali Loss: 0.6001992 Test Loss: 0.3160192
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.3127410113811493, mae:0.33506542444229126, rse:0.7359095811843872, corr:[0.46529737 0.47099388 0.4728481  0.47159305 0.46874708 0.4661315
 0.46488768 0.4649796  0.4657131  0.46626568 0.46610233 0.46514055
 0.46356288 0.46186933 0.46057823 0.4599155  0.45985782 0.4600839
 0.4602528  0.4600307  0.45929122 0.45809278 0.45670947 0.45535514
 0.45429084 0.45360258 0.45321697 0.4529181  0.45251137 0.4518314
 0.45089245 0.449814   0.44885004 0.44813633 0.4477947  0.4477095
 0.4477975  0.4478186  0.44761062 0.44710857 0.4463618  0.44554633
 0.44479644 0.44418368 0.44374606 0.44344443 0.44322497 0.4429877
 0.44250628 0.44182402 0.44100523 0.44007808 0.43921456 0.4385297
 0.43807018 0.4378004  0.43766165 0.4375229  0.4373284  0.4370086
 0.43658355 0.43608177 0.4355861  0.43514055 0.43478778 0.4345435
 0.4343483  0.43416202 0.43394434 0.43366468 0.4333599  0.4329542
 0.43258137 0.43225235 0.4319555  0.43168962 0.43145207 0.4312614
 0.4310616  0.43083608 0.4306392  0.4304632  0.43028334 0.43012655
 0.43001    0.429893   0.4297591  0.42948443 0.429146   0.42874813
 0.42836174 0.4280564  0.42783424 0.42774454 0.4277785  0.42786747
 0.42793328 0.42789894 0.42771816 0.4274204  0.42704016 0.4266283
 0.42625555 0.42594364 0.4256871  0.42545304 0.42519534 0.42492267
 0.42456594 0.42416966 0.42374092 0.4232901  0.42286825 0.42255116
 0.42233357 0.42218712 0.42210197 0.42203838 0.42192572 0.42168367
 0.42136526 0.421011   0.4206407  0.42031267 0.42003542 0.41981375
 0.41961187 0.4193706  0.41908392 0.41872668 0.41829988 0.41785297
 0.41745448 0.41714287 0.41695452 0.4168476  0.41678596 0.41672722
 0.41651583 0.4162331  0.41590726 0.41559222 0.41526332 0.41495776
 0.41470158 0.41448587 0.4143007  0.41412237 0.41381708 0.4132982
 0.4126019  0.41183174 0.41103378 0.41025555 0.4096498  0.40921828
 0.40899462 0.4087741  0.40851873 0.40817055 0.40772983 0.4072199
 0.406674   0.4061705  0.40572283 0.4053632  0.40502274 0.4046613
 0.40425727 0.4037563  0.40313613 0.4024362  0.40172222 0.40108305
 0.4005324  0.40008453 0.39970985 0.39936918 0.39899135 0.3984973
 0.39789692 0.39718145 0.39639118 0.3956126  0.39496723 0.39448357
 0.39416832 0.3939272  0.39370292 0.3934272  0.39303374 0.3925156
 0.3919283  0.3912438  0.39059746 0.3900203  0.38951713 0.38910377
 0.38872418 0.38835964 0.38801235 0.38761654 0.3871665  0.38666278
 0.38620266 0.38579804 0.3854437  0.3851413  0.3848041  0.38443512
 0.38399574 0.38349405 0.3829623  0.38246235 0.38202688 0.38165256
 0.3813079  0.3810702  0.3808083  0.38045624 0.3800136  0.37953803
 0.37903228 0.3785226  0.37806368 0.3776678  0.3773611  0.37715822
 0.3769853  0.37679666 0.37653247 0.37614694 0.3756572  0.37508354
 0.37452078 0.37405077 0.37366256 0.37337044 0.37310782 0.37287155
 0.37266424 0.37243244 0.37207854 0.3716697  0.3712552  0.37084097
 0.37042966 0.3700656  0.3697411  0.36944634 0.36914387 0.3688269
 0.36844447 0.36796394 0.36750236 0.36697352 0.36648703 0.36607388
 0.36578178 0.3655499  0.36526448 0.36496493 0.36458868 0.36422527
 0.36385882 0.36353493 0.36323646 0.36301014 0.362893   0.3627558
 0.36259544 0.36243752 0.36216685 0.36175054 0.36127776 0.36078578
 0.36030447 0.3598184  0.35940665 0.359062   0.35875377 0.35849166
 0.35826245 0.35795817 0.35759756 0.35716364 0.35666427 0.35608965
 0.3554966  0.354896   0.3542596  0.3535597  0.3528913  0.35226676
 0.35162982 0.3509868  0.35032862 0.34968403 0.3490294  0.34845683
 0.34786922 0.34731925 0.34680223 0.34633118 0.34587756 0.34541512
 0.34494618 0.34446368 0.34397867 0.34345707 0.34295303 0.3424293
 0.3419006  0.3414183  0.3409571  0.34050873 0.3400946  0.3396768
 0.3392522  0.33878088 0.33825827 0.3377198  0.33719948 0.33665547
 0.33613104 0.33564857 0.33517173 0.33474153 0.33438295 0.3340358
 0.33371165 0.33337945 0.33303946 0.332732   0.33240795 0.33209077
 0.3317528  0.33139732 0.33105877 0.3306994  0.33031934 0.32997656
 0.3296878  0.32940027 0.3291366  0.32890463 0.32871684 0.3285313
 0.32834917 0.32812026 0.3278449  0.32749325 0.3271021  0.32666662
 0.32621384 0.32579806 0.3254697  0.32520786 0.32501972 0.32490206
 0.32478568 0.3246556  0.32451874 0.32430908 0.3240425  0.32374468
 0.32342765 0.3231396  0.3229158  0.32271317 0.32253796 0.3223737
 0.322206   0.32199967 0.32176358 0.32151556 0.32126456 0.32100594
 0.32074806 0.32051528 0.32026142 0.31998262 0.3196902  0.31930923
 0.31886762 0.31838968 0.3179402  0.3175405  0.3172408  0.31705025
 0.31691694 0.31687483 0.31684965 0.316814   0.31673557 0.31660727
 0.31643566 0.316247   0.31605053 0.3158641  0.31571364 0.31556582
 0.3154677  0.31532902 0.3151492  0.3148764  0.31450006 0.31406674
 0.31363308 0.3132266  0.3128813  0.31257313 0.31234866 0.31220144
 0.31208664 0.31195152 0.31179667 0.31162754 0.31139773 0.3111428
 0.31085196 0.31060278 0.31034204 0.31010142 0.3098131  0.30946183
 0.30903408 0.30857316 0.30805042 0.30745968 0.30685192 0.30631575
 0.30583483 0.30541432 0.3050653  0.30474436 0.30440348 0.30401066
 0.30354568 0.30303082 0.30250993 0.3019809  0.30145985 0.30086216
 0.3002392  0.29958382 0.29886296 0.29813275 0.29738876 0.29669288
 0.2960876  0.29554203 0.2950632  0.2946274  0.29423416 0.29380697
 0.29333735 0.29281637 0.2922466  0.2916378  0.2910098  0.29045355
 0.28996432 0.28956154 0.28925136 0.28899458 0.2887507  0.28846946
 0.28815478 0.287802   0.28740528 0.28701106 0.28669265 0.28644958
 0.28628102 0.28616416 0.28604907 0.28590658 0.28571597 0.28543305
 0.28510857 0.28475296 0.28438288 0.28404784 0.28379226 0.2836457
 0.2835367  0.28344637 0.2833514  0.28321093 0.28302112 0.28276956
 0.2825051  0.28226906 0.28205433 0.28187513 0.2817232  0.28161794
 0.28154185 0.28147915 0.28139156 0.28125003 0.2810879  0.28090125
 0.28066927 0.280408   0.2801532  0.27984312 0.27955273 0.279277
 0.27907413 0.27893957 0.27888384 0.2788516  0.27883396 0.27882347
 0.2787917  0.2787244  0.27859628 0.2784208  0.27820203 0.27793157
 0.2776468  0.27734414 0.2770011  0.2766555  0.27626064 0.27584884
 0.27540514 0.27496442 0.27454367 0.27417946 0.27392447 0.2737359
 0.27360666 0.27349702 0.2733353  0.273099   0.27275923 0.2723116
 0.27179798 0.2713127  0.2708806  0.2705678  0.270388   0.27027437
 0.27018845 0.2700827  0.26990822 0.26959994 0.26917264 0.26862472
 0.26803628 0.2674916  0.26706904 0.26676577 0.2666048  0.2665115
 0.26644078 0.26629972 0.26604962 0.26569003 0.2652051  0.26464343
 0.26408687 0.2635969  0.2631948  0.26286903 0.26258716 0.2623309
 0.26204106 0.26169223 0.26127955 0.26079455 0.26023167 0.2595686
 0.2590966  0.25844267 0.25770313 0.2572549  0.25674742 0.25619647
 0.255583   0.25493142 0.2542562  0.25363234 0.25304225 0.2525416
 0.25210258 0.25169066 0.25126064 0.25081196 0.2503223  0.249818
 0.2493079  0.24881737 0.24838468 0.24802507 0.24773067 0.2474682
 0.24721043 0.24688676 0.24648464 0.24598667 0.24542396 0.2448815
 0.24431689 0.24384542 0.24340844 0.24299774 0.24263798 0.2423096
 0.2420027  0.24173726 0.24146488 0.24121554 0.24099538 0.24071833
 0.24043569 0.24017446 0.23988104 0.23954916 0.23919593 0.23886892
 0.23852059 0.23820108 0.23792185 0.23768911 0.23756132 0.23746882
 0.23742576 0.23738101 0.23734884 0.23728189 0.23717466 0.23705067
 0.23692821 0.2368028  0.23666085 0.23648937 0.23634245 0.23621665
 0.23614883 0.2361208  0.23610777 0.23603816 0.2359505  0.23583291
 0.23572025 0.23557256 0.23549631 0.23543535 0.23550248 0.23564717
 0.2358881  0.23613632 0.23631504 0.23635295 0.2362188  0.2359268
 0.23554628 0.23513655 0.23478656 0.2345699  0.23455854 0.23473354
 0.2349888  0.23519084 0.23518233 0.23492275 0.2344244  0.2337227
 0.23300388 0.2324157  0.23203517 0.23191091 0.23197508 0.23210731
 0.23212259 0.23188739 0.23136714 0.23057102 0.22968042 0.22885977
 0.22830169 0.22814004 0.22841862 0.2289645  0.22952329 0.2298271
 0.22967947 0.22898854 0.22793643 0.22678298 0.2258135  0.22526288
 0.22525197 0.22563273 0.22610092 0.22629432 0.22595493 0.22495645
 0.22348952 0.22191669 0.2206929  0.22024947 0.22073005 0.22190271
 0.22315778 0.22381648 0.22341146 0.22188264 0.2195471  0.21743087
 0.21669807 0.218387   0.22203125 0.22591135 0.22713909 0.22190833]
