Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=70, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35832
val 4935
test 10204
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=102, bias=True)
    (1): Linear(in_features=70, out_features=102, bias=True)
    (2): Linear(in_features=70, out_features=102, bias=True)
    (3): Linear(in_features=70, out_features=102, bias=True)
    (4): Linear(in_features=70, out_features=102, bias=True)
    (5): Linear(in_features=70, out_features=102, bias=True)
    (6): Linear(in_features=70, out_features=102, bias=True)
    (7): Linear(in_features=70, out_features=102, bias=True)
    (8): Linear(in_features=70, out_features=102, bias=True)
    (9): Linear(in_features=70, out_features=102, bias=True)
    (10): Linear(in_features=70, out_features=102, bias=True)
    (11): Linear(in_features=70, out_features=102, bias=True)
    (12): Linear(in_features=70, out_features=102, bias=True)
    (13): Linear(in_features=70, out_features=102, bias=True)
    (14): Linear(in_features=70, out_features=102, bias=True)
    (15): Linear(in_features=70, out_features=102, bias=True)
    (16): Linear(in_features=70, out_features=102, bias=True)
    (17): Linear(in_features=70, out_features=102, bias=True)
    (18): Linear(in_features=70, out_features=102, bias=True)
    (19): Linear(in_features=70, out_features=102, bias=True)
    (20): Linear(in_features=70, out_features=102, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  76769280.0
params:  152082.0
Trainable parameters:  152082
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 20.13914704322815
Epoch: 1, Steps: 69 | Train Loss: 0.7933044 Vali Loss: 0.6795871 Test Loss: 0.3091314
Validation loss decreased (inf --> 0.679587).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 20.060742378234863
Epoch: 2, Steps: 69 | Train Loss: 0.6315039 Vali Loss: 0.5981496 Test Loss: 0.2817718
Validation loss decreased (0.679587 --> 0.598150).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 21.838186025619507
Epoch: 3, Steps: 69 | Train Loss: 0.5771714 Vali Loss: 0.5700497 Test Loss: 0.2704003
Validation loss decreased (0.598150 --> 0.570050).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 22.822600603103638
Epoch: 4, Steps: 69 | Train Loss: 0.5496840 Vali Loss: 0.5459411 Test Loss: 0.2639144
Validation loss decreased (0.570050 --> 0.545941).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 21.996957778930664
Epoch: 5, Steps: 69 | Train Loss: 0.5325225 Vali Loss: 0.5363063 Test Loss: 0.2596465
Validation loss decreased (0.545941 --> 0.536306).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 20.66906452178955
Epoch: 6, Steps: 69 | Train Loss: 0.5229537 Vali Loss: 0.5267680 Test Loss: 0.2567426
Validation loss decreased (0.536306 --> 0.526768).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 20.378427028656006
Epoch: 7, Steps: 69 | Train Loss: 0.5163254 Vali Loss: 0.5236489 Test Loss: 0.2546170
Validation loss decreased (0.526768 --> 0.523649).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 22.25627613067627
Epoch: 8, Steps: 69 | Train Loss: 0.5131744 Vali Loss: 0.5234329 Test Loss: 0.2529803
Validation loss decreased (0.523649 --> 0.523433).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 22.866858959197998
Epoch: 9, Steps: 69 | Train Loss: 0.5096346 Vali Loss: 0.5224533 Test Loss: 0.2516874
Validation loss decreased (0.523433 --> 0.522453).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 21.913935661315918
Epoch: 10, Steps: 69 | Train Loss: 0.5087990 Vali Loss: 0.5162103 Test Loss: 0.2506171
Validation loss decreased (0.522453 --> 0.516210).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 22.47497296333313
Epoch: 11, Steps: 69 | Train Loss: 0.5055594 Vali Loss: 0.5157451 Test Loss: 0.2497299
Validation loss decreased (0.516210 --> 0.515745).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 23.54296326637268
Epoch: 12, Steps: 69 | Train Loss: 0.5056021 Vali Loss: 0.5157138 Test Loss: 0.2490249
Validation loss decreased (0.515745 --> 0.515714).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 22.39965295791626
Epoch: 13, Steps: 69 | Train Loss: 0.5055024 Vali Loss: 0.5175731 Test Loss: 0.2482930
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 21.267541646957397
Epoch: 14, Steps: 69 | Train Loss: 0.5046730 Vali Loss: 0.5154226 Test Loss: 0.2477356
Validation loss decreased (0.515714 --> 0.515423).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 21.468501806259155
Epoch: 15, Steps: 69 | Train Loss: 0.5046798 Vali Loss: 0.5123338 Test Loss: 0.2471841
Validation loss decreased (0.515423 --> 0.512334).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 21.02246356010437
Epoch: 16, Steps: 69 | Train Loss: 0.5035174 Vali Loss: 0.5120164 Test Loss: 0.2467297
Validation loss decreased (0.512334 --> 0.512016).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 20.916507720947266
Epoch: 17, Steps: 69 | Train Loss: 0.5027008 Vali Loss: 0.5112662 Test Loss: 0.2463011
Validation loss decreased (0.512016 --> 0.511266).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 21.572489261627197
Epoch: 18, Steps: 69 | Train Loss: 0.5022443 Vali Loss: 0.5143648 Test Loss: 0.2459395
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 21.644477605819702
Epoch: 19, Steps: 69 | Train Loss: 0.5022581 Vali Loss: 0.5146751 Test Loss: 0.2456479
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 20.727844953536987
Epoch: 20, Steps: 69 | Train Loss: 0.5023553 Vali Loss: 0.5135748 Test Loss: 0.2452345
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10204
mse:0.24582473933696747, mae:0.28721314668655396, rse:0.651175856590271, corr:[0.46485075 0.47151247 0.4746695  0.47453132 0.47241965 0.4700518
 0.46862984 0.4684258  0.46902665 0.46976218 0.47007647 0.4697228
 0.46871597 0.4673699  0.46616352 0.46535233 0.46503276 0.46497223
 0.4649512  0.4646742  0.46403348 0.4630168  0.46179867 0.46053073
 0.45943144 0.4586153  0.45805597 0.45759726 0.45710936 0.45645246
 0.45563155 0.45472226 0.4539211  0.45329142 0.4529336  0.4527387
 0.45265633 0.452488   0.45212862 0.4515438  0.45081106 0.4500647
 0.44939408 0.44883746 0.44840252 0.44803026 0.4476989  0.4473343
 0.4467597  0.44605803 0.44530326 0.44449806 0.44380677 0.44327536
 0.44290224 0.4426458  0.4424285  0.44215506 0.44179034 0.44131422
 0.4407628  0.44019732 0.43967432 0.4392598  0.43896702 0.43878776
 0.4386393  0.43847424 0.4382548  0.43796745 0.43765524 0.43726742
 0.43692535 0.43664095 0.43638298 0.4361567  0.43593618 0.43573898
 0.43549395 0.43520927 0.43495792 0.43473002 0.43450636 0.4343067
 0.43415105 0.43399894 0.43384373 0.43355536 0.43321514 0.4328201
 0.4324518  0.43217728 0.4319929  0.4319362  0.4319976  0.43211067
 0.43219012 0.43217304 0.43201467 0.43174013 0.43138602 0.43100965
 0.4306678  0.4303678  0.43011183 0.42988634 0.42962605 0.42933536
 0.42895868 0.428546   0.4281038  0.4276536  0.42723197 0.4269184
 0.4267009  0.42655504 0.42647117 0.42640632 0.42628205 0.42602307
 0.425688   0.4253074  0.42491934 0.42456377 0.424263   0.42403483
 0.42384517 0.42363966 0.42340308 0.42310688 0.42274046 0.4223429
 0.42196417 0.42163345 0.42139095 0.4212091  0.4210614  0.42093733
 0.4207011  0.4204282  0.42013282 0.4198595  0.41958404 0.4193143
 0.41906637 0.41883722 0.4186315  0.41842327 0.41811976 0.41765857
 0.41707548 0.41645885 0.41581503 0.4151668  0.4146198  0.41415632
 0.41384882 0.4135213  0.41318348 0.41279632 0.41238552 0.41195288
 0.41151366 0.41110617 0.41070616 0.41032743 0.40991884 0.40948358
 0.409019   0.40849704 0.40791723 0.407306   0.40671226 0.40618235
 0.40568465 0.4052265  0.4047777  0.4043428  0.40386853 0.40333334
 0.40278155 0.4021839  0.40155563 0.4009386  0.40037784 0.3998787
 0.3994358  0.39899504 0.3985664  0.39812613 0.39767143 0.3972179
 0.39683688 0.39644858 0.39610842 0.3957862  0.3954424  0.39505684
 0.39462066 0.3941483  0.3936644  0.39321697 0.39280808 0.39242634
 0.39210108 0.39180514 0.39149398 0.39114538 0.3906862  0.3901727
 0.3896047  0.3890617  0.38858554 0.38823226 0.38798305 0.38778153
 0.387552   0.38734382 0.3870334  0.38657686 0.38603202 0.38550517
 0.38502938 0.38464764 0.384376   0.38416874 0.3839894  0.38382012
 0.38359123 0.38329616 0.38290587 0.3824474  0.38196006 0.38149333
 0.3811146  0.3808418  0.38060108 0.38038105 0.38008112 0.37976182
 0.37943572 0.37911925 0.37878856 0.3785315  0.37838608 0.37830073
 0.37817752 0.37802914 0.37777397 0.37745756 0.37707266 0.376695
 0.37630755 0.3759388  0.37564108 0.37532413 0.37504616 0.3747306
 0.37446478 0.37417936 0.37379187 0.37340644 0.37306246 0.3727661
 0.37256035 0.37234968 0.37214965 0.37190577 0.37160105 0.37123686
 0.37082693 0.3704441  0.3701935  0.37002775 0.36995494 0.3699047
 0.36979052 0.36950096 0.36907867 0.3685318  0.367917   0.36735892
 0.36696643 0.36668348 0.3665157  0.36636573 0.3661336  0.36571807
 0.3651136  0.3643572  0.3634907  0.36260712 0.36194587 0.36153656
 0.36125714 0.36101997 0.3606657  0.3601167  0.35928595 0.35835055
 0.35727242 0.35629466 0.35556972 0.3551798  0.35502192 0.35490802
 0.35464838 0.3540853  0.35321152 0.3520646  0.35090733 0.349915
 0.34930235 0.34918445 0.34940127 0.34965324 0.34964254 0.34909555
 0.34797657 0.34639597 0.34471163 0.34338215 0.342789   0.3429331
 0.34352964 0.34402996 0.34385815 0.34282112 0.34106663 0.33906043
 0.3377382  0.33795846 0.33991852 0.34263194 0.3438526  0.34078464]
