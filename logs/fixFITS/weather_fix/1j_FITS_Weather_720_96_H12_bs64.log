Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j96_H12', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j96_H12_FITS_custom_ftM_sl720_ll48_pl96_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36072
val 5175
test 10444
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=82, out_features=92, bias=True)
    (1): Linear(in_features=82, out_features=92, bias=True)
    (2): Linear(in_features=82, out_features=92, bias=True)
    (3): Linear(in_features=82, out_features=92, bias=True)
    (4): Linear(in_features=82, out_features=92, bias=True)
    (5): Linear(in_features=82, out_features=92, bias=True)
    (6): Linear(in_features=82, out_features=92, bias=True)
    (7): Linear(in_features=82, out_features=92, bias=True)
    (8): Linear(in_features=82, out_features=92, bias=True)
    (9): Linear(in_features=82, out_features=92, bias=True)
    (10): Linear(in_features=82, out_features=92, bias=True)
    (11): Linear(in_features=82, out_features=92, bias=True)
    (12): Linear(in_features=82, out_features=92, bias=True)
    (13): Linear(in_features=82, out_features=92, bias=True)
    (14): Linear(in_features=82, out_features=92, bias=True)
    (15): Linear(in_features=82, out_features=92, bias=True)
    (16): Linear(in_features=82, out_features=92, bias=True)
    (17): Linear(in_features=82, out_features=92, bias=True)
    (18): Linear(in_features=82, out_features=92, bias=True)
    (19): Linear(in_features=82, out_features=92, bias=True)
    (20): Linear(in_features=82, out_features=92, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20278272.0
params:  160356.0
Trainable parameters:  160356
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4542203
	speed: 0.1799s/iter; left time: 2509.3441s
	iters: 200, epoch: 1 | loss: 0.3481508
	speed: 0.1711s/iter; left time: 2370.5279s
Epoch: 1 cost time: 50.045860290527344
Epoch: 1, Steps: 281 | Train Loss: 0.4853794 Vali Loss: 0.4091740 Test Loss: 0.1634716
Validation loss decreased (inf --> 0.409174).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2642938
	speed: 0.6776s/iter; left time: 9262.9740s
	iters: 200, epoch: 2 | loss: 0.6009163
	speed: 0.1839s/iter; left time: 2495.3288s
Epoch: 2 cost time: 52.18313145637512
Epoch: 2, Steps: 281 | Train Loss: 0.4059435 Vali Loss: 0.3956542 Test Loss: 0.1531632
Validation loss decreased (0.409174 --> 0.395654).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3271967
	speed: 0.6776s/iter; left time: 9072.2459s
	iters: 200, epoch: 3 | loss: 0.6279840
	speed: 0.1694s/iter; left time: 2251.4250s
Epoch: 3 cost time: 50.08493781089783
Epoch: 3, Steps: 281 | Train Loss: 0.3986773 Vali Loss: 0.3885741 Test Loss: 0.1499904
Validation loss decreased (0.395654 --> 0.388574).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3375525
	speed: 0.6195s/iter; left time: 8120.0081s
	iters: 200, epoch: 4 | loss: 0.5135582
	speed: 0.1751s/iter; left time: 2278.0158s
Epoch: 4 cost time: 49.43305969238281
Epoch: 4, Steps: 281 | Train Loss: 0.3945245 Vali Loss: 0.3825718 Test Loss: 0.1481027
Validation loss decreased (0.388574 --> 0.382572).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2908719
	speed: 0.6293s/iter; left time: 8071.5290s
	iters: 200, epoch: 5 | loss: 0.3026947
	speed: 0.1707s/iter; left time: 2173.0109s
Epoch: 5 cost time: 49.72450828552246
Epoch: 5, Steps: 281 | Train Loss: 0.3922943 Vali Loss: 0.3817052 Test Loss: 0.1466990
Validation loss decreased (0.382572 --> 0.381705).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.7754872
	speed: 0.6273s/iter; left time: 7869.7589s
	iters: 200, epoch: 6 | loss: 0.3581836
	speed: 0.1787s/iter; left time: 2224.0273s
Epoch: 6 cost time: 50.112905502319336
Epoch: 6, Steps: 281 | Train Loss: 0.3916021 Vali Loss: 0.3839059 Test Loss: 0.1460724
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2701023
	speed: 0.6432s/iter; left time: 7888.2350s
	iters: 200, epoch: 7 | loss: 0.2910597
	speed: 0.1863s/iter; left time: 2266.7756s
Epoch: 7 cost time: 53.13446807861328
Epoch: 7, Steps: 281 | Train Loss: 0.3898439 Vali Loss: 0.3829779 Test Loss: 0.1453058
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2730436
	speed: 0.6814s/iter; left time: 8165.6582s
	iters: 200, epoch: 8 | loss: 0.5414278
	speed: 0.1836s/iter; left time: 2181.4173s
Epoch: 8 cost time: 52.46835398674011
Epoch: 8, Steps: 281 | Train Loss: 0.3902591 Vali Loss: 0.3818229 Test Loss: 0.1450058
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j96_H12_FITS_custom_ftM_sl720_ll48_pl96_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10444
mse:0.14721046388149261, mae:0.20217691361904144, rse:0.5056169033050537, corr:[0.473424   0.47831455 0.47937018 0.4782135  0.47661552 0.4756661
 0.47555655 0.47585273 0.47598055 0.47556415 0.4746258  0.47355488
 0.4726842  0.47218576 0.47199494 0.4717742  0.4712712  0.4702761
 0.46903986 0.46783864 0.46698603 0.46650466 0.466266   0.46596086
 0.465361   0.4643516  0.46303266 0.46168286 0.4607254  0.46028963
 0.4602621  0.46029648 0.4601494  0.45962054 0.45880073 0.4577997
 0.45693085 0.45629722 0.45591354 0.4556666  0.4554362  0.45508963
 0.4544941  0.45362458 0.45259193 0.45154968 0.45075446 0.4502733
 0.44988415 0.44957817 0.44928104 0.4488304  0.44831577 0.44773504
 0.4471373  0.446527   0.4459422  0.44534504 0.44473967 0.444101
 0.4434639  0.44284746 0.4423278  0.44194022 0.44167694 0.44153312
 0.4414398  0.44138527 0.44134334 0.44122437 0.4409786  0.44044453
 0.43976152 0.43900135 0.43820646 0.43750674 0.43702006 0.43683428
 0.4368263  0.43681866 0.43680704 0.43666866 0.4363184  0.43585095
 0.43542752 0.43509296 0.43478453 0.43428448 0.43364432 0.43282858
 0.4320334  0.4315635  0.43162608 0.43206874 0.43202716 0.430313  ]
