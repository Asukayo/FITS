Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j192_H12', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j192_H12_FITS_custom_ftM_sl720_ll48_pl192_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35976
val 5079
test 10348
Model(
  (freq_upsampler): Linear(in_features=82, out_features=103, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  90811392.0
params:  8549.0
Trainable parameters:  8549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 13.048415660858154
Epoch: 1, Steps: 70 | Train Loss: 0.7134238 Vali Loss: 0.5939208 Test Loss: 0.2550798
Validation loss decreased (inf --> 0.593921).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 12.868850469589233
Epoch: 2, Steps: 70 | Train Loss: 0.5647292 Vali Loss: 0.5381587 Test Loss: 0.2343582
Validation loss decreased (0.593921 --> 0.538159).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 13.045867681503296
Epoch: 3, Steps: 70 | Train Loss: 0.5250290 Vali Loss: 0.5123099 Test Loss: 0.2259281
Validation loss decreased (0.538159 --> 0.512310).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 13.430333375930786
Epoch: 4, Steps: 70 | Train Loss: 0.5076406 Vali Loss: 0.4974256 Test Loss: 0.2210833
Validation loss decreased (0.512310 --> 0.497426).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 13.330387353897095
Epoch: 5, Steps: 70 | Train Loss: 0.4987856 Vali Loss: 0.4963003 Test Loss: 0.2181109
Validation loss decreased (0.497426 --> 0.496300).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 13.498108148574829
Epoch: 6, Steps: 70 | Train Loss: 0.4950498 Vali Loss: 0.4886530 Test Loss: 0.2162919
Validation loss decreased (0.496300 --> 0.488653).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 13.178863763809204
Epoch: 7, Steps: 70 | Train Loss: 0.4923240 Vali Loss: 0.4850038 Test Loss: 0.2144093
Validation loss decreased (0.488653 --> 0.485004).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.918334007263184
Epoch: 8, Steps: 70 | Train Loss: 0.4906082 Vali Loss: 0.4877864 Test Loss: 0.2133545
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.622171401977539
Epoch: 9, Steps: 70 | Train Loss: 0.4900931 Vali Loss: 0.4795629 Test Loss: 0.2124752
Validation loss decreased (0.485004 --> 0.479563).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 12.422165155410767
Epoch: 10, Steps: 70 | Train Loss: 0.4891314 Vali Loss: 0.4784709 Test Loss: 0.2117421
Validation loss decreased (0.479563 --> 0.478471).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.750255107879639
Epoch: 11, Steps: 70 | Train Loss: 0.4883218 Vali Loss: 0.4850702 Test Loss: 0.2112243
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 11.329694986343384
Epoch: 12, Steps: 70 | Train Loss: 0.4880255 Vali Loss: 0.4794437 Test Loss: 0.2106834
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.622577428817749
Epoch: 13, Steps: 70 | Train Loss: 0.4879378 Vali Loss: 0.4776199 Test Loss: 0.2103858
Validation loss decreased (0.478471 --> 0.477620).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 11.066547393798828
Epoch: 14, Steps: 70 | Train Loss: 0.4876740 Vali Loss: 0.4790132 Test Loss: 0.2099798
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.848659038543701
Epoch: 15, Steps: 70 | Train Loss: 0.4870319 Vali Loss: 0.4781350 Test Loss: 0.2097288
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 12.990901947021484
Epoch: 16, Steps: 70 | Train Loss: 0.4869155 Vali Loss: 0.4781977 Test Loss: 0.2094270
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j192_H12_FITS_custom_ftM_sl720_ll48_pl192_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10348
mse:0.2164967954158783, mae:0.2645065188407898, rse:0.6124827265739441, corr:[0.47627237 0.4841353  0.48679474 0.48549792 0.4831155  0.48196977
 0.48247322 0.48375258 0.4846577  0.4845873  0.48361927 0.4822757
 0.48116648 0.4807383  0.4809344  0.4812599  0.4812617  0.4805902
 0.47951323 0.47830328 0.4773546  0.47687835 0.47682208 0.47678652
 0.4764256  0.47557285 0.47434512 0.4729291  0.47172156 0.47086707
 0.47043693 0.47018352 0.46985102 0.46916023 0.46819994 0.46710184
 0.46613532 0.46531868 0.46472162 0.4642847  0.46392047 0.4635149
 0.4629163  0.4621783  0.4614168  0.46070212 0.4601591  0.45978197
 0.45938957 0.45894283 0.45841885 0.45776728 0.45717445 0.45662284
 0.45615143 0.45572418 0.4553782  0.45501718 0.45461568 0.45416453
 0.4537261  0.4532862  0.4529197  0.4525966  0.4523301  0.45212677
 0.45181963 0.4514479  0.4510227  0.45062447 0.45030048 0.44994098
 0.44964328 0.44929877 0.44877395 0.44818243 0.44765398 0.44729674
 0.44706112 0.4468676  0.44680193 0.44682303 0.44676557 0.44657958
 0.446246   0.44575542 0.44535214 0.44498104 0.44472122 0.4444668
 0.44430804 0.4442331  0.44413784 0.44402376 0.44383118 0.44355625
 0.44325364 0.44294807 0.44265264 0.4423788  0.44211516 0.4418515
 0.44160017 0.4413315  0.44105083 0.4407534  0.44034892 0.43995592
 0.43958405 0.43925864 0.43895617 0.43863493 0.4382349  0.437907
 0.43758652 0.43719462 0.4368167  0.436477   0.43614095 0.4357208
 0.4353422  0.4350318  0.43475404 0.43448073 0.43418142 0.43383083
 0.4333838  0.4328572  0.43230948 0.43179595 0.4313275  0.43090537
 0.43051937 0.43011698 0.4297441  0.42931205 0.42881748 0.4283791
 0.4279392  0.42765766 0.4274811  0.4273089  0.42704025 0.42660734
 0.4260057  0.42530525 0.42465094 0.42421272 0.4239249  0.42362407
 0.42319167 0.42254475 0.42164114 0.42052826 0.41933322 0.4183023
 0.41782978 0.41759267 0.41744646 0.4170338  0.41642264 0.41547143
 0.41440764 0.41356766 0.41317993 0.4133366  0.4135672  0.41354102
 0.4130466  0.4119128  0.41029957 0.40862596 0.4074077  0.40717873
 0.4075498  0.40809727 0.40821135 0.4077061  0.40651354 0.40498424
 0.4038695  0.40359172 0.40416035 0.4049056  0.40504202 0.40427765
 0.40275493 0.40096796 0.40035608 0.40218094 0.40549386 0.40613487]
