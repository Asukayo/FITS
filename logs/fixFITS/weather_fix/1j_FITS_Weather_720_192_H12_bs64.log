Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j192_H12', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j192_H12_FITS_custom_ftM_sl720_ll48_pl192_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35976
val 5079
test 10348
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=82, out_features=103, bias=True)
    (1): Linear(in_features=82, out_features=103, bias=True)
    (2): Linear(in_features=82, out_features=103, bias=True)
    (3): Linear(in_features=82, out_features=103, bias=True)
    (4): Linear(in_features=82, out_features=103, bias=True)
    (5): Linear(in_features=82, out_features=103, bias=True)
    (6): Linear(in_features=82, out_features=103, bias=True)
    (7): Linear(in_features=82, out_features=103, bias=True)
    (8): Linear(in_features=82, out_features=103, bias=True)
    (9): Linear(in_features=82, out_features=103, bias=True)
    (10): Linear(in_features=82, out_features=103, bias=True)
    (11): Linear(in_features=82, out_features=103, bias=True)
    (12): Linear(in_features=82, out_features=103, bias=True)
    (13): Linear(in_features=82, out_features=103, bias=True)
    (14): Linear(in_features=82, out_features=103, bias=True)
    (15): Linear(in_features=82, out_features=103, bias=True)
    (16): Linear(in_features=82, out_features=103, bias=True)
    (17): Linear(in_features=82, out_features=103, bias=True)
    (18): Linear(in_features=82, out_features=103, bias=True)
    (19): Linear(in_features=82, out_features=103, bias=True)
    (20): Linear(in_features=82, out_features=103, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  22702848.0
params:  179529.0
Trainable parameters:  179529
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4727727
	speed: 0.2038s/iter; left time: 2842.8370s
	iters: 200, epoch: 1 | loss: 0.3902331
	speed: 0.1884s/iter; left time: 2609.6111s
Epoch: 1 cost time: 54.759745836257935
Epoch: 1, Steps: 281 | Train Loss: 0.5678596 Vali Loss: 0.4768935 Test Loss: 0.2127181
Validation loss decreased (inf --> 0.476893).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3868871
	speed: 0.6265s/iter; left time: 8564.5736s
	iters: 200, epoch: 2 | loss: 0.5116315
	speed: 0.1493s/iter; left time: 2026.3848s
Epoch: 2 cost time: 42.57792139053345
Epoch: 2, Steps: 281 | Train Loss: 0.4648000 Vali Loss: 0.4578006 Test Loss: 0.2013487
Validation loss decreased (0.476893 --> 0.457801).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4283561
	speed: 0.5173s/iter; left time: 6926.3497s
	iters: 200, epoch: 3 | loss: 0.6174361
	speed: 0.1467s/iter; left time: 1949.1946s
Epoch: 3 cost time: 41.90018677711487
Epoch: 3, Steps: 281 | Train Loss: 0.4552712 Vali Loss: 0.4517490 Test Loss: 0.1964053
Validation loss decreased (0.457801 --> 0.451749).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3452342
	speed: 0.5342s/iter; left time: 7001.7305s
	iters: 200, epoch: 4 | loss: 0.3274578
	speed: 0.1397s/iter; left time: 1816.8767s
Epoch: 4 cost time: 40.70486068725586
Epoch: 4, Steps: 281 | Train Loss: 0.4513751 Vali Loss: 0.4433421 Test Loss: 0.1940917
Validation loss decreased (0.451749 --> 0.443342).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3894090
	speed: 0.5496s/iter; left time: 7049.9328s
	iters: 200, epoch: 5 | loss: 0.3715720
	speed: 0.1495s/iter; left time: 1902.5203s
Epoch: 5 cost time: 43.54398322105408
Epoch: 5, Steps: 281 | Train Loss: 0.4492695 Vali Loss: 0.4449895 Test Loss: 0.1920441
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4621682
	speed: 0.5440s/iter; left time: 6825.1245s
	iters: 200, epoch: 6 | loss: 0.3793473
	speed: 0.1544s/iter; left time: 1922.1545s
Epoch: 6 cost time: 42.94260501861572
Epoch: 6, Steps: 281 | Train Loss: 0.4477154 Vali Loss: 0.4423044 Test Loss: 0.1910078
Validation loss decreased (0.443342 --> 0.442304).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5181690
	speed: 0.5572s/iter; left time: 6834.4289s
	iters: 200, epoch: 7 | loss: 0.4167084
	speed: 0.1461s/iter; left time: 1777.0157s
Epoch: 7 cost time: 42.45597171783447
Epoch: 7, Steps: 281 | Train Loss: 0.4466208 Vali Loss: 0.4414836 Test Loss: 0.1903783
Validation loss decreased (0.442304 --> 0.441484).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3545015
	speed: 0.5939s/iter; left time: 7117.6799s
	iters: 200, epoch: 8 | loss: 0.3462353
	speed: 0.1682s/iter; left time: 1998.8635s
Epoch: 8 cost time: 48.48077654838562
Epoch: 8, Steps: 281 | Train Loss: 0.4458439 Vali Loss: 0.4394814 Test Loss: 0.1892947
Validation loss decreased (0.441484 --> 0.439481).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3889180
	speed: 0.6132s/iter; left time: 7176.4793s
	iters: 200, epoch: 9 | loss: 0.5054641
	speed: 0.1688s/iter; left time: 1958.2718s
Epoch: 9 cost time: 47.57592844963074
Epoch: 9, Steps: 281 | Train Loss: 0.4452279 Vali Loss: 0.4381330 Test Loss: 0.1889530
Validation loss decreased (0.439481 --> 0.438133).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4400297
	speed: 0.5656s/iter; left time: 6459.9701s
	iters: 200, epoch: 10 | loss: 0.6122032
	speed: 0.1548s/iter; left time: 1752.6769s
Epoch: 10 cost time: 44.649216413497925
Epoch: 10, Steps: 281 | Train Loss: 0.4447268 Vali Loss: 0.4368062 Test Loss: 0.1883279
Validation loss decreased (0.438133 --> 0.436806).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3958783
	speed: 0.5782s/iter; left time: 6441.9364s
	iters: 200, epoch: 11 | loss: 0.3620682
	speed: 0.1555s/iter; left time: 1716.8641s
Epoch: 11 cost time: 44.697585582733154
Epoch: 11, Steps: 281 | Train Loss: 0.4443253 Vali Loss: 0.4367498 Test Loss: 0.1883076
Validation loss decreased (0.436806 --> 0.436750).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4882025
	speed: 0.5579s/iter; left time: 6059.1687s
	iters: 200, epoch: 12 | loss: 0.3581063
	speed: 0.1444s/iter; left time: 1553.6058s
Epoch: 12 cost time: 42.39099740982056
Epoch: 12, Steps: 281 | Train Loss: 0.4440303 Vali Loss: 0.4383876 Test Loss: 0.1879333
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3891537
	speed: 0.5022s/iter; left time: 5313.2394s
	iters: 200, epoch: 13 | loss: 0.3123401
	speed: 0.1315s/iter; left time: 1377.7631s
Epoch: 13 cost time: 37.4642539024353
Epoch: 13, Steps: 281 | Train Loss: 0.4436477 Vali Loss: 0.4355219 Test Loss: 0.1877373
Validation loss decreased (0.436750 --> 0.435522).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5895063
	speed: 0.4620s/iter; left time: 4757.8409s
	iters: 200, epoch: 14 | loss: 0.5736438
	speed: 0.1424s/iter; left time: 1452.1736s
Epoch: 14 cost time: 39.23362326622009
Epoch: 14, Steps: 281 | Train Loss: 0.4434169 Vali Loss: 0.4353561 Test Loss: 0.1876901
Validation loss decreased (0.435522 --> 0.435356).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5643356
	speed: 0.5496s/iter; left time: 5505.0882s
	iters: 200, epoch: 15 | loss: 0.4060305
	speed: 0.1572s/iter; left time: 1559.2111s
Epoch: 15 cost time: 44.69293022155762
Epoch: 15, Steps: 281 | Train Loss: 0.4432089 Vali Loss: 0.4366461 Test Loss: 0.1872659
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3958503
	speed: 0.5616s/iter; left time: 5467.9384s
	iters: 200, epoch: 16 | loss: 0.4850221
	speed: 0.1479s/iter; left time: 1424.8994s
Epoch: 16 cost time: 43.70462894439697
Epoch: 16, Steps: 281 | Train Loss: 0.4429998 Vali Loss: 0.4360009 Test Loss: 0.1871648
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4461940
	speed: 0.5567s/iter; left time: 5263.1790s
	iters: 200, epoch: 17 | loss: 0.4550490
	speed: 0.1588s/iter; left time: 1485.3516s
Epoch: 17 cost time: 46.323883056640625
Epoch: 17, Steps: 281 | Train Loss: 0.4427818 Vali Loss: 0.4362686 Test Loss: 0.1870057
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j192_H12_FITS_custom_ftM_sl720_ll48_pl192_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10348
mse:0.18795354664325714, mae:0.23972374200820923, rse:0.5706809759140015, corr:[0.47653273 0.47783893 0.4768721  0.47588897 0.47557026 0.4757638
 0.47601798 0.47583586 0.47498566 0.47365013 0.47229105 0.47136092
 0.4708932  0.47066504 0.47041702 0.46987385 0.46907622 0.46806186
 0.46715483 0.46645683 0.46602824 0.46569854 0.46528497 0.4645447
 0.46347296 0.46222663 0.46104857 0.4601079  0.45953873 0.45919695
 0.4589132  0.45846367 0.45782435 0.45700714 0.45623502 0.45559248
 0.455214   0.45494446 0.45462117 0.45411566 0.45343474 0.45266113
 0.45188898 0.45121577 0.4506884  0.45026657 0.44995198 0.44961897
 0.4490599  0.44831783 0.44750798 0.4466463  0.44596103 0.44544894
 0.44510752 0.4447875  0.44440192 0.44388542 0.44327712 0.4426557
 0.44214064 0.4417846  0.44159716 0.44151986 0.44142362 0.44122362
 0.44085765 0.44036117 0.43983543 0.43937185 0.43906173 0.4387996
 0.43864974 0.43852594 0.43830755 0.43800613 0.43766612 0.4374085
 0.43723464 0.4371314  0.43711856 0.4370758  0.43688112 0.4365355
 0.4360696  0.43553007 0.4350053  0.43448368 0.4341408  0.43396175
 0.43396664 0.43409598 0.4341807  0.43419373 0.43412676 0.4339638
 0.4337416  0.4335033  0.43324772 0.43302348 0.43282524 0.43263462
 0.43243262 0.43220106 0.431942   0.43167964 0.43139616 0.43115702
 0.43090728 0.43068185 0.4304351  0.43012747 0.42974845 0.42940032
 0.42909113 0.42879942 0.42859027 0.42847475 0.4283958  0.4282769
 0.42815444 0.4279999  0.42778948 0.4275506  0.42728007 0.42700064
 0.42670622 0.42637295 0.42603382 0.42571327 0.42540595 0.42513382
 0.42491743 0.4247339  0.424606   0.42445284 0.42426518 0.4240519
 0.4237243  0.42339957 0.42312625 0.42295638 0.42284355 0.4227387
 0.42262104 0.42245856 0.4222554  0.4220004  0.4216526  0.42117733
 0.42062724 0.42007425 0.41947246 0.41878843 0.41810453 0.41747642
 0.41706836 0.41676638 0.41659772 0.41646853 0.41628107 0.41597039
 0.41550744 0.41498786 0.41443163 0.41396105 0.41356978 0.41327798
 0.41298327 0.41255745 0.41190454 0.4110797  0.4102492  0.40962076
 0.4092344  0.40909916 0.4089998  0.40873772 0.40813875 0.40723166
 0.40628353 0.40553656 0.40526828 0.4055274  0.40599564 0.40616095
 0.40566742 0.40458572 0.40345547 0.40301841 0.40373603 0.4042134 ]
