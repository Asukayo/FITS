Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=70, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j96_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36072
val 5175
test 10444
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=79, bias=True)
    (1): Linear(in_features=70, out_features=79, bias=True)
    (2): Linear(in_features=70, out_features=79, bias=True)
    (3): Linear(in_features=70, out_features=79, bias=True)
    (4): Linear(in_features=70, out_features=79, bias=True)
    (5): Linear(in_features=70, out_features=79, bias=True)
    (6): Linear(in_features=70, out_features=79, bias=True)
    (7): Linear(in_features=70, out_features=79, bias=True)
    (8): Linear(in_features=70, out_features=79, bias=True)
    (9): Linear(in_features=70, out_features=79, bias=True)
    (10): Linear(in_features=70, out_features=79, bias=True)
    (11): Linear(in_features=70, out_features=79, bias=True)
    (12): Linear(in_features=70, out_features=79, bias=True)
    (13): Linear(in_features=70, out_features=79, bias=True)
    (14): Linear(in_features=70, out_features=79, bias=True)
    (15): Linear(in_features=70, out_features=79, bias=True)
    (16): Linear(in_features=70, out_features=79, bias=True)
    (17): Linear(in_features=70, out_features=79, bias=True)
    (18): Linear(in_features=70, out_features=79, bias=True)
    (19): Linear(in_features=70, out_features=79, bias=True)
    (20): Linear(in_features=70, out_features=79, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  29729280.0
params:  117789.0
Trainable parameters:  117789
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4101170
	speed: 0.1692s/iter; left time: 1167.5473s
Epoch: 1 cost time: 23.507895708084106
Epoch: 1, Steps: 140 | Train Loss: 0.5619397 Vali Loss: 0.4386893 Test Loss: 0.1798626
Validation loss decreased (inf --> 0.438689).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4646202
	speed: 0.3982s/iter; left time: 2692.5503s
Epoch: 2 cost time: 25.177191019058228
Epoch: 2, Steps: 140 | Train Loss: 0.4249296 Vali Loss: 0.4096805 Test Loss: 0.1638185
Validation loss decreased (0.438689 --> 0.409681).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4088149
	speed: 0.4172s/iter; left time: 2762.3358s
Epoch: 3 cost time: 25.359490871429443
Epoch: 3, Steps: 140 | Train Loss: 0.4098484 Vali Loss: 0.3998715 Test Loss: 0.1575053
Validation loss decreased (0.409681 --> 0.399871).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6130679
	speed: 0.4218s/iter; left time: 2733.6185s
Epoch: 4 cost time: 24.926889896392822
Epoch: 4, Steps: 140 | Train Loss: 0.4032381 Vali Loss: 0.3943595 Test Loss: 0.1537517
Validation loss decreased (0.399871 --> 0.394359).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4060515
	speed: 0.3196s/iter; left time: 2026.7442s
Epoch: 5 cost time: 19.0840425491333
Epoch: 5, Steps: 140 | Train Loss: 0.4006428 Vali Loss: 0.3905351 Test Loss: 0.1515258
Validation loss decreased (0.394359 --> 0.390535).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4201601
	speed: 0.3420s/iter; left time: 2120.4548s
Epoch: 6 cost time: 18.360839128494263
Epoch: 6, Steps: 140 | Train Loss: 0.3981265 Vali Loss: 0.3892744 Test Loss: 0.1498198
Validation loss decreased (0.390535 --> 0.389274).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4732694
	speed: 0.2873s/iter; left time: 1741.2178s
Epoch: 7 cost time: 18.484558582305908
Epoch: 7, Steps: 140 | Train Loss: 0.3963566 Vali Loss: 0.3863071 Test Loss: 0.1488295
Validation loss decreased (0.389274 --> 0.386307).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3172235
	speed: 0.3376s/iter; left time: 1998.8087s
Epoch: 8 cost time: 20.040610551834106
Epoch: 8, Steps: 140 | Train Loss: 0.3950722 Vali Loss: 0.3856925 Test Loss: 0.1480915
Validation loss decreased (0.386307 --> 0.385692).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6462839
	speed: 0.3292s/iter; left time: 1903.1118s
Epoch: 9 cost time: 20.884708642959595
Epoch: 9, Steps: 140 | Train Loss: 0.3941543 Vali Loss: 0.3844410 Test Loss: 0.1473256
Validation loss decreased (0.385692 --> 0.384441).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5356315
	speed: 0.3371s/iter; left time: 1901.3348s
Epoch: 10 cost time: 21.033490896224976
Epoch: 10, Steps: 140 | Train Loss: 0.3931011 Vali Loss: 0.3853793 Test Loss: 0.1468922
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5010750
	speed: 0.3436s/iter; left time: 1890.1072s
Epoch: 11 cost time: 21.23034358024597
Epoch: 11, Steps: 140 | Train Loss: 0.3914163 Vali Loss: 0.3793578 Test Loss: 0.1465165
Validation loss decreased (0.384441 --> 0.379358).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2932819
	speed: 0.3460s/iter; left time: 1854.8026s
Epoch: 12 cost time: 21.428975820541382
Epoch: 12, Steps: 140 | Train Loss: 0.3913818 Vali Loss: 0.3847784 Test Loss: 0.1463173
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3072329
	speed: 0.3534s/iter; left time: 1845.0188s
Epoch: 13 cost time: 22.17210602760315
Epoch: 13, Steps: 140 | Train Loss: 0.3911798 Vali Loss: 0.3817671 Test Loss: 0.1459949
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2828017
	speed: 0.3742s/iter; left time: 1901.5279s
Epoch: 14 cost time: 22.52569007873535
Epoch: 14, Steps: 140 | Train Loss: 0.3912503 Vali Loss: 0.3817515 Test Loss: 0.1457415
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10444
mse:0.1470300257205963, mae:0.2017160803079605, rse:0.5053068995475769, corr:[0.47357655 0.47743586 0.47853655 0.4779286  0.476683   0.47557485
 0.47496158 0.47479892 0.4747683  0.47454086 0.47396922 0.47316656
 0.47228447 0.47150755 0.47097063 0.47059175 0.4702915  0.4698409
 0.46919242 0.4682654  0.46720123 0.46615812 0.4653315  0.4647108
 0.4642487  0.4638089  0.46323538 0.46241084 0.461396   0.4602574
 0.4591852  0.45832735 0.45783255 0.45759967 0.45754066 0.4573936
 0.4571032  0.45655662 0.4557837  0.45488548 0.45404822 0.45342398
 0.45303243 0.45278022 0.45254278 0.45218047 0.45170203 0.45110783
 0.4502982  0.4494661  0.44875833 0.44815516 0.44775897 0.44747698
 0.4472014  0.4468177  0.44630355 0.44565734 0.44495735 0.44426864
 0.44367912 0.44321227 0.44289052 0.4426633  0.442456   0.4422312
 0.4419326  0.44156802 0.44115824 0.4407154  0.44031006 0.43986368
 0.43949276 0.43918514 0.43884766 0.4384578  0.43801266 0.43757355
 0.4371187  0.43663165 0.4361988  0.4357899  0.43540037 0.4351006
 0.4349468  0.434849   0.43468258 0.43426532 0.43366724 0.43287706
 0.432064   0.43144172 0.43116435 0.4312276  0.43116385 0.4302112 ]
