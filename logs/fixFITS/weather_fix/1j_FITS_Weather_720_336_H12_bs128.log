Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j336_H12', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j336_H12_FITS_custom_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35832
val 4935
test 10204
Model(
  (freq_upsampler): Linear(in_features=82, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  52899840.0
params:  9960.0
Trainable parameters:  9960
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6677004
	speed: 0.1731s/iter; left time: 1185.5777s
Epoch: 1 cost time: 23.891595602035522
Epoch: 1, Steps: 139 | Train Loss: 0.6963901 Vali Loss: 0.5929664 Test Loss: 0.2847919
Validation loss decreased (inf --> 0.592966).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5302255
	speed: 0.3760s/iter; left time: 2523.7875s
Epoch: 2 cost time: 22.117189645767212
Epoch: 2, Steps: 139 | Train Loss: 0.5684380 Vali Loss: 0.5584350 Test Loss: 0.2716005
Validation loss decreased (0.592966 --> 0.558435).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6090387
	speed: 0.3774s/iter; left time: 2480.7938s
Epoch: 3 cost time: 22.61527705192566
Epoch: 3, Steps: 139 | Train Loss: 0.5446544 Vali Loss: 0.5473693 Test Loss: 0.2663269
Validation loss decreased (0.558435 --> 0.547369).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5375834
	speed: 0.3756s/iter; left time: 2416.4720s
Epoch: 4 cost time: 22.00443935394287
Epoch: 4, Steps: 139 | Train Loss: 0.5379636 Vali Loss: 0.5433462 Test Loss: 0.2637515
Validation loss decreased (0.547369 --> 0.543346).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5548970
	speed: 0.3746s/iter; left time: 2358.3955s
Epoch: 5 cost time: 22.906964778900146
Epoch: 5, Steps: 139 | Train Loss: 0.5350101 Vali Loss: 0.5409639 Test Loss: 0.2621402
Validation loss decreased (0.543346 --> 0.540964).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5507994
	speed: 0.3842s/iter; left time: 2365.3161s
Epoch: 6 cost time: 21.017054319381714
Epoch: 6, Steps: 139 | Train Loss: 0.5342745 Vali Loss: 0.5396953 Test Loss: 0.2612124
Validation loss decreased (0.540964 --> 0.539695).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4950604
	speed: 0.3364s/iter; left time: 2024.1097s
Epoch: 7 cost time: 19.910865783691406
Epoch: 7, Steps: 139 | Train Loss: 0.5335306 Vali Loss: 0.5369199 Test Loss: 0.2603737
Validation loss decreased (0.539695 --> 0.536920).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6180828
	speed: 0.3256s/iter; left time: 1913.8527s
Epoch: 8 cost time: 17.439009428024292
Epoch: 8, Steps: 139 | Train Loss: 0.5326654 Vali Loss: 0.5371882 Test Loss: 0.2598986
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4869971
	speed: 0.3290s/iter; left time: 1887.9428s
Epoch: 9 cost time: 21.843664407730103
Epoch: 9, Steps: 139 | Train Loss: 0.5324966 Vali Loss: 0.5372811 Test Loss: 0.2595771
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4698275
	speed: 0.3744s/iter; left time: 2096.3969s
Epoch: 10 cost time: 22.00493097305298
Epoch: 10, Steps: 139 | Train Loss: 0.5323939 Vali Loss: 0.5373764 Test Loss: 0.2592219
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j336_H12_FITS_custom_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10204
mse:0.26075422763824463, mae:0.2973363995552063, rse:0.6706581115722656, corr:[0.4753472  0.48238045 0.48476005 0.48348734 0.48111105 0.47984844
 0.48012647 0.48116374 0.48195398 0.48193565 0.4811102  0.479896
 0.47884187 0.47836563 0.47842965 0.47861186 0.47853374 0.47788215
 0.47686592 0.47571623 0.47478384 0.4742615  0.47409633 0.47394767
 0.47353283 0.4727128  0.47155616 0.4701972  0.46898174 0.46806082
 0.46752733 0.4671838  0.46683788 0.46624422 0.4654449  0.46449745
 0.46361107 0.46280324 0.4621543  0.46163866 0.46120802 0.46078998
 0.4602377  0.45956305 0.45884126 0.45812437 0.4575242  0.45705727
 0.45656425 0.45606953 0.45556834 0.4550061  0.45451227 0.45404792
 0.45361212 0.45319647 0.45285878 0.45252475 0.45218024 0.45179522
 0.4513909  0.45096037 0.45054737 0.450144   0.4497757  0.4494656
 0.44910192 0.4487428  0.44839013 0.44807836 0.44782937 0.44753093
 0.44724607 0.4468817  0.44634894 0.44576263 0.4452396  0.44487125
 0.44460404 0.44439057 0.4443082  0.44428694 0.44415763 0.44389048
 0.4435217  0.44309917 0.44278634 0.44249675 0.44229656 0.4420884
 0.44192943 0.44180074 0.44163117 0.4414529  0.44122797 0.44095603
 0.4406722  0.44039652 0.44012105 0.43984127 0.43953854 0.4392127
 0.43889496 0.4385684  0.43824613 0.43793684 0.43757397 0.4372284
 0.43687075 0.4365131  0.43612972 0.4357032  0.4352182  0.43480554
 0.43443835 0.43407357 0.43374455 0.43343577 0.43309343 0.4326698
 0.43226904 0.43191972 0.4316323  0.43138307 0.43114424 0.430875
 0.4305119  0.4300419  0.42950878 0.4289648  0.42844874 0.42802456
 0.4277144  0.42747906 0.42729002 0.42703944 0.42667818 0.42626253
 0.42575344 0.4253034  0.4249502  0.42469755 0.42447245 0.42419475
 0.4238212  0.42335254 0.42283553 0.42232597 0.42179987 0.42123434
 0.42066973 0.42008722 0.41940606 0.41855732 0.4175701  0.41655526
 0.41578636 0.41513327 0.4146639  0.41425472 0.4139474  0.41355258
 0.41307074 0.4125427  0.41198125 0.4114672  0.41092464 0.41040263
 0.4098974  0.4092876  0.40857232 0.407809   0.4070741  0.4065406
 0.40610445 0.4058337  0.40562236 0.40542915 0.40509865 0.40460563
 0.40406105 0.40342724 0.4028064  0.40226665 0.40185106 0.40155956
 0.40132743 0.40102    0.40069175 0.4002619  0.39975607 0.39919987
 0.3987636  0.3982799  0.39789557 0.3975723  0.3972747  0.39699554
 0.3967345  0.3965032  0.39629573 0.3960894  0.39580932 0.39539844
 0.39495593 0.39455044 0.39419395 0.39386848 0.3935095  0.39319262
 0.3928074  0.39236492 0.391855   0.3913718  0.39091766 0.3904756
 0.39011607 0.38988715 0.3896696  0.38930157 0.38884276 0.3884024
 0.38796255 0.38759372 0.38732764 0.38711402 0.38699508 0.3869685
 0.3869306  0.3868072  0.386542   0.38618004 0.3857158  0.38521034
 0.38475287 0.38441324 0.3841317  0.38395038 0.38366213 0.38340828
 0.3832089  0.38308203 0.3829937  0.38297203 0.38305792 0.38317034
 0.38320556 0.3832508  0.38313118 0.38301623 0.38282266 0.3826348
 0.38238212 0.3820689  0.3817655  0.38141167 0.381064   0.38070244
 0.3804737  0.38035598 0.38015932 0.37998465 0.37982208 0.37960663
 0.37943596 0.37923416 0.37906867 0.37893948 0.3788052  0.37863272
 0.37836626 0.3780109  0.37767234 0.377247   0.3768292  0.37643883
 0.37604958 0.37562945 0.37523994 0.37483612 0.3744259  0.3740453
 0.3737319  0.3733697  0.37296262 0.37246984 0.3718806  0.37118086
 0.37040824 0.36965623 0.36894137 0.36826682 0.36770964 0.36720815
 0.36660644 0.3658961  0.3650573  0.3641627  0.3632623  0.3625795
 0.3619332  0.36131588 0.36066717 0.3599204  0.35905758 0.35806432
 0.35709503 0.3561684  0.35564592 0.35543072 0.35520256 0.3548165
 0.3542792  0.35352153 0.3526831  0.35202134 0.35169753 0.35185832
 0.35240826 0.35271406 0.35242632 0.35153052 0.35012347 0.34852314
 0.3472785  0.34703648 0.34758368 0.34837177 0.34862795 0.3478531
 0.34632608 0.34486526 0.34493616 0.34788093 0.3529527  0.35709354]
