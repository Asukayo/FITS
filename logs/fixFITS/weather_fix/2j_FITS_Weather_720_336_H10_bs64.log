Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=70, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j336_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35832
val 4935
test 10204
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=102, bias=True)
    (1): Linear(in_features=70, out_features=102, bias=True)
    (2): Linear(in_features=70, out_features=102, bias=True)
    (3): Linear(in_features=70, out_features=102, bias=True)
    (4): Linear(in_features=70, out_features=102, bias=True)
    (5): Linear(in_features=70, out_features=102, bias=True)
    (6): Linear(in_features=70, out_features=102, bias=True)
    (7): Linear(in_features=70, out_features=102, bias=True)
    (8): Linear(in_features=70, out_features=102, bias=True)
    (9): Linear(in_features=70, out_features=102, bias=True)
    (10): Linear(in_features=70, out_features=102, bias=True)
    (11): Linear(in_features=70, out_features=102, bias=True)
    (12): Linear(in_features=70, out_features=102, bias=True)
    (13): Linear(in_features=70, out_features=102, bias=True)
    (14): Linear(in_features=70, out_features=102, bias=True)
    (15): Linear(in_features=70, out_features=102, bias=True)
    (16): Linear(in_features=70, out_features=102, bias=True)
    (17): Linear(in_features=70, out_features=102, bias=True)
    (18): Linear(in_features=70, out_features=102, bias=True)
    (19): Linear(in_features=70, out_features=102, bias=True)
    (20): Linear(in_features=70, out_features=102, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19192320.0
params:  152082.0
Trainable parameters:  152082
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6795468
	speed: 0.2156s/iter; left time: 2986.0344s
	iters: 200, epoch: 1 | loss: 0.4556725
	speed: 0.2050s/iter; left time: 2818.6234s
Epoch: 1 cost time: 56.708757638931274
Epoch: 1, Steps: 279 | Train Loss: 0.5982081 Vali Loss: 0.6254039 Test Loss: 0.2875929
Validation loss decreased (inf --> 0.625404).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3381487
	speed: 0.6840s/iter; left time: 9283.7618s
	iters: 200, epoch: 2 | loss: 0.3597684
	speed: 0.1895s/iter; left time: 2552.2878s
Epoch: 2 cost time: 52.590080976486206
Epoch: 2, Steps: 279 | Train Loss: 0.3880547 Vali Loss: 0.5733830 Test Loss: 0.2676772
Validation loss decreased (0.625404 --> 0.573383).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2935389
	speed: 0.6476s/iter; left time: 8608.8153s
	iters: 200, epoch: 3 | loss: 0.2678004
	speed: 0.1787s/iter; left time: 2357.7386s
Epoch: 3 cost time: 49.94767999649048
Epoch: 3, Steps: 279 | Train Loss: 0.3127797 Vali Loss: 0.5492912 Test Loss: 0.2583290
Validation loss decreased (0.573383 --> 0.549291).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2644285
	speed: 0.6134s/iter; left time: 7982.7311s
	iters: 200, epoch: 4 | loss: 0.3335653
	speed: 0.1719s/iter; left time: 2219.5162s
Epoch: 4 cost time: 47.158215045928955
Epoch: 4, Steps: 279 | Train Loss: 0.2746675 Vali Loss: 0.5344307 Test Loss: 0.2524785
Validation loss decreased (0.549291 --> 0.534431).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2640876
	speed: 0.5905s/iter; left time: 7519.7081s
	iters: 200, epoch: 5 | loss: 0.2980056
	speed: 0.1738s/iter; left time: 2195.7782s
Epoch: 5 cost time: 49.2351348400116
Epoch: 5, Steps: 279 | Train Loss: 0.2546628 Vali Loss: 0.5219295 Test Loss: 0.2488863
Validation loss decreased (0.534431 --> 0.521930).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2638341
	speed: 0.6237s/iter; left time: 7768.5377s
	iters: 200, epoch: 6 | loss: 0.2458772
	speed: 0.1749s/iter; left time: 2161.5078s
Epoch: 6 cost time: 50.1290328502655
Epoch: 6, Steps: 279 | Train Loss: 0.2436949 Vali Loss: 0.5173500 Test Loss: 0.2464161
Validation loss decreased (0.521930 --> 0.517350).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2468669
	speed: 0.6349s/iter; left time: 7731.1232s
	iters: 200, epoch: 7 | loss: 0.2110553
	speed: 0.1783s/iter; left time: 2153.1274s
Epoch: 7 cost time: 51.07984662055969
Epoch: 7, Steps: 279 | Train Loss: 0.2379403 Vali Loss: 0.5131648 Test Loss: 0.2450046
Validation loss decreased (0.517350 --> 0.513165).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2825461
	speed: 0.6158s/iter; left time: 7326.3867s
	iters: 200, epoch: 8 | loss: 0.2846102
	speed: 0.1830s/iter; left time: 2159.0574s
Epoch: 8 cost time: 50.41253662109375
Epoch: 8, Steps: 279 | Train Loss: 0.2351268 Vali Loss: 0.5124661 Test Loss: 0.2442657
Validation loss decreased (0.513165 --> 0.512466).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2668991
	speed: 0.6185s/iter; left time: 7186.4607s
	iters: 200, epoch: 9 | loss: 0.2157180
	speed: 0.1864s/iter; left time: 2147.2220s
Epoch: 9 cost time: 51.81930112838745
Epoch: 9, Steps: 279 | Train Loss: 0.2335268 Vali Loss: 0.5099232 Test Loss: 0.2437172
Validation loss decreased (0.512466 --> 0.509923).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1941109
	speed: 0.6946s/iter; left time: 7877.0883s
	iters: 200, epoch: 10 | loss: 0.2562625
	speed: 0.1741s/iter; left time: 1956.6484s
Epoch: 10 cost time: 50.51226758956909
Epoch: 10, Steps: 279 | Train Loss: 0.2326600 Vali Loss: 0.5092685 Test Loss: 0.2433461
Validation loss decreased (0.509923 --> 0.509268).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2641234
	speed: 0.6001s/iter; left time: 6637.8492s
	iters: 200, epoch: 11 | loss: 0.2508061
	speed: 0.1664s/iter; left time: 1823.9716s
Epoch: 11 cost time: 48.44282937049866
Epoch: 11, Steps: 279 | Train Loss: 0.2322465 Vali Loss: 0.5093595 Test Loss: 0.2429498
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2419526
	speed: 0.6308s/iter; left time: 6801.2676s
	iters: 200, epoch: 12 | loss: 0.2329726
	speed: 0.1774s/iter; left time: 1894.6557s
Epoch: 12 cost time: 50.85694718360901
Epoch: 12, Steps: 279 | Train Loss: 0.2322010 Vali Loss: 0.5086849 Test Loss: 0.2428119
Validation loss decreased (0.509268 --> 0.508685).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2486787
	speed: 0.6266s/iter; left time: 6581.3641s
	iters: 200, epoch: 13 | loss: 0.2259735
	speed: 0.1816s/iter; left time: 1889.6891s
Epoch: 13 cost time: 51.526426553726196
Epoch: 13, Steps: 279 | Train Loss: 0.2320339 Vali Loss: 0.5077878 Test Loss: 0.2425698
Validation loss decreased (0.508685 --> 0.507788).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1746067
	speed: 0.6867s/iter; left time: 7020.4366s
	iters: 200, epoch: 14 | loss: 0.2679996
	speed: 0.1707s/iter; left time: 1728.4774s
Epoch: 14 cost time: 50.85721182823181
Epoch: 14, Steps: 279 | Train Loss: 0.2320952 Vali Loss: 0.5084394 Test Loss: 0.2424368
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2125904
	speed: 0.6391s/iter; left time: 6355.7006s
	iters: 200, epoch: 15 | loss: 0.2946117
	speed: 0.1740s/iter; left time: 1713.0621s
Epoch: 15 cost time: 52.543092250823975
Epoch: 15, Steps: 279 | Train Loss: 0.2317371 Vali Loss: 0.5094390 Test Loss: 0.2423377
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1913573
	speed: 0.6547s/iter; left time: 6327.9708s
	iters: 200, epoch: 16 | loss: 0.2079817
	speed: 0.1674s/iter; left time: 1601.4970s
Epoch: 16 cost time: 48.52747559547424
Epoch: 16, Steps: 279 | Train Loss: 0.2319025 Vali Loss: 0.5104902 Test Loss: 0.2421744
EarlyStopping counter: 3 out of 3
Early stopping
train 35832
val 4935
test 10204
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=102, bias=True)
    (1): Linear(in_features=70, out_features=102, bias=True)
    (2): Linear(in_features=70, out_features=102, bias=True)
    (3): Linear(in_features=70, out_features=102, bias=True)
    (4): Linear(in_features=70, out_features=102, bias=True)
    (5): Linear(in_features=70, out_features=102, bias=True)
    (6): Linear(in_features=70, out_features=102, bias=True)
    (7): Linear(in_features=70, out_features=102, bias=True)
    (8): Linear(in_features=70, out_features=102, bias=True)
    (9): Linear(in_features=70, out_features=102, bias=True)
    (10): Linear(in_features=70, out_features=102, bias=True)
    (11): Linear(in_features=70, out_features=102, bias=True)
    (12): Linear(in_features=70, out_features=102, bias=True)
    (13): Linear(in_features=70, out_features=102, bias=True)
    (14): Linear(in_features=70, out_features=102, bias=True)
    (15): Linear(in_features=70, out_features=102, bias=True)
    (16): Linear(in_features=70, out_features=102, bias=True)
    (17): Linear(in_features=70, out_features=102, bias=True)
    (18): Linear(in_features=70, out_features=102, bias=True)
    (19): Linear(in_features=70, out_features=102, bias=True)
    (20): Linear(in_features=70, out_features=102, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19192320.0
params:  152082.0
Trainable parameters:  152082
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6452054
	speed: 0.1648s/iter; left time: 2283.1031s
	iters: 200, epoch: 1 | loss: 0.5866678
	speed: 0.1559s/iter; left time: 2143.5315s
Epoch: 1 cost time: 45.6423180103302
Epoch: 1, Steps: 279 | Train Loss: 0.5005374 Vali Loss: 0.5040007 Test Loss: 0.2404147
Validation loss decreased (inf --> 0.504001).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5378718
	speed: 0.6043s/iter; left time: 8201.5768s
	iters: 200, epoch: 2 | loss: 0.3995764
	speed: 0.1378s/iter; left time: 1856.0652s
Epoch: 2 cost time: 41.48592400550842
Epoch: 2, Steps: 279 | Train Loss: 0.4988925 Vali Loss: 0.5044784 Test Loss: 0.2396685
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4204920
	speed: 0.5958s/iter; left time: 7919.9840s
	iters: 200, epoch: 3 | loss: 0.4661748
	speed: 0.1682s/iter; left time: 2219.5927s
Epoch: 3 cost time: 47.989219188690186
Epoch: 3, Steps: 279 | Train Loss: 0.4974816 Vali Loss: 0.5017123 Test Loss: 0.2387003
Validation loss decreased (0.504001 --> 0.501712).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5091556
	speed: 0.5953s/iter; left time: 7747.6547s
	iters: 200, epoch: 4 | loss: 0.4666114
	speed: 0.1639s/iter; left time: 2116.0292s
Epoch: 4 cost time: 46.43461990356445
Epoch: 4, Steps: 279 | Train Loss: 0.4975705 Vali Loss: 0.5025569 Test Loss: 0.2383265
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3781022
	speed: 0.5894s/iter; left time: 7506.0533s
	iters: 200, epoch: 5 | loss: 0.4720689
	speed: 0.1709s/iter; left time: 2159.2211s
Epoch: 5 cost time: 49.98826766014099
Epoch: 5, Steps: 279 | Train Loss: 0.4967627 Vali Loss: 0.5022831 Test Loss: 0.2381348
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5042000
	speed: 0.6548s/iter; left time: 8156.3509s
	iters: 200, epoch: 6 | loss: 0.4868293
	speed: 0.1861s/iter; left time: 2299.3844s
Epoch: 6 cost time: 54.2069411277771
Epoch: 6, Steps: 279 | Train Loss: 0.4966992 Vali Loss: 0.5011925 Test Loss: 0.2373956
Validation loss decreased (0.501712 --> 0.501193).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5114225
	speed: 0.7476s/iter; left time: 9103.0318s
	iters: 200, epoch: 7 | loss: 0.4994190
	speed: 0.2078s/iter; left time: 2509.4899s
Epoch: 7 cost time: 58.327781438827515
Epoch: 7, Steps: 279 | Train Loss: 0.4967139 Vali Loss: 0.5017242 Test Loss: 0.2373137
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4315304
	speed: 0.6955s/iter; left time: 8275.2511s
	iters: 200, epoch: 8 | loss: 0.5279666
	speed: 0.1774s/iter; left time: 2093.0876s
Epoch: 8 cost time: 50.5522882938385
Epoch: 8, Steps: 279 | Train Loss: 0.4957796 Vali Loss: 0.5010830 Test Loss: 0.2372150
Validation loss decreased (0.501193 --> 0.501083).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4779930
	speed: 0.6464s/iter; left time: 7510.0163s
	iters: 200, epoch: 9 | loss: 0.4139318
	speed: 0.1733s/iter; left time: 1995.9784s
Epoch: 9 cost time: 51.23665761947632
Epoch: 9, Steps: 279 | Train Loss: 0.4958702 Vali Loss: 0.5004045 Test Loss: 0.2371629
Validation loss decreased (0.501083 --> 0.500405).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6286138
	speed: 0.6493s/iter; left time: 7363.3948s
	iters: 200, epoch: 10 | loss: 0.5102573
	speed: 0.1845s/iter; left time: 2073.9360s
Epoch: 10 cost time: 52.14986491203308
Epoch: 10, Steps: 279 | Train Loss: 0.4962373 Vali Loss: 0.4975042 Test Loss: 0.2370397
Validation loss decreased (0.500405 --> 0.497504).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4655166
	speed: 0.6344s/iter; left time: 7017.5053s
	iters: 200, epoch: 11 | loss: 0.4907697
	speed: 0.1800s/iter; left time: 1973.2133s
Epoch: 11 cost time: 50.721808195114136
Epoch: 11, Steps: 279 | Train Loss: 0.4955344 Vali Loss: 0.4999087 Test Loss: 0.2366482
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5536576
	speed: 0.6586s/iter; left time: 7101.3060s
	iters: 200, epoch: 12 | loss: 0.4089912
	speed: 0.1855s/iter; left time: 1981.6606s
Epoch: 12 cost time: 52.98187470436096
Epoch: 12, Steps: 279 | Train Loss: 0.4951222 Vali Loss: 0.4989623 Test Loss: 0.2366594
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5376366
	speed: 0.6603s/iter; left time: 6935.1638s
	iters: 200, epoch: 13 | loss: 0.4570228
	speed: 0.1874s/iter; left time: 1949.1764s
Epoch: 13 cost time: 54.949766397476196
Epoch: 13, Steps: 279 | Train Loss: 0.4955434 Vali Loss: 0.4997412 Test Loss: 0.2366197
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10204
mse:0.23747506737709045, mae:0.2787705063819885, rse:0.6400214433670044, corr:[0.47072768 0.47443914 0.47505394 0.47420415 0.47293907 0.4719526
 0.4715713  0.47169563 0.47190657 0.47171965 0.47090274 0.46964568
 0.4682868  0.46719518 0.46657112 0.4662989  0.46627727 0.46621698
 0.46598932 0.46542728 0.46459234 0.46357933 0.4625855  0.46165946
 0.46087164 0.46021122 0.45959753 0.458904   0.45807758 0.45706353
 0.45597926 0.45493752 0.454132   0.45355213 0.45320216 0.4529267
 0.45270526 0.45239016 0.45193076 0.4513321  0.4506966  0.45014045
 0.44967902 0.44925922 0.44882333 0.44829565 0.44771114 0.44707024
 0.44624153 0.44537348 0.44458243 0.44384208 0.44328788 0.44287136
 0.44252995 0.44217607 0.4417774  0.44129986 0.4407803  0.44024825
 0.43975723 0.43935198 0.43905193 0.4388555  0.4387182  0.43859693
 0.43841723 0.4381735  0.437875   0.43753365 0.4372171  0.43685627
 0.4365582  0.43630427 0.43602324 0.4357142  0.4353832  0.43507594
 0.434751   0.43441203 0.43412974 0.4338893  0.43367562 0.43350643
 0.43340382 0.43331623 0.4332236  0.43300468 0.43274716 0.43242925
 0.43212512 0.4318833  0.431673   0.4315287  0.4314522  0.43139055
 0.43130633 0.43117645 0.43097186 0.43073294 0.43047923 0.43024692
 0.43006098 0.42990214 0.42974547 0.42957178 0.4293318  0.42905247
 0.4287035  0.42835236 0.4280036  0.42765087 0.42732188 0.42709586
 0.42694595 0.4268287  0.4267472  0.4266791  0.42656824 0.42634955
 0.4260839  0.42578456 0.42547166 0.42518467 0.4249185  0.42468566
 0.42447257 0.4242363  0.423984   0.4237061  0.42337656 0.42303362
 0.42271414 0.42242563 0.4221949  0.4219728  0.42174643 0.4215137
 0.42117697 0.4208312  0.42050105 0.42022872 0.41998905 0.41977406
 0.41959736 0.4194507  0.4193366  0.41922233 0.41902503 0.41867042
 0.4181718  0.41759858 0.4169704  0.41634145 0.41582796 0.41539124
 0.4150912  0.41474593 0.41437942 0.4139838  0.4135894  0.41320604
 0.41282317 0.41247347 0.41212398 0.41179103 0.41142523 0.41104788
 0.4106582  0.41025338 0.4098112  0.40933663 0.4088613  0.40841025
 0.40794462 0.40747356 0.40697256 0.40646294 0.40592265 0.4053483
 0.4047995  0.4042464  0.40369037 0.40315273 0.4026671  0.40222862
 0.4018116  0.40137008 0.40092024 0.40045595 0.39999375 0.3995585
 0.39921704 0.39889827 0.39864653 0.3984187  0.39816004 0.39782658
 0.39739    0.39685595 0.3962581  0.39564717 0.39506742 0.39455244
 0.39418495 0.39396712 0.39386547 0.39381695 0.3936846  0.39345527
 0.393081   0.3925948  0.39201102 0.39141798 0.39086315 0.39037576
 0.3899373  0.3896361  0.3893404  0.38897508 0.38853592 0.38806644
 0.38755852 0.3870465  0.38659832 0.38624874 0.3860256  0.385945
 0.38592684 0.385918   0.3858128  0.38553405 0.38504845 0.38439152
 0.38368    0.38303024 0.38247097 0.3820636  0.38174498 0.38154322
 0.38140678 0.38126037 0.38099274 0.38065082 0.38027933 0.37989897
 0.3795045  0.3791817  0.37890276 0.3786845  0.37848553 0.37828842
 0.3780009  0.3775944  0.37712264 0.37657255 0.37609467 0.37570348
 0.37552014 0.3754526  0.3753613  0.3752519  0.37507015 0.37478194
 0.37444672 0.37404716 0.37370566 0.3734563  0.37330967 0.3732334
 0.37315387 0.3730258  0.37284186 0.3725108  0.37207833 0.37158456
 0.37109527 0.370613   0.37024587 0.36997706 0.3697664  0.36960882
 0.36947277 0.3692273  0.36888874 0.3684663  0.36800647 0.36752954
 0.36708072 0.36666125 0.36622223 0.36571625 0.36522236 0.36469704
 0.36406943 0.3634042  0.36274248 0.36216143 0.36163583 0.36125988
 0.36083865 0.36039448 0.3598733  0.35928228 0.3586193  0.3579412
 0.35734057 0.35687172 0.35657355 0.3563575  0.35620937 0.3559594
 0.35553116 0.35493174 0.3541751  0.35333312 0.35254034 0.3518433
 0.35128623 0.35079998 0.35032657 0.34979963 0.34918723 0.3484474
 0.3476579  0.3469207  0.34626892 0.34579772 0.34548882 0.345175
 0.34477374 0.34418476 0.34342876 0.3426714  0.34193775 0.3409838 ]
