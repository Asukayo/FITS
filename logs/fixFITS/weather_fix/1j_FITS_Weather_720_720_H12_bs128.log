Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H12', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j720_H12_FITS_custom_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): Linear(in_features=82, out_features=164, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  72296448.0
params:  13612.0
Trainable parameters:  13612
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7551681
	speed: 0.1460s/iter; left time: 993.2843s
Epoch: 1 cost time: 19.734370708465576
Epoch: 1, Steps: 138 | Train Loss: 0.7834901 Vali Loss: 0.6783623 Test Loss: 0.3460568
Validation loss decreased (inf --> 0.678362).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6181492
	speed: 0.3345s/iter; left time: 2228.5258s
Epoch: 2 cost time: 20.14044976234436
Epoch: 2, Steps: 138 | Train Loss: 0.6443872 Vali Loss: 0.6418808 Test Loss: 0.3348350
Validation loss decreased (0.678362 --> 0.641881).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6075325
	speed: 0.3382s/iter; left time: 2206.6926s
Epoch: 3 cost time: 20.095473766326904
Epoch: 3, Steps: 138 | Train Loss: 0.6154763 Vali Loss: 0.6258308 Test Loss: 0.3303229
Validation loss decreased (0.641881 --> 0.625831).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6033344
	speed: 0.3453s/iter; left time: 2205.5120s
Epoch: 4 cost time: 20.776367664337158
Epoch: 4, Steps: 138 | Train Loss: 0.6012435 Vali Loss: 0.6233384 Test Loss: 0.3281912
Validation loss decreased (0.625831 --> 0.623338).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5713163
	speed: 0.3442s/iter; left time: 2150.7827s
Epoch: 5 cost time: 20.5165057182312
Epoch: 5, Steps: 138 | Train Loss: 0.5950048 Vali Loss: 0.6150965 Test Loss: 0.3271835
Validation loss decreased (0.623338 --> 0.615097).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6097018
	speed: 0.3487s/iter; left time: 2130.9665s
Epoch: 6 cost time: 20.902998685836792
Epoch: 6, Steps: 138 | Train Loss: 0.5920710 Vali Loss: 0.6182081 Test Loss: 0.3266375
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5971230
	speed: 0.3431s/iter; left time: 2049.3875s
Epoch: 7 cost time: 20.712193489074707
Epoch: 7, Steps: 138 | Train Loss: 0.5901518 Vali Loss: 0.6156882 Test Loss: 0.3261874
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5811980
	speed: 0.3512s/iter; left time: 2049.0309s
Epoch: 8 cost time: 22.195651292800903
Epoch: 8, Steps: 138 | Train Loss: 0.5895467 Vali Loss: 0.6174322 Test Loss: 0.3259405
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H12_FITS_custom_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.3231704831123352, mae:0.3419939875602722, rse:0.748079776763916, corr:[0.47059843 0.4793281  0.48260444 0.4814566  0.4781428  0.47556704
 0.47510695 0.47624826 0.477665   0.47824636 0.47768724 0.47635993
 0.47484085 0.4738157  0.47357237 0.47387537 0.47419468 0.4739739
 0.47316554 0.4719419  0.4706352  0.4695615  0.46895298 0.4686654
 0.46844718 0.46794415 0.46703094 0.46576804 0.46447036 0.46334624
 0.46258768 0.4621342  0.46185797 0.4614552  0.46081805 0.45990744
 0.4589421  0.4580009  0.45721477 0.4566426  0.45629343 0.4560888
 0.45578727 0.45528924 0.45463565 0.45390373 0.45322654 0.45267227
 0.45213225 0.45166484 0.45122862 0.4507305  0.4502297  0.4497342
 0.44926718 0.4488156  0.4484369  0.44806445 0.4477103  0.44731292
 0.44688356 0.44641936 0.446027   0.4456954  0.4454333  0.44523653
 0.44496313 0.44461668 0.44419587 0.44375083 0.44335955 0.44297603
 0.44268766 0.4423988  0.4419862  0.4414882  0.44099703 0.44058064
 0.4402357  0.439962   0.43985912 0.43987375 0.4398202  0.4396555
 0.43936354 0.4389423  0.43852618 0.43808356 0.43772689 0.43742847
 0.4372474  0.43713877 0.4370183  0.43687358 0.43663642 0.4363047
 0.43595383 0.43563807 0.43538797 0.43519774 0.43502292 0.43482307
 0.43458393 0.4342709  0.43388543 0.43347636 0.43303102 0.43265495
 0.4323032  0.4319824  0.4316464  0.4312266  0.43071032 0.43023574
 0.4298194  0.42946565 0.42919767 0.42897525 0.42872465 0.42834383
 0.42787242 0.4273433  0.4267824  0.4262706  0.4258528  0.4255153
 0.42519447 0.4248438  0.42444044 0.4239516  0.42338187 0.42280474
 0.42230266 0.42189726 0.42160267 0.42133263 0.4210185  0.42065492
 0.42015144 0.41963476 0.41917425 0.41882533 0.41854498 0.41827732
 0.41795635 0.4175112  0.41691005 0.41619807 0.41538545 0.4145516
 0.4138207  0.41323164 0.41268778 0.41203937 0.4112447  0.41032
 0.4094886  0.40870366 0.40809676 0.40764543 0.40738517 0.40711606
 0.40677536 0.40633777 0.40577292 0.40518656 0.40456778 0.40401378
 0.4035607  0.40307608 0.40249115 0.40179646 0.40100253 0.40026763
 0.39956656 0.3990412  0.39867178 0.39841855 0.3981281  0.3976893
 0.39713064 0.39640936 0.3956557  0.3949815  0.39450592 0.39424342
 0.39411315 0.39391953 0.39364097 0.39319268 0.39258364 0.39189914
 0.3913388  0.39081088 0.39042315 0.3900836  0.38970146 0.389252
 0.38872245 0.38823    0.38788694 0.38772014 0.38765338 0.38754427
 0.3873667  0.38704565 0.38653007 0.38585553 0.38511115 0.38453612
 0.38416338 0.3839871  0.3838989  0.38379058 0.38351005 0.38295746
 0.3822338  0.38154057 0.38098425 0.3805788  0.38040054 0.38042182
 0.3804511  0.3803551  0.38006148 0.37955695 0.37901348 0.37862384
 0.37842426 0.37838835 0.37840033 0.37835452 0.3781032  0.3776339
 0.37704632 0.37649566 0.3760588  0.3758551  0.37571952 0.37563747
 0.37551653 0.37528113 0.37484127 0.37430844 0.37384167 0.37351528
 0.37334436 0.37334654 0.37330344 0.37321484 0.37296507 0.37260777
 0.37219524 0.37181267 0.37158665 0.37144744 0.37133196 0.37115008
 0.3708897  0.37053236 0.37002325 0.36953384 0.36917046 0.3689936
 0.3689845  0.36901078 0.36893547 0.3687043  0.36829153 0.3677065
 0.3670683  0.3665446  0.3662334  0.36601982 0.3658468  0.36560234
 0.36517987 0.36456218 0.36388427 0.3632597  0.36279842 0.36256042
 0.36248294 0.3623425  0.36201134 0.3613794  0.36043188 0.35925108
 0.35804504 0.35701606 0.35622722 0.3556302  0.35515663 0.354686
 0.3540568  0.353273   0.35241494 0.35161787 0.35095608 0.35052326
 0.35011038 0.34961486 0.34893253 0.34805328 0.34707782 0.3461153
 0.34534687 0.34477314 0.3444916  0.34428963 0.3439373  0.34335738
 0.34262982 0.34185737 0.34118265 0.34075058 0.3405701  0.34059218
 0.34069693 0.340539   0.3400141  0.33923304 0.33831343 0.3373793
 0.33667013 0.33636078 0.33627266 0.33627903 0.33620316 0.33588573
 0.33540535 0.33484608 0.33439338 0.33423442 0.33430386 0.33457243
 0.33477074 0.334738   0.33451742 0.3339926  0.33339828 0.33292094
 0.33271924 0.33279344 0.33303356 0.33324054 0.33318907 0.33275458
 0.3320228  0.3312099  0.33057964 0.33020604 0.33020893 0.3303888
 0.3305604  0.3305804  0.33036727 0.3298786  0.32925385 0.32871756
 0.32833952 0.3281983  0.32828516 0.32834858 0.32825798 0.32797912
 0.32751906 0.32703662 0.3267209  0.32657447 0.32663766 0.32686222
 0.32709137 0.32716167 0.32699242 0.3266706  0.3262743  0.32588243
 0.32556304 0.32535914 0.32513785 0.3248934  0.32462105 0.32417896
 0.32369757 0.3232964  0.32307428 0.32304695 0.32313722 0.32326812
 0.3232853  0.3232742  0.32319003 0.3231109  0.32308856 0.32316604
 0.32330006 0.32340935 0.32349613 0.32340014 0.32324645 0.32299763
 0.32289696 0.32282084 0.3228482  0.32282645 0.32269287 0.32244632
 0.32207707 0.32152733 0.32090434 0.32034642 0.3200212  0.31994566
 0.3200154  0.3200763  0.32004303 0.31986633 0.31950083 0.31902748
 0.3184811  0.31808084 0.317726   0.31744307 0.31703845 0.31650117
 0.31576312 0.31496206 0.31420636 0.31348822 0.3129317  0.3125957
 0.31234297 0.31204763 0.3116462  0.31107295 0.3103814  0.30964127
 0.30899864 0.30850235 0.3081014  0.3076443  0.30702406 0.30601546
 0.30479854 0.3035133  0.3023663  0.3015754  0.30114844 0.30101508
 0.30095842 0.30069315 0.30004385 0.29911527 0.29806775 0.29706213
 0.2964224  0.29609448 0.29597384 0.2959022  0.29565954 0.29526022
 0.2947096  0.29418063 0.29384318 0.29374173 0.2937795  0.29384163
 0.29374745 0.2934492  0.2928996  0.29220665 0.29161832 0.29131216
 0.2913655  0.29170394 0.29210111 0.2923616  0.29233286 0.29194006
 0.29138637 0.29086336 0.2905786  0.2906381  0.29097188 0.2914365
 0.29172745 0.2917277  0.29149362 0.29109934 0.29067028 0.290269
 0.29002815 0.28996977 0.29008806 0.29016063 0.28997475 0.28951776
 0.28896645 0.28850216 0.2882375  0.2883069  0.2886408  0.28906915
 0.2894026  0.28954846 0.28949612 0.28922284 0.28902477 0.28887942
 0.28892207 0.2891344  0.2894015  0.28945723 0.2892675  0.28894806
 0.2885785  0.28828505 0.28816417 0.2882404  0.28842595 0.2884824
 0.28841823 0.28819484 0.28782937 0.2875192  0.28726435 0.28715637
 0.28708607 0.2870473  0.28676993 0.28638238 0.2858904  0.28536904
 0.28489515 0.28458428 0.2843984  0.2843974  0.28443015 0.2843692
 0.28412697 0.28375965 0.28334576 0.2829885  0.2826893  0.28249237
 0.28235495 0.28224045 0.2820727  0.2817724  0.28140554 0.2809653
 0.2805787  0.28026927 0.2800397  0.27974337 0.27940255 0.2789147
 0.2782728  0.27760896 0.2770569  0.27671415 0.2764489  0.2761692
 0.2757858  0.27519193 0.27438706 0.27336648 0.27232033 0.2714134
 0.27070186 0.27017483 0.26971003 0.26915503 0.2683734  0.26736483
 0.26636982 0.26528376 0.26412356 0.26360938 0.2632399  0.26285702
 0.26230073 0.26147085 0.26040548 0.25932717 0.2584404  0.257959
 0.25785068 0.25794274 0.25796238 0.25771222 0.25709155 0.2562157
 0.2553143  0.25467145 0.25449756 0.25477743 0.25529    0.255677
 0.25571227 0.25520524 0.2542558  0.2531014  0.2520792  0.25152737
 0.25134286 0.25148576 0.25159794 0.25139615 0.25082847 0.24997959
 0.24910593 0.24853306 0.24838613 0.24856928 0.24898532 0.249234
 0.24918148 0.24887957 0.24834251 0.24783395 0.24760793 0.24776584
 0.24815886 0.24861188 0.2488748  0.24879374 0.24846308 0.24796091
 0.24759416 0.24750665 0.24780439 0.24830703 0.24883443 0.24915828
 0.24920416 0.24894282 0.24851358 0.24808693 0.24794218 0.24810457
 0.24846373 0.24882747 0.2490032  0.24882178 0.24843994 0.2479975
 0.24780096 0.247858   0.24823068 0.24862875 0.2489554  0.24895851
 0.24866714 0.24814779 0.24761997 0.24726932 0.24718921 0.24737293
 0.24770962 0.24792913 0.24796036 0.24773626 0.24741061 0.24717157
 0.247054   0.24706338 0.24707235 0.24693684 0.24658118 0.24595836
 0.24525812 0.24472266 0.24442275 0.24442045 0.24453552 0.24462867
 0.2445003  0.24409173 0.24350752 0.24289887 0.24254939 0.24253586
 0.24277401 0.24305671 0.24317601 0.2429401  0.24239112 0.2416963
 0.24110761 0.24072947 0.240666   0.240696   0.24049163 0.23981601
 0.2387424  0.23753782 0.23660201 0.2362659  0.23658435 0.23711383
 0.23729008 0.23659702 0.23490499 0.23265024 0.23077095 0.2301325
 0.23110677 0.23303792 0.23464613 0.2347028  0.23250562 0.22875495
 0.22524537 0.22491543 0.22884996 0.23524095 0.23913753 0.2352404 ]
