Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H12', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j720_H12_FITS_custom_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): Linear(in_features=82, out_features=164, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  144592896.0
params:  13612.0
Trainable parameters:  13612
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 15.320219278335571
Epoch: 1, Steps: 69 | Train Loss: 0.8657038 Vali Loss: 0.7449883 Test Loss: 0.3747511
Validation loss decreased (inf --> 0.744988).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 14.701370000839233
Epoch: 2, Steps: 69 | Train Loss: 0.6941320 Vali Loss: 0.6791877 Test Loss: 0.3514405
Validation loss decreased (0.744988 --> 0.679188).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 15.141607522964478
Epoch: 3, Steps: 69 | Train Loss: 0.6542428 Vali Loss: 0.6563835 Test Loss: 0.3447209
Validation loss decreased (0.679188 --> 0.656383).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.555237531661987
Epoch: 4, Steps: 69 | Train Loss: 0.6349780 Vali Loss: 0.6401657 Test Loss: 0.3409389
Validation loss decreased (0.656383 --> 0.640166).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 12.256772994995117
Epoch: 5, Steps: 69 | Train Loss: 0.6217205 Vali Loss: 0.6352211 Test Loss: 0.3383059
Validation loss decreased (0.640166 --> 0.635221).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 13.576143264770508
Epoch: 6, Steps: 69 | Train Loss: 0.6127382 Vali Loss: 0.6260620 Test Loss: 0.3365583
Validation loss decreased (0.635221 --> 0.626062).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 13.698346853256226
Epoch: 7, Steps: 69 | Train Loss: 0.6059156 Vali Loss: 0.6266955 Test Loss: 0.3352640
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 13.671367645263672
Epoch: 8, Steps: 69 | Train Loss: 0.6010039 Vali Loss: 0.6198989 Test Loss: 0.3344018
Validation loss decreased (0.626062 --> 0.619899).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 13.867026805877686
Epoch: 9, Steps: 69 | Train Loss: 0.5976202 Vali Loss: 0.6180668 Test Loss: 0.3337826
Validation loss decreased (0.619899 --> 0.618067).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 13.743671417236328
Epoch: 10, Steps: 69 | Train Loss: 0.5954431 Vali Loss: 0.6164489 Test Loss: 0.3333405
Validation loss decreased (0.618067 --> 0.616449).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 14.020493745803833
Epoch: 11, Steps: 69 | Train Loss: 0.5938291 Vali Loss: 0.6170764 Test Loss: 0.3329446
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 13.153253555297852
Epoch: 12, Steps: 69 | Train Loss: 0.5922763 Vali Loss: 0.6165462 Test Loss: 0.3326724
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.948860883712769
Epoch: 13, Steps: 69 | Train Loss: 0.5916692 Vali Loss: 0.6163756 Test Loss: 0.3324635
Validation loss decreased (0.616449 --> 0.616376).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 12.141402244567871
Epoch: 14, Steps: 69 | Train Loss: 0.5911112 Vali Loss: 0.6165764 Test Loss: 0.3323187
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 11.98990797996521
Epoch: 15, Steps: 69 | Train Loss: 0.5899782 Vali Loss: 0.6176968 Test Loss: 0.3321471
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 12.652588129043579
Epoch: 16, Steps: 69 | Train Loss: 0.5898046 Vali Loss: 0.6176529 Test Loss: 0.3320014
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H12_FITS_custom_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.3224608898162842, mae:0.3411087691783905, rse:0.7472580075263977, corr:[0.47217962 0.47974068 0.48223585 0.48088127 0.4780024  0.4761431
 0.47615653 0.47731644 0.4784104  0.4785986  0.47777912 0.47638592
 0.47498626 0.47418648 0.47410312 0.47437727 0.47451526 0.47407103
 0.4731391  0.47193274 0.47078392 0.4699575  0.46958882 0.46940112
 0.46911883 0.46845505 0.467413   0.4660982  0.46483904 0.4638051
 0.4631596  0.46276903 0.4624667  0.46195173 0.4612043  0.46023136
 0.45927873 0.45839396 0.45767698 0.45714176 0.45676276 0.45646945
 0.45605212 0.45546302 0.45476654 0.45403156 0.45338997 0.45290005
 0.45241982 0.45199347 0.4515842  0.45110777 0.45063812 0.45016307
 0.44969246 0.44920367 0.4487807  0.44835293 0.44794208 0.4475054
 0.44707835 0.44664636 0.4462922  0.4459683  0.44568473 0.44545704
 0.44515175 0.44479042 0.44437966 0.44397247 0.4436309  0.44326216
 0.44295052 0.44259837 0.44210628 0.44154575 0.44103172 0.4406469
 0.44037718 0.44017863 0.44012946 0.44016948 0.44011545 0.43993613
 0.43962246 0.43918365 0.4387902  0.43839595 0.43809366 0.43782005
 0.43763712 0.43751281 0.43737495 0.43723065 0.4370236  0.4367461
 0.43645298 0.43616962 0.43591055 0.43567112 0.4354204  0.4351381
 0.43484083 0.43451855 0.4341757  0.43385097 0.433488   0.43317592
 0.43285728 0.43253875 0.4321893  0.43175596 0.43123454 0.4307819
 0.43039298 0.4300473  0.42976162 0.4294924  0.42917266 0.42871302
 0.42819908 0.4277028  0.4272465  0.42688164 0.42660156 0.42635074
 0.4260388  0.42562115 0.42510185 0.42449522 0.4238447  0.4232477
 0.42278466 0.42244425 0.42221355 0.4219709  0.42163277 0.4212098
 0.4206319  0.42007372 0.41961977 0.41931212 0.41908637 0.41886196
 0.41856402 0.41813627 0.41756892 0.4169244  0.41621172 0.41547891
 0.4148113  0.41422722 0.4136298  0.41288924 0.41197252 0.41091904
 0.4100016  0.40915856 0.40850905 0.4079969  0.40766156 0.40729406
 0.4068579  0.40635958 0.40580276 0.40530315 0.40481994 0.4044021
 0.40403923 0.4035722  0.40293804 0.4021578  0.4012803  0.40051112
 0.39982685 0.39935896 0.39904994 0.39883307 0.39854312 0.39808208
 0.3975115  0.39680743 0.39611357 0.39552847 0.39513946 0.39493117
 0.3948126  0.39460415 0.39432442 0.393923   0.39343193 0.39293072
 0.39258596 0.39224592 0.3919763  0.39166826 0.3912477  0.3907192
 0.3901049  0.3895491  0.38917327 0.38899624 0.38892585 0.388808
 0.38862494 0.38831818 0.38785654 0.3872746  0.38663825 0.38615185
 0.3858068  0.3855749  0.38535792 0.3850979  0.38470158 0.38411024
 0.3834486  0.3828988  0.38250506 0.3821977  0.38200492 0.38189384
 0.38171068 0.38138637 0.38090992 0.38030833 0.3797636  0.37944224
 0.37932435 0.37933105 0.37932387 0.37921095 0.37888026 0.37836233
 0.37777656 0.37727314 0.37690142 0.3767561  0.37664112 0.37655884
 0.3764527  0.37628254 0.37598056 0.37565628 0.37542495 0.37529162
 0.37521055 0.37518024 0.37499678 0.37472007 0.37429225 0.3738111
 0.37334087 0.37294325 0.37270397 0.37251484 0.37229803 0.37198344
 0.37160426 0.3711826  0.37068138 0.37026694 0.37000287 0.36989775
 0.36989683 0.36985436 0.36966896 0.36933035 0.3688577  0.36829096
 0.36774164 0.36733308 0.36711407 0.36692673 0.366711   0.36638385
 0.3658842  0.365236   0.36459458 0.3640522  0.36366257 0.363425
 0.36324802 0.36291683 0.36235777 0.3615347  0.3605125  0.3594229
 0.3584706  0.35778967 0.35734722 0.35701594 0.3566911  0.3562678
 0.35563222 0.35484198 0.35401112 0.3532707  0.35265765 0.3522187
 0.3517171  0.35106403 0.35020152 0.34918007 0.34815052 0.3472364
 0.34658855 0.3461507  0.34597114 0.3458113  0.34545296 0.344857
 0.34414223 0.34342417 0.34284168 0.34250084 0.34236592 0.3423653
 0.3423744  0.34207323 0.34139347 0.3404766  0.3394575  0.3384568
 0.3376943  0.33735028 0.33722645 0.33720073 0.33711582 0.3368197
 0.3363874  0.33587188 0.3354175  0.3351806  0.3350866  0.33514926
 0.335155   0.33501506 0.33483648 0.33449528 0.334186   0.33400536
 0.33402607 0.33418903 0.33437797 0.3344539  0.33426878 0.33377337
 0.33307427 0.33236524 0.33185282 0.3315212  0.33144894 0.33143896
 0.3313645  0.33116803 0.3308382  0.33035883 0.32985014 0.3294673
 0.32919478 0.3290573  0.32906172 0.32900852 0.3288483  0.328609
 0.32831046 0.3280645  0.32797205 0.32793206 0.32793224 0.32794148
 0.32788405 0.32770082 0.32740402 0.3271347  0.32695088 0.32684505
 0.3267763  0.32670075 0.32646415 0.32610455 0.32570702 0.3252199
 0.32482767 0.32464576 0.32471028 0.32494286 0.32518768 0.32532817
 0.32521853 0.32500762 0.3247289  0.324529   0.3244868  0.32462993
 0.3248637  0.32505158 0.3251685  0.32503265 0.32480034 0.32446045
 0.32430583 0.3242083  0.32425055 0.3242585  0.32416347 0.32396334
 0.32365936 0.32320493 0.32270762 0.32228035 0.32205707 0.3220152
 0.32202968 0.32194895 0.3217207  0.32135123 0.32085007 0.32034126
 0.319863   0.31960776 0.31940663 0.31923607 0.3188528  0.318243
 0.3173623  0.31640542 0.3155362  0.3147701  0.31424174 0.31398812
 0.31383023 0.31361166 0.31324616 0.31266648 0.31194156 0.3111699
 0.31053105 0.31009072 0.3097969  0.3094809  0.3090084  0.3081291
 0.307      0.30574396 0.3045602  0.30367965 0.3031324  0.30288363
 0.30274647 0.30244076 0.30180043 0.30093095 0.29997563 0.29906985
 0.2985176  0.2982315  0.2980975  0.29795906 0.29760128 0.29705876
 0.29634535 0.29565197 0.2951655  0.29494566 0.29490367 0.2949369
 0.2948656  0.29464018 0.29418916 0.29358682 0.29304737 0.29272893
 0.29271457 0.29296225 0.2932917  0.29355434 0.29362282 0.2934143
 0.2930885  0.2927675  0.29259232 0.29263803 0.29285613 0.29316717
 0.2933393  0.29330865 0.29314673 0.29290637 0.29265776 0.29239002
 0.29218617 0.29206648 0.292086   0.292088   0.29191336 0.2915719
 0.29121828 0.29097462 0.29087624 0.291002   0.29126376 0.2915281
 0.29166043 0.2916373  0.29148903 0.2911916  0.29102778 0.2909147
 0.29095665 0.29112792 0.29132634 0.29130343 0.29105708 0.2907253
 0.2903776  0.290116   0.29000464 0.29005104 0.29016823 0.29013517
 0.29000023 0.2897568  0.28942874 0.28920156 0.28902864 0.28896683
 0.28887883 0.28876415 0.2883769  0.28789565 0.2873686  0.28689852
 0.2865333  0.28634888 0.2862551  0.2862916  0.2863049  0.2861905
 0.28590065 0.28553212 0.28518382 0.28494823 0.28478682 0.28469747
 0.28460214 0.28445533 0.28420067 0.28379953 0.2833632  0.28292042
 0.28260142 0.2824053  0.28229696 0.28209135 0.28178158 0.28125817
 0.2805242  0.27973127 0.27903768 0.27856827 0.27819228 0.2778242
 0.27738217 0.27676386 0.27597514 0.27503055 0.27413237 0.27344048
 0.2729858  0.2727123  0.27244696 0.2719907  0.2711854  0.27002746
 0.26878884 0.26743895 0.2660711  0.26545048 0.26510498 0.26484835
 0.2644544  0.2637447  0.26269278 0.2614857  0.2603407  0.2595428
 0.25915265 0.2590808  0.2590967  0.25898865 0.2585944  0.25793755
 0.25715417 0.25646833 0.2560922  0.25607353 0.25629693 0.25649673
 0.25649855 0.25611243 0.25539362 0.2545061  0.2537026  0.25326875
 0.25309616 0.2531831  0.25323057 0.2529888  0.25242484 0.25161415
 0.25078502 0.25024545 0.2501156  0.25030154 0.2507342  0.2510235
 0.25103343 0.25079507 0.25028017 0.24972591 0.24938515 0.24938859
 0.24963644 0.25000882 0.25028145 0.25029278 0.2500916  0.24967916
 0.2492997  0.24906808 0.24912353 0.24935794 0.24968515 0.24994361
 0.25008544 0.25004184 0.24987698 0.2496643  0.24962004 0.24975207
 0.24998741 0.2502029  0.2502754  0.25007647 0.24976256 0.24942732
 0.24931796 0.24939035 0.24970092 0.24998453 0.25020579 0.25015852
 0.249903   0.24949983 0.24912545 0.24891357 0.24891602 0.24911322
 0.24942409 0.24958614 0.24956425 0.24931312 0.24898481 0.24874844
 0.24861373 0.24858809 0.24857394 0.24846373 0.24821201 0.2477753
 0.24731418 0.24701843 0.24689205 0.24696189 0.24704252 0.24704011
 0.2468165  0.24636187 0.2458132  0.24531388 0.24509065 0.24515013
 0.24534707 0.24546443 0.24533193 0.2448146  0.2440298  0.24320616
 0.24263595 0.24241042 0.24258016 0.24286501 0.24288635 0.24237625
 0.24139303 0.2402059  0.23923537 0.23885226 0.23915073 0.2397161
 0.24001138 0.23951347 0.23804374 0.23595485 0.23409244 0.2332883
 0.23397997 0.23565578 0.23718461 0.23741634 0.23559932 0.23228312
 0.22902137 0.22853914 0.23198064 0.23794511 0.24197678 0.23913448]
