Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=70, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=140, bias=True)
    (1): Linear(in_features=70, out_features=140, bias=True)
    (2): Linear(in_features=70, out_features=140, bias=True)
    (3): Linear(in_features=70, out_features=140, bias=True)
    (4): Linear(in_features=70, out_features=140, bias=True)
    (5): Linear(in_features=70, out_features=140, bias=True)
    (6): Linear(in_features=70, out_features=140, bias=True)
    (7): Linear(in_features=70, out_features=140, bias=True)
    (8): Linear(in_features=70, out_features=140, bias=True)
    (9): Linear(in_features=70, out_features=140, bias=True)
    (10): Linear(in_features=70, out_features=140, bias=True)
    (11): Linear(in_features=70, out_features=140, bias=True)
    (12): Linear(in_features=70, out_features=140, bias=True)
    (13): Linear(in_features=70, out_features=140, bias=True)
    (14): Linear(in_features=70, out_features=140, bias=True)
    (15): Linear(in_features=70, out_features=140, bias=True)
    (16): Linear(in_features=70, out_features=140, bias=True)
    (17): Linear(in_features=70, out_features=140, bias=True)
    (18): Linear(in_features=70, out_features=140, bias=True)
    (19): Linear(in_features=70, out_features=140, bias=True)
    (20): Linear(in_features=70, out_features=140, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  105369600.0
params:  208740.0
Trainable parameters:  208740
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 22.961699724197388
Epoch: 1, Steps: 69 | Train Loss: 0.9078183 Vali Loss: 0.7789261 Test Loss: 0.3809136
Validation loss decreased (inf --> 0.778926).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 23.14498519897461
Epoch: 2, Steps: 69 | Train Loss: 0.7295925 Vali Loss: 0.7004580 Test Loss: 0.3542057
Validation loss decreased (0.778926 --> 0.700458).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 21.867165327072144
Epoch: 3, Steps: 69 | Train Loss: 0.6750867 Vali Loss: 0.6692374 Test Loss: 0.3452713
Validation loss decreased (0.700458 --> 0.669237).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 22.802372455596924
Epoch: 4, Steps: 69 | Train Loss: 0.6470955 Vali Loss: 0.6511096 Test Loss: 0.3401385
Validation loss decreased (0.669237 --> 0.651110).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 24.501288175582886
Epoch: 5, Steps: 69 | Train Loss: 0.6279191 Vali Loss: 0.6403551 Test Loss: 0.3366938
Validation loss decreased (0.651110 --> 0.640355).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 26.210774183273315
Epoch: 6, Steps: 69 | Train Loss: 0.6139506 Vali Loss: 0.6235011 Test Loss: 0.3340431
Validation loss decreased (0.640355 --> 0.623501).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 26.85837960243225
Epoch: 7, Steps: 69 | Train Loss: 0.6031988 Vali Loss: 0.6188719 Test Loss: 0.3319690
Validation loss decreased (0.623501 --> 0.618872).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 25.184098720550537
Epoch: 8, Steps: 69 | Train Loss: 0.5950849 Vali Loss: 0.6160195 Test Loss: 0.3304525
Validation loss decreased (0.618872 --> 0.616019).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 23.416067123413086
Epoch: 9, Steps: 69 | Train Loss: 0.5887560 Vali Loss: 0.6113554 Test Loss: 0.3291817
Validation loss decreased (0.616019 --> 0.611355).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 23.482333660125732
Epoch: 10, Steps: 69 | Train Loss: 0.5840005 Vali Loss: 0.6117736 Test Loss: 0.3281474
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 23.77620840072632
Epoch: 11, Steps: 69 | Train Loss: 0.5793897 Vali Loss: 0.6079892 Test Loss: 0.3274005
Validation loss decreased (0.611355 --> 0.607989).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 23.317203283309937
Epoch: 12, Steps: 69 | Train Loss: 0.5769086 Vali Loss: 0.6072823 Test Loss: 0.3267168
Validation loss decreased (0.607989 --> 0.607282).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 22.382142066955566
Epoch: 13, Steps: 69 | Train Loss: 0.5745820 Vali Loss: 0.6030687 Test Loss: 0.3261853
Validation loss decreased (0.607282 --> 0.603069).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 23.223756551742554
Epoch: 14, Steps: 69 | Train Loss: 0.5725647 Vali Loss: 0.6045802 Test Loss: 0.3256982
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 23.09832763671875
Epoch: 15, Steps: 69 | Train Loss: 0.5710772 Vali Loss: 0.6046063 Test Loss: 0.3253296
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 24.53324604034424
Epoch: 16, Steps: 69 | Train Loss: 0.5698480 Vali Loss: 0.6015918 Test Loss: 0.3249407
Validation loss decreased (0.603069 --> 0.601592).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 23.69417142868042
Epoch: 17, Steps: 69 | Train Loss: 0.5688600 Vali Loss: 0.6010778 Test Loss: 0.3247085
Validation loss decreased (0.601592 --> 0.601078).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 21.80684781074524
Epoch: 18, Steps: 69 | Train Loss: 0.5677492 Vali Loss: 0.6031125 Test Loss: 0.3244719
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 21.707584857940674
Epoch: 19, Steps: 69 | Train Loss: 0.5672834 Vali Loss: 0.6021839 Test Loss: 0.3242380
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 24.00789785385132
Epoch: 20, Steps: 69 | Train Loss: 0.5666602 Vali Loss: 0.6047685 Test Loss: 0.3240538
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.31349897384643555, mae:0.3359796702861786, rse:0.7368008494377136, corr:[0.46310484 0.46941507 0.47199807 0.4713343  0.46868023 0.4658804
 0.46433225 0.4642348  0.4649806  0.4657189  0.4658468  0.46519083
 0.4638231  0.46216998 0.46075672 0.45989162 0.45965004 0.45976242
 0.4598953  0.45969227 0.45899603 0.4578354  0.45645267 0.45503944
 0.4538569  0.45302954 0.45252913 0.4521771  0.45178086 0.4511635
 0.450324   0.4493593  0.44848993 0.44781527 0.44744632 0.4472821
 0.4472763  0.44722134 0.44697174 0.44646183 0.4457373  0.4449668
 0.444276   0.44371694 0.44331077 0.44301346 0.44278246 0.44253266
 0.44205225 0.44139105 0.44061363 0.43975028 0.43896315 0.43835196
 0.43794817 0.4377074  0.4375621  0.4373904  0.43714103 0.4367469
 0.43624455 0.43568245 0.4351579  0.43472877 0.43443733 0.4342872
 0.43419403 0.43409136 0.43392596 0.43366152 0.43333355 0.43288034
 0.4324486  0.43206587 0.43173847 0.43147367 0.43126094 0.4311035
 0.43092906 0.430709   0.43049118 0.43026122 0.43000743 0.429778
 0.42960933 0.42948648 0.4293952  0.42920473 0.4289698  0.42866495
 0.42833835 0.42804572 0.42778346 0.42760026 0.4275083  0.42746073
 0.42740563 0.42729026 0.42707926 0.42680317 0.42648664 0.42616335
 0.42588457 0.42565417 0.42545345 0.4252386  0.4249729  0.42467034
 0.42427394 0.42384043 0.4233889  0.42293212 0.42251375 0.42220354
 0.42198077 0.42180553 0.42166105 0.42150778 0.4212847  0.42093182
 0.42051888 0.42011273 0.41974777 0.41947895 0.41930243 0.41919455
 0.41909277 0.41891593 0.41864735 0.418262   0.41777    0.41724387
 0.41677642 0.41642573 0.4162349  0.41615713 0.41613665 0.4161097
 0.41591117 0.415602   0.4152123  0.41481304 0.41440532 0.41404933
 0.41378674 0.41361344 0.41350535 0.41341358 0.41319194 0.41273367
 0.41205356 0.41125786 0.41040817 0.40957198 0.40891898 0.40846124
 0.40822315 0.40799776 0.40774363 0.40739563 0.4069513  0.406434
 0.40588093 0.40537116 0.4049218  0.4045641  0.4042226  0.40385413
 0.4034284  0.40288427 0.4021958  0.40140846 0.40060198 0.39988518
 0.39928353 0.39882013 0.39846823 0.3981802  0.39787388 0.3974582
 0.39692974 0.3962709  0.39551356 0.39474073 0.39407462 0.39355022
 0.39318117 0.3928817  0.3925994  0.3922768  0.39185324 0.39133078
 0.39076912 0.39014637 0.38958594 0.38910896 0.3887049  0.38837352
 0.3880458  0.38769674 0.38733238 0.38689497 0.38639763 0.38585857
 0.38538364 0.3849888  0.38465723 0.3843757  0.3840426  0.3836485
 0.3831428  0.38253975 0.3818835  0.38125908 0.3807195  0.38028035
 0.3799098  0.37968463 0.37946463 0.37916994 0.37879318 0.37838665
 0.37794963 0.37750742 0.37711665 0.37679023 0.37653676 0.37636465
 0.37619147 0.37597153 0.37564692 0.3751785  0.37459794 0.3739435
 0.3733372  0.37287018 0.37251732 0.37227142 0.3720447  0.3718111
 0.37156963 0.3712763  0.37084922 0.37038672 0.36996603 0.36960956
 0.36931643 0.36910832 0.3689351  0.36874396 0.36847126 0.36809793
 0.36758912 0.3669439  0.36630484 0.36564288 0.36509472 0.36470693
 0.36452255 0.364455   0.36434394 0.36418048 0.3638756  0.36351904
 0.363108   0.36271244 0.3623523  0.36209276 0.36196348 0.3618518
 0.36173022 0.36159682 0.3613391  0.36091322 0.3604094  0.3598809
 0.3593701  0.35886917 0.3584546  0.35810614 0.35777754 0.3574669
 0.357153   0.35673296 0.3562425  0.35569182 0.35511175 0.3545095
 0.35394588 0.35341778 0.35287896 0.35227972 0.3516859  0.35109076
 0.3504377  0.34975037 0.34904686 0.34837627 0.34772494 0.3471843
 0.34664047 0.3461331  0.3456313  0.34514156 0.3446402  0.34411928
 0.343604   0.34309688 0.34261775 0.3421209  0.34163672 0.34111175
 0.34054473 0.33998546 0.33942035 0.33886504 0.33837074 0.33792487
 0.3375383  0.3371618  0.33676627 0.33635685 0.33594102 0.33545083
 0.3349234  0.33439225 0.3338442  0.33335507 0.3329671  0.33262646
 0.33233905 0.3320508  0.33173636 0.33142143 0.33105022 0.33065838
 0.33023646 0.32981625 0.32944694 0.32910347 0.32877883 0.3285039
 0.3282647  0.3279855  0.32767275 0.3273377  0.3270163  0.32670403
 0.32644123 0.32619277 0.3259651  0.3257132  0.3254444  0.3251253
 0.32475653 0.32437587 0.3240278  0.32370245 0.32342774 0.3232241
 0.32304004 0.32286537 0.32270366 0.32247782 0.32219383 0.32187223
 0.32152912 0.32122236 0.32100338 0.320836   0.32072875 0.3206497
 0.32056248 0.32040256 0.32015753 0.31984043 0.31946915 0.31906357
 0.31867087 0.31835106 0.31808484 0.31788024 0.31773582 0.317547
 0.31730643 0.31700456 0.31668162 0.31634825 0.31605884 0.31584
 0.3156619  0.31557578 0.31551808 0.3154601  0.31536037 0.31519768
 0.3149726  0.3147084  0.31442776 0.31416237 0.3139576  0.31379214
 0.31371498 0.3136307  0.31353417 0.3133601  0.3130877  0.31275815
 0.31242928 0.31212038 0.31186014 0.31161875 0.31143254 0.31128678
 0.3111331  0.31092364 0.31066594 0.31038564 0.3100477  0.30971375
 0.30937734 0.3091206  0.30887717 0.30867285 0.3084243  0.30810142
 0.30769297 0.307247   0.30674306 0.3061868  0.30563298 0.3051694
 0.3047644  0.30440268 0.30407503 0.30372542 0.30330196 0.3027895
 0.3021779  0.30151406 0.30086774 0.3002427  0.2996643  0.29901767
 0.29837236 0.29770735 0.29698902 0.2962736  0.29554456 0.2948649
 0.2942779  0.29375416 0.293302   0.2928949  0.29253432 0.29213953
 0.29170805 0.2912325  0.2907194  0.2901827  0.28964177 0.28918007
 0.28878108 0.2884482  0.28817433 0.28791183 0.2876254  0.2872764
 0.28688926 0.2864813  0.28606454 0.2856913  0.28542513 0.2852477
 0.28513315 0.28503987 0.28490335 0.28469598 0.28441164 0.28403458
 0.28364554 0.28327486 0.2829412  0.28267428 0.2824852  0.28237173
 0.2822413  0.2820757  0.28186807 0.28161332 0.2813408  0.2810686
 0.28086    0.2807426  0.28067917 0.2806376  0.2805621  0.28043902
 0.28024557 0.27998945 0.27967477 0.27932638 0.27903166 0.2788183
 0.27867725 0.2786047  0.27859876 0.2785422  0.27845863 0.27830106
 0.2781079  0.2778834  0.27765906 0.27741605 0.27717727 0.27696005
 0.2767526  0.27654657 0.27631631 0.27607897 0.27584177 0.27559477
 0.27538314 0.27519858 0.2750063  0.27482474 0.2745863  0.2742952
 0.27391806 0.27347714 0.27300587 0.27255848 0.27221602 0.27195928
 0.27179748 0.27169546 0.2715752  0.27140254 0.27113247 0.27074668
 0.2702785  0.2698151  0.26938415 0.26905632 0.26884457 0.26868358
 0.2685312  0.26834646 0.2680923  0.2677216  0.26726627 0.26673943
 0.26622212 0.26578462 0.26547834 0.2652615  0.26512477 0.2649621
 0.26473144 0.26435503 0.2638445  0.263257   0.26262158 0.26202172
 0.26154074 0.2612122  0.26100805 0.2608633  0.26069868 0.26047635
 0.26014093 0.25969633 0.25918606 0.25863808 0.258077   0.25747633
 0.25710043 0.25654587 0.25588635 0.2554744  0.25495762 0.2543577
 0.253693   0.25301808 0.25237668 0.25184983 0.2514143  0.2510916
 0.25081965 0.25053072 0.2501606  0.24971151 0.24918352 0.24862754
 0.2480886  0.24760874 0.24722555 0.24693798 0.24671425 0.24649586
 0.24624075 0.24587631 0.2454037  0.24482875 0.24421148 0.24364963
 0.24310979 0.24269623 0.24232864 0.24197356 0.24163792 0.2412827
 0.24090533 0.24054448 0.24017212 0.23983882 0.23956846 0.23927516
 0.23901002 0.23879904 0.23858292 0.23835123 0.23811594 0.23791756
 0.23770168 0.23750162 0.2373134  0.23712629 0.23698992 0.23684032
 0.23670697 0.23656984 0.23647001 0.2363906  0.23634283 0.23634481
 0.23638727 0.23642835 0.23641832 0.23631835 0.23616889 0.23597805
 0.2358092  0.23568897 0.23563108 0.2355834  0.23557943 0.2355762
 0.23556216 0.23545212 0.23532255 0.23511972 0.23498029 0.23490243
 0.23496066 0.23511037 0.23529585 0.23542875 0.23544018 0.23529422
 0.23501419 0.2346294  0.23422262 0.23388685 0.23373492 0.23379223
 0.2339866  0.2341934  0.2342434  0.234075   0.2336791  0.23307078
 0.23242275 0.2318832  0.2315413  0.23145811 0.2315762  0.23177041
 0.23184365 0.23165219 0.23115371 0.23034936 0.2294194  0.22853525
 0.22790393 0.22767024 0.22786935 0.22831045 0.2287279  0.22885521
 0.22851537 0.22764792 0.2264696  0.22528377 0.22440286 0.22405203
 0.22428541 0.22486286 0.22539558 0.22549067 0.22491616 0.22361349
 0.22187236 0.22016886 0.21904314 0.21892715 0.21984927 0.22138706
 0.22276407 0.22323701 0.22237623 0.22024925 0.21739766 0.21513611
 0.21481997 0.2173436  0.22174875 0.22575326 0.22611812 0.21860616]
