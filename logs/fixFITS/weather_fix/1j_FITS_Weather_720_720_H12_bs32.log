Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H12', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j720_H12_FITS_custom_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=82, out_features=164, bias=True)
    (1): Linear(in_features=82, out_features=164, bias=True)
    (2): Linear(in_features=82, out_features=164, bias=True)
    (3): Linear(in_features=82, out_features=164, bias=True)
    (4): Linear(in_features=82, out_features=164, bias=True)
    (5): Linear(in_features=82, out_features=164, bias=True)
    (6): Linear(in_features=82, out_features=164, bias=True)
    (7): Linear(in_features=82, out_features=164, bias=True)
    (8): Linear(in_features=82, out_features=164, bias=True)
    (9): Linear(in_features=82, out_features=164, bias=True)
    (10): Linear(in_features=82, out_features=164, bias=True)
    (11): Linear(in_features=82, out_features=164, bias=True)
    (12): Linear(in_features=82, out_features=164, bias=True)
    (13): Linear(in_features=82, out_features=164, bias=True)
    (14): Linear(in_features=82, out_features=164, bias=True)
    (15): Linear(in_features=82, out_features=164, bias=True)
    (16): Linear(in_features=82, out_features=164, bias=True)
    (17): Linear(in_features=82, out_features=164, bias=True)
    (18): Linear(in_features=82, out_features=164, bias=True)
    (19): Linear(in_features=82, out_features=164, bias=True)
    (20): Linear(in_features=82, out_features=164, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  18074112.0
params:  285852.0
Trainable parameters:  285852
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8058965
	speed: 0.1481s/iter; left time: 4081.3777s
	iters: 200, epoch: 1 | loss: 0.7559116
	speed: 0.1607s/iter; left time: 4410.9769s
	iters: 300, epoch: 1 | loss: 0.7648216
	speed: 0.1736s/iter; left time: 4748.8034s
	iters: 400, epoch: 1 | loss: 0.6919482
	speed: 0.1790s/iter; left time: 4878.1912s
	iters: 500, epoch: 1 | loss: 0.6370412
	speed: 0.1724s/iter; left time: 4682.1666s
Epoch: 1 cost time: 92.65922498703003
Epoch: 1, Steps: 553 | Train Loss: 0.6866828 Vali Loss: 0.6172956 Test Loss: 0.3213679
Validation loss decreased (inf --> 0.617296).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5204560
	speed: 0.8720s/iter; left time: 23541.2402s
	iters: 200, epoch: 2 | loss: 0.5497368
	speed: 0.1766s/iter; left time: 4749.5040s
	iters: 300, epoch: 2 | loss: 0.5556253
	speed: 0.1711s/iter; left time: 4584.1967s
	iters: 400, epoch: 2 | loss: 0.6073715
	speed: 0.1569s/iter; left time: 4188.8622s
	iters: 500, epoch: 2 | loss: 0.6391870
	speed: 0.1441s/iter; left time: 3833.0222s
Epoch: 2 cost time: 92.65298676490784
Epoch: 2, Steps: 553 | Train Loss: 0.5755212 Vali Loss: 0.6051884 Test Loss: 0.3150955
Validation loss decreased (0.617296 --> 0.605188).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6563143
	speed: 0.6808s/iter; left time: 18004.2365s
	iters: 200, epoch: 3 | loss: 0.5617551
	speed: 0.1356s/iter; left time: 3572.2927s
	iters: 300, epoch: 3 | loss: 0.5882469
	speed: 0.1398s/iter; left time: 3670.0107s
	iters: 400, epoch: 3 | loss: 0.5288438
	speed: 0.1350s/iter; left time: 3528.3249s
	iters: 500, epoch: 3 | loss: 0.5074289
	speed: 0.1340s/iter; left time: 3490.9546s
Epoch: 3 cost time: 76.98144555091858
Epoch: 3, Steps: 553 | Train Loss: 0.5667271 Vali Loss: 0.6021541 Test Loss: 0.3131936
Validation loss decreased (0.605188 --> 0.602154).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5413280
	speed: 0.6622s/iter; left time: 17146.4023s
	iters: 200, epoch: 4 | loss: 0.5302092
	speed: 0.1411s/iter; left time: 3639.1762s
	iters: 300, epoch: 4 | loss: 0.6078925
	speed: 0.1449s/iter; left time: 3722.0278s
	iters: 400, epoch: 4 | loss: 0.4905086
	speed: 0.1497s/iter; left time: 3829.9775s
	iters: 500, epoch: 4 | loss: 0.5357016
	speed: 0.1554s/iter; left time: 3960.6816s
Epoch: 4 cost time: 83.006263256073
Epoch: 4, Steps: 553 | Train Loss: 0.5648137 Vali Loss: 0.6009545 Test Loss: 0.3119858
Validation loss decreased (0.602154 --> 0.600954).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5551029
	speed: 0.8049s/iter; left time: 20394.2718s
	iters: 200, epoch: 5 | loss: 0.6052154
	speed: 0.1544s/iter; left time: 3897.9817s
	iters: 300, epoch: 5 | loss: 0.5205814
	speed: 0.1450s/iter; left time: 3644.2770s
	iters: 400, epoch: 5 | loss: 0.6733176
	speed: 0.1426s/iter; left time: 3570.4871s
	iters: 500, epoch: 5 | loss: 0.7086720
	speed: 0.1370s/iter; left time: 3417.4349s
Epoch: 5 cost time: 83.03265380859375
Epoch: 5, Steps: 553 | Train Loss: 0.5635879 Vali Loss: 0.5979279 Test Loss: 0.3111463
Validation loss decreased (0.600954 --> 0.597928).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4989334
	speed: 0.6657s/iter; left time: 16499.3690s
	iters: 200, epoch: 6 | loss: 0.4942079
	speed: 0.1243s/iter; left time: 3068.9919s
	iters: 300, epoch: 6 | loss: 0.5499901
	speed: 0.1257s/iter; left time: 3090.3160s
	iters: 400, epoch: 6 | loss: 0.6404187
	speed: 0.1245s/iter; left time: 3048.1835s
	iters: 500, epoch: 6 | loss: 0.4817198
	speed: 0.1256s/iter; left time: 3063.7060s
Epoch: 6 cost time: 70.54804849624634
Epoch: 6, Steps: 553 | Train Loss: 0.5628234 Vali Loss: 0.5974904 Test Loss: 0.3107333
Validation loss decreased (0.597928 --> 0.597490).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5786980
	speed: 0.6255s/iter; left time: 15157.3825s
	iters: 200, epoch: 7 | loss: 0.5742558
	speed: 0.1172s/iter; left time: 2829.3516s
	iters: 300, epoch: 7 | loss: 0.4984865
	speed: 0.1272s/iter; left time: 3058.0591s
	iters: 400, epoch: 7 | loss: 0.8341072
	speed: 0.1507s/iter; left time: 3606.0802s
	iters: 500, epoch: 7 | loss: 0.6476300
	speed: 0.1679s/iter; left time: 4001.7847s
Epoch: 7 cost time: 77.04038143157959
Epoch: 7, Steps: 553 | Train Loss: 0.5621257 Vali Loss: 0.5972770 Test Loss: 0.3099252
Validation loss decreased (0.597490 --> 0.597277).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5236605
	speed: 0.6936s/iter; left time: 16424.9506s
	iters: 200, epoch: 8 | loss: 0.5362462
	speed: 0.1045s/iter; left time: 2464.9334s
	iters: 300, epoch: 8 | loss: 0.5593325
	speed: 0.1293s/iter; left time: 3035.4016s
	iters: 400, epoch: 8 | loss: 0.4837337
	speed: 0.1524s/iter; left time: 3562.3097s
	iters: 500, epoch: 8 | loss: 0.5257638
	speed: 0.1500s/iter; left time: 3492.0319s
Epoch: 8 cost time: 77.06894135475159
Epoch: 8, Steps: 553 | Train Loss: 0.5618837 Vali Loss: 0.5956059 Test Loss: 0.3098940
Validation loss decreased (0.597277 --> 0.595606).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4854499
	speed: 0.7855s/iter; left time: 18165.8526s
	iters: 200, epoch: 9 | loss: 0.5015064
	speed: 0.1722s/iter; left time: 3966.1587s
	iters: 300, epoch: 9 | loss: 0.4991860
	speed: 0.1779s/iter; left time: 4079.5794s
	iters: 400, epoch: 9 | loss: 0.4797443
	speed: 0.1658s/iter; left time: 3784.9660s
	iters: 500, epoch: 9 | loss: 0.6724896
	speed: 0.1595s/iter; left time: 3625.6358s
Epoch: 9 cost time: 94.60686707496643
Epoch: 9, Steps: 553 | Train Loss: 0.5610608 Vali Loss: 0.5953187 Test Loss: 0.3094370
Validation loss decreased (0.595606 --> 0.595319).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6580364
	speed: 0.7752s/iter; left time: 17500.3661s
	iters: 200, epoch: 10 | loss: 0.5025351
	speed: 0.1557s/iter; left time: 3500.0294s
	iters: 300, epoch: 10 | loss: 0.4490037
	speed: 0.1558s/iter; left time: 3486.3633s
	iters: 400, epoch: 10 | loss: 0.5784652
	speed: 0.1550s/iter; left time: 3452.1185s
	iters: 500, epoch: 10 | loss: 0.6051828
	speed: 0.1500s/iter; left time: 3326.3865s
Epoch: 10 cost time: 86.57699179649353
Epoch: 10, Steps: 553 | Train Loss: 0.5609758 Vali Loss: 0.5955426 Test Loss: 0.3093807
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5087497
	speed: 0.7863s/iter; left time: 17315.3128s
	iters: 200, epoch: 11 | loss: 0.6851701
	speed: 0.1607s/iter; left time: 3522.2278s
	iters: 300, epoch: 11 | loss: 0.5252975
	speed: 0.1635s/iter; left time: 3567.0300s
	iters: 400, epoch: 11 | loss: 0.5413622
	speed: 0.1541s/iter; left time: 3346.2849s
	iters: 500, epoch: 11 | loss: 0.5623670
	speed: 0.1637s/iter; left time: 3540.3511s
Epoch: 11 cost time: 90.8172242641449
Epoch: 11, Steps: 553 | Train Loss: 0.5607314 Vali Loss: 0.5949759 Test Loss: 0.3089422
Validation loss decreased (0.595319 --> 0.594976).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6779903
	speed: 0.8018s/iter; left time: 17213.5871s
	iters: 200, epoch: 12 | loss: 0.4841252
	speed: 0.1582s/iter; left time: 3380.7355s
	iters: 300, epoch: 12 | loss: 0.6191894
	speed: 0.1471s/iter; left time: 3128.4047s
	iters: 400, epoch: 12 | loss: 0.4650840
	speed: 0.1527s/iter; left time: 3233.2344s
	iters: 500, epoch: 12 | loss: 0.5719717
	speed: 0.1541s/iter; left time: 3247.1685s
Epoch: 12 cost time: 87.36622023582458
Epoch: 12, Steps: 553 | Train Loss: 0.5608189 Vali Loss: 0.5954563 Test Loss: 0.3088499
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.7096038
	speed: 0.7536s/iter; left time: 15762.4900s
	iters: 200, epoch: 13 | loss: 0.4834709
	speed: 0.1565s/iter; left time: 3256.9979s
	iters: 300, epoch: 13 | loss: 0.5146782
	speed: 0.1598s/iter; left time: 3310.7906s
	iters: 400, epoch: 13 | loss: 0.4941292
	speed: 0.1576s/iter; left time: 3249.5871s
	iters: 500, epoch: 13 | loss: 0.5167101
	speed: 0.1511s/iter; left time: 3100.3858s
Epoch: 13 cost time: 87.47012853622437
Epoch: 13, Steps: 553 | Train Loss: 0.5603262 Vali Loss: 0.5943765 Test Loss: 0.3086645
Validation loss decreased (0.594976 --> 0.594377).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5650412
	speed: 0.7225s/iter; left time: 14711.5440s
	iters: 200, epoch: 14 | loss: 0.5064114
	speed: 0.1479s/iter; left time: 2996.6075s
	iters: 300, epoch: 14 | loss: 0.6593560
	speed: 0.1538s/iter; left time: 3101.4780s
	iters: 400, epoch: 14 | loss: 0.5506776
	speed: 0.1521s/iter; left time: 3051.9734s
	iters: 500, epoch: 14 | loss: 0.5155637
	speed: 0.1449s/iter; left time: 2891.9295s
Epoch: 14 cost time: 82.48043990135193
Epoch: 14, Steps: 553 | Train Loss: 0.5603763 Vali Loss: 0.5939915 Test Loss: 0.3086321
Validation loss decreased (0.594377 --> 0.593991).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.6011635
	speed: 0.5600s/iter; left time: 11093.9672s
	iters: 200, epoch: 15 | loss: 0.5218272
	speed: 0.1124s/iter; left time: 2214.4638s
	iters: 300, epoch: 15 | loss: 0.5908921
	speed: 0.1179s/iter; left time: 2312.7749s
	iters: 400, epoch: 15 | loss: 0.6170243
	speed: 0.1031s/iter; left time: 2010.6554s
	iters: 500, epoch: 15 | loss: 0.5368303
	speed: 0.1336s/iter; left time: 2592.5812s
Epoch: 15 cost time: 67.29922389984131
Epoch: 15, Steps: 553 | Train Loss: 0.5600363 Vali Loss: 0.5941202 Test Loss: 0.3086435
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.6818261
	speed: 0.7590s/iter; left time: 14615.5034s
	iters: 200, epoch: 16 | loss: 0.5533502
	speed: 0.1397s/iter; left time: 2676.6798s
	iters: 300, epoch: 16 | loss: 0.6949329
	speed: 0.1461s/iter; left time: 2783.4979s
	iters: 400, epoch: 16 | loss: 0.5584958
	speed: 0.1485s/iter; left time: 2814.0437s
	iters: 500, epoch: 16 | loss: 0.4412191
	speed: 0.1555s/iter; left time: 2932.1679s
Epoch: 16 cost time: 83.77758121490479
Epoch: 16, Steps: 553 | Train Loss: 0.5599833 Vali Loss: 0.5934658 Test Loss: 0.3085700
Validation loss decreased (0.593991 --> 0.593466).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5754710
	speed: 0.8877s/iter; left time: 16603.5847s
	iters: 200, epoch: 17 | loss: 0.5426512
	speed: 0.1891s/iter; left time: 3517.0139s
	iters: 300, epoch: 17 | loss: 0.5587955
	speed: 0.1795s/iter; left time: 3321.3135s
	iters: 400, epoch: 17 | loss: 0.6042387
	speed: 0.1639s/iter; left time: 3016.5615s
	iters: 500, epoch: 17 | loss: 0.5279729
	speed: 0.1714s/iter; left time: 3136.7687s
Epoch: 17 cost time: 98.70149683952332
Epoch: 17, Steps: 553 | Train Loss: 0.5600267 Vali Loss: 0.5939724 Test Loss: 0.3084248
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5992003
	speed: 0.8062s/iter; left time: 14632.0398s
	iters: 200, epoch: 18 | loss: 0.5387085
	speed: 0.1615s/iter; left time: 2915.0359s
	iters: 300, epoch: 18 | loss: 0.6510246
	speed: 0.1539s/iter; left time: 2761.7489s
	iters: 400, epoch: 18 | loss: 0.6387019
	speed: 0.1598s/iter; left time: 2852.1023s
	iters: 500, epoch: 18 | loss: 0.5511506
	speed: 0.1578s/iter; left time: 2800.8591s
Epoch: 18 cost time: 89.08975315093994
Epoch: 18, Steps: 553 | Train Loss: 0.5599360 Vali Loss: 0.5939048 Test Loss: 0.3084275
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5562522
	speed: 0.7869s/iter; left time: 13846.5462s
	iters: 200, epoch: 19 | loss: 0.6252239
	speed: 0.1707s/iter; left time: 2987.0488s
	iters: 300, epoch: 19 | loss: 0.7033500
	speed: 0.1661s/iter; left time: 2889.2966s
	iters: 400, epoch: 19 | loss: 0.6141096
	speed: 0.1690s/iter; left time: 2923.9338s
	iters: 500, epoch: 19 | loss: 0.4400228
	speed: 0.1779s/iter; left time: 3059.8618s
Epoch: 19 cost time: 95.62985014915466
Epoch: 19, Steps: 553 | Train Loss: 0.5596067 Vali Loss: 0.5930191 Test Loss: 0.3084328
Validation loss decreased (0.593466 --> 0.593019).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5160801
	speed: 0.8771s/iter; left time: 14948.8044s
	iters: 200, epoch: 20 | loss: 0.6444781
	speed: 0.1823s/iter; left time: 3089.0627s
	iters: 300, epoch: 20 | loss: 0.5736236
	speed: 0.1820s/iter; left time: 3065.8775s
	iters: 400, epoch: 20 | loss: 0.4918970
	speed: 0.1696s/iter; left time: 2839.1884s
	iters: 500, epoch: 20 | loss: 0.7199377
	speed: 0.1669s/iter; left time: 2777.4908s
Epoch: 20 cost time: 98.64087700843811
Epoch: 20, Steps: 553 | Train Loss: 0.5596118 Vali Loss: 0.5934960 Test Loss: 0.3083600
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4782992
	speed: 0.8337s/iter; left time: 13748.8293s
	iters: 200, epoch: 21 | loss: 0.5622674
	speed: 0.1768s/iter; left time: 2897.8303s
	iters: 300, epoch: 21 | loss: 0.5787824
	speed: 0.1751s/iter; left time: 2852.8647s
	iters: 400, epoch: 21 | loss: 0.5250241
	speed: 0.1764s/iter; left time: 2855.8591s
	iters: 500, epoch: 21 | loss: 0.5226819
	speed: 0.1838s/iter; left time: 2956.7753s
Epoch: 21 cost time: 99.46136522293091
Epoch: 21, Steps: 553 | Train Loss: 0.5593762 Vali Loss: 0.5935361 Test Loss: 0.3084389
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5051347
	speed: 0.8732s/iter; left time: 13916.3961s
	iters: 200, epoch: 22 | loss: 0.5662168
	speed: 0.1696s/iter; left time: 2686.3487s
	iters: 300, epoch: 22 | loss: 0.5770357
	speed: 0.1681s/iter; left time: 2645.5354s
	iters: 400, epoch: 22 | loss: 0.5470752
	speed: 0.1689s/iter; left time: 2641.6590s
	iters: 500, epoch: 22 | loss: 0.4864359
	speed: 0.1686s/iter; left time: 2619.4171s
Epoch: 22 cost time: 95.37897062301636
Epoch: 22, Steps: 553 | Train Loss: 0.5595454 Vali Loss: 0.5928118 Test Loss: 0.3082330
Validation loss decreased (0.593019 --> 0.592812).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.7039757
	speed: 0.8727s/iter; left time: 13427.1649s
	iters: 200, epoch: 23 | loss: 0.6467425
	speed: 0.1843s/iter; left time: 2817.4706s
	iters: 300, epoch: 23 | loss: 0.6670452
	speed: 0.1995s/iter; left time: 3028.8754s
	iters: 400, epoch: 23 | loss: 0.6322839
	speed: 0.1914s/iter; left time: 2886.6749s
	iters: 500, epoch: 23 | loss: 0.5630784
	speed: 0.2003s/iter; left time: 3001.5526s
Epoch: 23 cost time: 106.37261319160461
Epoch: 23, Steps: 553 | Train Loss: 0.5592529 Vali Loss: 0.5937322 Test Loss: 0.3083959
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4644149
	speed: 0.9016s/iter; left time: 13373.0529s
	iters: 200, epoch: 24 | loss: 0.6045236
	speed: 0.1614s/iter; left time: 2377.5529s
	iters: 300, epoch: 24 | loss: 0.4650826
	speed: 0.1433s/iter; left time: 2097.0575s
	iters: 400, epoch: 24 | loss: 0.7396787
	speed: 0.1532s/iter; left time: 2225.7532s
	iters: 500, epoch: 24 | loss: 0.6569894
	speed: 0.1673s/iter; left time: 2413.7972s
Epoch: 24 cost time: 87.47620487213135
Epoch: 24, Steps: 553 | Train Loss: 0.5593808 Vali Loss: 0.5933982 Test Loss: 0.3081117
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5496073
	speed: 0.7680s/iter; left time: 10965.9704s
	iters: 200, epoch: 25 | loss: 0.5797333
	speed: 0.1671s/iter; left time: 2369.8451s
	iters: 300, epoch: 25 | loss: 0.5476887
	speed: 0.1777s/iter; left time: 2501.3007s
	iters: 400, epoch: 25 | loss: 0.5927317
	speed: 0.1792s/iter; left time: 2504.8746s
	iters: 500, epoch: 25 | loss: 0.5660366
	speed: 0.1815s/iter; left time: 2518.6275s
Epoch: 25 cost time: 97.48487854003906
Epoch: 25, Steps: 553 | Train Loss: 0.5592094 Vali Loss: 0.5929514 Test Loss: 0.3082123
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H12_FITS_custom_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.3075486123561859, mae:0.32945433259010315, rse:0.7297749519348145, corr:[0.47154433 0.47227275 0.47197586 0.4718947  0.47189984 0.47165415
 0.4709251  0.4697665  0.46849063 0.46738726 0.46665758 0.4663637
 0.4662146  0.4659514  0.4653832  0.46446255 0.4634243  0.46241271
 0.46163604 0.46105936 0.46057215 0.4600048  0.45930013 0.45833072
 0.45720577 0.4560427  0.45495936 0.45398295 0.45309463 0.4521677
 0.4512086  0.45028532 0.44962004 0.44923252 0.44911322 0.44903803
 0.44894958 0.44866383 0.44811785 0.44734198 0.44646856 0.44571742
 0.44516045 0.44477504 0.4444737  0.444133   0.44372943 0.44324288
 0.44250727 0.44168887 0.4409273  0.44022015 0.43966272 0.4392286
 0.4388691  0.43850124 0.4381186  0.4376891  0.43730235 0.43695456
 0.43668586 0.43643805 0.4362022  0.43590346 0.43553057 0.43511626
 0.43468693 0.43430865 0.4340244  0.4338111  0.43365481 0.4333909
 0.43308905 0.4327287  0.4323041  0.43188667 0.43156385 0.43141913
 0.431393   0.43138966 0.43139234 0.43130583 0.43107033 0.4307456
 0.43040705 0.4300742  0.4297834  0.4294245  0.42911583 0.42882416
 0.42859706 0.42844915 0.4283159  0.42821965 0.42815182 0.42807516
 0.42797652 0.42785445 0.4277048  0.42757714 0.42746806 0.4273622
 0.42725593 0.42710346 0.4268643  0.42653787 0.4261211  0.42571303
 0.42531943 0.42502394 0.42482093 0.42464787 0.4244715  0.4243095
 0.4241137  0.42384332 0.4235522  0.42328188 0.42303693 0.42278183
 0.42257208 0.42240646 0.4222403  0.42208156 0.42190346 0.42171112
 0.42148966 0.42122227 0.42093903 0.4206339  0.42027777 0.41989094
 0.4195114  0.41915885 0.41887045 0.41860542 0.4183522  0.41812298
 0.4178109  0.41752598 0.4172994  0.4171804  0.41712412 0.41711184
 0.4171219  0.4171101  0.41704208 0.41687906 0.41652834 0.4159383
 0.41518295 0.4143718  0.4135448  0.41273978 0.41207987 0.4116278
 0.4114236  0.4112548  0.41109198 0.4108903  0.41063747 0.41035113
 0.41000652 0.40964752 0.4092258  0.4087763  0.40825728 0.40771484
 0.40719876 0.4067176  0.4062465  0.40578732 0.40533516 0.4048996
 0.4044123  0.40388247 0.40330958 0.4027385  0.402184   0.40164417
 0.40117756 0.4007303  0.40026543 0.39975977 0.39925322 0.39873955
 0.39824295 0.39773002 0.39724338 0.3967923  0.39636275 0.39596272
 0.3956092  0.39521465 0.39480418 0.394363   0.39388242 0.39338154
 0.39284468 0.3923152  0.39187765 0.39145622 0.3910816  0.39072895
 0.39044574 0.39019012 0.38992262 0.38961664 0.38918045 0.3886598
 0.38803115 0.3873466  0.38664722 0.38601792 0.3854898  0.38505217
 0.38467133 0.38444027 0.38421604 0.3839227  0.38356897 0.38319498
 0.38277632 0.38229424 0.3817842  0.38125405 0.38076323 0.3804049
 0.3801681  0.38006756 0.3800527  0.38002968 0.37991858 0.37964875
 0.37924162 0.37875122 0.37815863 0.37754118 0.3769313  0.3764347
 0.37613556 0.3760169  0.37591872 0.37583727 0.3757204  0.3754829
 0.37508643 0.37459147 0.37402412 0.37343836 0.3728694  0.37236437
 0.37189007 0.3714216  0.37100646 0.370578   0.37017334 0.36979955
 0.3695153  0.36928973 0.36899328 0.36868507 0.3683611  0.3680857
 0.3678669  0.36770332 0.36755288 0.36741996 0.36729622 0.3670606
 0.36673567 0.36639732 0.36604968 0.36566898 0.36535856 0.3651187
 0.3649108  0.3646347  0.36431763 0.3639237  0.3634333  0.36292887
 0.36247602 0.3620125  0.36158758 0.36116332 0.3606979  0.36011577
 0.35942638 0.35863665 0.357756   0.35681868 0.356009   0.35537922
 0.35485744 0.35442546 0.35401228 0.3535723  0.3530291  0.35248128
 0.35181922 0.3511575  0.35053608 0.35002002 0.3495856  0.34917533
 0.34875527 0.34829268 0.3477912  0.34723315 0.34672648 0.34623623
 0.34578407 0.34540182 0.34501833 0.34458035 0.3440998  0.3435561
 0.3429841  0.34237182 0.3417404  0.34113532 0.3405793  0.33998454
 0.3393703  0.33877176 0.33817208 0.33766937 0.33732754 0.33708102
 0.3369174  0.3367415  0.3364902  0.3361765  0.33573094 0.33521903
 0.33466604 0.334155   0.33375308 0.33345148 0.333216   0.33304703
 0.33288044 0.33261958 0.33228457 0.33191407 0.33158252 0.3312761
 0.3310374  0.3307891  0.33052513 0.3301796  0.32978678 0.32932538
 0.3288288  0.32836097 0.32798648 0.32767203 0.32742903 0.3272774
 0.3271418  0.32702345 0.32694638 0.326817   0.32664388 0.32642967
 0.3261596  0.3258705  0.32560408 0.32533544 0.32511395 0.3249543
 0.32486197 0.32478034 0.32468048 0.32452857 0.3242793  0.32391384
 0.32346243 0.32301083 0.32258868 0.3222466  0.32202014 0.3218049
 0.3215751  0.32128924 0.32095894 0.3205826  0.32021344 0.31989375
 0.31959873 0.3194033  0.3192362  0.31906298 0.3188445  0.3185759
 0.31827676 0.31799948 0.3177856  0.31765237 0.31760585 0.31757155
 0.3175424  0.31740156 0.31716636 0.3168321  0.31646246 0.3161741
 0.3160463  0.31606078 0.31616995 0.31622553 0.31619638 0.3160466
 0.31576014 0.31535837 0.3149451  0.3146258  0.31435725 0.3141728
 0.3140023  0.3138564  0.31364086 0.31339046 0.31306022 0.31268114
 0.31228513 0.31195572 0.31162184 0.31122822 0.31078026 0.31033108
 0.30984813 0.30934823 0.30887595 0.30841735 0.30792752 0.30737516
 0.30674183 0.30605668 0.30537954 0.30471712 0.30408004 0.3033869
 0.30270523 0.30202127 0.30128655 0.3005585  0.2998009  0.29907754
 0.29845327 0.29792014 0.2975102  0.2972051  0.29698536 0.2967254
 0.29638207 0.29592434 0.2953502  0.29468006 0.2939745  0.293363
 0.29285845 0.2924776  0.29220542 0.29196528 0.29169232 0.2913306
 0.2909137  0.29046294 0.2900014  0.28960094 0.2893413  0.28920206
 0.2891481  0.28912994 0.2890788  0.2889764  0.28883135 0.2886263
 0.28843278 0.28824356 0.2880351  0.28781366 0.28759354 0.28740758
 0.28719172 0.28697696 0.2867961  0.2866448  0.28651372 0.28636637
 0.2862182  0.28607273 0.2858846  0.2856432  0.2853277  0.28499165
 0.28465363 0.2843438  0.28405583 0.28378406 0.28359523 0.28348446
 0.28342813 0.28341627 0.28346574 0.28344864 0.28341776 0.2833335
 0.28323725 0.28311178 0.28295603 0.2827211  0.28240415 0.2820156
 0.281562   0.28106788 0.2805433  0.28006485 0.27967328 0.27937406
 0.27919093 0.27907887 0.2789524  0.2788036  0.27854875 0.27820197
 0.27776745 0.2773018  0.27685624 0.27646673 0.27615365 0.27582732
 0.27546015 0.27502236 0.2745112  0.27399212 0.27352983 0.27316943
 0.2729475  0.27288085 0.27286464 0.27284718 0.27276275 0.2725488
 0.27222568 0.27185774 0.27150023 0.27115506 0.27084473 0.27050376
 0.27011314 0.2696586  0.26916832 0.26862705 0.26813614 0.26771212
 0.26742548 0.2672371  0.2671116  0.26699746 0.2667825  0.266428
 0.2659656  0.26546118 0.26497    0.26454684 0.26421288 0.26397598
 0.26376754 0.26352194 0.26319492 0.26272765 0.26212046 0.26137027
 0.26082176 0.26017505 0.25954884 0.25929096 0.25904977 0.25878856
 0.2584334  0.25796497 0.25737238 0.25674373 0.25608513 0.25550675
 0.25502142 0.2546309  0.25430557 0.25403783 0.25377783 0.25352085
 0.25321048 0.2528415  0.25243872 0.25202087 0.25160527 0.2512042
 0.25082508 0.25041792 0.24997044 0.24944748 0.24884953 0.2482353
 0.24757911 0.24700338 0.24649733 0.24608211 0.2458113  0.24563943
 0.24550863 0.24538007 0.24514206 0.2447651  0.24428621 0.24366736
 0.24304898 0.24256627 0.24222694 0.24203695 0.24196728 0.24196322
 0.2418658  0.2416431  0.24127622 0.24080217 0.24035624 0.23994504
 0.23965281 0.2394618  0.23938304 0.23932865 0.23924698 0.2391257
 0.23896807 0.23878688 0.23859307 0.23838353 0.23820674 0.23803869
 0.23790304 0.23777887 0.23764972 0.23748353 0.23737167 0.23733187
 0.23740232 0.23747876 0.23757678 0.23754324 0.23743545 0.23719808
 0.23692884 0.23666593 0.23648119 0.23641385 0.23646638 0.23659402
 0.23672403 0.23674223 0.23659061 0.23627114 0.23588769 0.23554252
 0.23530586 0.23521091 0.23517697 0.23514852 0.23503381 0.23470418
 0.23419346 0.23355407 0.2328403  0.23218912 0.23170362 0.23145193
 0.23138936 0.23144267 0.23151521 0.23145866 0.23126577 0.2309201
 0.23049325 0.23008879 0.22981265 0.22964826 0.22954525 0.22941703
 0.2291898  0.22880332 0.22838429 0.22804718 0.22783712 0.22774494
 0.22774222 0.22770299 0.22752473 0.22716464 0.22669104 0.22617109
 0.22578014 0.22558787 0.2255405  0.22553723 0.22545086 0.22523314
 0.22489731 0.22457965 0.22446193 0.22465035 0.2249239  0.22509585
 0.22484156 0.22436069 0.2238306  0.22377926 0.22439529 0.22509609]
