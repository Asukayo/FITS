Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=70, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=140, bias=True)
    (1): Linear(in_features=70, out_features=140, bias=True)
    (2): Linear(in_features=70, out_features=140, bias=True)
    (3): Linear(in_features=70, out_features=140, bias=True)
    (4): Linear(in_features=70, out_features=140, bias=True)
    (5): Linear(in_features=70, out_features=140, bias=True)
    (6): Linear(in_features=70, out_features=140, bias=True)
    (7): Linear(in_features=70, out_features=140, bias=True)
    (8): Linear(in_features=70, out_features=140, bias=True)
    (9): Linear(in_features=70, out_features=140, bias=True)
    (10): Linear(in_features=70, out_features=140, bias=True)
    (11): Linear(in_features=70, out_features=140, bias=True)
    (12): Linear(in_features=70, out_features=140, bias=True)
    (13): Linear(in_features=70, out_features=140, bias=True)
    (14): Linear(in_features=70, out_features=140, bias=True)
    (15): Linear(in_features=70, out_features=140, bias=True)
    (16): Linear(in_features=70, out_features=140, bias=True)
    (17): Linear(in_features=70, out_features=140, bias=True)
    (18): Linear(in_features=70, out_features=140, bias=True)
    (19): Linear(in_features=70, out_features=140, bias=True)
    (20): Linear(in_features=70, out_features=140, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  52684800.0
params:  208740.0
Trainable parameters:  208740
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7182722
	speed: 0.2078s/iter; left time: 1413.3963s
Epoch: 1 cost time: 28.038023233413696
Epoch: 1, Steps: 138 | Train Loss: 0.7995545 Vali Loss: 0.8084015 Test Loss: 0.3779694
Validation loss decreased (inf --> 0.808401).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5691545
	speed: 0.5039s/iter; left time: 3357.4474s
Epoch: 2 cost time: 33.72173357009888
Epoch: 2, Steps: 138 | Train Loss: 0.5999413 Vali Loss: 0.7330122 Test Loss: 0.3522617
Validation loss decreased (0.808401 --> 0.733012).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5167974
	speed: 0.5348s/iter; left time: 3489.3902s
Epoch: 3 cost time: 33.29444646835327
Epoch: 3, Steps: 138 | Train Loss: 0.5282932 Vali Loss: 0.7024400 Test Loss: 0.3435076
Validation loss decreased (0.733012 --> 0.702440).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4467184
	speed: 0.5123s/iter; left time: 3272.3272s
Epoch: 4 cost time: 30.637159824371338
Epoch: 4, Steps: 138 | Train Loss: 0.4884000 Vali Loss: 0.6848250 Test Loss: 0.3383107
Validation loss decreased (0.702440 --> 0.684825).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4449206
	speed: 0.4963s/iter; left time: 3101.0827s
Epoch: 5 cost time: 29.7443687915802
Epoch: 5, Steps: 138 | Train Loss: 0.4603758 Vali Loss: 0.6705393 Test Loss: 0.3343230
Validation loss decreased (0.684825 --> 0.670539).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4707587
	speed: 0.4973s/iter; left time: 3039.1437s
Epoch: 6 cost time: 30.94122552871704
Epoch: 6, Steps: 138 | Train Loss: 0.4389729 Vali Loss: 0.6545883 Test Loss: 0.3311297
Validation loss decreased (0.670539 --> 0.654588).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3964157
	speed: 0.4814s/iter; left time: 2875.1859s
Epoch: 7 cost time: 30.317475080490112
Epoch: 7, Steps: 138 | Train Loss: 0.4219227 Vali Loss: 0.6457660 Test Loss: 0.3284692
Validation loss decreased (0.654588 --> 0.645766).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3906891
	speed: 0.4933s/iter; left time: 2878.4802s
Epoch: 8 cost time: 31.526779413223267
Epoch: 8, Steps: 138 | Train Loss: 0.4085664 Vali Loss: 0.6377337 Test Loss: 0.3261349
Validation loss decreased (0.645766 --> 0.637734).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3887515
	speed: 0.4791s/iter; left time: 2729.6152s
Epoch: 9 cost time: 29.985172033309937
Epoch: 9, Steps: 138 | Train Loss: 0.3977033 Vali Loss: 0.6283650 Test Loss: 0.3242010
Validation loss decreased (0.637734 --> 0.628365).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3567464
	speed: 0.4675s/iter; left time: 2598.8463s
Epoch: 10 cost time: 29.976444959640503
Epoch: 10, Steps: 138 | Train Loss: 0.3886728 Vali Loss: 0.6259677 Test Loss: 0.3226691
Validation loss decreased (0.628365 --> 0.625968).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4148549
	speed: 0.5055s/iter; left time: 2740.3269s
Epoch: 11 cost time: 33.827985763549805
Epoch: 11, Steps: 138 | Train Loss: 0.3811260 Vali Loss: 0.6199517 Test Loss: 0.3213656
Validation loss decreased (0.625968 --> 0.619952).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3665542
	speed: 0.5675s/iter; left time: 2997.9613s
Epoch: 12 cost time: 35.97328019142151
Epoch: 12, Steps: 138 | Train Loss: 0.3750067 Vali Loss: 0.6146663 Test Loss: 0.3201424
Validation loss decreased (0.619952 --> 0.614666).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3862564
	speed: 0.5496s/iter; left time: 2827.6652s
Epoch: 13 cost time: 33.48476433753967
Epoch: 13, Steps: 138 | Train Loss: 0.3695594 Vali Loss: 0.6098810 Test Loss: 0.3191988
Validation loss decreased (0.614666 --> 0.609881).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3878901
	speed: 0.5286s/iter; left time: 2646.7668s
Epoch: 14 cost time: 32.41451334953308
Epoch: 14, Steps: 138 | Train Loss: 0.3651414 Vali Loss: 0.6090608 Test Loss: 0.3184626
Validation loss decreased (0.609881 --> 0.609061).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3856852
	speed: 0.4756s/iter; left time: 2315.6914s
Epoch: 15 cost time: 27.788824319839478
Epoch: 15, Steps: 138 | Train Loss: 0.3610757 Vali Loss: 0.6051607 Test Loss: 0.3178664
Validation loss decreased (0.609061 --> 0.605161).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3497504
	speed: 0.4768s/iter; left time: 2255.5619s
Epoch: 16 cost time: 29.85853338241577
Epoch: 16, Steps: 138 | Train Loss: 0.3579944 Vali Loss: 0.6022986 Test Loss: 0.3173482
Validation loss decreased (0.605161 --> 0.602299).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3616154
	speed: 0.4399s/iter; left time: 2020.3798s
Epoch: 17 cost time: 27.585081338882446
Epoch: 17, Steps: 138 | Train Loss: 0.3552238 Vali Loss: 0.6038458 Test Loss: 0.3169186
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3606315
	speed: 0.4817s/iter; left time: 2145.7935s
Epoch: 18 cost time: 27.752633571624756
Epoch: 18, Steps: 138 | Train Loss: 0.3525522 Vali Loss: 0.6027802 Test Loss: 0.3165859
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3714437
	speed: 0.3898s/iter; left time: 1682.6997s
Epoch: 19 cost time: 25.261070489883423
Epoch: 19, Steps: 138 | Train Loss: 0.3505074 Vali Loss: 0.6013055 Test Loss: 0.3162045
Validation loss decreased (0.602299 --> 0.601305).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3583895
	speed: 0.4543s/iter; left time: 1898.6789s
Epoch: 20 cost time: 30.377471685409546
Epoch: 20, Steps: 138 | Train Loss: 0.3485683 Vali Loss: 0.5998855 Test Loss: 0.3159556
Validation loss decreased (0.601305 --> 0.599886).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3307708
	speed: 0.4566s/iter; left time: 1844.9949s
Epoch: 21 cost time: 26.54618215560913
Epoch: 21, Steps: 138 | Train Loss: 0.3469939 Vali Loss: 0.5960774 Test Loss: 0.3157514
Validation loss decreased (0.599886 --> 0.596077).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3367676
	speed: 0.4266s/iter; left time: 1665.1500s
Epoch: 22 cost time: 26.210813760757446
Epoch: 22, Steps: 138 | Train Loss: 0.3454204 Vali Loss: 0.6004560 Test Loss: 0.3155665
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3577462
	speed: 0.4765s/iter; left time: 1794.1862s
Epoch: 23 cost time: 31.070855140686035
Epoch: 23, Steps: 138 | Train Loss: 0.3443526 Vali Loss: 0.5960416 Test Loss: 0.3153891
Validation loss decreased (0.596077 --> 0.596042).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3557160
	speed: 0.4553s/iter; left time: 1651.2790s
Epoch: 24 cost time: 28.143507957458496
Epoch: 24, Steps: 138 | Train Loss: 0.3430531 Vali Loss: 0.5981498 Test Loss: 0.3152920
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3431202
	speed: 0.4674s/iter; left time: 1630.5974s
Epoch: 25 cost time: 30.51663064956665
Epoch: 25, Steps: 138 | Train Loss: 0.3420777 Vali Loss: 0.5944771 Test Loss: 0.3151708
Validation loss decreased (0.596042 --> 0.594477).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3272508
	speed: 0.4868s/iter; left time: 1631.1608s
Epoch: 26 cost time: 29.824206590652466
Epoch: 26, Steps: 138 | Train Loss: 0.3412092 Vali Loss: 0.5961282 Test Loss: 0.3150393
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3503453
	speed: 0.4637s/iter; left time: 1489.7253s
Epoch: 27 cost time: 28.96041965484619
Epoch: 27, Steps: 138 | Train Loss: 0.3405758 Vali Loss: 0.5964509 Test Loss: 0.3149718
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3571819
	speed: 0.5159s/iter; left time: 1586.3691s
Epoch: 28 cost time: 33.37939691543579
Epoch: 28, Steps: 138 | Train Loss: 0.3397428 Vali Loss: 0.5931190 Test Loss: 0.3149725
Validation loss decreased (0.594477 --> 0.593119).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3224366
	speed: 0.4908s/iter; left time: 1441.4932s
Epoch: 29 cost time: 27.806382179260254
Epoch: 29, Steps: 138 | Train Loss: 0.3393059 Vali Loss: 0.5947198 Test Loss: 0.3149107
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3092708
	speed: 0.4678s/iter; left time: 1309.3574s
Epoch: 30 cost time: 28.717586517333984
Epoch: 30, Steps: 138 | Train Loss: 0.3385854 Vali Loss: 0.5981319 Test Loss: 0.3148244
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3662782
	speed: 0.4613s/iter; left time: 1227.5709s
Epoch: 31 cost time: 27.380863666534424
Epoch: 31, Steps: 138 | Train Loss: 0.3381049 Vali Loss: 0.5943317 Test Loss: 0.3147902
EarlyStopping counter: 3 out of 3
Early stopping
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=140, bias=True)
    (1): Linear(in_features=70, out_features=140, bias=True)
    (2): Linear(in_features=70, out_features=140, bias=True)
    (3): Linear(in_features=70, out_features=140, bias=True)
    (4): Linear(in_features=70, out_features=140, bias=True)
    (5): Linear(in_features=70, out_features=140, bias=True)
    (6): Linear(in_features=70, out_features=140, bias=True)
    (7): Linear(in_features=70, out_features=140, bias=True)
    (8): Linear(in_features=70, out_features=140, bias=True)
    (9): Linear(in_features=70, out_features=140, bias=True)
    (10): Linear(in_features=70, out_features=140, bias=True)
    (11): Linear(in_features=70, out_features=140, bias=True)
    (12): Linear(in_features=70, out_features=140, bias=True)
    (13): Linear(in_features=70, out_features=140, bias=True)
    (14): Linear(in_features=70, out_features=140, bias=True)
    (15): Linear(in_features=70, out_features=140, bias=True)
    (16): Linear(in_features=70, out_features=140, bias=True)
    (17): Linear(in_features=70, out_features=140, bias=True)
    (18): Linear(in_features=70, out_features=140, bias=True)
    (19): Linear(in_features=70, out_features=140, bias=True)
    (20): Linear(in_features=70, out_features=140, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  52684800.0
params:  208740.0
Trainable parameters:  208740
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5364974
	speed: 0.2096s/iter; left time: 1425.5590s
Epoch: 1 cost time: 28.539085865020752
Epoch: 1, Steps: 138 | Train Loss: 0.5648131 Vali Loss: 0.5964715 Test Loss: 0.3145451
Validation loss decreased (inf --> 0.596471).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6382334
	speed: 0.4540s/iter; left time: 3024.8818s
Epoch: 2 cost time: 29.024285793304443
Epoch: 2, Steps: 138 | Train Loss: 0.5626070 Vali Loss: 0.5979674 Test Loss: 0.3142853
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5296465
	speed: 0.5014s/iter; left time: 3271.4729s
Epoch: 3 cost time: 32.261415243148804
Epoch: 3, Steps: 138 | Train Loss: 0.5613215 Vali Loss: 0.5963883 Test Loss: 0.3137630
Validation loss decreased (0.596471 --> 0.596388).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6358368
	speed: 0.4707s/iter; left time: 3006.4908s
Epoch: 4 cost time: 27.60617423057556
Epoch: 4, Steps: 138 | Train Loss: 0.5615969 Vali Loss: 0.5962669 Test Loss: 0.3134302
Validation loss decreased (0.596388 --> 0.596267).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6876915
	speed: 0.4525s/iter; left time: 2827.5842s
Epoch: 5 cost time: 29.981831073760986
Epoch: 5, Steps: 138 | Train Loss: 0.5615507 Vali Loss: 0.5948342 Test Loss: 0.3133869
Validation loss decreased (0.596267 --> 0.594834).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5986056
	speed: 0.4747s/iter; left time: 2901.1584s
Epoch: 6 cost time: 29.03116250038147
Epoch: 6, Steps: 138 | Train Loss: 0.5611125 Vali Loss: 0.5953925 Test Loss: 0.3131472
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6300400
	speed: 0.4830s/iter; left time: 2884.9896s
Epoch: 7 cost time: 31.01961660385132
Epoch: 7, Steps: 138 | Train Loss: 0.5606334 Vali Loss: 0.5950114 Test Loss: 0.3130771
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6648661
	speed: 0.5581s/iter; left time: 3256.2524s
Epoch: 8 cost time: 33.79845476150513
Epoch: 8, Steps: 138 | Train Loss: 0.5603471 Vali Loss: 0.5937511 Test Loss: 0.3130161
Validation loss decreased (0.594834 --> 0.593751).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5221151
	speed: 0.5209s/iter; left time: 2967.3171s
Epoch: 9 cost time: 30.94242835044861
Epoch: 9, Steps: 138 | Train Loss: 0.5605463 Vali Loss: 0.5931400 Test Loss: 0.3130878
Validation loss decreased (0.593751 --> 0.593140).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5773259
	speed: 0.4504s/iter; left time: 2503.8307s
Epoch: 10 cost time: 27.58393096923828
Epoch: 10, Steps: 138 | Train Loss: 0.5606371 Vali Loss: 0.5928335 Test Loss: 0.3128894
Validation loss decreased (0.593140 --> 0.592834).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5099750
	speed: 0.4279s/iter; left time: 2319.5848s
Epoch: 11 cost time: 26.35853147506714
Epoch: 11, Steps: 138 | Train Loss: 0.5603180 Vali Loss: 0.5953336 Test Loss: 0.3128604
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5581899
	speed: 0.4280s/iter; left time: 2261.3230s
Epoch: 12 cost time: 26.690578937530518
Epoch: 12, Steps: 138 | Train Loss: 0.5600166 Vali Loss: 0.5947315 Test Loss: 0.3127199
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5645409
	speed: 0.4542s/iter; left time: 2336.9776s
Epoch: 13 cost time: 30.418615341186523
Epoch: 13, Steps: 138 | Train Loss: 0.5601769 Vali Loss: 0.5928277 Test Loss: 0.3125717
Validation loss decreased (0.592834 --> 0.592828).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5023847
	speed: 0.5062s/iter; left time: 2534.3609s
Epoch: 14 cost time: 32.545355796813965
Epoch: 14, Steps: 138 | Train Loss: 0.5604822 Vali Loss: 0.5951886 Test Loss: 0.3125581
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5743214
	speed: 0.5079s/iter; left time: 2472.7219s
Epoch: 15 cost time: 30.76764941215515
Epoch: 15, Steps: 138 | Train Loss: 0.5602227 Vali Loss: 0.5908701 Test Loss: 0.3125449
Validation loss decreased (0.592828 --> 0.590870).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.6315866
	speed: 0.4958s/iter; left time: 2345.5580s
Epoch: 16 cost time: 31.009015560150146
Epoch: 16, Steps: 138 | Train Loss: 0.5604915 Vali Loss: 0.5938390 Test Loss: 0.3125173
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.6797754
	speed: 0.5335s/iter; left time: 2450.3002s
Epoch: 17 cost time: 33.645960330963135
Epoch: 17, Steps: 138 | Train Loss: 0.5597779 Vali Loss: 0.5944074 Test Loss: 0.3124208
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5812261
	speed: 0.5098s/iter; left time: 2271.2565s
Epoch: 18 cost time: 31.425450563430786
Epoch: 18, Steps: 138 | Train Loss: 0.5598979 Vali Loss: 0.5939851 Test Loss: 0.3124146
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.3079604506492615, mae:0.3300248682498932, rse:0.7302634119987488, corr:[0.4694759  0.4734606  0.47413927 0.47323272 0.4717092  0.47028276
 0.46936384 0.46895757 0.46880168 0.46853706 0.4679733  0.46719655
 0.4662769  0.46539995 0.4647046  0.46417272 0.46376726 0.46328327
 0.46261585 0.4616858  0.46057597 0.45941508 0.4583874  0.45745426
 0.4566465  0.45590845 0.45519263 0.45443127 0.45364898 0.45284796
 0.45212245 0.45149884 0.45105326 0.45068756 0.45038635 0.45000407
 0.44957283 0.4489951  0.44827762 0.44748405 0.4467253  0.44613832
 0.4457061  0.44534007 0.44496632 0.4445119  0.44399664 0.4434405
 0.44271746 0.44197622 0.44132113 0.44071475 0.4402239  0.4398198
 0.4394583  0.43906668 0.43864077 0.43813834 0.43763447 0.437132
 0.43669868 0.43633646 0.43607563 0.43586588 0.43566802 0.43546706
 0.43520042 0.43486667 0.43448985 0.43408915 0.43373102 0.433325
 0.43298    0.43267876 0.4323748  0.43206438 0.43176138 0.43151382
 0.43130034 0.431105   0.43097773 0.43086243 0.4306956  0.43050665
 0.4303299  0.43014994 0.429979   0.4297098  0.42945406 0.42919004
 0.428981   0.42884937 0.42873287 0.4286435  0.42856804 0.42846394
 0.428317   0.42811862 0.4278659  0.42761457 0.42737892 0.42716667
 0.42698827 0.42681646 0.42661053 0.4263514  0.42601752 0.4256773
 0.42531264 0.42501017 0.42477247 0.42455828 0.4243506  0.42419302
 0.42403767 0.42383733 0.4236244  0.42341563 0.42319855 0.42292857
 0.42267868 0.4224685  0.42226523 0.4220823  0.42187542 0.4216336
 0.42134002 0.42098272 0.42062002 0.42027923 0.4199659  0.4197121
 0.41954288 0.4194214  0.4193381  0.4192129  0.41902357 0.41878027
 0.41839874 0.41801947 0.41770303 0.41749334 0.41733366 0.41718718
 0.41702652 0.41682345 0.41657722 0.41628802 0.4158909  0.41534105
 0.41469604 0.41405022 0.4134214  0.41281825 0.41232684 0.4119057
 0.4115827  0.41118026 0.41074452 0.41029525 0.40989113 0.40954652
 0.40923706 0.408972   0.40867206 0.40833762 0.40788445 0.4073304
 0.4067319  0.40611416 0.4055085  0.4049698  0.4045229  0.4041874
 0.40387902 0.40356025 0.40316936 0.40269962 0.40214008 0.40148577
 0.40082937 0.40018022 0.3995663  0.39902335 0.39860147 0.39826712
 0.3979903  0.39768136 0.39733544 0.39694622 0.39650092 0.3960269
 0.39558458 0.3950978  0.39463526 0.39417183 0.39368716 0.39318997
 0.39265606 0.3921196  0.39164698 0.39119366 0.39076716 0.39033946
 0.389972   0.3896389  0.38931537 0.38899276 0.38858965 0.38815773
 0.38768095 0.38718203 0.38667068 0.38619143 0.38574222 0.38529703
 0.38482547 0.3844391  0.3840486  0.38362563 0.3832127  0.38288376
 0.38261604 0.38237604 0.38215303 0.3818988  0.38160378 0.3813001
 0.38095066 0.38057148 0.3801581  0.37970412 0.37922344 0.37873432
 0.37828487 0.37791273 0.37756488 0.37725115 0.3769036  0.37655225
 0.37624004 0.3759571  0.37560466 0.37525043 0.3749172  0.37458125
 0.37421477 0.3738682  0.37352037 0.37319115 0.37287498 0.3725917
 0.37229532 0.37194055 0.3716025  0.3711699  0.37070563 0.37023318
 0.36982894 0.369471   0.36907074 0.36871147 0.3683596  0.3681009
 0.3679093  0.36776346 0.36763462 0.3675293  0.3674442  0.36729744
 0.36710268 0.36691788 0.36669534 0.36638862 0.36607197 0.3657448
 0.36537927 0.3649061  0.36438835 0.36381996 0.36320534 0.3626307
 0.3621529  0.3616946  0.36130226 0.36094704 0.36059305 0.36017615
 0.35970572 0.35915974 0.35851085 0.357741   0.35698918 0.35630405
 0.35564366 0.35503367 0.35446003 0.35391852 0.3533437  0.35281008
 0.35217237 0.35150236 0.3508033  0.35013524 0.3495082  0.3489214
 0.3483924  0.34789744 0.34743112 0.34692344 0.34641856 0.3458583
 0.3452626  0.34470457 0.3441679  0.34365436 0.34319153 0.34273794
 0.34228435 0.34177506 0.34120482 0.34060875 0.34004068 0.33946773
 0.3389528  0.33851618 0.3381028  0.33774728 0.33745283 0.33712402
 0.33676323 0.3363315  0.33584464 0.3353837  0.33492374 0.33452767
 0.33417487 0.33387524 0.33364663 0.3334236  0.33317143 0.3329279
 0.33268642 0.3323844  0.3320539  0.33171633 0.33141387 0.3311306
 0.33090556 0.33069083 0.3304987  0.3302779  0.33005214 0.3297926
 0.32949883 0.32919592 0.32891303 0.32860625 0.32829654 0.32800758
 0.32770407 0.3274083  0.3271558  0.32688648 0.32661504 0.32635233
 0.326081   0.32582682 0.3256107  0.32537445 0.32513568 0.32488903
 0.32464585 0.32438117 0.32411456 0.32386386 0.32361624 0.32334954
 0.32305676 0.32276535 0.32243988 0.32209784 0.3217786  0.32141852
 0.32106033 0.3207195  0.32043937 0.32019415 0.3199922  0.3198147
 0.31959257 0.31939566 0.3191832  0.3189745  0.31877708 0.31860578
 0.31846026 0.3183471  0.31823674 0.3181119  0.31797093 0.31777817
 0.317596   0.31736833 0.31714526 0.3169107  0.31666526 0.31644097
 0.31626436 0.31610775 0.31595236 0.31572443 0.3154732  0.31522
 0.3149666  0.31471688 0.31451952 0.31440905 0.31430796 0.31422433
 0.31408846 0.3139413  0.31369412 0.31339395 0.31299624 0.3125218
 0.3120095  0.31154808 0.3110982  0.3106291  0.3101578  0.30974326
 0.30932587 0.3088994  0.30849287 0.3080925  0.30767173 0.307217
 0.30670944 0.30616337 0.30561078 0.30503008 0.30443388 0.30373803
 0.30303693 0.30234307 0.3016393  0.30098158 0.30034342 0.29973543
 0.29916066 0.29855233 0.297925   0.2972862  0.29669604 0.29612726
 0.29562268 0.2951984  0.2948389  0.29449353 0.29410684 0.2937085
 0.2932504  0.29275313 0.29226696 0.29181725 0.2914365  0.29112622
 0.29091576 0.29077402 0.29063386 0.29047316 0.29030627 0.29009876
 0.28985634 0.28959653 0.28932816 0.28908858 0.28889933 0.28872395
 0.2885912  0.2884566  0.28826952 0.28802738 0.28774917 0.28747728
 0.28717482 0.28688452 0.28663948 0.28643194 0.28625405 0.28606907
 0.28589493 0.28573227 0.2855351  0.28530642 0.28504527 0.28480923
 0.2846196  0.2844918  0.28439337 0.28428164 0.28417984 0.28406677
 0.28390914 0.2837265  0.2835685  0.28336698 0.28319874 0.28304183
 0.2829385  0.28285262 0.28275755 0.28257695 0.2823068  0.2819595
 0.28155544 0.28112975 0.2806993  0.28032085 0.2800057  0.2797229
 0.2794795  0.2792334  0.2789283  0.27859995 0.27820894 0.27780122
 0.27738425 0.27699474 0.2766638  0.27639964 0.2762341  0.27607965
 0.27591756 0.2757079  0.2754167  0.27505976 0.2746643  0.27425486
 0.2738804  0.27361327 0.2734073  0.27327263 0.27316922 0.27301365
 0.27277663 0.27246562 0.27209818 0.2716693  0.27123547 0.27078554
 0.27036244 0.26998818 0.269688   0.2693978  0.26913998 0.26885208
 0.2685568  0.26820907 0.2678422  0.26748994 0.2671127  0.26672003
 0.26633045 0.26595205 0.26556227 0.26515153 0.26471418 0.26429155
 0.26387715 0.26348454 0.26312044 0.262731   0.26228085 0.2616937
 0.2612221  0.26049504 0.25964695 0.25912228 0.258623   0.25820753
 0.257859   0.25757056 0.25729108 0.2570322  0.2567077  0.25634506
 0.25590643 0.2553924  0.25480568 0.2542092  0.25363114 0.25312755
 0.25269574 0.2523326  0.2520399  0.25179514 0.2515687  0.25133798
 0.2510898  0.250773   0.2503955  0.24995533 0.2494843  0.24903692
 0.248577   0.24817042 0.24775936 0.24733531 0.24692461 0.24650888
 0.24609607 0.24572843 0.24536338 0.24501342 0.24469608 0.24433132
 0.24396761 0.24365096 0.24334787 0.2430585  0.24280351 0.24261525
 0.24241826 0.2422302  0.24203084 0.24180698 0.24162367 0.2414213
 0.24123912 0.24105184 0.2408959  0.24073063 0.24054645 0.24035354
 0.2401495  0.23991634 0.23964101 0.23932478 0.2390374  0.23880161
 0.2386878  0.23869424 0.23879299 0.23889378 0.23900497 0.23907363
 0.23909995 0.23901346 0.23888762 0.23866294 0.23847024 0.23828416
 0.2381743  0.2380889  0.23799051 0.23783138 0.23759067 0.2372759
 0.23693018 0.23657364 0.23624948 0.23599266 0.23585957 0.2358437
 0.23588872 0.23593119 0.23586862 0.23569776 0.23542783 0.23504415
 0.23465995 0.23433162 0.23405915 0.23386842 0.23373048 0.23362829
 0.23348796 0.23328312 0.23301679 0.23265794 0.23228389 0.23189907
 0.23154476 0.23126407 0.23111024 0.23103061 0.2309618  0.23083824
 0.23060544 0.23020758 0.22975099 0.22931819 0.22896078 0.22871846
 0.22864358 0.22868112 0.2287463  0.22873616 0.22859384 0.22824782
 0.22778603 0.22729328 0.22684038 0.22652204 0.22637163 0.22636652
 0.22636993 0.22622797 0.22585061 0.22525522 0.22438747 0.22349395
 0.22270983 0.22244605 0.22254795 0.22278044 0.22242896 0.22028446]
