Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H12', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j720_H12_FITS_custom_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=82, out_features=164, bias=True)
    (1): Linear(in_features=82, out_features=164, bias=True)
    (2): Linear(in_features=82, out_features=164, bias=True)
    (3): Linear(in_features=82, out_features=164, bias=True)
    (4): Linear(in_features=82, out_features=164, bias=True)
    (5): Linear(in_features=82, out_features=164, bias=True)
    (6): Linear(in_features=82, out_features=164, bias=True)
    (7): Linear(in_features=82, out_features=164, bias=True)
    (8): Linear(in_features=82, out_features=164, bias=True)
    (9): Linear(in_features=82, out_features=164, bias=True)
    (10): Linear(in_features=82, out_features=164, bias=True)
    (11): Linear(in_features=82, out_features=164, bias=True)
    (12): Linear(in_features=82, out_features=164, bias=True)
    (13): Linear(in_features=82, out_features=164, bias=True)
    (14): Linear(in_features=82, out_features=164, bias=True)
    (15): Linear(in_features=82, out_features=164, bias=True)
    (16): Linear(in_features=82, out_features=164, bias=True)
    (17): Linear(in_features=82, out_features=164, bias=True)
    (18): Linear(in_features=82, out_features=164, bias=True)
    (19): Linear(in_features=82, out_features=164, bias=True)
    (20): Linear(in_features=82, out_features=164, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  36148224.0
params:  285852.0
Trainable parameters:  285852
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8191620
	speed: 0.1726s/iter; left time: 2364.9879s
	iters: 200, epoch: 1 | loss: 0.6473000
	speed: 0.1729s/iter; left time: 2351.4610s
Epoch: 1 cost time: 48.4716956615448
Epoch: 1, Steps: 276 | Train Loss: 0.7512247 Vali Loss: 0.6559553 Test Loss: 0.3313773
Validation loss decreased (inf --> 0.655955).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6030502
	speed: 0.6131s/iter; left time: 8230.3409s
	iters: 200, epoch: 2 | loss: 0.5758356
	speed: 0.1680s/iter; left time: 2238.1740s
Epoch: 2 cost time: 48.622631788253784
Epoch: 2, Steps: 276 | Train Loss: 0.6093064 Vali Loss: 0.6157184 Test Loss: 0.3198104
Validation loss decreased (0.655955 --> 0.615718).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5927467
	speed: 0.6212s/iter; left time: 8167.9592s
	iters: 200, epoch: 3 | loss: 0.5593750
	speed: 0.1822s/iter; left time: 2378.1372s
Epoch: 3 cost time: 48.768693685531616
Epoch: 3, Steps: 276 | Train Loss: 0.5783081 Vali Loss: 0.6048777 Test Loss: 0.3161331
Validation loss decreased (0.615718 --> 0.604878).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5200998
	speed: 0.5884s/iter; left time: 7573.8656s
	iters: 200, epoch: 4 | loss: 0.5870255
	speed: 0.1728s/iter; left time: 2207.1359s
Epoch: 4 cost time: 46.39396548271179
Epoch: 4, Steps: 276 | Train Loss: 0.5684127 Vali Loss: 0.6027822 Test Loss: 0.3143846
Validation loss decreased (0.604878 --> 0.602782).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5610953
	speed: 0.6433s/iter; left time: 8103.6056s
	iters: 200, epoch: 5 | loss: 0.5413169
	speed: 0.1554s/iter; left time: 1941.5073s
Epoch: 5 cost time: 44.51527166366577
Epoch: 5, Steps: 276 | Train Loss: 0.5662528 Vali Loss: 0.6021414 Test Loss: 0.3131700
Validation loss decreased (0.602782 --> 0.602141).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5616108
	speed: 0.6245s/iter; left time: 7693.9011s
	iters: 200, epoch: 6 | loss: 0.5319777
	speed: 0.1839s/iter; left time: 2247.7320s
Epoch: 6 cost time: 52.58631610870361
Epoch: 6, Steps: 276 | Train Loss: 0.5647200 Vali Loss: 0.6003562 Test Loss: 0.3123512
Validation loss decreased (0.602141 --> 0.600356).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5345225
	speed: 0.6202s/iter; left time: 7470.3400s
	iters: 200, epoch: 7 | loss: 0.5754551
	speed: 0.1638s/iter; left time: 1956.7559s
Epoch: 7 cost time: 47.6795380115509
Epoch: 7, Steps: 276 | Train Loss: 0.5639130 Vali Loss: 0.5987786 Test Loss: 0.3119369
Validation loss decreased (0.600356 --> 0.598779).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5343983
	speed: 0.6044s/iter; left time: 7113.0921s
	iters: 200, epoch: 8 | loss: 0.5293984
	speed: 0.1741s/iter; left time: 2031.5675s
Epoch: 8 cost time: 48.0351345539093
Epoch: 8, Steps: 276 | Train Loss: 0.5630300 Vali Loss: 0.5983887 Test Loss: 0.3113191
Validation loss decreased (0.598779 --> 0.598389).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.7265686
	speed: 0.6111s/iter; left time: 7023.6092s
	iters: 200, epoch: 9 | loss: 0.5870219
	speed: 0.1674s/iter; left time: 1907.0841s
Epoch: 9 cost time: 48.2567937374115
Epoch: 9, Steps: 276 | Train Loss: 0.5629038 Vali Loss: 0.5968050 Test Loss: 0.3112722
Validation loss decreased (0.598389 --> 0.596805).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5570611
	speed: 0.6060s/iter; left time: 6798.0089s
	iters: 200, epoch: 10 | loss: 0.5882719
	speed: 0.1693s/iter; left time: 1881.8507s
Epoch: 10 cost time: 47.97631287574768
Epoch: 10, Steps: 276 | Train Loss: 0.5625187 Vali Loss: 0.5976264 Test Loss: 0.3108718
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4693540
	speed: 0.5891s/iter; left time: 6444.9076s
	iters: 200, epoch: 11 | loss: 0.4623473
	speed: 0.1702s/iter; left time: 1844.7729s
Epoch: 11 cost time: 48.68142628669739
Epoch: 11, Steps: 276 | Train Loss: 0.5619967 Vali Loss: 0.5977284 Test Loss: 0.3104731
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5374346
	speed: 0.6668s/iter; left time: 7111.7257s
	iters: 200, epoch: 12 | loss: 0.5895212
	speed: 0.1872s/iter; left time: 1977.8091s
Epoch: 12 cost time: 54.27572679519653
Epoch: 12, Steps: 276 | Train Loss: 0.5615281 Vali Loss: 0.5952678 Test Loss: 0.3103750
Validation loss decreased (0.596805 --> 0.595268).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4683022
	speed: 0.6560s/iter; left time: 6815.2818s
	iters: 200, epoch: 13 | loss: 0.5279383
	speed: 0.1662s/iter; left time: 1709.8446s
Epoch: 13 cost time: 48.81132459640503
Epoch: 13, Steps: 276 | Train Loss: 0.5616414 Vali Loss: 0.5960521 Test Loss: 0.3101510
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5770921
	speed: 0.6148s/iter; left time: 6217.7776s
	iters: 200, epoch: 14 | loss: 0.4934767
	speed: 0.1652s/iter; left time: 1654.3031s
Epoch: 14 cost time: 48.49489450454712
Epoch: 14, Steps: 276 | Train Loss: 0.5614695 Vali Loss: 0.5963761 Test Loss: 0.3099919
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4955771
	speed: 0.6161s/iter; left time: 6060.1790s
	iters: 200, epoch: 15 | loss: 0.5045111
	speed: 0.1743s/iter; left time: 1696.7969s
Epoch: 15 cost time: 49.1779944896698
Epoch: 15, Steps: 276 | Train Loss: 0.5609163 Vali Loss: 0.5959645 Test Loss: 0.3097121
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H12_FITS_custom_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.3093605637550354, mae:0.33163246512413025, rse:0.7319215536117554, corr:[0.47058666 0.47330993 0.47216633 0.47036362 0.46926698 0.46905112
 0.46927288 0.46925995 0.4685799  0.46721816 0.4656153  0.46440092
 0.4637532  0.46357578 0.46350917 0.46314827 0.462387   0.46129072
 0.4601952  0.45934623 0.45885023 0.45858887 0.4583705  0.457863
 0.45697325 0.4557639  0.45448372 0.4533952  0.4526712  0.45223895
 0.45195782 0.4515944  0.4510601  0.4502752  0.44939438 0.44852823
 0.4479101  0.4475063  0.4472293  0.44695657 0.4465982  0.4461807
 0.44569144 0.4451486  0.44460875 0.44408938 0.443632   0.44320917
 0.4426335  0.4419804  0.44130835 0.44061136 0.44002098 0.43958586
 0.43928936 0.43903616 0.43876284 0.4383904  0.43795395 0.43745503
 0.4369802  0.43657097 0.43627387 0.43604034 0.43581718 0.4355723
 0.43526313 0.43490484 0.4345404  0.43419144 0.4339096  0.4335849
 0.4333024  0.4330131  0.43266636 0.43227473 0.43189538 0.43161932
 0.4314253  0.43127868 0.43119282 0.4310784  0.43086565 0.43059334
 0.43031603 0.4300558  0.4298512  0.42960835 0.42941248 0.42920795
 0.42900676 0.4288216  0.42860267 0.42839298 0.4282166  0.42805514
 0.42791218 0.4277708  0.42760673 0.42744148 0.42726177 0.42706203
 0.4268742  0.42668468 0.42648622 0.42627105 0.4260091  0.4257471
 0.42543894 0.42513877 0.42483696 0.42450294 0.4241524  0.4238588
 0.42360952 0.42337447 0.4231719  0.42300034 0.42283252 0.42261547
 0.42239952 0.42219767 0.42197928 0.42175844 0.42152134 0.42128202
 0.42104083 0.4207914  0.4205725  0.42036772 0.42014313 0.41988885
 0.4196032  0.4192754  0.41894686 0.41862574 0.41835433 0.41817024
 0.41795906 0.41779658 0.41766536 0.41755316 0.41736603 0.41709968
 0.41678149 0.4164455  0.4161204  0.41581106 0.41543934 0.41492143
 0.4142934  0.4136462  0.4129898  0.41232452 0.41174945 0.41128376
 0.4109677  0.4106402  0.41029984 0.4099123  0.40949267 0.40907478
 0.40866217 0.4083111  0.40798596 0.40769783 0.4073616  0.40695602
 0.40649414 0.40595734 0.40536052 0.40475604 0.4041957  0.40373194
 0.40330577 0.4028886  0.40244514 0.40197453 0.40148443 0.40098435
 0.40053928 0.40012416 0.39971733 0.39928204 0.3988328  0.39834064
 0.39781684 0.39722258 0.3966221  0.3960476  0.39550537 0.3950063
 0.39455757 0.39405376 0.3935456  0.39302623 0.392494   0.39199492
 0.39151374 0.39108574 0.39074478 0.3903868  0.3899943  0.38954246
 0.38912243 0.38875142 0.38843405 0.3881795  0.38789544 0.3875843
 0.3871756  0.38665915 0.38604805 0.38543388 0.38488182 0.38440925
 0.38399494 0.38372427 0.38345322 0.3830977  0.38267368 0.38225386
 0.3818658  0.38153026 0.38128704 0.3811003  0.38094005 0.3807935
 0.38057223 0.38026634 0.37988532 0.3794414  0.37897262 0.3784947
 0.37803444 0.3776165  0.37717015 0.37669516 0.37614483 0.37560862
 0.3752021  0.37495476 0.3747728  0.37467322 0.37458682 0.3744011
 0.37402743 0.37349337 0.37284127 0.37218618 0.37162307 0.37123656
 0.3709809  0.37074724 0.37054104 0.3701792  0.36974505 0.36928996
 0.36895072 0.36876848 0.3686439  0.36863524 0.36866832 0.36867502
 0.36856914 0.3683037  0.36787227 0.3673676  0.36688983 0.3664409
 0.36608207 0.3658449  0.36564583 0.36534858 0.36498028 0.36453778
 0.36404446 0.3635059  0.3630418  0.3626645  0.36233398 0.36203727
 0.36173576 0.36129263 0.3607452  0.36011404 0.3594474  0.35876647
 0.35814714 0.35758182 0.35701582 0.35638756 0.35578322 0.3551945
 0.35456473 0.3539336  0.35331738 0.35275078 0.3521778  0.35166878
 0.35105804 0.35039264 0.349665   0.34894365 0.34826213 0.34764293
 0.34711614 0.3466579  0.34624678 0.34580764 0.3453842  0.34491447
 0.34442514 0.34397826 0.3435466  0.34310535 0.3426532  0.34214297
 0.34157583 0.340936   0.34026724 0.33964953 0.3391435  0.3386977
 0.3383194  0.33796167 0.3375408  0.3370933  0.33666813 0.33623603
 0.33585784 0.3355228  0.33522755 0.33499038 0.33469927 0.3343419
 0.33387032 0.33332878 0.33280653 0.33234718 0.33199713 0.33179998
 0.3317125  0.33160624 0.33143038 0.3311513  0.3308061  0.33039936
 0.33001593 0.32965985 0.32937998 0.32914802 0.32899088 0.3288266
 0.32861888 0.32836765 0.3280993  0.32779124 0.3275017  0.32725006
 0.32701308 0.32678193 0.32654926 0.32623672 0.32585037 0.32541248
 0.32494646 0.32453144 0.32423127 0.32400286 0.32385477 0.3237475
 0.32365105 0.32350543 0.32331827 0.32310772 0.32287773 0.32262868
 0.32236928 0.32211593 0.3218133  0.3214489  0.32104293 0.3205368
 0.32000342 0.31950495 0.31912526 0.31886643 0.3187308  0.31867975
 0.31861246 0.3185569  0.31844684 0.31828976 0.31809616 0.31790164
 0.31773648 0.31761912 0.31754845 0.31749266 0.317444   0.3173569
 0.31726602 0.31711692 0.3169553  0.31676632 0.31656474 0.31640136
 0.3162889  0.31618074 0.31605834 0.31584328 0.31559956 0.31537086
 0.31516987 0.31499135 0.31486326 0.31478512 0.31465617 0.3144696
 0.3141784  0.31383744 0.31341013 0.312983   0.31253445 0.31209755
 0.3116862  0.31135482 0.31102103 0.3106233  0.3101775  0.30975497
 0.3093425  0.30895892 0.30863562 0.30832958 0.30795965 0.30747685
 0.30686116 0.30614907 0.30541098 0.30464804 0.3038574  0.30293435
 0.30197048 0.30101284 0.3001412  0.29952586 0.29907763 0.29879498
 0.29863173 0.2984555  0.29819918 0.2977913  0.29723766 0.29649937
 0.29568294 0.29487684 0.29416874 0.29360452 0.29318836 0.29293862
 0.29274157 0.2925176  0.2922225  0.29182738 0.2913622  0.29087082
 0.29045    0.2901263  0.28986752 0.2896558  0.2894845  0.289283
 0.2890209  0.2887059  0.28835365 0.28802678 0.28777665 0.28758577
 0.28748003 0.28739157 0.28724295 0.28701502 0.28673667 0.28648582
 0.28624758 0.2860791  0.2860014  0.28596726 0.28591868 0.28577456
 0.28553793 0.2852319  0.28486556 0.28449345 0.2841625  0.28395668
 0.28388315 0.28390762 0.28392655 0.2838425  0.28365478 0.28335953
 0.28296706 0.2825585  0.28224447 0.28197438 0.28182468 0.28172806
 0.2816681  0.2815658  0.28137672 0.28104678 0.28061557 0.28016996
 0.27979013 0.2795323  0.27937937 0.27932057 0.27928427 0.27917454
 0.27897617 0.27866903 0.278259   0.2778617  0.27750108 0.27724573
 0.2770691  0.27693623 0.27680144 0.2766055  0.27635226 0.27598336
 0.2755458  0.27507868 0.27461344 0.2742074  0.2738672  0.27356938
 0.2733015  0.27307096 0.27283686 0.27262062 0.27242503 0.2722063
 0.2719596  0.27168974 0.27138904 0.27102172 0.27062368 0.27019835
 0.26981232 0.26951748 0.26934305 0.26920044 0.26906395 0.26881075
 0.26842144 0.26785114 0.26717463 0.26651424 0.26594028 0.2655354
 0.2653318  0.26527882 0.26525065 0.26511014 0.2647634  0.26421005
 0.26347873 0.26267847 0.26194414 0.26134992 0.2609119  0.26053604
 0.26039568 0.26000857 0.25936866 0.2588539  0.2581765  0.25745335
 0.25677413 0.25621998 0.25579756 0.25552034 0.25527892 0.2550396
 0.2547147  0.25426257 0.25367334 0.253025   0.2523738  0.25179923
 0.25132552 0.25094238 0.25062954 0.2503446  0.25004494 0.24969505
 0.24929443 0.24881658 0.248301   0.24777237 0.24727188 0.24685735
 0.24647367 0.24617794 0.24587417 0.24552606 0.2451503  0.24473426
 0.24430293 0.24391988 0.2435445  0.24321829 0.24293976 0.24260981
 0.2422775  0.24198651 0.24170429 0.24144064 0.24121456 0.24104755
 0.24084981 0.24062188 0.24033315 0.2399806  0.23965448 0.23932621
 0.23907024 0.23888461 0.23880629 0.23877628 0.2387507  0.23871459
 0.23865859 0.23857921 0.23847996 0.23837087 0.23832166 0.23831138
 0.23833776 0.23832092 0.23818663 0.23787972 0.2375145  0.2372002
 0.23707719 0.23713483 0.23743057 0.2377807  0.23813772 0.23831333
 0.23827201 0.23798583 0.23753665 0.23705254 0.23666915 0.23647043
 0.2364652  0.23655593 0.2366285  0.23659708 0.2364716  0.23627612
 0.23605531 0.23585889 0.23565698 0.23545463 0.23521703 0.23487052
 0.23448321 0.23410779 0.23376295 0.23352268 0.23339136 0.23334418
 0.23326363 0.2330653  0.23272318 0.2322183  0.23168288 0.23119467
 0.23085104 0.23069885 0.2307338  0.23081258 0.23080651 0.23062693
 0.23027608 0.22979859 0.22939382 0.22916025 0.22904015 0.22887386
 0.2285445  0.22795704 0.2271655  0.22634056 0.22572975 0.22544931
 0.2255479  0.22582406 0.22594392 0.22567296 0.22496444 0.22403526
 0.2232171  0.22284788 0.22307609 0.22372836 0.22421987 0.2241912
 0.22350015 0.2228264  0.22287948 0.22426957 0.22636873 0.22663955]
