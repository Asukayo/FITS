Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j336_H12', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j336_H12_FITS_custom_ftM_sl720_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35832
val 4935
test 10204
Model(
  (freq_upsampler): Linear(in_features=82, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  105799680.0
params:  9960.0
Trainable parameters:  9960
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 13.408631801605225
Epoch: 1, Steps: 69 | Train Loss: 0.7687268 Vali Loss: 0.6535590 Test Loss: 0.3066173
Validation loss decreased (inf --> 0.653559).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 12.759760618209839
Epoch: 2, Steps: 69 | Train Loss: 0.6177176 Vali Loss: 0.5933548 Test Loss: 0.2850953
Validation loss decreased (0.653559 --> 0.593355).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 13.646733045578003
Epoch: 3, Steps: 69 | Train Loss: 0.5780225 Vali Loss: 0.5680830 Test Loss: 0.2771645
Validation loss decreased (0.593355 --> 0.568083).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.651069641113281
Epoch: 4, Steps: 69 | Train Loss: 0.5586505 Vali Loss: 0.5530849 Test Loss: 0.2725814
Validation loss decreased (0.568083 --> 0.553085).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.213653326034546
Epoch: 5, Steps: 69 | Train Loss: 0.5484300 Vali Loss: 0.5495369 Test Loss: 0.2696031
Validation loss decreased (0.553085 --> 0.549537).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.27084469795227
Epoch: 6, Steps: 69 | Train Loss: 0.5422908 Vali Loss: 0.5487041 Test Loss: 0.2675717
Validation loss decreased (0.549537 --> 0.548704).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 11.889206647872925
Epoch: 7, Steps: 69 | Train Loss: 0.5397900 Vali Loss: 0.5429479 Test Loss: 0.2661616
Validation loss decreased (0.548704 --> 0.542948).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.45227313041687
Epoch: 8, Steps: 69 | Train Loss: 0.5365013 Vali Loss: 0.5435668 Test Loss: 0.2650763
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 12.64851450920105
Epoch: 9, Steps: 69 | Train Loss: 0.5363311 Vali Loss: 0.5410579 Test Loss: 0.2643316
Validation loss decreased (0.542948 --> 0.541058).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 12.697464942932129
Epoch: 10, Steps: 69 | Train Loss: 0.5351053 Vali Loss: 0.5431983 Test Loss: 0.2636243
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.548037767410278
Epoch: 11, Steps: 69 | Train Loss: 0.5337173 Vali Loss: 0.5414686 Test Loss: 0.2630831
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 13.062645435333252
Epoch: 12, Steps: 69 | Train Loss: 0.5342314 Vali Loss: 0.5429650 Test Loss: 0.2625714
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j336_H12_FITS_custom_ftM_sl720_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10204
mse:0.2637663185596466, mae:0.3000277578830719, rse:0.6745205521583557, corr:[0.4724112  0.4812586  0.48514262 0.48462477 0.4817614  0.47947398
 0.4791381  0.48024648 0.48155582 0.48212543 0.48172015 0.480603
 0.4792591  0.4782835  0.47797397 0.4780995  0.47816816 0.47770908
 0.4767714  0.47557154 0.47445187 0.4736691  0.47332817 0.47323123
 0.47304377 0.47244826 0.4713777  0.4699755  0.46863082 0.46755415
 0.466883   0.46653056 0.4663249  0.46594152 0.4652757  0.46435648
 0.4634201  0.46253386 0.4617937  0.46122783 0.46082887 0.46050018
 0.4600325  0.4593873  0.45866525 0.45796198 0.4573787  0.45692268
 0.45647776 0.45606986 0.45564517 0.4551025  0.4545524  0.4540128
 0.4535266  0.4530929  0.45274326 0.45240885 0.4520681  0.45166832
 0.45121023 0.45073307 0.4503249  0.45000654 0.4497669  0.44957387
 0.44929913 0.4489699  0.4485648  0.4481282  0.44772652 0.44732368
 0.44699728 0.44662762 0.44611457 0.4455642  0.44506773 0.444682
 0.44437045 0.44412953 0.44404593 0.444045   0.4439548  0.44371694
 0.44334024 0.44286665 0.4424427  0.44203126 0.4417376  0.4415033
 0.441362   0.44126812 0.44114253 0.44098002 0.44072494 0.44039774
 0.440073   0.43980548 0.43957877 0.4393688  0.43914372 0.43888992
 0.43861124 0.4382673  0.4378766  0.43747574 0.4370484  0.4366683
 0.4363139  0.43599078 0.43566313 0.43529165 0.43485096 0.4344517
 0.4340863  0.433731   0.43340686 0.43308994 0.43272626 0.4322698
 0.43180814 0.43137264 0.43100044 0.43069404 0.4304326  0.43016398
 0.42980865 0.42933342 0.42875713 0.42811748 0.42747375 0.42694068
 0.42657968 0.42635837 0.42621765 0.42602158 0.42568308 0.4252171
 0.42457247 0.42393988 0.4234317  0.42311758 0.42293403 0.42276114
 0.42248526 0.42201996 0.4213481  0.42053658 0.4196673  0.41887236
 0.418291   0.41789544 0.4174932  0.4168741  0.41595235 0.41478178
 0.41367793 0.41267574 0.4120208  0.4116834  0.41164446 0.41157702
 0.4113236  0.41080955 0.4100346  0.409183   0.40837976 0.407843
 0.40760195 0.40743628 0.40715662 0.4066319  0.40582988 0.40491873
 0.40395027 0.40321177 0.40279087 0.40269026 0.40266135 0.4025111
 0.40217772 0.40153757 0.40070766 0.39988235 0.39927986 0.39901707
 0.39903155 0.399086   0.3990791  0.39881796 0.3982896  0.39756885
 0.39694867 0.39640316 0.3961321  0.39604962 0.3959915  0.39582378
 0.3954897  0.3950355  0.3945698  0.39422476 0.3940253  0.39390707
 0.39383087 0.39368337 0.39334083 0.39276642 0.39199725 0.3912934
 0.39073643 0.39042395 0.39027515 0.3901851  0.38994208 0.38940105
 0.38864738 0.38789263 0.38726583 0.38682    0.3866731  0.3867884
 0.3868899  0.38679278 0.3864007  0.38571468 0.38499323 0.38453448
 0.38445607 0.38470793 0.38505283 0.3852574  0.38506946 0.38442585
 0.3834782  0.38252932 0.3818218  0.38161108 0.3817179  0.38207385
 0.3824041  0.38247627 0.38215718 0.3815655  0.3809832  0.38062453
 0.3805767  0.38090843 0.38125733 0.38151795 0.38141456 0.3809833
 0.38027436 0.3795373  0.37909025 0.3789765  0.379154   0.37934458
 0.37941796 0.37919733 0.37852377 0.37769902 0.37701404 0.37665302
 0.3767681  0.37710652 0.3774318  0.37747222 0.37707266 0.37627974
 0.37529686 0.37446967 0.37414882 0.37423941 0.37459055 0.37486765
 0.3747603  0.37412852 0.3731424  0.37204868 0.37120125 0.37085357
 0.37099475 0.3712438  0.37126642 0.3707855  0.3697511  0.36833277
 0.36690465 0.36589506 0.36545002 0.36541304 0.36549854 0.36529505
 0.36448732 0.3631588  0.36160365 0.3603083  0.3596023  0.35965976
 0.35992926 0.359919   0.35922983 0.35775995 0.35578635 0.3538184
 0.35251647 0.35209525 0.35264608 0.35348666 0.353773   0.35312843
 0.35172778 0.3499782  0.34860903 0.3482673  0.34900337 0.3503575
 0.35151997 0.35149208 0.35004497 0.3476829  0.345251   0.3437259
 0.3437376  0.34517822 0.34671095 0.3471233  0.3457594  0.342853
 0.33988246 0.33893374 0.34171194 0.34744346 0.35196978 0.35137436]
