Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=70, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=140, bias=True)
    (1): Linear(in_features=70, out_features=140, bias=True)
    (2): Linear(in_features=70, out_features=140, bias=True)
    (3): Linear(in_features=70, out_features=140, bias=True)
    (4): Linear(in_features=70, out_features=140, bias=True)
    (5): Linear(in_features=70, out_features=140, bias=True)
    (6): Linear(in_features=70, out_features=140, bias=True)
    (7): Linear(in_features=70, out_features=140, bias=True)
    (8): Linear(in_features=70, out_features=140, bias=True)
    (9): Linear(in_features=70, out_features=140, bias=True)
    (10): Linear(in_features=70, out_features=140, bias=True)
    (11): Linear(in_features=70, out_features=140, bias=True)
    (12): Linear(in_features=70, out_features=140, bias=True)
    (13): Linear(in_features=70, out_features=140, bias=True)
    (14): Linear(in_features=70, out_features=140, bias=True)
    (15): Linear(in_features=70, out_features=140, bias=True)
    (16): Linear(in_features=70, out_features=140, bias=True)
    (17): Linear(in_features=70, out_features=140, bias=True)
    (18): Linear(in_features=70, out_features=140, bias=True)
    (19): Linear(in_features=70, out_features=140, bias=True)
    (20): Linear(in_features=70, out_features=140, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26342400.0
params:  208740.0
Trainable parameters:  208740
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7079589
	speed: 0.1582s/iter; left time: 2167.9011s
	iters: 200, epoch: 1 | loss: 0.6340499
	speed: 0.1501s/iter; left time: 2041.6492s
Epoch: 1 cost time: 39.7819709777832
Epoch: 1, Steps: 276 | Train Loss: 0.7531091 Vali Loss: 0.6534576 Test Loss: 0.3319059
Validation loss decreased (inf --> 0.653458).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6606993
	speed: 0.4591s/iter; left time: 6162.8605s
	iters: 200, epoch: 2 | loss: 0.5722428
	speed: 0.1718s/iter; left time: 2288.8418s
Epoch: 2 cost time: 48.54309105873108
Epoch: 2, Steps: 276 | Train Loss: 0.6092091 Vali Loss: 0.6141443 Test Loss: 0.3205363
Validation loss decreased (0.653458 --> 0.614144).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5423073
	speed: 0.6570s/iter; left time: 8639.3442s
	iters: 200, epoch: 3 | loss: 0.6040471
	speed: 0.1802s/iter; left time: 2351.2232s
Epoch: 3 cost time: 50.65049433708191
Epoch: 3, Steps: 276 | Train Loss: 0.5786670 Vali Loss: 0.6051106 Test Loss: 0.3164122
Validation loss decreased (0.614144 --> 0.605111).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5459750
	speed: 0.5733s/iter; left time: 7380.6306s
	iters: 200, epoch: 4 | loss: 0.5512244
	speed: 0.1608s/iter; left time: 2054.0100s
Epoch: 4 cost time: 45.50936245918274
Epoch: 4, Steps: 276 | Train Loss: 0.5694335 Vali Loss: 0.6044235 Test Loss: 0.3147556
Validation loss decreased (0.605111 --> 0.604423).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6016232
	speed: 0.5817s/iter; left time: 7327.9102s
	iters: 200, epoch: 5 | loss: 0.6142815
	speed: 0.1614s/iter; left time: 2016.7269s
Epoch: 5 cost time: 46.19709873199463
Epoch: 5, Steps: 276 | Train Loss: 0.5662450 Vali Loss: 0.6019696 Test Loss: 0.3135959
Validation loss decreased (0.604423 --> 0.601970).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4723299
	speed: 0.5608s/iter; left time: 6909.6569s
	iters: 200, epoch: 6 | loss: 0.5317701
	speed: 0.1617s/iter; left time: 1975.9741s
Epoch: 6 cost time: 45.046236515045166
Epoch: 6, Steps: 276 | Train Loss: 0.5647724 Vali Loss: 0.6024741 Test Loss: 0.3126274
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6666675
	speed: 0.5711s/iter; left time: 6879.1376s
	iters: 200, epoch: 7 | loss: 0.6028043
	speed: 0.1763s/iter; left time: 2106.1803s
Epoch: 7 cost time: 48.440190076828
Epoch: 7, Steps: 276 | Train Loss: 0.5641284 Vali Loss: 0.6000799 Test Loss: 0.3121287
Validation loss decreased (0.601970 --> 0.600080).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5739686
	speed: 0.6202s/iter; left time: 7299.2349s
	iters: 200, epoch: 8 | loss: 0.4595877
	speed: 0.1642s/iter; left time: 1915.8353s
Epoch: 8 cost time: 47.22469663619995
Epoch: 8, Steps: 276 | Train Loss: 0.5637147 Vali Loss: 0.5992785 Test Loss: 0.3117628
Validation loss decreased (0.600080 --> 0.599278).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5212855
	speed: 0.5522s/iter; left time: 6346.7465s
	iters: 200, epoch: 9 | loss: 0.6373195
	speed: 0.1485s/iter; left time: 1691.6080s
Epoch: 9 cost time: 39.77944350242615
Epoch: 9, Steps: 276 | Train Loss: 0.5628992 Vali Loss: 0.5969977 Test Loss: 0.3112043
Validation loss decreased (0.599278 --> 0.596998).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5384461
	speed: 0.4608s/iter; left time: 5169.3399s
	iters: 200, epoch: 10 | loss: 0.4962211
	speed: 0.1222s/iter; left time: 1358.5389s
Epoch: 10 cost time: 37.65693378448486
Epoch: 10, Steps: 276 | Train Loss: 0.5622455 Vali Loss: 0.5976775 Test Loss: 0.3108339
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5575920
	speed: 0.5401s/iter; left time: 5908.9172s
	iters: 200, epoch: 11 | loss: 0.5309430
	speed: 0.1526s/iter; left time: 1654.4832s
Epoch: 11 cost time: 44.28836464881897
Epoch: 11, Steps: 276 | Train Loss: 0.5621296 Vali Loss: 0.5966472 Test Loss: 0.3105230
Validation loss decreased (0.596998 --> 0.596647).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5834615
	speed: 0.5678s/iter; left time: 6056.0230s
	iters: 200, epoch: 12 | loss: 0.5984353
	speed: 0.1634s/iter; left time: 1726.0946s
Epoch: 12 cost time: 45.65087628364563
Epoch: 12, Steps: 276 | Train Loss: 0.5619731 Vali Loss: 0.5965460 Test Loss: 0.3105243
Validation loss decreased (0.596647 --> 0.596546).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6133326
	speed: 0.5840s/iter; left time: 6067.1193s
	iters: 200, epoch: 13 | loss: 0.5241774
	speed: 0.1664s/iter; left time: 1712.5028s
Epoch: 13 cost time: 47.025787353515625
Epoch: 13, Steps: 276 | Train Loss: 0.5617556 Vali Loss: 0.5964620 Test Loss: 0.3103642
Validation loss decreased (0.596546 --> 0.596462).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5306484
	speed: 0.5616s/iter; left time: 5679.6243s
	iters: 200, epoch: 14 | loss: 0.5868743
	speed: 0.1542s/iter; left time: 1544.2244s
Epoch: 14 cost time: 43.780694246292114
Epoch: 14, Steps: 276 | Train Loss: 0.5615318 Vali Loss: 0.5945645 Test Loss: 0.3099526
Validation loss decreased (0.596462 --> 0.594564).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5500863
	speed: 0.5471s/iter; left time: 5381.4310s
	iters: 200, epoch: 15 | loss: 0.5517202
	speed: 0.1571s/iter; left time: 1529.9705s
Epoch: 15 cost time: 44.36367654800415
Epoch: 15, Steps: 276 | Train Loss: 0.5613716 Vali Loss: 0.5953371 Test Loss: 0.3098436
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5578504
	speed: 0.5533s/iter; left time: 5290.1997s
	iters: 200, epoch: 16 | loss: 0.5867643
	speed: 0.1551s/iter; left time: 1467.4852s
Epoch: 16 cost time: 44.075565338134766
Epoch: 16, Steps: 276 | Train Loss: 0.5611951 Vali Loss: 0.5951468 Test Loss: 0.3097099
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5573943
	speed: 0.5575s/iter; left time: 5176.4246s
	iters: 200, epoch: 17 | loss: 0.5966913
	speed: 0.1670s/iter; left time: 1533.4462s
Epoch: 17 cost time: 45.856895208358765
Epoch: 17, Steps: 276 | Train Loss: 0.5610104 Vali Loss: 0.5943536 Test Loss: 0.3096010
Validation loss decreased (0.594564 --> 0.594354).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5306532
	speed: 0.5867s/iter; left time: 5285.3441s
	iters: 200, epoch: 18 | loss: 0.4875954
	speed: 0.1547s/iter; left time: 1378.0792s
Epoch: 18 cost time: 43.0338978767395
Epoch: 18, Steps: 276 | Train Loss: 0.5610571 Vali Loss: 0.5954074 Test Loss: 0.3095332
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4899536
	speed: 0.5280s/iter; left time: 4611.0135s
	iters: 200, epoch: 19 | loss: 0.5295733
	speed: 0.1601s/iter; left time: 1382.3440s
Epoch: 19 cost time: 44.981295108795166
Epoch: 19, Steps: 276 | Train Loss: 0.5609384 Vali Loss: 0.5945553 Test Loss: 0.3094527
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5473338
	speed: 0.5469s/iter; left time: 4625.2569s
	iters: 200, epoch: 20 | loss: 0.5825989
	speed: 0.1528s/iter; left time: 1276.8465s
Epoch: 20 cost time: 43.359848499298096
Epoch: 20, Steps: 276 | Train Loss: 0.5609203 Vali Loss: 0.5949306 Test Loss: 0.3095978
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.30857768654823303, mae:0.3308766782283783, rse:0.7309948205947876, corr:[0.47271618 0.47440538 0.47374502 0.4724906  0.47135743 0.47060552
 0.47020018 0.46989977 0.46943277 0.46858987 0.4674058  0.4661825
 0.46512786 0.4644405  0.4641163  0.4639226  0.46367648 0.46313652
 0.46225986 0.46105784 0.45970374 0.4584012  0.45738116 0.456606
 0.4560395  0.45553213 0.4549461  0.45418507 0.45329112 0.45233077
 0.4514788  0.45082745 0.45046428 0.45027342 0.45016903 0.44994438
 0.44958317 0.4489677  0.4481139  0.44712695 0.4461759  0.4454643
 0.44502413 0.4447624  0.44455734 0.44427857 0.44389322 0.44338924
 0.44263327 0.4417814  0.44096062 0.44018045 0.4395488  0.43906283
 0.43868446 0.4383253  0.43795165 0.43750468 0.4370362  0.43654296
 0.43609306 0.43570566 0.43541965 0.4351979  0.43500277 0.43481207
 0.43456927 0.43428025 0.43396658 0.4336402  0.4333526  0.43300566
 0.4327056  0.43242735 0.4321281  0.43181387 0.43151736 0.43129513
 0.43112686 0.43097904 0.4308928  0.4308118  0.43067345 0.43050295
 0.43033305 0.4301518  0.42996696 0.42968214 0.42940468 0.42912266
 0.42889434 0.42874503 0.4286202  0.42853463 0.42847416 0.4283904
 0.42826298 0.42808592 0.42786163 0.42764875 0.4274602  0.42730415
 0.42718798 0.42707893 0.426929   0.42670783 0.42639196 0.42603773
 0.4256268  0.4252381  0.42488348 0.4245352  0.42420408 0.42394423
 0.42372176 0.4234919  0.42327738 0.42308295 0.42288458 0.42263174
 0.42238924 0.4221761  0.42196932 0.42178762 0.4216004  0.42140257
 0.4211799  0.42090774 0.4206249  0.42033646 0.4200328  0.41973564
 0.41947073 0.41922697 0.41902012 0.41880664 0.41858605 0.418374
 0.41807723 0.41780895 0.41760144 0.41747126 0.41734368 0.4171782
 0.41696194 0.41668853 0.41639292 0.4161033  0.41576704 0.41535097
 0.4148896  0.41443074 0.41394827 0.413409   0.4128771  0.41232347
 0.4118356  0.41127178 0.4107022  0.4101651  0.40972    0.40937817
 0.40910175 0.40888035 0.40862918 0.40833414 0.4079146  0.40738395
 0.40679485 0.406174   0.40555355 0.40498808 0.40450305 0.40411478
 0.40374592 0.40336576 0.40292278 0.40241858 0.40184787 0.40120283
 0.40055794 0.3999112  0.39927566 0.39867637 0.39817047 0.3977352
 0.39735714 0.3969633  0.39655948 0.39614475 0.39570698 0.3952644
 0.39486414 0.39442047 0.39399686 0.39357135 0.39312854 0.39268807
 0.39222938 0.3917816  0.39139408 0.39101845 0.3906385  0.39021635
 0.38980523 0.38938242 0.3889374  0.38847968 0.38795197 0.38742277
 0.3868843  0.3863706  0.38589203 0.38547575 0.38510078 0.38472888
 0.3843148  0.38395697 0.3835691  0.38312897 0.38268238 0.38230908
 0.38199568 0.38171658 0.38146654 0.3812116  0.38094765 0.3807041
 0.38043866 0.38016173 0.37985316 0.3794765  0.37902424 0.3784967
 0.37795275 0.3774562  0.37698394 0.37656975 0.37617964 0.3758518
 0.37563163 0.37548223 0.3752864  0.37506855 0.37482268 0.3745069
 0.37410015 0.37366492 0.3732126  0.37278488 0.37240314 0.37209237
 0.37180695 0.37149286 0.3712048  0.3708209  0.37039256 0.36993822
 0.3695322  0.36916664 0.3687623  0.36839983 0.36804602 0.3677687
 0.36752644 0.36730242 0.3670627  0.3668398  0.36667338 0.3664821
 0.3663061  0.36620563 0.36609477 0.36591142 0.36569652 0.36541727
 0.3650396  0.3644997  0.36388722 0.36323535 0.36258456 0.36203614
 0.36164245 0.36130837 0.36103365 0.3607415  0.36036015 0.35981676
 0.35913426 0.35833225 0.35743767 0.3564895  0.35565883 0.3550109
 0.35449538 0.35410404 0.3537703  0.3534362  0.35299423 0.35249472
 0.35182017 0.35107112 0.35029736 0.349592   0.34898418 0.34847212
 0.34804884 0.34766027 0.34725943 0.3467568  0.346192   0.3455248
 0.3448009  0.34412903 0.34352437 0.34300643 0.34260327 0.34225592
 0.3419307  0.34154594 0.34107497 0.34054217 0.33999437 0.33940348
 0.3388334  0.3383164  0.3378071  0.33734402 0.3369343  0.33649406
 0.336031   0.33552304 0.3349986  0.33454245 0.3341326  0.33381286
 0.33354586 0.3333167  0.33312446 0.33289886 0.33261156 0.33231628
 0.3320239  0.33169642 0.3313731  0.3310765  0.33083466 0.3306091
 0.33040947 0.33017614 0.32990986 0.32957098 0.32920212 0.32880524
 0.32840973 0.3280613  0.32779548 0.32756254 0.3273485  0.3271646
 0.32693094 0.32665464 0.32638007 0.32604107 0.32567635 0.32531577
 0.32496732 0.32467243 0.32445782 0.32427013 0.32411674 0.32398516
 0.32387024 0.3237368  0.3235921  0.32344425 0.32327217 0.32304344
 0.32274422 0.32239607 0.32197088 0.3215026  0.3210544  0.32059154
 0.32017463 0.3198281  0.31959343 0.3194353  0.3193469  0.31928915
 0.3191779  0.31906295 0.31890085 0.31871414 0.31852287 0.31835437
 0.3182218  0.31812716 0.31804043 0.31793472 0.3178059  0.31760788
 0.31740412 0.3171433  0.31688234 0.31661752 0.31635892 0.3161462
 0.31600264 0.31590158 0.31582525 0.315707   0.31558937 0.31548348
 0.31537807 0.31525657 0.31514487 0.31506047 0.31491932 0.31472778
 0.3144246  0.3140649  0.31359196 0.31308383 0.3125284  0.31197348
 0.3114606  0.31105945 0.31071162 0.31035054 0.30995238 0.30954462
 0.30906588 0.30852243 0.3079724  0.30743948 0.30693713 0.30647817
 0.30604103 0.3056185  0.3052086  0.30474833 0.30421907 0.30350345
 0.30271962 0.301916   0.30113155 0.30046475 0.29988337 0.2994004
 0.29899257 0.29856765 0.29810944 0.29759598 0.29707533 0.29653096
 0.29602757 0.29560626 0.2952741  0.29499596 0.2947116  0.29441613
 0.29402786 0.2935318  0.29295874 0.2923395  0.29173273 0.29118246
 0.29076222 0.29048422 0.29030177 0.29018638 0.29012528 0.29004273
 0.2899036  0.2896985  0.28942612 0.28913203 0.28886148 0.2886108
 0.28844288 0.28832904 0.28821275 0.2880639  0.28786057 0.28760645
 0.2872446  0.2868138  0.2863684  0.28593984 0.285574   0.2852685
 0.2850661  0.28497314 0.2849314  0.28490347 0.2848375  0.28474528
 0.28462657 0.2844944  0.28433445 0.2841359  0.28395423 0.28378955
 0.28362194 0.28346297 0.28334266 0.28317586 0.28302458 0.28285873
 0.2827255  0.28261104 0.28251237 0.28237075 0.28218433 0.28196144
 0.2816987  0.28140944 0.28109425 0.2807909  0.28051028 0.2802206
 0.27995157 0.27968374 0.2793827  0.2790893  0.27876565 0.27844527
 0.2781119  0.27778363 0.27746797 0.2771774  0.27694735 0.27672225
 0.27650407 0.2762764  0.27600557 0.27569833 0.27535146 0.27495518
 0.27452725 0.27412146 0.27370843 0.2733263  0.27298272 0.27264348
 0.27231875 0.27202532 0.27176902 0.27150902 0.27125317 0.27094293
 0.270593   0.27021244 0.2698303  0.26942033 0.2690365  0.26865396
 0.2683042  0.26795742 0.2676258  0.26732197 0.26698616 0.2666097
 0.2662151  0.26581907 0.26541948 0.2650247  0.2646404  0.26430032
 0.26398003 0.2636659  0.26333728 0.2629329  0.26240993 0.26171172
 0.2611003  0.2602696  0.25931996 0.25870296 0.2581312  0.25764394
 0.25721377 0.25682795 0.25644493 0.2560918  0.25571328 0.25535712
 0.25500584 0.25465205 0.25427523 0.25389957 0.25350258 0.25310147
 0.25267038 0.25221005 0.25174743 0.25130168 0.25089142 0.2505168
 0.2501868  0.24984689 0.2494862  0.24907698 0.24862832 0.24819739
 0.24774429 0.24736555 0.24701542 0.2466957  0.24643947 0.24620727
 0.24596912 0.24572165 0.24539228 0.24497387 0.24449466 0.24389698
 0.24328065 0.24274223 0.24228153 0.24192703 0.24169663 0.24159926
 0.24151804 0.24142416 0.24125491 0.24097075 0.2406352  0.24019596
 0.23972942 0.23926322 0.23888749 0.23860925 0.2384472  0.23840788
 0.23846467 0.23856126 0.23862928 0.23860878 0.23852523 0.23837325
 0.23821041 0.23806095 0.23795128 0.23784988 0.23782144 0.23784274
 0.23792486 0.23797004 0.23802249 0.23798685 0.23795342 0.23788469
 0.23784341 0.23780666 0.23776723 0.237703   0.2375987  0.23744623
 0.2372577  0.23701057 0.23672485 0.23642944 0.23620111 0.23607813
 0.2360519  0.23609519 0.23612401 0.23610881 0.23600994 0.2357526
 0.23539056 0.2349484  0.23442991 0.23390487 0.2334103  0.23300491
 0.23268108 0.23243009 0.23223443 0.23199981 0.23171681 0.231327
 0.23083629 0.23030204 0.22983377 0.22946878 0.22923759 0.22913137
 0.22909479 0.22901484 0.22889522 0.22870754 0.22842367 0.22804846
 0.22766018 0.22728643 0.22694509 0.22662407 0.22630708 0.2259168
 0.22548538 0.22503547 0.22459693 0.22426103 0.22410926 0.22419249
 0.22442287 0.22465143 0.22470848 0.22447117 0.22378641 0.22285067
 0.22194403 0.22171383 0.2223596  0.22385629 0.22544906 0.22564442]
