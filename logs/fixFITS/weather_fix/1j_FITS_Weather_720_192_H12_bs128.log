Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j192_H12', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j192_H12_FITS_custom_ftM_sl720_ll48_pl192_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35976
val 5079
test 10348
Model(
  (freq_upsampler): Linear(in_features=82, out_features=103, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  45405696.0
params:  8549.0
Trainable parameters:  8549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4877949
	speed: 0.1912s/iter; left time: 1319.4364s
Epoch: 1 cost time: 26.165059328079224
Epoch: 1, Steps: 140 | Train Loss: 0.6410545 Vali Loss: 0.5327651 Test Loss: 0.2390243
Validation loss decreased (inf --> 0.532765).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4295829
	speed: 0.4461s/iter; left time: 3016.2046s
Epoch: 2 cost time: 25.72844934463501
Epoch: 2, Steps: 140 | Train Loss: 0.5164469 Vali Loss: 0.5001328 Test Loss: 0.2249281
Validation loss decreased (0.532765 --> 0.500133).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5345051
	speed: 0.4377s/iter; left time: 2897.7859s
Epoch: 3 cost time: 25.22705602645874
Epoch: 3, Steps: 140 | Train Loss: 0.4971115 Vali Loss: 0.4894264 Test Loss: 0.2203113
Validation loss decreased (0.500133 --> 0.489426).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6250379
	speed: 0.4177s/iter; left time: 2707.1819s
Epoch: 4 cost time: 24.479437828063965
Epoch: 4, Steps: 140 | Train Loss: 0.4914656 Vali Loss: 0.4861709 Test Loss: 0.2175411
Validation loss decreased (0.489426 --> 0.486171).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4634469
	speed: 0.3847s/iter; left time: 2439.2984s
Epoch: 5 cost time: 22.554777145385742
Epoch: 5, Steps: 140 | Train Loss: 0.4888710 Vali Loss: 0.4852469 Test Loss: 0.2150216
Validation loss decreased (0.486171 --> 0.485247).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4140190
	speed: 0.4275s/iter; left time: 2650.8683s
Epoch: 6 cost time: 24.77346158027649
Epoch: 6, Steps: 140 | Train Loss: 0.4874819 Vali Loss: 0.4779672 Test Loss: 0.2140362
Validation loss decreased (0.485247 --> 0.477967).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4201595
	speed: 0.4474s/iter; left time: 2711.4016s
Epoch: 7 cost time: 26.190059185028076
Epoch: 7, Steps: 140 | Train Loss: 0.4870309 Vali Loss: 0.4802500 Test Loss: 0.2132481
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4306048
	speed: 0.4483s/iter; left time: 2654.6025s
Epoch: 8 cost time: 26.97281289100647
Epoch: 8, Steps: 140 | Train Loss: 0.4869199 Vali Loss: 0.4783675 Test Loss: 0.2129455
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5242230
	speed: 0.4562s/iter; left time: 2637.5248s
Epoch: 9 cost time: 26.14576292037964
Epoch: 9, Steps: 140 | Train Loss: 0.4869678 Vali Loss: 0.4778095 Test Loss: 0.2123724
Validation loss decreased (0.477967 --> 0.477809).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4895965
	speed: 0.4468s/iter; left time: 2520.1981s
Epoch: 10 cost time: 25.979228734970093
Epoch: 10, Steps: 140 | Train Loss: 0.4858694 Vali Loss: 0.4758954 Test Loss: 0.2121066
Validation loss decreased (0.477809 --> 0.475895).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5616388
	speed: 0.4291s/iter; left time: 2360.3703s
Epoch: 11 cost time: 22.93361806869507
Epoch: 11, Steps: 140 | Train Loss: 0.4862413 Vali Loss: 0.4776953 Test Loss: 0.2121503
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4865582
	speed: 0.3537s/iter; left time: 1896.3519s
Epoch: 12 cost time: 23.301581144332886
Epoch: 12, Steps: 140 | Train Loss: 0.4855619 Vali Loss: 0.4742004 Test Loss: 0.2117844
Validation loss decreased (0.475895 --> 0.474200).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4620318
	speed: 0.3365s/iter; left time: 1756.8631s
Epoch: 13 cost time: 19.837308883666992
Epoch: 13, Steps: 140 | Train Loss: 0.4855398 Vali Loss: 0.4762655 Test Loss: 0.2118162
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4635244
	speed: 0.4423s/iter; left time: 2247.1054s
Epoch: 14 cost time: 26.367494106292725
Epoch: 14, Steps: 140 | Train Loss: 0.4858641 Vali Loss: 0.4773835 Test Loss: 0.2116181
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3650180
	speed: 0.4438s/iter; left time: 2193.0020s
Epoch: 15 cost time: 26.110393285751343
Epoch: 15, Steps: 140 | Train Loss: 0.4857741 Vali Loss: 0.4757960 Test Loss: 0.2112559
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j192_H12_FITS_custom_ftM_sl720_ll48_pl192_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10348
mse:0.21407094597816467, mae:0.2619304656982422, rse:0.609041690826416, corr:[0.48063067 0.48548198 0.4860562  0.48470366 0.48347315 0.4832038
 0.4836812  0.484279   0.4843882  0.48382476 0.48285025 0.4819353
 0.48137924 0.48124334 0.4812891  0.48116362 0.48073944 0.47985992
 0.47886825 0.47797155 0.47740966 0.4771898  0.47714102 0.47690228
 0.47630283 0.47532696 0.47412997 0.47284302 0.47178546 0.4710086
 0.47054052 0.47015426 0.46970233 0.4689841  0.46810186 0.46712044
 0.46627045 0.46552855 0.4649253  0.46438333 0.4638652  0.4633433
 0.46269652 0.46195486 0.4612069  0.46050325 0.4599593  0.45955536
 0.45910868 0.45863605 0.45814738 0.45755905 0.4570132  0.4564737
 0.4559861  0.4555081  0.4550961  0.45468524 0.4542929  0.45389986
 0.4535213  0.45309305 0.45269287 0.4523118  0.4519709  0.45170355
 0.45137993 0.45105824 0.4507248  0.45041317 0.45014703 0.44980162
 0.4494736  0.4490778  0.44853586 0.44796896 0.44747576 0.44714206
 0.44690582 0.4467031  0.44664428 0.4466928  0.44666815 0.4465398
 0.44629675 0.44588745 0.44552177 0.44513297 0.44482777 0.4445241
 0.44433323 0.4442447  0.44415724 0.44407535 0.44391158 0.4436252
 0.44326273 0.44288665 0.44254494 0.4422594  0.44202226 0.44181967
 0.4416474  0.44143203 0.44115505 0.44083017 0.44039348 0.4399885
 0.439621   0.43932664 0.4390911  0.43886074 0.4385245  0.4382123
 0.43787554 0.4374626  0.43707114 0.43671915 0.4363808  0.43597525
 0.43560708 0.4352743  0.43493643 0.43460444 0.4342811  0.43394488
 0.43354204 0.43308893 0.43262577 0.43216342 0.431683   0.43120977
 0.43078816 0.43039557 0.43007988 0.42974383 0.42936954 0.42902505
 0.4285919  0.42819515 0.4278471  0.4275577  0.42729044 0.4269893
 0.42663187 0.42622745 0.42579752 0.4253742  0.4248707  0.42424536
 0.4235557  0.42284787 0.42211485 0.4213432  0.42051145 0.41964355
 0.41900906 0.41837507 0.41783217 0.41724533 0.41678268 0.41625676
 0.41571724 0.4152254  0.4147831  0.41448745 0.41414943 0.41376933
 0.4133196  0.4126142  0.41164833 0.41052592 0.4094045  0.40866435
 0.40821865 0.40816057 0.40823844 0.40825996 0.40793645 0.4072479
 0.406477   0.40581214 0.40557167 0.4058089  0.40622252 0.4063863
 0.40597978 0.4049256  0.40378243 0.4034855  0.40513927 0.40743622]
