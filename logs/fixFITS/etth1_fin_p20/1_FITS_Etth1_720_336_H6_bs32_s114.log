Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  25200896.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5612000
	speed: 0.0099s/iter; left time: 115.4415s
Epoch: 1 cost time: 1.183133602142334
Epoch: 1, Steps: 118 | Train Loss: 0.6704748 Vali Loss: 1.3341717 Test Loss: 0.5464090
Validation loss decreased (inf --> 1.334172).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4827093
	speed: 0.0257s/iter; left time: 297.3909s
Epoch: 2 cost time: 1.1282074451446533
Epoch: 2, Steps: 118 | Train Loss: 0.5136964 Vali Loss: 1.2290423 Test Loss: 0.4673311
Validation loss decreased (1.334172 --> 1.229042).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4502082
	speed: 0.0279s/iter; left time: 320.1273s
Epoch: 3 cost time: 1.2297048568725586
Epoch: 3, Steps: 118 | Train Loss: 0.4730827 Vali Loss: 1.1923492 Test Loss: 0.4423936
Validation loss decreased (1.229042 --> 1.192349).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4534445
	speed: 0.0317s/iter; left time: 359.5037s
Epoch: 4 cost time: 1.202498435974121
Epoch: 4, Steps: 118 | Train Loss: 0.4569022 Vali Loss: 1.1870310 Test Loss: 0.4358377
Validation loss decreased (1.192349 --> 1.187031).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4794579
	speed: 0.0283s/iter; left time: 317.4993s
Epoch: 5 cost time: 1.2326879501342773
Epoch: 5, Steps: 118 | Train Loss: 0.4501670 Vali Loss: 1.1875582 Test Loss: 0.4348245
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4629883
	speed: 0.0254s/iter; left time: 282.5113s
Epoch: 6 cost time: 1.215515375137329
Epoch: 6, Steps: 118 | Train Loss: 0.4459016 Vali Loss: 1.1903509 Test Loss: 0.4352992
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4364696
	speed: 0.0257s/iter; left time: 282.9795s
Epoch: 7 cost time: 1.1583960056304932
Epoch: 7, Steps: 118 | Train Loss: 0.4435524 Vali Loss: 1.1922349 Test Loss: 0.4366346
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4045547
	speed: 0.0252s/iter; left time: 273.6248s
Epoch: 8 cost time: 1.2296113967895508
Epoch: 8, Steps: 118 | Train Loss: 0.4419627 Vali Loss: 1.1946200 Test Loss: 0.4372627
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4893596
	speed: 0.0244s/iter; left time: 262.8495s
Epoch: 9 cost time: 1.0914099216461182
Epoch: 9, Steps: 118 | Train Loss: 0.4404892 Vali Loss: 1.1978544 Test Loss: 0.4377741
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4286997
	speed: 0.0245s/iter; left time: 261.1829s
Epoch: 10 cost time: 1.2280123233795166
Epoch: 10, Steps: 118 | Train Loss: 0.4393287 Vali Loss: 1.2027229 Test Loss: 0.4382540
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4879199
	speed: 0.0246s/iter; left time: 258.6423s
Epoch: 11 cost time: 1.1749622821807861
Epoch: 11, Steps: 118 | Train Loss: 0.4383670 Vali Loss: 1.2025392 Test Loss: 0.4389195
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4258271
	speed: 0.0256s/iter; left time: 266.2682s
Epoch: 12 cost time: 1.1169826984405518
Epoch: 12, Steps: 118 | Train Loss: 0.4377102 Vali Loss: 1.2054904 Test Loss: 0.4392669
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4771745
	speed: 0.0242s/iter; left time: 248.5315s
Epoch: 13 cost time: 1.0503036975860596
Epoch: 13, Steps: 118 | Train Loss: 0.4371130 Vali Loss: 1.2068770 Test Loss: 0.4394640
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3934464
	speed: 0.0237s/iter; left time: 241.1519s
Epoch: 14 cost time: 1.0792348384857178
Epoch: 14, Steps: 118 | Train Loss: 0.4365546 Vali Loss: 1.2059296 Test Loss: 0.4397345
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4147303
	speed: 0.0254s/iter; left time: 255.2789s
Epoch: 15 cost time: 1.1681711673736572
Epoch: 15, Steps: 118 | Train Loss: 0.4358132 Vali Loss: 1.2073451 Test Loss: 0.4401405
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4807357
	speed: 0.0255s/iter; left time: 253.4522s
Epoch: 16 cost time: 1.195021390914917
Epoch: 16, Steps: 118 | Train Loss: 0.4353123 Vali Loss: 1.2116896 Test Loss: 0.4403702
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4776475
	speed: 0.0250s/iter; left time: 245.5275s
Epoch: 17 cost time: 1.2065775394439697
Epoch: 17, Steps: 118 | Train Loss: 0.4345259 Vali Loss: 1.2062411 Test Loss: 0.4404362
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4219556
	speed: 0.0254s/iter; left time: 246.3844s
Epoch: 18 cost time: 1.2279469966888428
Epoch: 18, Steps: 118 | Train Loss: 0.4347090 Vali Loss: 1.2091404 Test Loss: 0.4407590
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4224815
	speed: 0.0250s/iter; left time: 239.5745s
Epoch: 19 cost time: 1.1307644844055176
Epoch: 19, Steps: 118 | Train Loss: 0.4338457 Vali Loss: 1.2127976 Test Loss: 0.4409549
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4650003
	speed: 0.0250s/iter; left time: 236.0400s
Epoch: 20 cost time: 1.069502353668213
Epoch: 20, Steps: 118 | Train Loss: 0.4343576 Vali Loss: 1.2119514 Test Loss: 0.4409833
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4206165
	speed: 0.0232s/iter; left time: 216.3745s
Epoch: 21 cost time: 1.0938746929168701
Epoch: 21, Steps: 118 | Train Loss: 0.4337156 Vali Loss: 1.2092488 Test Loss: 0.4411184
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4396370
	speed: 0.0241s/iter; left time: 222.0967s
Epoch: 22 cost time: 1.0902395248413086
Epoch: 22, Steps: 118 | Train Loss: 0.4336662 Vali Loss: 1.2122757 Test Loss: 0.4412902
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4227887
	speed: 0.0249s/iter; left time: 227.1061s
Epoch: 23 cost time: 1.1533386707305908
Epoch: 23, Steps: 118 | Train Loss: 0.4334556 Vali Loss: 1.2132887 Test Loss: 0.4412766
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4453899
	speed: 0.0238s/iter; left time: 213.8223s
Epoch: 24 cost time: 1.0488979816436768
Epoch: 24, Steps: 118 | Train Loss: 0.4332507 Vali Loss: 1.2130786 Test Loss: 0.4414859
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.43485936522483826, mae:0.44036999344825745, rse:0.6278074383735657, corr:[0.25153318 0.26077297 0.2540564  0.2561842  0.25646973 0.252803
 0.25215337 0.25416753 0.2541448  0.25264797 0.25257826 0.2530488
 0.2525293  0.25158617 0.2511226  0.25089446 0.25045094 0.24963701
 0.24859874 0.24829839 0.24871372 0.24828267 0.24712703 0.24620326
 0.24621211 0.24627557 0.24603719 0.24623927 0.24731565 0.24831575
 0.24847154 0.24813007 0.24802716 0.24791834 0.24757035 0.2471849
 0.24713741 0.24751447 0.24798629 0.2481766  0.24796708 0.24796686
 0.24852584 0.2489873  0.24893843 0.24920303 0.25007537 0.25052267
 0.2502213  0.24950022 0.2488695  0.2482029  0.24706395 0.24593891
 0.24566934 0.24576555 0.24524601 0.24411763 0.24353336 0.24392042
 0.24431184 0.24406096 0.24353947 0.24340348 0.24367483 0.24402103
 0.24413452 0.24381481 0.24358699 0.24350616 0.24336286 0.2430576
 0.24273859 0.24236041 0.24218951 0.24236393 0.24241674 0.24214107
 0.24181913 0.24183077 0.24196532 0.24158382 0.24061643 0.24011272
 0.24053621 0.24109313 0.24061033 0.23958303 0.23938946 0.24010707
 0.24026094 0.2394248  0.23838347 0.2380771  0.23828827 0.23897588
 0.2400348  0.24075617 0.2409166  0.24104324 0.24114558 0.24121715
 0.24143602 0.24159983 0.24106021 0.23984277 0.23888357 0.23881102
 0.23908351 0.23911378 0.23896302 0.23895948 0.23927563 0.2398607
 0.24034747 0.24042664 0.2403378  0.24034518 0.24035695 0.24028596
 0.24021894 0.24002507 0.23956434 0.23882379 0.23795827 0.23737247
 0.23747699 0.23795192 0.23795989 0.23719358 0.23614709 0.23575912
 0.23620097 0.23629838 0.23542807 0.23445275 0.23440981 0.23506242
 0.23533353 0.23517652 0.23514067 0.23511821 0.23476227 0.23446208
 0.23425873 0.23395    0.2336793  0.23334354 0.23272143 0.23198353
 0.23191933 0.2323335  0.23222102 0.23152162 0.23112687 0.23141208
 0.23169385 0.23169139 0.23161758 0.23132883 0.23082031 0.23086831
 0.2314044  0.23145384 0.23072238 0.23016632 0.22996153 0.22974472
 0.22950135 0.22986154 0.23075745 0.23148452 0.23159273 0.2315093
 0.23151393 0.23147151 0.23120297 0.23083833 0.23042639 0.23017919
 0.22997911 0.22956027 0.22909755 0.22916585 0.22965881 0.22966965
 0.22902253 0.22898068 0.22992818 0.23032379 0.22919862 0.227876
 0.22778355 0.2284503  0.22871265 0.2283307  0.22772858 0.22704667
 0.2264671  0.22625563 0.22631988 0.22625296 0.22563158 0.22477004
 0.22437875 0.22540355 0.22694604 0.22736573 0.22675298 0.22656316
 0.22719386 0.22724637 0.22644053 0.22597061 0.22615352 0.22618718
 0.22580333 0.22567336 0.22591776 0.22598974 0.225553   0.22485158
 0.22445443 0.22436309 0.2242185  0.22381836 0.22353135 0.22400482
 0.22473529 0.22488739 0.22431916 0.22378466 0.22377262 0.2237837
 0.22309905 0.22232302 0.22208111 0.22177228 0.22090042 0.22002088
 0.22013964 0.22088534 0.22190373 0.22278734 0.22276269 0.22157241
 0.22021048 0.21981218 0.22024962 0.22059433 0.22031829 0.21959946
 0.21884653 0.21891205 0.2201806  0.22140752 0.22176477 0.22165486
 0.22158523 0.22115906 0.22073865 0.22093439 0.22125395 0.22039922
 0.21926317 0.21961202 0.22072574 0.22073013 0.21955204 0.21868351
 0.21837938 0.21789044 0.21751371 0.21742666 0.21670514 0.21591265
 0.21651292 0.21775289 0.2178158  0.21709543 0.21694337 0.2170477
 0.21632326 0.21560045 0.21573342 0.21524978 0.2137745  0.21329086
 0.21444751 0.21551849 0.21584082 0.2167343  0.21728525 0.21632677
 0.21455249 0.21355428 0.21329099 0.2136009  0.21449493 0.21508914
 0.21440236 0.21376482 0.2146553  0.21515197 0.21486133 0.21512865
 0.21600036 0.21475339 0.21270326 0.21278627 0.21398096 0.21275958
 0.21055095 0.21093495 0.21237667 0.21174818 0.21072748 0.21086314
 0.21029638 0.20804045 0.20643383 0.20560963 0.20378749 0.2031842
 0.20471187 0.20410827 0.20149472 0.20174591 0.20165256 0.19680978
 0.19389482 0.19674893 0.1918522  0.17779365 0.18259525 0.18237217]
