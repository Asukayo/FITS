Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  34420736.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6830205
	speed: 0.0110s/iter; left time: 122.6634s
Epoch: 1 cost time: 1.245913028717041
Epoch: 1, Steps: 112 | Train Loss: 0.8158586 Vali Loss: 1.7019790 Test Loss: 0.6203633
Validation loss decreased (inf --> 1.701979).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6379513
	speed: 0.0288s/iter; left time: 316.7729s
Epoch: 2 cost time: 1.2516798973083496
Epoch: 2, Steps: 112 | Train Loss: 0.6625643 Vali Loss: 1.5813901 Test Loss: 0.5335808
Validation loss decreased (1.701979 --> 1.581390).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5897224
	speed: 0.0295s/iter; left time: 321.3338s
Epoch: 3 cost time: 1.328913688659668
Epoch: 3, Steps: 112 | Train Loss: 0.6178469 Vali Loss: 1.5187236 Test Loss: 0.4875099
Validation loss decreased (1.581390 --> 1.518724).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5308332
	speed: 0.0287s/iter; left time: 309.1338s
Epoch: 4 cost time: 1.2221999168395996
Epoch: 4, Steps: 112 | Train Loss: 0.5935980 Vali Loss: 1.4798807 Test Loss: 0.4613984
Validation loss decreased (1.518724 --> 1.479881).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6094618
	speed: 0.0299s/iter; left time: 318.6937s
Epoch: 5 cost time: 1.3033008575439453
Epoch: 5, Steps: 112 | Train Loss: 0.5796007 Vali Loss: 1.4625326 Test Loss: 0.4464021
Validation loss decreased (1.479881 --> 1.462533).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5942917
	speed: 0.0299s/iter; left time: 314.6959s
Epoch: 6 cost time: 1.2821435928344727
Epoch: 6, Steps: 112 | Train Loss: 0.5710305 Vali Loss: 1.4504446 Test Loss: 0.4385257
Validation loss decreased (1.462533 --> 1.450445).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5721900
	speed: 0.0284s/iter; left time: 296.6357s
Epoch: 7 cost time: 1.2963223457336426
Epoch: 7, Steps: 112 | Train Loss: 0.5658003 Vali Loss: 1.4415290 Test Loss: 0.4342641
Validation loss decreased (1.450445 --> 1.441529).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5710343
	speed: 0.0288s/iter; left time: 297.2435s
Epoch: 8 cost time: 1.2950029373168945
Epoch: 8, Steps: 112 | Train Loss: 0.5625060 Vali Loss: 1.4388263 Test Loss: 0.4326189
Validation loss decreased (1.441529 --> 1.438826).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5652360
	speed: 0.0320s/iter; left time: 326.6671s
Epoch: 9 cost time: 1.3146884441375732
Epoch: 9, Steps: 112 | Train Loss: 0.5600060 Vali Loss: 1.4368639 Test Loss: 0.4312638
Validation loss decreased (1.438826 --> 1.436864).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5559145
	speed: 0.0302s/iter; left time: 304.8832s
Epoch: 10 cost time: 1.349390983581543
Epoch: 10, Steps: 112 | Train Loss: 0.5582993 Vali Loss: 1.4322972 Test Loss: 0.4315657
Validation loss decreased (1.436864 --> 1.432297).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5391092
	speed: 0.0281s/iter; left time: 280.7551s
Epoch: 11 cost time: 1.2127478122711182
Epoch: 11, Steps: 112 | Train Loss: 0.5571070 Vali Loss: 1.4343166 Test Loss: 0.4314996
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5400148
	speed: 0.0265s/iter; left time: 261.1652s
Epoch: 12 cost time: 1.2124412059783936
Epoch: 12, Steps: 112 | Train Loss: 0.5558883 Vali Loss: 1.4367988 Test Loss: 0.4316606
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6100905
	speed: 0.0269s/iter; left time: 262.1637s
Epoch: 13 cost time: 1.2407026290893555
Epoch: 13, Steps: 112 | Train Loss: 0.5553671 Vali Loss: 1.4338936 Test Loss: 0.4320065
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5188062
	speed: 0.0283s/iter; left time: 272.5813s
Epoch: 14 cost time: 1.2846815586090088
Epoch: 14, Steps: 112 | Train Loss: 0.5541584 Vali Loss: 1.4360609 Test Loss: 0.4320043
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5329609
	speed: 0.0274s/iter; left time: 261.2231s
Epoch: 15 cost time: 1.1906852722167969
Epoch: 15, Steps: 112 | Train Loss: 0.5537114 Vali Loss: 1.4350576 Test Loss: 0.4321185
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4856165
	speed: 0.0262s/iter; left time: 246.6650s
Epoch: 16 cost time: 1.2003047466278076
Epoch: 16, Steps: 112 | Train Loss: 0.5531388 Vali Loss: 1.4373291 Test Loss: 0.4321892
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5454956
	speed: 0.0261s/iter; left time: 243.3157s
Epoch: 17 cost time: 1.21535325050354
Epoch: 17, Steps: 112 | Train Loss: 0.5526628 Vali Loss: 1.4374343 Test Loss: 0.4325134
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5315047
	speed: 0.0279s/iter; left time: 256.5705s
Epoch: 18 cost time: 1.23297119140625
Epoch: 18, Steps: 112 | Train Loss: 0.5527112 Vali Loss: 1.4383945 Test Loss: 0.4326705
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5287819
	speed: 0.0260s/iter; left time: 236.4272s
Epoch: 19 cost time: 1.2008111476898193
Epoch: 19, Steps: 112 | Train Loss: 0.5517207 Vali Loss: 1.4354266 Test Loss: 0.4328028
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5376636
	speed: 0.0266s/iter; left time: 238.5654s
Epoch: 20 cost time: 1.2178595066070557
Epoch: 20, Steps: 112 | Train Loss: 0.5516454 Vali Loss: 1.4360858 Test Loss: 0.4328741
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5671526
	speed: 0.0276s/iter; left time: 244.9190s
Epoch: 21 cost time: 1.2037417888641357
Epoch: 21, Steps: 112 | Train Loss: 0.5515068 Vali Loss: 1.4393826 Test Loss: 0.4330484
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5352769
	speed: 0.0258s/iter; left time: 225.9518s
Epoch: 22 cost time: 1.1837742328643799
Epoch: 22, Steps: 112 | Train Loss: 0.5510009 Vali Loss: 1.4375296 Test Loss: 0.4329822
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5734239
	speed: 0.0273s/iter; left time: 235.7023s
Epoch: 23 cost time: 1.2062528133392334
Epoch: 23, Steps: 112 | Train Loss: 0.5509522 Vali Loss: 1.4412237 Test Loss: 0.4332101
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5480680
	speed: 0.0270s/iter; left time: 229.9919s
Epoch: 24 cost time: 1.2185518741607666
Epoch: 24, Steps: 112 | Train Loss: 0.5502543 Vali Loss: 1.4368373 Test Loss: 0.4332419
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.6102196
	speed: 0.0271s/iter; left time: 228.0561s
Epoch: 25 cost time: 1.234550952911377
Epoch: 25, Steps: 112 | Train Loss: 0.5504016 Vali Loss: 1.4410669 Test Loss: 0.4333637
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4854350
	speed: 0.0278s/iter; left time: 230.8904s
Epoch: 26 cost time: 1.2621300220489502
Epoch: 26, Steps: 112 | Train Loss: 0.5501838 Vali Loss: 1.4404161 Test Loss: 0.4334161
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5383602
	speed: 0.0264s/iter; left time: 216.3259s
Epoch: 27 cost time: 1.1656482219696045
Epoch: 27, Steps: 112 | Train Loss: 0.5497525 Vali Loss: 1.4386909 Test Loss: 0.4335418
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5269604
	speed: 0.0265s/iter; left time: 214.0275s
Epoch: 28 cost time: 1.265063762664795
Epoch: 28, Steps: 112 | Train Loss: 0.5499021 Vali Loss: 1.4355496 Test Loss: 0.4335986
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.5294975
	speed: 0.0269s/iter; left time: 214.3899s
Epoch: 29 cost time: 1.2800581455230713
Epoch: 29, Steps: 112 | Train Loss: 0.5496969 Vali Loss: 1.4426076 Test Loss: 0.4336790
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.5941114
	speed: 0.0269s/iter; left time: 211.2022s
Epoch: 30 cost time: 1.2293918132781982
Epoch: 30, Steps: 112 | Train Loss: 0.5494779 Vali Loss: 1.4370965 Test Loss: 0.4336915
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4303549826145172, mae:0.45522555708885193, rse:0.6280083060264587, corr:[0.21800977 0.23241903 0.23008466 0.23465726 0.23563695 0.23182788
 0.23131639 0.23308401 0.23270845 0.23131107 0.23092496 0.23129348
 0.2311217  0.23029515 0.22950605 0.22944055 0.2297973  0.22932027
 0.22832946 0.22833465 0.22923268 0.22927028 0.22889452 0.22922362
 0.23011544 0.23052736 0.230613   0.23115738 0.2318164  0.23164865
 0.23093943 0.23064438 0.230852   0.23074849 0.22996898 0.22897643
 0.22871049 0.22901864 0.22900964 0.22843525 0.22822377 0.22882156
 0.22929071 0.22903977 0.22895706 0.22985081 0.23070814 0.23077096
 0.23088174 0.2309255  0.23029688 0.22893335 0.22780252 0.22755365
 0.22735606 0.22629057 0.22542942 0.22518972 0.2253902  0.2254288
 0.2249944  0.22446245 0.22430019 0.2243376  0.22431888 0.22432902
 0.2244925  0.22466558 0.22457348 0.22430927 0.2242883  0.22463174
 0.2244307  0.2236507  0.2231075  0.22311527 0.2230212  0.22241941
 0.22186619 0.22189163 0.22228506 0.22230728 0.22186778 0.22136872
 0.22105387 0.22075205 0.22017093 0.21963637 0.21950306 0.21963367
 0.21952283 0.21941507 0.21955286 0.2199747  0.22009127 0.2205426
 0.22164012 0.22263671 0.22319858 0.22365232 0.22400849 0.2240839
 0.22389802 0.22365315 0.223476   0.22324339 0.22294524 0.22262102
 0.22235751 0.22238602 0.22261022 0.22267182 0.22250946 0.22245161
 0.22281371 0.22313106 0.22299074 0.22267377 0.22260298 0.22264804
 0.22240292 0.22192597 0.2215443  0.22136799 0.22089274 0.22030117
 0.22012256 0.22017385 0.21994571 0.21953006 0.21943432 0.21942781
 0.21917175 0.21870287 0.21831311 0.21807957 0.21795675 0.2181228
 0.21846923 0.21867862 0.2186558  0.21836744 0.21776989 0.2176714
 0.2180714  0.21820524 0.21766645 0.21679828 0.21637447 0.2164808
 0.21645421 0.21615113 0.21592934 0.21581751 0.2155792  0.2152132
 0.2148657  0.21466196 0.21460412 0.21481372 0.2151366  0.21535486
 0.21544214 0.21545745 0.21539643 0.21546102 0.21524066 0.21521422
 0.21537046 0.21571136 0.2162652  0.21683635 0.2168449  0.21659097
 0.21668328 0.21682641 0.21626768 0.21559076 0.21564029 0.2162229
 0.21633527 0.21598059 0.21583758 0.21595581 0.2159593  0.21582468
 0.21587536 0.21616027 0.21652025 0.21678771 0.21698377 0.21707869
 0.21681905 0.21626501 0.21568754 0.21525997 0.21502748 0.21489656
 0.21477488 0.21482667 0.21510288 0.21508831 0.21459883 0.21414441
 0.21435466 0.21488833 0.21508157 0.21484601 0.21468766 0.21469015
 0.2146281  0.21433544 0.21392655 0.2134645  0.21304134 0.21303137
 0.21321264 0.21310002 0.212823   0.21274234 0.21270244 0.21268997
 0.21292996 0.21310626 0.21270145 0.212182   0.21212159 0.21227846
 0.21213214 0.21193151 0.21187207 0.21165277 0.21129598 0.21131301
 0.21161774 0.21166062 0.21127804 0.21095473 0.21103847 0.21123184
 0.21146694 0.2116423  0.21191372 0.21214455 0.21211386 0.21183163
 0.2114989  0.21124044 0.21111001 0.21084397 0.21042146 0.21001443
 0.20985667 0.20992656 0.21016447 0.21025664 0.21015857 0.21027417
 0.21081737 0.21113499 0.21093099 0.21060298 0.2106052  0.21080647
 0.21065988 0.21030435 0.21012452 0.20994735 0.20941462 0.20918289
 0.20948072 0.20933826 0.20858338 0.20854253 0.20940301 0.20974462
 0.20909037 0.20865543 0.20865735 0.20835768 0.20794003 0.2084303
 0.20910539 0.20903018 0.20827208 0.20813003 0.20870626 0.20933048
 0.20960492 0.20975357 0.21005231 0.21046978 0.2109952  0.21141034
 0.21151392 0.2114508  0.21128833 0.21105194 0.21088916 0.21103327
 0.21110646 0.2106775  0.21000782 0.20984724 0.21003416 0.21021093
 0.21068569 0.21136501 0.2116864  0.2115485  0.21141604 0.21180367
 0.21219562 0.21217543 0.21226618 0.21235302 0.21155745 0.21038623
 0.21005528 0.21023427 0.20981564 0.20904122 0.20901239 0.20960379
 0.20972425 0.20923753 0.20894562 0.20920762 0.20956898 0.20993158
 0.21032286 0.21033956 0.20969865 0.20898668 0.2089867  0.20969598
 0.21021198 0.21010537 0.20992523 0.20969644 0.20900705 0.20822167
 0.20809147 0.20819366 0.20753215 0.20662363 0.20673618 0.20749585
 0.2074353  0.20659944 0.2060725  0.2061877  0.2063609  0.2068923
 0.20763284 0.2078356  0.20745142 0.20731752 0.207162   0.20676184
 0.20651692 0.2069382  0.20759021 0.20754024 0.20686676 0.20654562
 0.2065159  0.20616211 0.20570327 0.2055359  0.2052406  0.20481603
 0.20467468 0.20434086 0.20318222 0.20213266 0.2024515  0.20309567
 0.20280704 0.20222841 0.20243838 0.20297237 0.20283365 0.2029361
 0.20364872 0.20413862 0.20440266 0.20517378 0.20558427 0.20462476
 0.20373623 0.20384689 0.20358753 0.20231874 0.20181528 0.20277232
 0.20285976 0.20160969 0.20119868 0.2022195  0.20281759 0.20252532
 0.20249505 0.20295866 0.20314111 0.20303231 0.20292257 0.20284837
 0.20306283 0.20401934 0.2051147  0.20499279 0.20405102 0.2037908
 0.20423977 0.20388187 0.20292687 0.20255294 0.20271745 0.20288502
 0.20304516 0.20284763 0.20155832 0.20037523 0.20078966 0.20191331
 0.20217794 0.20179033 0.20137164 0.20162705 0.20236664 0.20364273
 0.20453149 0.20385101 0.20274429 0.20293917 0.2033233  0.20257449
 0.20198871 0.20253342 0.20256191 0.20155737 0.20124176 0.20220682
 0.2025183  0.20191622 0.20218444 0.20312509 0.20316222 0.20252538
 0.2023936  0.20237555 0.20207337 0.20219183 0.20281948 0.20310436
 0.20300002 0.20351686 0.20431507 0.20395839 0.20288503 0.20247152
 0.20239177 0.20160888 0.20091903 0.2012359  0.2015046  0.2014565
 0.20183203 0.202173   0.20143136 0.2007863  0.20171635 0.2031099
 0.20320064 0.20266683 0.20250805 0.20185228 0.20092605 0.20094308
 0.20178401 0.20190702 0.20161977 0.20217112 0.20243604 0.20133655
 0.20049936 0.20126775 0.20156169 0.2003866  0.19972059 0.20082866
 0.20175089 0.20137346 0.20111044 0.20137285 0.2010507  0.20019968
 0.1997922  0.19967757 0.19933213 0.19931255 0.19997804 0.20098679
 0.20189184 0.20253679 0.20252503 0.20205942 0.20212491 0.20258914
 0.20239809 0.20143795 0.20071645 0.20027797 0.19951142 0.1993573
 0.20051953 0.20145254 0.20090915 0.19998513 0.20060945 0.20198734
 0.20254587 0.20223516 0.20213579 0.20211162 0.20297779 0.20377034
 0.20371069 0.20262875 0.20242892 0.20348386 0.20329022 0.20210867
 0.2024592  0.20410122 0.20376147 0.20179118 0.20120813 0.20220315
 0.20209503 0.20094165 0.20109546 0.2022035  0.20300344 0.20361646
 0.20426843 0.20396566 0.20279755 0.2024213  0.2027879  0.20226094
 0.20073125 0.19962566 0.19910522 0.19882892 0.19926074 0.20008418
 0.19991413 0.19878559 0.19788305 0.1971988  0.19606933 0.19551055
 0.19593976 0.19582415 0.19501108 0.19478372 0.19585806 0.19606347
 0.1952473  0.19516529 0.196225   0.1963789  0.1953427  0.19477174
 0.1952908  0.19585612 0.19564004 0.19525883 0.19460386 0.19393554
 0.19441196 0.19494523 0.19428574 0.1929607  0.19253433 0.1923805
 0.19180048 0.1913487  0.19209231 0.19286382 0.1927122  0.19234985
 0.1927106  0.1933811  0.19332767 0.1924089  0.19129044 0.19062757
 0.19022168 0.18892999 0.18732528 0.18731263 0.18845668 0.18832509
 0.18719448 0.1872208  0.18763225 0.18614376 0.183664   0.1834251
 0.1850496  0.18561955 0.18504664 0.18499914 0.18550992 0.18563564
 0.18597157 0.18691677 0.18756802 0.18748532 0.18746963 0.18749782
 0.18668042 0.18518764 0.18425444 0.18422185 0.18419828 0.18369949
 0.18296647 0.18200237 0.1812956  0.18118587 0.18094951 0.18085368
 0.18142529 0.18206544 0.18211606 0.18202789 0.18207109 0.18108225
 0.17892948 0.17814462 0.17954022 0.1809067  0.18104114 0.18142232
 0.1822181  0.18183619 0.18054086 0.18053587 0.18075247 0.17960493
 0.17864469 0.17923655 0.17941287 0.17775586 0.17663004 0.17730457
 0.17753424 0.17595835 0.17434835 0.1738413  0.17455973 0.17630407
 0.17753167 0.1763237  0.17395504 0.17350575 0.17395927 0.17296274
 0.1714656  0.17165431 0.17174633 0.17002183 0.16881433 0.16848183
 0.16745664 0.16561918 0.16529167 0.16501068 0.16349669 0.16300099
 0.16394955 0.16308382 0.16096221 0.16122633 0.16339567 0.16351385
 0.16229343 0.16283908 0.16353239 0.16208239 0.16188629 0.16402341
 0.16409633 0.16213205 0.16178867 0.16247058 0.1598635  0.15747993
 0.15952654 0.16009885 0.15604292 0.15344319 0.15460174 0.15466319
 0.15416478 0.15505098 0.15342864 0.1493273  0.15003967 0.15202615
 0.14658134 0.14483602 0.15028135 0.13830808 0.12914322 0.16867515]
