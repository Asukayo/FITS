Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=134, out_features=151, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  18129664.0
params:  20385.0
Trainable parameters:  20385
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.091352224349976
Epoch: 1, Steps: 61 | Train Loss: 0.5905101 Vali Loss: 0.2809253 Test Loss: 0.3097096
Validation loss decreased (inf --> 0.280925).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.308730602264404
Epoch: 2, Steps: 61 | Train Loss: 0.4697817 Vali Loss: 0.2503036 Test Loss: 0.2876990
Validation loss decreased (0.280925 --> 0.250304).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.427089214324951
Epoch: 3, Steps: 61 | Train Loss: 0.4462493 Vali Loss: 0.2392676 Test Loss: 0.2825502
Validation loss decreased (0.250304 --> 0.239268).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.406484842300415
Epoch: 4, Steps: 61 | Train Loss: 0.4351286 Vali Loss: 0.2345479 Test Loss: 0.2800957
Validation loss decreased (0.239268 --> 0.234548).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.571908473968506
Epoch: 5, Steps: 61 | Train Loss: 0.4285501 Vali Loss: 0.2297699 Test Loss: 0.2790653
Validation loss decreased (0.234548 --> 0.229770).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.443962812423706
Epoch: 6, Steps: 61 | Train Loss: 0.4236986 Vali Loss: 0.2279387 Test Loss: 0.2778234
Validation loss decreased (0.229770 --> 0.227939).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.612498760223389
Epoch: 7, Steps: 61 | Train Loss: 0.4200658 Vali Loss: 0.2260412 Test Loss: 0.2769214
Validation loss decreased (0.227939 --> 0.226041).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.408075332641602
Epoch: 8, Steps: 61 | Train Loss: 0.4180381 Vali Loss: 0.2228653 Test Loss: 0.2766780
Validation loss decreased (0.226041 --> 0.222865).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.643349885940552
Epoch: 9, Steps: 61 | Train Loss: 0.4148757 Vali Loss: 0.2231447 Test Loss: 0.2762375
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.47700572013855
Epoch: 10, Steps: 61 | Train Loss: 0.4146239 Vali Loss: 0.2225027 Test Loss: 0.2755665
Validation loss decreased (0.222865 --> 0.222503).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.752131223678589
Epoch: 11, Steps: 61 | Train Loss: 0.4132080 Vali Loss: 0.2215710 Test Loss: 0.2756617
Validation loss decreased (0.222503 --> 0.221571).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 6.492269515991211
Epoch: 12, Steps: 61 | Train Loss: 0.4111266 Vali Loss: 0.2203328 Test Loss: 0.2750252
Validation loss decreased (0.221571 --> 0.220333).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.228024244308472
Epoch: 13, Steps: 61 | Train Loss: 0.4113586 Vali Loss: 0.2195628 Test Loss: 0.2748294
Validation loss decreased (0.220333 --> 0.219563).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.50659966468811
Epoch: 14, Steps: 61 | Train Loss: 0.4104613 Vali Loss: 0.2193556 Test Loss: 0.2746605
Validation loss decreased (0.219563 --> 0.219356).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.602662086486816
Epoch: 15, Steps: 61 | Train Loss: 0.4093730 Vali Loss: 0.2187217 Test Loss: 0.2746138
Validation loss decreased (0.219356 --> 0.218722).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.32339072227478
Epoch: 16, Steps: 61 | Train Loss: 0.4094441 Vali Loss: 0.2172264 Test Loss: 0.2743513
Validation loss decreased (0.218722 --> 0.217226).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.668942928314209
Epoch: 17, Steps: 61 | Train Loss: 0.4080457 Vali Loss: 0.2185008 Test Loss: 0.2742704
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.103204488754272
Epoch: 18, Steps: 61 | Train Loss: 0.4084801 Vali Loss: 0.2184434 Test Loss: 0.2740185
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 8.912625551223755
Epoch: 19, Steps: 61 | Train Loss: 0.4078582 Vali Loss: 0.2173901 Test Loss: 0.2739250
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.2737498879432678, mae:0.33878833055496216, rse:0.4216559827327728, corr:[0.27065247 0.27608612 0.27608377 0.27416858 0.2735161  0.27357972
 0.27338865 0.27234113 0.2710937  0.26977852 0.2683671  0.26705256
 0.26598218 0.26508757 0.26408863 0.2631455  0.26250538 0.26194558
 0.2611456  0.26003584 0.25863573 0.25728428 0.25595072 0.25436032
 0.25252566 0.25071102 0.24928063 0.24828668 0.24733791 0.24593043
 0.24399169 0.24211164 0.24095209 0.24037795 0.23966023 0.23826323
 0.23648168 0.23519443 0.23477875 0.2344848  0.23374142 0.23260395
 0.23160625 0.23114818 0.23108673 0.23069397 0.22947203 0.22741747
 0.22513601 0.22320339 0.22169188 0.22000675 0.2183499  0.2168103
 0.21503602 0.21317919 0.21124658 0.20951414 0.208504   0.20819238
 0.20793946 0.2070466  0.20608631 0.20588337 0.20595445 0.20607533
 0.20560367 0.20475839 0.2042761  0.2043686  0.2042442  0.20335528
 0.20168604 0.19998592 0.19911943 0.19836572 0.19745915 0.196152
 0.1951952  0.19471307 0.19515707 0.19520071 0.19376019 0.19201832
 0.19178608 0.19346817 0.19463693 0.19371459 0.1910049  0.18997058
 0.19127691 0.19176473 0.18989992 0.1872714  0.18860394 0.19245996]
