Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  142517760.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.683387279510498
Epoch: 1, Steps: 14 | Train Loss: 0.9800299 Vali Loss: 0.6451161 Test Loss: 0.4634033
Validation loss decreased (inf --> 0.645116).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.688716173171997
Epoch: 2, Steps: 14 | Train Loss: 0.8655598 Vali Loss: 0.5842970 Test Loss: 0.4259351
Validation loss decreased (0.645116 --> 0.584297).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.914991855621338
Epoch: 3, Steps: 14 | Train Loss: 0.7944007 Vali Loss: 0.5476817 Test Loss: 0.4034133
Validation loss decreased (0.584297 --> 0.547682).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.4567179679870605
Epoch: 4, Steps: 14 | Train Loss: 0.7622131 Vali Loss: 0.5207220 Test Loss: 0.3907617
Validation loss decreased (0.547682 --> 0.520722).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.45027756690979
Epoch: 5, Steps: 14 | Train Loss: 0.7268931 Vali Loss: 0.5026455 Test Loss: 0.3833798
Validation loss decreased (0.520722 --> 0.502645).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.833800792694092
Epoch: 6, Steps: 14 | Train Loss: 0.7144036 Vali Loss: 0.4846436 Test Loss: 0.3787804
Validation loss decreased (0.502645 --> 0.484644).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.608350992202759
Epoch: 7, Steps: 14 | Train Loss: 0.7067159 Vali Loss: 0.4765045 Test Loss: 0.3757195
Validation loss decreased (0.484644 --> 0.476505).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.6190011501312256
Epoch: 8, Steps: 14 | Train Loss: 0.6937433 Vali Loss: 0.4674843 Test Loss: 0.3735555
Validation loss decreased (0.476505 --> 0.467484).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.708124876022339
Epoch: 9, Steps: 14 | Train Loss: 0.6880925 Vali Loss: 0.4651710 Test Loss: 0.3719145
Validation loss decreased (0.467484 --> 0.465171).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.5936243534088135
Epoch: 10, Steps: 14 | Train Loss: 0.6849285 Vali Loss: 0.4576572 Test Loss: 0.3705862
Validation loss decreased (0.465171 --> 0.457657).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.8462612628936768
Epoch: 11, Steps: 14 | Train Loss: 0.6765496 Vali Loss: 0.4506856 Test Loss: 0.3695888
Validation loss decreased (0.457657 --> 0.450686).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.922370433807373
Epoch: 12, Steps: 14 | Train Loss: 0.6722426 Vali Loss: 0.4496037 Test Loss: 0.3687144
Validation loss decreased (0.450686 --> 0.449604).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.8406245708465576
Epoch: 13, Steps: 14 | Train Loss: 0.6702565 Vali Loss: 0.4453726 Test Loss: 0.3679736
Validation loss decreased (0.449604 --> 0.445373).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.7541120052337646
Epoch: 14, Steps: 14 | Train Loss: 0.6634409 Vali Loss: 0.4468507 Test Loss: 0.3673156
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.544423818588257
Epoch: 15, Steps: 14 | Train Loss: 0.6626877 Vali Loss: 0.4443180 Test Loss: 0.3668110
Validation loss decreased (0.445373 --> 0.444318).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.7752175331115723
Epoch: 16, Steps: 14 | Train Loss: 0.6586492 Vali Loss: 0.4415146 Test Loss: 0.3663076
Validation loss decreased (0.444318 --> 0.441515).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.6668496131896973
Epoch: 17, Steps: 14 | Train Loss: 0.6597891 Vali Loss: 0.4307118 Test Loss: 0.3658990
Validation loss decreased (0.441515 --> 0.430712).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.5335657596588135
Epoch: 18, Steps: 14 | Train Loss: 0.6610582 Vali Loss: 0.4330293 Test Loss: 0.3655410
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.8493916988372803
Epoch: 19, Steps: 14 | Train Loss: 0.6587965 Vali Loss: 0.4299518 Test Loss: 0.3651178
Validation loss decreased (0.430712 --> 0.429952).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.663414239883423
Epoch: 20, Steps: 14 | Train Loss: 0.6531419 Vali Loss: 0.4339674 Test Loss: 0.3647663
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.7258825302124023
Epoch: 21, Steps: 14 | Train Loss: 0.6579915 Vali Loss: 0.4244624 Test Loss: 0.3644380
Validation loss decreased (0.429952 --> 0.424462).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.7896087169647217
Epoch: 22, Steps: 14 | Train Loss: 0.6561404 Vali Loss: 0.4318016 Test Loss: 0.3641759
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.860412359237671
Epoch: 23, Steps: 14 | Train Loss: 0.6540244 Vali Loss: 0.4213211 Test Loss: 0.3638966
Validation loss decreased (0.424462 --> 0.421321).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.7395737171173096
Epoch: 24, Steps: 14 | Train Loss: 0.6553361 Vali Loss: 0.4251198 Test Loss: 0.3636675
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.708117961883545
Epoch: 25, Steps: 14 | Train Loss: 0.6549316 Vali Loss: 0.4282842 Test Loss: 0.3634191
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.66805362701416
Epoch: 26, Steps: 14 | Train Loss: 0.6526391 Vali Loss: 0.4186877 Test Loss: 0.3632259
Validation loss decreased (0.421321 --> 0.418688).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.9903056621551514
Epoch: 27, Steps: 14 | Train Loss: 0.6481026 Vali Loss: 0.4232115 Test Loss: 0.3630127
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.8699352741241455
Epoch: 28, Steps: 14 | Train Loss: 0.6483208 Vali Loss: 0.4212464 Test Loss: 0.3628290
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.7161059379577637
Epoch: 29, Steps: 14 | Train Loss: 0.6510384 Vali Loss: 0.4200408 Test Loss: 0.3626510
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.36190372705459595, mae:0.40440043807029724, rse:0.4809900224208832, corr:[0.24550907 0.25631377 0.25662413 0.2545197  0.2563142  0.25800717
 0.2574067  0.256348   0.25548127 0.2548817  0.25378472 0.25277907
 0.25218964 0.25116074 0.24987896 0.24887252 0.24836272 0.24749738
 0.24660455 0.24586304 0.24517229 0.24437669 0.24259466 0.24075443
 0.23936036 0.23856662 0.23777719 0.23699254 0.23623753 0.23541111
 0.23456772 0.23389074 0.23347776 0.23241128 0.23092137 0.22986108
 0.22967847 0.22934411 0.22799945 0.22652514 0.22600107 0.22590426
 0.22524883 0.22408548 0.22307298 0.22245343 0.2210175  0.21858408
 0.21653825 0.21548261 0.214499   0.21298994 0.21147344 0.21037145
 0.20931414 0.20769098 0.20565242 0.20424007 0.20355037 0.20260485
 0.20102128 0.20000091 0.20041728 0.20097291 0.20029998 0.199014
 0.19853912 0.19913562 0.1992134  0.19800526 0.19635706 0.1953085
 0.19457439 0.1932171  0.1915276  0.19051144 0.19012451 0.18927145
 0.18784511 0.1871438  0.1871712  0.18650961 0.18490739 0.1838352
 0.18414609 0.18450253 0.18361738 0.18213733 0.1817093  0.18210937
 0.18195969 0.18108323 0.18081728 0.18146352 0.1818066  0.18089098
 0.1796075  0.17908725 0.17892757 0.17810598 0.17700626 0.1765874
 0.17676823 0.17636749 0.17537875 0.1746538  0.17465723 0.17414428
 0.17270091 0.17175977 0.17212197 0.1726131  0.17164798 0.16980398
 0.16878854 0.16906816 0.1690746  0.16768129 0.16591828 0.16478276
 0.16379903 0.16212592 0.1602417  0.15917245 0.1589128  0.1585334
 0.15757927 0.15704753 0.15727152 0.15698572 0.15518996 0.15281996
 0.15169607 0.15178365 0.15183902 0.15121715 0.15052778 0.15006556
 0.14912187 0.1477091  0.14693263 0.1472363  0.14714989 0.14513254
 0.14193079 0.13978766 0.13913044 0.1384986  0.13708314 0.1357746
 0.13516735 0.1346145  0.13358186 0.13246205 0.13198146 0.13152583
 0.13021435 0.12867624 0.12849197 0.1293181  0.1291376  0.12753052
 0.12632887 0.12701671 0.1283589  0.12811288 0.12629685 0.124603
 0.12376184 0.12253343 0.12037732 0.11854295 0.11802449 0.11773594
 0.1166166  0.11505897 0.11458196 0.11496469 0.11446696 0.11289556
 0.11165167 0.11150912 0.11136266 0.11029449 0.10896683 0.10889269
 0.10932848 0.10877616 0.10782028 0.10803477 0.10929343 0.10916843
 0.10704603 0.10503072 0.10479067 0.10547124 0.10501924 0.10350262
 0.1026046  0.10259973 0.10254793 0.10203761 0.10201227 0.10244828
 0.10201289 0.10035195 0.09900811 0.099471   0.10038186 0.09988938
 0.09869447 0.09923516 0.10126432 0.10176984 0.09951692 0.09683017
 0.09607897 0.09611999 0.09472826 0.09268969 0.0923538  0.09399337
 0.09456207 0.093338   0.09236974 0.09330963 0.09452731 0.0937139
 0.09170471 0.09120231 0.09219709 0.09279317 0.09227725 0.09257201
 0.09410664 0.09502879 0.09428294 0.0936751  0.09465037 0.09536445
 0.09343497 0.0900609  0.08881869 0.09043355 0.09152021 0.09020723
 0.08901555 0.08978094 0.09104539 0.09066739 0.08962234 0.09011252
 0.09125928 0.09142485 0.09085265 0.09195127 0.09400321 0.09480011
 0.09386483 0.09426162 0.09710009 0.09911065 0.09826165 0.09616873
 0.09619585 0.0976084  0.09768049 0.09604482 0.09522872 0.09644041
 0.09736596 0.09663869 0.0963902  0.09785913 0.09954071 0.09843307
 0.09632523 0.0965597  0.09882655 0.09964205 0.09818657 0.09753769
 0.09905734 0.10026884 0.09899897 0.09785087 0.0997584  0.10243903
 0.10145879 0.09744433 0.09546008 0.09745612 0.09905858 0.09697063
 0.09427783 0.09459117 0.09656134 0.09641164 0.09489766 0.09490564
 0.09633875 0.09665854 0.0953982  0.09584655 0.0981722  0.09971064
 0.09803752 0.09679145 0.09893735 0.10181945 0.10085271 0.0974057
 0.09698251 0.09941672 0.09977059 0.09591075 0.09302793 0.09484077
 0.09760437 0.09671661 0.09417985 0.09583507 0.10004331 0.10111383
 0.09828227 0.09766306 0.10139152 0.10349997 0.10148019 0.10117462
 0.105078   0.10626425 0.10104881 0.10004072 0.11337667 0.11950099]
