Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.082900285720825
Epoch: 1, Steps: 59 | Train Loss: 0.8275458 Vali Loss: 0.5094085 Test Loss: 0.3947662
Validation loss decreased (inf --> 0.509408).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.359505414962769
Epoch: 2, Steps: 59 | Train Loss: 0.6995164 Vali Loss: 0.4534453 Test Loss: 0.3752927
Validation loss decreased (0.509408 --> 0.453445).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.456441879272461
Epoch: 3, Steps: 59 | Train Loss: 0.6679406 Vali Loss: 0.4299751 Test Loss: 0.3685392
Validation loss decreased (0.453445 --> 0.429975).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.948710441589355
Epoch: 4, Steps: 59 | Train Loss: 0.6544390 Vali Loss: 0.4168161 Test Loss: 0.3652873
Validation loss decreased (0.429975 --> 0.416816).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.778916597366333
Epoch: 5, Steps: 59 | Train Loss: 0.6454352 Vali Loss: 0.4122110 Test Loss: 0.3629307
Validation loss decreased (0.416816 --> 0.412211).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.159116506576538
Epoch: 6, Steps: 59 | Train Loss: 0.6387241 Vali Loss: 0.4063334 Test Loss: 0.3617113
Validation loss decreased (0.412211 --> 0.406333).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.167600154876709
Epoch: 7, Steps: 59 | Train Loss: 0.6362970 Vali Loss: 0.4034076 Test Loss: 0.3610323
Validation loss decreased (0.406333 --> 0.403408).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.403918743133545
Epoch: 8, Steps: 59 | Train Loss: 0.6311430 Vali Loss: 0.4017560 Test Loss: 0.3602428
Validation loss decreased (0.403408 --> 0.401756).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.735586881637573
Epoch: 9, Steps: 59 | Train Loss: 0.6290678 Vali Loss: 0.3992435 Test Loss: 0.3597837
Validation loss decreased (0.401756 --> 0.399244).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.815824031829834
Epoch: 10, Steps: 59 | Train Loss: 0.6252750 Vali Loss: 0.3955873 Test Loss: 0.3594921
Validation loss decreased (0.399244 --> 0.395587).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.229374885559082
Epoch: 11, Steps: 59 | Train Loss: 0.6253810 Vali Loss: 0.3962124 Test Loss: 0.3592274
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.085537910461426
Epoch: 12, Steps: 59 | Train Loss: 0.6249428 Vali Loss: 0.3918113 Test Loss: 0.3589538
Validation loss decreased (0.395587 --> 0.391811).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.971254110336304
Epoch: 13, Steps: 59 | Train Loss: 0.6235676 Vali Loss: 0.3891388 Test Loss: 0.3590616
Validation loss decreased (0.391811 --> 0.389139).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.902037858963013
Epoch: 14, Steps: 59 | Train Loss: 0.6229047 Vali Loss: 0.3900878 Test Loss: 0.3588305
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.408147096633911
Epoch: 15, Steps: 59 | Train Loss: 0.6221668 Vali Loss: 0.3892686 Test Loss: 0.3588248
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.531922578811646
Epoch: 16, Steps: 59 | Train Loss: 0.6197807 Vali Loss: 0.3900938 Test Loss: 0.3586052
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3548849821090698, mae:0.39692506194114685, rse:0.4763030409812927, corr:[0.2580861  0.26412165 0.25999403 0.2620802  0.26291656 0.26024434
 0.2601353  0.2610595  0.25926545 0.25756106 0.25734913 0.25645968
 0.25475985 0.2537917  0.2532084  0.25201845 0.25135514 0.25127268
 0.25065067 0.24915431 0.24813329 0.24781823 0.24675171 0.24532291
 0.24398708 0.24257135 0.24090067 0.23996408 0.23967294 0.23891725
 0.23777398 0.236878   0.23634942 0.23537761 0.234367   0.23371725
 0.23313236 0.23233938 0.23139016 0.23048732 0.22956975 0.22896178
 0.22889484 0.22833014 0.2266292  0.22528355 0.2245132  0.22304562
 0.22094692 0.21945173 0.2186575  0.21753979 0.21558931 0.21355592
 0.21246184 0.21193954 0.21065044 0.20869353 0.20714605 0.2062459
 0.2055251  0.20496157 0.20479184 0.20465422 0.20440075 0.20424545
 0.2037517  0.20315708 0.20291951 0.20277734 0.20196557 0.20079981
 0.19987382 0.1989411  0.19783603 0.1972463  0.19726287 0.19647832
 0.19471955 0.19356516 0.19369711 0.19362742 0.19294569 0.19238634
 0.19216055 0.1919224  0.19179922 0.1917003  0.19117725 0.19030756
 0.18977009 0.18954717 0.18918699 0.18880306 0.18889004 0.1889539
 0.18855505 0.18797773 0.18744622 0.18679838 0.18647037 0.18665422
 0.18653676 0.1856618  0.18521972 0.18523832 0.18479152 0.1836981
 0.18323886 0.1834665  0.1828943  0.18184419 0.18149152 0.18152806
 0.18062086 0.1794173  0.17885877 0.17824881 0.17733844 0.17662342
 0.17593776 0.17475525 0.17398956 0.17407772 0.17338803 0.17135654
 0.17016044 0.17078996 0.17067422 0.16861387 0.16696201 0.16661303
 0.16600294 0.16504247 0.16540422 0.16601467 0.16481018 0.16316748
 0.16294107 0.16284142 0.16150707 0.16046312 0.16048746 0.15939045
 0.1567462  0.15502381 0.15447746 0.15328859 0.1519773  0.15206738
 0.15204711 0.15044348 0.1493445  0.14962377 0.14902192 0.14686683
 0.14583229 0.14653082 0.14649594 0.14506553 0.14418893 0.14446059
 0.14405417 0.14307117 0.14301448 0.14353439 0.14344102 0.14264345
 0.14151548 0.13975868 0.13824204 0.137866   0.13779077 0.13668776
 0.13559717 0.13488683 0.133924   0.13244012 0.13118182 0.1305003
 0.12953842 0.12892275 0.12941493 0.12985681 0.12901767 0.12824579
 0.1278736  0.12706338 0.12642281 0.12689379 0.12751977 0.12661055
 0.12562838 0.12578766 0.1252844  0.12368938 0.1232914  0.12451484
 0.12460405 0.12303751 0.12266321 0.1232354  0.12243479 0.12096989
 0.12096164 0.12104156 0.11932395 0.11850349 0.12017291 0.12112119
 0.11928403 0.11815377 0.11965486 0.1207948  0.11988352 0.11889565
 0.11886275 0.11819129 0.1170005  0.11672834 0.11615416 0.11470789
 0.11398851 0.11513789 0.11552352 0.11428346 0.11359346 0.11402008
 0.11369454 0.11258826 0.11217793 0.11286715 0.11344092 0.11390538
 0.1140473  0.11353004 0.1130921  0.11364096 0.11397287 0.1131415
 0.11241179 0.11244515 0.11180743 0.11050901 0.11025047 0.11081876
 0.11060065 0.11008284 0.11097838 0.11153328 0.10999521 0.10878327
 0.1095944  0.11090741 0.11114724 0.11253191 0.11444382 0.11487961
 0.11416219 0.11509147 0.11631218 0.11495621 0.11365269 0.11488618
 0.11615577 0.1146488  0.11370271 0.11534192 0.11604495 0.11465849
 0.11433653 0.11555383 0.11532765 0.11380761 0.11473288 0.11605094
 0.11505021 0.11381388 0.11499234 0.1163811  0.11609204 0.11622284
 0.11732483 0.11704154 0.1158178  0.1165245  0.11751503 0.11589445
 0.11427658 0.11537287 0.11604459 0.1141973  0.11301199 0.11346441
 0.11235299 0.10983823 0.11018226 0.11232788 0.11222757 0.11041379
 0.11015078 0.11109568 0.11107603 0.11189321 0.1133989  0.11425576
 0.11371499 0.11400871 0.11401439 0.11300547 0.11329401 0.11449896
 0.1131338  0.1100854  0.11043435 0.11165558 0.10927022 0.10671544
 0.10836167 0.11014821 0.10858992 0.10951686 0.1129848  0.11354205
 0.11182167 0.11365843 0.11603934 0.11473364 0.11507561 0.11813084
 0.11630144 0.11354897 0.11894754 0.12130135 0.1182173  0.1321821 ]
