Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  25200896.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6493804
	speed: 0.1459s/iter; left time: 846.6045s
Epoch: 1 cost time: 17.2126886844635
Epoch: 1, Steps: 118 | Train Loss: 0.7687237 Vali Loss: 0.4607680 Test Loss: 0.3770347
Validation loss decreased (inf --> 0.460768).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6325674
	speed: 0.3433s/iter; left time: 1950.8456s
Epoch: 2 cost time: 15.765476703643799
Epoch: 2, Steps: 118 | Train Loss: 0.6653347 Vali Loss: 0.4208545 Test Loss: 0.3654255
Validation loss decreased (0.460768 --> 0.420855).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6928002
	speed: 0.3536s/iter; left time: 1967.6167s
Epoch: 3 cost time: 17.096150875091553
Epoch: 3, Steps: 118 | Train Loss: 0.6433442 Vali Loss: 0.4079971 Test Loss: 0.3618267
Validation loss decreased (0.420855 --> 0.407997).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5138511
	speed: 0.3477s/iter; left time: 1893.8021s
Epoch: 4 cost time: 16.484625101089478
Epoch: 4, Steps: 118 | Train Loss: 0.6345889 Vali Loss: 0.4016917 Test Loss: 0.3600761
Validation loss decreased (0.407997 --> 0.401692).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.8732764
	speed: 0.3076s/iter; left time: 1638.9840s
Epoch: 5 cost time: 12.949977159500122
Epoch: 5, Steps: 118 | Train Loss: 0.6292301 Vali Loss: 0.3967710 Test Loss: 0.3599764
Validation loss decreased (0.401692 --> 0.396771).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.7067703
	speed: 0.2891s/iter; left time: 1506.7047s
Epoch: 6 cost time: 15.091266632080078
Epoch: 6, Steps: 118 | Train Loss: 0.6252459 Vali Loss: 0.3925745 Test Loss: 0.3595476
Validation loss decreased (0.396771 --> 0.392575).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5150788
	speed: 0.3375s/iter; left time: 1718.6554s
Epoch: 7 cost time: 15.920398235321045
Epoch: 7, Steps: 118 | Train Loss: 0.6234424 Vali Loss: 0.3911577 Test Loss: 0.3591739
Validation loss decreased (0.392575 --> 0.391158).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5537165
	speed: 0.3527s/iter; left time: 1754.7993s
Epoch: 8 cost time: 17.27205753326416
Epoch: 8, Steps: 118 | Train Loss: 0.6211642 Vali Loss: 0.3887456 Test Loss: 0.3591043
Validation loss decreased (0.391158 --> 0.388746).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6769504
	speed: 0.3575s/iter; left time: 1736.3411s
Epoch: 9 cost time: 16.61481738090515
Epoch: 9, Steps: 118 | Train Loss: 0.6187892 Vali Loss: 0.3877539 Test Loss: 0.3590930
Validation loss decreased (0.388746 --> 0.387754).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5008869
	speed: 0.3424s/iter; left time: 1622.5814s
Epoch: 10 cost time: 16.41219735145569
Epoch: 10, Steps: 118 | Train Loss: 0.6171745 Vali Loss: 0.3857933 Test Loss: 0.3586489
Validation loss decreased (0.387754 --> 0.385793).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6633382
	speed: 0.3661s/iter; left time: 1691.7028s
Epoch: 11 cost time: 17.210339546203613
Epoch: 11, Steps: 118 | Train Loss: 0.6183360 Vali Loss: 0.3846067 Test Loss: 0.3585998
Validation loss decreased (0.385793 --> 0.384607).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.7061661
	speed: 0.3445s/iter; left time: 1551.1586s
Epoch: 12 cost time: 15.872329235076904
Epoch: 12, Steps: 118 | Train Loss: 0.6160882 Vali Loss: 0.3840337 Test Loss: 0.3588597
Validation loss decreased (0.384607 --> 0.384034).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4587939
	speed: 0.3197s/iter; left time: 1401.7497s
Epoch: 13 cost time: 14.059412002563477
Epoch: 13, Steps: 118 | Train Loss: 0.6171190 Vali Loss: 0.3825265 Test Loss: 0.3587872
Validation loss decreased (0.384034 --> 0.382526).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6397257
	speed: 0.2733s/iter; left time: 1166.2872s
Epoch: 14 cost time: 13.524601697921753
Epoch: 14, Steps: 118 | Train Loss: 0.6148801 Vali Loss: 0.3820476 Test Loss: 0.3587432
Validation loss decreased (0.382526 --> 0.382048).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.6536371
	speed: 0.2760s/iter; left time: 1144.9810s
Epoch: 15 cost time: 12.62509560585022
Epoch: 15, Steps: 118 | Train Loss: 0.6145547 Vali Loss: 0.3830675 Test Loss: 0.3586395
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5358120
	speed: 0.3723s/iter; left time: 1500.8240s
Epoch: 16 cost time: 18.229345560073853
Epoch: 16, Steps: 118 | Train Loss: 0.6153894 Vali Loss: 0.3817884 Test Loss: 0.3585809
Validation loss decreased (0.382048 --> 0.381788).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5561998
	speed: 0.3772s/iter; left time: 1476.1069s
Epoch: 17 cost time: 17.01628613471985
Epoch: 17, Steps: 118 | Train Loss: 0.6132614 Vali Loss: 0.3834637 Test Loss: 0.3584757
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.8758754
	speed: 0.3726s/iter; left time: 1413.9261s
Epoch: 18 cost time: 18.17638897895813
Epoch: 18, Steps: 118 | Train Loss: 0.6135756 Vali Loss: 0.3799228 Test Loss: 0.3585487
Validation loss decreased (0.381788 --> 0.379923).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.7164326
	speed: 0.3752s/iter; left time: 1379.4698s
Epoch: 19 cost time: 17.500224828720093
Epoch: 19, Steps: 118 | Train Loss: 0.6138313 Vali Loss: 0.3797019 Test Loss: 0.3584237
Validation loss decreased (0.379923 --> 0.379702).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.8611438
	speed: 0.3784s/iter; left time: 1346.8439s
Epoch: 20 cost time: 17.71949553489685
Epoch: 20, Steps: 118 | Train Loss: 0.6131564 Vali Loss: 0.3804117 Test Loss: 0.3585459
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.7306453
	speed: 0.3234s/iter; left time: 1112.6924s
Epoch: 21 cost time: 12.42940354347229
Epoch: 21, Steps: 118 | Train Loss: 0.6134491 Vali Loss: 0.3806002 Test Loss: 0.3584908
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.6969062
	speed: 0.3058s/iter; left time: 1016.1432s
Epoch: 22 cost time: 17.037494659423828
Epoch: 22, Steps: 118 | Train Loss: 0.6129393 Vali Loss: 0.3801608 Test Loss: 0.3586573
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.35376352071762085, mae:0.39543822407722473, rse:0.4755498468875885, corr:[0.2593652  0.2632327  0.26115564 0.26286995 0.26206827 0.26035333
 0.26079983 0.26061544 0.25869235 0.2575858  0.25709137 0.25588205
 0.25468028 0.25372145 0.25284785 0.25222543 0.25197464 0.25125402
 0.2500257  0.24902721 0.24846204 0.24782115 0.24653578 0.24508473
 0.24358396 0.24225149 0.24103181 0.23982209 0.2386868  0.23783796
 0.23715775 0.23619755 0.23537827 0.23484288 0.23411427 0.2331029
 0.23248842 0.23203328 0.2306784  0.22923769 0.22903503 0.22910754
 0.22789717 0.2261     0.2252594  0.2249686  0.22363317 0.22153467
 0.2201044  0.21893205 0.21711414 0.21544753 0.2142867  0.21267408
 0.21091622 0.20977479 0.20833161 0.20629108 0.20513216 0.20491484
 0.20411023 0.20300472 0.2030195  0.20342809 0.2025548  0.20144723
 0.20108198 0.20088305 0.20010526 0.1995732  0.19968611 0.19953018
 0.19839534 0.19703485 0.1960351  0.19502695 0.19417162 0.19343351
 0.19253056 0.19185375 0.19208445 0.192243   0.19127902 0.19018325
 0.19021    0.19074026 0.1903916  0.1894977  0.18932877 0.18977472
 0.18965441 0.18890901 0.18876562 0.18915823 0.18927151 0.1886545
 0.1878056  0.18736975 0.1873659  0.18701792 0.18625031 0.18570831
 0.18579057 0.18521664 0.18357773 0.18221064 0.18268146 0.1835418
 0.18288206 0.18184829 0.18182908 0.18194167 0.18058391 0.17921948
 0.17941971 0.18006518 0.17928039 0.17732687 0.17622495 0.1759384
 0.17529821 0.17391978 0.17237537 0.17102031 0.17027222 0.17022876
 0.1698953  0.16859752 0.16710743 0.1664371  0.1663974  0.16600193
 0.16516407 0.16425109 0.16376735 0.16357562 0.16323045 0.16288848
 0.1625588  0.16174693 0.16012017 0.15874526 0.15879883 0.15884139
 0.1569452  0.15448418 0.1537955  0.15387541 0.1523444  0.15038437
 0.15059337 0.15150931 0.1503524  0.14792882 0.147609   0.14882165
 0.14847864 0.14672758 0.14631982 0.14701238 0.14636382 0.14465834
 0.14350045 0.14328194 0.1433879  0.14354897 0.14353067 0.14265451
 0.14116868 0.13960218 0.13856071 0.13816279 0.13818456 0.13737091
 0.13556695 0.13360493 0.13281938 0.13258901 0.1317872  0.13115057
 0.13078338 0.13003883 0.12940694 0.13011724 0.13089016 0.1301535
 0.12831216 0.12763627 0.12868172 0.12961042 0.12976806 0.12918149
 0.12835175 0.12776318 0.12757911 0.12741728 0.12650798 0.12562728
 0.1258425  0.12603575 0.12523481 0.12431285 0.12444582 0.12433057
 0.12279389 0.12161653 0.12214089 0.12348792 0.12390205 0.12363841
 0.12349432 0.12373684 0.12404978 0.12381041 0.12259795 0.12107857
 0.12071549 0.12110972 0.12064926 0.11975115 0.11978691 0.12073649
 0.12065078 0.1202286  0.12029871 0.12029251 0.11936621 0.11822952
 0.11741988 0.11628009 0.114839   0.11493089 0.11616337 0.11700291
 0.11700886 0.11717413 0.11745948 0.11755849 0.11783299 0.11827403
 0.11812767 0.11768982 0.11796816 0.11848501 0.11791953 0.11716993
 0.11743452 0.11715134 0.11583486 0.11505103 0.11556228 0.11610554
 0.11579354 0.11605988 0.11675193 0.11815995 0.11986811 0.12136193
 0.12093297 0.12036549 0.12230224 0.12477559 0.12473915 0.12321606
 0.12392572 0.12520856 0.12418997 0.12271138 0.12362045 0.12489018
 0.12362745 0.1225066  0.12413105 0.12524217 0.12436557 0.12335935
 0.12378844 0.12342992 0.12200124 0.12222701 0.12429249 0.12541746
 0.12503617 0.12448321 0.12345587 0.12230645 0.12259384 0.12390809
 0.12415788 0.12328352 0.12253846 0.12178776 0.12056167 0.11998926
 0.12042785 0.11971094 0.11825608 0.11840476 0.11967222 0.11903036
 0.11747944 0.11816587 0.11915449 0.11902238 0.1194168  0.12184791
 0.12193738 0.1200277  0.11993583 0.1217188  0.12157743 0.11999551
 0.12007387 0.11998632 0.11823782 0.11700627 0.1175461  0.11722936
 0.11611449 0.11708059 0.1181558  0.11783244 0.11799128 0.1202238
 0.12013715 0.11800024 0.11859636 0.1208894  0.12077178 0.12067934
 0.12271208 0.12249812 0.12104622 0.12385426 0.12606788 0.11759591]
