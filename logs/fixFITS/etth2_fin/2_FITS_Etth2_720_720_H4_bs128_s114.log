Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  64354304.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.0204267501831055
Epoch: 1, Steps: 28 | Train Loss: 0.8978339 Vali Loss: 0.8823162 Test Loss: 0.5283867
Validation loss decreased (inf --> 0.882316).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.581578493118286
Epoch: 2, Steps: 28 | Train Loss: 0.7813206 Vali Loss: 0.8305630 Test Loss: 0.4922425
Validation loss decreased (0.882316 --> 0.830563).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.8523783683776855
Epoch: 3, Steps: 28 | Train Loss: 0.7063030 Vali Loss: 0.7962284 Test Loss: 0.4684431
Validation loss decreased (0.830563 --> 0.796228).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.083926677703857
Epoch: 4, Steps: 28 | Train Loss: 0.6565616 Vali Loss: 0.7719828 Test Loss: 0.4530475
Validation loss decreased (0.796228 --> 0.771983).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.3897864818573
Epoch: 5, Steps: 28 | Train Loss: 0.6222986 Vali Loss: 0.7596314 Test Loss: 0.4426920
Validation loss decreased (0.771983 --> 0.759631).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.553689479827881
Epoch: 6, Steps: 28 | Train Loss: 0.5981962 Vali Loss: 0.7475074 Test Loss: 0.4359249
Validation loss decreased (0.759631 --> 0.747507).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.314044952392578
Epoch: 7, Steps: 28 | Train Loss: 0.5806966 Vali Loss: 0.7394611 Test Loss: 0.4311619
Validation loss decreased (0.747507 --> 0.739461).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.401423215866089
Epoch: 8, Steps: 28 | Train Loss: 0.5669751 Vali Loss: 0.7339796 Test Loss: 0.4278524
Validation loss decreased (0.739461 --> 0.733980).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.710100173950195
Epoch: 9, Steps: 28 | Train Loss: 0.5556346 Vali Loss: 0.7294613 Test Loss: 0.4253553
Validation loss decreased (0.733980 --> 0.729461).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.330577373504639
Epoch: 10, Steps: 28 | Train Loss: 0.5477169 Vali Loss: 0.7207004 Test Loss: 0.4235182
Validation loss decreased (0.729461 --> 0.720700).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.5928285121917725
Epoch: 11, Steps: 28 | Train Loss: 0.5397173 Vali Loss: 0.7187419 Test Loss: 0.4220591
Validation loss decreased (0.720700 --> 0.718742).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.7459776401519775
Epoch: 12, Steps: 28 | Train Loss: 0.5337176 Vali Loss: 0.7164453 Test Loss: 0.4208771
Validation loss decreased (0.718742 --> 0.716445).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 6.164491176605225
Epoch: 13, Steps: 28 | Train Loss: 0.5275532 Vali Loss: 0.7082170 Test Loss: 0.4199293
Validation loss decreased (0.716445 --> 0.708217).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 6.0118091106414795
Epoch: 14, Steps: 28 | Train Loss: 0.5227125 Vali Loss: 0.7085277 Test Loss: 0.4190651
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.809704780578613
Epoch: 15, Steps: 28 | Train Loss: 0.5191392 Vali Loss: 0.7046971 Test Loss: 0.4183051
Validation loss decreased (0.708217 --> 0.704697).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 6.003751039505005
Epoch: 16, Steps: 28 | Train Loss: 0.5143915 Vali Loss: 0.7041110 Test Loss: 0.4176861
Validation loss decreased (0.704697 --> 0.704111).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.5888495445251465
Epoch: 17, Steps: 28 | Train Loss: 0.5113174 Vali Loss: 0.7047567 Test Loss: 0.4171191
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 6.410789251327515
Epoch: 18, Steps: 28 | Train Loss: 0.5083386 Vali Loss: 0.7029253 Test Loss: 0.4165479
Validation loss decreased (0.704111 --> 0.702925).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.370287656784058
Epoch: 19, Steps: 28 | Train Loss: 0.5050034 Vali Loss: 0.7021923 Test Loss: 0.4160996
Validation loss decreased (0.702925 --> 0.702192).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.524303913116455
Epoch: 20, Steps: 28 | Train Loss: 0.5022712 Vali Loss: 0.7009861 Test Loss: 0.4156306
Validation loss decreased (0.702192 --> 0.700986).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.349054574966431
Epoch: 21, Steps: 28 | Train Loss: 0.4995405 Vali Loss: 0.7003810 Test Loss: 0.4152080
Validation loss decreased (0.700986 --> 0.700381).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.715359210968018
Epoch: 22, Steps: 28 | Train Loss: 0.4978372 Vali Loss: 0.7006326 Test Loss: 0.4148526
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 5.17672324180603
Epoch: 23, Steps: 28 | Train Loss: 0.4959571 Vali Loss: 0.6989822 Test Loss: 0.4144604
Validation loss decreased (0.700381 --> 0.698982).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 5.679471969604492
Epoch: 24, Steps: 28 | Train Loss: 0.4935368 Vali Loss: 0.6940352 Test Loss: 0.4141467
Validation loss decreased (0.698982 --> 0.694035).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 5.511069297790527
Epoch: 25, Steps: 28 | Train Loss: 0.4911325 Vali Loss: 0.6903256 Test Loss: 0.4138366
Validation loss decreased (0.694035 --> 0.690326).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.7217607498168945
Epoch: 26, Steps: 28 | Train Loss: 0.4900739 Vali Loss: 0.6907414 Test Loss: 0.4135543
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.5055577754974365
Epoch: 27, Steps: 28 | Train Loss: 0.4885555 Vali Loss: 0.6965681 Test Loss: 0.4132773
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.397261619567871
Epoch: 28, Steps: 28 | Train Loss: 0.4867651 Vali Loss: 0.6917115 Test Loss: 0.4130017
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  64354304.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.410315990447998
Epoch: 1, Steps: 28 | Train Loss: 0.8416491 Vali Loss: 0.6900004 Test Loss: 0.4092950
Validation loss decreased (inf --> 0.690000).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.650101661682129
Epoch: 2, Steps: 28 | Train Loss: 0.8363065 Vali Loss: 0.6811652 Test Loss: 0.4058475
Validation loss decreased (0.690000 --> 0.681165).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.562315225601196
Epoch: 3, Steps: 28 | Train Loss: 0.8299745 Vali Loss: 0.6783409 Test Loss: 0.4033138
Validation loss decreased (0.681165 --> 0.678341).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.656614780426025
Epoch: 4, Steps: 28 | Train Loss: 0.8262679 Vali Loss: 0.6731077 Test Loss: 0.4013320
Validation loss decreased (0.678341 --> 0.673108).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.82562518119812
Epoch: 5, Steps: 28 | Train Loss: 0.8223921 Vali Loss: 0.6698272 Test Loss: 0.3997227
Validation loss decreased (0.673108 --> 0.669827).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 6.05220890045166
Epoch: 6, Steps: 28 | Train Loss: 0.8189282 Vali Loss: 0.6658684 Test Loss: 0.3984038
Validation loss decreased (0.669827 --> 0.665868).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.341493368148804
Epoch: 7, Steps: 28 | Train Loss: 0.8162107 Vali Loss: 0.6664975 Test Loss: 0.3973411
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.1173906326293945
Epoch: 8, Steps: 28 | Train Loss: 0.8147288 Vali Loss: 0.6658376 Test Loss: 0.3965111
Validation loss decreased (0.665868 --> 0.665838).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.6491899490356445
Epoch: 9, Steps: 28 | Train Loss: 0.8126700 Vali Loss: 0.6619611 Test Loss: 0.3957043
Validation loss decreased (0.665838 --> 0.661961).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.646860599517822
Epoch: 10, Steps: 28 | Train Loss: 0.8109581 Vali Loss: 0.6629729 Test Loss: 0.3950832
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.717680931091309
Epoch: 11, Steps: 28 | Train Loss: 0.8108995 Vali Loss: 0.6589326 Test Loss: 0.3945849
Validation loss decreased (0.661961 --> 0.658933).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.687742233276367
Epoch: 12, Steps: 28 | Train Loss: 0.8086735 Vali Loss: 0.6583483 Test Loss: 0.3941138
Validation loss decreased (0.658933 --> 0.658348).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.922434329986572
Epoch: 13, Steps: 28 | Train Loss: 0.8092700 Vali Loss: 0.6597762 Test Loss: 0.3937219
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.983556747436523
Epoch: 14, Steps: 28 | Train Loss: 0.8078282 Vali Loss: 0.6576660 Test Loss: 0.3933137
Validation loss decreased (0.658348 --> 0.657666).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.899786710739136
Epoch: 15, Steps: 28 | Train Loss: 0.8077584 Vali Loss: 0.6566332 Test Loss: 0.3930885
Validation loss decreased (0.657666 --> 0.656633).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.778724908828735
Epoch: 16, Steps: 28 | Train Loss: 0.8069347 Vali Loss: 0.6518496 Test Loss: 0.3928298
Validation loss decreased (0.656633 --> 0.651850).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.928275108337402
Epoch: 17, Steps: 28 | Train Loss: 0.8054449 Vali Loss: 0.6550009 Test Loss: 0.3925659
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.8400561809539795
Epoch: 18, Steps: 28 | Train Loss: 0.8053791 Vali Loss: 0.6550814 Test Loss: 0.3924064
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.839008092880249
Epoch: 19, Steps: 28 | Train Loss: 0.8054412 Vali Loss: 0.6516556 Test Loss: 0.3922170
Validation loss decreased (0.651850 --> 0.651656).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.770231008529663
Epoch: 20, Steps: 28 | Train Loss: 0.8033173 Vali Loss: 0.6521924 Test Loss: 0.3920769
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.440647602081299
Epoch: 21, Steps: 28 | Train Loss: 0.8049440 Vali Loss: 0.6502442 Test Loss: 0.3919080
Validation loss decreased (0.651656 --> 0.650244).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.07172155380249
Epoch: 22, Steps: 28 | Train Loss: 0.8047939 Vali Loss: 0.6540366 Test Loss: 0.3917927
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.4266021251678467
Epoch: 23, Steps: 28 | Train Loss: 0.8037465 Vali Loss: 0.6555409 Test Loss: 0.3917001
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.039953231811523
Epoch: 24, Steps: 28 | Train Loss: 0.8043694 Vali Loss: 0.6490021 Test Loss: 0.3916038
Validation loss decreased (0.650244 --> 0.649002).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.414343595504761
Epoch: 25, Steps: 28 | Train Loss: 0.8038129 Vali Loss: 0.6438279 Test Loss: 0.3915611
Validation loss decreased (0.649002 --> 0.643828).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.772989511489868
Epoch: 26, Steps: 28 | Train Loss: 0.8045206 Vali Loss: 0.6520701 Test Loss: 0.3914925
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.645277976989746
Epoch: 27, Steps: 28 | Train Loss: 0.8035619 Vali Loss: 0.6482929 Test Loss: 0.3914105
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.540644645690918
Epoch: 28, Steps: 28 | Train Loss: 0.8040721 Vali Loss: 0.6483738 Test Loss: 0.3913456
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3787359595298767, mae:0.4235392212867737, rse:0.4918968081474304, corr:[ 2.16585517e-01  2.20815256e-01  2.20240906e-01  2.18674362e-01
  2.18291014e-01  2.18689606e-01  2.18635589e-01  2.17452839e-01
  2.16034278e-01  2.14747459e-01  2.13782474e-01  2.12692425e-01
  2.11427361e-01  2.10228592e-01  2.09200591e-01  2.08395168e-01
  2.07548410e-01  2.06530631e-01  2.05512732e-01  2.04730690e-01
  2.04027310e-01  2.03140736e-01  2.01876998e-01  2.00298369e-01
  1.98891580e-01  1.97867870e-01  1.97073117e-01  1.96180165e-01
  1.95240051e-01  1.94292605e-01  1.93488523e-01  1.92750067e-01
  1.91970959e-01  1.91081181e-01  1.90147266e-01  1.89115345e-01
  1.87989548e-01  1.87057629e-01  1.86491087e-01  1.86093539e-01
  1.85645655e-01  1.84919894e-01  1.83945522e-01  1.83138117e-01
  1.82673588e-01  1.82316273e-01  1.81686804e-01  1.80273071e-01
  1.78470567e-01  1.76881254e-01  1.75878003e-01  1.75233185e-01
  1.74793646e-01  1.74288303e-01  1.73355594e-01  1.72329754e-01
  1.71542123e-01  1.71107307e-01  1.70888275e-01  1.70743674e-01
  1.70528650e-01  1.70062825e-01  1.69720501e-01  1.69897616e-01
  1.70118734e-01  1.70513496e-01  1.70782894e-01  1.70786798e-01
  1.70587748e-01  1.70405552e-01  1.70233443e-01  1.69960737e-01
  1.69543266e-01  1.69087023e-01  1.68816179e-01  1.68505341e-01
  1.68288440e-01  1.68071076e-01  1.67755321e-01  1.67385265e-01
  1.67355880e-01  1.67496651e-01  1.67432576e-01  1.67168990e-01
  1.66935444e-01  1.67000160e-01  1.67295262e-01  1.67632028e-01
  1.67672321e-01  1.67507604e-01  1.67287037e-01  1.67133883e-01
  1.67450726e-01  1.67889535e-01  1.68183118e-01  1.68097585e-01
  1.67751715e-01  1.67442992e-01  1.67240098e-01  1.67128637e-01
  1.67134002e-01  1.66965917e-01  1.66788325e-01  1.66464329e-01
  1.66224152e-01  1.66022256e-01  1.66057408e-01  1.66319251e-01
  1.66380644e-01  1.66144907e-01  1.65717468e-01  1.65363759e-01
  1.65153310e-01  1.65260181e-01  1.65492594e-01  1.65573761e-01
  1.65345863e-01  1.64685264e-01  1.64027914e-01  1.63416460e-01
  1.62837565e-01  1.62158221e-01  1.61435485e-01  1.60773203e-01
  1.60061911e-01  1.59401029e-01  1.58805534e-01  1.58310100e-01
  1.57993615e-01  1.57656699e-01  1.57332748e-01  1.56653881e-01
  1.55736327e-01  1.54772416e-01  1.54189318e-01  1.54151589e-01
  1.53964743e-01  1.53433770e-01  1.52628884e-01  1.51999369e-01
  1.52054206e-01  1.52356252e-01  1.52282164e-01  1.51310682e-01
  1.49509519e-01  1.47992715e-01  1.47338226e-01  1.47286668e-01
  1.47102252e-01  1.46499559e-01  1.45744637e-01  1.45146504e-01
  1.45045474e-01  1.45084962e-01  1.44731551e-01  1.43824607e-01
  1.42827585e-01  1.42237172e-01  1.42057419e-01  1.41899437e-01
  1.41365483e-01  1.40952647e-01  1.41001061e-01  1.41463265e-01
  1.42097980e-01  1.42297179e-01  1.41983688e-01  1.41205028e-01
  1.40549928e-01  1.40201971e-01  1.40043989e-01  1.39744997e-01
  1.39258623e-01  1.38652802e-01  1.38070658e-01  1.37512743e-01
  1.36996239e-01  1.36429951e-01  1.35593206e-01  1.34727105e-01
  1.34100810e-01  1.33853421e-01  1.34005710e-01  1.34228617e-01
  1.34302005e-01  1.34337530e-01  1.34310991e-01  1.34388432e-01
  1.34776831e-01  1.35447487e-01  1.36329755e-01  1.36744633e-01
  1.36476785e-01  1.36082143e-01  1.35982841e-01  1.36414096e-01
  1.37024358e-01  1.37166351e-01  1.36926129e-01  1.36407629e-01
  1.36090100e-01  1.36063740e-01  1.36253193e-01  1.36057004e-01
  1.35521799e-01  1.35193482e-01  1.35220528e-01  1.35557771e-01
  1.35985985e-01  1.36281237e-01  1.36406213e-01  1.36557907e-01
  1.36961296e-01  1.37415826e-01  1.37689278e-01  1.37543052e-01
  1.37154102e-01  1.36631921e-01  1.36040211e-01  1.35681018e-01
  1.35428444e-01  1.35596111e-01  1.35769859e-01  1.36041626e-01
  1.36041388e-01  1.35823756e-01  1.35687798e-01  1.35694534e-01
  1.35916233e-01  1.36112541e-01  1.36318818e-01  1.36699706e-01
  1.37237549e-01  1.37853071e-01  1.38402075e-01  1.38813049e-01
  1.39296576e-01  1.39938831e-01  1.40754551e-01  1.41508579e-01
  1.41940475e-01  1.41962036e-01  1.41899168e-01  1.42162666e-01
  1.42481431e-01  1.42526701e-01  1.42462760e-01  1.42464742e-01
  1.42901361e-01  1.43729404e-01  1.44610494e-01  1.45272106e-01
  1.45577192e-01  1.45834267e-01  1.46211013e-01  1.47016108e-01
  1.47840127e-01  1.48561448e-01  1.49142280e-01  1.49782509e-01
  1.50495842e-01  1.51221544e-01  1.52049080e-01  1.52677581e-01
  1.53205708e-01  1.53570011e-01  1.53892338e-01  1.54285803e-01
  1.54513597e-01  1.54759422e-01  1.55032501e-01  1.55301660e-01
  1.55599073e-01  1.55901402e-01  1.56262219e-01  1.56603768e-01
  1.56763002e-01  1.57092392e-01  1.57581985e-01  1.58334836e-01
  1.59271061e-01  1.60090923e-01  1.60474986e-01  1.60541669e-01
  1.60553277e-01  1.60828859e-01  1.61525786e-01  1.62192360e-01
  1.62537113e-01  1.62578076e-01  1.62450999e-01  1.62433177e-01
  1.62314683e-01  1.62048578e-01  1.61833033e-01  1.61876842e-01
  1.62240878e-01  1.62748948e-01  1.63113460e-01  1.63199395e-01
  1.62924930e-01  1.62842825e-01  1.63186625e-01  1.63824797e-01
  1.64333537e-01  1.64714366e-01  1.64757192e-01  1.64904267e-01
  1.65245503e-01  1.65769637e-01  1.66314200e-01  1.66537046e-01
  1.66566119e-01  1.66256219e-01  1.65940449e-01  1.65538236e-01
  1.65202901e-01  1.64990783e-01  1.64871410e-01  1.64820939e-01
  1.64602637e-01  1.64475515e-01  1.64404958e-01  1.64424911e-01
  1.64308608e-01  1.64152548e-01  1.64435595e-01  1.64886847e-01
  1.65297940e-01  1.65484846e-01  1.65502086e-01  1.65854514e-01
  1.66581005e-01  1.67465448e-01  1.68282613e-01  1.68631583e-01
  1.68426096e-01  1.68194443e-01  1.68350577e-01  1.68648884e-01
  1.68899164e-01  1.68819517e-01  1.68629870e-01  1.68486670e-01
  1.68582976e-01  1.68957263e-01  1.69196501e-01  1.69534311e-01
  1.69729859e-01  1.69930592e-01  1.70303032e-01  1.70901045e-01
  1.71555147e-01  1.72233775e-01  1.72750428e-01  1.73261240e-01
  1.73706725e-01  1.74120963e-01  1.74472257e-01  1.74734086e-01
  1.74877852e-01  1.74938634e-01  1.75134793e-01  1.75467700e-01
  1.75948501e-01  1.76376536e-01  1.76643640e-01  1.76779270e-01
  1.76749051e-01  1.76970363e-01  1.77173808e-01  1.77269086e-01
  1.77117035e-01  1.76960573e-01  1.77030236e-01  1.77326903e-01
  1.77671298e-01  1.77780300e-01  1.77548274e-01  1.77334577e-01
  1.77349702e-01  1.77496240e-01  1.77571610e-01  1.77576438e-01
  1.77478760e-01  1.77352220e-01  1.77301854e-01  1.77345514e-01
  1.77495018e-01  1.77793592e-01  1.78291321e-01  1.78894669e-01
  1.79276332e-01  1.79268226e-01  1.79102346e-01  1.79020807e-01
  1.79277837e-01  1.79347053e-01  1.79183990e-01  1.78797930e-01
  1.78234071e-01  1.77914217e-01  1.77939638e-01  1.78171262e-01
  1.78214967e-01  1.77972823e-01  1.77629277e-01  1.77502722e-01
  1.77473411e-01  1.77381173e-01  1.77059680e-01  1.76533237e-01
  1.76112682e-01  1.75916731e-01  1.75874233e-01  1.75969422e-01
  1.76089093e-01  1.76147699e-01  1.76031753e-01  1.75752163e-01
  1.75041124e-01  1.74127549e-01  1.73263967e-01  1.72581300e-01
  1.71862692e-01  1.70831814e-01  1.69768974e-01  1.68764338e-01
  1.68053225e-01  1.67623386e-01  1.67070270e-01  1.66393816e-01
  1.65576532e-01  1.64876759e-01  1.64579600e-01  1.64300844e-01
  1.63767874e-01  1.63014740e-01  1.62291661e-01  1.61942273e-01
  1.61586374e-01  1.61263794e-01  1.60697833e-01  1.60074294e-01
  1.59428269e-01  1.59113944e-01  1.58810481e-01  1.58468544e-01
  1.57965064e-01  1.57476202e-01  1.57186404e-01  1.57194078e-01
  1.56989276e-01  1.56591862e-01  1.56219721e-01  1.56062737e-01
  1.56120226e-01  1.56026363e-01  1.55660495e-01  1.55392200e-01
  1.55192003e-01  1.54861376e-01  1.54347137e-01  1.53879821e-01
  1.53411046e-01  1.53365299e-01  1.53525323e-01  1.53289095e-01
  1.52611867e-01  1.51621476e-01  1.51031956e-01  1.50991052e-01
  1.51239932e-01  1.51183710e-01  1.50652751e-01  1.49811983e-01
  1.49133638e-01  1.49083689e-01  1.49381191e-01  1.49485886e-01
  1.48981541e-01  1.48046151e-01  1.47142336e-01  1.46236420e-01
  1.45521283e-01  1.44930348e-01  1.44377261e-01  1.43699273e-01
  1.43041670e-01  1.42348304e-01  1.41671196e-01  1.40834525e-01
  1.39959529e-01  1.39292330e-01  1.38982743e-01  1.38795063e-01
  1.38282299e-01  1.37676746e-01  1.37169838e-01  1.36818275e-01
  1.36562347e-01  1.36167660e-01  1.35590121e-01  1.34593263e-01
  1.33288994e-01  1.32099852e-01  1.31244093e-01  1.30570814e-01
  1.29986957e-01  1.29528135e-01  1.28882378e-01  1.28133938e-01
  1.27423748e-01  1.26875535e-01  1.26484245e-01  1.25792816e-01
  1.24705687e-01  1.23435467e-01  1.22601487e-01  1.22344956e-01
  1.22455224e-01  1.22543029e-01  1.22000933e-01  1.20973669e-01
  1.19957089e-01  1.19394027e-01  1.19024411e-01  1.18308790e-01
  1.16906643e-01  1.15240201e-01  1.13870233e-01  1.12627029e-01
  1.11621112e-01  1.10721812e-01  1.09898783e-01  1.09043442e-01
  1.08072504e-01  1.07040606e-01  1.06049240e-01  1.05093524e-01
  1.04130149e-01  1.03044450e-01  1.01920694e-01  1.00721024e-01
  9.94181708e-02  9.84853059e-02  9.78638977e-02  9.76333097e-02
  9.75485519e-02  9.69823748e-02  9.60179865e-02  9.46770757e-02
  9.33878273e-02  9.22532156e-02  9.12769139e-02  9.03527215e-02
  8.94718170e-02  8.86357650e-02  8.76952335e-02  8.64802375e-02
  8.55701417e-02  8.50559697e-02  8.51087943e-02  8.52255821e-02
  8.50097239e-02  8.42578560e-02  8.32911059e-02  8.24152902e-02
  8.21320340e-02  8.20674300e-02  8.15786794e-02  8.05506557e-02
  7.93099850e-02  7.84002990e-02  7.76347592e-02  7.65991732e-02
  7.49415830e-02  7.32402354e-02  7.19048604e-02  7.10756704e-02
  7.05054402e-02  6.96836337e-02  6.84808567e-02  6.71960637e-02
  6.62752539e-02  6.58325255e-02  6.54043704e-02  6.47659227e-02
  6.37052134e-02  6.27457574e-02  6.23119995e-02  6.22155070e-02
  6.20612688e-02  6.15523867e-02  6.06996492e-02  6.00982569e-02
  5.98781481e-02  5.97370528e-02  5.90028800e-02  5.75792342e-02
  5.58724254e-02  5.44959232e-02  5.33751994e-02  5.26360720e-02
  5.17295524e-02  5.06842360e-02  4.93535586e-02  4.79183532e-02
  4.65736538e-02  4.57868911e-02  4.55637053e-02  4.51088995e-02
  4.42013629e-02  4.30106670e-02  4.21271883e-02  4.19153050e-02
  4.22512814e-02  4.29992825e-02  4.33354266e-02  4.30587903e-02
  4.23375256e-02  4.18957844e-02  4.16671894e-02  4.16635796e-02
  4.11695950e-02  3.98870409e-02  3.82545292e-02  3.69651802e-02
  3.63906696e-02  3.64754088e-02  3.65713984e-02  3.62976007e-02
  3.59054022e-02  3.57218347e-02  3.54352482e-02  3.49635631e-02
  3.42370421e-02  3.35334763e-02  3.33998092e-02  3.36425714e-02
  3.31616141e-02  3.21267508e-02  3.10916062e-02  3.05776708e-02
  3.09858918e-02  3.18040140e-02  3.16217951e-02  3.01696025e-02
  2.82168332e-02  2.69822311e-02  2.68155448e-02  2.69134212e-02
  2.67557129e-02  2.58407723e-02  2.44297944e-02  2.32159756e-02
  2.27042027e-02  2.30269022e-02  2.38720272e-02  2.46286821e-02
  2.45404113e-02  2.36869734e-02  2.28642002e-02  2.24798229e-02
  2.28786767e-02  2.36594416e-02  2.42016092e-02  2.41012536e-02
  2.33184695e-02  2.27760468e-02  2.27634683e-02  2.29594465e-02
  2.26087365e-02  2.16078628e-02  2.04149317e-02  1.91635992e-02
  1.86197106e-02  1.83409024e-02  1.84653159e-02  1.82061885e-02
  1.81333460e-02  1.81834828e-02  1.78679768e-02  1.70574915e-02
  1.64041240e-02  1.59972757e-02  1.65376347e-02  1.73379537e-02
  1.74409505e-02  1.67440847e-02  1.59592163e-02  1.52980881e-02
  1.53657803e-02  1.53291011e-02  1.43038137e-02  1.18469680e-02
  8.98072217e-03  7.02139176e-03  6.91718515e-03  7.47764250e-03
  7.63512729e-03  6.43361500e-03  4.63231234e-03  2.88923783e-03
  2.61066691e-03  3.96750867e-03  5.40866284e-03  5.78398257e-03
  4.16470086e-03  9.47377295e-04 -7.79942086e-04  2.02950061e-04
  2.83299014e-03  3.97016248e-03  6.87111460e-04 -5.35551598e-03
 -8.91592540e-03 -6.03385922e-03 -2.04931526e-03 -8.41493905e-03]
