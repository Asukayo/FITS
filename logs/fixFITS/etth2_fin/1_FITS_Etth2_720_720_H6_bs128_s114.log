Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  137682944.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.913224220275879
Epoch: 1, Steps: 28 | Train Loss: 1.0965039 Vali Loss: 0.8280854 Test Loss: 0.4829170
Validation loss decreased (inf --> 0.828085).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.871669292449951
Epoch: 2, Steps: 28 | Train Loss: 0.9508211 Vali Loss: 0.7737110 Test Loss: 0.4420371
Validation loss decreased (0.828085 --> 0.773711).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.726345777511597
Epoch: 3, Steps: 28 | Train Loss: 0.8983575 Vali Loss: 0.7477352 Test Loss: 0.4266397
Validation loss decreased (0.773711 --> 0.747735).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.82384729385376
Epoch: 4, Steps: 28 | Train Loss: 0.8750976 Vali Loss: 0.7336226 Test Loss: 0.4189993
Validation loss decreased (0.747735 --> 0.733623).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.901590347290039
Epoch: 5, Steps: 28 | Train Loss: 0.8630087 Vali Loss: 0.7208850 Test Loss: 0.4141224
Validation loss decreased (0.733623 --> 0.720885).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.922800540924072
Epoch: 6, Steps: 28 | Train Loss: 0.8548850 Vali Loss: 0.7105067 Test Loss: 0.4107437
Validation loss decreased (0.720885 --> 0.710507).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.833507061004639
Epoch: 7, Steps: 28 | Train Loss: 0.8485043 Vali Loss: 0.7055165 Test Loss: 0.4082854
Validation loss decreased (0.710507 --> 0.705516).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.119119644165039
Epoch: 8, Steps: 28 | Train Loss: 0.8430361 Vali Loss: 0.6995025 Test Loss: 0.4062448
Validation loss decreased (0.705516 --> 0.699502).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.120452642440796
Epoch: 9, Steps: 28 | Train Loss: 0.8398229 Vali Loss: 0.6958031 Test Loss: 0.4045188
Validation loss decreased (0.699502 --> 0.695803).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.659566402435303
Epoch: 10, Steps: 28 | Train Loss: 0.8369413 Vali Loss: 0.6904681 Test Loss: 0.4031288
Validation loss decreased (0.695803 --> 0.690468).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.854604959487915
Epoch: 11, Steps: 28 | Train Loss: 0.8334594 Vali Loss: 0.6882108 Test Loss: 0.4019419
Validation loss decreased (0.690468 --> 0.688211).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.494380712509155
Epoch: 12, Steps: 28 | Train Loss: 0.8292951 Vali Loss: 0.6875101 Test Loss: 0.4008475
Validation loss decreased (0.688211 --> 0.687510).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.325607776641846
Epoch: 13, Steps: 28 | Train Loss: 0.8288007 Vali Loss: 0.6834831 Test Loss: 0.3999554
Validation loss decreased (0.687510 --> 0.683483).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.273763418197632
Epoch: 14, Steps: 28 | Train Loss: 0.8283023 Vali Loss: 0.6802951 Test Loss: 0.3992056
Validation loss decreased (0.683483 --> 0.680295).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.311414003372192
Epoch: 15, Steps: 28 | Train Loss: 0.8238927 Vali Loss: 0.6782705 Test Loss: 0.3984705
Validation loss decreased (0.680295 --> 0.678270).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.146934270858765
Epoch: 16, Steps: 28 | Train Loss: 0.8240891 Vali Loss: 0.6773862 Test Loss: 0.3977990
Validation loss decreased (0.678270 --> 0.677386).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.212032794952393
Epoch: 17, Steps: 28 | Train Loss: 0.8221278 Vali Loss: 0.6778267 Test Loss: 0.3972667
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.334836721420288
Epoch: 18, Steps: 28 | Train Loss: 0.8213830 Vali Loss: 0.6771162 Test Loss: 0.3967097
Validation loss decreased (0.677386 --> 0.677116).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.005053758621216
Epoch: 19, Steps: 28 | Train Loss: 0.8209125 Vali Loss: 0.6763099 Test Loss: 0.3962786
Validation loss decreased (0.677116 --> 0.676310).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.040029764175415
Epoch: 20, Steps: 28 | Train Loss: 0.8195252 Vali Loss: 0.6706593 Test Loss: 0.3958150
Validation loss decreased (0.676310 --> 0.670659).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.414654016494751
Epoch: 21, Steps: 28 | Train Loss: 0.8194546 Vali Loss: 0.6737110 Test Loss: 0.3955443
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.611342906951904
Epoch: 22, Steps: 28 | Train Loss: 0.8186237 Vali Loss: 0.6702834 Test Loss: 0.3951779
Validation loss decreased (0.670659 --> 0.670283).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 5.466321229934692
Epoch: 23, Steps: 28 | Train Loss: 0.8168229 Vali Loss: 0.6690050 Test Loss: 0.3948827
Validation loss decreased (0.670283 --> 0.669005).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 5.37590479850769
Epoch: 24, Steps: 28 | Train Loss: 0.8175414 Vali Loss: 0.6711669 Test Loss: 0.3945624
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 5.236324071884155
Epoch: 25, Steps: 28 | Train Loss: 0.8152345 Vali Loss: 0.6682996 Test Loss: 0.3942946
Validation loss decreased (0.669005 --> 0.668300).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 5.06346583366394
Epoch: 26, Steps: 28 | Train Loss: 0.8155997 Vali Loss: 0.6657794 Test Loss: 0.3940535
Validation loss decreased (0.668300 --> 0.665779).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 5.409077167510986
Epoch: 27, Steps: 28 | Train Loss: 0.8153843 Vali Loss: 0.6709962 Test Loss: 0.3938727
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 5.3933820724487305
Epoch: 28, Steps: 28 | Train Loss: 0.8159078 Vali Loss: 0.6660771 Test Loss: 0.3936925
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 5.563888788223267
Epoch: 29, Steps: 28 | Train Loss: 0.8138753 Vali Loss: 0.6657692 Test Loss: 0.3935415
Validation loss decreased (0.665779 --> 0.665769).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 5.345712661743164
Epoch: 30, Steps: 28 | Train Loss: 0.8123765 Vali Loss: 0.6668082 Test Loss: 0.3933524
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.415860652923584
Epoch: 31, Steps: 28 | Train Loss: 0.8135565 Vali Loss: 0.6668791 Test Loss: 0.3931914
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.149132966995239
Epoch: 32, Steps: 28 | Train Loss: 0.8131072 Vali Loss: 0.6636802 Test Loss: 0.3930595
Validation loss decreased (0.665769 --> 0.663680).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.9133570194244385
Epoch: 33, Steps: 28 | Train Loss: 0.8123234 Vali Loss: 0.6649766 Test Loss: 0.3929184
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.8780176639556885
Epoch: 34, Steps: 28 | Train Loss: 0.8117426 Vali Loss: 0.6661431 Test Loss: 0.3928123
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.205033779144287
Epoch: 35, Steps: 28 | Train Loss: 0.8109483 Vali Loss: 0.6665247 Test Loss: 0.3927024
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3803655803203583, mae:0.4255998432636261, rse:0.49295392632484436, corr:[ 2.14370891e-01  2.20043391e-01  2.17308447e-01  2.17361242e-01
  2.18258828e-01  2.17009634e-01  2.16041908e-01  2.15972960e-01
  2.14879394e-01  2.13216543e-01  2.11914375e-01  2.10727185e-01
  2.09379539e-01  2.08066389e-01  2.06891060e-01  2.05807433e-01
  2.05113471e-01  2.04442799e-01  2.03607053e-01  2.02559754e-01
  2.01716632e-01  2.01138422e-01  1.99755132e-01  1.98256493e-01
  1.97035342e-01  1.96381569e-01  1.95461780e-01  1.94439679e-01
  1.93770900e-01  1.93404272e-01  1.92895785e-01  1.91933319e-01
  1.91195220e-01  1.90440491e-01  1.89443350e-01  1.88195229e-01
  1.87223062e-01  1.86791226e-01  1.86082661e-01  1.85023710e-01
  1.84045553e-01  1.83586612e-01  1.83373690e-01  1.82688951e-01
  1.81496486e-01  1.80918455e-01  1.80455208e-01  1.79113999e-01
  1.77393928e-01  1.76211044e-01  1.75572798e-01  1.75139636e-01
  1.74566150e-01  1.73792779e-01  1.73371404e-01  1.73122078e-01
  1.72336042e-01  1.71311960e-01  1.70811132e-01  1.70653209e-01
  1.70163557e-01  1.69810861e-01  1.70226604e-01  1.70483485e-01
  1.70240253e-01  1.70245364e-01  1.70567587e-01  1.70600042e-01
  1.70224279e-01  1.70026377e-01  1.69921249e-01  1.69467941e-01
  1.68977067e-01  1.68780968e-01  1.68456554e-01  1.67846367e-01
  1.67664140e-01  1.67941108e-01  1.67788878e-01  1.67344660e-01
  1.67090237e-01  1.67015418e-01  1.66700929e-01  1.66328728e-01
  1.66425079e-01  1.66650489e-01  1.66485235e-01  1.66214556e-01
  1.66604713e-01  1.67022094e-01  1.66709453e-01  1.66395828e-01
  1.66645601e-01  1.66948602e-01  1.66926324e-01  1.66784301e-01
  1.66954368e-01  1.67120412e-01  1.66983545e-01  1.66760415e-01
  1.66567862e-01  1.66241884e-01  1.65997729e-01  1.65905908e-01
  1.65706798e-01  1.65344298e-01  1.65317699e-01  1.65391818e-01
  1.65121883e-01  1.64596498e-01  1.64496854e-01  1.64531335e-01
  1.64236099e-01  1.63799927e-01  1.63817480e-01  1.64062709e-01
  1.63616657e-01  1.62733018e-01  1.62398949e-01  1.62326664e-01
  1.61564678e-01  1.60596669e-01  1.60109326e-01  1.59769058e-01
  1.59107447e-01  1.58606306e-01  1.58475965e-01  1.58275753e-01
  1.57635316e-01  1.56779259e-01  1.56232923e-01  1.55822963e-01
  1.55252755e-01  1.54539853e-01  1.53920889e-01  1.53360128e-01
  1.52966827e-01  1.52840927e-01  1.52351379e-01  1.51750773e-01
  1.51622206e-01  1.51821166e-01  1.51342243e-01  1.49949551e-01
  1.48655757e-01  1.48083583e-01  1.47441864e-01  1.46591768e-01
  1.46333963e-01  1.46569863e-01  1.46277860e-01  1.45504192e-01
  1.45010725e-01  1.44621879e-01  1.43864304e-01  1.42915383e-01
  1.42403886e-01  1.42377764e-01  1.42201290e-01  1.41554356e-01
  1.40905082e-01  1.40907854e-01  1.41377464e-01  1.41835198e-01
  1.42093509e-01  1.42185614e-01  1.42178804e-01  1.41718954e-01
  1.40877813e-01  1.40142500e-01  1.40081257e-01  1.40235588e-01
  1.39862761e-01  1.39123663e-01  1.38620749e-01  1.38254911e-01
  1.37490481e-01  1.36486784e-01  1.35864243e-01  1.35805294e-01
  1.35610759e-01  1.35136187e-01  1.34745225e-01  1.34462148e-01
  1.34475604e-01  1.34801239e-01  1.35072380e-01  1.35288388e-01
  1.35549843e-01  1.36043400e-01  1.36450902e-01  1.36321127e-01
  1.36068106e-01  1.36199877e-01  1.36513919e-01  1.36732265e-01
  1.36914343e-01  1.37089327e-01  1.37157589e-01  1.36841774e-01
  1.36491090e-01  1.36387661e-01  1.36438236e-01  1.36264980e-01
  1.36128798e-01  1.36080161e-01  1.35966897e-01  1.36052608e-01
  1.36464745e-01  1.36984602e-01  1.37203932e-01  1.37340024e-01
  1.37513965e-01  1.37716144e-01  1.37704715e-01  1.37374893e-01
  1.36856586e-01  1.36391640e-01  1.36219159e-01  1.36053905e-01
  1.35618538e-01  1.35247737e-01  1.35100096e-01  1.35081828e-01
  1.34944975e-01  1.34972811e-01  1.35176405e-01  1.35154575e-01
  1.34928927e-01  1.34824917e-01  1.35058269e-01  1.35467887e-01
  1.35921225e-01  1.36383057e-01  1.36810839e-01  1.37361184e-01
  1.38056353e-01  1.38550460e-01  1.38652191e-01  1.38765916e-01
  1.39258206e-01  1.39640272e-01  1.39707923e-01  1.39831021e-01
  1.40147105e-01  1.40307263e-01  1.40052006e-01  1.39915437e-01
  1.40301302e-01  1.40792340e-01  1.40949592e-01  1.41140088e-01
  1.41775057e-01  1.42795250e-01  1.43735737e-01  1.44301996e-01
  1.44653648e-01  1.45308286e-01  1.46236420e-01  1.46886513e-01
  1.47117600e-01  1.47794157e-01  1.48828089e-01  1.49314538e-01
  1.49446115e-01  1.50032550e-01  1.50989935e-01  1.51149243e-01
  1.50658816e-01  1.50767952e-01  1.51470080e-01  1.51772231e-01
  1.51594251e-01  1.51916295e-01  1.52804270e-01  1.53363809e-01
  1.53310120e-01  1.53635263e-01  1.54543743e-01  1.55435264e-01
  1.56053677e-01  1.56631038e-01  1.57149613e-01  1.57436773e-01
  1.57609299e-01  1.58117965e-01  1.58821687e-01  1.59204692e-01
  1.59299091e-01  1.59367234e-01  1.59469500e-01  1.59529656e-01
  1.59635544e-01  1.59787595e-01  1.59697875e-01  1.59308165e-01
  1.59252092e-01  1.59912482e-01  1.60511076e-01  1.60396382e-01
  1.60087690e-01  1.60414159e-01  1.61043912e-01  1.61412001e-01
  1.61840379e-01  1.62548676e-01  1.62891477e-01  1.62833959e-01
  1.62906244e-01  1.63561508e-01  1.64375499e-01  1.64543763e-01
  1.64132446e-01  1.63634539e-01  1.63471222e-01  1.63363665e-01
  1.63185820e-01  1.62931979e-01  1.62702575e-01  1.62507206e-01
  1.62296847e-01  1.62249774e-01  1.62238449e-01  1.62219808e-01
  1.62057593e-01  1.61937729e-01  1.62299126e-01  1.62807152e-01
  1.63252443e-01  1.63472444e-01  1.63620844e-01  1.63946629e-01
  1.64272025e-01  1.64742917e-01  1.65383786e-01  1.65616870e-01
  1.65280581e-01  1.64994985e-01  1.64983377e-01  1.65085420e-01
  1.65299237e-01  1.65406838e-01  1.65369093e-01  1.65190369e-01
  1.65017024e-01  1.65014282e-01  1.65041268e-01  1.65256932e-01
  1.65412351e-01  1.65644377e-01  1.65922359e-01  1.66189909e-01
  1.66760385e-01  1.67647585e-01  1.68284968e-01  1.68526202e-01
  1.68655589e-01  1.68995723e-01  1.69162706e-01  1.69224739e-01
  1.69531971e-01  1.69893458e-01  1.69739813e-01  1.69340551e-01
  1.69372991e-01  1.69842333e-01  1.70389146e-01  1.70779556e-01
  1.70823574e-01  1.70684606e-01  1.70356363e-01  1.70239732e-01
  1.70203105e-01  1.70122117e-01  1.70335591e-01  1.70990825e-01
  1.71351403e-01  1.70866683e-01  1.70435175e-01  1.70509264e-01
  1.70546159e-01  1.70309782e-01  1.70525447e-01  1.71076909e-01
  1.70924082e-01  1.70218363e-01  1.69970810e-01  1.70116663e-01
  1.70101315e-01  1.70205966e-01  1.70831814e-01  1.71377197e-01
  1.71407640e-01  1.71606258e-01  1.72293901e-01  1.72775611e-01
  1.72706932e-01  1.72071517e-01  1.71737492e-01  1.71615541e-01
  1.71368048e-01  1.71111315e-01  1.70935914e-01  1.70857295e-01
  1.71013147e-01  1.71342343e-01  1.71195388e-01  1.70722067e-01
  1.70394167e-01  1.70349181e-01  1.69976056e-01  1.69534773e-01
  1.69592172e-01  1.69685766e-01  1.69404507e-01  1.69130340e-01
  1.69109061e-01  1.68906659e-01  1.68682501e-01  1.68784648e-01
  1.68623894e-01  1.67909294e-01  1.67090103e-01  1.66649297e-01
  1.66138723e-01  1.65102497e-01  1.63914829e-01  1.62901253e-01
  1.62194937e-01  1.61536857e-01  1.60755396e-01  1.60035700e-01
  1.59363553e-01  1.58952579e-01  1.58686697e-01  1.58031151e-01
  1.57295674e-01  1.56786084e-01  1.56287670e-01  1.55744195e-01
  1.55213326e-01  1.55043393e-01  1.54654384e-01  1.54049486e-01
  1.53336361e-01  1.52843907e-01  1.52416125e-01  1.52399659e-01
  1.52555585e-01  1.52279407e-01  1.51568756e-01  1.51324838e-01
  1.51533157e-01  1.51621237e-01  1.51297227e-01  1.51029840e-01
  1.51092961e-01  1.50870740e-01  1.50423840e-01  1.50330842e-01
  1.50233671e-01  1.49712890e-01  1.49349436e-01  1.49483204e-01
  1.49367183e-01  1.48997635e-01  1.48623973e-01  1.48282483e-01
  1.47857577e-01  1.47250414e-01  1.47103474e-01  1.47183925e-01
  1.46958351e-01  1.46533787e-01  1.46562845e-01  1.46647796e-01
  1.46347269e-01  1.46302089e-01  1.46604255e-01  1.46466464e-01
  1.45514354e-01  1.44561231e-01  1.44011050e-01  1.43368706e-01
  1.42675370e-01  1.42129675e-01  1.41652212e-01  1.40779808e-01
  1.39796555e-01  1.39124051e-01  1.38960555e-01  1.38675079e-01
  1.38098225e-01  1.37471870e-01  1.36888161e-01  1.36303753e-01
  1.35725722e-01  1.35404810e-01  1.34992138e-01  1.34436637e-01
  1.34163469e-01  1.33972704e-01  1.33355364e-01  1.32276833e-01
  1.31285191e-01  1.30485058e-01  1.29476994e-01  1.28564730e-01
  1.28215656e-01  1.27966568e-01  1.27054662e-01  1.26071975e-01
  1.25511259e-01  1.25065610e-01  1.24438323e-01  1.23675078e-01
  1.22998565e-01  1.22319594e-01  1.21693037e-01  1.21124752e-01
  1.20804876e-01  1.20659232e-01  1.20196819e-01  1.19448669e-01
  1.18663087e-01  1.18091494e-01  1.17466532e-01  1.16396472e-01
  1.14899449e-01  1.13743596e-01  1.13137871e-01  1.12194471e-01
  1.10859632e-01  1.09587841e-01  1.08827196e-01  1.08118370e-01
  1.07061021e-01  1.05898045e-01  1.04940467e-01  1.04013622e-01
  1.02836616e-01  1.01628236e-01  1.00756526e-01  9.99411866e-02
  9.90510508e-02  9.83828604e-02  9.75948051e-02  9.68632549e-02
  9.62853804e-02  9.55379531e-02  9.47271734e-02  9.35894176e-02
  9.23006535e-02  9.09624249e-02  8.97823945e-02  8.89267549e-02
  8.82912725e-02  8.74262676e-02  8.62415954e-02  8.52200463e-02
  8.49371925e-02  8.46260563e-02  8.40776935e-02  8.34881291e-02
  8.32298771e-02  8.31128508e-02  8.27062801e-02  8.18039104e-02
  8.11247081e-02  8.07532668e-02  8.02722648e-02  7.94385001e-02
  7.85026625e-02  7.78750479e-02  7.71090761e-02  7.56779686e-02
  7.38806203e-02  7.27408826e-02  7.19388053e-02  7.07923174e-02
  6.96125105e-02  6.89234361e-02  6.83611631e-02  6.71190098e-02
  6.57541752e-02  6.53335303e-02  6.52921945e-02  6.45440668e-02
  6.32026568e-02  6.25186786e-02  6.23225905e-02  6.16780967e-02
  6.10615797e-02  6.10787123e-02  6.11383282e-02  6.06773831e-02
  5.99229187e-02  5.94423674e-02  5.87835759e-02  5.76649420e-02
  5.62923811e-02  5.49450628e-02  5.34289591e-02  5.23051433e-02
  5.13734519e-02  5.03320210e-02  4.87680361e-02  4.73404154e-02
  4.64625880e-02  4.57873419e-02  4.48447019e-02  4.37815711e-02
  4.33168113e-02  4.31092493e-02  4.28165868e-02  4.27481309e-02
  4.31024767e-02  4.35550585e-02  4.32677530e-02  4.28759865e-02
  4.29039262e-02  4.31664251e-02  4.26636860e-02  4.17088419e-02
  4.06603627e-02  3.97607312e-02  3.90024781e-02  3.81187312e-02
  3.73480245e-02  3.72597463e-02  3.72118764e-02  3.64475213e-02
  3.54427174e-02  3.52043398e-02  3.53914239e-02  3.53784636e-02
  3.49122845e-02  3.45762968e-02  3.46952416e-02  3.44889238e-02
  3.36360112e-02  3.33800875e-02  3.32906991e-02  3.22758816e-02
  3.12628560e-02  3.15062702e-02  3.17899771e-02  3.10631730e-02
  2.95416061e-02  2.84276456e-02  2.79364269e-02  2.72518676e-02
  2.65634917e-02  2.58245505e-02  2.52627563e-02  2.49319244e-02
  2.45269556e-02  2.41160151e-02  2.40981337e-02  2.45177858e-02
  2.44604871e-02  2.38454770e-02  2.36356091e-02  2.41006631e-02
  2.47848276e-02  2.46690474e-02  2.40918342e-02  2.40977854e-02
  2.44687479e-02  2.47003566e-02  2.43690275e-02  2.39091702e-02
  2.34371331e-02  2.28419267e-02  2.20264997e-02  2.10288949e-02
  2.08941642e-02  2.08031069e-02  2.04091631e-02  1.92177724e-02
  1.92215107e-02  2.03677174e-02  2.08349433e-02  1.99930184e-02
  1.94382928e-02  1.95354465e-02  1.98853891e-02  1.95054784e-02
  1.94280371e-02  2.05482915e-02  2.11706515e-02  1.95450708e-02
  1.82329565e-02  1.84935723e-02  1.85936932e-02  1.66871455e-02
  1.41154379e-02  1.24443220e-02  1.17536122e-02  1.04355132e-02
  8.82583763e-03  7.83530343e-03  8.44378304e-03  8.57914239e-03
  7.22081820e-03  6.09663548e-03  6.85139745e-03  8.03573988e-03
  6.29869429e-03  3.07404366e-03  3.87436710e-03  5.80694200e-03
  2.91121635e-03 -1.37337274e-03 -2.03465053e-04  1.20822364e-03
 -6.54045632e-03 -1.35744447e-02 -6.14872482e-03 -3.74089926e-03]
