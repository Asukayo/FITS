Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  34420736.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8919387
	speed: 0.1464s/iter; left time: 805.2617s
Epoch: 1 cost time: 16.549094438552856
Epoch: 1, Steps: 112 | Train Loss: 0.9721695 Vali Loss: 0.7408456 Test Loss: 0.4117728
Validation loss decreased (inf --> 0.740846).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7269154
	speed: 0.3501s/iter; left time: 1886.9137s
Epoch: 2 cost time: 17.451719760894775
Epoch: 2, Steps: 112 | Train Loss: 0.8588853 Vali Loss: 0.7014332 Test Loss: 0.3947244
Validation loss decreased (0.740846 --> 0.701433).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.7945670
	speed: 0.3431s/iter; left time: 1810.4911s
Epoch: 3 cost time: 16.614901781082153
Epoch: 3, Steps: 112 | Train Loss: 0.8377637 Vali Loss: 0.6831636 Test Loss: 0.3880433
Validation loss decreased (0.701433 --> 0.683164).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.9742769
	speed: 0.3472s/iter; left time: 1793.1801s
Epoch: 4 cost time: 17.385141611099243
Epoch: 4, Steps: 112 | Train Loss: 0.8240451 Vali Loss: 0.6757308 Test Loss: 0.3843652
Validation loss decreased (0.683164 --> 0.675731).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.8586404
	speed: 0.3599s/iter; left time: 1818.6741s
Epoch: 5 cost time: 17.698999404907227
Epoch: 5, Steps: 112 | Train Loss: 0.8188813 Vali Loss: 0.6686752 Test Loss: 0.3822629
Validation loss decreased (0.675731 --> 0.668675).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.9039017
	speed: 0.3438s/iter; left time: 1698.8452s
Epoch: 6 cost time: 16.740675926208496
Epoch: 6, Steps: 112 | Train Loss: 0.8141709 Vali Loss: 0.6633949 Test Loss: 0.3811781
Validation loss decreased (0.668675 --> 0.663395).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6193386
	speed: 0.3057s/iter; left time: 1476.0249s
Epoch: 7 cost time: 13.933993577957153
Epoch: 7, Steps: 112 | Train Loss: 0.8117867 Vali Loss: 0.6609504 Test Loss: 0.3803083
Validation loss decreased (0.663395 --> 0.660950).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5997772
	speed: 0.2796s/iter; left time: 1318.9767s
Epoch: 8 cost time: 14.71349024772644
Epoch: 8, Steps: 112 | Train Loss: 0.8086872 Vali Loss: 0.6579480 Test Loss: 0.3798801
Validation loss decreased (0.660950 --> 0.657948).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.7446051
	speed: 0.3425s/iter; left time: 1577.0169s
Epoch: 9 cost time: 17.357454538345337
Epoch: 9, Steps: 112 | Train Loss: 0.8070701 Vali Loss: 0.6541378 Test Loss: 0.3796355
Validation loss decreased (0.657948 --> 0.654138).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.8097446
	speed: 0.3538s/iter; left time: 1589.6127s
Epoch: 10 cost time: 17.40371870994568
Epoch: 10, Steps: 112 | Train Loss: 0.8077562 Vali Loss: 0.6559415 Test Loss: 0.3794998
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6870888
	speed: 0.2938s/iter; left time: 1287.3086s
Epoch: 11 cost time: 14.240803241729736
Epoch: 11, Steps: 112 | Train Loss: 0.8050200 Vali Loss: 0.6532098 Test Loss: 0.3794264
Validation loss decreased (0.654138 --> 0.653210).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6106157
	speed: 0.2873s/iter; left time: 1226.5803s
Epoch: 12 cost time: 13.5293710231781
Epoch: 12, Steps: 112 | Train Loss: 0.8044916 Vali Loss: 0.6512918 Test Loss: 0.3792334
Validation loss decreased (0.653210 --> 0.651292).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6200854
	speed: 0.2599s/iter; left time: 1080.3063s
Epoch: 13 cost time: 12.709990739822388
Epoch: 13, Steps: 112 | Train Loss: 0.8031953 Vali Loss: 0.6495974 Test Loss: 0.3791899
Validation loss decreased (0.651292 --> 0.649597).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6987969
	speed: 0.2859s/iter; left time: 1156.2853s
Epoch: 14 cost time: 14.730307340621948
Epoch: 14, Steps: 112 | Train Loss: 0.8028688 Vali Loss: 0.6480547 Test Loss: 0.3793026
Validation loss decreased (0.649597 --> 0.648055).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.7733704
	speed: 0.2906s/iter; left time: 1143.0112s
Epoch: 15 cost time: 14.412961483001709
Epoch: 15, Steps: 112 | Train Loss: 0.8011903 Vali Loss: 0.6495761 Test Loss: 0.3793249
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.6156089
	speed: 0.2920s/iter; left time: 1115.7926s
Epoch: 16 cost time: 14.559027433395386
Epoch: 16, Steps: 112 | Train Loss: 0.8025424 Vali Loss: 0.6475534 Test Loss: 0.3792111
Validation loss decreased (0.648055 --> 0.647553).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.7695222
	speed: 0.2960s/iter; left time: 1098.0434s
Epoch: 17 cost time: 14.621129989624023
Epoch: 17, Steps: 112 | Train Loss: 0.8018809 Vali Loss: 0.6461633 Test Loss: 0.3792626
Validation loss decreased (0.647553 --> 0.646163).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.8919312
	speed: 0.2886s/iter; left time: 1037.9966s
Epoch: 18 cost time: 14.849577903747559
Epoch: 18, Steps: 112 | Train Loss: 0.8016722 Vali Loss: 0.6487662 Test Loss: 0.3791817
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.8572479
	speed: 0.2995s/iter; left time: 1043.6347s
Epoch: 19 cost time: 14.864442586898804
Epoch: 19, Steps: 112 | Train Loss: 0.8005466 Vali Loss: 0.6452386 Test Loss: 0.3792738
Validation loss decreased (0.646163 --> 0.645239).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5841627
	speed: 0.3014s/iter; left time: 1016.5555s
Epoch: 20 cost time: 14.392712593078613
Epoch: 20, Steps: 112 | Train Loss: 0.7993141 Vali Loss: 0.6486939 Test Loss: 0.3791317
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.9228463
	speed: 0.3015s/iter; left time: 983.2000s
Epoch: 21 cost time: 15.381367206573486
Epoch: 21, Steps: 112 | Train Loss: 0.7997889 Vali Loss: 0.6468090 Test Loss: 0.3791974
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.8967940
	speed: 0.3073s/iter; left time: 967.5654s
Epoch: 22 cost time: 15.417635202407837
Epoch: 22, Steps: 112 | Train Loss: 0.7996181 Vali Loss: 0.6458004 Test Loss: 0.3791755
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3777105510234833, mae:0.42287907004356384, rse:0.49123045802116394, corr:[ 0.21560335  0.22053324  0.2181592   0.219292    0.21891072  0.21689178
  0.21695672  0.21715109  0.21523023  0.21357304  0.21314104  0.21206205
  0.21034288  0.20931722  0.20878103  0.20801274  0.20741177  0.2068434
  0.20570056  0.20443612  0.20381938  0.20338154  0.20212208  0.2006851
  0.19953842  0.19861306  0.19748233  0.19642515  0.19564278  0.19481947
  0.19397303  0.19315822  0.19252056  0.19170919  0.19076061  0.18969761
  0.18878065  0.18829817  0.18775286  0.18693423  0.1861894   0.18580902
  0.18540086  0.18454559  0.18359299  0.18321195  0.18268423  0.18104275
  0.17916395  0.17807174  0.17740914  0.17653085  0.17557003  0.17491055
  0.17466651  0.17435321  0.17345877  0.17251658  0.17216371  0.17192413
  0.17119999  0.17070484  0.171273    0.172011    0.17195629  0.17181733
  0.17193584  0.17194355  0.1716795   0.17148905  0.17138042  0.17108345
  0.17075554  0.17041881  0.1699101   0.16924109  0.16909626  0.16936642
  0.16941208  0.16925701  0.16902024  0.16866085  0.1683619   0.16861387
  0.16913809  0.16925208  0.16897538  0.16896567  0.16924547  0.16921441
  0.16882564  0.16881998  0.16936633  0.16970532  0.16965519  0.16946115
  0.16938291  0.16937701  0.16928072  0.16894375  0.16845341  0.16806959
  0.16818744  0.16811298  0.16748995  0.16696155  0.16736095  0.16783439
  0.16748327  0.16699426  0.16715051  0.16723806  0.1666473   0.16633707
  0.16666113  0.16673806  0.16610087  0.16557217  0.16547665  0.16473156
  0.16350536  0.16319937  0.16356258  0.16304645  0.16179276  0.16129991
  0.16137888  0.16084756  0.15991943  0.15933625  0.15906188  0.15843838
  0.15776972  0.15728241  0.15680324  0.15627702  0.15579832  0.15540445
  0.15473178  0.15436591  0.15475115  0.15515608  0.15479936  0.15378657
  0.15263984  0.15169793  0.15076217  0.15016198  0.15003845  0.1498613
  0.1492299   0.1485037   0.14813635  0.14771461  0.14697805  0.14612164
  0.14562377  0.14558543  0.14560355  0.145306    0.14489736  0.14503318
  0.1451984   0.14490934  0.14494276  0.14565498  0.1459732   0.14473608
  0.14325531  0.142985    0.14346369  0.14324333  0.14261433  0.14256096
  0.14270125  0.14199007  0.14087074  0.14033866  0.14003873  0.13950892
  0.13893129  0.13872041  0.13875099  0.13868676  0.1386974   0.13873692
  0.13834317  0.13820414  0.13898149  0.14019868  0.14082724  0.14050673
  0.14020303  0.14041913  0.14064349  0.14066808  0.14077209  0.14117321
  0.14188802  0.14208794  0.14158024  0.14093383  0.14078818  0.14064963
  0.14031011  0.14027019  0.14057063  0.14090613  0.14117338  0.14189601
  0.14284375  0.14324737  0.14306574  0.14303383  0.14317803  0.14277084
  0.1420346   0.14177907  0.14184318  0.1414891   0.14090316  0.14118125
  0.14179002  0.14196424  0.14156684  0.14132278  0.14136992  0.1412784
  0.14110796  0.14092878  0.14100918  0.14171274  0.14288996  0.14385092
  0.14421234  0.14439507  0.14478943  0.14526516  0.14582646  0.14653586
  0.14711675  0.14727452  0.14765032  0.14843388  0.1486006   0.14784853
  0.14736672  0.14794795  0.14897844  0.14933792  0.14936836  0.14986274
  0.15078954  0.15195458  0.15308253  0.15371065  0.15348907  0.15343216
  0.15414234  0.1548374   0.15488352  0.15530764  0.15660444  0.15740252
  0.15722956  0.15740845  0.1588757   0.16021928  0.16024959  0.15994258
  0.16034128  0.16093716  0.1609637   0.16079803  0.16132498  0.16227715
  0.162789    0.16273089  0.1624056   0.16256765  0.16372737  0.16533008
  0.16606277  0.16565463  0.16523546  0.16587012  0.16696237  0.16735479
  0.16729805  0.16737068  0.16739145  0.16708554  0.16666426  0.16672379
  0.1672257   0.1676492   0.16787696  0.16810009  0.16809702  0.16768663
  0.16738613  0.16787004  0.16870548  0.1691759   0.1694537   0.1700646
  0.17041332  0.17023939  0.16988884  0.17020483  0.17109667  0.17146356
  0.1708376   0.16988578  0.16969568  0.17000899  0.16998424  0.16935799
  0.16884701  0.16906835  0.1695349   0.16974     0.16953975  0.16949901
  0.1695728   0.1694755   0.16949797  0.16980106  0.17062879  0.17148264
  0.1715835   0.1710573   0.17055902  0.17097624  0.17217112  0.17297666
  0.17305042  0.17303902  0.17319164  0.1730814   0.17294508  0.17312646
  0.17381354  0.17442472  0.17456524  0.17433411  0.17413145  0.17462637
  0.17523043  0.17552283  0.1757327   0.17643502  0.17743167  0.17789033
  0.17759237  0.17740387  0.17770407  0.17840841  0.17941435  0.18063581
  0.1812554   0.18059011  0.17940141  0.17908342  0.17977156  0.18050954
  0.18102948  0.18153726  0.18169475  0.18160206  0.18147415  0.1815789
  0.1815909   0.18157026  0.18176797  0.18187004  0.18150184  0.18111373
  0.18142581  0.18179452  0.18126865  0.18021767  0.18018445  0.18120462
  0.18193579  0.1817155   0.18120676  0.1810443   0.1812544   0.18161006
  0.18197238  0.18208994  0.18206125  0.18242307  0.18336153  0.18415508
  0.18428457  0.18368654  0.18324718  0.18302673  0.18257342  0.18226905
  0.18251804  0.1828277   0.18259043  0.18222824  0.18204992  0.18183446
  0.18129744  0.18091199  0.18066256  0.18040238  0.18054032  0.18106641
  0.18112977  0.18039033  0.17982171  0.17995422  0.17993726  0.17940317
  0.17904817  0.17930715  0.17888771  0.17727405  0.17579073  0.17512152
  0.17428786  0.17282996  0.17214696  0.17223223  0.17161249  0.17053689
  0.1701191   0.17020771  0.16994384  0.16945238  0.16945691  0.16915831
  0.16798183  0.16719723  0.16707113  0.16665594  0.16552114  0.16515827
  0.16550127  0.16537158  0.16446432  0.16379775  0.16318575  0.16232525
  0.1622636   0.16333784  0.16374832  0.1628726   0.16215183  0.16247755
  0.16300946  0.16274862  0.16196932  0.16138308  0.16124217  0.16150805
  0.1615047   0.1605806   0.15950415  0.1599562   0.16071577  0.15993962
  0.15880434  0.15876842  0.1590894   0.15824075  0.15713976  0.15695333
  0.15680969  0.15581366  0.15514031  0.15591753  0.15682933  0.15669098
  0.15586334  0.15502647  0.15420255  0.15328881  0.1523197   0.15083869
  0.14948453  0.1492644   0.14941551  0.14831187  0.14700006  0.14710808
  0.14761625  0.1467926   0.14550477  0.14516503  0.14464326  0.14345594
  0.14326431  0.1444825   0.14488332  0.14338906  0.1419977   0.14168502
  0.1410513   0.13964622  0.13866751  0.1382513   0.1373076   0.13611503
  0.13560411  0.13552931  0.13465334  0.13327992  0.13254778  0.13221669
  0.13188095  0.13166003  0.13143265  0.13059886  0.12980194  0.12981775
  0.12961002  0.12858135  0.12755857  0.12700789  0.12570333  0.12350067
  0.12198415  0.12210029  0.12255621  0.12177466  0.1203862   0.1191105
  0.11821771  0.1174387   0.11627311  0.11456452  0.11335768  0.11346573
  0.11346757  0.11177324  0.10967453  0.10891806  0.10858461  0.10725464
  0.10563212  0.10565893  0.10637444  0.10549562  0.10363081  0.10223686
  0.1015335   0.10048547  0.09918016  0.09823306  0.09760214  0.09679423
  0.09567568  0.09430616  0.09328963  0.09272671  0.09285642  0.09294886
  0.0923635   0.09142308  0.09100346  0.09099649  0.09081548  0.08974001
  0.08823558  0.08731131  0.08669563  0.08561803  0.08438112  0.08396046
  0.08367923  0.08246007  0.0807271   0.08017425  0.08039881  0.0794793
  0.07772508  0.0768857   0.07664119  0.07553904  0.073955    0.07338665
  0.07296932  0.07174451  0.07071432  0.07068145  0.07055517  0.06935113
  0.06836923  0.06881981  0.06900981  0.06770276  0.06596578  0.06509225
  0.06446038  0.06346825  0.0621803   0.06099352  0.05952013  0.05866651
  0.05849295  0.05771821  0.05551837  0.05348645  0.05299797  0.05293071
  0.05216381  0.0508831   0.05014209  0.05022904  0.05072049  0.05132792
  0.05128143  0.05097819  0.05106551  0.05156798  0.05119354  0.05007232
  0.04904167  0.04874481  0.04882277  0.0484204   0.04749744  0.0468576
  0.04644834  0.04559683  0.04395965  0.04254713  0.04242147  0.04346024
  0.04368011  0.04233006  0.04121161  0.04113897  0.04087862  0.04039657
  0.03993125  0.03927552  0.03859264  0.03823962  0.03767001  0.036891
  0.03627954  0.03621795  0.03603379  0.03490144  0.03401168  0.03366809
  0.03292523  0.03128801  0.03008247  0.03044936  0.03154366  0.03204725
  0.03152707  0.03049447  0.02975241  0.03002738  0.0311808   0.03109105
  0.02950756  0.02916679  0.03079279  0.03199354  0.0309227   0.0296677
  0.02986994  0.03038683  0.0297834   0.02833302  0.02741938  0.02644468
  0.02576404  0.02491396  0.02462129  0.02497191  0.02569977  0.0258542
  0.02469323  0.02298271  0.02290646  0.02343774  0.02309733  0.02294462
  0.02347308  0.02253455  0.02098913  0.02104399  0.02204631  0.02080701
  0.01778317  0.01585026  0.01561723  0.01478688  0.01350583  0.01212042
  0.01064601  0.00901925  0.00850705  0.00838275  0.00719769  0.00682817
  0.0073159   0.00551316  0.00320401  0.00389425  0.00570317  0.00374337
  0.0010034   0.00285004  0.00287642 -0.00302939 -0.00275189  0.00866104]
