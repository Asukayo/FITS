Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16088576.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8697471
	speed: 0.1250s/iter; left time: 687.5482s
Epoch: 1 cost time: 14.024521589279175
Epoch: 1, Steps: 112 | Train Loss: 0.9834308 Vali Loss: 0.7457941 Test Loss: 0.4157510
Validation loss decreased (inf --> 0.745794).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.9028015
	speed: 0.2814s/iter; left time: 1516.2692s
Epoch: 2 cost time: 14.242815494537354
Epoch: 2, Steps: 112 | Train Loss: 0.8624017 Vali Loss: 0.7037153 Test Loss: 0.3963104
Validation loss decreased (0.745794 --> 0.703715).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 1.0411947
	speed: 0.2928s/iter; left time: 1545.0792s
Epoch: 3 cost time: 14.514985799789429
Epoch: 3, Steps: 112 | Train Loss: 0.8392905 Vali Loss: 0.6844124 Test Loss: 0.3889067
Validation loss decreased (0.703715 --> 0.684412).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.7636741
	speed: 0.2915s/iter; left time: 1505.7997s
Epoch: 4 cost time: 14.564650297164917
Epoch: 4, Steps: 112 | Train Loss: 0.8280393 Vali Loss: 0.6755178 Test Loss: 0.3848538
Validation loss decreased (0.684412 --> 0.675518).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6510087
	speed: 0.2902s/iter; left time: 1466.3318s
Epoch: 5 cost time: 14.594950914382935
Epoch: 5, Steps: 112 | Train Loss: 0.8197727 Vali Loss: 0.6679383 Test Loss: 0.3831116
Validation loss decreased (0.675518 --> 0.667938).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6606492
	speed: 0.3099s/iter; left time: 1531.1695s
Epoch: 6 cost time: 15.37506628036499
Epoch: 6, Steps: 112 | Train Loss: 0.8164774 Vali Loss: 0.6650834 Test Loss: 0.3816202
Validation loss decreased (0.667938 --> 0.665083).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.8571209
	speed: 0.3101s/iter; left time: 1497.3987s
Epoch: 7 cost time: 15.632614612579346
Epoch: 7, Steps: 112 | Train Loss: 0.8129388 Vali Loss: 0.6603346 Test Loss: 0.3810626
Validation loss decreased (0.665083 --> 0.660335).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.9979426
	speed: 0.3048s/iter; left time: 1437.8421s
Epoch: 8 cost time: 14.723320960998535
Epoch: 8, Steps: 112 | Train Loss: 0.8104668 Vali Loss: 0.6608668 Test Loss: 0.3805888
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6383276
	speed: 0.2813s/iter; left time: 1295.1752s
Epoch: 9 cost time: 14.304345846176147
Epoch: 9, Steps: 112 | Train Loss: 0.8083677 Vali Loss: 0.6562123 Test Loss: 0.3803776
Validation loss decreased (0.660335 --> 0.656212).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.8216458
	speed: 0.2798s/iter; left time: 1257.2010s
Epoch: 10 cost time: 13.833401918411255
Epoch: 10, Steps: 112 | Train Loss: 0.8096583 Vali Loss: 0.6536639 Test Loss: 0.3801063
Validation loss decreased (0.656212 --> 0.653664).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.7371756
	speed: 0.2870s/iter; left time: 1257.2711s
Epoch: 11 cost time: 14.674663066864014
Epoch: 11, Steps: 112 | Train Loss: 0.8056105 Vali Loss: 0.6568574 Test Loss: 0.3800832
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.7416999
	speed: 0.2893s/iter; left time: 1235.1600s
Epoch: 12 cost time: 14.306630373001099
Epoch: 12, Steps: 112 | Train Loss: 0.8056935 Vali Loss: 0.6508139 Test Loss: 0.3799956
Validation loss decreased (0.653664 --> 0.650814).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.9302292
	speed: 0.2871s/iter; left time: 1193.3962s
Epoch: 13 cost time: 14.439958333969116
Epoch: 13, Steps: 112 | Train Loss: 0.8047705 Vali Loss: 0.6531943 Test Loss: 0.3800609
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.9619042
	speed: 0.2878s/iter; left time: 1164.2240s
Epoch: 14 cost time: 14.356765270233154
Epoch: 14, Steps: 112 | Train Loss: 0.8058343 Vali Loss: 0.6501805 Test Loss: 0.3800468
Validation loss decreased (0.650814 --> 0.650181).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.6526447
	speed: 0.2897s/iter; left time: 1139.3659s
Epoch: 15 cost time: 14.699236392974854
Epoch: 15, Steps: 112 | Train Loss: 0.8050865 Vali Loss: 0.6519678 Test Loss: 0.3800013
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.8918197
	speed: 0.2877s/iter; left time: 1099.2707s
Epoch: 16 cost time: 14.516487836837769
Epoch: 16, Steps: 112 | Train Loss: 0.8036720 Vali Loss: 0.6466224 Test Loss: 0.3799182
Validation loss decreased (0.650181 --> 0.646622).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.9657063
	speed: 0.2912s/iter; left time: 1079.9398s
Epoch: 17 cost time: 14.624438524246216
Epoch: 17, Steps: 112 | Train Loss: 0.8041766 Vali Loss: 0.6487511 Test Loss: 0.3799555
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.6613191
	speed: 0.3023s/iter; left time: 1087.2704s
Epoch: 18 cost time: 15.459191083908081
Epoch: 18, Steps: 112 | Train Loss: 0.8032864 Vali Loss: 0.6485425 Test Loss: 0.3799566
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.6078117
	speed: 0.3050s/iter; left time: 1062.8199s
Epoch: 19 cost time: 15.332638263702393
Epoch: 19, Steps: 112 | Train Loss: 0.8026405 Vali Loss: 0.6484235 Test Loss: 0.3799451
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.37835419178009033, mae:0.4234393239021301, rse:0.4916488230228424, corr:[ 0.21488984  0.2201735   0.21930002  0.2172121   0.21694095  0.21763052
  0.21734484  0.21562421  0.21387587  0.2127133   0.21196572  0.21094258
  0.20959939  0.20842507  0.20767821  0.20725934  0.20679034  0.20584822
  0.20480655  0.20399515  0.20326096  0.20237918  0.20106919  0.19968227
  0.19848159  0.19751313  0.196478    0.195404    0.19450624  0.1937747
  0.19313172  0.19240566  0.191613    0.19072981  0.18992084  0.18911764
  0.1882786   0.1875766   0.18706235  0.18664408  0.18612976  0.18535827
  0.18448375  0.18388015  0.18337345  0.18274304  0.18156841  0.17973776
  0.17794079  0.17671649  0.17604749  0.17558528  0.17507719  0.17440645
  0.17355497  0.17273258  0.17199336  0.17134485  0.1708911   0.17075862
  0.17092182  0.17109035  0.17120306  0.17123748  0.17106257  0.17100711
  0.17100085  0.17101583  0.17092311  0.1707516   0.17042431  0.1699431
  0.16950634  0.16929494  0.16937044  0.16943465  0.16936004  0.169032
  0.16849698  0.1681413   0.16811615  0.16820794  0.16810966  0.16783261
  0.16763362  0.16771695  0.16803043  0.16833356  0.16833386  0.16808341
  0.16781276  0.1677691   0.16806635  0.16838177  0.16851652  0.1683636
  0.16810007  0.16784506  0.16749118  0.1670426   0.16672556  0.1665478
  0.1666492   0.16679023  0.16681543  0.1665651   0.16633292  0.16631094
  0.16636266  0.16628496  0.16597967  0.16548406  0.16505218  0.16500232
  0.1652374   0.16548444  0.16541529  0.16492544  0.16435155  0.16376932
  0.16314115  0.1624837   0.1617286   0.16098809  0.1603572   0.16000327
  0.15975639  0.15954207  0.15933195  0.15896861  0.15854256  0.15785465
  0.15698348  0.15610696  0.15553983  0.15528926  0.1547673   0.15396115
  0.15306488  0.15265074  0.15291487  0.15333162  0.15310678  0.15184686
  0.14997761  0.14862068  0.14816782  0.14821436  0.14808752  0.14756788
  0.14694405  0.14657609  0.14649864  0.14625178  0.14554314  0.14454055
  0.14384325  0.14379439  0.14404663  0.14392847  0.14322615  0.14263009
  0.14267734  0.14338888  0.14434405  0.14481683  0.14457862  0.1437758
  0.14303984  0.14256337  0.14228508  0.14198297  0.14165917  0.14133814
  0.1408923   0.14023033  0.13941696  0.1386404   0.13796937  0.13762525
  0.13751423  0.13743868  0.1374018   0.13737698  0.13755454  0.13792558
  0.13820525  0.13837735  0.13861616  0.13915144  0.13993648  0.14025965
  0.13977556  0.1389697   0.13852084  0.13885671  0.13970082  0.14025562
  0.140298    0.13991603  0.1396596   0.13981995  0.14027771  0.14030571
  0.13984646  0.13934793  0.13916312  0.13941154  0.1399051   0.14027546
  0.14040789  0.14054514  0.14081453  0.14102836  0.14090304  0.14035268
  0.13970174  0.13922141  0.13898553  0.13895716  0.1388726   0.13889831
  0.13898279  0.13934387  0.13971013  0.13993315  0.14005761  0.14006843
  0.14012976  0.14017387  0.14025559  0.14040934  0.14063263  0.14098719
  0.14153916  0.14232168  0.1433479   0.144441    0.14531025  0.14563768
  0.14536725  0.14475344  0.14434764  0.14457136  0.14527062  0.14603704
  0.14672013  0.14712377  0.14734389  0.14757152  0.14784096  0.14828086
  0.14888334  0.14962712  0.15031219  0.15101069  0.1515846   0.1521932
  0.1529106   0.15361859  0.15400353  0.15410863  0.15427022  0.15473323
  0.15569353  0.15679556  0.15765363  0.15804906  0.1580221   0.15802121
  0.15828906  0.15873715  0.15917887  0.15952738  0.15978476  0.16000435
  0.16012372  0.16051054  0.1611726   0.16207306  0.16298349  0.16346285
  0.16322681  0.16262497  0.16225246  0.16261216  0.16367273  0.16465738
  0.16507517  0.16502306  0.16492802  0.16525331  0.16573578  0.16601074
  0.16592844  0.16570415  0.16571303  0.16617845  0.16680063  0.16715698
  0.16687094  0.16645092  0.16643347  0.16696545  0.1676959   0.16832055
  0.16842031  0.16826281  0.16810232  0.16816428  0.1685151   0.16891609
  0.16939504  0.16966821  0.16969408  0.16928178  0.16866551  0.1682839
  0.16843769  0.1690779   0.16962638  0.1697245   0.1691776   0.16832836
  0.1676181   0.16749102  0.16825382  0.16923603  0.16994374  0.17019165
  0.17016676  0.17037815  0.17079532  0.17127195  0.17166385  0.1717575
  0.17155403  0.1713022   0.17122431  0.17126155  0.17150638  0.17184421
  0.17229381  0.17265241  0.17273141  0.1725833   0.17217472  0.1720482
  0.17211209  0.17237674  0.17290144  0.17372866  0.1748058   0.17599437
  0.17692724  0.1775039   0.17763318  0.17756884  0.17757466  0.17783578
  0.17821231  0.17843288  0.17846516  0.17840372  0.17841795  0.17858951
  0.17890237  0.17920782  0.17928308  0.17942327  0.17952791  0.17974547
  0.18001449  0.18035908  0.180698    0.18089785  0.18088658  0.1806408
  0.1802465   0.18003151  0.18010995  0.18026108  0.18032165  0.18034488
  0.18034957  0.18037929  0.18050843  0.18069257  0.18083878  0.18096015
  0.18115802  0.18146981  0.18174699  0.1818736   0.18197666  0.18214107
  0.18248661  0.1825503   0.18241082  0.18204637  0.18142734  0.18091051
  0.18066987  0.18067347  0.18070549  0.18060455  0.1802786   0.17989168
  0.17942089  0.17912526  0.17909971  0.17936788  0.17978778  0.17998472
  0.1797388   0.1792441   0.17881365  0.1786382   0.17863674  0.17857724
  0.17808639  0.1773493   0.17664446  0.17605212  0.17534566  0.17428984
  0.17320178  0.1723171   0.1718825   0.17175294  0.17139404  0.17072877
  0.16983005  0.16909374  0.16880693  0.16856942  0.16805074  0.16729122
  0.1666118   0.16634999  0.16615131  0.16592154  0.1654162   0.16482976
  0.16426115  0.16396329  0.16358402  0.16303921  0.16231877  0.16175878
  0.16157608  0.16180141  0.16186197  0.16174932  0.16163358  0.16165468
  0.16174465  0.16157694  0.16117947  0.16105294  0.16115999  0.16113031
  0.16060892  0.15968156  0.15860114  0.15817328  0.15837325  0.15843095
  0.15792963  0.15679933  0.15582572  0.15545264  0.15563878  0.1557935
  0.15568034  0.15535764  0.15522048  0.15545304  0.1555313   0.15491836
  0.15362754  0.15237817  0.15185982  0.15173632  0.15151732  0.15077066
  0.14964433  0.14854741  0.14793321  0.14758267  0.14713226  0.14622724
  0.14515501  0.14437912  0.14398266  0.14351477  0.14247537  0.14133362
  0.14064154  0.14060687  0.14093576  0.14099897  0.14055678  0.1396216
  0.13858831  0.13788126  0.13747276  0.1370675   0.13661268  0.13620356
  0.1355539   0.13470073  0.13376912  0.13304737  0.13274564  0.13241799
  0.13166793  0.13031138  0.1288266   0.1276938   0.12721023  0.12720335
  0.12698723  0.12647662  0.12593892  0.12568276  0.12542367  0.12465484
  0.12315018  0.12153262  0.12043289  0.11959421  0.11876332  0.11762726
  0.11635319  0.11531006  0.11465475  0.11411675  0.11318535  0.11156448
  0.10972312  0.10840683  0.10808098  0.10823039  0.10794553  0.10711641
  0.10592788  0.10512413  0.10481774  0.10425566  0.10312752  0.10139687
  0.09979811  0.0986676   0.09797135  0.09724223  0.09622487  0.09506256
  0.09400263  0.09296404  0.09227746  0.09162579  0.09112787  0.09064652
  0.09021706  0.0897586   0.08927608  0.08865959  0.08819085  0.08771845
  0.08701312  0.08619867  0.08555266  0.08526111  0.08491836  0.08403721
  0.08240674  0.08070021  0.0793303   0.07843731  0.07771822  0.07669929
  0.07535149  0.07407428  0.07325631  0.07277001  0.07201763  0.07080352
  0.06926351  0.06822528  0.06810406  0.06845194  0.06851444  0.06785299
  0.06676086  0.06623165  0.06648041  0.06696949  0.06670204  0.06544735
  0.06370135  0.06225726  0.06110322  0.06026028  0.05915902  0.05787148
  0.05639894  0.05499932  0.05381921  0.05314131  0.05288731  0.05231735
  0.05130697  0.05009706  0.0493461   0.04946108  0.05027438  0.05140225
  0.05179444  0.05113833  0.04989028  0.04913993  0.04904655  0.04938207
  0.04908576  0.04768632  0.04576078  0.04431297  0.04377255  0.04387524
  0.04382071  0.04338425  0.04308381  0.04332946  0.04352611  0.04308487
  0.04179115  0.04027534  0.03961737  0.03989669  0.03986572  0.03923355
  0.03824303  0.03746894  0.03749113  0.03794128  0.03754631  0.03610105
  0.03444473  0.03367113  0.03386832  0.03398539  0.03341485  0.03193688
  0.03023205  0.02910009  0.02874136  0.02880616  0.02885247  0.02865223
  0.02808006  0.0274959   0.02742246  0.02774006  0.0284167   0.02906502
  0.02927077  0.02887601  0.02799221  0.02759068  0.02793208  0.02852516
  0.02848444  0.02760585  0.02640781  0.02523932  0.02480712  0.02451247
  0.02433099  0.02368441  0.02344037  0.02368888  0.02374508  0.02294849
  0.02164581  0.02017353  0.01977059  0.02016922  0.0203997   0.02005634
  0.01966711  0.01960027  0.0203859   0.02103244  0.02046214  0.01827213
  0.0155493   0.01357959  0.0130999   0.0129168   0.01221843  0.0104326
  0.00842985  0.00664891  0.00608055  0.00673377  0.0074289   0.00751102
  0.00638848  0.00398279  0.00243493  0.00269737  0.004631    0.00629071
  0.00480609  0.00063457 -0.0023211  -0.00024055  0.00321513 -0.00105586]
