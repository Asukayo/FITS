Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  110584320.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.4383161067962646
Epoch: 1, Steps: 15 | Train Loss: 0.7322340 Vali Loss: 0.3898704 Test Loss: 0.4195822
Validation loss decreased (inf --> 0.389870).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.6928579807281494
Epoch: 2, Steps: 15 | Train Loss: 0.5877440 Vali Loss: 0.3253772 Test Loss: 0.3636141
Validation loss decreased (0.389870 --> 0.325377).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.742809295654297
Epoch: 3, Steps: 15 | Train Loss: 0.5256110 Vali Loss: 0.2991663 Test Loss: 0.3391906
Validation loss decreased (0.325377 --> 0.299166).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.799976110458374
Epoch: 4, Steps: 15 | Train Loss: 0.4951046 Vali Loss: 0.2811715 Test Loss: 0.3269545
Validation loss decreased (0.299166 --> 0.281172).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.771158218383789
Epoch: 5, Steps: 15 | Train Loss: 0.4811154 Vali Loss: 0.2711542 Test Loss: 0.3196380
Validation loss decreased (0.281172 --> 0.271154).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.7861571311950684
Epoch: 6, Steps: 15 | Train Loss: 0.4731309 Vali Loss: 0.2662756 Test Loss: 0.3150248
Validation loss decreased (0.271154 --> 0.266276).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.002077341079712
Epoch: 7, Steps: 15 | Train Loss: 0.4645142 Vali Loss: 0.2609355 Test Loss: 0.3116306
Validation loss decreased (0.266276 --> 0.260936).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.9859046936035156
Epoch: 8, Steps: 15 | Train Loss: 0.4619556 Vali Loss: 0.2566833 Test Loss: 0.3090909
Validation loss decreased (0.260936 --> 0.256683).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9358251094818115
Epoch: 9, Steps: 15 | Train Loss: 0.4512314 Vali Loss: 0.2539780 Test Loss: 0.3071087
Validation loss decreased (0.256683 --> 0.253978).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.848536968231201
Epoch: 10, Steps: 15 | Train Loss: 0.4506762 Vali Loss: 0.2515742 Test Loss: 0.3057098
Validation loss decreased (0.253978 --> 0.251574).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.877962350845337
Epoch: 11, Steps: 15 | Train Loss: 0.4457365 Vali Loss: 0.2484195 Test Loss: 0.3043779
Validation loss decreased (0.251574 --> 0.248420).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.6446425914764404
Epoch: 12, Steps: 15 | Train Loss: 0.4422690 Vali Loss: 0.2469060 Test Loss: 0.3034069
Validation loss decreased (0.248420 --> 0.246906).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.478261947631836
Epoch: 13, Steps: 15 | Train Loss: 0.4418130 Vali Loss: 0.2460344 Test Loss: 0.3025095
Validation loss decreased (0.246906 --> 0.246034).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.7195210456848145
Epoch: 14, Steps: 15 | Train Loss: 0.4399705 Vali Loss: 0.2453351 Test Loss: 0.3017427
Validation loss decreased (0.246034 --> 0.245335).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.679234743118286
Epoch: 15, Steps: 15 | Train Loss: 0.4359699 Vali Loss: 0.2423838 Test Loss: 0.3011455
Validation loss decreased (0.245335 --> 0.242384).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.9220962524414062
Epoch: 16, Steps: 15 | Train Loss: 0.4355037 Vali Loss: 0.2428897 Test Loss: 0.3006135
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.3106980323791504
Epoch: 17, Steps: 15 | Train Loss: 0.4354826 Vali Loss: 0.2404780 Test Loss: 0.3000148
Validation loss decreased (0.242384 --> 0.240478).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.115180730819702
Epoch: 18, Steps: 15 | Train Loss: 0.4345077 Vali Loss: 0.2389097 Test Loss: 0.2996086
Validation loss decreased (0.240478 --> 0.238910).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.273185968399048
Epoch: 19, Steps: 15 | Train Loss: 0.4338855 Vali Loss: 0.2391801 Test Loss: 0.2992335
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.93369460105896
Epoch: 20, Steps: 15 | Train Loss: 0.4317815 Vali Loss: 0.2380151 Test Loss: 0.2988978
Validation loss decreased (0.238910 --> 0.238015).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.8895256519317627
Epoch: 21, Steps: 15 | Train Loss: 0.4317578 Vali Loss: 0.2382303 Test Loss: 0.2985443
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.8803818225860596
Epoch: 22, Steps: 15 | Train Loss: 0.4271011 Vali Loss: 0.2367564 Test Loss: 0.2982555
Validation loss decreased (0.238015 --> 0.236756).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.8877999782562256
Epoch: 23, Steps: 15 | Train Loss: 0.4286341 Vali Loss: 0.2359335 Test Loss: 0.2979602
Validation loss decreased (0.236756 --> 0.235934).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.84114146232605
Epoch: 24, Steps: 15 | Train Loss: 0.4263628 Vali Loss: 0.2343941 Test Loss: 0.2977571
Validation loss decreased (0.235934 --> 0.234394).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.724785566329956
Epoch: 25, Steps: 15 | Train Loss: 0.4253648 Vali Loss: 0.2344224 Test Loss: 0.2974873
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.3588991165161133
Epoch: 26, Steps: 15 | Train Loss: 0.4281931 Vali Loss: 0.2352979 Test Loss: 0.2972582
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.308349847793579
Epoch: 27, Steps: 15 | Train Loss: 0.4254330 Vali Loss: 0.2348130 Test Loss: 0.2970954
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.2761528789997101, mae:0.34346628189086914, rse:0.4235025942325592, corr:[0.26857036 0.27632433 0.2758567  0.27415276 0.27448943 0.2742319
 0.27304628 0.27196848 0.27079907 0.26925004 0.26791054 0.26682064
 0.2655948  0.2640032  0.2629953  0.26252124 0.26185003 0.26061252
 0.25964525 0.25892767 0.25782114 0.25624755 0.25462365 0.25330555
 0.25181654 0.25003642 0.24847367 0.2473824  0.24611934 0.24420483
 0.24227443 0.24121965 0.24052677 0.23924802 0.23750111 0.23639826
 0.23593095 0.23505501 0.23352477 0.23234494 0.23199879 0.23159464
 0.230519   0.22952    0.22925772 0.22881715 0.22726183 0.22499281
 0.22334625 0.22215769 0.22053759 0.21861285 0.21782143 0.21749398
 0.2157916  0.21308091 0.21112974 0.21045665 0.20958206 0.20787811
 0.20660579 0.20668629 0.2071857  0.20690137 0.20592427 0.20607711
 0.20660524 0.20596902 0.20453443 0.20401162 0.204232   0.20366104
 0.20193246 0.20059206 0.20053986 0.19985181 0.19804761 0.1964713
 0.19659105 0.19665454 0.19586109 0.19461477 0.19426382 0.19463587
 0.19414447 0.19325607 0.19343048 0.19411437 0.19277725 0.19100924
 0.19152267 0.19203326 0.18957685 0.18617645 0.1880997  0.18466775]
