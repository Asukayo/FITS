Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  24393600.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8774279
	speed: 0.1281s/iter; left time: 704.4747s
Epoch: 1 cost time: 14.27875566482544
Epoch: 1, Steps: 112 | Train Loss: 0.7861721 Vali Loss: 0.7890747 Test Loss: 0.4447980
Validation loss decreased (inf --> 0.789075).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5768126
	speed: 0.2841s/iter; left time: 1531.1169s
Epoch: 2 cost time: 14.116970539093018
Epoch: 2, Steps: 112 | Train Loss: 0.6044342 Vali Loss: 0.7415332 Test Loss: 0.4173294
Validation loss decreased (0.789075 --> 0.741533).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4030527
	speed: 0.2752s/iter; left time: 1452.4104s
Epoch: 3 cost time: 13.065511226654053
Epoch: 3, Steps: 112 | Train Loss: 0.5466545 Vali Loss: 0.7188654 Test Loss: 0.4090752
Validation loss decreased (0.741533 --> 0.718865).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3993812
	speed: 0.2772s/iter; left time: 1431.7143s
Epoch: 4 cost time: 14.153200626373291
Epoch: 4, Steps: 112 | Train Loss: 0.5150781 Vali Loss: 0.7050391 Test Loss: 0.4044018
Validation loss decreased (0.718865 --> 0.705039).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4599559
	speed: 0.2995s/iter; left time: 1513.1703s
Epoch: 5 cost time: 15.815897941589355
Epoch: 5, Steps: 112 | Train Loss: 0.4932742 Vali Loss: 0.6945808 Test Loss: 0.4011207
Validation loss decreased (0.705039 --> 0.694581).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4972275
	speed: 0.3257s/iter; left time: 1609.2519s
Epoch: 6 cost time: 15.952881813049316
Epoch: 6, Steps: 112 | Train Loss: 0.4768650 Vali Loss: 0.6873272 Test Loss: 0.3982924
Validation loss decreased (0.694581 --> 0.687327).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3221693
	speed: 0.3091s/iter; left time: 1492.8434s
Epoch: 7 cost time: 15.613492012023926
Epoch: 7, Steps: 112 | Train Loss: 0.4645839 Vali Loss: 0.6870276 Test Loss: 0.3960378
Validation loss decreased (0.687327 --> 0.687028).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3463395
	speed: 0.3262s/iter; left time: 1538.6713s
Epoch: 8 cost time: 16.705519437789917
Epoch: 8, Steps: 112 | Train Loss: 0.4554856 Vali Loss: 0.6777457 Test Loss: 0.3940231
Validation loss decreased (0.687028 --> 0.677746).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4043032
	speed: 0.3341s/iter; left time: 1538.6294s
Epoch: 9 cost time: 16.330181121826172
Epoch: 9, Steps: 112 | Train Loss: 0.4477847 Vali Loss: 0.6763653 Test Loss: 0.3924552
Validation loss decreased (0.677746 --> 0.676365).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5444850
	speed: 0.3167s/iter; left time: 1422.9275s
Epoch: 10 cost time: 15.515769243240356
Epoch: 10, Steps: 112 | Train Loss: 0.4430340 Vali Loss: 0.6720127 Test Loss: 0.3909910
Validation loss decreased (0.676365 --> 0.672013).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3992883
	speed: 0.3329s/iter; left time: 1458.4007s
Epoch: 11 cost time: 17.516237258911133
Epoch: 11, Steps: 112 | Train Loss: 0.4377114 Vali Loss: 0.6678295 Test Loss: 0.3898771
Validation loss decreased (0.672013 --> 0.667829).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4159840
	speed: 0.3409s/iter; left time: 1455.2130s
Epoch: 12 cost time: 16.403332233428955
Epoch: 12, Steps: 112 | Train Loss: 0.4342626 Vali Loss: 0.6673188 Test Loss: 0.3888099
Validation loss decreased (0.667829 --> 0.667319).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3155888
	speed: 0.3069s/iter; left time: 1275.8618s
Epoch: 13 cost time: 16.156185626983643
Epoch: 13, Steps: 112 | Train Loss: 0.4301815 Vali Loss: 0.6618542 Test Loss: 0.3880239
Validation loss decreased (0.667319 --> 0.661854).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5134985
	speed: 0.3351s/iter; left time: 1355.3903s
Epoch: 14 cost time: 17.264410734176636
Epoch: 14, Steps: 112 | Train Loss: 0.4278091 Vali Loss: 0.6600754 Test Loss: 0.3873805
Validation loss decreased (0.661854 --> 0.660075).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4823591
	speed: 0.3310s/iter; left time: 1301.9449s
Epoch: 15 cost time: 15.783610343933105
Epoch: 15, Steps: 112 | Train Loss: 0.4268158 Vali Loss: 0.6588843 Test Loss: 0.3867173
Validation loss decreased (0.660075 --> 0.658884).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4434700
	speed: 0.3115s/iter; left time: 1190.2039s
Epoch: 16 cost time: 15.589738607406616
Epoch: 16, Steps: 112 | Train Loss: 0.4248139 Vali Loss: 0.6609045 Test Loss: 0.3863394
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4472061
	speed: 0.3234s/iter; left time: 1199.6680s
Epoch: 17 cost time: 16.27295732498169
Epoch: 17, Steps: 112 | Train Loss: 0.4230956 Vali Loss: 0.6556786 Test Loss: 0.3858232
Validation loss decreased (0.658884 --> 0.655679).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3886490
	speed: 0.3125s/iter; left time: 1123.9035s
Epoch: 18 cost time: 14.855594635009766
Epoch: 18, Steps: 112 | Train Loss: 0.4219449 Vali Loss: 0.6574236 Test Loss: 0.3854792
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4184704
	speed: 0.2972s/iter; left time: 1035.8387s
Epoch: 19 cost time: 15.060791015625
Epoch: 19, Steps: 112 | Train Loss: 0.4209737 Vali Loss: 0.6544876 Test Loss: 0.3851459
Validation loss decreased (0.655679 --> 0.654488).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3312370
	speed: 0.3250s/iter; left time: 1096.3745s
Epoch: 20 cost time: 16.546086311340332
Epoch: 20, Steps: 112 | Train Loss: 0.4197869 Vali Loss: 0.6526524 Test Loss: 0.3849113
Validation loss decreased (0.654488 --> 0.652652).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3863314
	speed: 0.3141s/iter; left time: 1024.4276s
Epoch: 21 cost time: 15.216081380844116
Epoch: 21, Steps: 112 | Train Loss: 0.4185958 Vali Loss: 0.6544055 Test Loss: 0.3846292
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4635334
	speed: 0.3084s/iter; left time: 971.1299s
Epoch: 22 cost time: 15.478704690933228
Epoch: 22, Steps: 112 | Train Loss: 0.4189168 Vali Loss: 0.6516300 Test Loss: 0.3844728
Validation loss decreased (0.652652 --> 0.651630).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3625047
	speed: 0.3367s/iter; left time: 1022.6317s
Epoch: 23 cost time: 17.026258945465088
Epoch: 23, Steps: 112 | Train Loss: 0.4172797 Vali Loss: 0.6529364 Test Loss: 0.3842664
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3788775
	speed: 0.3137s/iter; left time: 917.6608s
Epoch: 24 cost time: 15.610795497894287
Epoch: 24, Steps: 112 | Train Loss: 0.4179461 Vali Loss: 0.6531664 Test Loss: 0.3841132
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4295098
	speed: 0.3084s/iter; left time: 867.5614s
Epoch: 25 cost time: 15.871287822723389
Epoch: 25, Steps: 112 | Train Loss: 0.4170838 Vali Loss: 0.6500207 Test Loss: 0.3839094
Validation loss decreased (0.651630 --> 0.650021).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3166634
	speed: 0.3176s/iter; left time: 857.9268s
Epoch: 26 cost time: 15.333999395370483
Epoch: 26, Steps: 112 | Train Loss: 0.4170277 Vali Loss: 0.6523263 Test Loss: 0.3837798
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4573180
	speed: 0.3074s/iter; left time: 795.8293s
Epoch: 27 cost time: 14.776129245758057
Epoch: 27, Steps: 112 | Train Loss: 0.4163594 Vali Loss: 0.6488820 Test Loss: 0.3836631
Validation loss decreased (0.650021 --> 0.648882).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3816458
	speed: 0.3096s/iter; left time: 766.9559s
Epoch: 28 cost time: 16.322404861450195
Epoch: 28, Steps: 112 | Train Loss: 0.4159761 Vali Loss: 0.6504291 Test Loss: 0.3835450
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3692670
	speed: 0.3312s/iter; left time: 783.3364s
Epoch: 29 cost time: 16.32885503768921
Epoch: 29, Steps: 112 | Train Loss: 0.4156316 Vali Loss: 0.6504598 Test Loss: 0.3834061
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.5879996
	speed: 0.3082s/iter; left time: 694.4344s
Epoch: 30 cost time: 15.447025060653687
Epoch: 30, Steps: 112 | Train Loss: 0.4154385 Vali Loss: 0.6521652 Test Loss: 0.3833185
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  24393600.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8158271
	speed: 0.1430s/iter; left time: 786.7834s
Epoch: 1 cost time: 16.150099992752075
Epoch: 1, Steps: 112 | Train Loss: 0.8047725 Vali Loss: 0.6461466 Test Loss: 0.3817099
Validation loss decreased (inf --> 0.646147).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6110247
	speed: 0.3291s/iter; left time: 1773.4086s
Epoch: 2 cost time: 16.0478675365448
Epoch: 2, Steps: 112 | Train Loss: 0.8022920 Vali Loss: 0.6443281 Test Loss: 0.3809178
Validation loss decreased (0.646147 --> 0.644328).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.7335080
	speed: 0.3118s/iter; left time: 1645.2423s
Epoch: 3 cost time: 15.538692235946655
Epoch: 3, Steps: 112 | Train Loss: 0.8027860 Vali Loss: 0.6414065 Test Loss: 0.3806548
Validation loss decreased (0.644328 --> 0.641406).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5561848
	speed: 0.3044s/iter; left time: 1572.1080s
Epoch: 4 cost time: 16.13805890083313
Epoch: 4, Steps: 112 | Train Loss: 0.8019244 Vali Loss: 0.6425490 Test Loss: 0.3806885
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.8969258
	speed: 0.3542s/iter; left time: 1789.8210s
Epoch: 5 cost time: 16.547693490982056
Epoch: 5, Steps: 112 | Train Loss: 0.7990129 Vali Loss: 0.6431575 Test Loss: 0.3806618
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.7206087
	speed: 0.3169s/iter; left time: 1565.7729s
Epoch: 6 cost time: 14.767126560211182
Epoch: 6, Steps: 112 | Train Loss: 0.8007436 Vali Loss: 0.6416435 Test Loss: 0.3805977
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.37912076711654663, mae:0.42374593019485474, rse:0.4921466112136841, corr:[ 2.21123248e-01  2.22140461e-01  2.20982835e-01  2.19442233e-01
  2.18963623e-01  2.18761340e-01  2.17747077e-01  2.16489673e-01
  2.15501755e-01  2.14598551e-01  2.13305295e-01  2.11496562e-01
  2.09848166e-01  2.08824933e-01  2.08304539e-01  2.07848489e-01
  2.07167447e-01  2.06131130e-01  2.04938293e-01  2.03889504e-01
  2.03105897e-01  2.02463776e-01  2.01406419e-01  1.99944437e-01
  1.98335096e-01  1.96922168e-01  1.95747733e-01  1.94910154e-01
  1.94407701e-01  1.93889558e-01  1.93246782e-01  1.92520782e-01
  1.91937730e-01  1.91222638e-01  1.90173775e-01  1.88981205e-01
  1.88037619e-01  1.87530681e-01  1.87092498e-01  1.86502412e-01
  1.85871184e-01  1.85393825e-01  1.85022771e-01  1.84549615e-01
  1.83713973e-01  1.82548866e-01  1.81145355e-01  1.79484338e-01
  1.78076297e-01  1.76842347e-01  1.75359815e-01  1.73772261e-01
  1.72902197e-01  1.72785506e-01  1.72670230e-01  1.72111109e-01
  1.71179533e-01  1.70542568e-01  1.70531332e-01  1.70740470e-01
  1.70605198e-01  1.70130268e-01  1.70078993e-01  1.70604393e-01
  1.70765668e-01  1.70534536e-01  1.70212612e-01  1.70174763e-01
  1.70294777e-01  1.70195550e-01  1.69796318e-01  1.69368953e-01
  1.69091895e-01  1.68667734e-01  1.67996883e-01  1.67178303e-01
  1.66707188e-01  1.66613191e-01  1.66688636e-01  1.67066544e-01
  1.67813122e-01  1.68137699e-01  1.67535037e-01  1.66648820e-01
  1.66297600e-01  1.66541442e-01  1.66786596e-01  1.66872084e-01
  1.67085171e-01  1.67531446e-01  1.67669833e-01  1.67144835e-01
  1.66712478e-01  1.66797087e-01  1.67329028e-01  1.67689040e-01
  1.67735845e-01  1.67837247e-01  1.67956099e-01  1.67748898e-01
  1.67326599e-01  1.66954190e-01  1.67137966e-01  1.67409554e-01
  1.67295679e-01  1.66743129e-01  1.66483626e-01  1.66677266e-01
  1.66580290e-01  1.66019097e-01  1.65663913e-01  1.66043147e-01
  1.66654915e-01  1.66886747e-01  1.66546583e-01  1.66140720e-01
  1.65962785e-01  1.65608495e-01  1.65035725e-01  1.64322004e-01
  1.63677186e-01  1.63018674e-01  1.62057206e-01  1.60894498e-01
  1.59975514e-01  1.59706116e-01  1.59637660e-01  1.59208983e-01
  1.58548191e-01  1.57917425e-01  1.57539606e-01  1.56914875e-01
  1.56168461e-01  1.55619636e-01  1.55509874e-01  1.55346215e-01
  1.54267639e-01  1.52723283e-01  1.51696369e-01  1.51758894e-01
  1.52356789e-01  1.52222022e-01  1.51152372e-01  1.49798051e-01
  1.48741707e-01  1.48322880e-01  1.47825673e-01  1.46917105e-01
  1.46192163e-01  1.46222308e-01  1.46583483e-01  1.46231383e-01
  1.45222649e-01  1.44329131e-01  1.44154638e-01  1.44184813e-01
  1.43635273e-01  1.42498821e-01  1.41541049e-01  1.41228870e-01
  1.41157925e-01  1.41099036e-01  1.41074464e-01  1.41400933e-01
  1.42168030e-01  1.42635331e-01  1.42406762e-01  1.41667381e-01
  1.41164541e-01  1.41005993e-01  1.40784025e-01  1.40293345e-01
  1.39948651e-01  1.39770716e-01  1.39245167e-01  1.37947246e-01
  1.36389345e-01  1.35473251e-01  1.35260776e-01  1.35313645e-01
  1.35125831e-01  1.34898126e-01  1.35156408e-01  1.35596469e-01
  1.35610878e-01  1.35282680e-01  1.35043100e-01  1.35378331e-01
  1.36145681e-01  1.36666998e-01  1.36760950e-01  1.36602983e-01
  1.36661798e-01  1.37003422e-01  1.36890784e-01  1.36253700e-01
  1.35854334e-01  1.36158362e-01  1.37000352e-01  1.37329876e-01
  1.37048349e-01  1.36746645e-01  1.37019247e-01  1.37118995e-01
  1.36730254e-01  1.36488974e-01  1.36946633e-01  1.37951180e-01
  1.38649642e-01  1.38663888e-01  1.38416767e-01  1.38407007e-01
  1.38578400e-01  1.38388276e-01  1.37882695e-01  1.37483224e-01
  1.37499511e-01  1.37766138e-01  1.38083413e-01  1.38446018e-01
  1.38470426e-01  1.38265520e-01  1.37902826e-01  1.38048649e-01
  1.38382062e-01  1.38254538e-01  1.37574285e-01  1.36926204e-01
  1.37008056e-01  1.37463853e-01  1.37718931e-01  1.37904406e-01
  1.38608456e-01  1.39951214e-01  1.41096443e-01  1.41375199e-01
  1.41343534e-01  1.41794428e-01  1.42703116e-01  1.43245488e-01
  1.43074632e-01  1.42586976e-01  1.42481521e-01  1.43098623e-01
  1.44306049e-01  1.45893022e-01  1.47402793e-01  1.47649795e-01
  1.46827891e-01  1.46284223e-01  1.47345901e-01  1.49298996e-01
  1.50162190e-01  1.49442062e-01  1.48583278e-01  1.49171889e-01
  1.50639907e-01  1.51833847e-01  1.52424604e-01  1.53097153e-01
  1.53895393e-01  1.54278055e-01  1.54159084e-01  1.53972670e-01
  1.54337138e-01  1.55027106e-01  1.55720234e-01  1.56436965e-01
  1.57342866e-01  1.58565491e-01  1.59544200e-01  1.59761935e-01
  1.59429759e-01  1.59119949e-01  1.59578174e-01  1.60709679e-01
  1.61790639e-01  1.62247047e-01  1.61885977e-01  1.61696583e-01
  1.62637472e-01  1.64311171e-01  1.65286064e-01  1.65054142e-01
  1.64397135e-01  1.64381042e-01  1.64897040e-01  1.64880350e-01
  1.64442122e-01  1.64582118e-01  1.65478453e-01  1.66153356e-01
  1.65579170e-01  1.64708704e-01  1.65125221e-01  1.66624010e-01
  1.67287961e-01  1.66289464e-01  1.65031701e-01  1.65173277e-01
  1.66323692e-01  1.67126790e-01  1.67003036e-01  1.66921079e-01
  1.67669460e-01  1.68849126e-01  1.69099540e-01  1.68516845e-01
  1.67860270e-01  1.67941988e-01  1.68656901e-01  1.69319078e-01
  1.69627339e-01  1.69448301e-01  1.69314936e-01  1.69389650e-01
  1.69670448e-01  1.69693500e-01  1.68976739e-01  1.67925388e-01
  1.67424053e-01  1.68045849e-01  1.68681681e-01  1.68193221e-01
  1.66900262e-01  1.66635379e-01  1.68221131e-01  1.69841349e-01
  1.69819891e-01  1.68587551e-01  1.67859510e-01  1.68713495e-01
  1.69911921e-01  1.70211688e-01  1.70030519e-01  1.70437440e-01
  1.71590462e-01  1.72683686e-01  1.72838271e-01  1.71964392e-01
  1.71354964e-01  1.71648607e-01  1.72557756e-01  1.73104584e-01
  1.73037156e-01  1.72810718e-01  1.72740430e-01  1.73201904e-01
  1.73785478e-01  1.74293593e-01  1.74705297e-01  1.75059944e-01
  1.75418332e-01  1.76133364e-01  1.77193865e-01  1.78461701e-01
  1.79249018e-01  1.79176673e-01  1.78445488e-01  1.77751258e-01
  1.77827284e-01  1.78749576e-01  1.80067271e-01  1.81130439e-01
  1.81753024e-01  1.82072565e-01  1.82587832e-01  1.83453664e-01
  1.84041113e-01  1.83974147e-01  1.83004603e-01  1.81973904e-01
  1.81657523e-01  1.82103053e-01  1.82648718e-01  1.82758138e-01
  1.82651177e-01  1.82703853e-01  1.82792723e-01  1.82556435e-01
  1.81817353e-01  1.80924311e-01  1.80473492e-01  1.80699348e-01
  1.81053802e-01  1.81101412e-01  1.81131676e-01  1.81566939e-01
  1.82494611e-01  1.83529809e-01  1.84161678e-01  1.84507474e-01
  1.85051054e-01  1.85909897e-01  1.86620995e-01  1.86349288e-01
  1.85203984e-01  1.83942690e-01  1.83968335e-01  1.85120821e-01
  1.85811013e-01  1.85373396e-01  1.84592649e-01  1.84494644e-01
  1.84706241e-01  1.84275180e-01  1.83008462e-01  1.82019725e-01
  1.82168409e-01  1.83066383e-01  1.83476314e-01  1.82969525e-01
  1.82398424e-01  1.82347849e-01  1.82393909e-01  1.81978732e-01
  1.81359708e-01  1.81387901e-01  1.81941956e-01  1.82169497e-01
  1.81326404e-01  1.80300876e-01  1.80191681e-01  1.80497989e-01
  1.79735452e-01  1.77650109e-01  1.75983399e-01  1.75807431e-01
  1.76298574e-01  1.75856873e-01  1.74238294e-01  1.72969788e-01
  1.72657713e-01  1.72530591e-01  1.71968818e-01  1.71236068e-01
  1.71249062e-01  1.71703219e-01  1.71517447e-01  1.70621827e-01
  1.69815555e-01  1.69823393e-01  1.69379741e-01  1.67619839e-01
  1.65350437e-01  1.64794505e-01  1.66027233e-01  1.67305946e-01
  1.66934371e-01  1.65342227e-01  1.64229184e-01  1.64343387e-01
  1.64708823e-01  1.64857343e-01  1.65034741e-01  1.65313214e-01
  1.65301248e-01  1.64589226e-01  1.63573876e-01  1.63049698e-01
  1.62839845e-01  1.62443355e-01  1.61856264e-01  1.61514014e-01
  1.61248237e-01  1.61194757e-01  1.61121786e-01  1.60787597e-01
  1.60549268e-01  1.60339311e-01  1.60210773e-01  1.59896165e-01
  1.59246475e-01  1.58287808e-01  1.57437146e-01  1.56881332e-01
  1.56519517e-01  1.56328887e-01  1.56118900e-01  1.55987605e-01
  1.55982867e-01  1.55958697e-01  1.55462921e-01  1.54069841e-01
  1.52500734e-01  1.51428476e-01  1.51006356e-01  1.50685668e-01
  1.50182784e-01  1.49491951e-01  1.48903355e-01  1.48292631e-01
  1.47537768e-01  1.46756157e-01  1.46103755e-01  1.45607740e-01
  1.45093128e-01  1.44703716e-01  1.44181296e-01  1.43367037e-01
  1.42782822e-01  1.42840624e-01  1.43161014e-01  1.42695785e-01
  1.41149968e-01  1.39563128e-01  1.39027998e-01  1.39386728e-01
  1.39568344e-01  1.38620242e-01  1.36463538e-01  1.34680718e-01
  1.34223461e-01  1.34428814e-01  1.34133354e-01  1.32962540e-01
  1.32108748e-01  1.32322848e-01  1.32963181e-01  1.32511079e-01
  1.30727872e-01  1.28811494e-01  1.27624944e-01  1.27231807e-01
  1.26923651e-01  1.26368463e-01  1.25486270e-01  1.24346435e-01
  1.22739561e-01  1.20952278e-01  1.19567849e-01  1.18587777e-01
  1.17985062e-01  1.17258705e-01  1.16302691e-01  1.15214437e-01
  1.13991238e-01  1.12484284e-01  1.10787325e-01  1.09523058e-01
  1.09323546e-01  1.09867029e-01  1.10189185e-01  1.09430790e-01
  1.07635438e-01  1.06016681e-01  1.05079241e-01  1.04968362e-01
  1.05162397e-01  1.04902901e-01  1.04142934e-01  1.02612808e-01
  1.00675069e-01  9.87739339e-02  9.75222215e-02  9.69128609e-02
  9.63267013e-02  9.51157659e-02  9.34514478e-02  9.23081264e-02
  9.28299129e-02  9.38339233e-02  9.37344804e-02  9.18768197e-02
  8.98003951e-02  8.93681198e-02  9.05096903e-02  9.10998583e-02
  9.00640115e-02  8.80430713e-02  8.65085274e-02  8.60420913e-02
  8.57589990e-02  8.48938748e-02  8.35280791e-02  8.22554678e-02
  8.10301155e-02  7.97982737e-02  7.85340518e-02  7.77093321e-02
  7.73070902e-02  7.65551180e-02  7.51160681e-02  7.36970529e-02
  7.31440187e-02  7.32115060e-02  7.27838948e-02  7.16370493e-02
  7.03811646e-02  6.98085278e-02  6.96375892e-02  6.90862313e-02
  6.82182088e-02  6.74691498e-02  6.70216605e-02  6.69353455e-02
  6.67360798e-02  6.61195666e-02  6.47026598e-02  6.26988336e-02
  6.09277673e-02  6.04348853e-02  6.07904755e-02  6.10140227e-02
  5.97511604e-02  5.73397391e-02  5.48842847e-02  5.35263494e-02
  5.31753451e-02  5.32212444e-02  5.29985838e-02  5.18028326e-02
  5.00481054e-02  4.84594479e-02  4.76680323e-02  4.75874320e-02
  4.75879759e-02  4.76127528e-02  4.76541668e-02  4.82516885e-02
  4.92341593e-02  4.99536879e-02  4.94042449e-02  4.80095334e-02
  4.63674255e-02  4.51045819e-02  4.43957224e-02  4.39780317e-02
  4.35534939e-02  4.30914201e-02  4.24089916e-02  4.16045748e-02
  4.10025939e-02  4.06688377e-02  4.01594117e-02  3.97438630e-02
  3.96488123e-02  3.98071893e-02  4.01234441e-02  4.01313044e-02
  3.93285416e-02  3.86369452e-02  3.82862911e-02  3.76758240e-02
  3.67795303e-02  3.60843763e-02  3.55157480e-02  3.50343660e-02
  3.42452861e-02  3.31828259e-02  3.22263390e-02  3.14839892e-02
  3.11232042e-02  3.06113586e-02  2.97917333e-02  2.88879629e-02
  2.79171579e-02  2.69620959e-02  2.64883060e-02  2.69222707e-02
  2.78243199e-02  2.84084361e-02  2.80698929e-02  2.67965533e-02
  2.57900115e-02  2.58079898e-02  2.66830027e-02  2.76302379e-02
  2.78531257e-02  2.77759954e-02  2.75920499e-02  2.72205230e-02
  2.64617652e-02  2.56372597e-02  2.51193438e-02  2.48044748e-02
  2.52026021e-02  2.56501064e-02  2.59007942e-02  2.48801485e-02
  2.35869084e-02  2.28958372e-02  2.28917543e-02  2.28929203e-02
  2.23553572e-02  2.09636763e-02  2.03024931e-02  2.07872260e-02
  2.13802140e-02  2.09102985e-02  1.93083081e-02  1.73733830e-02
  1.69899389e-02  1.77314114e-02  1.81885958e-02  1.74000859e-02
  1.58877671e-02  1.39698237e-02  1.18504902e-02  9.57348756e-03
  8.41745455e-03  8.36392120e-03  8.78331251e-03  8.14927369e-03
  6.89822854e-03  5.54343639e-03  3.47115658e-03  1.25143863e-03
  2.53696431e-04  1.07242039e-03  2.97715864e-03  2.98072281e-03
  9.06641828e-04 -2.14037194e-04  1.40398846e-03  3.75867565e-03
  3.69811477e-03  3.03124008e-03  5.98531403e-03  1.17791379e-02]
