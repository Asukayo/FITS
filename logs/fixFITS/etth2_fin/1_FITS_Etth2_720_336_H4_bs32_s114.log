Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11766272.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8054177
	speed: 0.1244s/iter; left time: 721.7128s
Epoch: 1 cost time: 14.471531629562378
Epoch: 1, Steps: 118 | Train Loss: 0.7749959 Vali Loss: 0.4603243 Test Loss: 0.3724247
Validation loss decreased (inf --> 0.460324).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7510716
	speed: 0.2799s/iter; left time: 1590.8725s
Epoch: 2 cost time: 12.763300895690918
Epoch: 2, Steps: 118 | Train Loss: 0.6641545 Vali Loss: 0.4221749 Test Loss: 0.3628975
Validation loss decreased (0.460324 --> 0.422175).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6000821
	speed: 0.2662s/iter; left time: 1481.1550s
Epoch: 3 cost time: 12.822257995605469
Epoch: 3, Steps: 118 | Train Loss: 0.6446383 Vali Loss: 0.4064540 Test Loss: 0.3609401
Validation loss decreased (0.422175 --> 0.406454).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6411753
	speed: 0.3047s/iter; left time: 1659.8946s
Epoch: 4 cost time: 14.704522371292114
Epoch: 4, Steps: 118 | Train Loss: 0.6346252 Vali Loss: 0.4012955 Test Loss: 0.3601561
Validation loss decreased (0.406454 --> 0.401296).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6342648
	speed: 0.3227s/iter; left time: 1719.4154s
Epoch: 5 cost time: 15.479659795761108
Epoch: 5, Steps: 118 | Train Loss: 0.6306011 Vali Loss: 0.3967778 Test Loss: 0.3603920
Validation loss decreased (0.401296 --> 0.396778).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5525013
	speed: 0.3171s/iter; left time: 1652.6084s
Epoch: 6 cost time: 15.428219318389893
Epoch: 6, Steps: 118 | Train Loss: 0.6285946 Vali Loss: 0.3940493 Test Loss: 0.3596194
Validation loss decreased (0.396778 --> 0.394049).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.7676384
	speed: 0.3179s/iter; left time: 1619.1919s
Epoch: 7 cost time: 14.768786907196045
Epoch: 7, Steps: 118 | Train Loss: 0.6258765 Vali Loss: 0.3915696 Test Loss: 0.3598130
Validation loss decreased (0.394049 --> 0.391570).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.7189677
	speed: 0.3200s/iter; left time: 1591.9387s
Epoch: 8 cost time: 15.152681589126587
Epoch: 8, Steps: 118 | Train Loss: 0.6237513 Vali Loss: 0.3886216 Test Loss: 0.3595300
Validation loss decreased (0.391570 --> 0.388622).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5438358
	speed: 0.3116s/iter; left time: 1513.3798s
Epoch: 9 cost time: 14.767962217330933
Epoch: 9, Steps: 118 | Train Loss: 0.6206683 Vali Loss: 0.3872536 Test Loss: 0.3595996
Validation loss decreased (0.388622 --> 0.387254).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6821526
	speed: 0.3043s/iter; left time: 1441.8512s
Epoch: 10 cost time: 14.751684665679932
Epoch: 10, Steps: 118 | Train Loss: 0.6206656 Vali Loss: 0.3859987 Test Loss: 0.3594424
Validation loss decreased (0.387254 --> 0.385999).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5985946
	speed: 0.3055s/iter; left time: 1411.8116s
Epoch: 11 cost time: 14.667978763580322
Epoch: 11, Steps: 118 | Train Loss: 0.6182427 Vali Loss: 0.3839224 Test Loss: 0.3594019
Validation loss decreased (0.385999 --> 0.383922).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5369210
	speed: 0.3108s/iter; left time: 1399.6694s
Epoch: 12 cost time: 15.096644401550293
Epoch: 12, Steps: 118 | Train Loss: 0.6188533 Vali Loss: 0.3845876 Test Loss: 0.3593592
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5643203
	speed: 0.3163s/iter; left time: 1386.8288s
Epoch: 13 cost time: 14.939496755599976
Epoch: 13, Steps: 118 | Train Loss: 0.6178597 Vali Loss: 0.3834961 Test Loss: 0.3594411
Validation loss decreased (0.383922 --> 0.383496).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5163654
	speed: 0.3127s/iter; left time: 1334.4117s
Epoch: 14 cost time: 15.001258134841919
Epoch: 14, Steps: 118 | Train Loss: 0.6177008 Vali Loss: 0.3819058 Test Loss: 0.3595037
Validation loss decreased (0.383496 --> 0.381906).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.9710317
	speed: 0.3129s/iter; left time: 1298.1697s
Epoch: 15 cost time: 14.88103723526001
Epoch: 15, Steps: 118 | Train Loss: 0.6181985 Vali Loss: 0.3814532 Test Loss: 0.3593377
Validation loss decreased (0.381906 --> 0.381453).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5425152
	speed: 0.3448s/iter; left time: 1389.9958s
Epoch: 16 cost time: 16.844454526901245
Epoch: 16, Steps: 118 | Train Loss: 0.6170303 Vali Loss: 0.3814736 Test Loss: 0.3591475
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4967183
	speed: 0.3598s/iter; left time: 1407.8557s
Epoch: 17 cost time: 17.52990198135376
Epoch: 17, Steps: 118 | Train Loss: 0.6162916 Vali Loss: 0.3806957 Test Loss: 0.3593200
Validation loss decreased (0.381453 --> 0.380696).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.7669497
	speed: 0.3455s/iter; left time: 1311.2047s
Epoch: 18 cost time: 15.716123104095459
Epoch: 18, Steps: 118 | Train Loss: 0.6163060 Vali Loss: 0.3781097 Test Loss: 0.3593901
Validation loss decreased (0.380696 --> 0.378110).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.7102453
	speed: 0.3161s/iter; left time: 1162.4662s
Epoch: 19 cost time: 15.27527141571045
Epoch: 19, Steps: 118 | Train Loss: 0.6164231 Vali Loss: 0.3813437 Test Loss: 0.3593441
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.6848539
	speed: 0.3055s/iter; left time: 1087.2333s
Epoch: 20 cost time: 14.972525119781494
Epoch: 20, Steps: 118 | Train Loss: 0.6140669 Vali Loss: 0.3814618 Test Loss: 0.3592618
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5226276
	speed: 0.2693s/iter; left time: 926.5479s
Epoch: 21 cost time: 12.515320062637329
Epoch: 21, Steps: 118 | Train Loss: 0.6119172 Vali Loss: 0.3802028 Test Loss: 0.3593810
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.35474786162376404, mae:0.3960188627243042, rse:0.4762110114097595, corr:[0.2581032  0.26434934 0.26253712 0.26080677 0.26133114 0.2621138
 0.26110706 0.25899914 0.25777027 0.25735474 0.25682008 0.25545266
 0.25392243 0.25282666 0.25229388 0.2519127  0.2512444  0.25018093
 0.24919315 0.24840526 0.24751739 0.24629161 0.24470943 0.24335866
 0.24237908 0.2416068  0.24051309 0.23913468 0.23796906 0.23722962
 0.23667751 0.23596925 0.2350496  0.23411438 0.2333727  0.23271015
 0.23196271 0.2311388  0.23028411 0.22951095 0.22882418 0.2280595
 0.22722322 0.22649278 0.2257657  0.22488207 0.22349273 0.22157939
 0.21962589 0.2180736  0.21679637 0.21548374 0.2141254  0.21265127
 0.21094394 0.20928971 0.20771123 0.20616023 0.20488119 0.20401809
 0.20349607 0.20299383 0.20247816 0.20204347 0.20149927 0.20110263
 0.20075732 0.20060024 0.20046894 0.20021966 0.19942392 0.19808695
 0.19654128 0.19525759 0.19442953 0.1937361  0.1930506  0.19228683
 0.19168766 0.19139452 0.1914534  0.1913316  0.19081132 0.19015397
 0.18982366 0.1898257  0.18978482 0.18941419 0.18875232 0.18820544
 0.18809462 0.1881311  0.18819076 0.1880222  0.18761799 0.18711866
 0.18660535 0.18591274 0.18502146 0.18392986 0.18311392 0.1828252
 0.18303938 0.18304281 0.18264158 0.18185806 0.18132266 0.18118136
 0.1810867  0.18087047 0.18026176 0.17950435 0.1787201  0.17824312
 0.17791785 0.17756459 0.17714536 0.17646427 0.17563657 0.17446813
 0.17297278 0.17136392 0.1700604  0.16929193 0.16890727 0.16877146
 0.1685325  0.16819173 0.16774629 0.16708857 0.16619478 0.1649965
 0.16385302 0.16302088 0.16270733 0.16265836 0.16232868 0.16153535
 0.16049671 0.15971021 0.15939657 0.15917587 0.15851155 0.1570637
 0.1550682  0.15323694 0.15189597 0.15096419 0.15013404 0.149316
 0.14846092 0.14748926 0.14662874 0.14579481 0.14507332 0.14460672
 0.14462808 0.14490578 0.1449972  0.14455019 0.14355911 0.14273718
 0.14242192 0.14247118 0.14245407 0.14197634 0.14108995 0.13995977
 0.13901003 0.13813072 0.13715638 0.13594429 0.13470472 0.13348572
 0.13260414 0.13168779 0.13073663 0.12976784 0.12870458 0.12794794
 0.12766454 0.12781753 0.12811723 0.12827009 0.12782368 0.127259
 0.12691781 0.12685874 0.12704474 0.1271398  0.12717398 0.12686116
 0.12658137 0.1266788  0.12689961 0.12690653 0.12613772 0.12453536
 0.12277982 0.12157241 0.12136171 0.12161771 0.12160141 0.12091529
 0.11995816 0.11957463 0.11980715 0.12039636 0.12065857 0.12050728
 0.12024815 0.12050246 0.12123205 0.12172965 0.12139739 0.12006287
 0.11843818 0.11733857 0.11705675 0.11738168 0.11755917 0.11760844
 0.11733401 0.11732027 0.11748406 0.11765175 0.11760712 0.11713614
 0.11631773 0.11553624 0.1149473  0.11496975 0.11542389 0.11629593
 0.11723007 0.11800865 0.118393   0.11838587 0.11802615 0.11739061
 0.11659463 0.11581305 0.11519541 0.11490112 0.11445677 0.11365529
 0.11308006 0.11297332 0.11349402 0.1142679  0.11487566 0.11518297
 0.11509126 0.11533952 0.11608274 0.11771178 0.11914686 0.12008491
 0.11996301 0.11943126 0.11939589 0.12008366 0.12168217 0.12304243
 0.12348043 0.12276153 0.1219095  0.12169059 0.12187319 0.12200644
 0.12164873 0.12097901 0.12070211 0.12068675 0.12130389 0.12164741
 0.12164938 0.12156459 0.12185159 0.12254386 0.12306695 0.12282526
 0.12188775 0.12115564 0.12107629 0.12151535 0.12208251 0.12229142
 0.12207618 0.12176518 0.12144525 0.12104128 0.12010515 0.11864615
 0.11770104 0.11774296 0.11842797 0.11868553 0.11833464 0.11760972
 0.11686673 0.11690805 0.11740746 0.11812212 0.11819889 0.11854102
 0.11866529 0.11877114 0.11863883 0.11859679 0.11897249 0.11953855
 0.11996754 0.11933873 0.11827725 0.11728636 0.11678476 0.11649236
 0.11602386 0.11553546 0.11522462 0.11612935 0.11741325 0.11807847
 0.11721865 0.11653005 0.11797766 0.12072846 0.12198702 0.12047158
 0.11810723 0.11814352 0.121138   0.12317068 0.12160125 0.11563334]
