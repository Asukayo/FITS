Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30898560.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.056097507476807
Epoch: 1, Steps: 60 | Train Loss: 0.6130338 Vali Loss: 0.4870854 Test Loss: 0.4541455
Validation loss decreased (inf --> 0.487085).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.277347326278687
Epoch: 2, Steps: 60 | Train Loss: 0.4770140 Vali Loss: 0.4287425 Test Loss: 0.4148690
Validation loss decreased (0.487085 --> 0.428742).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 6.845608472824097
Epoch: 3, Steps: 60 | Train Loss: 0.4072249 Vali Loss: 0.4010244 Test Loss: 0.3988516
Validation loss decreased (0.428742 --> 0.401024).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 7.1039440631866455
Epoch: 4, Steps: 60 | Train Loss: 0.3670614 Vali Loss: 0.3862106 Test Loss: 0.3912962
Validation loss decreased (0.401024 --> 0.386211).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.056954860687256
Epoch: 5, Steps: 60 | Train Loss: 0.3381395 Vali Loss: 0.3771737 Test Loss: 0.3877490
Validation loss decreased (0.386211 --> 0.377174).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.547428369522095
Epoch: 6, Steps: 60 | Train Loss: 0.3159707 Vali Loss: 0.3704432 Test Loss: 0.3853781
Validation loss decreased (0.377174 --> 0.370443).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.333272695541382
Epoch: 7, Steps: 60 | Train Loss: 0.2973073 Vali Loss: 0.3655090 Test Loss: 0.3836569
Validation loss decreased (0.370443 --> 0.365509).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.118235111236572
Epoch: 8, Steps: 60 | Train Loss: 0.2824565 Vali Loss: 0.3612890 Test Loss: 0.3823284
Validation loss decreased (0.365509 --> 0.361289).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.407035112380981
Epoch: 9, Steps: 60 | Train Loss: 0.2695873 Vali Loss: 0.3576065 Test Loss: 0.3810036
Validation loss decreased (0.361289 --> 0.357606).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.947980165481567
Epoch: 10, Steps: 60 | Train Loss: 0.2583297 Vali Loss: 0.3547671 Test Loss: 0.3800305
Validation loss decreased (0.357606 --> 0.354767).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.54547643661499
Epoch: 11, Steps: 60 | Train Loss: 0.2482707 Vali Loss: 0.3515434 Test Loss: 0.3791743
Validation loss decreased (0.354767 --> 0.351543).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.356035947799683
Epoch: 12, Steps: 60 | Train Loss: 0.2398730 Vali Loss: 0.3490736 Test Loss: 0.3781596
Validation loss decreased (0.351543 --> 0.349074).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 7.998540878295898
Epoch: 13, Steps: 60 | Train Loss: 0.2317977 Vali Loss: 0.3464932 Test Loss: 0.3770604
Validation loss decreased (0.349074 --> 0.346493).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.746564865112305
Epoch: 14, Steps: 60 | Train Loss: 0.2254302 Vali Loss: 0.3442295 Test Loss: 0.3761582
Validation loss decreased (0.346493 --> 0.344229).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.769718885421753
Epoch: 15, Steps: 60 | Train Loss: 0.2191121 Vali Loss: 0.3421433 Test Loss: 0.3752423
Validation loss decreased (0.344229 --> 0.342143).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.828802585601807
Epoch: 16, Steps: 60 | Train Loss: 0.2136705 Vali Loss: 0.3403149 Test Loss: 0.3743367
Validation loss decreased (0.342143 --> 0.340315).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.44020414352417
Epoch: 17, Steps: 60 | Train Loss: 0.2089865 Vali Loss: 0.3382153 Test Loss: 0.3735424
Validation loss decreased (0.340315 --> 0.338215).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.504687309265137
Epoch: 18, Steps: 60 | Train Loss: 0.2043724 Vali Loss: 0.3367455 Test Loss: 0.3728192
Validation loss decreased (0.338215 --> 0.336746).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.027770042419434
Epoch: 19, Steps: 60 | Train Loss: 0.2004830 Vali Loss: 0.3352768 Test Loss: 0.3720409
Validation loss decreased (0.336746 --> 0.335277).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.108463525772095
Epoch: 20, Steps: 60 | Train Loss: 0.1965425 Vali Loss: 0.3336770 Test Loss: 0.3714017
Validation loss decreased (0.335277 --> 0.333677).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.425339221954346
Epoch: 21, Steps: 60 | Train Loss: 0.1932831 Vali Loss: 0.3321600 Test Loss: 0.3708156
Validation loss decreased (0.333677 --> 0.332160).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 6.2776198387146
Epoch: 22, Steps: 60 | Train Loss: 0.1899829 Vali Loss: 0.3309289 Test Loss: 0.3701362
Validation loss decreased (0.332160 --> 0.330929).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 7.460178852081299
Epoch: 23, Steps: 60 | Train Loss: 0.1872412 Vali Loss: 0.3298129 Test Loss: 0.3695990
Validation loss decreased (0.330929 --> 0.329813).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 7.225549697875977
Epoch: 24, Steps: 60 | Train Loss: 0.1846287 Vali Loss: 0.3287575 Test Loss: 0.3691007
Validation loss decreased (0.329813 --> 0.328758).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 6.51087498664856
Epoch: 25, Steps: 60 | Train Loss: 0.1822633 Vali Loss: 0.3276082 Test Loss: 0.3685956
Validation loss decreased (0.328758 --> 0.327608).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 8.954354286193848
Epoch: 26, Steps: 60 | Train Loss: 0.1800706 Vali Loss: 0.3266004 Test Loss: 0.3681800
Validation loss decreased (0.327608 --> 0.326600).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.888480186462402
Epoch: 27, Steps: 60 | Train Loss: 0.1777696 Vali Loss: 0.3257717 Test Loss: 0.3677063
Validation loss decreased (0.326600 --> 0.325772).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.767394304275513
Epoch: 28, Steps: 60 | Train Loss: 0.1760699 Vali Loss: 0.3246745 Test Loss: 0.3673350
Validation loss decreased (0.325772 --> 0.324675).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.867860794067383
Epoch: 29, Steps: 60 | Train Loss: 0.1739586 Vali Loss: 0.3240732 Test Loss: 0.3669209
Validation loss decreased (0.324675 --> 0.324073).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.13504433631897
Epoch: 30, Steps: 60 | Train Loss: 0.1725929 Vali Loss: 0.3231278 Test Loss: 0.3666356
Validation loss decreased (0.324073 --> 0.323128).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 8.996356964111328
Epoch: 31, Steps: 60 | Train Loss: 0.1711060 Vali Loss: 0.3225048 Test Loss: 0.3662137
Validation loss decreased (0.323128 --> 0.322505).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.807055711746216
Epoch: 32, Steps: 60 | Train Loss: 0.1698120 Vali Loss: 0.3218656 Test Loss: 0.3659122
Validation loss decreased (0.322505 --> 0.321866).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.921616077423096
Epoch: 33, Steps: 60 | Train Loss: 0.1685032 Vali Loss: 0.3209633 Test Loss: 0.3656678
Validation loss decreased (0.321866 --> 0.320963).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 9.175595998764038
Epoch: 34, Steps: 60 | Train Loss: 0.1670352 Vali Loss: 0.3203644 Test Loss: 0.3653635
Validation loss decreased (0.320963 --> 0.320364).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 8.905537843704224
Epoch: 35, Steps: 60 | Train Loss: 0.1662172 Vali Loss: 0.3199663 Test Loss: 0.3650926
Validation loss decreased (0.320364 --> 0.319966).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.97899580001831
Epoch: 36, Steps: 60 | Train Loss: 0.1651106 Vali Loss: 0.3193841 Test Loss: 0.3649044
Validation loss decreased (0.319966 --> 0.319384).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 8.765543699264526
Epoch: 37, Steps: 60 | Train Loss: 0.1637919 Vali Loss: 0.3188040 Test Loss: 0.3646998
Validation loss decreased (0.319384 --> 0.318804).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 9.491865634918213
Epoch: 38, Steps: 60 | Train Loss: 0.1633673 Vali Loss: 0.3183828 Test Loss: 0.3644403
Validation loss decreased (0.318804 --> 0.318383).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 8.953107118606567
Epoch: 39, Steps: 60 | Train Loss: 0.1621756 Vali Loss: 0.3179053 Test Loss: 0.3642148
Validation loss decreased (0.318383 --> 0.317905).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 8.669718027114868
Epoch: 40, Steps: 60 | Train Loss: 0.1609724 Vali Loss: 0.3174301 Test Loss: 0.3641166
Validation loss decreased (0.317905 --> 0.317430).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 8.876112699508667
Epoch: 41, Steps: 60 | Train Loss: 0.1607834 Vali Loss: 0.3169695 Test Loss: 0.3639154
Validation loss decreased (0.317430 --> 0.316970).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 9.230256795883179
Epoch: 42, Steps: 60 | Train Loss: 0.1602091 Vali Loss: 0.3166200 Test Loss: 0.3637463
Validation loss decreased (0.316970 --> 0.316620).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 8.513383626937866
Epoch: 43, Steps: 60 | Train Loss: 0.1594155 Vali Loss: 0.3163132 Test Loss: 0.3635921
Validation loss decreased (0.316620 --> 0.316313).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 8.724477529525757
Epoch: 44, Steps: 60 | Train Loss: 0.1584255 Vali Loss: 0.3159081 Test Loss: 0.3635091
Validation loss decreased (0.316313 --> 0.315908).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 8.619561433792114
Epoch: 45, Steps: 60 | Train Loss: 0.1582392 Vali Loss: 0.3154747 Test Loss: 0.3633175
Validation loss decreased (0.315908 --> 0.315475).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.39124846458435
Epoch: 46, Steps: 60 | Train Loss: 0.1575304 Vali Loss: 0.3153095 Test Loss: 0.3632160
Validation loss decreased (0.315475 --> 0.315309).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 9.490693807601929
Epoch: 47, Steps: 60 | Train Loss: 0.1571609 Vali Loss: 0.3149757 Test Loss: 0.3630848
Validation loss decreased (0.315309 --> 0.314976).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 8.70618486404419
Epoch: 48, Steps: 60 | Train Loss: 0.1563819 Vali Loss: 0.3146466 Test Loss: 0.3629965
Validation loss decreased (0.314976 --> 0.314647).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 8.302727222442627
Epoch: 49, Steps: 60 | Train Loss: 0.1559508 Vali Loss: 0.3144877 Test Loss: 0.3628509
Validation loss decreased (0.314647 --> 0.314488).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 8.174782752990723
Epoch: 50, Steps: 60 | Train Loss: 0.1553140 Vali Loss: 0.3141341 Test Loss: 0.3627780
Validation loss decreased (0.314488 --> 0.314134).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30898560.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.148849725723267
Epoch: 1, Steps: 60 | Train Loss: 0.5314523 Vali Loss: 0.2954116 Test Loss: 0.3540874
Validation loss decreased (inf --> 0.295412).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.516561031341553
Epoch: 2, Steps: 60 | Train Loss: 0.5208306 Vali Loss: 0.2895981 Test Loss: 0.3531452
Validation loss decreased (0.295412 --> 0.289598).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.979692220687866
Epoch: 3, Steps: 60 | Train Loss: 0.5163510 Vali Loss: 0.2871197 Test Loss: 0.3530569
Validation loss decreased (0.289598 --> 0.287120).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.535036325454712
Epoch: 4, Steps: 60 | Train Loss: 0.5149397 Vali Loss: 0.2855682 Test Loss: 0.3530532
Validation loss decreased (0.287120 --> 0.285568).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.201225280761719
Epoch: 5, Steps: 60 | Train Loss: 0.5129984 Vali Loss: 0.2845492 Test Loss: 0.3526523
Validation loss decreased (0.285568 --> 0.284549).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.37067198753357
Epoch: 6, Steps: 60 | Train Loss: 0.5131128 Vali Loss: 0.2845023 Test Loss: 0.3526751
Validation loss decreased (0.284549 --> 0.284502).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.033384084701538
Epoch: 7, Steps: 60 | Train Loss: 0.5104462 Vali Loss: 0.2835517 Test Loss: 0.3524137
Validation loss decreased (0.284502 --> 0.283552).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.722610473632812
Epoch: 8, Steps: 60 | Train Loss: 0.5115051 Vali Loss: 0.2825727 Test Loss: 0.3527041
Validation loss decreased (0.283552 --> 0.282573).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.073075771331787
Epoch: 9, Steps: 60 | Train Loss: 0.5128221 Vali Loss: 0.2823761 Test Loss: 0.3524552
Validation loss decreased (0.282573 --> 0.282376).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.474264144897461
Epoch: 10, Steps: 60 | Train Loss: 0.5101214 Vali Loss: 0.2826848 Test Loss: 0.3521528
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.897377967834473
Epoch: 11, Steps: 60 | Train Loss: 0.5093847 Vali Loss: 0.2818624 Test Loss: 0.3522182
Validation loss decreased (0.282376 --> 0.281862).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.057157039642334
Epoch: 12, Steps: 60 | Train Loss: 0.5107872 Vali Loss: 0.2820993 Test Loss: 0.3520294
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.26567816734314
Epoch: 13, Steps: 60 | Train Loss: 0.5112105 Vali Loss: 0.2815301 Test Loss: 0.3520678
Validation loss decreased (0.281862 --> 0.281530).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.142167568206787
Epoch: 14, Steps: 60 | Train Loss: 0.5100042 Vali Loss: 0.2815647 Test Loss: 0.3521052
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.942314147949219
Epoch: 15, Steps: 60 | Train Loss: 0.5096479 Vali Loss: 0.2814729 Test Loss: 0.3519933
Validation loss decreased (0.281530 --> 0.281473).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.239313840866089
Epoch: 16, Steps: 60 | Train Loss: 0.5090184 Vali Loss: 0.2811341 Test Loss: 0.3519300
Validation loss decreased (0.281473 --> 0.281134).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.71566891670227
Epoch: 17, Steps: 60 | Train Loss: 0.5090941 Vali Loss: 0.2813240 Test Loss: 0.3518339
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.767627477645874
Epoch: 18, Steps: 60 | Train Loss: 0.5083152 Vali Loss: 0.2814361 Test Loss: 0.3516405
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.086134195327759
Epoch: 19, Steps: 60 | Train Loss: 0.5092435 Vali Loss: 0.2808650 Test Loss: 0.3518701
Validation loss decreased (0.281134 --> 0.280865).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.436012029647827
Epoch: 20, Steps: 60 | Train Loss: 0.5085195 Vali Loss: 0.2808343 Test Loss: 0.3519278
Validation loss decreased (0.280865 --> 0.280834).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.515630722045898
Epoch: 21, Steps: 60 | Train Loss: 0.5098667 Vali Loss: 0.2808357 Test Loss: 0.3517393
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.940035104751587
Epoch: 22, Steps: 60 | Train Loss: 0.5074381 Vali Loss: 0.2808701 Test Loss: 0.3517114
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.06192398071289
Epoch: 23, Steps: 60 | Train Loss: 0.5087805 Vali Loss: 0.2807121 Test Loss: 0.3517170
Validation loss decreased (0.280834 --> 0.280712).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.327604055404663
Epoch: 24, Steps: 60 | Train Loss: 0.5077954 Vali Loss: 0.2806370 Test Loss: 0.3516581
Validation loss decreased (0.280712 --> 0.280637).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.293808698654175
Epoch: 25, Steps: 60 | Train Loss: 0.5090442 Vali Loss: 0.2805368 Test Loss: 0.3516490
Validation loss decreased (0.280637 --> 0.280537).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 7.0644450187683105
Epoch: 26, Steps: 60 | Train Loss: 0.5080240 Vali Loss: 0.2805235 Test Loss: 0.3516628
Validation loss decreased (0.280537 --> 0.280523).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 7.006297588348389
Epoch: 27, Steps: 60 | Train Loss: 0.5069680 Vali Loss: 0.2799701 Test Loss: 0.3516044
Validation loss decreased (0.280523 --> 0.279970).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.95687222480774
Epoch: 28, Steps: 60 | Train Loss: 0.5072710 Vali Loss: 0.2805724 Test Loss: 0.3515787
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.590591430664062
Epoch: 29, Steps: 60 | Train Loss: 0.5078179 Vali Loss: 0.2804416 Test Loss: 0.3515472
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.323609352111816
Epoch: 30, Steps: 60 | Train Loss: 0.5070363 Vali Loss: 0.2803166 Test Loss: 0.3515302
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.33238744735717773, mae:0.37482160329818726, rse:0.4623417854309082, corr:[0.26559398 0.2683653  0.26711804 0.2667279  0.26678377 0.2661946
 0.26539335 0.2649293  0.26454806 0.26328504 0.2615625  0.26002365
 0.2590507  0.25834388 0.25746566 0.25686732 0.256608   0.25621912
 0.25510472 0.2537758  0.2528076  0.25197592 0.2505585  0.2483621
 0.2462142  0.24472134 0.24346662 0.24181286 0.24005488 0.2386857
 0.23770306 0.23640841 0.23489757 0.23361847 0.23269552 0.2317804
 0.23063019 0.22947872 0.22860102 0.22779202 0.22695619 0.22612365
 0.22533289 0.22451138 0.22370243 0.22286023 0.22185397 0.22026849
 0.21829206 0.21634902 0.21467431 0.21304016 0.2115672  0.20988847
 0.20776336 0.2060233  0.20461123 0.20288025 0.20101483 0.19960907
 0.19901977 0.19851099 0.19814706 0.19817407 0.1979948  0.19759656
 0.19675338 0.19592294 0.19563791 0.19538003 0.19438775 0.19306973
 0.19212347 0.19157791 0.19075312 0.18923154 0.18814942 0.18780524
 0.1874711  0.18636993 0.18568552 0.1854479  0.18514745 0.18456775
 0.18405747 0.18402453 0.1839685  0.18347518 0.18260571 0.18220969
 0.18223552 0.18191497 0.18159533 0.1813305  0.1811869  0.18082346
 0.17990491 0.17879184 0.17794162 0.1771454  0.17607011 0.17471181
 0.17380026 0.17320721 0.1728816  0.17239293 0.17180347 0.17160901
 0.17105037 0.1702479  0.16928224 0.16872081 0.16818066 0.16792499
 0.16769293 0.16723873 0.16653185 0.16513677 0.16359957 0.16213514
 0.16109411 0.15997785 0.15889238 0.15782812 0.15665895 0.15544406
 0.15438294 0.15334354 0.15264848 0.15210772 0.15155502 0.15071319
 0.15019481 0.14993031 0.14942662 0.14880475 0.14836837 0.14817408
 0.14741084 0.14583544 0.14470062 0.14475325 0.14516905 0.1441818
 0.14156845 0.13941003 0.13876747 0.13849807 0.13719258 0.13517134
 0.13408917 0.1337172  0.13382787 0.13358933 0.13282074 0.13178286
 0.1313151  0.1314437  0.13213308 0.13296045 0.1324578  0.13140988
 0.13092172 0.13131006 0.13185231 0.13174574 0.13154745 0.13113327
 0.13024865 0.12833409 0.12690881 0.1266627  0.12717588 0.12604499
 0.12361445 0.12142111 0.12085678 0.12081556 0.11910673 0.11785463
 0.11853682 0.11978605 0.11978744 0.11926655 0.11963426 0.12089697
 0.1205017  0.11865699 0.11984159 0.12215807 0.12087099 0.11895072]
