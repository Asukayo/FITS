Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  38986752.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.892531394958496
Epoch: 1, Steps: 61 | Train Loss: 0.5887790 Vali Loss: 0.2816602 Test Loss: 0.3035195
Validation loss decreased (inf --> 0.281660).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.10555100440979
Epoch: 2, Steps: 61 | Train Loss: 0.4662978 Vali Loss: 0.2512979 Test Loss: 0.2853140
Validation loss decreased (0.281660 --> 0.251298).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.782771825790405
Epoch: 3, Steps: 61 | Train Loss: 0.4434641 Vali Loss: 0.2413035 Test Loss: 0.2800891
Validation loss decreased (0.251298 --> 0.241304).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.494004726409912
Epoch: 4, Steps: 61 | Train Loss: 0.4326387 Vali Loss: 0.2361222 Test Loss: 0.2781087
Validation loss decreased (0.241304 --> 0.236122).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.832313299179077
Epoch: 5, Steps: 61 | Train Loss: 0.4264874 Vali Loss: 0.2308095 Test Loss: 0.2767994
Validation loss decreased (0.236122 --> 0.230810).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.46091103553772
Epoch: 6, Steps: 61 | Train Loss: 0.4217356 Vali Loss: 0.2282248 Test Loss: 0.2760386
Validation loss decreased (0.230810 --> 0.228225).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.445461750030518
Epoch: 7, Steps: 61 | Train Loss: 0.4181524 Vali Loss: 0.2246952 Test Loss: 0.2754208
Validation loss decreased (0.228225 --> 0.224695).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.820558071136475
Epoch: 8, Steps: 61 | Train Loss: 0.4160854 Vali Loss: 0.2233332 Test Loss: 0.2747914
Validation loss decreased (0.224695 --> 0.223333).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.051103591918945
Epoch: 9, Steps: 61 | Train Loss: 0.4137765 Vali Loss: 0.2239989 Test Loss: 0.2743717
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.060220718383789
Epoch: 10, Steps: 61 | Train Loss: 0.4110919 Vali Loss: 0.2219680 Test Loss: 0.2741265
Validation loss decreased (0.223333 --> 0.221968).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.554398536682129
Epoch: 11, Steps: 61 | Train Loss: 0.4100053 Vali Loss: 0.2215974 Test Loss: 0.2737989
Validation loss decreased (0.221968 --> 0.221597).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.486059665679932
Epoch: 12, Steps: 61 | Train Loss: 0.4095466 Vali Loss: 0.2193274 Test Loss: 0.2736427
Validation loss decreased (0.221597 --> 0.219327).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.47556209564209
Epoch: 13, Steps: 61 | Train Loss: 0.4083567 Vali Loss: 0.2193805 Test Loss: 0.2732848
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.164242506027222
Epoch: 14, Steps: 61 | Train Loss: 0.4070728 Vali Loss: 0.2186967 Test Loss: 0.2732789
Validation loss decreased (0.219327 --> 0.218697).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.26230263710022
Epoch: 15, Steps: 61 | Train Loss: 0.4072077 Vali Loss: 0.2174434 Test Loss: 0.2730510
Validation loss decreased (0.218697 --> 0.217443).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.52787160873413
Epoch: 16, Steps: 61 | Train Loss: 0.4060746 Vali Loss: 0.2179448 Test Loss: 0.2729980
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.778028726577759
Epoch: 17, Steps: 61 | Train Loss: 0.4056148 Vali Loss: 0.2184971 Test Loss: 0.2728111
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.686168193817139
Epoch: 18, Steps: 61 | Train Loss: 0.4054661 Vali Loss: 0.2167965 Test Loss: 0.2728513
Validation loss decreased (0.217443 --> 0.216796).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.2226881980896
Epoch: 19, Steps: 61 | Train Loss: 0.4048455 Vali Loss: 0.2179470 Test Loss: 0.2725499
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.739105939865112
Epoch: 20, Steps: 61 | Train Loss: 0.4047784 Vali Loss: 0.2170326 Test Loss: 0.2724218
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.155904531478882
Epoch: 21, Steps: 61 | Train Loss: 0.4042643 Vali Loss: 0.2161311 Test Loss: 0.2725153
Validation loss decreased (0.216796 --> 0.216131).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 7.8633739948272705
Epoch: 22, Steps: 61 | Train Loss: 0.4044857 Vali Loss: 0.2157945 Test Loss: 0.2724567
Validation loss decreased (0.216131 --> 0.215795).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.32877230644226
Epoch: 23, Steps: 61 | Train Loss: 0.4040750 Vali Loss: 0.2168566 Test Loss: 0.2723119
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.635301113128662
Epoch: 24, Steps: 61 | Train Loss: 0.4036686 Vali Loss: 0.2160376 Test Loss: 0.2723109
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.097959518432617
Epoch: 25, Steps: 61 | Train Loss: 0.4035697 Vali Loss: 0.2149138 Test Loss: 0.2722593
Validation loss decreased (0.215795 --> 0.214914).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 7.583144426345825
Epoch: 26, Steps: 61 | Train Loss: 0.4033400 Vali Loss: 0.2153248 Test Loss: 0.2722628
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 7.8312811851501465
Epoch: 27, Steps: 61 | Train Loss: 0.4029693 Vali Loss: 0.2144237 Test Loss: 0.2721738
Validation loss decreased (0.214914 --> 0.214424).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 7.925149917602539
Epoch: 28, Steps: 61 | Train Loss: 0.4029056 Vali Loss: 0.2150665 Test Loss: 0.2721498
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.841510772705078
Epoch: 29, Steps: 61 | Train Loss: 0.4027588 Vali Loss: 0.2157328 Test Loss: 0.2721043
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 8.707126140594482
Epoch: 30, Steps: 61 | Train Loss: 0.4026169 Vali Loss: 0.2151399 Test Loss: 0.2721597
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.27165788412094116, mae:0.3370708227157593, rse:0.4200417399406433, corr:[0.2710764  0.27562663 0.274043   0.27492276 0.2746697  0.27321774
 0.27288267 0.2726108  0.27127635 0.26988918 0.26894942 0.26785168
 0.2664586  0.2650885  0.26424363 0.2638834  0.2634847  0.26294398
 0.26215062 0.26093754 0.25964612 0.25872427 0.25743783 0.2553309
 0.25332218 0.25213668 0.25075412 0.24872571 0.24738845 0.2468063
 0.24558438 0.2436427  0.2425194  0.2419758  0.24047197 0.2385269
 0.23758268 0.23688406 0.2354082  0.23425445 0.23410404 0.23374715
 0.23278695 0.2320283  0.23137853 0.2302181  0.22900663 0.22787428
 0.2260773  0.22374502 0.22213252 0.22116613 0.22018659 0.21863882
 0.21680725 0.2152889  0.2135386  0.21179514 0.21055986 0.20961837
 0.208891   0.20842256 0.2081396  0.20802534 0.20788608 0.20775723
 0.20679627 0.20573923 0.20559987 0.20528503 0.204311   0.20381588
 0.20316125 0.20124811 0.19961108 0.19922744 0.19923328 0.19852652
 0.19826558 0.19759613 0.19631878 0.19561736 0.19631025 0.19621603
 0.19448625 0.19446917 0.1956341  0.19462827 0.19263996 0.19389719
 0.19367859 0.1905368  0.19168915 0.19278902 0.1882308  0.19403559]
