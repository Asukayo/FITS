Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  61797120.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.722416400909424
Epoch: 1, Steps: 30 | Train Loss: 0.7938242 Vali Loss: 0.4517244 Test Loss: 0.4191925
Validation loss decreased (inf --> 0.451724).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.671184301376343
Epoch: 2, Steps: 30 | Train Loss: 0.6460396 Vali Loss: 0.3862452 Test Loss: 0.3817857
Validation loss decreased (0.451724 --> 0.386245).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.7107765674591064
Epoch: 3, Steps: 30 | Train Loss: 0.5995784 Vali Loss: 0.3567713 Test Loss: 0.3687138
Validation loss decreased (0.386245 --> 0.356771).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.561776638031006
Epoch: 4, Steps: 30 | Train Loss: 0.5782216 Vali Loss: 0.3401592 Test Loss: 0.3627153
Validation loss decreased (0.356771 --> 0.340159).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.6470561027526855
Epoch: 5, Steps: 30 | Train Loss: 0.5665951 Vali Loss: 0.3330325 Test Loss: 0.3591223
Validation loss decreased (0.340159 --> 0.333033).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.54091739654541
Epoch: 6, Steps: 30 | Train Loss: 0.5584178 Vali Loss: 0.3263080 Test Loss: 0.3568383
Validation loss decreased (0.333033 --> 0.326308).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.6683056354522705
Epoch: 7, Steps: 30 | Train Loss: 0.5521082 Vali Loss: 0.3218303 Test Loss: 0.3553809
Validation loss decreased (0.326308 --> 0.321830).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.388463258743286
Epoch: 8, Steps: 30 | Train Loss: 0.5477830 Vali Loss: 0.3181585 Test Loss: 0.3544551
Validation loss decreased (0.321830 --> 0.318158).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.693566799163818
Epoch: 9, Steps: 30 | Train Loss: 0.5432288 Vali Loss: 0.3123484 Test Loss: 0.3537939
Validation loss decreased (0.318158 --> 0.312348).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.71160101890564
Epoch: 10, Steps: 30 | Train Loss: 0.5399959 Vali Loss: 0.3112114 Test Loss: 0.3533429
Validation loss decreased (0.312348 --> 0.311211).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.738400936126709
Epoch: 11, Steps: 30 | Train Loss: 0.5395992 Vali Loss: 0.3088413 Test Loss: 0.3530401
Validation loss decreased (0.311211 --> 0.308841).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.711488485336304
Epoch: 12, Steps: 30 | Train Loss: 0.5356823 Vali Loss: 0.3067986 Test Loss: 0.3527399
Validation loss decreased (0.308841 --> 0.306799).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.55215311050415
Epoch: 13, Steps: 30 | Train Loss: 0.5352283 Vali Loss: 0.3066087 Test Loss: 0.3526256
Validation loss decreased (0.306799 --> 0.306609).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.475328207015991
Epoch: 14, Steps: 30 | Train Loss: 0.5340798 Vali Loss: 0.3038914 Test Loss: 0.3524904
Validation loss decreased (0.306609 --> 0.303891).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.563545227050781
Epoch: 15, Steps: 30 | Train Loss: 0.5310256 Vali Loss: 0.3026650 Test Loss: 0.3524556
Validation loss decreased (0.303891 --> 0.302665).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.534172534942627
Epoch: 16, Steps: 30 | Train Loss: 0.5310881 Vali Loss: 0.3020088 Test Loss: 0.3523827
Validation loss decreased (0.302665 --> 0.302009).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.612127065658569
Epoch: 17, Steps: 30 | Train Loss: 0.5288637 Vali Loss: 0.3006362 Test Loss: 0.3523419
Validation loss decreased (0.302009 --> 0.300636).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.4248738288879395
Epoch: 18, Steps: 30 | Train Loss: 0.5291817 Vali Loss: 0.3009917 Test Loss: 0.3523144
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.437721490859985
Epoch: 19, Steps: 30 | Train Loss: 0.5279565 Vali Loss: 0.2985480 Test Loss: 0.3523741
Validation loss decreased (0.300636 --> 0.298548).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.782840967178345
Epoch: 20, Steps: 30 | Train Loss: 0.5280020 Vali Loss: 0.2997723 Test Loss: 0.3522549
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.677719593048096
Epoch: 21, Steps: 30 | Train Loss: 0.5263478 Vali Loss: 0.2972969 Test Loss: 0.3522674
Validation loss decreased (0.298548 --> 0.297297).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.6779210567474365
Epoch: 22, Steps: 30 | Train Loss: 0.5251031 Vali Loss: 0.2981656 Test Loss: 0.3523064
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.644453763961792
Epoch: 23, Steps: 30 | Train Loss: 0.5256293 Vali Loss: 0.2979451 Test Loss: 0.3523276
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.428427457809448
Epoch: 24, Steps: 30 | Train Loss: 0.5255930 Vali Loss: 0.2968963 Test Loss: 0.3523242
Validation loss decreased (0.297297 --> 0.296896).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.370851039886475
Epoch: 25, Steps: 30 | Train Loss: 0.5244290 Vali Loss: 0.2971271 Test Loss: 0.3522695
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.813605785369873
Epoch: 26, Steps: 30 | Train Loss: 0.5236478 Vali Loss: 0.2988982 Test Loss: 0.3522964
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.8163628578186035
Epoch: 27, Steps: 30 | Train Loss: 0.5227562 Vali Loss: 0.2975767 Test Loss: 0.3522754
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.33166682720184326, mae:0.3765166401863098, rse:0.46184033155441284, corr:[0.26143643 0.26856038 0.26664487 0.26558474 0.26678059 0.26708016
 0.2657282  0.26486376 0.26418528 0.26314494 0.26172194 0.2605628
 0.25963005 0.2585202  0.25747028 0.25673836 0.2562344  0.25540903
 0.25428772 0.25300187 0.2517935  0.25062916 0.24912561 0.24739088
 0.24571869 0.24425243 0.24286236 0.2413489  0.23966837 0.23808233
 0.23700213 0.2359907  0.23496403 0.23351781 0.23222272 0.23148818
 0.23084417 0.22981423 0.22844853 0.22761644 0.22733048 0.22684295
 0.22573873 0.22460021 0.22380611 0.2230559  0.22163522 0.21953832
 0.2176014  0.21611576 0.2146973  0.21323617 0.21199703 0.21055847
 0.20851964 0.20652774 0.20502156 0.20380174 0.20255908 0.2012593
 0.20053095 0.20037429 0.20035936 0.19997524 0.1992012  0.19866481
 0.198238   0.1977326  0.19725017 0.19681117 0.1961298  0.19509166
 0.19383393 0.19268315 0.19162937 0.19042577 0.18937998 0.18861958
 0.18807903 0.18747085 0.18694194 0.18627462 0.18571453 0.18536302
 0.18508728 0.18486685 0.18472476 0.18463595 0.18420935 0.18346101
 0.18280071 0.18254468 0.18269873 0.18255876 0.18204048 0.18146783
 0.18089555 0.18013354 0.17914076 0.17817621 0.17744447 0.17670594
 0.17590421 0.1750403  0.1745258  0.17418917 0.17379877 0.17340825
 0.17281248 0.172174   0.17139268 0.17074162 0.17031747 0.17026529
 0.16996998 0.16910657 0.16812986 0.16726777 0.16643545 0.16501483
 0.16328329 0.16178983 0.16099462 0.1603416  0.15923218 0.15792261
 0.15695834 0.15620054 0.15548798 0.15470023 0.15395229 0.15294097
 0.15188631 0.15102527 0.15065672 0.15056638 0.15004519 0.14899153
 0.14787552 0.1472332  0.14692797 0.14639871 0.14547059 0.14415771
 0.14242904 0.14046256 0.13847457 0.13696612 0.13622592 0.13565002
 0.13474232 0.1333085  0.1325939  0.13240574 0.13185872 0.13066739
 0.13000329 0.13004674 0.13016762 0.13008226 0.12981758 0.13002649
 0.12992276 0.12889938 0.12806807 0.12841591 0.12908551 0.12786935
 0.12519221 0.12321027 0.12322954 0.12302794 0.12135851 0.11895451
 0.11795631 0.11714546 0.11519434 0.11323781 0.11236948 0.11262864
 0.11159357 0.10980589 0.11035624 0.11283698 0.11234097 0.10928953
 0.10860292 0.11069702 0.11058329 0.1063634  0.11021861 0.12420567]
