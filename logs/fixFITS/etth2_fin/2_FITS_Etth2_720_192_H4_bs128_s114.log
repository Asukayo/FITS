Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  40581632.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.033851385116577
Epoch: 1, Steps: 30 | Train Loss: 0.6408331 Vali Loss: 0.5227445 Test Loss: 0.4832449
Validation loss decreased (inf --> 0.522744).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.0622265338897705
Epoch: 2, Steps: 30 | Train Loss: 0.5458060 Vali Loss: 0.4705519 Test Loss: 0.4483638
Validation loss decreased (0.522744 --> 0.470552).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.237619876861572
Epoch: 3, Steps: 30 | Train Loss: 0.4846961 Vali Loss: 0.4394302 Test Loss: 0.4248148
Validation loss decreased (0.470552 --> 0.439430).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 6.052772045135498
Epoch: 4, Steps: 30 | Train Loss: 0.4425423 Vali Loss: 0.4188361 Test Loss: 0.4097204
Validation loss decreased (0.439430 --> 0.418836).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.283958435058594
Epoch: 5, Steps: 30 | Train Loss: 0.4122679 Vali Loss: 0.4047602 Test Loss: 0.3997399
Validation loss decreased (0.418836 --> 0.404760).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.984258651733398
Epoch: 6, Steps: 30 | Train Loss: 0.3894538 Vali Loss: 0.3930392 Test Loss: 0.3931813
Validation loss decreased (0.404760 --> 0.393039).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.714687347412109
Epoch: 7, Steps: 30 | Train Loss: 0.3717444 Vali Loss: 0.3840236 Test Loss: 0.3889018
Validation loss decreased (0.393039 --> 0.384024).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.97563362121582
Epoch: 8, Steps: 30 | Train Loss: 0.3569753 Vali Loss: 0.3796409 Test Loss: 0.3860049
Validation loss decreased (0.384024 --> 0.379641).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 6.01801609992981
Epoch: 9, Steps: 30 | Train Loss: 0.3444339 Vali Loss: 0.3763454 Test Loss: 0.3839389
Validation loss decreased (0.379641 --> 0.376345).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 6.43851375579834
Epoch: 10, Steps: 30 | Train Loss: 0.3339969 Vali Loss: 0.3720312 Test Loss: 0.3824686
Validation loss decreased (0.376345 --> 0.372031).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.433968782424927
Epoch: 11, Steps: 30 | Train Loss: 0.3243660 Vali Loss: 0.3681735 Test Loss: 0.3814466
Validation loss decreased (0.372031 --> 0.368173).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 6.651261806488037
Epoch: 12, Steps: 30 | Train Loss: 0.3173116 Vali Loss: 0.3669586 Test Loss: 0.3806543
Validation loss decreased (0.368173 --> 0.366959).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 6.900118589401245
Epoch: 13, Steps: 30 | Train Loss: 0.3100669 Vali Loss: 0.3642015 Test Loss: 0.3800413
Validation loss decreased (0.366959 --> 0.364202).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 6.931172132492065
Epoch: 14, Steps: 30 | Train Loss: 0.3035112 Vali Loss: 0.3627030 Test Loss: 0.3794890
Validation loss decreased (0.364202 --> 0.362703).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 6.722661733627319
Epoch: 15, Steps: 30 | Train Loss: 0.2975551 Vali Loss: 0.3614546 Test Loss: 0.3791061
Validation loss decreased (0.362703 --> 0.361455).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 6.513114929199219
Epoch: 16, Steps: 30 | Train Loss: 0.2925338 Vali Loss: 0.3591300 Test Loss: 0.3787845
Validation loss decreased (0.361455 --> 0.359130).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.0285255908966064
Epoch: 17, Steps: 30 | Train Loss: 0.2873096 Vali Loss: 0.3569743 Test Loss: 0.3784890
Validation loss decreased (0.359130 --> 0.356974).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.6757893562316895
Epoch: 18, Steps: 30 | Train Loss: 0.2826507 Vali Loss: 0.3556226 Test Loss: 0.3781578
Validation loss decreased (0.356974 --> 0.355623).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.945223569869995
Epoch: 19, Steps: 30 | Train Loss: 0.2785711 Vali Loss: 0.3554084 Test Loss: 0.3779442
Validation loss decreased (0.355623 --> 0.355408).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.570582151412964
Epoch: 20, Steps: 30 | Train Loss: 0.2751010 Vali Loss: 0.3550797 Test Loss: 0.3776833
Validation loss decreased (0.355408 --> 0.355080).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.872105836868286
Epoch: 21, Steps: 30 | Train Loss: 0.2717801 Vali Loss: 0.3546556 Test Loss: 0.3773794
Validation loss decreased (0.355080 --> 0.354656).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.042115688323975
Epoch: 22, Steps: 30 | Train Loss: 0.2687442 Vali Loss: 0.3522210 Test Loss: 0.3772156
Validation loss decreased (0.354656 --> 0.352221).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 6.2226996421813965
Epoch: 23, Steps: 30 | Train Loss: 0.2653015 Vali Loss: 0.3515815 Test Loss: 0.3770288
Validation loss decreased (0.352221 --> 0.351582).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 6.161255359649658
Epoch: 24, Steps: 30 | Train Loss: 0.2630508 Vali Loss: 0.3515208 Test Loss: 0.3767750
Validation loss decreased (0.351582 --> 0.351521).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 6.171972036361694
Epoch: 25, Steps: 30 | Train Loss: 0.2599005 Vali Loss: 0.3499879 Test Loss: 0.3765601
Validation loss decreased (0.351521 --> 0.349988).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 6.212261199951172
Epoch: 26, Steps: 30 | Train Loss: 0.2575075 Vali Loss: 0.3500497 Test Loss: 0.3764077
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 6.39126992225647
Epoch: 27, Steps: 30 | Train Loss: 0.2556139 Vali Loss: 0.3489340 Test Loss: 0.3762220
Validation loss decreased (0.349988 --> 0.348934).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 6.14752197265625
Epoch: 28, Steps: 30 | Train Loss: 0.2532919 Vali Loss: 0.3497169 Test Loss: 0.3760578
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 6.106585264205933
Epoch: 29, Steps: 30 | Train Loss: 0.2515330 Vali Loss: 0.3476532 Test Loss: 0.3758970
Validation loss decreased (0.348934 --> 0.347653).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 6.2679057121276855
Epoch: 30, Steps: 30 | Train Loss: 0.2493280 Vali Loss: 0.3471870 Test Loss: 0.3756937
Validation loss decreased (0.347653 --> 0.347187).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 6.200894355773926
Epoch: 31, Steps: 30 | Train Loss: 0.2477048 Vali Loss: 0.3471823 Test Loss: 0.3756036
Validation loss decreased (0.347187 --> 0.347182).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 6.256911039352417
Epoch: 32, Steps: 30 | Train Loss: 0.2457322 Vali Loss: 0.3468858 Test Loss: 0.3754577
Validation loss decreased (0.347182 --> 0.346886).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 6.322994947433472
Epoch: 33, Steps: 30 | Train Loss: 0.2442879 Vali Loss: 0.3459907 Test Loss: 0.3752949
Validation loss decreased (0.346886 --> 0.345991).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 6.088528394699097
Epoch: 34, Steps: 30 | Train Loss: 0.2428258 Vali Loss: 0.3468892 Test Loss: 0.3752022
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 5.837579011917114
Epoch: 35, Steps: 30 | Train Loss: 0.2416615 Vali Loss: 0.3441050 Test Loss: 0.3750695
Validation loss decreased (0.345991 --> 0.344105).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 5.289674758911133
Epoch: 36, Steps: 30 | Train Loss: 0.2400608 Vali Loss: 0.3435386 Test Loss: 0.3749469
Validation loss decreased (0.344105 --> 0.343539).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 5.653127193450928
Epoch: 37, Steps: 30 | Train Loss: 0.2390996 Vali Loss: 0.3431483 Test Loss: 0.3748096
Validation loss decreased (0.343539 --> 0.343148).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 5.405572414398193
Epoch: 38, Steps: 30 | Train Loss: 0.2376447 Vali Loss: 0.3416964 Test Loss: 0.3746894
Validation loss decreased (0.343148 --> 0.341696).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 6.291366100311279
Epoch: 39, Steps: 30 | Train Loss: 0.2365603 Vali Loss: 0.3447022 Test Loss: 0.3745875
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 6.420847177505493
Epoch: 40, Steps: 30 | Train Loss: 0.2355878 Vali Loss: 0.3434561 Test Loss: 0.3745002
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 6.272202968597412
Epoch: 41, Steps: 30 | Train Loss: 0.2345539 Vali Loss: 0.3437155 Test Loss: 0.3743812
EarlyStopping counter: 3 out of 3
Early stopping
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  40581632.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 6.947207927703857
Epoch: 1, Steps: 30 | Train Loss: 0.5713851 Vali Loss: 0.3228492 Test Loss: 0.3620109
Validation loss decreased (inf --> 0.322849).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.211104154586792
Epoch: 2, Steps: 30 | Train Loss: 0.5490235 Vali Loss: 0.3102713 Test Loss: 0.3554470
Validation loss decreased (0.322849 --> 0.310271).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.797523736953735
Epoch: 3, Steps: 30 | Train Loss: 0.5370200 Vali Loss: 0.3036360 Test Loss: 0.3523899
Validation loss decreased (0.310271 --> 0.303636).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 6.055164813995361
Epoch: 4, Steps: 30 | Train Loss: 0.5282287 Vali Loss: 0.2985951 Test Loss: 0.3510087
Validation loss decreased (0.303636 --> 0.298595).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 6.166276693344116
Epoch: 5, Steps: 30 | Train Loss: 0.5240473 Vali Loss: 0.2955065 Test Loss: 0.3505699
Validation loss decreased (0.298595 --> 0.295507).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 6.206170320510864
Epoch: 6, Steps: 30 | Train Loss: 0.5221466 Vali Loss: 0.2932504 Test Loss: 0.3505629
Validation loss decreased (0.295507 --> 0.293250).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 6.105087041854858
Epoch: 7, Steps: 30 | Train Loss: 0.5211944 Vali Loss: 0.2906013 Test Loss: 0.3507334
Validation loss decreased (0.293250 --> 0.290601).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 6.319824457168579
Epoch: 8, Steps: 30 | Train Loss: 0.5187429 Vali Loss: 0.2910237 Test Loss: 0.3510456
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.968379020690918
Epoch: 9, Steps: 30 | Train Loss: 0.5166607 Vali Loss: 0.2904899 Test Loss: 0.3512389
Validation loss decreased (0.290601 --> 0.290490).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 6.315300464630127
Epoch: 10, Steps: 30 | Train Loss: 0.5164709 Vali Loss: 0.2885666 Test Loss: 0.3514273
Validation loss decreased (0.290490 --> 0.288567).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.23929238319397
Epoch: 11, Steps: 30 | Train Loss: 0.5141124 Vali Loss: 0.2882211 Test Loss: 0.3517075
Validation loss decreased (0.288567 --> 0.288221).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.808904409408569
Epoch: 12, Steps: 30 | Train Loss: 0.5137292 Vali Loss: 0.2879807 Test Loss: 0.3518173
Validation loss decreased (0.288221 --> 0.287981).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 6.411234140396118
Epoch: 13, Steps: 30 | Train Loss: 0.5153730 Vali Loss: 0.2862573 Test Loss: 0.3518928
Validation loss decreased (0.287981 --> 0.286257).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 6.649857044219971
Epoch: 14, Steps: 30 | Train Loss: 0.5155954 Vali Loss: 0.2873626 Test Loss: 0.3519971
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.6778244972229
Epoch: 15, Steps: 30 | Train Loss: 0.5146957 Vali Loss: 0.2876669 Test Loss: 0.3521520
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.045216798782349
Epoch: 16, Steps: 30 | Train Loss: 0.5142811 Vali Loss: 0.2837477 Test Loss: 0.3523148
Validation loss decreased (0.286257 --> 0.283748).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.928528070449829
Epoch: 17, Steps: 30 | Train Loss: 0.5122763 Vali Loss: 0.2864473 Test Loss: 0.3524031
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.9774322509765625
Epoch: 18, Steps: 30 | Train Loss: 0.5131021 Vali Loss: 0.2843722 Test Loss: 0.3523571
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 6.183487892150879
Epoch: 19, Steps: 30 | Train Loss: 0.5136651 Vali Loss: 0.2851194 Test Loss: 0.3525182
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3312855064868927, mae:0.3747045397758484, rse:0.4615747928619385, corr:[0.26498103 0.26906762 0.2685529  0.26707003 0.26702148 0.26764405
 0.26767826 0.26653862 0.26520732 0.26409405 0.26310182 0.2619568
 0.26079202 0.2598586  0.25916803 0.25872362 0.258197   0.257307
 0.25602797 0.25478753 0.25374833 0.25277966 0.25147167 0.24960528
 0.24772307 0.24613796 0.24496067 0.24385393 0.2425679  0.24109861
 0.2396592  0.23815979 0.23679905 0.23546812 0.23426066 0.23301421
 0.2317983  0.2309373  0.23042865 0.22984304 0.22909977 0.22826031
 0.22734492 0.22639301 0.22540723 0.2242224  0.22280912 0.22117038
 0.21956058 0.21815246 0.21680881 0.21511455 0.2132901  0.2114963
 0.20951112 0.20765625 0.20595635 0.20429845 0.20305847 0.20228103
 0.20196097 0.20156835 0.20115902 0.20082423 0.20020187 0.19958252
 0.19887505 0.19816148 0.19758649 0.19709635 0.19641505 0.19558704
 0.19460103 0.19351697 0.19224803 0.19062763 0.18927792 0.18848054
 0.18825915 0.18808743 0.1880688  0.1876167  0.18675672 0.18600647
 0.18579789 0.18600833 0.18587856 0.18520886 0.18418328 0.18347986
 0.18317784 0.18272088 0.18229426 0.18185802 0.18162335 0.1816019
 0.18137033 0.18065527 0.17950986 0.17825048 0.17727804 0.17654449
 0.17600591 0.17522629 0.17460169 0.17432597 0.17431232 0.1742359
 0.17312373 0.17145784 0.17004278 0.16973633 0.16995205 0.17016657
 0.16981241 0.1688951  0.16795488 0.16688158 0.16570626 0.16410409
 0.16254868 0.16124406 0.16048269 0.15976037 0.15841545 0.15665838
 0.15532102 0.15459028 0.15433784 0.15388261 0.1530489  0.15176255
 0.1506836  0.14999008 0.14947084 0.14899959 0.1484535  0.14807177
 0.14784281 0.14746916 0.14676493 0.14564283 0.14452453 0.14350714
 0.14220718 0.14060026 0.13877751 0.13714504 0.13604286 0.13537885
 0.1348295  0.1335241  0.13247006 0.13221656 0.13254124 0.1325051
 0.13198562 0.13110155 0.13055746 0.13094115 0.13126189 0.13121003
 0.1306456  0.13001105 0.12996538 0.13026461 0.13032697 0.12910612
 0.12729606 0.12583755 0.12564246 0.12547928 0.1244523  0.12180247
 0.11909587 0.11737434 0.11706734 0.11728144 0.11605826 0.11431094
 0.11317758 0.11348866 0.11457399 0.11501504 0.1138842  0.11313692
 0.11378615 0.11383986 0.11269554 0.11097198 0.11387891 0.12166609]
