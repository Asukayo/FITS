Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  201607168.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.4159138202667236
Epoch: 1, Steps: 14 | Train Loss: 0.7484193 Vali Loss: 0.6725966 Test Loss: 0.4898216
Validation loss decreased (inf --> 0.672597).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.4529149532318115
Epoch: 2, Steps: 14 | Train Loss: 0.6736152 Vali Loss: 0.6301123 Test Loss: 0.4691349
Validation loss decreased (0.672597 --> 0.630112).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.098393678665161
Epoch: 3, Steps: 14 | Train Loss: 0.6269141 Vali Loss: 0.6058496 Test Loss: 0.4529313
Validation loss decreased (0.630112 --> 0.605850).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.080397605895996
Epoch: 4, Steps: 14 | Train Loss: 0.5832368 Vali Loss: 0.5858976 Test Loss: 0.4403852
Validation loss decreased (0.605850 --> 0.585898).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.1149511337280273
Epoch: 5, Steps: 14 | Train Loss: 0.5530504 Vali Loss: 0.5626327 Test Loss: 0.4307355
Validation loss decreased (0.585898 --> 0.562633).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.100965738296509
Epoch: 6, Steps: 14 | Train Loss: 0.5329136 Vali Loss: 0.5491283 Test Loss: 0.4233304
Validation loss decreased (0.562633 --> 0.549128).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.0019948482513428
Epoch: 7, Steps: 14 | Train Loss: 0.5092047 Vali Loss: 0.5404894 Test Loss: 0.4175945
Validation loss decreased (0.549128 --> 0.540489).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.1419105529785156
Epoch: 8, Steps: 14 | Train Loss: 0.4950320 Vali Loss: 0.5316952 Test Loss: 0.4129987
Validation loss decreased (0.540489 --> 0.531695).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.10042142868042
Epoch: 9, Steps: 14 | Train Loss: 0.4848497 Vali Loss: 0.5134616 Test Loss: 0.4093291
Validation loss decreased (0.531695 --> 0.513462).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.0564160346984863
Epoch: 10, Steps: 14 | Train Loss: 0.4712446 Vali Loss: 0.5159011 Test Loss: 0.4065168
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.147469997406006
Epoch: 11, Steps: 14 | Train Loss: 0.4610478 Vali Loss: 0.5052404 Test Loss: 0.4042358
Validation loss decreased (0.513462 --> 0.505240).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.138725757598877
Epoch: 12, Steps: 14 | Train Loss: 0.4536059 Vali Loss: 0.5056785 Test Loss: 0.4022953
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.257629871368408
Epoch: 13, Steps: 14 | Train Loss: 0.4472929 Vali Loss: 0.5011532 Test Loss: 0.4006731
Validation loss decreased (0.505240 --> 0.501153).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.264610767364502
Epoch: 14, Steps: 14 | Train Loss: 0.4419612 Vali Loss: 0.4968995 Test Loss: 0.3993335
Validation loss decreased (0.501153 --> 0.496899).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.0044262409210205
Epoch: 15, Steps: 14 | Train Loss: 0.4356289 Vali Loss: 0.4933704 Test Loss: 0.3982934
Validation loss decreased (0.496899 --> 0.493370).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.9092652797698975
Epoch: 16, Steps: 14 | Train Loss: 0.4291563 Vali Loss: 0.4902827 Test Loss: 0.3973518
Validation loss decreased (0.493370 --> 0.490283).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.0646228790283203
Epoch: 17, Steps: 14 | Train Loss: 0.4232256 Vali Loss: 0.4819240 Test Loss: 0.3965643
Validation loss decreased (0.490283 --> 0.481924).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.6669986248016357
Epoch: 18, Steps: 14 | Train Loss: 0.4203344 Vali Loss: 0.4845562 Test Loss: 0.3958543
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.157414674758911
Epoch: 19, Steps: 14 | Train Loss: 0.4160957 Vali Loss: 0.4800993 Test Loss: 0.3952947
Validation loss decreased (0.481924 --> 0.480099).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.6882927417755127
Epoch: 20, Steps: 14 | Train Loss: 0.4142566 Vali Loss: 0.4764957 Test Loss: 0.3947620
Validation loss decreased (0.480099 --> 0.476496).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.5473079681396484
Epoch: 21, Steps: 14 | Train Loss: 0.4079434 Vali Loss: 0.4736756 Test Loss: 0.3943394
Validation loss decreased (0.476496 --> 0.473676).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.829455614089966
Epoch: 22, Steps: 14 | Train Loss: 0.4064126 Vali Loss: 0.4763129 Test Loss: 0.3939193
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.81905198097229
Epoch: 23, Steps: 14 | Train Loss: 0.4047645 Vali Loss: 0.4750750 Test Loss: 0.3936011
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.595033645629883
Epoch: 24, Steps: 14 | Train Loss: 0.4012465 Vali Loss: 0.4703007 Test Loss: 0.3932642
Validation loss decreased (0.473676 --> 0.470301).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.7151644229888916
Epoch: 25, Steps: 14 | Train Loss: 0.3994612 Vali Loss: 0.4669734 Test Loss: 0.3929579
Validation loss decreased (0.470301 --> 0.466973).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.726240634918213
Epoch: 26, Steps: 14 | Train Loss: 0.3963197 Vali Loss: 0.4733055 Test Loss: 0.3927228
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.6392202377319336
Epoch: 27, Steps: 14 | Train Loss: 0.3948463 Vali Loss: 0.4724937 Test Loss: 0.3925154
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.8820788860321045
Epoch: 28, Steps: 14 | Train Loss: 0.3925033 Vali Loss: 0.4696531 Test Loss: 0.3922953
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  201607168.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.4076781272888184
Epoch: 1, Steps: 14 | Train Loss: 0.6912732 Vali Loss: 0.4565954 Test Loss: 0.3858609
Validation loss decreased (inf --> 0.456595).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.8332746028900146
Epoch: 2, Steps: 14 | Train Loss: 0.6788060 Vali Loss: 0.4418494 Test Loss: 0.3810988
Validation loss decreased (0.456595 --> 0.441849).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.860867738723755
Epoch: 3, Steps: 14 | Train Loss: 0.6632058 Vali Loss: 0.4358009 Test Loss: 0.3779297
Validation loss decreased (0.441849 --> 0.435801).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.887930393218994
Epoch: 4, Steps: 14 | Train Loss: 0.6623775 Vali Loss: 0.4320118 Test Loss: 0.3752924
Validation loss decreased (0.435801 --> 0.432012).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.539531946182251
Epoch: 5, Steps: 14 | Train Loss: 0.6598245 Vali Loss: 0.4250860 Test Loss: 0.3731030
Validation loss decreased (0.432012 --> 0.425086).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.514558792114258
Epoch: 6, Steps: 14 | Train Loss: 0.6489370 Vali Loss: 0.4184086 Test Loss: 0.3713072
Validation loss decreased (0.425086 --> 0.418409).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.6349306106567383
Epoch: 7, Steps: 14 | Train Loss: 0.6473829 Vali Loss: 0.4207200 Test Loss: 0.3696381
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.487612247467041
Epoch: 8, Steps: 14 | Train Loss: 0.6482297 Vali Loss: 0.4133671 Test Loss: 0.3682758
Validation loss decreased (0.418409 --> 0.413367).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.6214654445648193
Epoch: 9, Steps: 14 | Train Loss: 0.6463899 Vali Loss: 0.4105054 Test Loss: 0.3670545
Validation loss decreased (0.413367 --> 0.410505).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.592580795288086
Epoch: 10, Steps: 14 | Train Loss: 0.6386956 Vali Loss: 0.4095080 Test Loss: 0.3661444
Validation loss decreased (0.410505 --> 0.409508).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.076378107070923
Epoch: 11, Steps: 14 | Train Loss: 0.6380681 Vali Loss: 0.4085175 Test Loss: 0.3651978
Validation loss decreased (0.409508 --> 0.408518).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.529352903366089
Epoch: 12, Steps: 14 | Train Loss: 0.6402626 Vali Loss: 0.4030828 Test Loss: 0.3644464
Validation loss decreased (0.408518 --> 0.403083).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.5995593070983887
Epoch: 13, Steps: 14 | Train Loss: 0.6359458 Vali Loss: 0.4055812 Test Loss: 0.3637514
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.4937031269073486
Epoch: 14, Steps: 14 | Train Loss: 0.6328779 Vali Loss: 0.4023544 Test Loss: 0.3630237
Validation loss decreased (0.403083 --> 0.402354).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.3691844940185547
Epoch: 15, Steps: 14 | Train Loss: 0.6278841 Vali Loss: 0.3954739 Test Loss: 0.3625249
Validation loss decreased (0.402354 --> 0.395474).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.609036684036255
Epoch: 16, Steps: 14 | Train Loss: 0.6315041 Vali Loss: 0.4035681 Test Loss: 0.3621435
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.561166524887085
Epoch: 17, Steps: 14 | Train Loss: 0.6276669 Vali Loss: 0.4017474 Test Loss: 0.3616203
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.6326467990875244
Epoch: 18, Steps: 14 | Train Loss: 0.6321369 Vali Loss: 0.3928598 Test Loss: 0.3612686
Validation loss decreased (0.395474 --> 0.392860).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.714419364929199
Epoch: 19, Steps: 14 | Train Loss: 0.6254308 Vali Loss: 0.4027436 Test Loss: 0.3608556
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.8904590606689453
Epoch: 20, Steps: 14 | Train Loss: 0.6250502 Vali Loss: 0.3948772 Test Loss: 0.3605400
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.5238637924194336
Epoch: 21, Steps: 14 | Train Loss: 0.6279036 Vali Loss: 0.3979490 Test Loss: 0.3602949
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.35992953181266785, mae:0.4011937379837036, rse:0.4796762764453888, corr:[0.26016355 0.26695538 0.2629601  0.26475915 0.26568276 0.26350445
 0.26304886 0.26367158 0.26248938 0.26082492 0.26012126 0.25908947
 0.25764644 0.25644922 0.25542653 0.2543018  0.25367185 0.25322756
 0.25225753 0.25089583 0.2499097  0.24919818 0.24775338 0.24602233
 0.24462588 0.24348745 0.24207775 0.24086887 0.24022931 0.23968606
 0.2388241  0.23762666 0.23651274 0.23554601 0.23458326 0.23361379
 0.23282333 0.23237376 0.23165523 0.23058254 0.22966619 0.22915387
 0.22868265 0.22766499 0.22622137 0.22523664 0.22440511 0.22273952
 0.220618   0.21901336 0.21786813 0.2165655  0.21498579 0.21333207
 0.21186921 0.2105111  0.20854086 0.20644788 0.2052141  0.20443599
 0.20346466 0.20272657 0.20269373 0.20256977 0.20184582 0.20142497
 0.20106429 0.20039882 0.19958034 0.19893228 0.19803959 0.19680864
 0.1956664  0.1946969  0.19354159 0.19237763 0.1918092  0.19109493
 0.18961273 0.18822618 0.1878831  0.18760364 0.18690717 0.18629825
 0.18608373 0.18593426 0.18562542 0.18521854 0.18472333 0.18417285
 0.18359372 0.18305162 0.18283796 0.18301885 0.18329975 0.182716
 0.18141569 0.18060297 0.18045896 0.17972212 0.17843847 0.17779428
 0.17790395 0.17751032 0.176961   0.17683575 0.17692389 0.17616327
 0.17492275 0.17436308 0.17407434 0.17359869 0.17299962 0.17257246
 0.17197496 0.17149878 0.17116185 0.17007619 0.16854438 0.16734588
 0.1663649  0.16474012 0.16316368 0.16249824 0.16192724 0.16049078
 0.1589128  0.15827253 0.15786    0.1568097  0.15587343 0.15529731
 0.1543914  0.15327169 0.15307124 0.15313138 0.15202463 0.15056093
 0.14985995 0.14948346 0.14878629 0.14840947 0.14806949 0.14625145
 0.14332049 0.14156355 0.1407566  0.13920532 0.1376335  0.13761054
 0.1376373  0.1360856  0.13466202 0.1342527  0.13341719 0.13160267
 0.1304465  0.1303858  0.12990427 0.12883835 0.12838624 0.12880765
 0.12837814 0.12735298 0.12718779 0.12764926 0.12756395 0.12638868
 0.1246402  0.12270139 0.12155565 0.12131066 0.1204862  0.11826035
 0.11669972 0.11630186 0.11557356 0.1138119  0.112399   0.11224671
 0.11161751 0.1102773  0.1097199  0.11003914 0.10953702 0.10869358
 0.10817784 0.10782181 0.10790246 0.10866235 0.10941666 0.10844842
 0.10697622 0.10671234 0.10660638 0.105604   0.1045799  0.10448944
 0.10422795 0.10303073 0.10253108 0.10295589 0.10283194 0.10191519
 0.10155205 0.10170056 0.10082794 0.09979732 0.09987808 0.10067818
 0.10092276 0.10081606 0.1006264  0.10015191 0.10007385 0.10027026
 0.0994505  0.09721848 0.0958176  0.09637077 0.09631404 0.0949817
 0.09418532 0.09526144 0.09575666 0.09498216 0.09458512 0.09461793
 0.09358533 0.0921028  0.0917486  0.09265599 0.09312929 0.09319779
 0.09313879 0.09309997 0.09349833 0.09441934 0.09457591 0.09340584
 0.09233439 0.09199415 0.09138761 0.09051947 0.09039737 0.09074356
 0.09034196 0.08940432 0.08978387 0.09073756 0.09040843 0.08975824
 0.09012736 0.09146848 0.09215097 0.09300476 0.09423097 0.09561628
 0.0963787  0.09703634 0.09750463 0.09749733 0.09837886 0.09920097
 0.09869383 0.09732628 0.09818058 0.10029107 0.09995645 0.09819311
 0.09853306 0.10053733 0.10075888 0.09913111 0.09967784 0.10115722
 0.10115384 0.10038805 0.10096899 0.10208574 0.10217948 0.1020202
 0.1026494  0.10332559 0.10305331 0.10274798 0.10284615 0.10303045
 0.10337966 0.10323425 0.10154675 0.10026117 0.10121908 0.10203803
 0.10025879 0.09792268 0.09855061 0.10002068 0.09958684 0.09811142
 0.09805041 0.09904105 0.09913223 0.09985509 0.10086378 0.10198178
 0.10172845 0.10182729 0.10196054 0.10209368 0.10315436 0.10376006
 0.10227644 0.10028043 0.10136119 0.10201859 0.09966771 0.09835374
 0.10075074 0.10240899 0.10056411 0.10146468 0.10478669 0.10573786
 0.10411199 0.10518172 0.10726926 0.10657863 0.10662574 0.10845271
 0.10733838 0.10603407 0.10930137 0.10916269 0.11024214 0.12510075]
