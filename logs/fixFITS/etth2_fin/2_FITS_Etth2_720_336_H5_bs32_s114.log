Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17814720.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5237569
	speed: 0.1405s/iter; left time: 815.1353s
Epoch: 1 cost time: 16.550969123840332
Epoch: 1, Steps: 118 | Train Loss: 0.6149281 Vali Loss: 0.5434044 Test Loss: 0.4138550
Validation loss decreased (inf --> 0.543404).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4521121
	speed: 0.3535s/iter; left time: 2008.8587s
Epoch: 2 cost time: 16.651387691497803
Epoch: 2, Steps: 118 | Train Loss: 0.4431255 Vali Loss: 0.4821919 Test Loss: 0.3904468
Validation loss decreased (0.543404 --> 0.482192).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4671661
	speed: 0.3590s/iter; left time: 1998.0892s
Epoch: 3 cost time: 16.89485764503479
Epoch: 3, Steps: 118 | Train Loss: 0.3765186 Vali Loss: 0.4569739 Test Loss: 0.3838527
Validation loss decreased (0.482192 --> 0.456974).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2937157
	speed: 0.3418s/iter; left time: 1861.7263s
Epoch: 4 cost time: 16.23035955429077
Epoch: 4, Steps: 118 | Train Loss: 0.3376633 Vali Loss: 0.4405467 Test Loss: 0.3803391
Validation loss decreased (0.456974 --> 0.440547).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2849146
	speed: 0.3399s/iter; left time: 1811.2384s
Epoch: 5 cost time: 16.280489444732666
Epoch: 5, Steps: 118 | Train Loss: 0.3106164 Vali Loss: 0.4311012 Test Loss: 0.3773812
Validation loss decreased (0.440547 --> 0.431101).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3039323
	speed: 0.3605s/iter; left time: 1878.6785s
Epoch: 6 cost time: 16.872685432434082
Epoch: 6, Steps: 118 | Train Loss: 0.2897664 Vali Loss: 0.4244069 Test Loss: 0.3753941
Validation loss decreased (0.431101 --> 0.424407).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3192059
	speed: 0.3445s/iter; left time: 1754.3663s
Epoch: 7 cost time: 16.318295001983643
Epoch: 7, Steps: 118 | Train Loss: 0.2745300 Vali Loss: 0.4173614 Test Loss: 0.3733713
Validation loss decreased (0.424407 --> 0.417361).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2985272
	speed: 0.3476s/iter; left time: 1729.2558s
Epoch: 8 cost time: 16.56525158882141
Epoch: 8, Steps: 118 | Train Loss: 0.2625534 Vali Loss: 0.4116894 Test Loss: 0.3714373
Validation loss decreased (0.417361 --> 0.411689).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2395364
	speed: 0.3407s/iter; left time: 1654.8911s
Epoch: 9 cost time: 15.732177495956421
Epoch: 9, Steps: 118 | Train Loss: 0.2535900 Vali Loss: 0.4073676 Test Loss: 0.3697216
Validation loss decreased (0.411689 --> 0.407368).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2260614
	speed: 0.3383s/iter; left time: 1602.9827s
Epoch: 10 cost time: 16.136127471923828
Epoch: 10, Steps: 118 | Train Loss: 0.2457427 Vali Loss: 0.4055353 Test Loss: 0.3685684
Validation loss decreased (0.407368 --> 0.405535).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2383253
	speed: 0.3501s/iter; left time: 1617.9600s
Epoch: 11 cost time: 16.79854154586792
Epoch: 11, Steps: 118 | Train Loss: 0.2397640 Vali Loss: 0.4031168 Test Loss: 0.3676980
Validation loss decreased (0.405535 --> 0.403117).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3024974
	speed: 0.3248s/iter; left time: 1462.6856s
Epoch: 12 cost time: 13.8853120803833
Epoch: 12, Steps: 118 | Train Loss: 0.2352822 Vali Loss: 0.4016925 Test Loss: 0.3668936
Validation loss decreased (0.403117 --> 0.401692).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3373186
	speed: 0.2929s/iter; left time: 1284.2227s
Epoch: 13 cost time: 14.71503758430481
Epoch: 13, Steps: 118 | Train Loss: 0.2316617 Vali Loss: 0.3978984 Test Loss: 0.3661249
Validation loss decreased (0.401692 --> 0.397898).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2893397
	speed: 0.3405s/iter; left time: 1452.9231s
Epoch: 14 cost time: 16.843836545944214
Epoch: 14, Steps: 118 | Train Loss: 0.2279222 Vali Loss: 0.3971682 Test Loss: 0.3657691
Validation loss decreased (0.397898 --> 0.397168).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2155155
	speed: 0.3381s/iter; left time: 1402.6486s
Epoch: 15 cost time: 16.09365153312683
Epoch: 15, Steps: 118 | Train Loss: 0.2258007 Vali Loss: 0.3963384 Test Loss: 0.3652329
Validation loss decreased (0.397168 --> 0.396338).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2736751
	speed: 0.3439s/iter; left time: 1386.2644s
Epoch: 16 cost time: 16.733642578125
Epoch: 16, Steps: 118 | Train Loss: 0.2234440 Vali Loss: 0.3940384 Test Loss: 0.3648608
Validation loss decreased (0.396338 --> 0.394038).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1884761
	speed: 0.3561s/iter; left time: 1393.2369s
Epoch: 17 cost time: 15.985836029052734
Epoch: 17, Steps: 118 | Train Loss: 0.2224202 Vali Loss: 0.3945749 Test Loss: 0.3647253
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2214526
	speed: 0.3329s/iter; left time: 1263.3270s
Epoch: 18 cost time: 15.833463907241821
Epoch: 18, Steps: 118 | Train Loss: 0.2209720 Vali Loss: 0.3926057 Test Loss: 0.3644416
Validation loss decreased (0.394038 --> 0.392606).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1692982
	speed: 0.3542s/iter; left time: 1302.4172s
Epoch: 19 cost time: 16.85831594467163
Epoch: 19, Steps: 118 | Train Loss: 0.2200835 Vali Loss: 0.3918516 Test Loss: 0.3643040
Validation loss decreased (0.392606 --> 0.391852).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2818402
	speed: 0.3424s/iter; left time: 1218.6562s
Epoch: 20 cost time: 15.98386836051941
Epoch: 20, Steps: 118 | Train Loss: 0.2189190 Vali Loss: 0.3915522 Test Loss: 0.3643033
Validation loss decreased (0.391852 --> 0.391552).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2197920
	speed: 0.3431s/iter; left time: 1180.4699s
Epoch: 21 cost time: 16.52375841140747
Epoch: 21, Steps: 118 | Train Loss: 0.2185212 Vali Loss: 0.3908219 Test Loss: 0.3640998
Validation loss decreased (0.391552 --> 0.390822).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2322098
	speed: 0.3385s/iter; left time: 1124.8456s
Epoch: 22 cost time: 14.52426028251648
Epoch: 22, Steps: 118 | Train Loss: 0.2177189 Vali Loss: 0.3898888 Test Loss: 0.3641922
Validation loss decreased (0.390822 --> 0.389889).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2434961
	speed: 0.3298s/iter; left time: 1057.0727s
Epoch: 23 cost time: 15.7622549533844
Epoch: 23, Steps: 118 | Train Loss: 0.2172649 Vali Loss: 0.3897712 Test Loss: 0.3640641
Validation loss decreased (0.389889 --> 0.389771).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2125165
	speed: 0.3407s/iter; left time: 1051.6475s
Epoch: 24 cost time: 16.731425285339355
Epoch: 24, Steps: 118 | Train Loss: 0.2167683 Vali Loss: 0.3868579 Test Loss: 0.3639973
Validation loss decreased (0.389771 --> 0.386858).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1266945
	speed: 0.3448s/iter; left time: 1023.5795s
Epoch: 25 cost time: 15.587502241134644
Epoch: 25, Steps: 118 | Train Loss: 0.2157401 Vali Loss: 0.3876288 Test Loss: 0.3640314
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2172435
	speed: 0.3290s/iter; left time: 937.9363s
Epoch: 26 cost time: 15.754586458206177
Epoch: 26, Steps: 118 | Train Loss: 0.2157232 Vali Loss: 0.3846905 Test Loss: 0.3640430
Validation loss decreased (0.386858 --> 0.384690).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1608402
	speed: 0.3539s/iter; left time: 967.2274s
Epoch: 27 cost time: 17.193206787109375
Epoch: 27, Steps: 118 | Train Loss: 0.2153245 Vali Loss: 0.3869695 Test Loss: 0.3639077
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2162871
	speed: 0.3312s/iter; left time: 866.1189s
Epoch: 28 cost time: 14.469569206237793
Epoch: 28, Steps: 118 | Train Loss: 0.2149273 Vali Loss: 0.3882256 Test Loss: 0.3640655
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1984315
	speed: 0.3119s/iter; left time: 778.8940s
Epoch: 29 cost time: 14.844918251037598
Epoch: 29, Steps: 118 | Train Loss: 0.2152112 Vali Loss: 0.3867841 Test Loss: 0.3639867
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17814720.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4659838
	speed: 0.1355s/iter; left time: 786.3087s
Epoch: 1 cost time: 15.924433469772339
Epoch: 1, Steps: 118 | Train Loss: 0.6187546 Vali Loss: 0.3820586 Test Loss: 0.3614229
Validation loss decreased (inf --> 0.382059).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5358504
	speed: 0.3550s/iter; left time: 2017.4330s
Epoch: 2 cost time: 17.35940170288086
Epoch: 2, Steps: 118 | Train Loss: 0.6156267 Vali Loss: 0.3815415 Test Loss: 0.3611694
Validation loss decreased (0.382059 --> 0.381542).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5531826
	speed: 0.3672s/iter; left time: 2043.6326s
Epoch: 3 cost time: 16.410667181015015
Epoch: 3, Steps: 118 | Train Loss: 0.6152709 Vali Loss: 0.3781168 Test Loss: 0.3606924
Validation loss decreased (0.381542 --> 0.378117).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6482869
	speed: 0.3470s/iter; left time: 1889.8725s
Epoch: 4 cost time: 16.433233499526978
Epoch: 4, Steps: 118 | Train Loss: 0.6127140 Vali Loss: 0.3770039 Test Loss: 0.3603958
Validation loss decreased (0.378117 --> 0.377004).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5738267
	speed: 0.3694s/iter; left time: 1968.3192s
Epoch: 5 cost time: 17.83308172225952
Epoch: 5, Steps: 118 | Train Loss: 0.6136237 Vali Loss: 0.3764852 Test Loss: 0.3602293
Validation loss decreased (0.377004 --> 0.376485).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5265245
	speed: 0.3479s/iter; left time: 1812.9227s
Epoch: 6 cost time: 16.073764085769653
Epoch: 6, Steps: 118 | Train Loss: 0.6140003 Vali Loss: 0.3770868 Test Loss: 0.3606099
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5617868
	speed: 0.3331s/iter; left time: 1696.5046s
Epoch: 7 cost time: 16.836267709732056
Epoch: 7, Steps: 118 | Train Loss: 0.6135237 Vali Loss: 0.3760737 Test Loss: 0.3602183
Validation loss decreased (0.376485 --> 0.376074).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6994197
	speed: 0.3556s/iter; left time: 1768.9186s
Epoch: 8 cost time: 15.373379230499268
Epoch: 8, Steps: 118 | Train Loss: 0.6142968 Vali Loss: 0.3768141 Test Loss: 0.3603597
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6453880
	speed: 0.3362s/iter; left time: 1632.9085s
Epoch: 9 cost time: 16.23175597190857
Epoch: 9, Steps: 118 | Train Loss: 0.6126804 Vali Loss: 0.3757524 Test Loss: 0.3599502
Validation loss decreased (0.376074 --> 0.375752).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4250926
	speed: 0.3417s/iter; left time: 1619.3670s
Epoch: 10 cost time: 16.87709379196167
Epoch: 10, Steps: 118 | Train Loss: 0.6123067 Vali Loss: 0.3774495 Test Loss: 0.3596954
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5867936
	speed: 0.3429s/iter; left time: 1584.5063s
Epoch: 11 cost time: 15.81166410446167
Epoch: 11, Steps: 118 | Train Loss: 0.6108508 Vali Loss: 0.3773420 Test Loss: 0.3599803
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5955023
	speed: 0.3452s/iter; left time: 1554.4183s
Epoch: 12 cost time: 16.53138756752014
Epoch: 12, Steps: 118 | Train Loss: 0.6109323 Vali Loss: 0.3762867 Test Loss: 0.3596082
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3552579879760742, mae:0.39618703722953796, rse:0.4765532910823822, corr:[0.26282793 0.26614258 0.26407412 0.26318637 0.26314634 0.26224896
 0.26098043 0.26049364 0.26010266 0.25865924 0.25689837 0.25577876
 0.2551773  0.2541073  0.25281855 0.2521196  0.251908   0.25125986
 0.24991746 0.24867082 0.24788873 0.24699323 0.24529713 0.24342997
 0.24218185 0.2415003  0.24052493 0.2391557  0.23799467 0.23715946
 0.23622537 0.23510458 0.23423292 0.23350736 0.23246007 0.2312166
 0.23032741 0.2299307  0.22930433 0.22827503 0.22743684 0.227161
 0.22694813 0.2261578  0.22487813 0.22376925 0.22284979 0.22150604
 0.21963932 0.2176492  0.21586166 0.21436843 0.21312927 0.2115385
 0.20943698 0.20746033 0.20553514 0.20338735 0.201521   0.200705
 0.20083429 0.20090501 0.20064494 0.20060237 0.20078988 0.20115767
 0.20067364 0.19939381 0.19820713 0.19771807 0.19733569 0.1966753
 0.19577271 0.19493261 0.19408044 0.19280347 0.19133626 0.1898712
 0.188705   0.18786044 0.18778035 0.18804362 0.1883943  0.18871431
 0.18875125 0.18820882 0.18726812 0.1865932  0.18641536 0.18651707
 0.18660031 0.18659969 0.18694417 0.18708451 0.18645492 0.18528621
 0.18432538 0.18379226 0.18338867 0.18271092 0.18225673 0.18216611
 0.18205503 0.18111089 0.1801626  0.18001628 0.1806607  0.18081708
 0.1796049  0.17839003 0.1781996  0.17892091 0.17921892 0.17888044
 0.17812017 0.17707601 0.17571117 0.17401628 0.1729081  0.17241699
 0.1719499  0.17087422 0.16948354 0.16818044 0.16674352 0.16520125
 0.16393243 0.16361055 0.16398904 0.16383786 0.16270123 0.16141209
 0.16112076 0.16120344 0.1605109  0.15918916 0.15863709 0.15946989
 0.16027546 0.15968402 0.15820718 0.15716927 0.15683334 0.15603463
 0.15401605 0.15169577 0.14999284 0.14896189 0.14816217 0.14750913
 0.14684692 0.14573282 0.14460236 0.14394835 0.14406411 0.14434105
 0.14400826 0.14313494 0.14268541 0.14286757 0.14237472 0.14107864
 0.14001365 0.14044717 0.14204238 0.14293222 0.1423018  0.140971
 0.14039014 0.1402228  0.13934444 0.13734914 0.13538957 0.13410364
 0.1335027  0.13230368 0.13052535 0.12895842 0.12797542 0.12779602
 0.127684   0.12705404 0.12599058 0.1251998  0.12494773 0.12560013
 0.12622085 0.12590875 0.12520073 0.12483673 0.12523498 0.12548813
 0.12550353 0.1256873  0.12567678 0.12492663 0.12322247 0.12160472
 0.12134484 0.12220271 0.12330142 0.12380864 0.12365101 0.1227559
 0.12110097 0.11961992 0.11913816 0.12004491 0.12111522 0.12146191
 0.12117606 0.12124743 0.12194119 0.12243377 0.12243699 0.12217864
 0.12197038 0.12137669 0.12013321 0.11908096 0.1186342  0.11880293
 0.11840937 0.11807092 0.11837515 0.11936727 0.11997813 0.11935144
 0.11775693 0.1161577  0.11486162 0.11436773 0.11449971 0.11527058
 0.11601756 0.11648727 0.11700544 0.1180072  0.11906251 0.1194172
 0.1192035  0.1191733  0.11948702 0.11956788 0.11876829 0.1180388
 0.11849974 0.11886884 0.11794464 0.11631211 0.11573935 0.11647414
 0.11669569 0.11637083 0.11660706 0.11873583 0.1205724  0.12083411
 0.11995146 0.12013762 0.1217167  0.12285165 0.12348401 0.12422861
 0.12573208 0.12653016 0.12603873 0.12522738 0.1253409  0.1262481
 0.12639643 0.12571001 0.1259273  0.1272428  0.12880655 0.12835066
 0.12652402 0.12514596 0.12497935 0.12527223 0.12536596 0.12553076
 0.12601465 0.12643053 0.12592247 0.12504359 0.12501124 0.12572667
 0.12610824 0.12568665 0.12488618 0.12442879 0.12419565 0.12387124
 0.1238007  0.12318742 0.12149956 0.11940255 0.11879451 0.11954922
 0.11980384 0.11948676 0.11935782 0.1202281  0.12036244 0.11981492
 0.11870958 0.11883581 0.11962782 0.11978303 0.11907776 0.1186555
 0.11917137 0.1188812  0.11790795 0.11753158 0.11854242 0.11892474
 0.11706237 0.11480588 0.1148168  0.11772293 0.11983098 0.11972162
 0.11857188 0.11852242 0.11897895 0.11860167 0.1185009  0.11974153
 0.12015896 0.11816516 0.11719617 0.12013886 0.12293506 0.11537955]
