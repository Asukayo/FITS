Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  275365888.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.12654972076416
Epoch: 1, Steps: 14 | Train Loss: 1.1481970 Vali Loss: 0.8757338 Test Loss: 0.5465751
Validation loss decreased (inf --> 0.875734).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.3812255859375
Epoch: 2, Steps: 14 | Train Loss: 1.0339035 Vali Loss: 0.8259603 Test Loss: 0.5039476
Validation loss decreased (0.875734 --> 0.825960).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.403648614883423
Epoch: 3, Steps: 14 | Train Loss: 0.9648750 Vali Loss: 0.7919230 Test Loss: 0.4779481
Validation loss decreased (0.825960 --> 0.791923).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.2425568103790283
Epoch: 4, Steps: 14 | Train Loss: 0.9275777 Vali Loss: 0.7725279 Test Loss: 0.4629766
Validation loss decreased (0.791923 --> 0.772528).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.216663360595703
Epoch: 5, Steps: 14 | Train Loss: 0.9027079 Vali Loss: 0.7559249 Test Loss: 0.4539216
Validation loss decreased (0.772528 --> 0.755925).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.376403570175171
Epoch: 6, Steps: 14 | Train Loss: 0.8890926 Vali Loss: 0.7473193 Test Loss: 0.4479582
Validation loss decreased (0.755925 --> 0.747319).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.138965368270874
Epoch: 7, Steps: 14 | Train Loss: 0.8797885 Vali Loss: 0.7419250 Test Loss: 0.4437986
Validation loss decreased (0.747319 --> 0.741925).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.2225759029388428
Epoch: 8, Steps: 14 | Train Loss: 0.8726079 Vali Loss: 0.7325384 Test Loss: 0.4406974
Validation loss decreased (0.741925 --> 0.732538).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.510221481323242
Epoch: 9, Steps: 14 | Train Loss: 0.8675544 Vali Loss: 0.7285479 Test Loss: 0.4383085
Validation loss decreased (0.732538 --> 0.728548).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2204806804656982
Epoch: 10, Steps: 14 | Train Loss: 0.8621276 Vali Loss: 0.7259346 Test Loss: 0.4363038
Validation loss decreased (0.728548 --> 0.725935).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.5132174491882324
Epoch: 11, Steps: 14 | Train Loss: 0.8570702 Vali Loss: 0.7187619 Test Loss: 0.4346193
Validation loss decreased (0.725935 --> 0.718762).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.1987078189849854
Epoch: 12, Steps: 14 | Train Loss: 0.8561720 Vali Loss: 0.7163122 Test Loss: 0.4332093
Validation loss decreased (0.718762 --> 0.716312).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.054732084274292
Epoch: 13, Steps: 14 | Train Loss: 0.8550908 Vali Loss: 0.7175558 Test Loss: 0.4320531
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.109771966934204
Epoch: 14, Steps: 14 | Train Loss: 0.8510087 Vali Loss: 0.7107924 Test Loss: 0.4309672
Validation loss decreased (0.716312 --> 0.710792).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.1871588230133057
Epoch: 15, Steps: 14 | Train Loss: 0.8500797 Vali Loss: 0.7082161 Test Loss: 0.4300179
Validation loss decreased (0.710792 --> 0.708216).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.263089656829834
Epoch: 16, Steps: 14 | Train Loss: 0.8478053 Vali Loss: 0.7086508 Test Loss: 0.4291461
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.1024413108825684
Epoch: 17, Steps: 14 | Train Loss: 0.8455156 Vali Loss: 0.7052175 Test Loss: 0.4283845
Validation loss decreased (0.708216 --> 0.705218).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.1643612384796143
Epoch: 18, Steps: 14 | Train Loss: 0.8445110 Vali Loss: 0.7025098 Test Loss: 0.4276845
Validation loss decreased (0.705218 --> 0.702510).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.093444347381592
Epoch: 19, Steps: 14 | Train Loss: 0.8426690 Vali Loss: 0.7032071 Test Loss: 0.4270710
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.079890489578247
Epoch: 20, Steps: 14 | Train Loss: 0.8410274 Vali Loss: 0.7005353 Test Loss: 0.4265034
Validation loss decreased (0.702510 --> 0.700535).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.255371332168579
Epoch: 21, Steps: 14 | Train Loss: 0.8400930 Vali Loss: 0.6996547 Test Loss: 0.4259958
Validation loss decreased (0.700535 --> 0.699655).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.5417089462280273
Epoch: 22, Steps: 14 | Train Loss: 0.8387554 Vali Loss: 0.6974908 Test Loss: 0.4255004
Validation loss decreased (0.699655 --> 0.697491).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.3025221824645996
Epoch: 23, Steps: 14 | Train Loss: 0.8379097 Vali Loss: 0.6936170 Test Loss: 0.4250474
Validation loss decreased (0.697491 --> 0.693617).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.220151424407959
Epoch: 24, Steps: 14 | Train Loss: 0.8371585 Vali Loss: 0.6935127 Test Loss: 0.4246393
Validation loss decreased (0.693617 --> 0.693513).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2475712299346924
Epoch: 25, Steps: 14 | Train Loss: 0.8357856 Vali Loss: 0.6948034 Test Loss: 0.4242508
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.2428030967712402
Epoch: 26, Steps: 14 | Train Loss: 0.8354905 Vali Loss: 0.6928053 Test Loss: 0.4239028
Validation loss decreased (0.693513 --> 0.692805).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.3569695949554443
Epoch: 27, Steps: 14 | Train Loss: 0.8340130 Vali Loss: 0.6928160 Test Loss: 0.4235953
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.142718553543091
Epoch: 28, Steps: 14 | Train Loss: 0.8356873 Vali Loss: 0.6923295 Test Loss: 0.4232805
Validation loss decreased (0.692805 --> 0.692330).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.16475772857666
Epoch: 29, Steps: 14 | Train Loss: 0.8335897 Vali Loss: 0.6892533 Test Loss: 0.4229945
Validation loss decreased (0.692330 --> 0.689253).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.680058717727661
Epoch: 30, Steps: 14 | Train Loss: 0.8331500 Vali Loss: 0.6894851 Test Loss: 0.4227138
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.7322919368743896
Epoch: 31, Steps: 14 | Train Loss: 0.8318721 Vali Loss: 0.6880906 Test Loss: 0.4224514
Validation loss decreased (0.689253 --> 0.688091).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.7727296352386475
Epoch: 32, Steps: 14 | Train Loss: 0.8318055 Vali Loss: 0.6886705 Test Loss: 0.4222128
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.5236825942993164
Epoch: 33, Steps: 14 | Train Loss: 0.8307446 Vali Loss: 0.6893283 Test Loss: 0.4219925
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.588245391845703
Epoch: 34, Steps: 14 | Train Loss: 0.8327669 Vali Loss: 0.6850520 Test Loss: 0.4217857
Validation loss decreased (0.688091 --> 0.685052).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.5096232891082764
Epoch: 35, Steps: 14 | Train Loss: 0.8308063 Vali Loss: 0.6824828 Test Loss: 0.4216110
Validation loss decreased (0.685052 --> 0.682483).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.4488396644592285
Epoch: 36, Steps: 14 | Train Loss: 0.8298802 Vali Loss: 0.6881832 Test Loss: 0.4214312
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.3756048679351807
Epoch: 37, Steps: 14 | Train Loss: 0.8316770 Vali Loss: 0.6857691 Test Loss: 0.4212461
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.5748791694641113
Epoch: 38, Steps: 14 | Train Loss: 0.8300385 Vali Loss: 0.6816392 Test Loss: 0.4210686
Validation loss decreased (0.682483 --> 0.681639).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.842947006225586
Epoch: 39, Steps: 14 | Train Loss: 0.8296157 Vali Loss: 0.6826299 Test Loss: 0.4209316
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.465780258178711
Epoch: 40, Steps: 14 | Train Loss: 0.8298319 Vali Loss: 0.6837342 Test Loss: 0.4207892
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.700608015060425
Epoch: 41, Steps: 14 | Train Loss: 0.8294161 Vali Loss: 0.6835867 Test Loss: 0.4206532
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.38835838437080383, mae:0.4318124055862427, rse:0.49810633063316345, corr:[ 0.21048035  0.216075    0.21506602  0.21440719  0.21467961  0.21437709
  0.2137462   0.2130933   0.21165064  0.21071087  0.20941049  0.20757243
  0.20621873  0.20533974  0.2040005   0.2022349   0.20154148  0.20114824
  0.20015553  0.19888438  0.19817157  0.19775674  0.1960057   0.19428323
  0.19329315  0.19284905  0.19190237  0.19096422  0.19038473  0.18997355
  0.1895383   0.18872589  0.1879052   0.18694307  0.1860708   0.18497846
  0.18378289  0.18306448  0.18239208  0.18132867  0.17996335  0.17932339
  0.17939064  0.17871639  0.17705224  0.17639978  0.1761292   0.17461435
  0.1725359   0.17153539  0.17123051  0.17076477  0.17019041  0.16962215
  0.16916652  0.16859074  0.16789356  0.1671692   0.16646421  0.16583396
  0.16539294  0.16532007  0.16551368  0.16539072  0.16528921  0.16528773
  0.16515031  0.16504137  0.1649625   0.16477077  0.16428086  0.16369386
  0.16326626  0.16289438  0.16240941  0.16203842  0.16202514  0.16214235
  0.16199908  0.16177526  0.1612871   0.16076697  0.16053638  0.16038965
  0.16020729  0.16007474  0.16007634  0.15987997  0.15991302  0.16018808
  0.16019943  0.1600556   0.16001248  0.16025144  0.16047364  0.1601695
  0.15999952  0.16027652  0.16050717  0.16030225  0.15994234  0.15974334
  0.15970169  0.15958735  0.15929483  0.15899183  0.15886611  0.15856302
  0.15815386  0.15778787  0.15764078  0.15724969  0.15671048  0.1563132
  0.15637684  0.1566545   0.15632765  0.15551579  0.15498158  0.15465626
  0.15384234  0.15300399  0.15256867  0.15223639  0.15166026  0.1512288
  0.15109853  0.15103449  0.15060066  0.1497033   0.14884257  0.14826317
  0.14769027  0.14692208  0.14620996  0.14556241  0.14505829  0.14480872
  0.14438409  0.14396413  0.14369012  0.14354739  0.14289579  0.14148147
  0.14012983  0.13940199  0.13881247  0.1380829   0.13770193  0.13777998
  0.1376976   0.13730937  0.13673265  0.13588719  0.13493489  0.13419543
  0.1338567   0.13376866  0.13340364  0.13264969  0.13216999  0.13225204
  0.13257019  0.13300146  0.13354614  0.13377367  0.13340193  0.13257232
  0.13174559  0.13111718  0.13090713  0.13096529  0.13078192  0.13019976
  0.12959121  0.12926134  0.12879944  0.12787013  0.12707146  0.12700085
  0.1269764   0.1264708   0.12593068  0.1256762   0.12588198  0.12636246
  0.12676238  0.12693174  0.12690993  0.12718776  0.12750147  0.12714818
  0.1265417   0.12660828  0.12727346  0.12770902  0.12774055  0.12784213
  0.12801269  0.12774682  0.12743069  0.12758695  0.12776391  0.12728515
  0.1269585   0.12714754  0.12718871  0.12705217  0.12738155  0.12811126
  0.12832613  0.12833473  0.12853938  0.12871064  0.12820521  0.1273986
  0.12681986  0.12636338  0.12598945  0.12576143  0.12556015  0.1251825
  0.12488287  0.12492245  0.12487765  0.12471429  0.12472373  0.12482221
  0.12475153  0.12468635  0.12496119  0.12529798  0.12555283  0.12602788
  0.126611    0.12704062  0.12751065  0.1282399   0.12862498  0.12833238
  0.12818319  0.12853524  0.12885194  0.12880316  0.12880439  0.12900364
  0.12872073  0.12831244  0.12854388  0.12917776  0.1293907   0.12958503
  0.130348    0.13126619  0.13187873  0.1323935   0.13290647  0.13331398
  0.1340508   0.13512184  0.13570257  0.13605388  0.13658707  0.1369203
  0.136847    0.13709489  0.13801172  0.13844252  0.13817234  0.13844681
  0.13930525  0.13952214  0.13911061  0.13960537  0.14080213  0.14128755
  0.14110646  0.14171895  0.1426386   0.14289008  0.14335503  0.14468212
  0.14556098  0.1454106   0.14564906  0.14691857  0.14771591  0.1473889
  0.14727987  0.14785254  0.14812346  0.14785473  0.14810267  0.14886703
  0.149011    0.14836869  0.14817834  0.14890708  0.14960806  0.14978912
  0.14979444  0.15007022  0.15052813  0.1510817   0.1516314   0.15201217
  0.15223238  0.15256123  0.15284096  0.1531752   0.15378484  0.15394278
  0.15313505  0.15214808  0.15214726  0.15252683  0.1523132   0.1517264
  0.1516572   0.15179619  0.15155178  0.1514659   0.15168367  0.15157835
  0.15104885  0.1510673   0.15185453  0.15217909  0.15212403  0.15234576
  0.15277246  0.1530465   0.15343131  0.15428522  0.15483202  0.15437217
  0.15374891  0.15375786  0.15368126  0.15340525  0.15367264  0.1541342
  0.15395048  0.15335587  0.15292619  0.15281427  0.15283467  0.1532166
  0.15339325  0.15331422  0.15357308  0.15430598  0.15489061  0.1551566
  0.15579073  0.15667257  0.1568354   0.1566703   0.15690795  0.15720613
  0.15682307  0.15633684  0.15633039  0.15648657  0.1565077   0.15690541
  0.15782018  0.15826589  0.15802298  0.15801102  0.15796132  0.1576583
  0.15748216  0.1576405   0.1577958   0.15813355  0.15880187  0.15893698
  0.15840891  0.15816443  0.15869337  0.15900867  0.15873927  0.15856563
  0.15840876  0.15768717  0.15690579  0.15680633  0.15707038  0.15712877
  0.15742394  0.15817235  0.15865274  0.15880413  0.1592539   0.15985428
  0.15974541  0.15861961  0.15826361  0.15852818  0.1581743   0.15750833
  0.15758783  0.15804158  0.15817639  0.15835972  0.15851752  0.15827715
  0.1575701   0.15716635  0.15684864  0.15651582  0.15651399  0.15671638
  0.15674761  0.15652694  0.15628298  0.15593149  0.1557605   0.15593514
  0.15569288  0.15469833  0.15367293  0.15345305  0.15335256  0.15241441
  0.15099217  0.14985672  0.14922632  0.14869012  0.14800133  0.1471968
  0.14623474  0.14564586  0.14537263  0.14455418  0.14358386  0.14320798
  0.14293252  0.14209287  0.1412015   0.14142431  0.14150238  0.140517
  0.13920274  0.1388436   0.13859461  0.1382468   0.13831295  0.13859773
  0.1380771   0.13732508  0.1373455   0.13791415  0.13784295  0.13724932
  0.13707009  0.13685304  0.1364039   0.13654423  0.13692088  0.13652572
  0.13592066  0.13618478  0.13648413  0.13606872  0.13530515  0.13499516
  0.13477187  0.13395275  0.13357005  0.13394812  0.13417146  0.1337164
  0.13350257  0.13354723  0.13346837  0.13381927  0.13437234  0.13395286
  0.13251287  0.13155395  0.13111657  0.13010518  0.12913977  0.12911278
  0.12912644  0.12797174  0.12681477  0.12672696  0.12700553  0.12637757
  0.12554017  0.12519136  0.12476119  0.12402161  0.12351554  0.12338165
  0.12269863  0.12173475  0.12152486  0.12165941  0.12118164  0.1200735
  0.11900769  0.11800694  0.11687609  0.11605124  0.11565484  0.11520415
  0.11443024  0.11374688  0.11305235  0.11236886  0.11203081  0.11152993
  0.11035474  0.10912382  0.10875796  0.10857394  0.10802712  0.10772318
  0.10761099  0.10693537  0.1057695   0.1051712   0.1046578   0.10301725
  0.10087964  0.10000902  0.09970362  0.09825066  0.09676734  0.09629353
  0.09574892  0.09417962  0.09271581  0.09203606  0.09111933  0.08956769
  0.08828137  0.08753637  0.08677323  0.08582345  0.08496098  0.08404148
  0.08272046  0.08195937  0.08179545  0.08103688  0.07998679  0.07899963
  0.07764176  0.07561455  0.07411305  0.07386217  0.07360398  0.07239803
  0.07116253  0.07063853  0.07027518  0.06952212  0.06929991  0.06940147
  0.06895871  0.0682843   0.06815358  0.06775694  0.06664744  0.0655221
  0.06511796  0.06476974  0.06388652  0.06316866  0.06247151  0.0609217
  0.05878934  0.05747039  0.05663024  0.05552223  0.05463604  0.0541412
  0.05324642  0.05181478  0.05114872  0.05136691  0.05081555  0.04930392
  0.04824957  0.04826194  0.0480606   0.04717143  0.04675423  0.04699296
  0.04698062  0.046736    0.04644721  0.04586903  0.04472622  0.04341777
  0.04191411  0.04027745  0.03895146  0.03818269  0.03674098  0.03492651
  0.0337769   0.03317091  0.03183622  0.03026227  0.02969889  0.02957489
  0.02892925  0.02808541  0.02807842  0.02852698  0.02860323  0.02881348
  0.02913114  0.02924167  0.0290079   0.0291229   0.02886102  0.02764294
  0.02589786  0.02492267  0.02469443  0.02409154  0.023373    0.02330801
  0.02308111  0.0222124   0.0217651   0.02217806  0.02229614  0.02198586
  0.02180782  0.02170012  0.02133605  0.0209047   0.0206978   0.02083698
  0.02011798  0.01854734  0.01788989  0.01857767  0.01867346  0.0178149
  0.01649549  0.01547426  0.01493808  0.01444816  0.01402447  0.01341406
  0.01300828  0.01276851  0.01205485  0.01144685  0.01210325  0.01328507
  0.01273281  0.01128034  0.01156477  0.01296523  0.01324984  0.01245455
  0.01248788  0.0130291   0.01261126  0.01232864  0.01291632  0.01318781
  0.01219412  0.01120576  0.0109041   0.01037839  0.01021955  0.01017012
  0.0100349   0.0091441   0.00953934  0.01088357  0.01110806  0.01033999
  0.01060108  0.01101014  0.01077232  0.0103588   0.01123731  0.01250376
  0.01216571  0.01056759  0.01034051  0.01094085  0.01086588  0.00949831
  0.00757908  0.00586458  0.00501094  0.00384601  0.0027373   0.00277227
  0.00410845  0.00369836  0.00176009  0.00205006  0.0043695   0.00467184
  0.00149455 -0.0002715   0.00211362  0.00197311 -0.00239995 -0.00360571
 -0.00121645 -0.00385137 -0.01120992 -0.01216136 -0.00719389 -0.010732  ]
