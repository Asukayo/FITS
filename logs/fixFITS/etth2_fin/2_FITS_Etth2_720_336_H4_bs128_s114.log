Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  47065088.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 6.116002798080444
Epoch: 1, Steps: 29 | Train Loss: 0.7107273 Vali Loss: 0.6375537 Test Loss: 0.4654478
Validation loss decreased (inf --> 0.637554).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 6.497199535369873
Epoch: 2, Steps: 29 | Train Loss: 0.6090917 Vali Loss: 0.5854709 Test Loss: 0.4365202
Validation loss decreased (0.637554 --> 0.585471).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 6.254512548446655
Epoch: 3, Steps: 29 | Train Loss: 0.5454143 Vali Loss: 0.5494230 Test Loss: 0.4172721
Validation loss decreased (0.585471 --> 0.549423).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 6.2769670486450195
Epoch: 4, Steps: 29 | Train Loss: 0.5013532 Vali Loss: 0.5216013 Test Loss: 0.4049906
Validation loss decreased (0.549423 --> 0.521601).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.88796067237854
Epoch: 5, Steps: 29 | Train Loss: 0.4692620 Vali Loss: 0.5080836 Test Loss: 0.3971307
Validation loss decreased (0.521601 --> 0.508084).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 6.695807933807373
Epoch: 6, Steps: 29 | Train Loss: 0.4455146 Vali Loss: 0.4965290 Test Loss: 0.3918383
Validation loss decreased (0.508084 --> 0.496529).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 6.340692758560181
Epoch: 7, Steps: 29 | Train Loss: 0.4283989 Vali Loss: 0.4883605 Test Loss: 0.3882996
Validation loss decreased (0.496529 --> 0.488360).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.9858479499816895
Epoch: 8, Steps: 29 | Train Loss: 0.4119456 Vali Loss: 0.4786175 Test Loss: 0.3858568
Validation loss decreased (0.488360 --> 0.478618).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.865459203720093
Epoch: 9, Steps: 29 | Train Loss: 0.4001267 Vali Loss: 0.4731498 Test Loss: 0.3841055
Validation loss decreased (0.478618 --> 0.473150).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.831643581390381
Epoch: 10, Steps: 29 | Train Loss: 0.3901727 Vali Loss: 0.4662760 Test Loss: 0.3828298
Validation loss decreased (0.473150 --> 0.466276).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.095623016357422
Epoch: 11, Steps: 29 | Train Loss: 0.3816206 Vali Loss: 0.4595789 Test Loss: 0.3818440
Validation loss decreased (0.466276 --> 0.459579).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 6.077747583389282
Epoch: 12, Steps: 29 | Train Loss: 0.3742033 Vali Loss: 0.4565235 Test Loss: 0.3810708
Validation loss decreased (0.459579 --> 0.456523).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.961967706680298
Epoch: 13, Steps: 29 | Train Loss: 0.3663352 Vali Loss: 0.4543015 Test Loss: 0.3804552
Validation loss decreased (0.456523 --> 0.454302).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.018138885498047
Epoch: 14, Steps: 29 | Train Loss: 0.3594924 Vali Loss: 0.4525603 Test Loss: 0.3799411
Validation loss decreased (0.454302 --> 0.452560).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.202911853790283
Epoch: 15, Steps: 29 | Train Loss: 0.3550167 Vali Loss: 0.4483382 Test Loss: 0.3794477
Validation loss decreased (0.452560 --> 0.448338).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.9750330448150635
Epoch: 16, Steps: 29 | Train Loss: 0.3491702 Vali Loss: 0.4481742 Test Loss: 0.3790505
Validation loss decreased (0.448338 --> 0.448174).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 6.291061878204346
Epoch: 17, Steps: 29 | Train Loss: 0.3450434 Vali Loss: 0.4465742 Test Loss: 0.3786570
Validation loss decreased (0.448174 --> 0.446574).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 6.247028589248657
Epoch: 18, Steps: 29 | Train Loss: 0.3405988 Vali Loss: 0.4421724 Test Loss: 0.3784136
Validation loss decreased (0.446574 --> 0.442172).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.2133564949035645
Epoch: 19, Steps: 29 | Train Loss: 0.3365105 Vali Loss: 0.4394598 Test Loss: 0.3780142
Validation loss decreased (0.442172 --> 0.439460).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.0182836055755615
Epoch: 20, Steps: 29 | Train Loss: 0.3329162 Vali Loss: 0.4378308 Test Loss: 0.3777056
Validation loss decreased (0.439460 --> 0.437831).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.1011669635772705
Epoch: 21, Steps: 29 | Train Loss: 0.3296823 Vali Loss: 0.4372462 Test Loss: 0.3774301
Validation loss decreased (0.437831 --> 0.437246).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 6.392268419265747
Epoch: 22, Steps: 29 | Train Loss: 0.3283778 Vali Loss: 0.4348503 Test Loss: 0.3771089
Validation loss decreased (0.437246 --> 0.434850).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 6.503943920135498
Epoch: 23, Steps: 29 | Train Loss: 0.3242547 Vali Loss: 0.4363613 Test Loss: 0.3768925
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 6.14885401725769
Epoch: 24, Steps: 29 | Train Loss: 0.3225040 Vali Loss: 0.4326889 Test Loss: 0.3766192
Validation loss decreased (0.434850 --> 0.432689).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 6.461488723754883
Epoch: 25, Steps: 29 | Train Loss: 0.3189787 Vali Loss: 0.4352084 Test Loss: 0.3764370
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 6.355351686477661
Epoch: 26, Steps: 29 | Train Loss: 0.3182722 Vali Loss: 0.4326450 Test Loss: 0.3762229
Validation loss decreased (0.432689 --> 0.432645).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 6.357102155685425
Epoch: 27, Steps: 29 | Train Loss: 0.3148019 Vali Loss: 0.4325377 Test Loss: 0.3759963
Validation loss decreased (0.432645 --> 0.432538).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 5.911375045776367
Epoch: 28, Steps: 29 | Train Loss: 0.3129715 Vali Loss: 0.4290412 Test Loss: 0.3758202
Validation loss decreased (0.432538 --> 0.429041).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 6.765194892883301
Epoch: 29, Steps: 29 | Train Loss: 0.3122554 Vali Loss: 0.4302439 Test Loss: 0.3756376
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 6.3819286823272705
Epoch: 30, Steps: 29 | Train Loss: 0.3097578 Vali Loss: 0.4313926 Test Loss: 0.3754848
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 6.702727794647217
Epoch: 31, Steps: 29 | Train Loss: 0.3085414 Vali Loss: 0.4291920 Test Loss: 0.3753191
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  47065088.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 6.962896823883057
Epoch: 1, Steps: 29 | Train Loss: 0.6618323 Vali Loss: 0.4145807 Test Loss: 0.3690484
Validation loss decreased (inf --> 0.414581).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.3150224685668945
Epoch: 2, Steps: 29 | Train Loss: 0.6471860 Vali Loss: 0.4046490 Test Loss: 0.3653881
Validation loss decreased (0.414581 --> 0.404649).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.91827654838562
Epoch: 3, Steps: 29 | Train Loss: 0.6359820 Vali Loss: 0.4029511 Test Loss: 0.3629221
Validation loss decreased (0.404649 --> 0.402951).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.567991495132446
Epoch: 4, Steps: 29 | Train Loss: 0.6309464 Vali Loss: 0.3957280 Test Loss: 0.3614457
Validation loss decreased (0.402951 --> 0.395728).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 6.690516948699951
Epoch: 5, Steps: 29 | Train Loss: 0.6278953 Vali Loss: 0.3912068 Test Loss: 0.3599952
Validation loss decreased (0.395728 --> 0.391207).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 6.56502890586853
Epoch: 6, Steps: 29 | Train Loss: 0.6246540 Vali Loss: 0.3897721 Test Loss: 0.3593016
Validation loss decreased (0.391207 --> 0.389772).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.512662172317505
Epoch: 7, Steps: 29 | Train Loss: 0.6223250 Vali Loss: 0.3886891 Test Loss: 0.3588576
Validation loss decreased (0.389772 --> 0.388689).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.35003399848938
Epoch: 8, Steps: 29 | Train Loss: 0.6216129 Vali Loss: 0.3871275 Test Loss: 0.3583669
Validation loss decreased (0.388689 --> 0.387128).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 6.073130369186401
Epoch: 9, Steps: 29 | Train Loss: 0.6198357 Vali Loss: 0.3849737 Test Loss: 0.3580114
Validation loss decreased (0.387128 --> 0.384974).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 6.500110864639282
Epoch: 10, Steps: 29 | Train Loss: 0.6189176 Vali Loss: 0.3857697 Test Loss: 0.3575391
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.800416469573975
Epoch: 11, Steps: 29 | Train Loss: 0.6201984 Vali Loss: 0.3873726 Test Loss: 0.3574204
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.84158182144165
Epoch: 12, Steps: 29 | Train Loss: 0.6166925 Vali Loss: 0.3852159 Test Loss: 0.3573907
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3552716374397278, mae:0.39680349826812744, rse:0.4765624403953552, corr:[0.2603064  0.2662419  0.2655306  0.26305863 0.26291797 0.26385325
 0.2637493  0.26220337 0.2606146  0.25974813 0.25915465 0.2580504
 0.2566685  0.25540757 0.2547003  0.2542613  0.25350386 0.2521756
 0.2508204  0.25000203 0.24953805 0.24883702 0.24742417 0.24568786
 0.24425833 0.24340312 0.2425892  0.2414195  0.24005045 0.23890238
 0.23817092 0.23760091 0.23687117 0.23591445 0.23492824 0.23409547
 0.23337413 0.2327135  0.23198003 0.23118114 0.23040438 0.22959171
 0.22867627 0.22776744 0.22689801 0.2260988  0.22507824 0.22354312
 0.22163685 0.21975361 0.21815477 0.21686092 0.21578829 0.214451
 0.2124429  0.21031329 0.20862773 0.20753278 0.20687991 0.20621075
 0.20531362 0.20432748 0.20371577 0.20364155 0.20343006 0.20290212
 0.2018498  0.20079498 0.20009309 0.19988744 0.19962774 0.19901748
 0.1980768  0.1969846  0.19584367 0.1945268  0.1932673  0.19219449
 0.19148467 0.19102444 0.19075166 0.19015527 0.18923299 0.18843576
 0.18817733 0.1882517  0.18818814 0.18782891 0.1873111  0.1869009
 0.18664369 0.18624398 0.1859357  0.18583491 0.18597528 0.18607691
 0.1857229  0.18471125 0.18347122 0.18246783 0.18211912 0.18209314
 0.18192592 0.18114254 0.18028247 0.17977612 0.17992635 0.18018603
 0.1799219  0.17939857 0.17887157 0.1785902  0.17806384 0.1771805
 0.17599516 0.17493908 0.17427452 0.17349704 0.17232206 0.17073733
 0.16947801 0.16912405 0.16938955 0.16918768 0.16775644 0.16564855
 0.16393729 0.16349438 0.16381305 0.16360566 0.16234387 0.1605038
 0.15932097 0.15923254 0.15971807 0.15982828 0.15909505 0.15795122
 0.15702006 0.15654035 0.15616968 0.15546179 0.15425134 0.152626
 0.15074079 0.14894086 0.14738192 0.14623688 0.14553498 0.14518452
 0.14475824 0.14393777 0.14311351 0.14243728 0.14180396 0.14081247
 0.13939789 0.1379229  0.13704033 0.13702162 0.13731183 0.13741197
 0.13690995 0.13611376 0.13565469 0.13569984 0.13584892 0.13530134
 0.13406616 0.13250484 0.1311801  0.13016054 0.12918627 0.12773122
 0.12614755 0.1246106  0.12358437 0.12299924 0.1223551  0.12169045
 0.12111264 0.120724   0.12033725 0.11974381 0.1186518  0.11771718
 0.11736809 0.11752269 0.11797851 0.11835631 0.11873914 0.11867619
 0.11819567 0.11744225 0.11654813 0.1160614  0.1160437  0.11616869
 0.11602896 0.11532916 0.11444543 0.11371794 0.11337049 0.11313077
 0.1127452  0.11238321 0.1121498  0.11240824 0.11289048 0.11318387
 0.11293555 0.11256144 0.11249445 0.11264426 0.11254294 0.11158148
 0.11002596 0.10865483 0.10815056 0.10858078 0.10889253 0.10865257
 0.10757738 0.10671353 0.1064711  0.10677078 0.10699225 0.10654359
 0.10551064 0.10464004 0.10432769 0.1048222  0.10549697 0.10613742
 0.1065442  0.10687161 0.10715255 0.10739522 0.10746963 0.10719573
 0.10648424 0.1054358  0.10446841 0.10410116 0.10404554 0.1039622
 0.1040582  0.10411588 0.10415258 0.10414802 0.10420865 0.10445268
 0.10447796 0.10463051 0.10492689 0.10600568 0.1072777  0.10869254
 0.10959878 0.1101829  0.11085522 0.11158088 0.11266059 0.11316729
 0.11293923 0.11217842 0.11218683 0.11333539 0.11445829 0.1145008
 0.11318532 0.11157744 0.11124749 0.11220426 0.11415038 0.11517133
 0.11476491 0.11345268 0.11250572 0.11276681 0.113912   0.11482571
 0.11470723 0.11391952 0.11298752 0.1125528  0.11297104 0.11374835
 0.11405243 0.11355241 0.11247472 0.11162963 0.11112877 0.11062906
 0.11026888 0.10982283 0.10930812 0.10872528 0.10863115 0.10868528
 0.10822589 0.10786279 0.10796432 0.10907272 0.1099467  0.11077262
 0.11057774 0.11026117 0.11044299 0.11134622 0.11207041 0.11145268
 0.11001722 0.1086229  0.10892102 0.11007439 0.11031128 0.10874575
 0.10662375 0.10623249 0.10771418 0.11031008 0.11156533 0.11105453
 0.1094055  0.1090851  0.1110971  0.11333031 0.11349935 0.11203941
 0.11100797 0.11157112 0.1126711  0.11300012 0.11510523 0.11799881]
