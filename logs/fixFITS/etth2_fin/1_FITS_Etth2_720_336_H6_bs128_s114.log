Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  100803584.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.143280744552612
Epoch: 1, Steps: 29 | Train Loss: 0.8914313 Vali Loss: 0.5627932 Test Loss: 0.4241304
Validation loss decreased (inf --> 0.562793).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.99809193611145
Epoch: 2, Steps: 29 | Train Loss: 0.7540856 Vali Loss: 0.4989235 Test Loss: 0.3913455
Validation loss decreased (0.562793 --> 0.498923).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.0744948387146
Epoch: 3, Steps: 29 | Train Loss: 0.7083369 Vali Loss: 0.4703990 Test Loss: 0.3799158
Validation loss decreased (0.498923 --> 0.470399).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.239373683929443
Epoch: 4, Steps: 29 | Train Loss: 0.6838585 Vali Loss: 0.4535816 Test Loss: 0.3742440
Validation loss decreased (0.470399 --> 0.453582).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.954001426696777
Epoch: 5, Steps: 29 | Train Loss: 0.6744178 Vali Loss: 0.4400509 Test Loss: 0.3706553
Validation loss decreased (0.453582 --> 0.440051).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.091926336288452
Epoch: 6, Steps: 29 | Train Loss: 0.6630084 Vali Loss: 0.4330709 Test Loss: 0.3681532
Validation loss decreased (0.440051 --> 0.433071).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.89231538772583
Epoch: 7, Steps: 29 | Train Loss: 0.6600560 Vali Loss: 0.4265402 Test Loss: 0.3662241
Validation loss decreased (0.433071 --> 0.426540).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.8258554935455322
Epoch: 8, Steps: 29 | Train Loss: 0.6522607 Vali Loss: 0.4225055 Test Loss: 0.3648315
Validation loss decreased (0.426540 --> 0.422505).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.8737497329711914
Epoch: 9, Steps: 29 | Train Loss: 0.6489103 Vali Loss: 0.4201873 Test Loss: 0.3636552
Validation loss decreased (0.422505 --> 0.420187).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.348406076431274
Epoch: 10, Steps: 29 | Train Loss: 0.6462116 Vali Loss: 0.4128268 Test Loss: 0.3627137
Validation loss decreased (0.420187 --> 0.412827).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.23593544960022
Epoch: 11, Steps: 29 | Train Loss: 0.6417089 Vali Loss: 0.4124758 Test Loss: 0.3619630
Validation loss decreased (0.412827 --> 0.412476).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.372164964675903
Epoch: 12, Steps: 29 | Train Loss: 0.6424056 Vali Loss: 0.4087561 Test Loss: 0.3614199
Validation loss decreased (0.412476 --> 0.408756).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.456885576248169
Epoch: 13, Steps: 29 | Train Loss: 0.6380115 Vali Loss: 0.4088976 Test Loss: 0.3606957
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.929434299468994
Epoch: 14, Steps: 29 | Train Loss: 0.6339995 Vali Loss: 0.4088274 Test Loss: 0.3603147
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.9803242683410645
Epoch: 15, Steps: 29 | Train Loss: 0.6374029 Vali Loss: 0.4059244 Test Loss: 0.3599137
Validation loss decreased (0.408756 --> 0.405924).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.65299654006958
Epoch: 16, Steps: 29 | Train Loss: 0.6343219 Vali Loss: 0.4068916 Test Loss: 0.3596418
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.785525560379028
Epoch: 17, Steps: 29 | Train Loss: 0.6310044 Vali Loss: 0.4051059 Test Loss: 0.3593644
Validation loss decreased (0.405924 --> 0.405106).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.113796949386597
Epoch: 18, Steps: 29 | Train Loss: 0.6285661 Vali Loss: 0.4020531 Test Loss: 0.3590187
Validation loss decreased (0.405106 --> 0.402053).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.257011413574219
Epoch: 19, Steps: 29 | Train Loss: 0.6324156 Vali Loss: 0.4035116 Test Loss: 0.3587807
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.1645283699035645
Epoch: 20, Steps: 29 | Train Loss: 0.6310292 Vali Loss: 0.4013905 Test Loss: 0.3587207
Validation loss decreased (0.402053 --> 0.401390).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.188827753067017
Epoch: 21, Steps: 29 | Train Loss: 0.6278802 Vali Loss: 0.3985624 Test Loss: 0.3584379
Validation loss decreased (0.401390 --> 0.398562).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.732336044311523
Epoch: 22, Steps: 29 | Train Loss: 0.6283515 Vali Loss: 0.3971840 Test Loss: 0.3583155
Validation loss decreased (0.398562 --> 0.397184).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.793013095855713
Epoch: 23, Steps: 29 | Train Loss: 0.6267271 Vali Loss: 0.3960802 Test Loss: 0.3582615
Validation loss decreased (0.397184 --> 0.396080).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.993495464324951
Epoch: 24, Steps: 29 | Train Loss: 0.6265978 Vali Loss: 0.3977889 Test Loss: 0.3581892
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.893815517425537
Epoch: 25, Steps: 29 | Train Loss: 0.6242854 Vali Loss: 0.3978585 Test Loss: 0.3580438
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 5.02979850769043
Epoch: 26, Steps: 29 | Train Loss: 0.6266412 Vali Loss: 0.3983635 Test Loss: 0.3578875
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.35549527406692505, mae:0.3978445827960968, rse:0.4767123758792877, corr:[0.25729388 0.2644095  0.25994027 0.26141793 0.26286075 0.2607196
 0.26013133 0.26096812 0.2595316  0.257718   0.25715536 0.2564554
 0.2550192  0.25372812 0.25288057 0.25188944 0.2513245  0.2509603
 0.25027424 0.24908727 0.24813741 0.24760629 0.24625611 0.2447051
 0.2434572  0.24239548 0.24102597 0.23996459 0.23938937 0.23871592
 0.23786089 0.2369763  0.23631811 0.23541822 0.2344499  0.23365386
 0.23306899 0.23244832 0.2314414  0.23040003 0.22967915 0.22924402
 0.22885768 0.2279482  0.22645243 0.22537507 0.22442125 0.22289713
 0.22118221 0.21977384 0.21857214 0.21740356 0.21597074 0.21417843
 0.21266006 0.2116565  0.21025163 0.20849201 0.20721126 0.2063998
 0.2055096  0.20490535 0.20503284 0.2050336  0.20443407 0.20396405
 0.2037563  0.20351556 0.2030371  0.20243005 0.20153268 0.20043717
 0.19960539 0.19898191 0.19799069 0.19674753 0.19595759 0.19546844
 0.19467159 0.19380116 0.19322199 0.19264081 0.19214268 0.1919365
 0.19180414 0.19145797 0.19104889 0.1905887  0.19003604 0.18955217
 0.18942663 0.18935657 0.18895233 0.18855499 0.18873365 0.18884845
 0.18830825 0.18764743 0.18743655 0.18706203 0.18634264 0.18594491
 0.18589504 0.18526514 0.18434185 0.18375897 0.18362093 0.1830373
 0.18227069 0.18198605 0.18169221 0.18104056 0.18047173 0.18024935
 0.17968722 0.17887394 0.17827651 0.17756377 0.1765935  0.17562963
 0.17479488 0.1737393  0.1727318  0.17222366 0.17169935 0.17049724
 0.16923681 0.16888168 0.16868389 0.16763775 0.16645391 0.16578633
 0.16516414 0.16419902 0.16360399 0.16327652 0.16251625 0.16167413
 0.16113676 0.16045389 0.15944834 0.15892203 0.15874122 0.15746398
 0.15520182 0.1537151  0.15303251 0.1519134  0.15075058 0.15052542
 0.15020254 0.1488243  0.14769854 0.14738792 0.14680815 0.14549913
 0.14457494 0.14434038 0.14394526 0.14313585 0.14248183 0.14231478
 0.1419877  0.14160722 0.14152656 0.14140305 0.14099385 0.14020287
 0.13894655 0.13715549 0.13592316 0.13566162 0.13523422 0.13373694
 0.13246071 0.13162851 0.13065828 0.12952967 0.12883888 0.12859471
 0.12785459 0.12714331 0.12708029 0.12690741 0.12601934 0.12563774
 0.12551555 0.12473866 0.12417243 0.12475145 0.12546267 0.12454544
 0.12339263 0.12342466 0.1232179  0.12194631 0.12102587 0.12117073
 0.12089106 0.11964822 0.11924235 0.11972915 0.11962806 0.11886628
 0.11847632 0.11818494 0.1171649  0.11668019 0.11718485 0.11744206
 0.11688942 0.11679282 0.1171483  0.11669061 0.11580281 0.11531582
 0.11468411 0.11321969 0.11217104 0.11242102 0.11231465 0.11156372
 0.11121267 0.11173569 0.11140944 0.11038294 0.110037   0.10996574
 0.10915437 0.10843221 0.10837629 0.10848251 0.10826203 0.10857347
 0.10902146 0.10891669 0.10890365 0.10956462 0.10959074 0.10845567
 0.10765231 0.10759122 0.10692818 0.1059027  0.10592298 0.10635
 0.10567701 0.10452373 0.10473597 0.10537306 0.10509127 0.1050014
 0.1054828  0.10583636 0.10574976 0.10696282 0.10853137 0.10934135
 0.10942885 0.10998947 0.11031187 0.10969435 0.1098839  0.11066083
 0.11042051 0.10924242 0.10969862 0.11106896 0.11069173 0.10972334
 0.11037508 0.11132745 0.11068805 0.10984889 0.11122809 0.11197187
 0.11078848 0.11012311 0.11123437 0.11189727 0.11143737 0.11174942
 0.1126949  0.11249131 0.11156274 0.11181441 0.11233599 0.11179255
 0.11142981 0.11165351 0.11065752 0.10904285 0.1091482  0.10983544
 0.10868865 0.10689917 0.10721319 0.10798432 0.10737215 0.10643994
 0.10638792 0.10653074 0.10636891 0.10753136 0.10858497 0.10900832
 0.10886522 0.10932425 0.10891606 0.10824412 0.10932607 0.11025728
 0.10847241 0.10630598 0.10725449 0.1072949  0.10424573 0.10307998
 0.10559496 0.10642806 0.10442009 0.10600578 0.10918938 0.10892844
 0.10745131 0.10945477 0.11106711 0.10995432 0.11129218 0.11353792
 0.11083603 0.10973068 0.11514001 0.11412439 0.11310109 0.13294767]
