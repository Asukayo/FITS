Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  87105536.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.202371835708618
Epoch: 1, Steps: 30 | Train Loss: 0.7706659 Vali Loss: 0.4380453 Test Loss: 0.4101403
Validation loss decreased (inf --> 0.438045).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.324236869812012
Epoch: 2, Steps: 30 | Train Loss: 0.6336535 Vali Loss: 0.3792151 Test Loss: 0.3780473
Validation loss decreased (0.438045 --> 0.379215).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.412482738494873
Epoch: 3, Steps: 30 | Train Loss: 0.5908983 Vali Loss: 0.3491816 Test Loss: 0.3666227
Validation loss decreased (0.379215 --> 0.349182).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.1421167850494385
Epoch: 4, Steps: 30 | Train Loss: 0.5713906 Vali Loss: 0.3361627 Test Loss: 0.3611597
Validation loss decreased (0.349182 --> 0.336163).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.927361965179443
Epoch: 5, Steps: 30 | Train Loss: 0.5605032 Vali Loss: 0.3287252 Test Loss: 0.3580859
Validation loss decreased (0.336163 --> 0.328725).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.258060455322266
Epoch: 6, Steps: 30 | Train Loss: 0.5536196 Vali Loss: 0.3202947 Test Loss: 0.3561428
Validation loss decreased (0.328725 --> 0.320295).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.302949905395508
Epoch: 7, Steps: 30 | Train Loss: 0.5463716 Vali Loss: 0.3179266 Test Loss: 0.3547406
Validation loss decreased (0.320295 --> 0.317927).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.180405855178833
Epoch: 8, Steps: 30 | Train Loss: 0.5425903 Vali Loss: 0.3110247 Test Loss: 0.3537882
Validation loss decreased (0.317927 --> 0.311025).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.223819017410278
Epoch: 9, Steps: 30 | Train Loss: 0.5397903 Vali Loss: 0.3106982 Test Loss: 0.3532124
Validation loss decreased (0.311025 --> 0.310698).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.248032331466675
Epoch: 10, Steps: 30 | Train Loss: 0.5359596 Vali Loss: 0.3071487 Test Loss: 0.3528541
Validation loss decreased (0.310698 --> 0.307149).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.410574197769165
Epoch: 11, Steps: 30 | Train Loss: 0.5336955 Vali Loss: 0.3066165 Test Loss: 0.3524628
Validation loss decreased (0.307149 --> 0.306617).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.218489408493042
Epoch: 12, Steps: 30 | Train Loss: 0.5327886 Vali Loss: 0.3027848 Test Loss: 0.3522383
Validation loss decreased (0.306617 --> 0.302785).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.138242483139038
Epoch: 13, Steps: 30 | Train Loss: 0.5308249 Vali Loss: 0.3045926 Test Loss: 0.3520755
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.98997688293457
Epoch: 14, Steps: 30 | Train Loss: 0.5300068 Vali Loss: 0.3033565 Test Loss: 0.3518979
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.359117269515991
Epoch: 15, Steps: 30 | Train Loss: 0.5281616 Vali Loss: 0.3020355 Test Loss: 0.3518456
Validation loss decreased (0.302785 --> 0.302036).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.155498027801514
Epoch: 16, Steps: 30 | Train Loss: 0.5264371 Vali Loss: 0.3002898 Test Loss: 0.3518059
Validation loss decreased (0.302036 --> 0.300290).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.2489142417907715
Epoch: 17, Steps: 30 | Train Loss: 0.5246290 Vali Loss: 0.2995991 Test Loss: 0.3517049
Validation loss decreased (0.300290 --> 0.299599).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.4457972049713135
Epoch: 18, Steps: 30 | Train Loss: 0.5257157 Vali Loss: 0.2980804 Test Loss: 0.3516794
Validation loss decreased (0.299599 --> 0.298080).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.280853033065796
Epoch: 19, Steps: 30 | Train Loss: 0.5236551 Vali Loss: 0.2975033 Test Loss: 0.3516612
Validation loss decreased (0.298080 --> 0.297503).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.9796364307403564
Epoch: 20, Steps: 30 | Train Loss: 0.5239790 Vali Loss: 0.2993084 Test Loss: 0.3516357
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.919896841049194
Epoch: 21, Steps: 30 | Train Loss: 0.5216310 Vali Loss: 0.2982261 Test Loss: 0.3516364
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.893284559249878
Epoch: 22, Steps: 30 | Train Loss: 0.5202983 Vali Loss: 0.2951535 Test Loss: 0.3516421
Validation loss decreased (0.297503 --> 0.295153).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 5.350940704345703
Epoch: 23, Steps: 30 | Train Loss: 0.5218358 Vali Loss: 0.2952189 Test Loss: 0.3516251
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 5.205437183380127
Epoch: 24, Steps: 30 | Train Loss: 0.5219504 Vali Loss: 0.2956885 Test Loss: 0.3516246
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 5.335733890533447
Epoch: 25, Steps: 30 | Train Loss: 0.5216855 Vali Loss: 0.2955219 Test Loss: 0.3515908
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3308735489845276, mae:0.3759090006351471, rse:0.4612877070903778, corr:[0.26166695 0.269126   0.2662646  0.26719743 0.268457   0.2673099
 0.26670516 0.2667143  0.2657765  0.2643368  0.2629136  0.26176208
 0.26092127 0.25974053 0.2582666  0.2574482  0.25723252 0.25656477
 0.25548804 0.25440064 0.25304848 0.2516629  0.25041804 0.24906528
 0.2472021  0.24550714 0.24439076 0.24288392 0.24062769 0.23899047
 0.23851751 0.23742978 0.23587899 0.23485309 0.23416077 0.23280773
 0.23138197 0.23101576 0.23040617 0.22889566 0.22788538 0.22781247
 0.22725119 0.2259023  0.22483462 0.22413802 0.22275478 0.22079448
 0.21913137 0.2174962  0.21561937 0.21433666 0.21334948 0.21130541
 0.20891537 0.20783064 0.20671445 0.20446123 0.20294409 0.20266183
 0.20198734 0.20056671 0.20027088 0.20077425 0.20025584 0.19928867
 0.19913346 0.19917798 0.19827771 0.19703001 0.19634229 0.19567214
 0.19438478 0.19329153 0.19269109 0.19169521 0.19044088 0.18956882
 0.18898821 0.18819    0.18768345 0.18732616 0.18689662 0.18649612
 0.18633951 0.18614976 0.18546575 0.18491986 0.18495716 0.18497762
 0.18422061 0.1832269  0.18308258 0.18325444 0.18307191 0.18250489
 0.18184718 0.18119794 0.18045545 0.17955704 0.1786408  0.17773053
 0.17689325 0.176012   0.1754609  0.17512913 0.17486204 0.17452657
 0.17374201 0.1728178  0.1720502  0.17181212 0.1716546  0.17121334
 0.17041945 0.1697905  0.16927275 0.16807538 0.16669454 0.16562772
 0.16457844 0.16296774 0.1618771  0.16149038 0.16055088 0.15894736
 0.15806127 0.15777086 0.15682372 0.15527199 0.15455984 0.15415268
 0.15311457 0.15191804 0.15160936 0.15160728 0.1509122  0.15014032
 0.14951472 0.1485202  0.14765915 0.14763689 0.14737262 0.14550433
 0.14313124 0.14173101 0.14028731 0.13817146 0.13693151 0.13692063
 0.1366023  0.13511582 0.13423446 0.13385347 0.13325047 0.13270436
 0.13251127 0.13174295 0.13126594 0.13207586 0.1320758  0.13091493
 0.13041703 0.13105468 0.13079901 0.12966298 0.12997109 0.13015984
 0.12783088 0.12479416 0.12466193 0.12460075 0.12226387 0.11991392
 0.11989523 0.11850943 0.11563141 0.11535298 0.11587034 0.11437735
 0.11271501 0.11330608 0.11328202 0.11231644 0.11250106 0.1118113
 0.10798368 0.10748874 0.11169815 0.10707839 0.1053086  0.12626904]
