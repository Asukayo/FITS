Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.852030515670776
Epoch: 1, Steps: 59 | Train Loss: 0.6630233 Vali Loss: 0.5847979 Test Loss: 0.4425953
Validation loss decreased (inf --> 0.584798).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.40422248840332
Epoch: 2, Steps: 59 | Train Loss: 0.5180689 Vali Loss: 0.5195724 Test Loss: 0.4115012
Validation loss decreased (0.584798 --> 0.519572).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.229270935058594
Epoch: 3, Steps: 59 | Train Loss: 0.4497764 Vali Loss: 0.4871767 Test Loss: 0.3998267
Validation loss decreased (0.519572 --> 0.487177).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.085672616958618
Epoch: 4, Steps: 59 | Train Loss: 0.4111115 Vali Loss: 0.4679656 Test Loss: 0.3949058
Validation loss decreased (0.487177 --> 0.467966).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.928406953811646
Epoch: 5, Steps: 59 | Train Loss: 0.3846817 Vali Loss: 0.4581062 Test Loss: 0.3918915
Validation loss decreased (0.467966 --> 0.458106).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.110916376113892
Epoch: 6, Steps: 59 | Train Loss: 0.3635848 Vali Loss: 0.4486586 Test Loss: 0.3901159
Validation loss decreased (0.458106 --> 0.448659).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.391008138656616
Epoch: 7, Steps: 59 | Train Loss: 0.3480898 Vali Loss: 0.4437888 Test Loss: 0.3887329
Validation loss decreased (0.448659 --> 0.443789).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.989287614822388
Epoch: 8, Steps: 59 | Train Loss: 0.3334635 Vali Loss: 0.4397244 Test Loss: 0.3870352
Validation loss decreased (0.443789 --> 0.439724).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.022450685501099
Epoch: 9, Steps: 59 | Train Loss: 0.3218113 Vali Loss: 0.4357772 Test Loss: 0.3857048
Validation loss decreased (0.439724 --> 0.435777).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.9635488986969
Epoch: 10, Steps: 59 | Train Loss: 0.3112876 Vali Loss: 0.4302780 Test Loss: 0.3844132
Validation loss decreased (0.435777 --> 0.430278).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.850769519805908
Epoch: 11, Steps: 59 | Train Loss: 0.3028645 Vali Loss: 0.4294816 Test Loss: 0.3831335
Validation loss decreased (0.430278 --> 0.429482).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.266126155853271
Epoch: 12, Steps: 59 | Train Loss: 0.2956591 Vali Loss: 0.4241609 Test Loss: 0.3818590
Validation loss decreased (0.429482 --> 0.424161).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.136997938156128
Epoch: 13, Steps: 59 | Train Loss: 0.2887579 Vali Loss: 0.4203328 Test Loss: 0.3807758
Validation loss decreased (0.424161 --> 0.420333).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.977744102478027
Epoch: 14, Steps: 59 | Train Loss: 0.2829203 Vali Loss: 0.4199522 Test Loss: 0.3797051
Validation loss decreased (0.420333 --> 0.419952).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.698999166488647
Epoch: 15, Steps: 59 | Train Loss: 0.2775624 Vali Loss: 0.4182093 Test Loss: 0.3786706
Validation loss decreased (0.419952 --> 0.418209).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.141157627105713
Epoch: 16, Steps: 59 | Train Loss: 0.2722326 Vali Loss: 0.4181631 Test Loss: 0.3776936
Validation loss decreased (0.418209 --> 0.418163).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 7.525341272354126
Epoch: 17, Steps: 59 | Train Loss: 0.2684316 Vali Loss: 0.4167546 Test Loss: 0.3768391
Validation loss decreased (0.418163 --> 0.416755).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 7.492690563201904
Epoch: 18, Steps: 59 | Train Loss: 0.2646309 Vali Loss: 0.4143831 Test Loss: 0.3760588
Validation loss decreased (0.416755 --> 0.414383).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 8.559965372085571
Epoch: 19, Steps: 59 | Train Loss: 0.2609341 Vali Loss: 0.4138068 Test Loss: 0.3753558
Validation loss decreased (0.414383 --> 0.413807).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.169248819351196
Epoch: 20, Steps: 59 | Train Loss: 0.2574995 Vali Loss: 0.4140448 Test Loss: 0.3746875
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.136449575424194
Epoch: 21, Steps: 59 | Train Loss: 0.2546947 Vali Loss: 0.4094878 Test Loss: 0.3741482
Validation loss decreased (0.413807 --> 0.409488).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.225980281829834
Epoch: 22, Steps: 59 | Train Loss: 0.2525251 Vali Loss: 0.4098978 Test Loss: 0.3735797
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.27045202255249
Epoch: 23, Steps: 59 | Train Loss: 0.2502755 Vali Loss: 0.4092538 Test Loss: 0.3730384
Validation loss decreased (0.409488 --> 0.409254).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.308882474899292
Epoch: 24, Steps: 59 | Train Loss: 0.2482429 Vali Loss: 0.4099390 Test Loss: 0.3725942
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 9.45169472694397
Epoch: 25, Steps: 59 | Train Loss: 0.2463724 Vali Loss: 0.4068376 Test Loss: 0.3721645
Validation loss decreased (0.409254 --> 0.406838).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 8.867274522781372
Epoch: 26, Steps: 59 | Train Loss: 0.2444003 Vali Loss: 0.4052339 Test Loss: 0.3717243
Validation loss decreased (0.406838 --> 0.405234).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 9.022085666656494
Epoch: 27, Steps: 59 | Train Loss: 0.2433263 Vali Loss: 0.4080616 Test Loss: 0.3713459
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.31486177444458
Epoch: 28, Steps: 59 | Train Loss: 0.2413873 Vali Loss: 0.4054693 Test Loss: 0.3710404
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.554855823516846
Epoch: 29, Steps: 59 | Train Loss: 0.2403932 Vali Loss: 0.4042337 Test Loss: 0.3706993
Validation loss decreased (0.405234 --> 0.404234).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 6.717258930206299
Epoch: 30, Steps: 59 | Train Loss: 0.2391647 Vali Loss: 0.4001610 Test Loss: 0.3703969
Validation loss decreased (0.404234 --> 0.400161).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 6.87492561340332
Epoch: 31, Steps: 59 | Train Loss: 0.2375366 Vali Loss: 0.4069434 Test Loss: 0.3701582
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 6.364388465881348
Epoch: 32, Steps: 59 | Train Loss: 0.2369472 Vali Loss: 0.4020453 Test Loss: 0.3699124
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 8.161608695983887
Epoch: 33, Steps: 59 | Train Loss: 0.2359327 Vali Loss: 0.4022850 Test Loss: 0.3696504
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.374321699142456
Epoch: 1, Steps: 59 | Train Loss: 0.6330244 Vali Loss: 0.3917738 Test Loss: 0.3642885
Validation loss decreased (inf --> 0.391774).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.965447664260864
Epoch: 2, Steps: 59 | Train Loss: 0.6224297 Vali Loss: 0.3839439 Test Loss: 0.3614738
Validation loss decreased (0.391774 --> 0.383944).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.74557089805603
Epoch: 3, Steps: 59 | Train Loss: 0.6161019 Vali Loss: 0.3833824 Test Loss: 0.3604709
Validation loss decreased (0.383944 --> 0.383382).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.44154167175293
Epoch: 4, Steps: 59 | Train Loss: 0.6142688 Vali Loss: 0.3794169 Test Loss: 0.3598547
Validation loss decreased (0.383382 --> 0.379417).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.716081619262695
Epoch: 5, Steps: 59 | Train Loss: 0.6128680 Vali Loss: 0.3803663 Test Loss: 0.3593883
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.922540664672852
Epoch: 6, Steps: 59 | Train Loss: 0.6133777 Vali Loss: 0.3793973 Test Loss: 0.3593189
Validation loss decreased (0.379417 --> 0.379397).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.723740816116333
Epoch: 7, Steps: 59 | Train Loss: 0.6129328 Vali Loss: 0.3795503 Test Loss: 0.3590110
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.387036561965942
Epoch: 8, Steps: 59 | Train Loss: 0.6125652 Vali Loss: 0.3795829 Test Loss: 0.3589750
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.682808637619019
Epoch: 9, Steps: 59 | Train Loss: 0.6124488 Vali Loss: 0.3780252 Test Loss: 0.3587716
Validation loss decreased (0.379397 --> 0.378025).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.384018659591675
Epoch: 10, Steps: 59 | Train Loss: 0.6103862 Vali Loss: 0.3790117 Test Loss: 0.3588061
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.760608673095703
Epoch: 11, Steps: 59 | Train Loss: 0.6099225 Vali Loss: 0.3799747 Test Loss: 0.3588334
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.6428542137146
Epoch: 12, Steps: 59 | Train Loss: 0.6103788 Vali Loss: 0.3780237 Test Loss: 0.3588521
Validation loss decreased (0.378025 --> 0.378024).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.530788898468018
Epoch: 13, Steps: 59 | Train Loss: 0.6106756 Vali Loss: 0.3782325 Test Loss: 0.3587149
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.503217458724976
Epoch: 14, Steps: 59 | Train Loss: 0.6102869 Vali Loss: 0.3779076 Test Loss: 0.3585629
Validation loss decreased (0.378024 --> 0.377908).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.643264770507812
Epoch: 15, Steps: 59 | Train Loss: 0.6094505 Vali Loss: 0.3778784 Test Loss: 0.3587540
Validation loss decreased (0.377908 --> 0.377878).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.684091567993164
Epoch: 16, Steps: 59 | Train Loss: 0.6104937 Vali Loss: 0.3780126 Test Loss: 0.3586528
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.306894540786743
Epoch: 17, Steps: 59 | Train Loss: 0.6103108 Vali Loss: 0.3773750 Test Loss: 0.3585964
Validation loss decreased (0.377878 --> 0.377375).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.1423921585083
Epoch: 18, Steps: 59 | Train Loss: 0.6100449 Vali Loss: 0.3751788 Test Loss: 0.3584455
Validation loss decreased (0.377375 --> 0.375179).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.665189504623413
Epoch: 19, Steps: 59 | Train Loss: 0.6084161 Vali Loss: 0.3749042 Test Loss: 0.3586391
Validation loss decreased (0.375179 --> 0.374904).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.714983224868774
Epoch: 20, Steps: 59 | Train Loss: 0.6074802 Vali Loss: 0.3756653 Test Loss: 0.3585181
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.924472332000732
Epoch: 21, Steps: 59 | Train Loss: 0.6099418 Vali Loss: 0.3756370 Test Loss: 0.3585487
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.117228746414185
Epoch: 22, Steps: 59 | Train Loss: 0.6090182 Vali Loss: 0.3742047 Test Loss: 0.3586446
Validation loss decreased (0.374904 --> 0.374205).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.594301700592041
Epoch: 23, Steps: 59 | Train Loss: 0.6091406 Vali Loss: 0.3764071 Test Loss: 0.3584289
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.20857548713684
Epoch: 24, Steps: 59 | Train Loss: 0.6095762 Vali Loss: 0.3741307 Test Loss: 0.3583936
Validation loss decreased (0.374205 --> 0.374131).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.018489360809326
Epoch: 25, Steps: 59 | Train Loss: 0.6097826 Vali Loss: 0.3761770 Test Loss: 0.3584382
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 6.654388666152954
Epoch: 26, Steps: 59 | Train Loss: 0.6101766 Vali Loss: 0.3741342 Test Loss: 0.3584630
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 7.406423568725586
Epoch: 27, Steps: 59 | Train Loss: 0.6099899 Vali Loss: 0.3739660 Test Loss: 0.3585231
Validation loss decreased (0.374131 --> 0.373966).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 7.898313760757446
Epoch: 28, Steps: 59 | Train Loss: 0.6080531 Vali Loss: 0.3779287 Test Loss: 0.3584727
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.571238994598389
Epoch: 29, Steps: 59 | Train Loss: 0.6091039 Vali Loss: 0.3757510 Test Loss: 0.3584270
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 8.201749086380005
Epoch: 30, Steps: 59 | Train Loss: 0.6091099 Vali Loss: 0.3762778 Test Loss: 0.3584703
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.35423654317855835, mae:0.39535653591156006, rse:0.4758676588535309, corr:[0.2623037  0.2647327  0.263363   0.26399922 0.26310498 0.2617491
 0.26170668 0.2612332  0.25980803 0.25870544 0.25800008 0.25662014
 0.25522205 0.25413486 0.25329036 0.25273687 0.2523967  0.25178406
 0.25067425 0.249499   0.24847998 0.24758387 0.24666214 0.24549371
 0.2437404  0.24199356 0.24088944 0.24014497 0.23919325 0.23821728
 0.2375894  0.2367709  0.23549429 0.23434566 0.23349564 0.23270833
 0.23197305 0.2314723  0.23060037 0.22940384 0.22874942 0.22857176
 0.22785091 0.2264372  0.22521774 0.22446865 0.22343701 0.22180526
 0.22032626 0.21903658 0.21734662 0.21565098 0.21460052 0.2133673
 0.21137483 0.20942794 0.20763813 0.20591527 0.2047531  0.20385817
 0.20259425 0.2015904  0.2018044  0.202355   0.20177014 0.20126298
 0.20129505 0.20099606 0.19988953 0.1990601  0.19877498 0.19813518
 0.1966493  0.19552962 0.19519459 0.19441621 0.19328852 0.19244604
 0.19181982 0.19095056 0.19068664 0.19071008 0.19023393 0.18938741
 0.18901941 0.18919027 0.18903741 0.18853089 0.18818481 0.18796618
 0.1872685  0.18648893 0.18668324 0.18688968 0.18640538 0.18592845
 0.18601301 0.18578883 0.18483503 0.18382885 0.18335435 0.18286732
 0.18263026 0.18253861 0.18217973 0.18109986 0.18073912 0.18151297
 0.18150385 0.18033661 0.17941783 0.17965169 0.1794498  0.17859255
 0.1780408  0.17808376 0.17782357 0.17671122 0.17573017 0.17472339
 0.17341009 0.17214845 0.17135951 0.17039503 0.16916837 0.16844983
 0.16787478 0.16676103 0.16585281 0.16574514 0.16545646 0.1640843
 0.16298413 0.16283266 0.16273616 0.1620822  0.16150281 0.16126868
 0.16043891 0.15946235 0.1596027  0.15999593 0.15906088 0.15704785
 0.15540278 0.15450291 0.15321122 0.15169854 0.15095851 0.15090056
 0.15019958 0.14830619 0.14677052 0.1464079  0.14668764 0.14630538
 0.14501625 0.1440899  0.14450192 0.14530787 0.14498411 0.14437614
 0.14381722 0.1429854  0.14245795 0.14312665 0.1440669  0.14310765
 0.14082979 0.13905245 0.13830048 0.13745318 0.1365898  0.13574497
 0.134675   0.13275532 0.13145119 0.13117363 0.13027187 0.12894268
 0.12828681 0.12835059 0.12814419 0.12795536 0.12809014 0.12844037
 0.12808718 0.12764186 0.12806101 0.12843135 0.12863012 0.12871456
 0.12854654 0.12746848 0.12603714 0.12587336 0.12633896 0.1260293
 0.12529683 0.12476339 0.12433207 0.12370741 0.12360733 0.12375856
 0.12310681 0.12249631 0.12256207 0.12283197 0.12246566 0.1227519
 0.12410817 0.12487337 0.12432413 0.12393104 0.12430413 0.12376639
 0.12233967 0.12145814 0.12090518 0.12001826 0.1194533  0.12047213
 0.12097619 0.12040242 0.11995513 0.12032872 0.11989442 0.11844306
 0.1179011  0.11848713 0.11812165 0.11768439 0.11850886 0.11990827
 0.11985159 0.11923965 0.11966652 0.12050351 0.12058368 0.12039014
 0.120242   0.11956349 0.11915708 0.12002494 0.12032329 0.11877319
 0.1176869  0.11849333 0.11975324 0.11963994 0.11931997 0.12015422
 0.12061173 0.12064318 0.12097638 0.12254759 0.12354542 0.12431663
 0.12523411 0.12606303 0.12613042 0.12578292 0.12651856 0.12735234
 0.12780683 0.12763125 0.12756461 0.12786122 0.12821168 0.12797828
 0.1267964  0.12630521 0.12759677 0.12825301 0.12751333 0.12629217
 0.12645121 0.12687516 0.12650366 0.12672171 0.12797491 0.12845692
 0.12772703 0.12754923 0.1276885  0.12706898 0.12631911 0.12653962
 0.12709792 0.12695433 0.12630236 0.12554388 0.12428641 0.12299644
 0.12302659 0.12317283 0.12241641 0.12179197 0.12221474 0.12150676
 0.11940201 0.11927336 0.12076724 0.12126221 0.12029159 0.12138559
 0.12256147 0.12201403 0.12082578 0.12127654 0.1221736  0.12200768
 0.12195826 0.12144876 0.12005208 0.11876928 0.11886712 0.11835343
 0.11649708 0.11642685 0.11793587 0.11845683 0.11738271 0.11828947
 0.11944776 0.11853839 0.11814799 0.1199911  0.12042425 0.11848581
 0.11860403 0.12020896 0.11863678 0.11681749 0.11947264 0.11343872]
