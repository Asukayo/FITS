Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  100803584.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.6810996532440186
Epoch: 1, Steps: 29 | Train Loss: 0.7161453 Vali Loss: 0.6318707 Test Loss: 0.4706841
Validation loss decreased (inf --> 0.631871).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.04077935218811
Epoch: 2, Steps: 29 | Train Loss: 0.6039075 Vali Loss: 0.5767201 Test Loss: 0.4401088
Validation loss decreased (0.631871 --> 0.576720).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.351935625076294
Epoch: 3, Steps: 29 | Train Loss: 0.5385150 Vali Loss: 0.5440282 Test Loss: 0.4217475
Validation loss decreased (0.576720 --> 0.544028).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.9791059494018555
Epoch: 4, Steps: 29 | Train Loss: 0.4924655 Vali Loss: 0.5216148 Test Loss: 0.4108244
Validation loss decreased (0.544028 --> 0.521615).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.079148054122925
Epoch: 5, Steps: 29 | Train Loss: 0.4652725 Vali Loss: 0.5042479 Test Loss: 0.4041869
Validation loss decreased (0.521615 --> 0.504248).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.316592693328857
Epoch: 6, Steps: 29 | Train Loss: 0.4421597 Vali Loss: 0.4920369 Test Loss: 0.3999131
Validation loss decreased (0.504248 --> 0.492037).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.320201396942139
Epoch: 7, Steps: 29 | Train Loss: 0.4261598 Vali Loss: 0.4827869 Test Loss: 0.3971067
Validation loss decreased (0.492037 --> 0.482787).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.2473838329315186
Epoch: 8, Steps: 29 | Train Loss: 0.4113841 Vali Loss: 0.4762951 Test Loss: 0.3951082
Validation loss decreased (0.482787 --> 0.476295).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.257096529006958
Epoch: 9, Steps: 29 | Train Loss: 0.3997511 Vali Loss: 0.4710327 Test Loss: 0.3936733
Validation loss decreased (0.476295 --> 0.471033).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.415569067001343
Epoch: 10, Steps: 29 | Train Loss: 0.3904808 Vali Loss: 0.4625024 Test Loss: 0.3926239
Validation loss decreased (0.471033 --> 0.462502).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.988391637802124
Epoch: 11, Steps: 29 | Train Loss: 0.3804325 Vali Loss: 0.4607160 Test Loss: 0.3918240
Validation loss decreased (0.462502 --> 0.460716).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.936378002166748
Epoch: 12, Steps: 29 | Train Loss: 0.3743911 Vali Loss: 0.4559795 Test Loss: 0.3909889
Validation loss decreased (0.460716 --> 0.455979).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.926739454269409
Epoch: 13, Steps: 29 | Train Loss: 0.3663433 Vali Loss: 0.4542661 Test Loss: 0.3903583
Validation loss decreased (0.455979 --> 0.454266).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.292663335800171
Epoch: 14, Steps: 29 | Train Loss: 0.3597352 Vali Loss: 0.4529341 Test Loss: 0.3897502
Validation loss decreased (0.454266 --> 0.452934).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.024693489074707
Epoch: 15, Steps: 29 | Train Loss: 0.3562024 Vali Loss: 0.4497797 Test Loss: 0.3892498
Validation loss decreased (0.452934 --> 0.449780).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.195570230484009
Epoch: 16, Steps: 29 | Train Loss: 0.3502885 Vali Loss: 0.4496346 Test Loss: 0.3887777
Validation loss decreased (0.449780 --> 0.449635).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.1863319873809814
Epoch: 17, Steps: 29 | Train Loss: 0.3451725 Vali Loss: 0.4474271 Test Loss: 0.3883010
Validation loss decreased (0.449635 --> 0.447427).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.381121397018433
Epoch: 18, Steps: 29 | Train Loss: 0.3403985 Vali Loss: 0.4438532 Test Loss: 0.3878385
Validation loss decreased (0.447427 --> 0.443853).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.327116966247559
Epoch: 19, Steps: 29 | Train Loss: 0.3379285 Vali Loss: 0.4437841 Test Loss: 0.3873082
Validation loss decreased (0.443853 --> 0.443784).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.370552062988281
Epoch: 20, Steps: 29 | Train Loss: 0.3340915 Vali Loss: 0.4418162 Test Loss: 0.3869953
Validation loss decreased (0.443784 --> 0.441816).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.396679162979126
Epoch: 21, Steps: 29 | Train Loss: 0.3297904 Vali Loss: 0.4387917 Test Loss: 0.3865547
Validation loss decreased (0.441816 --> 0.438792).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.3346147537231445
Epoch: 22, Steps: 29 | Train Loss: 0.3278429 Vali Loss: 0.4365364 Test Loss: 0.3861740
Validation loss decreased (0.438792 --> 0.436536).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 5.172124862670898
Epoch: 23, Steps: 29 | Train Loss: 0.3244525 Vali Loss: 0.4352438 Test Loss: 0.3858666
Validation loss decreased (0.436536 --> 0.435244).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 5.066279411315918
Epoch: 24, Steps: 29 | Train Loss: 0.3219497 Vali Loss: 0.4358231 Test Loss: 0.3854985
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 5.275047540664673
Epoch: 25, Steps: 29 | Train Loss: 0.3187100 Vali Loss: 0.4361217 Test Loss: 0.3852105
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 5.103265047073364
Epoch: 26, Steps: 29 | Train Loss: 0.3174400 Vali Loss: 0.4364245 Test Loss: 0.3848543
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  100803584.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.241430044174194
Epoch: 1, Steps: 29 | Train Loss: 0.6680106 Vali Loss: 0.4209788 Test Loss: 0.3765426
Validation loss decreased (inf --> 0.420979).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.173105478286743
Epoch: 2, Steps: 29 | Train Loss: 0.6512347 Vali Loss: 0.4098102 Test Loss: 0.3701279
Validation loss decreased (0.420979 --> 0.409810).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.123939037322998
Epoch: 3, Steps: 29 | Train Loss: 0.6422548 Vali Loss: 0.4046561 Test Loss: 0.3664448
Validation loss decreased (0.409810 --> 0.404656).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.233051538467407
Epoch: 4, Steps: 29 | Train Loss: 0.6348749 Vali Loss: 0.3999344 Test Loss: 0.3641950
Validation loss decreased (0.404656 --> 0.399934).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.387781858444214
Epoch: 5, Steps: 29 | Train Loss: 0.6297336 Vali Loss: 0.3943819 Test Loss: 0.3621520
Validation loss decreased (0.399934 --> 0.394382).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.562670707702637
Epoch: 6, Steps: 29 | Train Loss: 0.6275871 Vali Loss: 0.3938012 Test Loss: 0.3608705
Validation loss decreased (0.394382 --> 0.393801).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.974603891372681
Epoch: 7, Steps: 29 | Train Loss: 0.6243659 Vali Loss: 0.3910184 Test Loss: 0.3599070
Validation loss decreased (0.393801 --> 0.391018).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.366568565368652
Epoch: 8, Steps: 29 | Train Loss: 0.6215975 Vali Loss: 0.3890283 Test Loss: 0.3593493
Validation loss decreased (0.391018 --> 0.389028).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.15282678604126
Epoch: 9, Steps: 29 | Train Loss: 0.6187620 Vali Loss: 0.3876378 Test Loss: 0.3587414
Validation loss decreased (0.389028 --> 0.387638).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.25850772857666
Epoch: 10, Steps: 29 | Train Loss: 0.6155143 Vali Loss: 0.3836361 Test Loss: 0.3582631
Validation loss decreased (0.387638 --> 0.383636).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.622116804122925
Epoch: 11, Steps: 29 | Train Loss: 0.6206872 Vali Loss: 0.3822689 Test Loss: 0.3580583
Validation loss decreased (0.383636 --> 0.382269).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.3430705070495605
Epoch: 12, Steps: 29 | Train Loss: 0.6166552 Vali Loss: 0.3867330 Test Loss: 0.3577231
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.360905170440674
Epoch: 13, Steps: 29 | Train Loss: 0.6156512 Vali Loss: 0.3836048 Test Loss: 0.3574304
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.063344717025757
Epoch: 14, Steps: 29 | Train Loss: 0.6127564 Vali Loss: 0.3861056 Test Loss: 0.3572106
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3552989661693573, mae:0.3967868387699127, rse:0.47658073902130127, corr:[0.26171347 0.26602346 0.26281166 0.26435176 0.26477823 0.2628577
 0.26266846 0.2631113  0.26179367 0.2601658  0.25954476 0.25851
 0.25701052 0.25584853 0.25498554 0.254113   0.25356054 0.2531074
 0.25212947 0.25079107 0.2498472  0.2491228  0.24780521 0.24629705
 0.24497257 0.24362838 0.24208654 0.24094686 0.24026833 0.23943369
 0.2384489  0.2375417  0.23651174 0.23537542 0.23468478 0.23455736
 0.23401977 0.23276477 0.23134439 0.23052077 0.23018143 0.2297995
 0.22925371 0.22832975 0.22680898 0.22541615 0.22458448 0.22357851
 0.22191758 0.21997945 0.21822143 0.21697491 0.21595888 0.21448551
 0.21265814 0.2112409  0.20992158 0.2082805  0.20668012 0.20537087
 0.20430851 0.20362681 0.20357023 0.20367138 0.20322351 0.20267251
 0.20198764 0.20162271 0.20168106 0.20148045 0.20032987 0.19896078
 0.1981848  0.19764562 0.19659413 0.19514394 0.19413492 0.19340096
 0.19261791 0.19193849 0.19158283 0.1908607  0.1900718  0.18979293
 0.18974198 0.18949766 0.18902612 0.18829913 0.18730013 0.18674062
 0.18696856 0.18712018 0.18665273 0.18603033 0.18609206 0.18630588
 0.18603097 0.18568751 0.1856821  0.18534078 0.18451694 0.18357818
 0.18274495 0.1817615  0.1814413  0.18175146 0.18194383 0.18124406
 0.18009874 0.1794452  0.17884642 0.17816207 0.17761804 0.17763397
 0.17759222 0.17711355 0.17615408 0.17476255 0.17389056 0.17343174
 0.1723703  0.1704274  0.1690945  0.16868326 0.16780263 0.16632272
 0.16558708 0.16558793 0.1645862  0.16262615 0.16176866 0.16202894
 0.16173303 0.16068973 0.16040035 0.1604861  0.15961006 0.1584434
 0.15762208 0.15653172 0.15527013 0.15498222 0.1550754  0.15344466
 0.15051611 0.14908376 0.14908327 0.14866975 0.14798614 0.14785987
 0.14691207 0.14454627 0.14317572 0.14324167 0.14249457 0.14052019
 0.13963337 0.14018852 0.1401866  0.13934663 0.13883157 0.1388206
 0.13789299 0.13681008 0.13694952 0.13738304 0.13702683 0.1362384
 0.1355489  0.13390192 0.13189386 0.13107947 0.1309417  0.12932244
 0.12716354 0.12602203 0.12581399 0.12513705 0.12404128 0.12363224
 0.12300269 0.12180841 0.12117346 0.12131995 0.12061889 0.11945774
 0.11881273 0.11881591 0.11919817 0.1198379  0.12083972 0.12083223
 0.11979415 0.11876044 0.11790832 0.1172085  0.11668544 0.11642998
 0.1161316  0.11573488 0.11605705 0.11642298 0.11578699 0.11455741
 0.11402795 0.11433466 0.11405085 0.11356436 0.11350489 0.11371958
 0.11367957 0.11374908 0.11390639 0.11350112 0.11288572 0.11236369
 0.11168073 0.11065478 0.11018101 0.11052694 0.11008567 0.10913403
 0.10887568 0.10993556 0.1101766  0.10927099 0.10863557 0.10849803
 0.10789515 0.10716922 0.10698298 0.10750455 0.10775547 0.10778017
 0.10736272 0.10697512 0.10776631 0.10954673 0.10982046 0.10790614
 0.10677955 0.10776742 0.10827713 0.10678701 0.10533697 0.10567025
 0.10620448 0.10518539 0.10448729 0.10510152 0.10562845 0.10572568
 0.10606157 0.10722819 0.1079133  0.10875237 0.10950699 0.11014897
 0.11041948 0.11142054 0.11269186 0.11269761 0.11279467 0.11358602
 0.11443449 0.11414535 0.11460152 0.11610823 0.11600192 0.11401207
 0.11305653 0.11453408 0.11596014 0.11517685 0.11436926 0.11370694
 0.11294904 0.11286153 0.11425626 0.11587877 0.11647362 0.1167178
 0.11712929 0.11696258 0.11595038 0.11550239 0.11571033 0.11561888
 0.1157041  0.11588639 0.1144194  0.11227472 0.11243071 0.11401253
 0.1135966  0.11103775 0.11010062 0.11070181 0.11044976 0.1093748
 0.10971952 0.11107931 0.11067271 0.11011261 0.11044531 0.111819
 0.11210162 0.11280058 0.11338866 0.11300627 0.11305281 0.1139207
 0.1133518  0.11043753 0.1096733  0.1108816  0.11032402 0.10835382
 0.1088603  0.11086735 0.11002055 0.10945237 0.11136953 0.11351834
 0.11314139 0.11356504 0.11546036 0.11547622 0.1147955  0.11559049
 0.11488237 0.11243866 0.11382479 0.11609818 0.11677285 0.12178718]
