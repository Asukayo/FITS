Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  24393600.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.2452469
	speed: 0.1564s/iter; left time: 860.1594s
Epoch: 1 cost time: 17.50330424308777
Epoch: 1, Steps: 112 | Train Loss: 0.9926888 Vali Loss: 0.7501793 Test Loss: 0.4171828
Validation loss decreased (inf --> 0.750179).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.8712468
	speed: 0.3716s/iter; left time: 2002.4224s
Epoch: 2 cost time: 19.197421550750732
Epoch: 2, Steps: 112 | Train Loss: 0.8662199 Vali Loss: 0.7106125 Test Loss: 0.3982186
Validation loss decreased (0.750179 --> 0.710612).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6113836
	speed: 0.3892s/iter; left time: 2054.0065s
Epoch: 3 cost time: 19.495362281799316
Epoch: 3, Steps: 112 | Train Loss: 0.8418724 Vali Loss: 0.6894010 Test Loss: 0.3906158
Validation loss decreased (0.710612 --> 0.689401).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6388530
	speed: 0.3809s/iter; left time: 1967.4155s
Epoch: 4 cost time: 18.874923944473267
Epoch: 4, Steps: 112 | Train Loss: 0.8296115 Vali Loss: 0.6774133 Test Loss: 0.3862744
Validation loss decreased (0.689401 --> 0.677413).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.7734162
	speed: 0.3856s/iter; left time: 1948.2840s
Epoch: 5 cost time: 18.84485149383545
Epoch: 5, Steps: 112 | Train Loss: 0.8217023 Vali Loss: 0.6687645 Test Loss: 0.3838117
Validation loss decreased (0.677413 --> 0.668764).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.8658066
	speed: 0.3753s/iter; left time: 1854.3949s
Epoch: 6 cost time: 18.4460186958313
Epoch: 6, Steps: 112 | Train Loss: 0.8160617 Vali Loss: 0.6637304 Test Loss: 0.3822635
Validation loss decreased (0.668764 --> 0.663730).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5417468
	speed: 0.3752s/iter; left time: 1811.6260s
Epoch: 7 cost time: 18.596078157424927
Epoch: 7, Steps: 112 | Train Loss: 0.8124656 Vali Loss: 0.6654203 Test Loss: 0.3813445
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5984441
	speed: 0.3823s/iter; left time: 1803.2360s
Epoch: 8 cost time: 19.35777187347412
Epoch: 8, Steps: 112 | Train Loss: 0.8108371 Vali Loss: 0.6581156 Test Loss: 0.3809063
Validation loss decreased (0.663730 --> 0.658116).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.7270519
	speed: 0.3754s/iter; left time: 1728.6491s
Epoch: 9 cost time: 18.052985668182373
Epoch: 9, Steps: 112 | Train Loss: 0.8084244 Vali Loss: 0.6585670 Test Loss: 0.3805845
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 1.0263209
	speed: 0.3306s/iter; left time: 1485.5470s
Epoch: 10 cost time: 14.899681806564331
Epoch: 10, Steps: 112 | Train Loss: 0.8091710 Vali Loss: 0.6560180 Test Loss: 0.3802679
Validation loss decreased (0.658116 --> 0.656018).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.7311242
	speed: 0.3533s/iter; left time: 1547.6490s
Epoch: 11 cost time: 17.77523183822632
Epoch: 11, Steps: 112 | Train Loss: 0.8067320 Vali Loss: 0.6531777 Test Loss: 0.3801692
Validation loss decreased (0.656018 --> 0.653178).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.7731645
	speed: 0.3631s/iter; left time: 1549.8973s
Epoch: 12 cost time: 18.131335258483887
Epoch: 12, Steps: 112 | Train Loss: 0.8065147 Vali Loss: 0.6540372 Test Loss: 0.3800429
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5712333
	speed: 0.3607s/iter; left time: 1499.5589s
Epoch: 13 cost time: 17.780457735061646
Epoch: 13, Steps: 112 | Train Loss: 0.8037248 Vali Loss: 0.6498503 Test Loss: 0.3799757
Validation loss decreased (0.653178 --> 0.649850).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.9749368
	speed: 0.3530s/iter; left time: 1427.8793s
Epoch: 14 cost time: 17.260497570037842
Epoch: 14, Steps: 112 | Train Loss: 0.8032992 Vali Loss: 0.6491299 Test Loss: 0.3799599
Validation loss decreased (0.649850 --> 0.649130).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.9150195
	speed: 0.3237s/iter; left time: 1272.9204s
Epoch: 15 cost time: 16.697163343429565
Epoch: 15, Steps: 112 | Train Loss: 0.8049141 Vali Loss: 0.6488633 Test Loss: 0.3799035
Validation loss decreased (0.649130 --> 0.648863).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.8416901
	speed: 0.2621s/iter; left time: 1001.5107s
Epoch: 16 cost time: 12.520759105682373
Epoch: 16, Steps: 112 | Train Loss: 0.8038555 Vali Loss: 0.6516955 Test Loss: 0.3799828
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.8534647
	speed: 0.3230s/iter; left time: 1197.8356s
Epoch: 17 cost time: 17.274961948394775
Epoch: 17, Steps: 112 | Train Loss: 0.8028931 Vali Loss: 0.6471717 Test Loss: 0.3799571
Validation loss decreased (0.648863 --> 0.647172).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.7379934
	speed: 0.3431s/iter; left time: 1234.1209s
Epoch: 18 cost time: 16.91437578201294
Epoch: 18, Steps: 112 | Train Loss: 0.8026607 Vali Loss: 0.6496409 Test Loss: 0.3798779
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.7983233
	speed: 0.3400s/iter; left time: 1184.9248s
Epoch: 19 cost time: 17.646658658981323
Epoch: 19, Steps: 112 | Train Loss: 0.8024301 Vali Loss: 0.6472434 Test Loss: 0.3799263
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.6287706
	speed: 0.3597s/iter; left time: 1213.4355s
Epoch: 20 cost time: 18.51818823814392
Epoch: 20, Steps: 112 | Train Loss: 0.8015277 Vali Loss: 0.6458879 Test Loss: 0.3799562
Validation loss decreased (0.647172 --> 0.645888).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.7372767
	speed: 0.3600s/iter; left time: 1173.8386s
Epoch: 21 cost time: 17.836947202682495
Epoch: 21, Steps: 112 | Train Loss: 0.8003872 Vali Loss: 0.6480554 Test Loss: 0.3799136
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.8900414
	speed: 0.3462s/iter; left time: 1090.3284s
Epoch: 22 cost time: 17.278040885925293
Epoch: 22, Steps: 112 | Train Loss: 0.8020849 Vali Loss: 0.6455837 Test Loss: 0.3800146
Validation loss decreased (0.645888 --> 0.645584).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.6934570
	speed: 0.3510s/iter; left time: 1065.8554s
Epoch: 23 cost time: 17.4333074092865
Epoch: 23, Steps: 112 | Train Loss: 0.7997139 Vali Loss: 0.6472878 Test Loss: 0.3800005
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.7273891
	speed: 0.3419s/iter; left time: 1000.0756s
Epoch: 24 cost time: 17.178324937820435
Epoch: 24, Steps: 112 | Train Loss: 0.8018547 Vali Loss: 0.6477837 Test Loss: 0.3800161
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.8242728
	speed: 0.3247s/iter; left time: 913.4845s
Epoch: 25 cost time: 16.22483205795288
Epoch: 25, Steps: 112 | Train Loss: 0.8008342 Vali Loss: 0.6448637 Test Loss: 0.3799708
Validation loss decreased (0.645584 --> 0.644864).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.6001223
	speed: 0.3411s/iter; left time: 921.2982s
Epoch: 26 cost time: 17.32191276550293
Epoch: 26, Steps: 112 | Train Loss: 0.8013348 Vali Loss: 0.6474274 Test Loss: 0.3800102
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.8816817
	speed: 0.3356s/iter; left time: 868.9298s
Epoch: 27 cost time: 17.338329076766968
Epoch: 27, Steps: 112 | Train Loss: 0.8005293 Vali Loss: 0.6441284 Test Loss: 0.3800255
Validation loss decreased (0.644864 --> 0.644128).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.7302212
	speed: 0.3504s/iter; left time: 867.8226s
Epoch: 28 cost time: 17.435532331466675
Epoch: 28, Steps: 112 | Train Loss: 0.8002376 Vali Loss: 0.6458797 Test Loss: 0.3800273
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.7091066
	speed: 0.3478s/iter; left time: 822.6324s
Epoch: 29 cost time: 17.646042346954346
Epoch: 29, Steps: 112 | Train Loss: 0.7999940 Vali Loss: 0.6460462 Test Loss: 0.3800258
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 1.1439055
	speed: 0.3506s/iter; left time: 789.9833s
Epoch: 30 cost time: 17.001157760620117
Epoch: 30, Steps: 112 | Train Loss: 0.7999824 Vali Loss: 0.6478958 Test Loss: 0.3800651
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.37847214937210083, mae:0.4232741892337799, rse:0.49172547459602356, corr:[ 0.21492307  0.2204378   0.21798359  0.21768263  0.21860942  0.21766447
  0.21604411  0.21553837  0.21530978  0.21384937  0.21203662  0.21092105
  0.2102424   0.2091241   0.20785528  0.20723727  0.20700456  0.2061971
  0.20479116  0.20377481  0.20325716  0.20261018  0.20128724  0.1998712
  0.19876242  0.19788553  0.19680698  0.19576852  0.19515346  0.19464421
  0.19387254  0.19281812  0.19192368  0.19108929  0.19021994  0.1894293
  0.18879287  0.18816358  0.18725511  0.1863446   0.18581401  0.1855393
  0.18503907  0.18423489  0.18336734  0.18272527  0.18199547  0.18066975
  0.17912121  0.1777219   0.1766137   0.17582607  0.17529123  0.17469142
  0.17394266  0.17330368  0.17268968  0.17192435  0.17118454  0.17090183
  0.17096712  0.17082433  0.17057316  0.17059284  0.17081961  0.17126393
  0.17142445  0.17115864  0.17079084  0.17059788  0.17045976  0.17023735
  0.1699946   0.1696888   0.16924238  0.16863179  0.16836408  0.16851526
  0.168586    0.1683516   0.16804129  0.16784517  0.16773927  0.16766895
  0.16761154  0.16763873  0.16780093  0.16804133  0.16818422  0.1681833
  0.16812536  0.16819453  0.16855738  0.16885963  0.16893406  0.16874391
  0.16850902  0.16834062  0.16814442  0.16792507  0.16772872  0.1673392
  0.16698007  0.16677952  0.16688694  0.16691634  0.16673502  0.16648601
  0.16634226  0.16629434  0.16609064  0.16566598  0.16540371  0.16567013
  0.16598226  0.16580486  0.16521947  0.16474748  0.16469434  0.16446665
  0.1636617   0.16269366  0.1620061   0.16151452  0.16089748  0.16038351
  0.16004035  0.1597087   0.15915811  0.15833768  0.1577759   0.15732697
  0.15669738  0.15578857  0.15521936  0.15528104  0.15526178  0.15470058
  0.1537351   0.15318488  0.15334167  0.15348868  0.1530386   0.15201811
  0.15072669  0.14969122  0.14889432  0.14829227  0.14791507  0.14759819
  0.1471014   0.1465158   0.1462125   0.14602914  0.14558528  0.14480852
  0.14425962  0.14412558  0.14400385  0.14350964  0.14289209  0.14295894
  0.14353064  0.14386772  0.14388885  0.14394112  0.14417681  0.14391252
  0.14298259  0.14192031  0.14152047  0.14170946  0.14178205  0.14123702
  0.14036377  0.13969652  0.13931702  0.1388838   0.13808289  0.13732797
  0.13689484  0.13674061  0.1368197   0.13693792  0.13705407  0.1371224
  0.13704374  0.1372219   0.13791156  0.13880688  0.13939479  0.13939108
  0.13923956  0.13938113  0.13949494  0.13942128  0.13942613  0.13959421
  0.13979171  0.13944305  0.13886659  0.1386074   0.13884924  0.13885275
  0.13852017  0.13836199  0.13854098  0.13887112  0.13919574  0.13964865
  0.1402842   0.1408165   0.14092588  0.14077961  0.14087781  0.14110917
  0.14092033  0.14008734  0.13927649  0.13918687  0.13936733  0.13936703
  0.13899186  0.13900027  0.13938232  0.13968305  0.13962989  0.1394198
  0.13955534  0.13989113  0.14020334  0.14050673  0.14093038  0.14155121
  0.1421975   0.1427556   0.14339484  0.14415307  0.14485054  0.14523534
  0.14539582  0.14548074  0.14561616  0.14571212  0.14565538  0.14569254
  0.146112    0.14646861  0.1466265   0.14669746  0.14718626  0.148098
  0.14898156  0.14962399  0.15024774  0.15135516  0.15255164  0.15339525
  0.1537481   0.15407278  0.15474004  0.15571925  0.15654036  0.15670624
  0.15664025  0.15696393  0.15791589  0.15889116  0.15914221  0.15884605
  0.1585983   0.15877505  0.15933782  0.15994668  0.16049726  0.16088507
  0.16105488  0.16127048  0.16151378  0.16190156  0.16246597  0.1631183
  0.16369413  0.16422476  0.16468439  0.16523759  0.16595304  0.16644375
  0.16649231  0.16615938  0.16579106  0.16565697  0.16536352  0.16479324
  0.16440186  0.1647627   0.16570005  0.16650221  0.16665533  0.16645025
  0.16647321  0.16704138  0.16752154  0.16747801  0.16736713  0.16799574
  0.16884916  0.16926342  0.16895893  0.16870737  0.16907924  0.1695018
  0.16938639  0.16882886  0.16872284  0.1690482   0.16920123  0.16884065
  0.16830988  0.1680966   0.1680161   0.16791576  0.1677381   0.16778499
  0.16793506  0.16801882  0.16830856  0.16885069  0.16965228  0.17020908
  0.17009987  0.16992939  0.17024173  0.17114109  0.17209408  0.17245936
  0.17234129  0.17231405  0.17260239  0.17273971  0.17277308  0.17276002
  0.17287815  0.17294209  0.17294048  0.17301348  0.17296173  0.17297071
  0.17297883  0.17335534  0.174325    0.17551014  0.17625274  0.17643446
  0.17634985  0.17659734  0.17715937  0.17787158  0.17854783  0.17916912
  0.1797019   0.17989543  0.17961505  0.17907438  0.17881568  0.17913862
  0.17998043  0.18091027  0.18143749  0.18180487  0.18186678  0.18161985
  0.1810282   0.18049315  0.18043418  0.18087818  0.18141232  0.18157864
  0.18144266  0.18141802  0.1815602   0.18160327  0.18150605  0.18143098
  0.18133096  0.18117619  0.18118234  0.18152513  0.18207647  0.1825048
  0.18266149  0.1827524   0.18296875  0.18327183  0.18355508  0.18364345
  0.18366972  0.18341058  0.18322478  0.18311532  0.18279168  0.18237005
  0.18209003  0.18209589  0.18217967  0.18221387  0.18211918  0.18200292
  0.1817168   0.18134594  0.18087216  0.18046162  0.18038402  0.18051927
  0.18063082  0.18068758  0.18064165  0.18043724  0.18007867  0.17979175
  0.17954853  0.17939647  0.17901833  0.17806095  0.1765801   0.17512822
  0.17434585  0.1738981   0.17332792  0.17253515  0.17178039  0.17144045
  0.17113258  0.17056873  0.16998066  0.16935484  0.16878189  0.16822918
  0.1677445   0.167512    0.16720678  0.16680543  0.16605444  0.16532049
  0.16478492  0.16478327  0.16494341  0.16512111  0.16504976  0.16461778
  0.16387422  0.16313466  0.16243005  0.16211982  0.16222903  0.16245027
  0.16250776  0.16222592  0.1618258   0.161704    0.16161595  0.16122633
  0.16061477  0.16020614  0.15996215  0.16000836  0.15992795  0.1593443
  0.15877993  0.15844136  0.1582824   0.1577027   0.15667264  0.15575631
  0.15550257  0.15554813  0.15537949  0.15522714  0.15523691  0.15516722
  0.15452036  0.15344712  0.15263611  0.15219095  0.1519536   0.15140764
  0.15055716  0.14975819  0.14936912  0.14899431  0.14836672  0.14750248
  0.14678429  0.14630406  0.14569959  0.14476945  0.14373945  0.14321573
  0.1429275   0.14239618  0.14187527  0.14175332  0.14187409  0.14136332
  0.13992448  0.1382428   0.13709146  0.13667637  0.13678795  0.13687818
  0.13615689  0.13485695  0.13366821  0.13311331  0.13295649  0.13224827
  0.13091995  0.12970981  0.12947313  0.12979515  0.12981151  0.12912013
  0.12790525  0.127081    0.1268927   0.1268172   0.12600578  0.12439057
  0.12255809  0.12140838  0.12101652  0.12045538  0.11947732  0.1181321
  0.11688019  0.11595433  0.11516767  0.11418533  0.11303248  0.11216084
  0.11178671  0.1113579   0.11025153  0.10847288  0.1068223   0.10616773
  0.10576181  0.10501754  0.1040374   0.10326795  0.10308225  0.10256363
  0.10124817  0.09938695  0.09795838  0.09721296  0.09659026  0.0955979
  0.09447028  0.09356741  0.09331033  0.09286278  0.09207366  0.09117333
  0.09080835  0.09091478  0.09084713  0.08996958  0.08881667  0.08786942
  0.08729234  0.08699159  0.0865982   0.08589432  0.08471237  0.0833412
  0.0821007   0.081347    0.08060519  0.07963206  0.07865013  0.07781988
  0.07698974  0.07581921  0.074288    0.07290828  0.07191885  0.07119206
  0.07018773  0.06908213  0.0684361   0.06844853  0.06884968  0.06887332
  0.06810925  0.06724045  0.06668612  0.0664316   0.06592515  0.06512403
  0.06419323  0.06330512  0.0620521   0.0607166   0.05925019  0.0579465
  0.05647307  0.054822    0.05327708  0.05235532  0.05200158  0.05129012
  0.05017644  0.04916108  0.04880345  0.04902076  0.0493158   0.04968495
  0.04983112  0.04991874  0.04992307  0.0499623   0.04964185  0.04920691
  0.04848874  0.04757736  0.0467006   0.04587322  0.04500993  0.04434524
  0.04389421  0.04358185  0.04318596  0.04248241  0.04134671  0.0405375
  0.04020047  0.03986528  0.03949361  0.03914578  0.03858894  0.0382979
  0.0380288   0.03743552  0.03709251  0.03736114  0.03732163  0.03656866
  0.03532245  0.03435014  0.03395096  0.03360102  0.03319225  0.03228327
  0.0309033   0.02951904  0.02866855  0.02866632  0.02912371  0.02928322
  0.02874853  0.02838438  0.02899943  0.02968586  0.02957527  0.02871204
  0.02819023  0.02870041  0.02946091  0.03000739  0.03002087  0.02964987
  0.02899678  0.02833009  0.02771818  0.02664987  0.02548626  0.02401537
  0.0231609   0.02255914  0.02260215  0.02274402  0.02265483  0.02262102
  0.02278544  0.02199818  0.02076839  0.02017086  0.02098284  0.02249718
  0.02297327  0.02137882  0.01980113  0.01940942  0.0194365   0.01811456
  0.01561532  0.01328916  0.01224402  0.01142735  0.01030779  0.00857681
  0.00701404  0.00540181  0.00447356  0.00443522  0.00423911  0.00338734
  0.00227404  0.0019113   0.00320424  0.003878    0.0028776   0.00236479
  0.00350511  0.00371457  0.00022099 -0.00257867  0.00226998  0.00836464]
