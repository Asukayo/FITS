Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  275365888.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.274576425552368
Epoch: 1, Steps: 14 | Train Loss: 0.9418047 Vali Loss: 0.9096169 Test Loss: 0.5728185
Validation loss decreased (inf --> 0.909617).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.3843300342559814
Epoch: 2, Steps: 14 | Train Loss: 0.8578099 Vali Loss: 0.8720450 Test Loss: 0.5440520
Validation loss decreased (0.909617 --> 0.872045).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.6795268058776855
Epoch: 3, Steps: 14 | Train Loss: 0.7947162 Vali Loss: 0.8414840 Test Loss: 0.5216959
Validation loss decreased (0.872045 --> 0.841484).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.5548956394195557
Epoch: 4, Steps: 14 | Train Loss: 0.7501926 Vali Loss: 0.8211401 Test Loss: 0.5042804
Validation loss decreased (0.841484 --> 0.821140).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2060956954956055
Epoch: 5, Steps: 14 | Train Loss: 0.7148557 Vali Loss: 0.8019727 Test Loss: 0.4912897
Validation loss decreased (0.821140 --> 0.801973).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.3677749633789062
Epoch: 6, Steps: 14 | Train Loss: 0.6886605 Vali Loss: 0.7904041 Test Loss: 0.4812915
Validation loss decreased (0.801973 --> 0.790404).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.3751773834228516
Epoch: 7, Steps: 14 | Train Loss: 0.6683015 Vali Loss: 0.7827350 Test Loss: 0.4736854
Validation loss decreased (0.790404 --> 0.782735).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.3864338397979736
Epoch: 8, Steps: 14 | Train Loss: 0.6517303 Vali Loss: 0.7709233 Test Loss: 0.4677671
Validation loss decreased (0.782735 --> 0.770923).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.427107810974121
Epoch: 9, Steps: 14 | Train Loss: 0.6378714 Vali Loss: 0.7651992 Test Loss: 0.4630350
Validation loss decreased (0.770923 --> 0.765199).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.460726499557495
Epoch: 10, Steps: 14 | Train Loss: 0.6263925 Vali Loss: 0.7614031 Test Loss: 0.4592758
Validation loss decreased (0.765199 --> 0.761403).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.3106284141540527
Epoch: 11, Steps: 14 | Train Loss: 0.6162631 Vali Loss: 0.7534414 Test Loss: 0.4562476
Validation loss decreased (0.761403 --> 0.753441).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.4169492721557617
Epoch: 12, Steps: 14 | Train Loss: 0.6092656 Vali Loss: 0.7500033 Test Loss: 0.4537789
Validation loss decreased (0.753441 --> 0.750003).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.563499689102173
Epoch: 13, Steps: 14 | Train Loss: 0.6036701 Vali Loss: 0.7506123 Test Loss: 0.4517937
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.660761833190918
Epoch: 14, Steps: 14 | Train Loss: 0.5969934 Vali Loss: 0.7433113 Test Loss: 0.4501082
Validation loss decreased (0.750003 --> 0.743311).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.628138542175293
Epoch: 15, Steps: 14 | Train Loss: 0.5925321 Vali Loss: 0.7404475 Test Loss: 0.4487197
Validation loss decreased (0.743311 --> 0.740447).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.4021873474121094
Epoch: 16, Steps: 14 | Train Loss: 0.5874327 Vali Loss: 0.7405624 Test Loss: 0.4475177
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.5658633708953857
Epoch: 17, Steps: 14 | Train Loss: 0.5828683 Vali Loss: 0.7373030 Test Loss: 0.4465063
Validation loss decreased (0.740447 --> 0.737303).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.6521358489990234
Epoch: 18, Steps: 14 | Train Loss: 0.5797867 Vali Loss: 0.7342113 Test Loss: 0.4456401
Validation loss decreased (0.737303 --> 0.734211).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.585893392562866
Epoch: 19, Steps: 14 | Train Loss: 0.5762048 Vali Loss: 0.7347251 Test Loss: 0.4448989
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.993389129638672
Epoch: 20, Steps: 14 | Train Loss: 0.5727184 Vali Loss: 0.7321396 Test Loss: 0.4442266
Validation loss decreased (0.734211 --> 0.732140).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.7744531631469727
Epoch: 21, Steps: 14 | Train Loss: 0.5698435 Vali Loss: 0.7310910 Test Loss: 0.4436643
Validation loss decreased (0.732140 --> 0.731091).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.8443844318389893
Epoch: 22, Steps: 14 | Train Loss: 0.5676630 Vali Loss: 0.7288067 Test Loss: 0.4431501
Validation loss decreased (0.731091 --> 0.728807).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.271860361099243
Epoch: 23, Steps: 14 | Train Loss: 0.5652652 Vali Loss: 0.7250348 Test Loss: 0.4426748
Validation loss decreased (0.728807 --> 0.725035).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.077930212020874
Epoch: 24, Steps: 14 | Train Loss: 0.5634821 Vali Loss: 0.7248676 Test Loss: 0.4422896
Validation loss decreased (0.725035 --> 0.724868).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.8958795070648193
Epoch: 25, Steps: 14 | Train Loss: 0.5606791 Vali Loss: 0.7262653 Test Loss: 0.4419026
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.8273041248321533
Epoch: 26, Steps: 14 | Train Loss: 0.5593886 Vali Loss: 0.7239783 Test Loss: 0.4415746
Validation loss decreased (0.724868 --> 0.723978).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.634587287902832
Epoch: 27, Steps: 14 | Train Loss: 0.5570746 Vali Loss: 0.7237442 Test Loss: 0.4412891
Validation loss decreased (0.723978 --> 0.723744).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.9732606410980225
Epoch: 28, Steps: 14 | Train Loss: 0.5572999 Vali Loss: 0.7236847 Test Loss: 0.4410057
Validation loss decreased (0.723744 --> 0.723685).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.0985708236694336
Epoch: 29, Steps: 14 | Train Loss: 0.5546425 Vali Loss: 0.7204829 Test Loss: 0.4407555
Validation loss decreased (0.723685 --> 0.720483).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.8109798431396484
Epoch: 30, Steps: 14 | Train Loss: 0.5534957 Vali Loss: 0.7205036 Test Loss: 0.4405022
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.987854242324829
Epoch: 31, Steps: 14 | Train Loss: 0.5516648 Vali Loss: 0.7191670 Test Loss: 0.4402824
Validation loss decreased (0.720483 --> 0.719167).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.2863926887512207
Epoch: 32, Steps: 14 | Train Loss: 0.5507401 Vali Loss: 0.7197933 Test Loss: 0.4400907
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.980639934539795
Epoch: 33, Steps: 14 | Train Loss: 0.5492223 Vali Loss: 0.7203547 Test Loss: 0.4398893
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.917525291442871
Epoch: 34, Steps: 14 | Train Loss: 0.5495300 Vali Loss: 0.7158998 Test Loss: 0.4397117
Validation loss decreased (0.719167 --> 0.715900).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.8832545280456543
Epoch: 35, Steps: 14 | Train Loss: 0.5478158 Vali Loss: 0.7136027 Test Loss: 0.4395712
Validation loss decreased (0.715900 --> 0.713603).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.3338449001312256
Epoch: 36, Steps: 14 | Train Loss: 0.5465903 Vali Loss: 0.7190505 Test Loss: 0.4394249
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.4189865589141846
Epoch: 37, Steps: 14 | Train Loss: 0.5466286 Vali Loss: 0.7166774 Test Loss: 0.4392775
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.680811643600464
Epoch: 38, Steps: 14 | Train Loss: 0.5451721 Vali Loss: 0.7127411 Test Loss: 0.4391337
Validation loss decreased (0.713603 --> 0.712741).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.5629987716674805
Epoch: 39, Steps: 14 | Train Loss: 0.5443149 Vali Loss: 0.7135423 Test Loss: 0.4390167
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.2958405017852783
Epoch: 40, Steps: 14 | Train Loss: 0.5438919 Vali Loss: 0.7146128 Test Loss: 0.4389024
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.6663730144500732
Epoch: 41, Steps: 14 | Train Loss: 0.5432953 Vali Loss: 0.7147232 Test Loss: 0.4387959
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  275365888.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.8684072494506836
Epoch: 1, Steps: 14 | Train Loss: 0.8640064 Vali Loss: 0.7064864 Test Loss: 0.4355881
Validation loss decreased (inf --> 0.706486).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.9480185508728027
Epoch: 2, Steps: 14 | Train Loss: 0.8537970 Vali Loss: 0.7020304 Test Loss: 0.4331153
Validation loss decreased (0.706486 --> 0.702030).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.8932371139526367
Epoch: 3, Steps: 14 | Train Loss: 0.8484439 Vali Loss: 0.6956334 Test Loss: 0.4309441
Validation loss decreased (0.702030 --> 0.695633).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.054516315460205
Epoch: 4, Steps: 14 | Train Loss: 0.8455809 Vali Loss: 0.6924394 Test Loss: 0.4292998
Validation loss decreased (0.695633 --> 0.692439).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.0232725143432617
Epoch: 5, Steps: 14 | Train Loss: 0.8404948 Vali Loss: 0.6874319 Test Loss: 0.4279782
Validation loss decreased (0.692439 --> 0.687432).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.0534963607788086
Epoch: 6, Steps: 14 | Train Loss: 0.8391157 Vali Loss: 0.6873788 Test Loss: 0.4268164
Validation loss decreased (0.687432 --> 0.687379).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.233916997909546
Epoch: 7, Steps: 14 | Train Loss: 0.8351478 Vali Loss: 0.6845633 Test Loss: 0.4257312
Validation loss decreased (0.687379 --> 0.684563).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.3101625442504883
Epoch: 8, Steps: 14 | Train Loss: 0.8334490 Vali Loss: 0.6799646 Test Loss: 0.4247584
Validation loss decreased (0.684563 --> 0.679965).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9095640182495117
Epoch: 9, Steps: 14 | Train Loss: 0.8308510 Vali Loss: 0.6853924 Test Loss: 0.4239101
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2876458168029785
Epoch: 10, Steps: 14 | Train Loss: 0.8297721 Vali Loss: 0.6795061 Test Loss: 0.4231254
Validation loss decreased (0.679965 --> 0.679506).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.9422733783721924
Epoch: 11, Steps: 14 | Train Loss: 0.8282105 Vali Loss: 0.6818915 Test Loss: 0.4225459
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.883349895477295
Epoch: 12, Steps: 14 | Train Loss: 0.8246352 Vali Loss: 0.6777344 Test Loss: 0.4218411
Validation loss decreased (0.679506 --> 0.677734).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.003765821456909
Epoch: 13, Steps: 14 | Train Loss: 0.8260060 Vali Loss: 0.6810510 Test Loss: 0.4213138
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.057451009750366
Epoch: 14, Steps: 14 | Train Loss: 0.8247209 Vali Loss: 0.6761376 Test Loss: 0.4207787
Validation loss decreased (0.677734 --> 0.676138).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.4812026023864746
Epoch: 15, Steps: 14 | Train Loss: 0.8227682 Vali Loss: 0.6739920 Test Loss: 0.4203091
Validation loss decreased (0.676138 --> 0.673992).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.467107057571411
Epoch: 16, Steps: 14 | Train Loss: 0.8222484 Vali Loss: 0.6712288 Test Loss: 0.4198225
Validation loss decreased (0.673992 --> 0.671229).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.403459072113037
Epoch: 17, Steps: 14 | Train Loss: 0.8218173 Vali Loss: 0.6738514 Test Loss: 0.4194483
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.548772096633911
Epoch: 18, Steps: 14 | Train Loss: 0.8212810 Vali Loss: 0.6749455 Test Loss: 0.4190919
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.646205186843872
Epoch: 19, Steps: 14 | Train Loss: 0.8189031 Vali Loss: 0.6690449 Test Loss: 0.4187496
Validation loss decreased (0.671229 --> 0.669045).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.504826784133911
Epoch: 20, Steps: 14 | Train Loss: 0.8190485 Vali Loss: 0.6657225 Test Loss: 0.4184698
Validation loss decreased (0.669045 --> 0.665722).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.3623311519622803
Epoch: 21, Steps: 14 | Train Loss: 0.8193645 Vali Loss: 0.6721871 Test Loss: 0.4181385
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.3777151107788086
Epoch: 22, Steps: 14 | Train Loss: 0.8184837 Vali Loss: 0.6726960 Test Loss: 0.4178532
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.460838794708252
Epoch: 23, Steps: 14 | Train Loss: 0.8169114 Vali Loss: 0.6719126 Test Loss: 0.4175593
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.38535770773887634, mae:0.4288180470466614, rse:0.4961782991886139, corr:[ 2.16475278e-01  2.21732751e-01  2.19159260e-01  2.19933048e-01
  2.20702946e-01  2.19225779e-01  2.18331471e-01  2.18401358e-01
  2.17710838e-01  2.15947211e-01  2.14546174e-01  2.13462830e-01
  2.12116331e-01  2.10415989e-01  2.08777487e-01  2.07699791e-01
  2.06953630e-01  2.05945298e-01  2.04706356e-01  2.03626558e-01
  2.02755064e-01  2.01784208e-01  2.00365543e-01  1.98839977e-01
  1.97532624e-01  1.96500912e-01  1.95404142e-01  1.94255888e-01
  1.93604305e-01  1.93231955e-01  1.92613363e-01  1.91573918e-01
  1.90694302e-01  1.89959288e-01  1.88809216e-01  1.87462762e-01
  1.86494306e-01  1.85939595e-01  1.84898615e-01  1.83506936e-01
  1.82658434e-01  1.82319313e-01  1.81807756e-01  1.80903330e-01
  1.80013865e-01  1.79549664e-01  1.79166481e-01  1.77789032e-01
  1.75722390e-01  1.74042448e-01  1.73312351e-01  1.72846958e-01
  1.72184557e-01  1.71513304e-01  1.71150237e-01  1.70908481e-01
  1.69859067e-01  1.68509036e-01  1.67988494e-01  1.67938337e-01
  1.67254806e-01  1.66459218e-01  1.66792944e-01  1.67432457e-01
  1.66977540e-01  1.66712224e-01  1.67252392e-01  1.67550921e-01
  1.67160362e-01  1.66953102e-01  1.67036146e-01  1.66552469e-01
  1.65655777e-01  1.65301815e-01  1.65272042e-01  1.64651468e-01
  1.64339766e-01  1.64658979e-01  1.64588600e-01  1.63721502e-01
  1.63294405e-01  1.63384870e-01  1.63039282e-01  1.62458986e-01
  1.62455112e-01  1.62823901e-01  1.62652418e-01  1.62364990e-01
  1.62703842e-01  1.63119003e-01  1.62718445e-01  1.62145898e-01
  1.62595674e-01  1.63162664e-01  1.63197875e-01  1.63058490e-01
  1.63224235e-01  1.63299650e-01  1.63012251e-01  1.62786141e-01
  1.62737101e-01  1.62316859e-01  1.61956802e-01  1.61837950e-01
  1.61901101e-01  1.61611661e-01  1.61452502e-01  1.61609679e-01
  1.61381140e-01  1.60846248e-01  1.60657823e-01  1.60774425e-01
  1.60354242e-01  1.59765422e-01  1.59786999e-01  1.60067379e-01
  1.59673512e-01  1.58801749e-01  1.58702031e-01  1.58713058e-01
  1.57707751e-01  1.56150475e-01  1.55480117e-01  1.55282766e-01
  1.54497072e-01  1.53690264e-01  1.53619751e-01  1.53565645e-01
  1.52828515e-01  1.51826710e-01  1.51406035e-01  1.50974914e-01
  1.50174513e-01  1.49130225e-01  1.48334116e-01  1.47983804e-01
  1.47725746e-01  1.47485629e-01  1.46831274e-01  1.45908952e-01
  1.45762071e-01  1.46107301e-01  1.45947888e-01  1.44794315e-01
  1.43290833e-01  1.42276928e-01  1.41245767e-01  1.40185207e-01
  1.39893398e-01  1.40264809e-01  1.40036419e-01  1.38933823e-01
  1.38389930e-01  1.38426304e-01  1.37945101e-01  1.36709049e-01
  1.35673478e-01  1.35367423e-01  1.35201201e-01  1.34695813e-01
  1.33972421e-01  1.33933336e-01  1.34328142e-01  1.34498119e-01
  1.34619847e-01  1.34903044e-01  1.35295972e-01  1.34838954e-01
  1.33554012e-01  1.32419884e-01  1.32266924e-01  1.32359013e-01
  1.31752551e-01  1.30789429e-01  1.30447775e-01  1.30212203e-01
  1.29287094e-01  1.28040984e-01  1.27299786e-01  1.27131134e-01
  1.26702026e-01  1.25974208e-01  1.25680715e-01  1.25781938e-01
  1.25727519e-01  1.25734553e-01  1.25833735e-01  1.26132116e-01
  1.26591206e-01  1.26972929e-01  1.27287954e-01  1.27136901e-01
  1.26764983e-01  1.26770556e-01  1.26990706e-01  1.27345890e-01
  1.27610713e-01  1.27661824e-01  1.27864972e-01  1.27977267e-01
  1.28082067e-01  1.27977028e-01  1.27740785e-01  1.27165601e-01
  1.26654834e-01  1.26616612e-01  1.26720145e-01  1.26809418e-01
  1.26938939e-01  1.27419189e-01  1.27841860e-01  1.27798870e-01
  1.27725124e-01  1.28111333e-01  1.28588662e-01  1.28017113e-01
  1.26695201e-01  1.25828117e-01  1.25797957e-01  1.25880599e-01
  1.25376076e-01  1.25281632e-01  1.25359759e-01  1.25222132e-01
  1.24694765e-01  1.24703169e-01  1.25293240e-01  1.25422657e-01
  1.25092119e-01  1.24946922e-01  1.25285998e-01  1.25753641e-01
  1.26057714e-01  1.26276180e-01  1.26578674e-01  1.27243012e-01
  1.28322572e-01  1.29078969e-01  1.29236057e-01  1.29308790e-01
  1.29617244e-01  1.29605249e-01  1.29229233e-01  1.29397139e-01
  1.30168006e-01  1.30753681e-01  1.30618766e-01  1.30338654e-01
  1.30788013e-01  1.31609023e-01  1.32097095e-01  1.32315367e-01
  1.32645175e-01  1.33356452e-01  1.34005070e-01  1.34687588e-01
  1.35590911e-01  1.36867836e-01  1.37657255e-01  1.37763992e-01
  1.37982681e-01  1.38999626e-01  1.40362084e-01  1.40855104e-01
  1.40942976e-01  1.41240865e-01  1.41671985e-01  1.41800568e-01
  1.41686231e-01  1.42030135e-01  1.42480850e-01  1.42700553e-01
  1.42947018e-01  1.43465027e-01  1.44179165e-01  1.44586638e-01
  1.44604236e-01  1.44993946e-01  1.45878583e-01  1.46902665e-01
  1.47548929e-01  1.47867441e-01  1.48296311e-01  1.48967803e-01
  1.49467304e-01  1.49871081e-01  1.50552645e-01  1.51203826e-01
  1.51356071e-01  1.51096210e-01  1.50915548e-01  1.51168689e-01
  1.51595086e-01  1.51787341e-01  1.51622146e-01  1.51316449e-01
  1.51447833e-01  1.52010754e-01  1.52353272e-01  1.52106076e-01
  1.51928961e-01  1.52492806e-01  1.53080389e-01  1.53213277e-01
  1.53428108e-01  1.54273570e-01  1.54604569e-01  1.54479310e-01
  1.54643804e-01  1.55601129e-01  1.56582281e-01  1.56701192e-01
  1.56319588e-01  1.55826420e-01  1.55738086e-01  1.55572683e-01
  1.55260995e-01  1.54759675e-01  1.54388100e-01  1.54384121e-01
  1.54331714e-01  1.54426500e-01  1.54534504e-01  1.54618829e-01
  1.54176652e-01  1.53634757e-01  1.54091552e-01  1.55024305e-01
  1.55434340e-01  1.55150980e-01  1.55186385e-01  1.56037986e-01
  1.56698123e-01  1.57015413e-01  1.57872453e-01  1.58641800e-01
  1.58200324e-01  1.57315522e-01  1.57107562e-01  1.57514006e-01
  1.58004120e-01  1.58038437e-01  1.57863006e-01  1.57558441e-01
  1.57453880e-01  1.57696247e-01  1.57823011e-01  1.57797039e-01
  1.57533541e-01  1.57736272e-01  1.58399031e-01  1.59017026e-01
  1.59437999e-01  1.60228923e-01  1.61097750e-01  1.61673263e-01
  1.61917016e-01  1.62390918e-01  1.62943780e-01  1.63038924e-01
  1.62790090e-01  1.62634611e-01  1.62699774e-01  1.62617564e-01
  1.62584513e-01  1.62867174e-01  1.63548142e-01  1.64066985e-01
  1.63902849e-01  1.64004773e-01  1.64412394e-01  1.64436206e-01
  1.63703293e-01  1.63424253e-01  1.64273709e-01  1.65068239e-01
  1.64814666e-01  1.64224565e-01  1.64487436e-01  1.65030271e-01
  1.64807379e-01  1.64363995e-01  1.64743215e-01  1.65449083e-01
  1.65249676e-01  1.64445162e-01  1.64000481e-01  1.64006203e-01
  1.64242283e-01  1.64596990e-01  1.64977714e-01  1.65251613e-01
  1.65560037e-01  1.66052297e-01  1.66357458e-01  1.66338176e-01
  1.66330412e-01  1.65845394e-01  1.65344477e-01  1.65332958e-01
  1.65373474e-01  1.64919451e-01  1.64322138e-01  1.64577559e-01
  1.65327027e-01  1.65635467e-01  1.65350705e-01  1.65163144e-01
  1.65100187e-01  1.64611086e-01  1.63768053e-01  1.63259536e-01
  1.63380921e-01  1.63502067e-01  1.63302973e-01  1.62957087e-01
  1.62544534e-01  1.62308633e-01  1.62379473e-01  1.62281051e-01
  1.61216214e-01  1.60207808e-01  1.59986287e-01  1.59735665e-01
  1.58541426e-01  1.57197401e-01  1.56576559e-01  1.55849308e-01
  1.54860198e-01  1.54223040e-01  1.53804839e-01  1.53096840e-01
  1.51997268e-01  1.51319832e-01  1.51150972e-01  1.50564671e-01
  1.49759129e-01  1.49096206e-01  1.48369446e-01  1.47695392e-01
  1.47067711e-01  1.46766528e-01  1.46035329e-01  1.45202458e-01
  1.44331828e-01  1.43630192e-01  1.43045008e-01  1.43144906e-01
  1.43240511e-01  1.42392009e-01  1.41228244e-01  1.41179278e-01
  1.41678914e-01  1.41641781e-01  1.41278401e-01  1.41261339e-01
  1.41354606e-01  1.40757158e-01  1.40041247e-01  1.40047893e-01
  1.40143901e-01  1.39785439e-01  1.39613733e-01  1.39820114e-01
  1.39467075e-01  1.38896421e-01  1.38371810e-01  1.37779787e-01
  1.37433380e-01  1.37264177e-01  1.37176991e-01  1.36650518e-01
  1.36019975e-01  1.35880664e-01  1.36054784e-01  1.35861307e-01
  1.35520652e-01  1.35900334e-01  1.36264190e-01  1.35714889e-01
  1.34643003e-01  1.34144768e-01  1.33858308e-01  1.32846832e-01
  1.31745040e-01  1.31098330e-01  1.30751386e-01  1.30000174e-01
  1.28833666e-01  1.27580285e-01  1.27112716e-01  1.27157301e-01
  1.26757309e-01  1.25610933e-01  1.24610119e-01  1.24332443e-01
  1.23946197e-01  1.23218454e-01  1.22679204e-01  1.22692108e-01
  1.22714363e-01  1.22114509e-01  1.21344753e-01  1.20624594e-01
  1.19702823e-01  1.18316375e-01  1.16777115e-01  1.15817249e-01
  1.15667351e-01  1.15609951e-01  1.14701852e-01  1.13491312e-01
  1.12797849e-01  1.12553447e-01  1.12101726e-01  1.11117959e-01
  1.10201441e-01  1.09623820e-01  1.09063953e-01  1.08174153e-01
  1.07640944e-01  1.07817397e-01  1.07554987e-01  1.06450409e-01
  1.05350949e-01  1.05040841e-01  1.04735121e-01  1.03577480e-01
  1.01848990e-01  1.00614242e-01  9.99532863e-02  9.87934396e-02
  9.73882452e-02  9.62440372e-02  9.56472531e-02  9.48797762e-02
  9.34750438e-02  9.20315087e-02  9.12319645e-02  9.06692520e-02
  8.93234313e-02  8.75553936e-02  8.65887702e-02  8.61806497e-02
  8.51916075e-02  8.39563608e-02  8.31007212e-02  8.29575583e-02
  8.26047584e-02  8.13540742e-02  8.03025812e-02  7.94349685e-02
  7.82348365e-02  7.65752867e-02  7.50637129e-02  7.42728710e-02
  7.40566999e-02  7.36647770e-02  7.25218132e-02  7.10988715e-02
  7.04404041e-02  7.01522902e-02  6.97146952e-02  6.89835325e-02
  6.86187521e-02  6.85404688e-02  6.79390058e-02  6.64817467e-02
  6.55811802e-02  6.53405637e-02  6.46966845e-02  6.34755343e-02
  6.24813065e-02  6.21003024e-02  6.12183288e-02  5.95094711e-02
  5.76558150e-02  5.66224009e-02  5.57667427e-02  5.44344597e-02
  5.32330722e-02  5.26575744e-02  5.23974113e-02  5.15366346e-02
  5.01238331e-02  4.92774472e-02  4.91694212e-02  4.90846336e-02
  4.80872430e-02  4.69369590e-02  4.65187095e-02  4.63058017e-02
  4.57321107e-02  4.50604185e-02  4.48103845e-02  4.49100323e-02
  4.44280915e-02  4.33985889e-02  4.25033234e-02  4.18416187e-02
  4.04729731e-02  3.83522920e-02  3.65442447e-02  3.61155570e-02
  3.55988964e-02  3.42735834e-02  3.25819068e-02  3.16100903e-02
  3.10747139e-02  3.02530788e-02  2.89973877e-02  2.78528463e-02
  2.76042335e-02  2.76045296e-02  2.70643085e-02  2.63480507e-02
  2.62935590e-02  2.69547980e-02  2.68758237e-02  2.62862649e-02
  2.60685124e-02  2.64494363e-02  2.59585008e-02  2.47887373e-02
  2.36328263e-02  2.29444262e-02  2.23903582e-02  2.13723145e-02
  2.03818437e-02  2.03454625e-02  2.06817351e-02  2.03378722e-02
  1.95578374e-02  1.93559770e-02  1.96128450e-02  1.97321214e-02
  1.90162826e-02  1.80892460e-02  1.79835372e-02  1.79564357e-02
  1.70165952e-02  1.64769478e-02  1.65905468e-02  1.58699527e-02
  1.45225041e-02  1.44209592e-02  1.51383365e-02  1.49304271e-02
  1.29379518e-02  1.12669552e-02  1.13273393e-02  1.14775337e-02
  1.06434226e-02  9.33420565e-03  8.93357676e-03  9.32432525e-03
  9.27298982e-03  8.85867234e-03  8.87691975e-03  9.37845465e-03
  9.21332743e-03  8.46828148e-03  8.24468024e-03  8.69857427e-03
  9.38333385e-03  9.12288763e-03  8.06386676e-03  7.51894619e-03
  7.91322161e-03  8.45391676e-03  7.81656243e-03  6.81764074e-03
  6.49911026e-03  6.59176148e-03  5.93744265e-03  4.59179701e-03
  4.73277783e-03  5.24931261e-03  4.88880137e-03  3.30134807e-03
  3.61004146e-03  5.60741965e-03  6.42408058e-03  5.24750026e-03
  4.30592103e-03  4.19493252e-03  4.42631310e-03  3.51906358e-03
  2.53098342e-03  3.31588462e-03  4.59035859e-03  3.22710839e-03
  1.08082639e-03  7.78533344e-04  1.57244818e-03 -1.83699463e-04
 -3.82729108e-03 -5.31610195e-03 -3.70483380e-03 -3.72763467e-03
 -6.07213983e-03 -6.84706541e-03 -3.89868044e-03 -2.33357563e-03
 -4.04540217e-03 -4.78905253e-03 -2.53064302e-03 -7.74094660e-04
 -3.41742369e-03 -6.97592832e-03 -5.20919776e-03 -3.02340346e-03
 -6.77003525e-03 -1.13177849e-02 -9.58211534e-03 -7.98609015e-03
 -1.62596218e-02 -2.45450642e-02 -1.57911163e-02 -4.92306286e-03]
