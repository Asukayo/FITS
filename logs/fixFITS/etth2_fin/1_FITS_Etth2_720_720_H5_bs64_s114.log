Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  48787200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.1876802444458
Epoch: 1, Steps: 56 | Train Loss: 1.0551540 Vali Loss: 0.7918763 Test Loss: 0.4415547
Validation loss decreased (inf --> 0.791876).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.320133686065674
Epoch: 2, Steps: 56 | Train Loss: 0.9054587 Vali Loss: 0.7455804 Test Loss: 0.4133241
Validation loss decreased (0.791876 --> 0.745580).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 8.357176303863525
Epoch: 3, Steps: 56 | Train Loss: 0.8686557 Vali Loss: 0.7231034 Test Loss: 0.4031166
Validation loss decreased (0.745580 --> 0.723103).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.368146419525146
Epoch: 4, Steps: 56 | Train Loss: 0.8534732 Vali Loss: 0.7054152 Test Loss: 0.3972613
Validation loss decreased (0.723103 --> 0.705415).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.397011995315552
Epoch: 5, Steps: 56 | Train Loss: 0.8419383 Vali Loss: 0.6950375 Test Loss: 0.3934010
Validation loss decreased (0.705415 --> 0.695037).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.454802751541138
Epoch: 6, Steps: 56 | Train Loss: 0.8358857 Vali Loss: 0.6843969 Test Loss: 0.3904257
Validation loss decreased (0.695037 --> 0.684397).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.022258996963501
Epoch: 7, Steps: 56 | Train Loss: 0.8290450 Vali Loss: 0.6796778 Test Loss: 0.3882840
Validation loss decreased (0.684397 --> 0.679678).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.34615707397461
Epoch: 8, Steps: 56 | Train Loss: 0.8259484 Vali Loss: 0.6747906 Test Loss: 0.3866242
Validation loss decreased (0.679678 --> 0.674791).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.65443754196167
Epoch: 9, Steps: 56 | Train Loss: 0.8211648 Vali Loss: 0.6754885 Test Loss: 0.3852408
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.373482465744019
Epoch: 10, Steps: 56 | Train Loss: 0.8209770 Vali Loss: 0.6666740 Test Loss: 0.3842058
Validation loss decreased (0.674791 --> 0.666674).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.28163194656372
Epoch: 11, Steps: 56 | Train Loss: 0.8164843 Vali Loss: 0.6692547 Test Loss: 0.3833493
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.10668134689331
Epoch: 12, Steps: 56 | Train Loss: 0.8166548 Vali Loss: 0.6650660 Test Loss: 0.3826995
Validation loss decreased (0.666674 --> 0.665066).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.005220413208008
Epoch: 13, Steps: 56 | Train Loss: 0.8146820 Vali Loss: 0.6650295 Test Loss: 0.3821484
Validation loss decreased (0.665066 --> 0.665030).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.778146505355835
Epoch: 14, Steps: 56 | Train Loss: 0.8131191 Vali Loss: 0.6624900 Test Loss: 0.3816789
Validation loss decreased (0.665030 --> 0.662490).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.114241123199463
Epoch: 15, Steps: 56 | Train Loss: 0.8109816 Vali Loss: 0.6634430 Test Loss: 0.3813271
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.005259275436401
Epoch: 16, Steps: 56 | Train Loss: 0.8096477 Vali Loss: 0.6625537 Test Loss: 0.3810438
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 7.91486120223999
Epoch: 17, Steps: 56 | Train Loss: 0.8083892 Vali Loss: 0.6574575 Test Loss: 0.3808063
Validation loss decreased (0.662490 --> 0.657457).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.032700061798096
Epoch: 18, Steps: 56 | Train Loss: 0.8100369 Vali Loss: 0.6602112 Test Loss: 0.3806401
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 7.5680131912231445
Epoch: 19, Steps: 56 | Train Loss: 0.8081589 Vali Loss: 0.6551601 Test Loss: 0.3804457
Validation loss decreased (0.657457 --> 0.655160).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 6.337185621261597
Epoch: 20, Steps: 56 | Train Loss: 0.8073493 Vali Loss: 0.6562375 Test Loss: 0.3803287
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.145480632781982
Epoch: 21, Steps: 56 | Train Loss: 0.8066845 Vali Loss: 0.6613529 Test Loss: 0.3801789
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 7.894647121429443
Epoch: 22, Steps: 56 | Train Loss: 0.8065946 Vali Loss: 0.6548566 Test Loss: 0.3800381
Validation loss decreased (0.655160 --> 0.654857).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.037567138671875
Epoch: 23, Steps: 56 | Train Loss: 0.8063705 Vali Loss: 0.6578085 Test Loss: 0.3799955
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.078072547912598
Epoch: 24, Steps: 56 | Train Loss: 0.8067520 Vali Loss: 0.6550415 Test Loss: 0.3799407
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.329960823059082
Epoch: 25, Steps: 56 | Train Loss: 0.8056459 Vali Loss: 0.6534903 Test Loss: 0.3798576
Validation loss decreased (0.654857 --> 0.653490).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 8.24040174484253
Epoch: 26, Steps: 56 | Train Loss: 0.8066669 Vali Loss: 0.6521914 Test Loss: 0.3798858
Validation loss decreased (0.653490 --> 0.652191).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 9.062493324279785
Epoch: 27, Steps: 56 | Train Loss: 0.8054986 Vali Loss: 0.6557181 Test Loss: 0.3798001
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.186940908432007
Epoch: 28, Steps: 56 | Train Loss: 0.8054406 Vali Loss: 0.6533259 Test Loss: 0.3798076
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.742233037948608
Epoch: 29, Steps: 56 | Train Loss: 0.8051028 Vali Loss: 0.6522822 Test Loss: 0.3797559
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.37849971652030945, mae:0.4238266348838806, rse:0.4917433559894562, corr:[ 0.21500103  0.22038378  0.21802448  0.21706052  0.21809967  0.21816331
  0.21647625  0.21526223  0.21465555  0.21374227  0.21210377  0.2105746
  0.20961767  0.208767    0.20768778  0.20667583  0.20598023  0.20520696
  0.20416798  0.20320234  0.2025295   0.20186001  0.20056431  0.19911797
  0.19789086  0.19708112  0.19611865  0.19504817  0.19422245  0.19372907
  0.19335586  0.19267638  0.19181009  0.19077025  0.18983549  0.18906276
  0.18828675  0.18744329  0.18652293  0.18588327  0.18548624  0.18504244
  0.18439676  0.18373053  0.18300909  0.18238588  0.1814847   0.18007307
  0.17858654  0.17731184  0.17634577  0.17580612  0.17544082  0.17466731
  0.17358121  0.17285076  0.17259161  0.17225708  0.17151773  0.17076959
  0.17047003  0.17052743  0.17062563  0.1705183   0.17032455  0.1704723
  0.17073543  0.17090274  0.17092554  0.17090732  0.17072098  0.1703129
  0.16979137  0.16926214  0.16872916  0.16818996  0.1679913   0.16817746
  0.16833381  0.16824007  0.1678449   0.16743676  0.1673665   0.16758145
  0.1676443   0.16735071  0.16710478  0.16728252  0.16768098  0.16781864
  0.16765112  0.16769683  0.16812497  0.16847658  0.16848332  0.16823944
  0.16813847  0.16815777  0.16798511  0.16766201  0.1674672   0.16731861
  0.16716775  0.16691577  0.16674     0.16666639  0.16671789  0.1666929
  0.1665301   0.16632614  0.16615176  0.16587771  0.16562015  0.16560717
  0.16571832  0.16569993  0.16528757  0.16462354  0.16407995  0.1636004
  0.1629476   0.1622253   0.16142008  0.16065611  0.16006343  0.15985602
  0.15965857  0.15926014  0.15876786  0.15826856  0.15795678  0.15756205
  0.15693252  0.1561064   0.15546256  0.15508917  0.15462214  0.15404968
  0.1533507   0.1529168   0.15286416  0.152926    0.15267204  0.15185621
  0.15059194  0.14948027  0.14872798  0.14821982  0.14785218  0.14749135
  0.14706497  0.14672217  0.14653425  0.14620455  0.14561552  0.14486998
  0.14430727  0.14401075  0.14377409  0.14338426  0.14295864  0.14295349
  0.14326577  0.14357029  0.14376524  0.14379938  0.14367257  0.14319617
  0.14258008  0.14201054  0.14170994  0.14163055  0.14162421  0.14139493
  0.1406529   0.13959427  0.13866918  0.1381153   0.13767919  0.13728075
  0.1368641   0.1365586   0.13656172  0.13671152  0.13696261  0.13718358
  0.13720201  0.13725148  0.13753939  0.13810742  0.13873175  0.13904111
  0.13908128  0.1391413   0.13922554  0.13929209  0.1393798   0.13949858
  0.13978924  0.1399247   0.13972837  0.13928269  0.13898908  0.13884754
  0.13890845  0.13898122  0.13881668  0.13863383  0.13887154  0.13957469
  0.14024019  0.14053123  0.14049578  0.14052913  0.14078239  0.14090654
  0.14055859  0.13978882  0.13911174  0.13889487  0.13893142  0.1390048
  0.13890627  0.1388544   0.13879903  0.13880162  0.13890955  0.13894464
  0.1388558   0.13875014  0.1390686   0.1398239   0.14044651  0.14061093
  0.14070657  0.14129828  0.14223555  0.14283566  0.14290833  0.14300016
  0.14359996  0.14425023  0.1444228   0.14418672  0.14406677  0.14435038
  0.14475524  0.14477643  0.14475699  0.14510234  0.14577158  0.1463358
  0.14675     0.14733693  0.14809163  0.14877982  0.14922065  0.14987834
  0.15089417  0.15176906  0.15203519  0.15226647  0.15310737  0.1542966
  0.15518188  0.1554517   0.15557171  0.15586455  0.15627295  0.15664014
  0.15680219  0.15664317  0.15631169  0.15618883  0.15661997  0.15747987
  0.15824527  0.15889008  0.15943556  0.16007549  0.16074543  0.16123934
  0.16155359  0.1619412   0.16232152  0.16265863  0.16303866  0.16346508
  0.1638957   0.16405681  0.16395132  0.16388315  0.16384302  0.16377988
  0.16370784  0.16378088  0.16405217  0.16443726  0.16465305  0.16474086
  0.16493101  0.16552843  0.16611809  0.16628656  0.16631693  0.16675867
  0.16731088  0.16760652  0.16751537  0.16748108  0.16778539  0.16799508
  0.16795017  0.16780569  0.16786     0.16774893  0.16722089  0.16664934
  0.16656774  0.16687594  0.1668503   0.16640636  0.16598842  0.16605963
  0.16628285  0.16632213  0.16659205  0.1672497   0.16815072  0.16862585
  0.16845995  0.16844623  0.16894047  0.16967154  0.17003447  0.16977695
  0.16941158  0.16943137  0.16975376  0.1699892   0.17029294  0.17051144
  0.17048976  0.17019102  0.1700057   0.17024381  0.17042919  0.17050846
  0.17048942  0.17079933  0.17147657  0.17207569  0.17251185  0.17321667
  0.17412855  0.1747983   0.17477825  0.1745726   0.17481145  0.17548998
  0.17597307  0.17591825  0.17566824  0.17574331  0.17621219  0.17672704
  0.17713194  0.17732131  0.17715403  0.17703214  0.1769739   0.17712213
  0.17724599  0.17723848  0.17719407  0.17730992  0.17746268  0.17728117
  0.17690213  0.17678745  0.17698278  0.17699297  0.17670026  0.17651376
  0.17667134  0.17696926  0.17714162  0.17716806  0.1772953   0.17770109
  0.1782526   0.17880446  0.17929582  0.17968576  0.17986706  0.17973103
  0.17959353  0.17937335  0.17941038  0.17942673  0.17905445  0.17865054
  0.17858969  0.1786964   0.1785202   0.17812753  0.17776975  0.1776848
  0.17756373  0.17750989  0.17748745  0.17744043  0.17725942  0.1768598
  0.17655498  0.17665564  0.17693312  0.17694229  0.17665577  0.17631152
  0.17584196  0.17527287  0.17457522  0.17379178  0.17294332  0.17200494
  0.1710977   0.17020157  0.16951351  0.16900907  0.1684178   0.16785471
  0.16732542  0.16691925  0.1666152   0.16596672  0.16513892  0.16450366
  0.1641597   0.16397572  0.16351648  0.16293406  0.16219804  0.16152307
  0.1609365   0.16082415  0.16088237  0.16079406  0.16021843  0.15954973
  0.15927337  0.15943548  0.159279    0.15889987  0.15878889  0.15903658
  0.15913828  0.15853877  0.15765378  0.15742187  0.15771708  0.15780973
  0.15742193  0.15696707  0.15666029  0.15674876  0.15678853  0.15642104
  0.1559684   0.15544842  0.15498433  0.15450399  0.15418482  0.15402009
  0.15394284  0.15369347  0.15340054  0.15338948  0.15328851  0.1527111
  0.15182845  0.15120974  0.15088299  0.15017985  0.1492502   0.14855987
  0.14826515  0.14773674  0.14672339  0.14556098  0.14497371  0.1447422
  0.14434654  0.1437343   0.14324379  0.14282523  0.14205374  0.1412227
  0.14069635  0.14053528  0.14047205  0.1401065   0.1395511   0.13891633
  0.13816752  0.1371685   0.13591997  0.13486812  0.13440073  0.1341771
  0.13341156  0.13253978  0.13218674  0.132133    0.1315867   0.13015197
  0.12868665  0.12793373  0.12782209  0.127666    0.12748902  0.12747462
  0.12708989  0.1261743   0.12518878  0.12478081  0.12451185  0.12356035
  0.12177478  0.12020494  0.11950338  0.1188108   0.11766595  0.11627867
  0.11527666  0.11456323  0.11360208  0.11234843  0.11128478  0.11052115
  0.10961927  0.10835368  0.10733275  0.10686804  0.10647994  0.10578163
  0.10456324  0.10361982  0.10328811  0.10296689  0.10233222  0.10106684
  0.09953824  0.09792617  0.09665681  0.0959317   0.09548741  0.09466963
  0.09327471  0.09179399  0.09139167  0.09151323  0.09137408  0.0904972
  0.08960486  0.08933507  0.08939553  0.08894972  0.08816924  0.08737666
  0.08674166  0.08627242  0.08580504  0.08522021  0.08413863  0.08249192
  0.08064616  0.07943735  0.07861874  0.07762574  0.07635093  0.07525819
  0.07476687  0.07447983  0.07370358  0.07245667  0.07128832  0.07062687
  0.07000419  0.06915528  0.06830028  0.06780842  0.06782752  0.06781162
  0.06725638  0.06659263  0.06604979  0.06562795  0.06495254  0.06406794
  0.06292954  0.06157962  0.05995105  0.05880478  0.05795556  0.05698795
  0.05533778  0.05357025  0.0524721   0.05211028  0.05166914  0.05036221
  0.04900753  0.04852787  0.04882323  0.0490245   0.04871593  0.04861148
  0.04875712  0.04894853  0.04879143  0.04857205  0.04827258  0.04811443
  0.04761124  0.04656299  0.04521799  0.04386304  0.0427593   0.04232095
  0.0422471   0.04194617  0.04125812  0.04071292  0.04057735  0.04080809
  0.04047686  0.03926131  0.03847354  0.0389546   0.03957741  0.03947455
  0.03829765  0.03677545  0.03634077  0.03711509  0.03747457  0.0366506
  0.03493882  0.03337773  0.03257957  0.03220272  0.03201539  0.03127494
  0.0300001   0.02897059  0.02870649  0.02896313  0.02898301  0.0283535
  0.0273431   0.02697187  0.02764099  0.02829287  0.02835323  0.02808085
  0.02818531  0.02872066  0.02881728  0.02859537  0.02835224  0.02822497
  0.02778515  0.02689219  0.02571908  0.02437882  0.02382105  0.02360394
  0.02360583  0.0227663   0.02203106  0.02205413  0.02266645  0.02304264
  0.02265603  0.02124619  0.02059051  0.02122338  0.02221439  0.02249316
  0.02195148  0.02098428  0.02088494  0.02108943  0.0205814   0.01872194
  0.01608559  0.0135184   0.01220073  0.0116841   0.01109426  0.00917217
  0.00714615  0.00652843  0.00771585  0.00838379  0.00637441  0.00371387
  0.00336233  0.00466719  0.005185    0.00295755  0.00103891  0.00275552
  0.00484579  0.00117832 -0.00709691 -0.00955298 -0.00257336 -0.00281469]
