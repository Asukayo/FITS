Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  77973504.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.110992670059204
Epoch: 1, Steps: 30 | Train Loss: 0.5909173 Vali Loss: 0.4422067 Test Loss: 0.4184958
Validation loss decreased (inf --> 0.442207).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.520956754684448
Epoch: 2, Steps: 30 | Train Loss: 0.4976676 Vali Loss: 0.3849164 Test Loss: 0.3804132
Validation loss decreased (0.442207 --> 0.384916).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.774097681045532
Epoch: 3, Steps: 30 | Train Loss: 0.4405835 Vali Loss: 0.3558957 Test Loss: 0.3598070
Validation loss decreased (0.384916 --> 0.355896).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.3732898235321045
Epoch: 4, Steps: 30 | Train Loss: 0.4013305 Vali Loss: 0.3412378 Test Loss: 0.3487197
Validation loss decreased (0.355896 --> 0.341238).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.311032056808472
Epoch: 5, Steps: 30 | Train Loss: 0.3743409 Vali Loss: 0.3298156 Test Loss: 0.3429836
Validation loss decreased (0.341238 --> 0.329816).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.772509574890137
Epoch: 6, Steps: 30 | Train Loss: 0.3540611 Vali Loss: 0.3238701 Test Loss: 0.3396181
Validation loss decreased (0.329816 --> 0.323870).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.840974569320679
Epoch: 7, Steps: 30 | Train Loss: 0.3375842 Vali Loss: 0.3235274 Test Loss: 0.3377778
Validation loss decreased (0.323870 --> 0.323527).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.862616777420044
Epoch: 8, Steps: 30 | Train Loss: 0.3243845 Vali Loss: 0.3202193 Test Loss: 0.3363963
Validation loss decreased (0.323527 --> 0.320219).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 6.314681529998779
Epoch: 9, Steps: 30 | Train Loss: 0.3125931 Vali Loss: 0.3190924 Test Loss: 0.3356353
Validation loss decreased (0.320219 --> 0.319092).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 6.200824022293091
Epoch: 10, Steps: 30 | Train Loss: 0.3016355 Vali Loss: 0.3186725 Test Loss: 0.3351401
Validation loss decreased (0.319092 --> 0.318672).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.697929620742798
Epoch: 11, Steps: 30 | Train Loss: 0.2934705 Vali Loss: 0.3181272 Test Loss: 0.3346911
Validation loss decreased (0.318672 --> 0.318127).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.094872951507568
Epoch: 12, Steps: 30 | Train Loss: 0.2853664 Vali Loss: 0.3169639 Test Loss: 0.3342782
Validation loss decreased (0.318127 --> 0.316964).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.936842441558838
Epoch: 13, Steps: 30 | Train Loss: 0.2784594 Vali Loss: 0.3164213 Test Loss: 0.3337063
Validation loss decreased (0.316964 --> 0.316421).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.5650129318237305
Epoch: 14, Steps: 30 | Train Loss: 0.2726551 Vali Loss: 0.3157136 Test Loss: 0.3332393
Validation loss decreased (0.316421 --> 0.315714).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.692368030548096
Epoch: 15, Steps: 30 | Train Loss: 0.2667461 Vali Loss: 0.3117211 Test Loss: 0.3326648
Validation loss decreased (0.315714 --> 0.311721).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.268949747085571
Epoch: 16, Steps: 30 | Train Loss: 0.2613619 Vali Loss: 0.3131254 Test Loss: 0.3322539
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.267355918884277
Epoch: 17, Steps: 30 | Train Loss: 0.2559397 Vali Loss: 0.3113123 Test Loss: 0.3317959
Validation loss decreased (0.311721 --> 0.311312).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.424493789672852
Epoch: 18, Steps: 30 | Train Loss: 0.2520537 Vali Loss: 0.3123058 Test Loss: 0.3311959
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.844527244567871
Epoch: 19, Steps: 30 | Train Loss: 0.2476611 Vali Loss: 0.3109511 Test Loss: 0.3306416
Validation loss decreased (0.311312 --> 0.310951).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.173267841339111
Epoch: 20, Steps: 30 | Train Loss: 0.2438945 Vali Loss: 0.3107300 Test Loss: 0.3302068
Validation loss decreased (0.310951 --> 0.310730).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.167373895645142
Epoch: 21, Steps: 30 | Train Loss: 0.2400307 Vali Loss: 0.3082235 Test Loss: 0.3297729
Validation loss decreased (0.310730 --> 0.308223).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.126388788223267
Epoch: 22, Steps: 30 | Train Loss: 0.2363469 Vali Loss: 0.3080724 Test Loss: 0.3292918
Validation loss decreased (0.308223 --> 0.308072).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.63535737991333
Epoch: 23, Steps: 30 | Train Loss: 0.2342230 Vali Loss: 0.3074901 Test Loss: 0.3287417
Validation loss decreased (0.308072 --> 0.307490).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.8559205532073975
Epoch: 24, Steps: 30 | Train Loss: 0.2299577 Vali Loss: 0.3063038 Test Loss: 0.3282734
Validation loss decreased (0.307490 --> 0.306304).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.6828577518463135
Epoch: 25, Steps: 30 | Train Loss: 0.2275961 Vali Loss: 0.3064721 Test Loss: 0.3278730
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.3861775398254395
Epoch: 26, Steps: 30 | Train Loss: 0.2243085 Vali Loss: 0.3059235 Test Loss: 0.3274475
Validation loss decreased (0.306304 --> 0.305924).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.204143762588501
Epoch: 27, Steps: 30 | Train Loss: 0.2224893 Vali Loss: 0.3063853 Test Loss: 0.3269156
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 5.075995206832886
Epoch: 28, Steps: 30 | Train Loss: 0.2199009 Vali Loss: 0.3039317 Test Loss: 0.3264915
Validation loss decreased (0.305924 --> 0.303932).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 5.292407035827637
Epoch: 29, Steps: 30 | Train Loss: 0.2180731 Vali Loss: 0.3027228 Test Loss: 0.3261610
Validation loss decreased (0.303932 --> 0.302723).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 5.4324259757995605
Epoch: 30, Steps: 30 | Train Loss: 0.2165309 Vali Loss: 0.3035611 Test Loss: 0.3257059
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 5.373983144760132
Epoch: 31, Steps: 30 | Train Loss: 0.2143382 Vali Loss: 0.3038105 Test Loss: 0.3253675
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 5.228723764419556
Epoch: 32, Steps: 30 | Train Loss: 0.2117567 Vali Loss: 0.3035651 Test Loss: 0.3250991
EarlyStopping counter: 3 out of 3
Early stopping
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  77973504.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.527193069458008
Epoch: 1, Steps: 30 | Train Loss: 0.4910126 Vali Loss: 0.2654096 Test Loss: 0.2957253
Validation loss decreased (inf --> 0.265410).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.045891761779785
Epoch: 2, Steps: 30 | Train Loss: 0.4517738 Vali Loss: 0.2462631 Test Loss: 0.2820191
Validation loss decreased (0.265410 --> 0.246263).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.293704032897949
Epoch: 3, Steps: 30 | Train Loss: 0.4326632 Vali Loss: 0.2389098 Test Loss: 0.2765577
Validation loss decreased (0.246263 --> 0.238910).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.2589638233184814
Epoch: 4, Steps: 30 | Train Loss: 0.4237733 Vali Loss: 0.2314846 Test Loss: 0.2742839
Validation loss decreased (0.238910 --> 0.231485).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.479825973510742
Epoch: 5, Steps: 30 | Train Loss: 0.4179797 Vali Loss: 0.2284059 Test Loss: 0.2734653
Validation loss decreased (0.231485 --> 0.228406).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.2061097621917725
Epoch: 6, Steps: 30 | Train Loss: 0.4133666 Vali Loss: 0.2261653 Test Loss: 0.2731559
Validation loss decreased (0.228406 --> 0.226165).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.08064079284668
Epoch: 7, Steps: 30 | Train Loss: 0.4114752 Vali Loss: 0.2243269 Test Loss: 0.2728896
Validation loss decreased (0.226165 --> 0.224327).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.7115466594696045
Epoch: 8, Steps: 30 | Train Loss: 0.4095365 Vali Loss: 0.2235695 Test Loss: 0.2729545
Validation loss decreased (0.224327 --> 0.223570).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.456247806549072
Epoch: 9, Steps: 30 | Train Loss: 0.4073138 Vali Loss: 0.2213182 Test Loss: 0.2729451
Validation loss decreased (0.223570 --> 0.221318).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.545025825500488
Epoch: 10, Steps: 30 | Train Loss: 0.4086112 Vali Loss: 0.2197295 Test Loss: 0.2730557
Validation loss decreased (0.221318 --> 0.219729).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.3955652713775635
Epoch: 11, Steps: 30 | Train Loss: 0.4047401 Vali Loss: 0.2204485 Test Loss: 0.2729341
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.524531126022339
Epoch: 12, Steps: 30 | Train Loss: 0.4059878 Vali Loss: 0.2194478 Test Loss: 0.2728968
Validation loss decreased (0.219729 --> 0.219448).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.68994927406311
Epoch: 13, Steps: 30 | Train Loss: 0.4054238 Vali Loss: 0.2184298 Test Loss: 0.2730216
Validation loss decreased (0.219448 --> 0.218430).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.729884147644043
Epoch: 14, Steps: 30 | Train Loss: 0.4055339 Vali Loss: 0.2196270 Test Loss: 0.2729002
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.703211784362793
Epoch: 15, Steps: 30 | Train Loss: 0.4055391 Vali Loss: 0.2183869 Test Loss: 0.2729017
Validation loss decreased (0.218430 --> 0.218387).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.415409326553345
Epoch: 16, Steps: 30 | Train Loss: 0.4031399 Vali Loss: 0.2162301 Test Loss: 0.2729920
Validation loss decreased (0.218387 --> 0.216230).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.28038215637207
Epoch: 17, Steps: 30 | Train Loss: 0.4039926 Vali Loss: 0.2179497 Test Loss: 0.2730187
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.666776895523071
Epoch: 18, Steps: 30 | Train Loss: 0.4017464 Vali Loss: 0.2172286 Test Loss: 0.2729230
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.465723276138306
Epoch: 19, Steps: 30 | Train Loss: 0.4038747 Vali Loss: 0.2175393 Test Loss: 0.2729132
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.2708550989627838, mae:0.33732664585113525, rse:0.41942062973976135, corr:[0.27386814 0.27691466 0.2756848  0.27599654 0.27548143 0.2743167
 0.27382666 0.27324292 0.27228227 0.27108738 0.26983902 0.26851317
 0.26714504 0.26581088 0.264668   0.2641525  0.26375932 0.26325613
 0.26260987 0.26157197 0.26029065 0.25920057 0.25827837 0.25671196
 0.2545975  0.25267196 0.25109628 0.24961537 0.24847637 0.24760751
 0.24644561 0.24489821 0.24325883 0.24194597 0.2406694  0.23959926
 0.23885328 0.23793897 0.23662601 0.23526706 0.2344471  0.23390853
 0.23350453 0.23303583 0.23223047 0.23108006 0.23005345 0.22865023
 0.22656612 0.2246387  0.2234677  0.22199903 0.22075994 0.21964967
 0.21794237 0.21632542 0.21487223 0.21315281 0.21113779 0.2094699
 0.20882167 0.20843853 0.20805667 0.20825632 0.2085434  0.2086448
 0.20762783 0.20615782 0.20568624 0.2055911  0.20496991 0.2041699
 0.20295161 0.20177287 0.20153086 0.20049694 0.19860828 0.19731598
 0.19801755 0.1976828  0.19653806 0.1962869  0.19672662 0.19603288
 0.19448754 0.19518283 0.19604445 0.19480878 0.1930586  0.19406682
 0.19346258 0.19050792 0.19106862 0.190986   0.18788148 0.19099024]
