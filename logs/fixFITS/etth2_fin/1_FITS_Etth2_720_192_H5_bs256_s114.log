Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  123594240.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.209423065185547
Epoch: 1, Steps: 15 | Train Loss: 0.8545188 Vali Loss: 0.5119206 Test Loss: 0.5143203
Validation loss decreased (inf --> 0.511921).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.2896502017974854
Epoch: 2, Steps: 15 | Train Loss: 0.7290535 Vali Loss: 0.4441925 Test Loss: 0.4636506
Validation loss decreased (0.511921 --> 0.444192).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.8412883281707764
Epoch: 3, Steps: 15 | Train Loss: 0.6629464 Vali Loss: 0.4091147 Test Loss: 0.4373848
Validation loss decreased (0.444192 --> 0.409115).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.921478748321533
Epoch: 4, Steps: 15 | Train Loss: 0.6265855 Vali Loss: 0.3863958 Test Loss: 0.4232272
Validation loss decreased (0.409115 --> 0.386396).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.6667585372924805
Epoch: 5, Steps: 15 | Train Loss: 0.6052571 Vali Loss: 0.3709332 Test Loss: 0.4146871
Validation loss decreased (0.386396 --> 0.370933).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.3395707607269287
Epoch: 6, Steps: 15 | Train Loss: 0.5907341 Vali Loss: 0.3599456 Test Loss: 0.4090990
Validation loss decreased (0.370933 --> 0.359946).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1213912963867188
Epoch: 7, Steps: 15 | Train Loss: 0.5833307 Vali Loss: 0.3519195 Test Loss: 0.4051807
Validation loss decreased (0.359946 --> 0.351920).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.0198965072631836
Epoch: 8, Steps: 15 | Train Loss: 0.5759363 Vali Loss: 0.3461618 Test Loss: 0.4024335
Validation loss decreased (0.351920 --> 0.346162).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9150359630584717
Epoch: 9, Steps: 15 | Train Loss: 0.5719374 Vali Loss: 0.3388104 Test Loss: 0.4001707
Validation loss decreased (0.346162 --> 0.338810).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.766522169113159
Epoch: 10, Steps: 15 | Train Loss: 0.5652068 Vali Loss: 0.3347771 Test Loss: 0.3984259
Validation loss decreased (0.338810 --> 0.334777).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.9785521030426025
Epoch: 11, Steps: 15 | Train Loss: 0.5610086 Vali Loss: 0.3340960 Test Loss: 0.3970033
Validation loss decreased (0.334777 --> 0.334096).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.874171495437622
Epoch: 12, Steps: 15 | Train Loss: 0.5596331 Vali Loss: 0.3284851 Test Loss: 0.3958995
Validation loss decreased (0.334096 --> 0.328485).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.808997869491577
Epoch: 13, Steps: 15 | Train Loss: 0.5559465 Vali Loss: 0.3274503 Test Loss: 0.3949342
Validation loss decreased (0.328485 --> 0.327450).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.8728160858154297
Epoch: 14, Steps: 15 | Train Loss: 0.5551276 Vali Loss: 0.3263561 Test Loss: 0.3941400
Validation loss decreased (0.327450 --> 0.326356).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.819140911102295
Epoch: 15, Steps: 15 | Train Loss: 0.5530834 Vali Loss: 0.3240819 Test Loss: 0.3934853
Validation loss decreased (0.326356 --> 0.324082).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.920016288757324
Epoch: 16, Steps: 15 | Train Loss: 0.5508215 Vali Loss: 0.3220796 Test Loss: 0.3928860
Validation loss decreased (0.324082 --> 0.322080).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.826638698577881
Epoch: 17, Steps: 15 | Train Loss: 0.5491075 Vali Loss: 0.3204925 Test Loss: 0.3923951
Validation loss decreased (0.322080 --> 0.320493).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.050416946411133
Epoch: 18, Steps: 15 | Train Loss: 0.5474586 Vali Loss: 0.3204505 Test Loss: 0.3920137
Validation loss decreased (0.320493 --> 0.320450).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.7671728134155273
Epoch: 19, Steps: 15 | Train Loss: 0.5462632 Vali Loss: 0.3173835 Test Loss: 0.3916333
Validation loss decreased (0.320450 --> 0.317383).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.730860710144043
Epoch: 20, Steps: 15 | Train Loss: 0.5439427 Vali Loss: 0.3168279 Test Loss: 0.3913263
Validation loss decreased (0.317383 --> 0.316828).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.974759817123413
Epoch: 21, Steps: 15 | Train Loss: 0.5446715 Vali Loss: 0.3157539 Test Loss: 0.3910389
Validation loss decreased (0.316828 --> 0.315754).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.723304271697998
Epoch: 22, Steps: 15 | Train Loss: 0.5428034 Vali Loss: 0.3162290 Test Loss: 0.3908116
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.0014615058898926
Epoch: 23, Steps: 15 | Train Loss: 0.5438175 Vali Loss: 0.3148583 Test Loss: 0.3905996
Validation loss decreased (0.315754 --> 0.314858).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.3497824668884277
Epoch: 24, Steps: 15 | Train Loss: 0.5420718 Vali Loss: 0.3151784 Test Loss: 0.3903900
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.5293538570404053
Epoch: 25, Steps: 15 | Train Loss: 0.5399007 Vali Loss: 0.3127829 Test Loss: 0.3902447
Validation loss decreased (0.314858 --> 0.312783).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.430584669113159
Epoch: 26, Steps: 15 | Train Loss: 0.5385489 Vali Loss: 0.3123819 Test Loss: 0.3900950
Validation loss decreased (0.312783 --> 0.312382).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.5100584030151367
Epoch: 27, Steps: 15 | Train Loss: 0.5389552 Vali Loss: 0.3114617 Test Loss: 0.3899515
Validation loss decreased (0.312382 --> 0.311462).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.2422733306884766
Epoch: 28, Steps: 15 | Train Loss: 0.5391230 Vali Loss: 0.3110684 Test Loss: 0.3898316
Validation loss decreased (0.311462 --> 0.311068).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.1266491413116455
Epoch: 29, Steps: 15 | Train Loss: 0.5383370 Vali Loss: 0.3101694 Test Loss: 0.3897232
Validation loss decreased (0.311068 --> 0.310169).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.351365089416504
Epoch: 30, Steps: 15 | Train Loss: 0.5386489 Vali Loss: 0.3104420 Test Loss: 0.3896043
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.79384708404541
Epoch: 31, Steps: 15 | Train Loss: 0.5375829 Vali Loss: 0.3100878 Test Loss: 0.3895480
Validation loss decreased (0.310169 --> 0.310088).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.1654696464538574
Epoch: 32, Steps: 15 | Train Loss: 0.5378236 Vali Loss: 0.3100122 Test Loss: 0.3895049
Validation loss decreased (0.310088 --> 0.310012).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.124000072479248
Epoch: 33, Steps: 15 | Train Loss: 0.5368075 Vali Loss: 0.3084773 Test Loss: 0.3893950
Validation loss decreased (0.310012 --> 0.308477).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.249575614929199
Epoch: 34, Steps: 15 | Train Loss: 0.5358615 Vali Loss: 0.3104972 Test Loss: 0.3893360
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.1256637573242188
Epoch: 35, Steps: 15 | Train Loss: 0.5359555 Vali Loss: 0.3095424 Test Loss: 0.3892871
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.113703966140747
Epoch: 36, Steps: 15 | Train Loss: 0.5348778 Vali Loss: 0.3065168 Test Loss: 0.3892346
Validation loss decreased (0.308477 --> 0.306517).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.209153890609741
Epoch: 37, Steps: 15 | Train Loss: 0.5356040 Vali Loss: 0.3074274 Test Loss: 0.3892033
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.1789236068725586
Epoch: 38, Steps: 15 | Train Loss: 0.5343187 Vali Loss: 0.3078473 Test Loss: 0.3891302
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.9975550174713135
Epoch: 39, Steps: 15 | Train Loss: 0.5352002 Vali Loss: 0.3065919 Test Loss: 0.3890986
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.331682026386261, mae:0.3781053423881531, rse:0.46185094118118286, corr:[0.25857618 0.26713476 0.26667178 0.26491144 0.2659341  0.26658252
 0.2657087  0.26466176 0.2636491  0.2625317  0.26144    0.2604899
 0.25949314 0.25803423 0.2568349  0.25608698 0.2555029  0.25442448
 0.25332516 0.25228998 0.2511134  0.24978979 0.24814942 0.24660447
 0.24511416 0.24358627 0.24209987 0.24081463 0.2393006  0.23748414
 0.23598903 0.23503931 0.23452911 0.23323618 0.23153564 0.23045337
 0.2300245  0.22929464 0.22775143 0.22643706 0.22607206 0.22584714
 0.22494188 0.22382647 0.22304368 0.22230008 0.22075899 0.2185295
 0.21672341 0.21549195 0.21398245 0.2121544  0.21084377 0.20977934
 0.20792897 0.2056204  0.20388368 0.20308775 0.20234066 0.20081633
 0.19942814 0.19915989 0.19957775 0.19927399 0.19808592 0.19742706
 0.19755118 0.19738145 0.19644414 0.1954921  0.194991   0.19429551
 0.19269776 0.19086841 0.18983549 0.18926805 0.18832356 0.18690799
 0.18604381 0.1860685  0.18608242 0.18498434 0.1836712  0.18344708
 0.1838635  0.1836675  0.18278332 0.18239936 0.18259744 0.18236348
 0.18140098 0.18066706 0.18093917 0.18104582 0.18026538 0.17929602
 0.17895153 0.17873417 0.17773083 0.17638528 0.17581269 0.17568298
 0.17490028 0.17335124 0.172593   0.17295115 0.17309576 0.17221378
 0.17091915 0.17059149 0.17065038 0.1700065  0.16878203 0.16838804
 0.16862687 0.16820458 0.16699491 0.16594683 0.1655253  0.1644234
 0.16230835 0.1603997  0.1599526  0.15972719 0.15816474 0.1559858
 0.15510744 0.15525694 0.15473573 0.15308024 0.15186177 0.15161078
 0.15133858 0.15005763 0.14880973 0.14884411 0.14910756 0.14813115
 0.14631818 0.14547677 0.14572805 0.14539601 0.14383157 0.14206494
 0.14080577 0.13927874 0.13681564 0.13471133 0.13441344 0.1347137
 0.13375223 0.13148092 0.13068257 0.13144718 0.13147675 0.12971263
 0.12839018 0.12875952 0.12945421 0.12909417 0.12805699 0.12822229
 0.1287992  0.12814893 0.12699609 0.12714475 0.12810694 0.12710442
 0.12417802 0.12202179 0.12217222 0.1217614  0.11944166 0.11689589
 0.11681189 0.11677497 0.11448889 0.11195043 0.11164054 0.11294623
 0.11206976 0.1094219  0.10919081 0.11148369 0.11114776 0.10817213
 0.10698508 0.10765233 0.1063198  0.10315181 0.10821584 0.11465434]
