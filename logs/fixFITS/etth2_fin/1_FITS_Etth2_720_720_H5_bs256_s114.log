Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  195148800.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.895453929901123
Epoch: 1, Steps: 14 | Train Loss: 1.1778358 Vali Loss: 0.8973897 Test Loss: 0.5580220
Validation loss decreased (inf --> 0.897390).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.0869853496551514
Epoch: 2, Steps: 14 | Train Loss: 1.0641342 Vali Loss: 0.8444330 Test Loss: 0.5168279
Validation loss decreased (0.897390 --> 0.844433).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.823822259902954
Epoch: 3, Steps: 14 | Train Loss: 0.9928997 Vali Loss: 0.8100482 Test Loss: 0.4894123
Validation loss decreased (0.844433 --> 0.810048).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.0406692028045654
Epoch: 4, Steps: 14 | Train Loss: 0.9505523 Vali Loss: 0.7914264 Test Loss: 0.4725811
Validation loss decreased (0.810048 --> 0.791426).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.3708479404449463
Epoch: 5, Steps: 14 | Train Loss: 0.9228369 Vali Loss: 0.7743829 Test Loss: 0.4620737
Validation loss decreased (0.791426 --> 0.774383).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.3498570919036865
Epoch: 6, Steps: 14 | Train Loss: 0.9064489 Vali Loss: 0.7618665 Test Loss: 0.4550448
Validation loss decreased (0.774383 --> 0.761866).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.360618829727173
Epoch: 7, Steps: 14 | Train Loss: 0.8932648 Vali Loss: 0.7545926 Test Loss: 0.4501339
Validation loss decreased (0.761866 --> 0.754593).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.9988226890563965
Epoch: 8, Steps: 14 | Train Loss: 0.8859468 Vali Loss: 0.7454433 Test Loss: 0.4464758
Validation loss decreased (0.754593 --> 0.745443).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.2308571338653564
Epoch: 9, Steps: 14 | Train Loss: 0.8773911 Vali Loss: 0.7368350 Test Loss: 0.4437127
Validation loss decreased (0.745443 --> 0.736835).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.0053017139434814
Epoch: 10, Steps: 14 | Train Loss: 0.8730772 Vali Loss: 0.7326376 Test Loss: 0.4413822
Validation loss decreased (0.736835 --> 0.732638).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.1234045028686523
Epoch: 11, Steps: 14 | Train Loss: 0.8685544 Vali Loss: 0.7312210 Test Loss: 0.4394994
Validation loss decreased (0.732638 --> 0.731221).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.657442808151245
Epoch: 12, Steps: 14 | Train Loss: 0.8659658 Vali Loss: 0.7258101 Test Loss: 0.4378872
Validation loss decreased (0.731221 --> 0.725810).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.7374417781829834
Epoch: 13, Steps: 14 | Train Loss: 0.8623216 Vali Loss: 0.7205156 Test Loss: 0.4365067
Validation loss decreased (0.725810 --> 0.720516).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.6749210357666016
Epoch: 14, Steps: 14 | Train Loss: 0.8591648 Vali Loss: 0.7199249 Test Loss: 0.4352659
Validation loss decreased (0.720516 --> 0.719925).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.6909356117248535
Epoch: 15, Steps: 14 | Train Loss: 0.8560057 Vali Loss: 0.7141910 Test Loss: 0.4341890
Validation loss decreased (0.719925 --> 0.714191).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.523744583129883
Epoch: 16, Steps: 14 | Train Loss: 0.8558403 Vali Loss: 0.7138840 Test Loss: 0.4332079
Validation loss decreased (0.714191 --> 0.713884).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.558962345123291
Epoch: 17, Steps: 14 | Train Loss: 0.8525926 Vali Loss: 0.7114216 Test Loss: 0.4323592
Validation loss decreased (0.713884 --> 0.711422).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.282965660095215
Epoch: 18, Steps: 14 | Train Loss: 0.8517962 Vali Loss: 0.7072415 Test Loss: 0.4315173
Validation loss decreased (0.711422 --> 0.707242).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.607489585876465
Epoch: 19, Steps: 14 | Train Loss: 0.8502541 Vali Loss: 0.7071277 Test Loss: 0.4308130
Validation loss decreased (0.707242 --> 0.707128).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.329164981842041
Epoch: 20, Steps: 14 | Train Loss: 0.8485355 Vali Loss: 0.7059637 Test Loss: 0.4301720
Validation loss decreased (0.707128 --> 0.705964).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.232398509979248
Epoch: 21, Steps: 14 | Train Loss: 0.8480864 Vali Loss: 0.7073138 Test Loss: 0.4295653
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.0968408584594727
Epoch: 22, Steps: 14 | Train Loss: 0.8465316 Vali Loss: 0.7020042 Test Loss: 0.4289992
Validation loss decreased (0.705964 --> 0.702004).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.087376356124878
Epoch: 23, Steps: 14 | Train Loss: 0.8455273 Vali Loss: 0.7006075 Test Loss: 0.4284893
Validation loss decreased (0.702004 --> 0.700608).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.097409725189209
Epoch: 24, Steps: 14 | Train Loss: 0.8435324 Vali Loss: 0.6994907 Test Loss: 0.4280080
Validation loss decreased (0.700608 --> 0.699491).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.8160390853881836
Epoch: 25, Steps: 14 | Train Loss: 0.8432367 Vali Loss: 0.7012470 Test Loss: 0.4275633
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.144763946533203
Epoch: 26, Steps: 14 | Train Loss: 0.8428466 Vali Loss: 0.6965455 Test Loss: 0.4271644
Validation loss decreased (0.699491 --> 0.696545).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.2387943267822266
Epoch: 27, Steps: 14 | Train Loss: 0.8413346 Vali Loss: 0.7008693 Test Loss: 0.4267763
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.486908435821533
Epoch: 28, Steps: 14 | Train Loss: 0.8410711 Vali Loss: 0.6991604 Test Loss: 0.4264196
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.4927382469177246
Epoch: 29, Steps: 14 | Train Loss: 0.8409960 Vali Loss: 0.6950496 Test Loss: 0.4261183
Validation loss decreased (0.696545 --> 0.695050).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.7313759326934814
Epoch: 30, Steps: 14 | Train Loss: 0.8397534 Vali Loss: 0.6947269 Test Loss: 0.4258059
Validation loss decreased (0.695050 --> 0.694727).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.4619839191436768
Epoch: 31, Steps: 14 | Train Loss: 0.8383685 Vali Loss: 0.6949281 Test Loss: 0.4255055
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.636591911315918
Epoch: 32, Steps: 14 | Train Loss: 0.8390274 Vali Loss: 0.6933815 Test Loss: 0.4252404
Validation loss decreased (0.694727 --> 0.693382).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.424751043319702
Epoch: 33, Steps: 14 | Train Loss: 0.8358879 Vali Loss: 0.6926600 Test Loss: 0.4249770
Validation loss decreased (0.693382 --> 0.692660).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.7002477645874023
Epoch: 34, Steps: 14 | Train Loss: 0.8376666 Vali Loss: 0.6912386 Test Loss: 0.4247308
Validation loss decreased (0.692660 --> 0.691239).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.079900026321411
Epoch: 35, Steps: 14 | Train Loss: 0.8361770 Vali Loss: 0.6911275 Test Loss: 0.4245268
Validation loss decreased (0.691239 --> 0.691128).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.74753737449646
Epoch: 36, Steps: 14 | Train Loss: 0.8368489 Vali Loss: 0.6913274 Test Loss: 0.4243131
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.4941368103027344
Epoch: 37, Steps: 14 | Train Loss: 0.8349992 Vali Loss: 0.6926884 Test Loss: 0.4241072
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.647153854370117
Epoch: 38, Steps: 14 | Train Loss: 0.8347534 Vali Loss: 0.6904578 Test Loss: 0.4239191
Validation loss decreased (0.691128 --> 0.690458).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.539030075073242
Epoch: 39, Steps: 14 | Train Loss: 0.8356322 Vali Loss: 0.6910996 Test Loss: 0.4237521
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.5830137729644775
Epoch: 40, Steps: 14 | Train Loss: 0.8339362 Vali Loss: 0.6872315 Test Loss: 0.4235848
Validation loss decreased (0.690458 --> 0.687231).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.4533658027648926
Epoch: 41, Steps: 14 | Train Loss: 0.8338517 Vali Loss: 0.6904134 Test Loss: 0.4234267
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.6096363067626953
Epoch: 42, Steps: 14 | Train Loss: 0.8346670 Vali Loss: 0.6872692 Test Loss: 0.4232727
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.629730224609375
Epoch: 43, Steps: 14 | Train Loss: 0.8316833 Vali Loss: 0.6891410 Test Loss: 0.4231311
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3910631239414215, mae:0.4337531626224518, rse:0.49983787536621094, corr:[ 2.07394361e-01  2.13179529e-01  2.13662922e-01  2.12344483e-01
  2.12518781e-01  2.12643638e-01  2.12196484e-01  2.11535186e-01
  2.09948018e-01  2.08084673e-01  2.06732303e-01  2.06180170e-01
  2.05286339e-01  2.03131258e-01  2.01087907e-01  2.00394496e-01
  2.00385317e-01  1.99302405e-01  1.97631896e-01  1.96686581e-01
  1.96518511e-01  1.96033582e-01  1.93976119e-01  1.92137882e-01
  1.91215947e-01  1.90877661e-01  1.89970106e-01  1.89095512e-01
  1.88664645e-01  1.88290402e-01  1.87517390e-01  1.86665371e-01
  1.86394036e-01  1.85721636e-01  1.84333190e-01  1.82731971e-01
  1.82006657e-01  1.81862578e-01  1.81069583e-01  1.79706499e-01
  1.78683951e-01  1.78412437e-01  1.78271726e-01  1.77557483e-01
  1.76215932e-01  1.75342172e-01  1.74328834e-01  1.72561452e-01
  1.70639589e-01  1.69604540e-01  1.69112071e-01  1.68524116e-01
  1.67710453e-01  1.67010501e-01  1.66786149e-01  1.66278258e-01
  1.65049925e-01  1.63981766e-01  1.63931936e-01  1.64008468e-01
  1.63012475e-01  1.61523163e-01  1.61278486e-01  1.61954507e-01
  1.62260681e-01  1.61538407e-01  1.60934106e-01  1.61571294e-01
  1.62415981e-01  1.62166938e-01  1.60834178e-01  1.59811750e-01
  1.59479469e-01  1.58975527e-01  1.57943830e-01  1.57443762e-01
  1.57869712e-01  1.58160105e-01  1.57569602e-01  1.57248795e-01
  1.57590032e-01  1.57746643e-01  1.57003164e-01  1.56013742e-01
  1.56051099e-01  1.56726465e-01  1.56870395e-01  1.56026930e-01
  1.55561268e-01  1.56065121e-01  1.56731740e-01  1.56855479e-01
  1.56474829e-01  1.56456605e-01  1.56909704e-01  1.56872913e-01
  1.56225264e-01  1.55742273e-01  1.55860662e-01  1.56168669e-01
  1.56137526e-01  1.55931786e-01  1.56018853e-01  1.56223238e-01
  1.55858740e-01  1.55023813e-01  1.54590562e-01  1.54548824e-01
  1.54309407e-01  1.53436139e-01  1.52673930e-01  1.52477324e-01
  1.52670383e-01  1.52394354e-01  1.51750937e-01  1.51676804e-01
  1.52098790e-01  1.52119756e-01  1.51152939e-01  1.49748623e-01
  1.48712561e-01  1.48395017e-01  1.47862628e-01  1.46814823e-01
  1.46069378e-01  1.46278486e-01  1.46601647e-01  1.46380484e-01
  1.45856455e-01  1.45277128e-01  1.44616827e-01  1.43596500e-01
  1.42488241e-01  1.41783684e-01  1.41440213e-01  1.40719041e-01
  1.39572158e-01  1.38820797e-01  1.38594031e-01  1.38588771e-01
  1.38224885e-01  1.37795195e-01  1.37420520e-01  1.36636361e-01
  1.35085016e-01  1.33323491e-01  1.32172972e-01  1.31897777e-01
  1.32151514e-01  1.32323101e-01  1.32177383e-01  1.32000610e-01
  1.31675154e-01  1.30938187e-01  1.30188182e-01  1.29706204e-01
  1.29288197e-01  1.28626168e-01  1.27886280e-01  1.27603754e-01
  1.27919734e-01  1.28176913e-01  1.27880126e-01  1.27711415e-01
  1.28330991e-01  1.29108906e-01  1.28804073e-01  1.27295360e-01
  1.25995159e-01  1.25710666e-01  1.25895008e-01  1.25589266e-01
  1.24987625e-01  1.24759130e-01  1.24631427e-01  1.24037780e-01
  1.22959115e-01  1.21946082e-01  1.21376976e-01  1.21116489e-01
  1.20686196e-01  1.20256923e-01  1.20217696e-01  1.20235182e-01
  1.20289937e-01  1.20525971e-01  1.21077009e-01  1.21743254e-01
  1.22005276e-01  1.21993251e-01  1.22025676e-01  1.22109115e-01
  1.21890269e-01  1.21348895e-01  1.21050857e-01  1.21473990e-01
  1.22388020e-01  1.23126172e-01  1.23544604e-01  1.23814650e-01
  1.23878457e-01  1.23622403e-01  1.23343475e-01  1.23348586e-01
  1.23725228e-01  1.23730861e-01  1.23161331e-01  1.22901559e-01
  1.23610042e-01  1.24701440e-01  1.25058576e-01  1.25005901e-01
  1.25166029e-01  1.25547394e-01  1.25176236e-01  1.23944081e-01
  1.22737758e-01  1.22293681e-01  1.22286938e-01  1.21804789e-01
  1.21133193e-01  1.21104822e-01  1.21772610e-01  1.22112960e-01
  1.21630333e-01  1.20987833e-01  1.20762967e-01  1.20756321e-01
  1.20614313e-01  1.20575093e-01  1.20954297e-01  1.21350028e-01
  1.21393889e-01  1.21522062e-01  1.22182868e-01  1.23047732e-01
  1.23336911e-01  1.23241276e-01  1.23373076e-01  1.23683996e-01
  1.23611674e-01  1.23069972e-01  1.22904077e-01  1.23498835e-01
  1.24086931e-01  1.24071576e-01  1.23860605e-01  1.23953246e-01
  1.24233253e-01  1.24277972e-01  1.24440156e-01  1.25071391e-01
  1.25988334e-01  1.26576215e-01  1.26870424e-01  1.27441838e-01
  1.28242075e-01  1.28705844e-01  1.29019022e-01  1.30010217e-01
  1.31682321e-01  1.33216873e-01  1.33499950e-01  1.32862940e-01
  1.32556200e-01  1.32991955e-01  1.33542463e-01  1.33646801e-01
  1.33931160e-01  1.34892523e-01  1.35744780e-01  1.35577008e-01
  1.34926766e-01  1.34874970e-01  1.35433376e-01  1.35967150e-01
  1.36311576e-01  1.37304664e-01  1.38645306e-01  1.39223337e-01
  1.38950780e-01  1.39041319e-01  1.40270919e-01  1.41836792e-01
  1.42465293e-01  1.42519310e-01  1.43139288e-01  1.44200385e-01
  1.44531235e-01  1.43745944e-01  1.42996654e-01  1.43241629e-01
  1.43990755e-01  1.44394591e-01  1.44473538e-01  1.44713849e-01
  1.45047054e-01  1.45109609e-01  1.44895583e-01  1.45037070e-01
  1.45548776e-01  1.45879358e-01  1.45682767e-01  1.45635262e-01
  1.46464527e-01  1.47626445e-01  1.48122236e-01  1.48237318e-01
  1.48996025e-01  1.50250420e-01  1.50738433e-01  1.49719939e-01
  1.48348317e-01  1.47958681e-01  1.48309022e-01  1.48173466e-01
  1.47529602e-01  1.47365510e-01  1.47925928e-01  1.48199141e-01
  1.47676706e-01  1.47121131e-01  1.47093743e-01  1.47188351e-01
  1.46861240e-01  1.46628439e-01  1.47185534e-01  1.47929296e-01
  1.48270890e-01  1.48440614e-01  1.49115890e-01  1.50243968e-01
  1.50850043e-01  1.50828764e-01  1.50891170e-01  1.51128396e-01
  1.51019022e-01  1.50285751e-01  1.49394616e-01  1.49227813e-01
  1.49796173e-01  1.49889052e-01  1.49169326e-01  1.48539737e-01
  1.48571864e-01  1.48923531e-01  1.48804575e-01  1.48769513e-01
  1.49066791e-01  1.49597108e-01  1.49964973e-01  1.50266796e-01
  1.50871962e-01  1.51663899e-01  1.52130052e-01  1.52432352e-01
  1.52891979e-01  1.53409779e-01  1.53366491e-01  1.52941108e-01
  1.52861685e-01  1.53445408e-01  1.53791115e-01  1.53231397e-01
  1.52443156e-01  1.52620271e-01  1.53837189e-01  1.54703647e-01
  1.54379711e-01  1.53633878e-01  1.53106824e-01  1.53144240e-01
  1.53468952e-01  1.53897002e-01  1.54397339e-01  1.54979706e-01
  1.55528814e-01  1.55899644e-01  1.56336308e-01  1.56744540e-01
  1.56719640e-01  1.56104818e-01  1.55556470e-01  1.55527964e-01
  1.55530378e-01  1.55045897e-01  1.54404923e-01  1.54340491e-01
  1.54934540e-01  1.55567363e-01  1.55762240e-01  1.55728579e-01
  1.56037673e-01  1.56826928e-01  1.57358810e-01  1.57170966e-01
  1.56735003e-01  1.56345218e-01  1.56846464e-01  1.57480836e-01
  1.57434106e-01  1.57097399e-01  1.57366797e-01  1.58076555e-01
  1.58584312e-01  1.58809662e-01  1.58791557e-01  1.58677876e-01
  1.58076763e-01  1.57477200e-01  1.57087028e-01  1.56874761e-01
  1.56411022e-01  1.55723184e-01  1.55612722e-01  1.56268701e-01
  1.56758502e-01  1.56156570e-01  1.55146986e-01  1.54810071e-01
  1.54847607e-01  1.54383615e-01  1.53308794e-01  1.52504414e-01
  1.52276471e-01  1.51849270e-01  1.50770739e-01  1.49685904e-01
  1.49431333e-01  1.49518445e-01  1.48850501e-01  1.47559106e-01
  1.46494120e-01  1.46020174e-01  1.45342723e-01  1.43927678e-01
  1.42795697e-01  1.42661914e-01  1.42704993e-01  1.42120898e-01
  1.41202584e-01  1.41014859e-01  1.40931070e-01  1.39917374e-01
  1.38138533e-01  1.37504607e-01  1.38087466e-01  1.38691962e-01
  1.38290972e-01  1.37812749e-01  1.38052136e-01  1.38486713e-01
  1.37917653e-01  1.37175858e-01  1.37424260e-01  1.38050660e-01
  1.37781084e-01  1.36565432e-01  1.35960281e-01  1.36407956e-01
  1.36282265e-01  1.35012895e-01  1.34293750e-01  1.35195792e-01
  1.36048123e-01  1.35525346e-01  1.34220496e-01  1.33890450e-01
  1.34600595e-01  1.34475127e-01  1.33263066e-01  1.32513210e-01
  1.33233771e-01  1.34114116e-01  1.34027779e-01  1.33523270e-01
  1.33887202e-01  1.34755015e-01  1.34520084e-01  1.33260101e-01
  1.32491827e-01  1.32481202e-01  1.31728098e-01  1.29756927e-01
  1.28539309e-01  1.28936470e-01  1.29301652e-01  1.28007591e-01
  1.26323983e-01  1.26007408e-01  1.26616389e-01  1.25908613e-01
  1.23949632e-01  1.22920923e-01  1.23572946e-01  1.23867474e-01
  1.22388177e-01  1.20928951e-01  1.21078663e-01  1.21920861e-01
  1.21850230e-01  1.20960020e-01  1.20570369e-01  1.20338134e-01
  1.18964233e-01  1.16702706e-01  1.15143538e-01  1.14808172e-01
  1.14349656e-01  1.12908535e-01  1.11455202e-01  1.11531503e-01
  1.12292357e-01  1.11809775e-01  1.10116556e-01  1.08946376e-01
  1.08937114e-01  1.08525433e-01  1.06940694e-01  1.05480433e-01
  1.05671443e-01  1.06408812e-01  1.05675027e-01  1.04024567e-01
  1.03420481e-01  1.03893667e-01  1.03447579e-01  1.01395212e-01
  9.94066894e-02  9.86548662e-02  9.79465544e-02  9.59794000e-02
  9.42223594e-02  9.38505977e-02  9.36619565e-02  9.19518769e-02
  8.94895345e-02  8.84488001e-02  8.88035670e-02  8.82655382e-02
  8.58459622e-02  8.34725276e-02  8.29301551e-02  8.30603614e-02
  8.20405111e-02  8.05098116e-02  7.98771381e-02  8.03927034e-02
  8.07442963e-02  8.01602751e-02  7.95570761e-02  7.87590444e-02
  7.70067945e-02  7.43774548e-02  7.24175572e-02  7.18237162e-02
  7.12927505e-02  6.96338415e-02  6.77713901e-02  6.73577040e-02
  6.81657717e-02  6.79601133e-02  6.65777102e-02  6.56942502e-02
  6.62578121e-02  6.66862205e-02  6.54555857e-02  6.34855330e-02
  6.30109906e-02  6.36372343e-02  6.34260997e-02  6.21105805e-02
  6.10932037e-02  6.08884171e-02  6.02093637e-02  5.83511442e-02
  5.61290123e-02  5.47424741e-02  5.36011159e-02  5.20551540e-02
  5.07713817e-02  5.03240861e-02  4.99573350e-02  4.86293249e-02
  4.70343418e-02  4.67888005e-02  4.75868545e-02  4.74713556e-02
  4.55259308e-02  4.35728543e-02  4.32801470e-02  4.39383388e-02
  4.39764112e-02  4.30937372e-02  4.25046049e-02  4.29951511e-02
  4.35196720e-02  4.32888307e-02  4.24154326e-02  4.14011404e-02
  3.98728214e-02  3.78157087e-02  3.57580744e-02  3.45514789e-02
  3.32282558e-02  3.13017070e-02  2.94355880e-02  2.88227834e-02
  2.88440939e-02  2.81313527e-02  2.66095083e-02  2.54487097e-02
  2.55717393e-02  2.57802084e-02  2.48394702e-02  2.35789530e-02
  2.36117169e-02  2.50527374e-02  2.58077476e-02  2.53659766e-02
  2.49383748e-02  2.56108604e-02  2.60661095e-02  2.54166350e-02
  2.36619134e-02  2.17018202e-02  2.00669430e-02  1.87823456e-02
  1.83492377e-02  1.89755075e-02  1.93047244e-02  1.81709174e-02
  1.66819971e-02  1.67545993e-02  1.80285778e-02  1.84671804e-02
  1.68344304e-02  1.48969060e-02  1.50051843e-02  1.64054167e-02
  1.63754560e-02  1.50421290e-02  1.42349349e-02  1.46856708e-02
  1.54550839e-02  1.54133569e-02  1.43088503e-02  1.32705979e-02
  1.21884821e-02  1.08194789e-02  9.59217735e-03  8.94455519e-03
  8.77803285e-03  7.93619826e-03  6.67028688e-03  6.35518692e-03
  7.08532333e-03  7.49331340e-03  6.81124255e-03  6.31825533e-03
  6.81588566e-03  7.34470040e-03  6.65833335e-03  5.35552669e-03
  5.75659331e-03  7.73144281e-03  8.77134409e-03  7.74271134e-03
  6.64780708e-03  7.77077768e-03  9.75955930e-03  1.00152465e-02
  8.31511803e-03  6.81000296e-03  6.37048390e-03  5.92465280e-03
  5.65032661e-03  5.50557533e-03  5.52279362e-03  4.47665341e-03
  4.04947530e-03  5.63846575e-03  8.14984925e-03  8.59719422e-03
  6.35900768e-03  4.06349963e-03  5.00254519e-03  7.36800814e-03
  7.51774106e-03  5.81353065e-03  5.58148511e-03  7.18972087e-03
  8.39717314e-03  7.33921351e-03  5.83250867e-03  5.13500487e-03
  4.24514478e-03  1.92514679e-03 -1.67184568e-04 -1.07859203e-03
 -1.32934016e-03 -2.09703180e-03 -1.71555649e-03  5.00569986e-05
  1.39626907e-03  2.27764667e-05 -2.13411055e-03 -5.58270025e-04
  3.06025636e-03  1.89386634e-03 -3.45718907e-03 -5.29615674e-03
  2.22440445e-04  4.53276793e-03 -1.57939084e-03 -1.10561727e-02
 -1.15623008e-02 -5.66742988e-03 -9.89819597e-03 -2.50398852e-02]
