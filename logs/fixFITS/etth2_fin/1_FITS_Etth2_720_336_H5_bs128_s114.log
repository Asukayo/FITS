Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  71258880.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.563281536102295
Epoch: 1, Steps: 29 | Train Loss: 0.9208107 Vali Loss: 0.5854642 Test Loss: 0.4274077
Validation loss decreased (inf --> 0.585464).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.7536890506744385
Epoch: 2, Steps: 29 | Train Loss: 0.7732242 Vali Loss: 0.5141954 Test Loss: 0.3908868
Validation loss decreased (0.585464 --> 0.514195).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.865161657333374
Epoch: 3, Steps: 29 | Train Loss: 0.7157047 Vali Loss: 0.4825712 Test Loss: 0.3787396
Validation loss decreased (0.514195 --> 0.482571).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.540924072265625
Epoch: 4, Steps: 29 | Train Loss: 0.6924266 Vali Loss: 0.4602788 Test Loss: 0.3734983
Validation loss decreased (0.482571 --> 0.460279).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.653594732284546
Epoch: 5, Steps: 29 | Train Loss: 0.6788145 Vali Loss: 0.4479410 Test Loss: 0.3703747
Validation loss decreased (0.460279 --> 0.447941).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.429034948348999
Epoch: 6, Steps: 29 | Train Loss: 0.6718981 Vali Loss: 0.4392739 Test Loss: 0.3681845
Validation loss decreased (0.447941 --> 0.439274).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.54339337348938
Epoch: 7, Steps: 29 | Train Loss: 0.6637794 Vali Loss: 0.4364609 Test Loss: 0.3666950
Validation loss decreased (0.439274 --> 0.436461).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.5554656982421875
Epoch: 8, Steps: 29 | Train Loss: 0.6610879 Vali Loss: 0.4297581 Test Loss: 0.3654438
Validation loss decreased (0.436461 --> 0.429758).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.659055948257446
Epoch: 9, Steps: 29 | Train Loss: 0.6546297 Vali Loss: 0.4282196 Test Loss: 0.3645586
Validation loss decreased (0.429758 --> 0.428220).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.563838720321655
Epoch: 10, Steps: 29 | Train Loss: 0.6503139 Vali Loss: 0.4173439 Test Loss: 0.3636539
Validation loss decreased (0.428220 --> 0.417344).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.512107610702515
Epoch: 11, Steps: 29 | Train Loss: 0.6443410 Vali Loss: 0.4159476 Test Loss: 0.3630371
Validation loss decreased (0.417344 --> 0.415948).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.529840707778931
Epoch: 12, Steps: 29 | Train Loss: 0.6419438 Vali Loss: 0.4132051 Test Loss: 0.3624188
Validation loss decreased (0.415948 --> 0.413205).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.536470413208008
Epoch: 13, Steps: 29 | Train Loss: 0.6425250 Vali Loss: 0.4165705 Test Loss: 0.3619066
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.4736738204956055
Epoch: 14, Steps: 29 | Train Loss: 0.6420570 Vali Loss: 0.4151365 Test Loss: 0.3614910
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.5781943798065186
Epoch: 15, Steps: 29 | Train Loss: 0.6377207 Vali Loss: 0.4099414 Test Loss: 0.3611308
Validation loss decreased (0.413205 --> 0.409941).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.527828931808472
Epoch: 16, Steps: 29 | Train Loss: 0.6388888 Vali Loss: 0.4033430 Test Loss: 0.3608202
Validation loss decreased (0.409941 --> 0.403343).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.530324935913086
Epoch: 17, Steps: 29 | Train Loss: 0.6400511 Vali Loss: 0.4048185 Test Loss: 0.3605504
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.4796812534332275
Epoch: 18, Steps: 29 | Train Loss: 0.6368027 Vali Loss: 0.4079400 Test Loss: 0.3603006
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.552717447280884
Epoch: 19, Steps: 29 | Train Loss: 0.6315346 Vali Loss: 0.4059111 Test Loss: 0.3600731
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3580337166786194, mae:0.40034469962120056, rse:0.47841137647628784, corr:[0.25226578 0.261495   0.25966492 0.2573499  0.25924557 0.26038212
 0.2590095  0.25774607 0.25710955 0.25642592 0.2551797  0.25417402
 0.25370398 0.25261    0.2512348  0.25027037 0.24988477 0.2489802
 0.24795161 0.24716446 0.24661967 0.2458439  0.24399076 0.24218763
 0.2409965  0.24035585 0.23938234 0.23832133 0.23752323 0.23690622
 0.23613665 0.23532659 0.23478825 0.23389043 0.23269275 0.23175065
 0.23148605 0.23111619 0.22991338 0.22859527 0.22813478 0.22808154
 0.22751592 0.2263778  0.22524826 0.22447726 0.22308572 0.22085355
 0.21899137 0.21796997 0.21700086 0.21562356 0.21421416 0.21297772
 0.21168214 0.21010923 0.2083827  0.20710404 0.20620352 0.20516405
 0.20391649 0.20326649 0.2035896  0.20379491 0.20303178 0.20192285
 0.2013937  0.20163482 0.2016928  0.20110705 0.20021608 0.19946472
 0.19873217 0.19755583 0.19612117 0.19494781 0.19412984 0.19324239
 0.19227862 0.19185714 0.19168305 0.19088733 0.18964079 0.1889961
 0.18918425 0.18912172 0.18817404 0.18707538 0.18690826 0.18720426
 0.18701199 0.18633011 0.1861853  0.18665738 0.1868736  0.18619639
 0.18524969 0.18477495 0.18458481 0.18399617 0.18317945 0.1826446
 0.18246368 0.18204243 0.1814208  0.18092285 0.18083403 0.18044968
 0.17949574 0.17871328 0.17844915 0.17827737 0.17754787 0.17657287
 0.17591867 0.17576244 0.17547268 0.17448835 0.17328    0.17227356
 0.1714438  0.1704944  0.16930635 0.16806838 0.16714022 0.16660853
 0.16605371 0.16544287 0.16481028 0.16402039 0.1629531  0.16171506
 0.16081257 0.16031799 0.16005318 0.15963662 0.15897574 0.15817887
 0.15718886 0.15618719 0.15548693 0.15514527 0.15464629 0.15334556
 0.15136763 0.14957792 0.14834823 0.14740297 0.14643484 0.14555089
 0.14480372 0.14406438 0.143249   0.14224683 0.14134    0.14056334
 0.13979015 0.13902101 0.13871771 0.13879983 0.13845824 0.13748361
 0.13651358 0.13648155 0.13727084 0.13760267 0.13664493 0.13487624
 0.13346747 0.13251655 0.13147132 0.13007389 0.12880269 0.12782516
 0.12712076 0.12612762 0.12516141 0.12453452 0.12392164 0.12317993
 0.12230109 0.12156716 0.1209874  0.12041628 0.11977395 0.11963488
 0.11960565 0.11901117 0.11826465 0.11820679 0.11888727 0.11875723
 0.11734187 0.11583915 0.11540747 0.11588816 0.11583638 0.11479917
 0.11377729 0.11336964 0.11332163 0.11298332 0.11263011 0.11247731
 0.11208776 0.11111923 0.11003546 0.10998809 0.11061987 0.11075456
 0.11018775 0.11024857 0.11124939 0.11162835 0.11030645 0.108357
 0.1075632  0.10769784 0.10709818 0.10554108 0.10439972 0.10475729
 0.10502776 0.1044017  0.10350263 0.10352281 0.10398252 0.10335198
 0.10174502 0.10094065 0.1013799  0.10203801 0.10191488 0.10201728
 0.10282171 0.10339144 0.10296706 0.10263751 0.10344304 0.10413886
 0.10272276 0.09984861 0.09841258 0.09959952 0.10082525 0.09997811
 0.09871827 0.09894317 0.10012761 0.10021371 0.09927131 0.09912109
 0.09978469 0.10026643 0.10005243 0.1007435  0.10214136 0.10314251
 0.10292178 0.10323152 0.10510153 0.10668896 0.10652254 0.10504888
 0.10473631 0.10574767 0.10649304 0.10580319 0.10470263 0.10484336
 0.10565056 0.10581044 0.10560879 0.10563967 0.10625067 0.10587817
 0.10493225 0.10478102 0.10568108 0.10632218 0.10605422 0.1059302
 0.10655441 0.10701037 0.10616324 0.10536004 0.1064055  0.10824914
 0.10807213 0.10565223 0.10384379 0.10464067 0.10585868 0.10477761
 0.10263328 0.10195561 0.10297531 0.1033428  0.10258421 0.10192309
 0.10210653 0.10262854 0.10254715 0.10304742 0.10412833 0.10532732
 0.10480109 0.10415092 0.10508685 0.10677239 0.10646521 0.10410818
 0.10305038 0.10404691 0.10461369 0.10217448 0.09924594 0.0994698
 0.10201527 0.10289851 0.10116091 0.10110956 0.10389891 0.10622513
 0.10504213 0.1036704  0.10585014 0.10889923 0.10871824 0.10746197
 0.10904801 0.11149798 0.10978525 0.10737709 0.11631644 0.12862475]
