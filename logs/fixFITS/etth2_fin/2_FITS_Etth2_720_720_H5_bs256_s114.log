Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  195148800.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.740947961807251
Epoch: 1, Steps: 14 | Train Loss: 0.9562745 Vali Loss: 0.9316382 Test Loss: 0.5828593
Validation loss decreased (inf --> 0.931638).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.3593943119049072
Epoch: 2, Steps: 14 | Train Loss: 0.8748013 Vali Loss: 0.8921478 Test Loss: 0.5556417
Validation loss decreased (0.931638 --> 0.892148).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.6327319145202637
Epoch: 3, Steps: 14 | Train Loss: 0.8134641 Vali Loss: 0.8633152 Test Loss: 0.5340151
Validation loss decreased (0.892148 --> 0.863315).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.3950297832489014
Epoch: 4, Steps: 14 | Train Loss: 0.7684841 Vali Loss: 0.8451880 Test Loss: 0.5168090
Validation loss decreased (0.863315 --> 0.845188).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.7867109775543213
Epoch: 5, Steps: 14 | Train Loss: 0.7320228 Vali Loss: 0.8265716 Test Loss: 0.5034112
Validation loss decreased (0.845188 --> 0.826572).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.2293801307678223
Epoch: 6, Steps: 14 | Train Loss: 0.7038979 Vali Loss: 0.8114596 Test Loss: 0.4929350
Validation loss decreased (0.826572 --> 0.811460).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.3310396671295166
Epoch: 7, Steps: 14 | Train Loss: 0.6809959 Vali Loss: 0.8015133 Test Loss: 0.4847816
Validation loss decreased (0.811460 --> 0.801513).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.284560441970825
Epoch: 8, Steps: 14 | Train Loss: 0.6633008 Vali Loss: 0.7898643 Test Loss: 0.4782458
Validation loss decreased (0.801513 --> 0.789864).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.140042304992676
Epoch: 9, Steps: 14 | Train Loss: 0.6469522 Vali Loss: 0.7793813 Test Loss: 0.4730955
Validation loss decreased (0.789864 --> 0.779381).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.2175238132476807
Epoch: 10, Steps: 14 | Train Loss: 0.6351857 Vali Loss: 0.7736166 Test Loss: 0.4688149
Validation loss decreased (0.779381 --> 0.773617).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.3737857341766357
Epoch: 11, Steps: 14 | Train Loss: 0.6242668 Vali Loss: 0.7704653 Test Loss: 0.4653869
Validation loss decreased (0.773617 --> 0.770465).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.1883485317230225
Epoch: 12, Steps: 14 | Train Loss: 0.6157616 Vali Loss: 0.7641203 Test Loss: 0.4625907
Validation loss decreased (0.770465 --> 0.764120).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.435513734817505
Epoch: 13, Steps: 14 | Train Loss: 0.6077017 Vali Loss: 0.7578535 Test Loss: 0.4602621
Validation loss decreased (0.764120 --> 0.757854).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.434382677078247
Epoch: 14, Steps: 14 | Train Loss: 0.6010276 Vali Loss: 0.7565091 Test Loss: 0.4582797
Validation loss decreased (0.757854 --> 0.756509).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.4954001903533936
Epoch: 15, Steps: 14 | Train Loss: 0.5947450 Vali Loss: 0.7504941 Test Loss: 0.4566135
Validation loss decreased (0.756509 --> 0.750494).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.5602357387542725
Epoch: 16, Steps: 14 | Train Loss: 0.5908071 Vali Loss: 0.7492882 Test Loss: 0.4552155
Validation loss decreased (0.750494 --> 0.749288).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.48122501373291
Epoch: 17, Steps: 14 | Train Loss: 0.5854745 Vali Loss: 0.7467059 Test Loss: 0.4540346
Validation loss decreased (0.749288 --> 0.746706).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.5497372150421143
Epoch: 18, Steps: 14 | Train Loss: 0.5819395 Vali Loss: 0.7419648 Test Loss: 0.4529689
Validation loss decreased (0.746706 --> 0.741965).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.638094902038574
Epoch: 19, Steps: 14 | Train Loss: 0.5782215 Vali Loss: 0.7413456 Test Loss: 0.4520902
Validation loss decreased (0.741965 --> 0.741346).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.350477933883667
Epoch: 20, Steps: 14 | Train Loss: 0.5749555 Vali Loss: 0.7403092 Test Loss: 0.4513172
Validation loss decreased (0.741346 --> 0.740309).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.453284502029419
Epoch: 21, Steps: 14 | Train Loss: 0.5721455 Vali Loss: 0.7410986 Test Loss: 0.4506169
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.427246332168579
Epoch: 22, Steps: 14 | Train Loss: 0.5689765 Vali Loss: 0.7356685 Test Loss: 0.4499856
Validation loss decreased (0.740309 --> 0.735669).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.519585371017456
Epoch: 23, Steps: 14 | Train Loss: 0.5669781 Vali Loss: 0.7342082 Test Loss: 0.4494476
Validation loss decreased (0.735669 --> 0.734208).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.60087513923645
Epoch: 24, Steps: 14 | Train Loss: 0.5641113 Vali Loss: 0.7329432 Test Loss: 0.4489537
Validation loss decreased (0.734208 --> 0.732943).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.4747350215911865
Epoch: 25, Steps: 14 | Train Loss: 0.5622654 Vali Loss: 0.7346292 Test Loss: 0.4485121
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.4555540084838867
Epoch: 26, Steps: 14 | Train Loss: 0.5603391 Vali Loss: 0.7297800 Test Loss: 0.4481251
Validation loss decreased (0.732943 --> 0.729780).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.5106348991394043
Epoch: 27, Steps: 14 | Train Loss: 0.5581579 Vali Loss: 0.7339675 Test Loss: 0.4477586
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.3621556758880615
Epoch: 28, Steps: 14 | Train Loss: 0.5567783 Vali Loss: 0.7323285 Test Loss: 0.4474358
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.5126805305480957
Epoch: 29, Steps: 14 | Train Loss: 0.5556951 Vali Loss: 0.7280912 Test Loss: 0.4471514
Validation loss decreased (0.729780 --> 0.728091).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.407252311706543
Epoch: 30, Steps: 14 | Train Loss: 0.5539135 Vali Loss: 0.7277454 Test Loss: 0.4468730
Validation loss decreased (0.728091 --> 0.727745).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.772622585296631
Epoch: 31, Steps: 14 | Train Loss: 0.5522477 Vali Loss: 0.7278726 Test Loss: 0.4466103
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.514630079269409
Epoch: 32, Steps: 14 | Train Loss: 0.5514888 Vali Loss: 0.7261400 Test Loss: 0.4463837
Validation loss decreased (0.727745 --> 0.726140).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.6685760021209717
Epoch: 33, Steps: 14 | Train Loss: 0.5487845 Vali Loss: 0.7254562 Test Loss: 0.4461580
Validation loss decreased (0.726140 --> 0.725456).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.497180223464966
Epoch: 34, Steps: 14 | Train Loss: 0.5488912 Vali Loss: 0.7236906 Test Loss: 0.4459604
Validation loss decreased (0.725456 --> 0.723691).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.350942850112915
Epoch: 35, Steps: 14 | Train Loss: 0.5472498 Vali Loss: 0.7238759 Test Loss: 0.4457930
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.5820119380950928
Epoch: 36, Steps: 14 | Train Loss: 0.5470277 Vali Loss: 0.7239306 Test Loss: 0.4456144
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.580059051513672
Epoch: 37, Steps: 14 | Train Loss: 0.5453501 Vali Loss: 0.7250869 Test Loss: 0.4454512
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  195148800.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.6261227130889893
Epoch: 1, Steps: 14 | Train Loss: 0.8693785 Vali Loss: 0.7171378 Test Loss: 0.4420723
Validation loss decreased (inf --> 0.717138).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.6863152980804443
Epoch: 2, Steps: 14 | Train Loss: 0.8607733 Vali Loss: 0.7076854 Test Loss: 0.4389307
Validation loss decreased (0.717138 --> 0.707685).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.713595151901245
Epoch: 3, Steps: 14 | Train Loss: 0.8550729 Vali Loss: 0.7062451 Test Loss: 0.4365491
Validation loss decreased (0.707685 --> 0.706245).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.8519420623779297
Epoch: 4, Steps: 14 | Train Loss: 0.8498741 Vali Loss: 0.6997361 Test Loss: 0.4344563
Validation loss decreased (0.706245 --> 0.699736).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.819852590560913
Epoch: 5, Steps: 14 | Train Loss: 0.8472748 Vali Loss: 0.6985359 Test Loss: 0.4326072
Validation loss decreased (0.699736 --> 0.698536).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.9296462535858154
Epoch: 6, Steps: 14 | Train Loss: 0.8435155 Vali Loss: 0.6905657 Test Loss: 0.4310694
Validation loss decreased (0.698536 --> 0.690566).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.8076999187469482
Epoch: 7, Steps: 14 | Train Loss: 0.8406570 Vali Loss: 0.6856186 Test Loss: 0.4297031
Validation loss decreased (0.690566 --> 0.685619).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.861079454421997
Epoch: 8, Steps: 14 | Train Loss: 0.8386246 Vali Loss: 0.6890630 Test Loss: 0.4285088
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9077749252319336
Epoch: 9, Steps: 14 | Train Loss: 0.8374588 Vali Loss: 0.6872824 Test Loss: 0.4274094
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.9651193618774414
Epoch: 10, Steps: 14 | Train Loss: 0.8345042 Vali Loss: 0.6840827 Test Loss: 0.4264901
Validation loss decreased (0.685619 --> 0.684083).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.8706321716308594
Epoch: 11, Steps: 14 | Train Loss: 0.8335359 Vali Loss: 0.6855698 Test Loss: 0.4256015
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.9357645511627197
Epoch: 12, Steps: 14 | Train Loss: 0.8309306 Vali Loss: 0.6775324 Test Loss: 0.4247972
Validation loss decreased (0.684083 --> 0.677532).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.868001937866211
Epoch: 13, Steps: 14 | Train Loss: 0.8311784 Vali Loss: 0.6839672 Test Loss: 0.4241709
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.825460195541382
Epoch: 14, Steps: 14 | Train Loss: 0.8286973 Vali Loss: 0.6778139 Test Loss: 0.4235412
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.7117819786071777
Epoch: 15, Steps: 14 | Train Loss: 0.8274179 Vali Loss: 0.6795377 Test Loss: 0.4229058
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3919471204280853, mae:0.4335803985595703, rse:0.5004025101661682, corr:[ 2.15119466e-01  2.21754283e-01  2.20432565e-01  2.18478635e-01
  2.18747720e-01  2.19247997e-01  2.18433514e-01  2.16884062e-01
  2.15505943e-01  2.14380160e-01  2.13324159e-01  2.11841926e-01
  2.10013404e-01  2.08229423e-01  2.06922337e-01  2.05966398e-01
  2.04844952e-01  2.03285977e-01  2.01837823e-01  2.00929120e-01
  2.00337842e-01  1.99435517e-01  1.97774991e-01  1.95923969e-01
  1.94501325e-01  1.93608567e-01  1.92782000e-01  1.91799164e-01
  1.90910771e-01  1.90255567e-01  1.89803883e-01  1.89188540e-01
  1.88289389e-01  1.87154740e-01  1.86124429e-01  1.85312971e-01
  1.84427574e-01  1.83264092e-01  1.81871474e-01  1.80799246e-01
  1.80269912e-01  1.79910883e-01  1.79226398e-01  1.78223953e-01
  1.77216321e-01  1.76496059e-01  1.75828338e-01  1.74498931e-01
  1.72692314e-01  1.70923799e-01  1.69637293e-01  1.68916792e-01
  1.68728992e-01  1.68289736e-01  1.67082340e-01  1.65839359e-01
  1.65233210e-01  1.65055349e-01  1.64578527e-01  1.63699836e-01
  1.62848532e-01  1.62348881e-01  1.62305146e-01  1.62382796e-01
  1.62110776e-01  1.62031919e-01  1.62320748e-01  1.62752822e-01
  1.62930429e-01  1.62748098e-01  1.62240028e-01  1.61504149e-01
  1.60631135e-01  1.59807742e-01  1.59246922e-01  1.58758000e-01
  1.58525825e-01  1.58428833e-01  1.58310518e-01  1.58134133e-01
  1.58034727e-01  1.57752663e-01  1.57336369e-01  1.57203108e-01
  1.57282382e-01  1.57146484e-01  1.56615391e-01  1.56269059e-01
  1.56579942e-01  1.57150239e-01  1.57253355e-01  1.56908348e-01
  1.56991810e-01  1.57521576e-01  1.58022761e-01  1.57838359e-01
  1.57147542e-01  1.56628087e-01  1.56564206e-01  1.56768858e-01
  1.56927586e-01  1.56766832e-01  1.56648502e-01  1.56548977e-01
  1.56563014e-01  1.56401888e-01  1.56229392e-01  1.56148627e-01
  1.55866489e-01  1.55366972e-01  1.54779986e-01  1.54318035e-01
  1.53979361e-01  1.53820604e-01  1.53685555e-01  1.53561413e-01
  1.53451711e-01  1.53157666e-01  1.52747869e-01  1.51971817e-01
  1.50863573e-01  1.49709940e-01  1.48897633e-01  1.48407444e-01
  1.47982121e-01  1.47716522e-01  1.47507370e-01  1.47174075e-01
  1.46808952e-01  1.46393269e-01  1.46038815e-01  1.45247802e-01
  1.44137904e-01  1.43000856e-01  1.42250448e-01  1.41903281e-01
  1.41332328e-01  1.40562594e-01  1.39872223e-01  1.39472559e-01
  1.39484778e-01  1.39460444e-01  1.39082745e-01  1.38025224e-01
  1.36269331e-01  1.34602815e-01  1.33586198e-01  1.33202791e-01
  1.33003578e-01  1.32686466e-01  1.32247552e-01  1.31778672e-01
  1.31611884e-01  1.31383538e-01  1.30843326e-01  1.30073845e-01
  1.29333079e-01  1.28797248e-01  1.28408462e-01  1.28077179e-01
  1.27615601e-01  1.27392873e-01  1.27419606e-01  1.27693787e-01
  1.28267050e-01  1.28575280e-01  1.28166825e-01  1.26942158e-01
  1.25724077e-01  1.25106737e-01  1.24993190e-01  1.24790564e-01
  1.24365769e-01  1.23849958e-01  1.23324022e-01  1.22620754e-01
  1.21810481e-01  1.21088013e-01  1.20466731e-01  1.19943090e-01
  1.19354643e-01  1.18722789e-01  1.18322358e-01  1.18135542e-01
  1.18114628e-01  1.18321218e-01  1.18540511e-01  1.18733525e-01
  1.19076155e-01  1.19536653e-01  1.19985722e-01  1.19899005e-01
  1.19349390e-01  1.18999243e-01  1.19002245e-01  1.19280919e-01
  1.19576894e-01  1.19801097e-01  1.20172553e-01  1.20339483e-01
  1.20121360e-01  1.19636029e-01  1.19491488e-01  1.19552374e-01
  1.19491048e-01  1.19090594e-01  1.18506759e-01  1.18353300e-01
  1.18793249e-01  1.19523942e-01  1.20072551e-01  1.20539024e-01
  1.21042065e-01  1.21178634e-01  1.20719999e-01  1.19847260e-01
  1.19054943e-01  1.18381880e-01  1.17695406e-01  1.17242731e-01
  1.17049165e-01  1.17260098e-01  1.17195711e-01  1.16963573e-01
  1.16671905e-01  1.16599120e-01  1.16639890e-01  1.16410121e-01
  1.16030827e-01  1.15802832e-01  1.15951478e-01  1.16280898e-01
  1.16534375e-01  1.16883770e-01  1.17425621e-01  1.17969863e-01
  1.18352093e-01  1.18557423e-01  1.18760429e-01  1.18939608e-01
  1.18961744e-01  1.18749775e-01  1.18545666e-01  1.18659459e-01
  1.18840471e-01  1.19116664e-01  1.19613595e-01  1.19981550e-01
  1.20139301e-01  1.20231949e-01  1.20753087e-01  1.21463552e-01
  1.21750116e-01  1.21814832e-01  1.22261301e-01  1.23397499e-01
  1.24377035e-01  1.24889098e-01  1.25372082e-01  1.26413167e-01
  1.27563462e-01  1.28205329e-01  1.28618062e-01  1.28995210e-01
  1.29380539e-01  1.29298031e-01  1.28875226e-01  1.28870681e-01
  1.29537523e-01  1.30781814e-01  1.31719202e-01  1.31893188e-01
  1.31758288e-01  1.31757349e-01  1.32167935e-01  1.32782876e-01
  1.33414507e-01  1.34246275e-01  1.34846240e-01  1.35134995e-01
  1.35504514e-01  1.36320278e-01  1.37423992e-01  1.38452768e-01
  1.39119178e-01  1.39701605e-01  1.40547201e-01  1.41255349e-01
  1.41273081e-01  1.40659630e-01  1.40125528e-01  1.40251741e-01
  1.40697479e-01  1.40898511e-01  1.40775368e-01  1.40640646e-01
  1.40903950e-01  1.41477212e-01  1.41975567e-01  1.42078593e-01
  1.41923040e-01  1.42099276e-01  1.42540976e-01  1.42840281e-01
  1.42840177e-01  1.43226668e-01  1.44009456e-01  1.45048812e-01
  1.45821229e-01  1.46260932e-01  1.46571338e-01  1.46448195e-01
  1.45770520e-01  1.44678324e-01  1.44088954e-01  1.44027159e-01
  1.44142017e-01  1.43901363e-01  1.43526390e-01  1.43561527e-01
  1.43849537e-01  1.44123450e-01  1.43929303e-01  1.43631205e-01
  1.43542007e-01  1.43712074e-01  1.43967539e-01  1.43933937e-01
  1.44151956e-01  1.44841835e-01  1.45465031e-01  1.45787299e-01
  1.46054491e-01  1.46872491e-01  1.48084790e-01  1.48320720e-01
  1.47111744e-01  1.45880386e-01  1.45826101e-01  1.46639481e-01
  1.47451803e-01  1.47378296e-01  1.46756142e-01  1.46166906e-01
  1.46051615e-01  1.46340147e-01  1.46497861e-01  1.46737874e-01
  1.46790177e-01  1.46853358e-01  1.47138894e-01  1.47826388e-01
  1.48509800e-01  1.48989171e-01  1.49017200e-01  1.49093479e-01
  1.49541274e-01  1.50412276e-01  1.51270777e-01  1.51549682e-01
  1.51288688e-01  1.50993362e-01  1.51178181e-01  1.51497424e-01
  1.51680991e-01  1.51596069e-01  1.51735812e-01  1.52401984e-01
  1.52958155e-01  1.53112933e-01  1.52622685e-01  1.52291700e-01
  1.52532443e-01  1.53057486e-01  1.53260782e-01  1.53221846e-01
  1.53576255e-01  1.54202893e-01  1.54522538e-01  1.54450670e-01
  1.54282674e-01  1.54312909e-01  1.54593766e-01  1.54931471e-01
  1.54820308e-01  1.54155180e-01  1.53385326e-01  1.53078958e-01
  1.53462201e-01  1.54217854e-01  1.55010626e-01  1.55668333e-01
  1.56160489e-01  1.56461194e-01  1.56336516e-01  1.55946806e-01
  1.55955955e-01  1.56095147e-01  1.56331435e-01  1.56127557e-01
  1.55414656e-01  1.55223742e-01  1.56179056e-01  1.57598734e-01
  1.58217847e-01  1.58032596e-01  1.57637939e-01  1.57258272e-01
  1.56492427e-01  1.55704558e-01  1.55429751e-01  1.55579254e-01
  1.55519560e-01  1.54938743e-01  1.54513776e-01  1.54856160e-01
  1.55375436e-01  1.55184716e-01  1.54444292e-01  1.53949812e-01
  1.53427601e-01  1.52489290e-01  1.51323825e-01  1.50705770e-01
  1.50461093e-01  1.49717942e-01  1.48414850e-01  1.47393927e-01
  1.47467494e-01  1.47950128e-01  1.47523314e-01  1.46291584e-01
  1.45017534e-01  1.44145489e-01  1.43231377e-01  1.41742468e-01
  1.40567064e-01  1.40449107e-01  1.40610084e-01  1.40036717e-01
  1.38525993e-01  1.37615845e-01  1.37478113e-01  1.37335032e-01
  1.36244893e-01  1.34982973e-01  1.34094641e-01  1.33814678e-01
  1.33603662e-01  1.33493572e-01  1.33512691e-01  1.33518994e-01
  1.32948458e-01  1.32430315e-01  1.32709980e-01  1.33253559e-01
  1.33166581e-01  1.32259265e-01  1.31579146e-01  1.31857350e-01
  1.32237107e-01  1.31976664e-01  1.31620690e-01  1.31710663e-01
  1.31474584e-01  1.30956098e-01  1.30631700e-01  1.30864620e-01
  1.31297454e-01  1.30673677e-01  1.29227668e-01  1.28172144e-01
  1.28274783e-01  1.28737897e-01  1.28721908e-01  1.28472343e-01
  1.28783464e-01  1.29750431e-01  1.30128711e-01  1.29415423e-01
  1.28335699e-01  1.27741098e-01  1.27157673e-01  1.25823006e-01
  1.24591768e-01  1.24228254e-01  1.24241643e-01  1.23352610e-01
  1.21741652e-01  1.20505720e-01  1.20437756e-01  1.20447792e-01
  1.19580284e-01  1.18222728e-01  1.17405489e-01  1.17102571e-01
  1.16569810e-01  1.16104282e-01  1.16043068e-01  1.16245240e-01
  1.16282187e-01  1.15875222e-01  1.15256160e-01  1.14203051e-01
  1.12691797e-01  1.11167431e-01  1.10012278e-01  1.09203003e-01
  1.08465411e-01  1.07671745e-01  1.06744379e-01  1.06246032e-01
  1.06279597e-01  1.06251895e-01  1.05657972e-01  1.04540423e-01
  1.03504166e-01  1.02662221e-01  1.01966925e-01  1.01279140e-01
  1.00949705e-01  1.00917421e-01  1.00366727e-01  9.93018523e-02
  9.84599069e-02  9.83416885e-02  9.81751159e-02  9.70904231e-02
  9.52471420e-02  9.35788602e-02  9.23578516e-02  9.08876508e-02
  8.95552263e-02  8.87971222e-02  8.85268450e-02  8.78066346e-02
  8.62133875e-02  8.45381320e-02  8.36784318e-02  8.34236592e-02
  8.25933069e-02  8.07929859e-02  7.89112002e-02  7.77256340e-02
  7.69635066e-02  7.63105527e-02  7.53733367e-02  7.47803748e-02
  7.47372359e-02  7.45253041e-02  7.39547685e-02  7.26320744e-02
  7.08538294e-02  6.89654499e-02  6.73092753e-02  6.62504360e-02
  6.57440424e-02  6.52250573e-02  6.41297624e-02  6.28192127e-02
  6.23186491e-02  6.22962452e-02  6.22411296e-02  6.15489297e-02
  6.05713129e-02  5.97070567e-02  5.89005351e-02  5.77623248e-02
  5.69535308e-02  5.66322021e-02  5.64154051e-02  5.59566766e-02
  5.50310500e-02  5.40530421e-02  5.31704016e-02  5.24278767e-02
  5.12139648e-02  4.95778807e-02  4.76616435e-02  4.60543707e-02
  4.53766957e-02  4.52256016e-02  4.48140316e-02  4.37081270e-02
  4.24138494e-02  4.17957306e-02  4.17005233e-02  4.14808020e-02
  4.04813364e-02  3.92053612e-02  3.83558683e-02  3.79654095e-02
  3.78911272e-02  3.78047004e-02  3.74965817e-02  3.73109207e-02
  3.71301211e-02  3.66755240e-02  3.55910137e-02  3.42008509e-02
  3.28036882e-02  3.15224081e-02  3.00193187e-02  2.87959445e-02
  2.74633989e-02  2.61628218e-02  2.47790739e-02  2.37120390e-02
  2.30302047e-02  2.25787126e-02  2.18311381e-02  2.03396920e-02
  1.88900828e-02  1.80319119e-02  1.75138675e-02  1.68733671e-02
  1.61911193e-02  1.63621325e-02  1.68872382e-02  1.71758831e-02
  1.68135706e-02  1.65437814e-02  1.63227785e-02  1.62987132e-02
  1.56004839e-02  1.40792262e-02  1.25670489e-02  1.16349710e-02
  1.13565503e-02  1.15570193e-02  1.16447760e-02  1.12164300e-02
  1.06204469e-02  1.04176505e-02  1.05283828e-02  1.07748630e-02
  1.02651212e-02  8.80184490e-03  7.58181419e-03  7.41674425e-03
  7.51302717e-03  7.34312879e-03  6.65739877e-03  5.78769948e-03
  5.67159336e-03  6.30182261e-03  6.18803827e-03  5.14997169e-03
  3.72446049e-03  2.91602546e-03  2.69379467e-03  2.18300405e-03
  1.40106201e-03  3.80886602e-04 -4.19608521e-04 -4.38938121e-04
  1.62342621e-04  7.24471000e-04  5.87002898e-04  2.29317811e-05
 -5.57904888e-04 -7.73587904e-04 -7.90625927e-04 -1.33277895e-03
 -1.92662643e-03 -1.99114112e-03 -1.57972600e-03 -1.47611613e-03
 -2.02769414e-03 -2.14990345e-03 -1.43040251e-03 -5.32362785e-04
 -8.11574049e-04 -2.23257067e-03 -3.69603001e-03 -4.55114897e-03
 -3.99802346e-03 -3.40245664e-03 -3.38902324e-03 -4.65905853e-03
 -5.17410785e-03 -4.06516204e-03 -2.36559450e-03 -2.06331513e-03
 -3.56330327e-03 -5.98590542e-03 -6.66623237e-03 -5.59207611e-03
 -4.75355657e-03 -5.44213690e-03 -6.86488254e-03 -7.98729435e-03
 -7.59177608e-03 -7.06986291e-03 -7.85637461e-03 -1.00936256e-02
 -1.18769854e-02 -1.20427674e-02 -1.10757928e-02 -1.11902067e-02
 -1.23447226e-02 -1.32976342e-02 -1.19617051e-02 -9.63028241e-03
 -8.25564470e-03 -9.16948635e-03 -1.14581240e-02 -1.14312042e-02
 -9.26295947e-03 -9.19197034e-03 -1.22358212e-02 -1.56992748e-02
 -1.50083536e-02 -1.13663459e-02 -1.24338400e-02 -1.97338574e-02
 -2.58994121e-02 -2.29968186e-02 -1.67393200e-02 -2.06731837e-02]
