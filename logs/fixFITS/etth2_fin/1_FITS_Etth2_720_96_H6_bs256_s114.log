Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=196, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  155947008.0
params:  43734.0
Trainable parameters:  43734
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.444267749786377
Epoch: 1, Steps: 15 | Train Loss: 0.7286702 Vali Loss: 0.4013612 Test Loss: 0.4208936
Validation loss decreased (inf --> 0.401361).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.2122325897216797
Epoch: 2, Steps: 15 | Train Loss: 0.5834084 Vali Loss: 0.3329372 Test Loss: 0.3657193
Validation loss decreased (0.401361 --> 0.332937).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.1476268768310547
Epoch: 3, Steps: 15 | Train Loss: 0.5207558 Vali Loss: 0.2972917 Test Loss: 0.3410182
Validation loss decreased (0.332937 --> 0.297292).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.1078410148620605
Epoch: 4, Steps: 15 | Train Loss: 0.4912449 Vali Loss: 0.2794800 Test Loss: 0.3279986
Validation loss decreased (0.297292 --> 0.279480).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2427988052368164
Epoch: 5, Steps: 15 | Train Loss: 0.4788354 Vali Loss: 0.2702574 Test Loss: 0.3200207
Validation loss decreased (0.279480 --> 0.270257).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.197882652282715
Epoch: 6, Steps: 15 | Train Loss: 0.4650264 Vali Loss: 0.2625817 Test Loss: 0.3148786
Validation loss decreased (0.270257 --> 0.262582).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.269218921661377
Epoch: 7, Steps: 15 | Train Loss: 0.4562276 Vali Loss: 0.2579015 Test Loss: 0.3112732
Validation loss decreased (0.262582 --> 0.257901).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.4030654430389404
Epoch: 8, Steps: 15 | Train Loss: 0.4535666 Vali Loss: 0.2527085 Test Loss: 0.3084171
Validation loss decreased (0.257901 --> 0.252709).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.4284985065460205
Epoch: 9, Steps: 15 | Train Loss: 0.4500643 Vali Loss: 0.2514411 Test Loss: 0.3062821
Validation loss decreased (0.252709 --> 0.251441).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.1114540100097656
Epoch: 10, Steps: 15 | Train Loss: 0.4465996 Vali Loss: 0.2468974 Test Loss: 0.3046260
Validation loss decreased (0.251441 --> 0.246897).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.3586881160736084
Epoch: 11, Steps: 15 | Train Loss: 0.4424645 Vali Loss: 0.2461493 Test Loss: 0.3032423
Validation loss decreased (0.246897 --> 0.246149).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3626041412353516
Epoch: 12, Steps: 15 | Train Loss: 0.4384188 Vali Loss: 0.2428761 Test Loss: 0.3020786
Validation loss decreased (0.246149 --> 0.242876).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.0297064781188965
Epoch: 13, Steps: 15 | Train Loss: 0.4373070 Vali Loss: 0.2421980 Test Loss: 0.3012030
Validation loss decreased (0.242876 --> 0.242198).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.2175285816192627
Epoch: 14, Steps: 15 | Train Loss: 0.4353780 Vali Loss: 0.2417682 Test Loss: 0.3003083
Validation loss decreased (0.242198 --> 0.241768).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.3471624851226807
Epoch: 15, Steps: 15 | Train Loss: 0.4338271 Vali Loss: 0.2391683 Test Loss: 0.2996798
Validation loss decreased (0.241768 --> 0.239168).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.0425052642822266
Epoch: 16, Steps: 15 | Train Loss: 0.4326960 Vali Loss: 0.2375458 Test Loss: 0.2990755
Validation loss decreased (0.239168 --> 0.237546).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.259349822998047
Epoch: 17, Steps: 15 | Train Loss: 0.4315030 Vali Loss: 0.2371368 Test Loss: 0.2985291
Validation loss decreased (0.237546 --> 0.237137).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.479280710220337
Epoch: 18, Steps: 15 | Train Loss: 0.4305472 Vali Loss: 0.2374951 Test Loss: 0.2980579
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.50339674949646
Epoch: 19, Steps: 15 | Train Loss: 0.4289693 Vali Loss: 0.2356025 Test Loss: 0.2977157
Validation loss decreased (0.237137 --> 0.235602).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.3614389896392822
Epoch: 20, Steps: 15 | Train Loss: 0.4261225 Vali Loss: 0.2366818 Test Loss: 0.2973246
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.666980266571045
Epoch: 21, Steps: 15 | Train Loss: 0.4246460 Vali Loss: 0.2342815 Test Loss: 0.2969739
Validation loss decreased (0.235602 --> 0.234281).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.3183934688568115
Epoch: 22, Steps: 15 | Train Loss: 0.4251014 Vali Loss: 0.2322833 Test Loss: 0.2967356
Validation loss decreased (0.234281 --> 0.232283).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.2359156608581543
Epoch: 23, Steps: 15 | Train Loss: 0.4252339 Vali Loss: 0.2345093 Test Loss: 0.2964462
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.1129462718963623
Epoch: 24, Steps: 15 | Train Loss: 0.4249469 Vali Loss: 0.2310125 Test Loss: 0.2962497
Validation loss decreased (0.232283 --> 0.231013).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.0192723274230957
Epoch: 25, Steps: 15 | Train Loss: 0.4216867 Vali Loss: 0.2323782 Test Loss: 0.2960506
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.3811352252960205
Epoch: 26, Steps: 15 | Train Loss: 0.4228407 Vali Loss: 0.2312176 Test Loss: 0.2958446
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.266366481781006
Epoch: 27, Steps: 15 | Train Loss: 0.4210914 Vali Loss: 0.2315248 Test Loss: 0.2956639
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.27508965134620667, mae:0.3423394560813904, rse:0.42268654704093933, corr:[0.26988566 0.27726486 0.27513385 0.27504703 0.2757336  0.27463382
 0.2734855  0.27304065 0.27210307 0.27071953 0.26927254 0.2678132
 0.26666197 0.26536092 0.26388752 0.2630773  0.26278585 0.26198912
 0.2608214  0.2599709  0.2589119  0.25757435 0.2561198  0.25462705
 0.2526941  0.25090107 0.24971005 0.248273   0.24645665 0.24508762
 0.2443623  0.24299982 0.24130137 0.24034683 0.23955509 0.23809555
 0.23674789 0.23627867 0.2354577  0.23391786 0.23298876 0.2329561
 0.23233949 0.23098741 0.230029   0.229322   0.22799516 0.22622265
 0.22467153 0.22293322 0.22108074 0.2199321  0.21942465 0.21786112
 0.21546517 0.21420152 0.21311265 0.21115188 0.20958713 0.20906472
 0.20827733 0.20722671 0.20746218 0.2081061  0.20728855 0.20665121
 0.2069524  0.20655122 0.20535949 0.20500606 0.20491773 0.20343322
 0.2015373  0.20119156 0.20098047 0.19883226 0.19754012 0.19821197
 0.19806047 0.19604628 0.19576712 0.19667073 0.1955282  0.19402282
 0.194325   0.19460092 0.19325916 0.19331485 0.19367231 0.19166799
 0.18951699 0.19050272 0.18942119 0.18412484 0.18567963 0.18254535]
