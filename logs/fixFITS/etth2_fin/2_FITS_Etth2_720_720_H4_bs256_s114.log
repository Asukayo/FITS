Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  128708608.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.9691734313964844
Epoch: 1, Steps: 14 | Train Loss: 0.9317170 Vali Loss: 0.9198958 Test Loss: 0.5775698
Validation loss decreased (inf --> 0.919896).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.822850465774536
Epoch: 2, Steps: 14 | Train Loss: 0.8554282 Vali Loss: 0.8778902 Test Loss: 0.5521729
Validation loss decreased (0.919896 --> 0.877890).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.60158634185791
Epoch: 3, Steps: 14 | Train Loss: 0.7994471 Vali Loss: 0.8533999 Test Loss: 0.5321244
Validation loss decreased (0.877890 --> 0.853400).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.176408290863037
Epoch: 4, Steps: 14 | Train Loss: 0.7538437 Vali Loss: 0.8326405 Test Loss: 0.5158989
Validation loss decreased (0.853400 --> 0.832640).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.89194393157959
Epoch: 5, Steps: 14 | Train Loss: 0.7202932 Vali Loss: 0.8171202 Test Loss: 0.5032588
Validation loss decreased (0.832640 --> 0.817120).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2015609741210938
Epoch: 6, Steps: 14 | Train Loss: 0.6931250 Vali Loss: 0.8020349 Test Loss: 0.4928559
Validation loss decreased (0.817120 --> 0.802035).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.9000279903411865
Epoch: 7, Steps: 14 | Train Loss: 0.6718558 Vali Loss: 0.7873747 Test Loss: 0.4847530
Validation loss decreased (0.802035 --> 0.787375).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.7099854946136475
Epoch: 8, Steps: 14 | Train Loss: 0.6525327 Vali Loss: 0.7808961 Test Loss: 0.4781062
Validation loss decreased (0.787375 --> 0.780896).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.517566204071045
Epoch: 9, Steps: 14 | Train Loss: 0.6374571 Vali Loss: 0.7716204 Test Loss: 0.4727405
Validation loss decreased (0.780896 --> 0.771620).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.733454704284668
Epoch: 10, Steps: 14 | Train Loss: 0.6245077 Vali Loss: 0.7666016 Test Loss: 0.4683498
Validation loss decreased (0.771620 --> 0.766602).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.6141114234924316
Epoch: 11, Steps: 14 | Train Loss: 0.6150936 Vali Loss: 0.7601877 Test Loss: 0.4646991
Validation loss decreased (0.766602 --> 0.760188).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.381166458129883
Epoch: 12, Steps: 14 | Train Loss: 0.6057662 Vali Loss: 0.7580972 Test Loss: 0.4616658
Validation loss decreased (0.760188 --> 0.758097).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.266606330871582
Epoch: 13, Steps: 14 | Train Loss: 0.5980767 Vali Loss: 0.7490019 Test Loss: 0.4591146
Validation loss decreased (0.758097 --> 0.749002).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.350123643875122
Epoch: 14, Steps: 14 | Train Loss: 0.5910146 Vali Loss: 0.7433754 Test Loss: 0.4570095
Validation loss decreased (0.749002 --> 0.743375).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.640580654144287
Epoch: 15, Steps: 14 | Train Loss: 0.5852716 Vali Loss: 0.7419365 Test Loss: 0.4551862
Validation loss decreased (0.743375 --> 0.741937).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.0023155212402344
Epoch: 16, Steps: 14 | Train Loss: 0.5799551 Vali Loss: 0.7418151 Test Loss: 0.4536282
Validation loss decreased (0.741937 --> 0.741815).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.2133162021636963
Epoch: 17, Steps: 14 | Train Loss: 0.5755464 Vali Loss: 0.7389683 Test Loss: 0.4522979
Validation loss decreased (0.741815 --> 0.738968).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.3069186210632324
Epoch: 18, Steps: 14 | Train Loss: 0.5709907 Vali Loss: 0.7374008 Test Loss: 0.4511553
Validation loss decreased (0.738968 --> 0.737401).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.2254347801208496
Epoch: 19, Steps: 14 | Train Loss: 0.5672488 Vali Loss: 0.7325118 Test Loss: 0.4501545
Validation loss decreased (0.737401 --> 0.732512).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.076812267303467
Epoch: 20, Steps: 14 | Train Loss: 0.5635860 Vali Loss: 0.7344154 Test Loss: 0.4492742
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.1684608459472656
Epoch: 21, Steps: 14 | Train Loss: 0.5604592 Vali Loss: 0.7295308 Test Loss: 0.4484922
Validation loss decreased (0.732512 --> 0.729531).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.218388080596924
Epoch: 22, Steps: 14 | Train Loss: 0.5569973 Vali Loss: 0.7275213 Test Loss: 0.4477981
Validation loss decreased (0.729531 --> 0.727521).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.1365628242492676
Epoch: 23, Steps: 14 | Train Loss: 0.5562044 Vali Loss: 0.7286829 Test Loss: 0.4472013
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.2029922008514404
Epoch: 24, Steps: 14 | Train Loss: 0.5535915 Vali Loss: 0.7274630 Test Loss: 0.4466701
Validation loss decreased (0.727521 --> 0.727463).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.0027565956115723
Epoch: 25, Steps: 14 | Train Loss: 0.5513399 Vali Loss: 0.7270843 Test Loss: 0.4461820
Validation loss decreased (0.727463 --> 0.727084).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.0996615886688232
Epoch: 26, Steps: 14 | Train Loss: 0.5490282 Vali Loss: 0.7247881 Test Loss: 0.4457356
Validation loss decreased (0.727084 --> 0.724788).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.068464517593384
Epoch: 27, Steps: 14 | Train Loss: 0.5473394 Vali Loss: 0.7214872 Test Loss: 0.4453395
Validation loss decreased (0.724788 --> 0.721487).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.0599722862243652
Epoch: 28, Steps: 14 | Train Loss: 0.5459439 Vali Loss: 0.7250574 Test Loss: 0.4449880
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.1898083686828613
Epoch: 29, Steps: 14 | Train Loss: 0.5428274 Vali Loss: 0.7250006 Test Loss: 0.4446720
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.911524772644043
Epoch: 30, Steps: 14 | Train Loss: 0.5431450 Vali Loss: 0.7218151 Test Loss: 0.4443684
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  128708608.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.1230838298797607
Epoch: 1, Steps: 14 | Train Loss: 0.8686031 Vali Loss: 0.7143114 Test Loss: 0.4404556
Validation loss decreased (inf --> 0.714311).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.2612626552581787
Epoch: 2, Steps: 14 | Train Loss: 0.8579057 Vali Loss: 0.7028889 Test Loss: 0.4372441
Validation loss decreased (0.714311 --> 0.702889).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.171210527420044
Epoch: 3, Steps: 14 | Train Loss: 0.8522743 Vali Loss: 0.6978215 Test Loss: 0.4347526
Validation loss decreased (0.702889 --> 0.697821).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.177741527557373
Epoch: 4, Steps: 14 | Train Loss: 0.8481962 Vali Loss: 0.6953076 Test Loss: 0.4327044
Validation loss decreased (0.697821 --> 0.695308).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.5013973712921143
Epoch: 5, Steps: 14 | Train Loss: 0.8444592 Vali Loss: 0.6888276 Test Loss: 0.4310107
Validation loss decreased (0.695308 --> 0.688828).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.38547682762146
Epoch: 6, Steps: 14 | Train Loss: 0.8410433 Vali Loss: 0.6901198 Test Loss: 0.4295373
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.277714729309082
Epoch: 7, Steps: 14 | Train Loss: 0.8390247 Vali Loss: 0.6893828 Test Loss: 0.4282958
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.291752815246582
Epoch: 8, Steps: 14 | Train Loss: 0.8380243 Vali Loss: 0.6879966 Test Loss: 0.4271990
Validation loss decreased (0.688828 --> 0.687997).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.2845299243927
Epoch: 9, Steps: 14 | Train Loss: 0.8350556 Vali Loss: 0.6839606 Test Loss: 0.4261687
Validation loss decreased (0.687997 --> 0.683961).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2995927333831787
Epoch: 10, Steps: 14 | Train Loss: 0.8337037 Vali Loss: 0.6853858 Test Loss: 0.4253007
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.3014400005340576
Epoch: 11, Steps: 14 | Train Loss: 0.8312092 Vali Loss: 0.6821156 Test Loss: 0.4245742
Validation loss decreased (0.683961 --> 0.682116).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.076427459716797
Epoch: 12, Steps: 14 | Train Loss: 0.8296576 Vali Loss: 0.6819107 Test Loss: 0.4238650
Validation loss decreased (0.682116 --> 0.681911).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.3399648666381836
Epoch: 13, Steps: 14 | Train Loss: 0.8276588 Vali Loss: 0.6807321 Test Loss: 0.4232177
Validation loss decreased (0.681911 --> 0.680732).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.1269619464874268
Epoch: 14, Steps: 14 | Train Loss: 0.8281173 Vali Loss: 0.6774877 Test Loss: 0.4225827
Validation loss decreased (0.680732 --> 0.677488).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.0246500968933105
Epoch: 15, Steps: 14 | Train Loss: 0.8254138 Vali Loss: 0.6756245 Test Loss: 0.4220155
Validation loss decreased (0.677488 --> 0.675624).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.171586036682129
Epoch: 16, Steps: 14 | Train Loss: 0.8247300 Vali Loss: 0.6784447 Test Loss: 0.4214990
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.142343521118164
Epoch: 17, Steps: 14 | Train Loss: 0.8231486 Vali Loss: 0.6758701 Test Loss: 0.4211001
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.82336688041687
Epoch: 18, Steps: 14 | Train Loss: 0.8226580 Vali Loss: 0.6748399 Test Loss: 0.4206796
Validation loss decreased (0.675624 --> 0.674840).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.9551258087158203
Epoch: 19, Steps: 14 | Train Loss: 0.8238411 Vali Loss: 0.6737837 Test Loss: 0.4202610
Validation loss decreased (0.674840 --> 0.673784).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.8579320907592773
Epoch: 20, Steps: 14 | Train Loss: 0.8236773 Vali Loss: 0.6717497 Test Loss: 0.4199173
Validation loss decreased (0.673784 --> 0.671750).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.135817050933838
Epoch: 21, Steps: 14 | Train Loss: 0.8224156 Vali Loss: 0.6705838 Test Loss: 0.4196015
Validation loss decreased (0.671750 --> 0.670584).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.9936628341674805
Epoch: 22, Steps: 14 | Train Loss: 0.8213344 Vali Loss: 0.6728147 Test Loss: 0.4192629
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.8392233848571777
Epoch: 23, Steps: 14 | Train Loss: 0.8206436 Vali Loss: 0.6696565 Test Loss: 0.4190109
Validation loss decreased (0.670584 --> 0.669657).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.4667649269104004
Epoch: 24, Steps: 14 | Train Loss: 0.8201992 Vali Loss: 0.6703606 Test Loss: 0.4187496
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.134552240371704
Epoch: 25, Steps: 14 | Train Loss: 0.8204305 Vali Loss: 0.6718610 Test Loss: 0.4184908
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.1364822387695312
Epoch: 26, Steps: 14 | Train Loss: 0.8188941 Vali Loss: 0.6715595 Test Loss: 0.4182828
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3862072825431824, mae:0.42913752794265747, rse:0.496724933385849, corr:[ 0.21422265  0.22039138  0.22110225  0.21919714  0.21837175  0.21855603
  0.21858898  0.21767658  0.21627025  0.21479274  0.21347462  0.21219131
  0.21097675  0.20980676  0.20861088  0.2073539   0.20608312  0.20487276
  0.20388815  0.20321214  0.20252803  0.20152454  0.20002954  0.1983072
  0.19682887  0.19583435  0.19508828  0.19432057  0.19347355  0.19247137
  0.19146457  0.19052935  0.18978283  0.18900906  0.18808462  0.18692364
  0.185611    0.18455786  0.1839067   0.18342987  0.18284656  0.18194811
  0.18085793  0.18002826  0.17948343  0.17901635  0.17823736  0.17674439
  0.17496826  0.17346737  0.17244506  0.1716857   0.17114635  0.17067525
  0.16993973  0.16908622  0.16831337  0.16772674  0.16732036  0.16700634
  0.1666274   0.16604508  0.1656012   0.16565654  0.16589174  0.16629678
  0.16651584  0.1665241   0.16641094  0.16638505  0.16631305  0.1659677
  0.16531603  0.16462173  0.1642481   0.16403818  0.16394314  0.16365334
  0.16310136  0.1625929   0.1626395   0.16294938  0.16294032  0.16245715
  0.16180556  0.16152263  0.16178256  0.16230488  0.16258606  0.16245095
  0.16202588  0.16170123  0.16193801  0.1623905   0.16272527  0.16264473
  0.16228893  0.16197143  0.16174725  0.16158138  0.16148768  0.16127558
  0.16112728  0.16093494  0.16087061  0.16080305  0.16081257  0.1608041
  0.16050233  0.16000612  0.15953547  0.1592432   0.15905717  0.15901372
  0.15903947  0.15908803  0.1590638   0.1587145   0.15816487  0.15734941
  0.15639769  0.15550454  0.15483263  0.15431932  0.1536852   0.15302311
  0.15246452  0.15216433  0.15212663  0.15190294  0.15139744  0.15038857
  0.14922158  0.14824228  0.14776273  0.1477056   0.14736757  0.14663377
  0.14570211  0.14503244  0.14500894  0.14519879  0.14502253  0.14399225
  0.14218794  0.14062406  0.13982245  0.13955925  0.13920854  0.1385215
  0.13771524  0.13707682  0.13690628  0.13687532  0.13652681  0.13566871
  0.13460128  0.13383588  0.13347605  0.1332313   0.13274777  0.13239431
  0.13246927  0.13303252  0.13392888  0.13444528  0.13420892  0.13310131
  0.13183634  0.13098344  0.13072875  0.13070245  0.13050431  0.12992713
  0.1291283   0.12839082  0.1279461   0.12753977  0.1267642   0.12574297
  0.12486307  0.12444941  0.12451524  0.12466081  0.12462445  0.12453755
  0.12451645  0.1247685   0.12537171  0.12611787  0.12685806  0.12700203
  0.12649937  0.1260233   0.1259626   0.12647478  0.12717408  0.12741198
  0.12721391  0.12671751  0.12637104  0.12626825  0.12633361  0.12596358
  0.1252367   0.12472954  0.12470191  0.1251206   0.12558474  0.12572235
  0.12549724  0.12538816  0.12581787  0.12652238  0.12690032  0.12646706
  0.12553154  0.12459066  0.12405328  0.12410268  0.12417109  0.12424672
  0.12406479  0.12411409  0.12419616  0.1241336   0.12385668  0.12334965
  0.12291892  0.12274119  0.12300994  0.12363084  0.12427506  0.12473955
  0.12503116  0.12531225  0.1258695   0.12660329  0.12731299  0.12770765
  0.12764764  0.12731725  0.12717211  0.12755859  0.12797113  0.12799028
  0.12777784  0.12775052  0.12821881  0.12904741  0.12971653  0.1299917
  0.12992856  0.13009626  0.13067411  0.13168348  0.13248652  0.1329876
  0.13344124  0.1342965   0.13546072  0.13649109  0.13721411  0.13740484
  0.13755569  0.1379188   0.13853118  0.13921355  0.13946518  0.13953885
  0.13966137  0.14000946  0.14057827  0.14116059  0.14156227  0.14167988
  0.14145255  0.14155093  0.14213076  0.14319663  0.1444424   0.14534722
  0.14556588  0.14549205  0.14577095  0.14677283  0.14826193  0.14926097
  0.14926824  0.14872785  0.1483931   0.14866783  0.14910744  0.1491942
  0.14894521  0.14866628  0.14877823  0.14927758  0.14976068  0.14990075
  0.14958668  0.14952667  0.15001528  0.15076184  0.15118013  0.15137377
  0.15141924  0.15197107  0.15298629  0.15396973  0.1544354   0.15410703
  0.15359755  0.15313575  0.15303919  0.15284714  0.1524633   0.1519636
  0.15163182  0.151681    0.15179746  0.15195169  0.15185703  0.15159917
  0.15119913  0.15098497  0.151502    0.15227422  0.15290342  0.15309636
  0.1529593   0.15309483  0.15378258  0.15488113  0.15604505  0.15658453
  0.15619561  0.15554948  0.15528128  0.15549156  0.15603364  0.15635113
  0.15627435  0.155867    0.1555135   0.15559715  0.15579161  0.15614414
  0.15617833  0.15607335  0.15622602  0.1568782   0.15773018  0.15849747
  0.1588726   0.1591796   0.15963039  0.16030109  0.16091388  0.16114224
  0.16087599  0.16034088  0.16002491  0.16011     0.16063541  0.16135266
  0.16199896  0.16240314  0.162439    0.16255951  0.16255157  0.16244584
  0.1621809   0.16199473  0.16207668  0.16243194  0.1629019   0.1631362
  0.16299903  0.16286561  0.16301525  0.16334714  0.16355616  0.16347688
  0.16300249  0.16242102  0.16223791  0.16261156  0.16324916  0.16369642
  0.16383916  0.16389547  0.16407928  0.16446093  0.16484828  0.16497943
  0.16497791  0.16455346  0.16416407  0.16389251  0.16346751  0.16307187
  0.16296414  0.1633395   0.1638625   0.1641448   0.16397491  0.16358
  0.1631125   0.16281205  0.16259979  0.1623155   0.16202642  0.16181909
  0.16173182  0.161804    0.16184278  0.16169216  0.16137104  0.1610727
  0.16046457  0.1595771   0.15851295  0.15758036  0.15678026  0.15589425
  0.1550505   0.15412763  0.15338837  0.15305461  0.15279195  0.15234068
  0.1513116   0.14997359  0.14908743  0.14876509  0.14866498  0.14826639
  0.14730531  0.14625731  0.14529328  0.14496565  0.14481857  0.14448166
  0.14361002  0.1428631   0.14230864  0.1420896   0.14177534  0.14122137
  0.1406114   0.14046389  0.14051299  0.14056478  0.14039384  0.13994214
  0.13950013  0.13914503  0.13894337  0.1389718   0.13883303  0.13835242
  0.13786632  0.13781361  0.13789739  0.13811961  0.13805725  0.1374034
  0.13652124  0.13567661  0.13534655  0.13534585  0.13540977  0.13524328
  0.13493179  0.13457172  0.1343308   0.1344724   0.13469015  0.13459115
  0.13387558  0.13276319  0.13168073  0.13078578  0.13034974  0.13009104
  0.12962556  0.12869187  0.12767355  0.12681922  0.12634958  0.1258582
  0.1251957   0.12442368  0.12387957  0.12351888  0.12295141  0.1223551
  0.12184519  0.12153468  0.12139335  0.12113225  0.12060957  0.11948365
  0.11798918  0.11675543  0.116016    0.11542     0.11463625  0.11381457
  0.11302758  0.11254179  0.11240561  0.11234944  0.11200532  0.11097658
  0.10954558  0.10824446  0.10762392  0.10745592  0.10730142  0.10692198
  0.10609933  0.1051902   0.10456307  0.10430881  0.10389181  0.10275154
  0.10084242  0.09885531  0.09745442  0.09635399  0.09554297  0.09479833
  0.09409759  0.09335358  0.09248618  0.09154045  0.09059749  0.08966354
  0.08868409  0.0875987   0.08650191  0.0853401   0.08400591  0.0829231
  0.0820694   0.08169604  0.08170161  0.08135061  0.0805127   0.0789943
  0.07728295  0.07584903  0.07482895  0.07404209  0.07321884  0.07224226
  0.07108569  0.06999084  0.06956317  0.06959901  0.06987658  0.06974388
  0.06909445  0.06813098  0.06731426  0.06670075  0.06650093  0.06623646
  0.06542976  0.06418902  0.06286412  0.06194241  0.06111191  0.05998743
  0.05824267  0.05643116  0.05494764  0.05392191  0.05327743  0.05263436
  0.05183321  0.05090336  0.05009305  0.04958607  0.04914148  0.04871605
  0.0480073   0.04725935  0.04672195  0.0463631   0.04612953  0.04588578
  0.04542954  0.04503031  0.04469052  0.04420622  0.04315732  0.04165307
  0.04012875  0.03897613  0.03793212  0.03708921  0.03595251  0.03467644
  0.03324644  0.03194574  0.03096016  0.0306409   0.03075085  0.03029897
  0.02909679  0.02761785  0.02669947  0.02668207  0.0271369   0.0277399
  0.02775249  0.02726705  0.02662576  0.02651075  0.02649985  0.02637239
  0.02542671  0.02363752  0.02181954  0.02076436  0.02063625  0.02102201
  0.02109471  0.02056752  0.02005496  0.02011864  0.02034969  0.02031039
  0.01957181  0.01841054  0.0177787   0.01792865  0.01790671  0.01752892
  0.01684588  0.01609491  0.01586535  0.01605972  0.0155481   0.01408437
  0.01226186  0.01125835  0.01136322  0.0116789   0.01155688  0.01048848
  0.00891013  0.00786357  0.00794189  0.00892766  0.00994327  0.01023458
  0.00946531  0.00840545  0.0080793   0.00837546  0.00888285  0.0089899
  0.00866217  0.0082148   0.00782506  0.0079592   0.00828418  0.00830458
  0.00749503  0.00613204  0.00488827  0.00383711  0.00367661  0.00384881
  0.00433187  0.00417353  0.00394173  0.0038359   0.0037797   0.00362909
  0.00357588  0.00306736  0.00275627  0.00255193  0.00226496  0.00198545
  0.0018887   0.00141397  0.00099718  0.00026142 -0.00110369 -0.00333582
 -0.0055761  -0.00680413 -0.00633164 -0.0055519  -0.00548174 -0.00687425
 -0.00843654 -0.00902544 -0.00748856 -0.00467321 -0.00312183 -0.0039347
 -0.00674389 -0.00967022 -0.00957748 -0.00707899 -0.00521621 -0.00679819
 -0.01227612 -0.01800434 -0.019117   -0.01488164 -0.01296656 -0.02278915]
