Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  35629440.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.164318799972534
Epoch: 1, Steps: 59 | Train Loss: 0.8550550 Vali Loss: 0.5226097 Test Loss: 0.3953645
Validation loss decreased (inf --> 0.522610).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.284988403320312
Epoch: 2, Steps: 59 | Train Loss: 0.7102910 Vali Loss: 0.4661221 Test Loss: 0.3747040
Validation loss decreased (0.522610 --> 0.466122).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.821880578994751
Epoch: 3, Steps: 59 | Train Loss: 0.6756354 Vali Loss: 0.4381624 Test Loss: 0.3686550
Validation loss decreased (0.466122 --> 0.438162).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.641271591186523
Epoch: 4, Steps: 59 | Train Loss: 0.6594466 Vali Loss: 0.4259675 Test Loss: 0.3656755
Validation loss decreased (0.438162 --> 0.425968).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.529029846191406
Epoch: 5, Steps: 59 | Train Loss: 0.6508301 Vali Loss: 0.4174989 Test Loss: 0.3637617
Validation loss decreased (0.425968 --> 0.417499).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.948474645614624
Epoch: 6, Steps: 59 | Train Loss: 0.6432020 Vali Loss: 0.4124092 Test Loss: 0.3628269
Validation loss decreased (0.417499 --> 0.412409).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.462031602859497
Epoch: 7, Steps: 59 | Train Loss: 0.6389182 Vali Loss: 0.4075470 Test Loss: 0.3622108
Validation loss decreased (0.412409 --> 0.407547).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 7.718026638031006
Epoch: 8, Steps: 59 | Train Loss: 0.6368037 Vali Loss: 0.4044042 Test Loss: 0.3613988
Validation loss decreased (0.407547 --> 0.404404).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 7.843246698379517
Epoch: 9, Steps: 59 | Train Loss: 0.6335835 Vali Loss: 0.4025218 Test Loss: 0.3608659
Validation loss decreased (0.404404 --> 0.402522).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 7.987527370452881
Epoch: 10, Steps: 59 | Train Loss: 0.6311182 Vali Loss: 0.4013847 Test Loss: 0.3607027
Validation loss decreased (0.402522 --> 0.401385).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.462821006774902
Epoch: 11, Steps: 59 | Train Loss: 0.6297234 Vali Loss: 0.3962263 Test Loss: 0.3602684
Validation loss decreased (0.401385 --> 0.396226).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 7.9668800830841064
Epoch: 12, Steps: 59 | Train Loss: 0.6288027 Vali Loss: 0.3965916 Test Loss: 0.3601384
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 7.981276035308838
Epoch: 13, Steps: 59 | Train Loss: 0.6272803 Vali Loss: 0.3967232 Test Loss: 0.3600143
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 7.994586229324341
Epoch: 14, Steps: 59 | Train Loss: 0.6257259 Vali Loss: 0.3952210 Test Loss: 0.3598258
Validation loss decreased (0.396226 --> 0.395221).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.312514305114746
Epoch: 15, Steps: 59 | Train Loss: 0.6236907 Vali Loss: 0.3943089 Test Loss: 0.3599837
Validation loss decreased (0.395221 --> 0.394309).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.064343929290771
Epoch: 16, Steps: 59 | Train Loss: 0.6233022 Vali Loss: 0.3906426 Test Loss: 0.3596779
Validation loss decreased (0.394309 --> 0.390643).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.512084722518921
Epoch: 17, Steps: 59 | Train Loss: 0.6207870 Vali Loss: 0.3917469 Test Loss: 0.3597150
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.891605138778687
Epoch: 18, Steps: 59 | Train Loss: 0.6236103 Vali Loss: 0.3921944 Test Loss: 0.3596752
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.071487426757812
Epoch: 19, Steps: 59 | Train Loss: 0.6214478 Vali Loss: 0.3899033 Test Loss: 0.3595518
Validation loss decreased (0.390643 --> 0.389903).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.536957263946533
Epoch: 20, Steps: 59 | Train Loss: 0.6200013 Vali Loss: 0.3882526 Test Loss: 0.3595422
Validation loss decreased (0.389903 --> 0.388253).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 7.85555624961853
Epoch: 21, Steps: 59 | Train Loss: 0.6213293 Vali Loss: 0.3887575 Test Loss: 0.3594255
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.296744108200073
Epoch: 22, Steps: 59 | Train Loss: 0.6203684 Vali Loss: 0.3873411 Test Loss: 0.3593754
Validation loss decreased (0.388253 --> 0.387341).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.049669027328491
Epoch: 23, Steps: 59 | Train Loss: 0.6210719 Vali Loss: 0.3869786 Test Loss: 0.3592744
Validation loss decreased (0.387341 --> 0.386979).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 8.825844049453735
Epoch: 24, Steps: 59 | Train Loss: 0.6201193 Vali Loss: 0.3876141 Test Loss: 0.3592996
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.801679611206055
Epoch: 25, Steps: 59 | Train Loss: 0.6195290 Vali Loss: 0.3868602 Test Loss: 0.3592896
Validation loss decreased (0.386979 --> 0.386860).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 8.159104108810425
Epoch: 26, Steps: 59 | Train Loss: 0.6184791 Vali Loss: 0.3855683 Test Loss: 0.3593429
Validation loss decreased (0.386860 --> 0.385568).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.037193775177002
Epoch: 27, Steps: 59 | Train Loss: 0.6186435 Vali Loss: 0.3872643 Test Loss: 0.3593284
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 8.289412498474121
Epoch: 28, Steps: 59 | Train Loss: 0.6189638 Vali Loss: 0.3876514 Test Loss: 0.3592314
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.01601243019104
Epoch: 29, Steps: 59 | Train Loss: 0.6188754 Vali Loss: 0.3853487 Test Loss: 0.3591636
Validation loss decreased (0.385568 --> 0.385349).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 8.21041464805603
Epoch: 30, Steps: 59 | Train Loss: 0.6168589 Vali Loss: 0.3872956 Test Loss: 0.3592184
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 8.355342626571655
Epoch: 31, Steps: 59 | Train Loss: 0.6166000 Vali Loss: 0.3865538 Test Loss: 0.3591042
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 8.546032667160034
Epoch: 32, Steps: 59 | Train Loss: 0.6172973 Vali Loss: 0.3850054 Test Loss: 0.3591437
Validation loss decreased (0.385349 --> 0.385005).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 8.996352195739746
Epoch: 33, Steps: 59 | Train Loss: 0.6153629 Vali Loss: 0.3843718 Test Loss: 0.3591201
Validation loss decreased (0.385005 --> 0.384372).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 8.682157039642334
Epoch: 34, Steps: 59 | Train Loss: 0.6167113 Vali Loss: 0.3831224 Test Loss: 0.3591432
Validation loss decreased (0.384372 --> 0.383122).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 8.384554147720337
Epoch: 35, Steps: 59 | Train Loss: 0.6174585 Vali Loss: 0.3853999 Test Loss: 0.3591174
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.486629486083984
Epoch: 36, Steps: 59 | Train Loss: 0.6172344 Vali Loss: 0.3836102 Test Loss: 0.3590327
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 8.391379833221436
Epoch: 37, Steps: 59 | Train Loss: 0.6181364 Vali Loss: 0.3833114 Test Loss: 0.3590754
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3549186885356903, mae:0.39635610580444336, rse:0.476325660943985, corr:[0.2581113  0.26444018 0.26096302 0.2608888  0.26244128 0.26163152
 0.2596725  0.25950906 0.25960177 0.25816822 0.25610337 0.25517786
 0.25495997 0.25392902 0.25251535 0.2518746  0.25179544 0.2510435
 0.24958242 0.2484244  0.24786092 0.24718931 0.2456363  0.24397074
 0.24269766 0.24169788 0.24037576 0.23913418 0.2384638  0.23799512
 0.23714648 0.23598377 0.23515834 0.23439282 0.23336238 0.23236215
 0.23184328 0.23153262 0.2307127  0.22959445 0.22883728 0.22845851
 0.22790675 0.22694622 0.22580995 0.22492853 0.2237832  0.22200577
 0.22012018 0.2186195  0.21737216 0.21614526 0.21481626 0.21317574
 0.21139513 0.20992963 0.20849904 0.20678717 0.2051393  0.20413242
 0.20390804 0.20391057 0.20376128 0.20346369 0.20306768 0.20285787
 0.20240438 0.20175706 0.2010951  0.20058912 0.19997899 0.19922419
 0.19838382 0.19743337 0.19637728 0.19530013 0.19456469 0.19405918
 0.19354269 0.19288927 0.19230871 0.19159284 0.19091755 0.19054715
 0.19046998 0.19031683 0.18993944 0.1895469  0.18929271 0.18903787
 0.18863218 0.18817993 0.18818289 0.18842651 0.18843324 0.18797153
 0.18733464 0.18677683 0.18627414 0.18549043 0.18474641 0.18439324
 0.18441984 0.18406582 0.18340285 0.18286054 0.1829712  0.1831589
 0.18263361 0.1816876  0.18084505 0.1804249  0.17998987 0.17951544
 0.17900503 0.17858402 0.1781363  0.17731248 0.17638646 0.17547297
 0.17453204 0.17338331 0.17215657 0.17120422 0.1706161  0.17021471
 0.16950041 0.16861255 0.16783224 0.16703993 0.16608028 0.16506498
 0.1645071  0.16427027 0.16396572 0.1633508  0.16268668 0.16223635
 0.16165693 0.16078426 0.15997376 0.1594626  0.15892233 0.15777445
 0.15610228 0.15469514 0.15365458 0.15254426 0.15132584 0.15049964
 0.15016617 0.14974087 0.14893878 0.14783606 0.14697386 0.14645736
 0.14599505 0.14532898 0.144815   0.1446325  0.14429443 0.1437184
 0.1430253  0.1426638  0.14281389 0.14303713 0.14280134 0.14177342
 0.14037709 0.13891096 0.137745   0.13690259 0.13618462 0.13499334
 0.13361885 0.1323141  0.1317051  0.1315162  0.13082619 0.12971376
 0.1286776  0.12815359 0.12799431 0.12786674 0.12739716 0.12710117
 0.1268831  0.12650114 0.1262938  0.12646157 0.12695953 0.12686561
 0.12620918 0.12559642 0.12520814 0.12491608 0.12436014 0.12369356
 0.1232572  0.1228057  0.12220172 0.12150425 0.12111727 0.12096365
 0.12066163 0.12028331 0.12000358 0.12026471 0.12069748 0.12090642
 0.1207208  0.12051246 0.1204366  0.1203469  0.12025762 0.1199079
 0.11904703 0.11759368 0.11611906 0.11554539 0.11560748 0.11570244
 0.11509436 0.11474136 0.11498443 0.11537346 0.11511657 0.11418047
 0.11329141 0.11294368 0.11270612 0.11272984 0.11311323 0.1140409
 0.11477376 0.11486974 0.11466658 0.1148307  0.11522202 0.11510894
 0.11428159 0.11335622 0.11289235 0.11281472 0.11239339 0.11171097
 0.11152003 0.11156881 0.1116735  0.11189961 0.11241034 0.11295647
 0.1128699  0.11296208 0.11368255 0.11528958 0.11626817 0.11660036
 0.11648424 0.11665597 0.11706597 0.11728927 0.11808654 0.11919456
 0.12000009 0.11967359 0.11907745 0.11913992 0.11956526 0.11962871
 0.11907025 0.11870417 0.1191519  0.11934118 0.11919442 0.11859608
 0.11858317 0.11910649 0.11938165 0.11926992 0.11933627 0.1196831
 0.1196317  0.11911251 0.11870514 0.11918511 0.12022921 0.12060374
 0.11996404 0.11916199 0.11869058 0.11829866 0.11748465 0.11659024
 0.11644235 0.11621215 0.1152894  0.11431447 0.11453214 0.11500015
 0.11426422 0.11345411 0.11383596 0.11546105 0.11601196 0.1159774
 0.11584037 0.11657739 0.11712526 0.11702543 0.11700505 0.11742304
 0.11751182 0.11600016 0.11459126 0.11448792 0.1147113  0.11359201
 0.11210288 0.11257365 0.11407389 0.11479584 0.11412038 0.11471916
 0.1164083  0.11715178 0.11656728 0.11724511 0.12002955 0.12163948
 0.12004241 0.11939898 0.12320936 0.12574743 0.12263212 0.11948373]
