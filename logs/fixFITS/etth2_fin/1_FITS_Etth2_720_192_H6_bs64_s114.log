Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.4317946434021
Epoch: 1, Steps: 60 | Train Loss: 0.7046047 Vali Loss: 0.3803487 Test Loss: 0.3885184
Validation loss decreased (inf --> 0.380349).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.713154315948486
Epoch: 2, Steps: 60 | Train Loss: 0.5845649 Vali Loss: 0.3359129 Test Loss: 0.3699719
Validation loss decreased (0.380349 --> 0.335913).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.464699983596802
Epoch: 3, Steps: 60 | Train Loss: 0.5553983 Vali Loss: 0.3199016 Test Loss: 0.3637134
Validation loss decreased (0.335913 --> 0.319902).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.306415557861328
Epoch: 4, Steps: 60 | Train Loss: 0.5456589 Vali Loss: 0.3123785 Test Loss: 0.3605548
Validation loss decreased (0.319902 --> 0.312378).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.593709945678711
Epoch: 5, Steps: 60 | Train Loss: 0.5385112 Vali Loss: 0.3070332 Test Loss: 0.3591176
Validation loss decreased (0.312378 --> 0.307033).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.62236499786377
Epoch: 6, Steps: 60 | Train Loss: 0.5328273 Vali Loss: 0.3037241 Test Loss: 0.3577549
Validation loss decreased (0.307033 --> 0.303724).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.995667934417725
Epoch: 7, Steps: 60 | Train Loss: 0.5293492 Vali Loss: 0.3001906 Test Loss: 0.3572857
Validation loss decreased (0.303724 --> 0.300191).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.05866527557373
Epoch: 8, Steps: 60 | Train Loss: 0.5267319 Vali Loss: 0.2982672 Test Loss: 0.3566965
Validation loss decreased (0.300191 --> 0.298267).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.992203712463379
Epoch: 9, Steps: 60 | Train Loss: 0.5251167 Vali Loss: 0.2965156 Test Loss: 0.3559532
Validation loss decreased (0.298267 --> 0.296516).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.34341311454773
Epoch: 10, Steps: 60 | Train Loss: 0.5229321 Vali Loss: 0.2948030 Test Loss: 0.3557777
Validation loss decreased (0.296516 --> 0.294803).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.90319561958313
Epoch: 11, Steps: 60 | Train Loss: 0.5213029 Vali Loss: 0.2934271 Test Loss: 0.3554657
Validation loss decreased (0.294803 --> 0.293427).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.048702478408813
Epoch: 12, Steps: 60 | Train Loss: 0.5197486 Vali Loss: 0.2925670 Test Loss: 0.3551612
Validation loss decreased (0.293427 --> 0.292567).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.15411925315857
Epoch: 13, Steps: 60 | Train Loss: 0.5190500 Vali Loss: 0.2917311 Test Loss: 0.3548971
Validation loss decreased (0.292567 --> 0.291731).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.1810941696167
Epoch: 14, Steps: 60 | Train Loss: 0.5174379 Vali Loss: 0.2906874 Test Loss: 0.3546338
Validation loss decreased (0.291731 --> 0.290687).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.056048154830933
Epoch: 15, Steps: 60 | Train Loss: 0.5168257 Vali Loss: 0.2901335 Test Loss: 0.3544956
Validation loss decreased (0.290687 --> 0.290134).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.901920557022095
Epoch: 16, Steps: 60 | Train Loss: 0.5162364 Vali Loss: 0.2896960 Test Loss: 0.3542217
Validation loss decreased (0.290134 --> 0.289696).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.785836696624756
Epoch: 17, Steps: 60 | Train Loss: 0.5150001 Vali Loss: 0.2889845 Test Loss: 0.3541382
Validation loss decreased (0.289696 --> 0.288984).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.25098180770874
Epoch: 18, Steps: 60 | Train Loss: 0.5134033 Vali Loss: 0.2881272 Test Loss: 0.3540320
Validation loss decreased (0.288984 --> 0.288127).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.594923257827759
Epoch: 19, Steps: 60 | Train Loss: 0.5141880 Vali Loss: 0.2878633 Test Loss: 0.3539807
Validation loss decreased (0.288127 --> 0.287863).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 6.993008136749268
Epoch: 20, Steps: 60 | Train Loss: 0.5123298 Vali Loss: 0.2874660 Test Loss: 0.3538296
Validation loss decreased (0.287863 --> 0.287466).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 7.021979570388794
Epoch: 21, Steps: 60 | Train Loss: 0.5144803 Vali Loss: 0.2870782 Test Loss: 0.3538219
Validation loss decreased (0.287466 --> 0.287078).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.138773441314697
Epoch: 22, Steps: 60 | Train Loss: 0.5131408 Vali Loss: 0.2869038 Test Loss: 0.3537357
Validation loss decreased (0.287078 --> 0.286904).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.981412410736084
Epoch: 23, Steps: 60 | Train Loss: 0.5132591 Vali Loss: 0.2865734 Test Loss: 0.3535964
Validation loss decreased (0.286904 --> 0.286573).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.396974325180054
Epoch: 24, Steps: 60 | Train Loss: 0.5122915 Vali Loss: 0.2861682 Test Loss: 0.3535380
Validation loss decreased (0.286573 --> 0.286168).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 9.16939377784729
Epoch: 25, Steps: 60 | Train Loss: 0.5128024 Vali Loss: 0.2861933 Test Loss: 0.3533628
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 9.045708656311035
Epoch: 26, Steps: 60 | Train Loss: 0.5126660 Vali Loss: 0.2859366 Test Loss: 0.3533970
Validation loss decreased (0.286168 --> 0.285937).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 9.739317893981934
Epoch: 27, Steps: 60 | Train Loss: 0.5137726 Vali Loss: 0.2858593 Test Loss: 0.3532011
Validation loss decreased (0.285937 --> 0.285859).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.512432336807251
Epoch: 28, Steps: 60 | Train Loss: 0.5125845 Vali Loss: 0.2856110 Test Loss: 0.3531831
Validation loss decreased (0.285859 --> 0.285611).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.094679355621338
Epoch: 29, Steps: 60 | Train Loss: 0.5116049 Vali Loss: 0.2854542 Test Loss: 0.3531253
Validation loss decreased (0.285611 --> 0.285454).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.179798603057861
Epoch: 30, Steps: 60 | Train Loss: 0.5130094 Vali Loss: 0.2851643 Test Loss: 0.3530921
Validation loss decreased (0.285454 --> 0.285164).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 9.410760164260864
Epoch: 31, Steps: 60 | Train Loss: 0.5123642 Vali Loss: 0.2852931 Test Loss: 0.3530093
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.318049430847168
Epoch: 32, Steps: 60 | Train Loss: 0.5115640 Vali Loss: 0.2847915 Test Loss: 0.3530254
Validation loss decreased (0.285164 --> 0.284792).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.00980281829834
Epoch: 33, Steps: 60 | Train Loss: 0.5124440 Vali Loss: 0.2847666 Test Loss: 0.3529996
Validation loss decreased (0.284792 --> 0.284767).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 9.208755016326904
Epoch: 34, Steps: 60 | Train Loss: 0.5119791 Vali Loss: 0.2846685 Test Loss: 0.3529857
Validation loss decreased (0.284767 --> 0.284669).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 6.881765365600586
Epoch: 35, Steps: 60 | Train Loss: 0.5115426 Vali Loss: 0.2844920 Test Loss: 0.3529721
Validation loss decreased (0.284669 --> 0.284492).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.09313416481018
Epoch: 36, Steps: 60 | Train Loss: 0.5118884 Vali Loss: 0.2845202 Test Loss: 0.3529009
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 8.806023359298706
Epoch: 37, Steps: 60 | Train Loss: 0.5098862 Vali Loss: 0.2842081 Test Loss: 0.3528501
Validation loss decreased (0.284492 --> 0.284208).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 8.573348045349121
Epoch: 38, Steps: 60 | Train Loss: 0.5113515 Vali Loss: 0.2840588 Test Loss: 0.3528085
Validation loss decreased (0.284208 --> 0.284059).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 8.73258924484253
Epoch: 39, Steps: 60 | Train Loss: 0.5106672 Vali Loss: 0.2841600 Test Loss: 0.3527961
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 8.877980470657349
Epoch: 40, Steps: 60 | Train Loss: 0.5093739 Vali Loss: 0.2840522 Test Loss: 0.3528317
Validation loss decreased (0.284059 --> 0.284052).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 8.396633625030518
Epoch: 41, Steps: 60 | Train Loss: 0.5095660 Vali Loss: 0.2839926 Test Loss: 0.3527563
Validation loss decreased (0.284052 --> 0.283993).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 9.031481981277466
Epoch: 42, Steps: 60 | Train Loss: 0.5093518 Vali Loss: 0.2838881 Test Loss: 0.3527563
Validation loss decreased (0.283993 --> 0.283888).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 8.34976077079773
Epoch: 43, Steps: 60 | Train Loss: 0.5109124 Vali Loss: 0.2837201 Test Loss: 0.3527599
Validation loss decreased (0.283888 --> 0.283720).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 8.004495859146118
Epoch: 44, Steps: 60 | Train Loss: 0.5108064 Vali Loss: 0.2838144 Test Loss: 0.3526938
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 8.629692316055298
Epoch: 45, Steps: 60 | Train Loss: 0.5097149 Vali Loss: 0.2835883 Test Loss: 0.3527427
Validation loss decreased (0.283720 --> 0.283588).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.19284987449646
Epoch: 46, Steps: 60 | Train Loss: 0.5084719 Vali Loss: 0.2836398 Test Loss: 0.3527257
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 9.155911445617676
Epoch: 47, Steps: 60 | Train Loss: 0.5094214 Vali Loss: 0.2836362 Test Loss: 0.3526962
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 8.414372205734253
Epoch: 48, Steps: 60 | Train Loss: 0.5091734 Vali Loss: 0.2835511 Test Loss: 0.3526871
Validation loss decreased (0.283588 --> 0.283551).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 8.860996007919312
Epoch: 49, Steps: 60 | Train Loss: 0.5083465 Vali Loss: 0.2834151 Test Loss: 0.3526941
Validation loss decreased (0.283551 --> 0.283415).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 9.103734254837036
Epoch: 50, Steps: 60 | Train Loss: 0.5092843 Vali Loss: 0.2834935 Test Loss: 0.3526719
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.0497355408796396e-05
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3314039707183838, mae:0.3745976984500885, rse:0.4616572856903076, corr:[0.2628444  0.26724482 0.2655463  0.26699015 0.2669138  0.26551762
 0.26556897 0.26562452 0.26425782 0.26295644 0.2621283  0.2609508
 0.2595096  0.25828394 0.25761256 0.25722963 0.25683323 0.25628743
 0.255419   0.25414744 0.25280264 0.25177702 0.2505603  0.24867834
 0.24656895 0.24511287 0.24388093 0.24205899 0.24024852 0.23922044
 0.23834188 0.23663266 0.23519608 0.23440626 0.2332628  0.23165374
 0.23073718 0.23038313 0.22926593 0.22793444 0.22755209 0.22734527
 0.22634836 0.2252649  0.22458196 0.22355692 0.22191375 0.2203973
 0.2190473  0.21716863 0.21514386 0.21390799 0.2128831  0.21081996
 0.2084829  0.20710301 0.20552036 0.20332126 0.20209935 0.20161745
 0.20065439 0.19964208 0.1998166  0.20016313 0.19941656 0.1987428
 0.1985741  0.19810028 0.19722076 0.19661273 0.19602929 0.19489881
 0.19368975 0.19305299 0.19212963 0.19032791 0.18924512 0.18914212
 0.18865599 0.1875118  0.18720546 0.18713476 0.18655133 0.18609925
 0.18608625 0.18584716 0.18518403 0.18499567 0.18489578 0.18425065
 0.18348053 0.18331814 0.18337911 0.18266195 0.18218818 0.18248637
 0.18213065 0.18059969 0.1793809  0.17894651 0.1780833  0.176557
 0.17580444 0.17547491 0.17470191 0.17368284 0.17345162 0.17362337
 0.17283493 0.17190929 0.17145433 0.17112905 0.17031567 0.16987589
 0.1696814  0.16919021 0.1685475  0.1676384  0.16643909 0.16486543
 0.16359155 0.16236976 0.16103746 0.15976769 0.15879722 0.15770248
 0.15644439 0.15555415 0.15519723 0.15431693 0.15314786 0.152473
 0.15222025 0.15129103 0.15012708 0.14981385 0.1497152  0.14915487
 0.14843024 0.14787383 0.1472905  0.14671427 0.14631848 0.14512238
 0.14285636 0.14116335 0.1403176  0.13890052 0.13711375 0.13651559
 0.13657402 0.13513084 0.13397405 0.13432993 0.13435957 0.13279484
 0.1320272  0.1327745  0.13306502 0.1325197  0.13213119 0.13242325
 0.13194527 0.1311706  0.13143225 0.13158666 0.13103835 0.13031848
 0.12955588 0.1275278  0.12606211 0.12604682 0.12564531 0.12301205
 0.12132362 0.12103579 0.11951208 0.11757777 0.11732652 0.11800021
 0.11664757 0.11596575 0.11788496 0.11838309 0.1163999  0.11707585
 0.1178885  0.11527753 0.11712451 0.12143404 0.11895879 0.12643865]
