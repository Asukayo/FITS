Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  174211072.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.375866174697876
Epoch: 1, Steps: 15 | Train Loss: 0.8279924 Vali Loss: 0.4997219 Test Loss: 0.4991684
Validation loss decreased (inf --> 0.499722).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.014864444732666
Epoch: 2, Steps: 15 | Train Loss: 0.7072530 Vali Loss: 0.4352961 Test Loss: 0.4522291
Validation loss decreased (0.499722 --> 0.435296).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.160076379776001
Epoch: 3, Steps: 15 | Train Loss: 0.6449294 Vali Loss: 0.3978015 Test Loss: 0.4301309
Validation loss decreased (0.435296 --> 0.397802).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.245210647583008
Epoch: 4, Steps: 15 | Train Loss: 0.6148681 Vali Loss: 0.3776600 Test Loss: 0.4182163
Validation loss decreased (0.397802 --> 0.377660).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.187112808227539
Epoch: 5, Steps: 15 | Train Loss: 0.5970298 Vali Loss: 0.3610479 Test Loss: 0.4110350
Validation loss decreased (0.377660 --> 0.361048).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.220609426498413
Epoch: 6, Steps: 15 | Train Loss: 0.5849104 Vali Loss: 0.3511015 Test Loss: 0.4062708
Validation loss decreased (0.361048 --> 0.351101).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.3600871562957764
Epoch: 7, Steps: 15 | Train Loss: 0.5764269 Vali Loss: 0.3448083 Test Loss: 0.4029117
Validation loss decreased (0.351101 --> 0.344808).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.435407876968384
Epoch: 8, Steps: 15 | Train Loss: 0.5691259 Vali Loss: 0.3385696 Test Loss: 0.4004262
Validation loss decreased (0.344808 --> 0.338570).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.438067674636841
Epoch: 9, Steps: 15 | Train Loss: 0.5651221 Vali Loss: 0.3339506 Test Loss: 0.3985246
Validation loss decreased (0.338570 --> 0.333951).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.3921937942504883
Epoch: 10, Steps: 15 | Train Loss: 0.5605162 Vali Loss: 0.3315042 Test Loss: 0.3969861
Validation loss decreased (0.333951 --> 0.331504).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.5565404891967773
Epoch: 11, Steps: 15 | Train Loss: 0.5562362 Vali Loss: 0.3271196 Test Loss: 0.3958077
Validation loss decreased (0.331504 --> 0.327120).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.415578603744507
Epoch: 12, Steps: 15 | Train Loss: 0.5532748 Vali Loss: 0.3230407 Test Loss: 0.3947345
Validation loss decreased (0.327120 --> 0.323041).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.432713747024536
Epoch: 13, Steps: 15 | Train Loss: 0.5505568 Vali Loss: 0.3233999 Test Loss: 0.3939720
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.3324525356292725
Epoch: 14, Steps: 15 | Train Loss: 0.5499285 Vali Loss: 0.3196246 Test Loss: 0.3932065
Validation loss decreased (0.323041 --> 0.319625).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.4131486415863037
Epoch: 15, Steps: 15 | Train Loss: 0.5479571 Vali Loss: 0.3196880 Test Loss: 0.3926346
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.317303419113159
Epoch: 16, Steps: 15 | Train Loss: 0.5452834 Vali Loss: 0.3171801 Test Loss: 0.3921509
Validation loss decreased (0.319625 --> 0.317180).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.3072683811187744
Epoch: 17, Steps: 15 | Train Loss: 0.5455496 Vali Loss: 0.3170606 Test Loss: 0.3916787
Validation loss decreased (0.317180 --> 0.317061).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.4619810581207275
Epoch: 18, Steps: 15 | Train Loss: 0.5428538 Vali Loss: 0.3147886 Test Loss: 0.3913189
Validation loss decreased (0.317061 --> 0.314789).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.344918966293335
Epoch: 19, Steps: 15 | Train Loss: 0.5427943 Vali Loss: 0.3150995 Test Loss: 0.3909439
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.5495445728302
Epoch: 20, Steps: 15 | Train Loss: 0.5396291 Vali Loss: 0.3120628 Test Loss: 0.3906555
Validation loss decreased (0.314789 --> 0.312063).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.257253408432007
Epoch: 21, Steps: 15 | Train Loss: 0.5404635 Vali Loss: 0.3130503 Test Loss: 0.3904212
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.403634786605835
Epoch: 22, Steps: 15 | Train Loss: 0.5397143 Vali Loss: 0.3119115 Test Loss: 0.3901869
Validation loss decreased (0.312063 --> 0.311911).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.3172519207000732
Epoch: 23, Steps: 15 | Train Loss: 0.5384018 Vali Loss: 0.3113481 Test Loss: 0.3900241
Validation loss decreased (0.311911 --> 0.311348).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.4536402225494385
Epoch: 24, Steps: 15 | Train Loss: 0.5355913 Vali Loss: 0.3109458 Test Loss: 0.3898522
Validation loss decreased (0.311348 --> 0.310946).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.40063214302063
Epoch: 25, Steps: 15 | Train Loss: 0.5361890 Vali Loss: 0.3080537 Test Loss: 0.3896972
Validation loss decreased (0.310946 --> 0.308054).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.303997039794922
Epoch: 26, Steps: 15 | Train Loss: 0.5359680 Vali Loss: 0.3108497 Test Loss: 0.3895563
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.4410674571990967
Epoch: 27, Steps: 15 | Train Loss: 0.5342624 Vali Loss: 0.3082108 Test Loss: 0.3894204
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.3976945877075195
Epoch: 28, Steps: 15 | Train Loss: 0.5355877 Vali Loss: 0.3070802 Test Loss: 0.3893074
Validation loss decreased (0.308054 --> 0.307080).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.295745611190796
Epoch: 29, Steps: 15 | Train Loss: 0.5320611 Vali Loss: 0.3079796 Test Loss: 0.3892159
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.3286995887756348
Epoch: 30, Steps: 15 | Train Loss: 0.5338070 Vali Loss: 0.3060274 Test Loss: 0.3891425
Validation loss decreased (0.307080 --> 0.306027).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.4328489303588867
Epoch: 31, Steps: 15 | Train Loss: 0.5326441 Vali Loss: 0.3067536 Test Loss: 0.3890693
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.6960551738739014
Epoch: 32, Steps: 15 | Train Loss: 0.5333650 Vali Loss: 0.3063042 Test Loss: 0.3889885
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.194939374923706
Epoch: 33, Steps: 15 | Train Loss: 0.5320969 Vali Loss: 0.3055475 Test Loss: 0.3889267
Validation loss decreased (0.306027 --> 0.305548).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.3338780403137207
Epoch: 34, Steps: 15 | Train Loss: 0.5309695 Vali Loss: 0.3056155 Test Loss: 0.3888777
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.34118914604187
Epoch: 35, Steps: 15 | Train Loss: 0.5301609 Vali Loss: 0.3034967 Test Loss: 0.3888072
Validation loss decreased (0.305548 --> 0.303497).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.397956132888794
Epoch: 36, Steps: 15 | Train Loss: 0.5300638 Vali Loss: 0.3045459 Test Loss: 0.3887562
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.583204507827759
Epoch: 37, Steps: 15 | Train Loss: 0.5310923 Vali Loss: 0.3040741 Test Loss: 0.3887096
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.545823812484741
Epoch: 38, Steps: 15 | Train Loss: 0.5281190 Vali Loss: 0.3041666 Test Loss: 0.3886730
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3310144245624542, mae:0.377276211977005, rse:0.46138590574264526, corr:[0.259687   0.2691055  0.26656157 0.26681077 0.26843458 0.2678557
 0.26675692 0.26659563 0.26593825 0.26465678 0.26305974 0.26185033
 0.26119778 0.2598315  0.25801468 0.25708723 0.2569409  0.25608048
 0.25481948 0.25401264 0.25283816 0.25127724 0.2497876  0.24862094
 0.2470467  0.24532583 0.24412203 0.24266285 0.24049862 0.23884995
 0.23841608 0.23724811 0.23558392 0.234587   0.2339935  0.23245573
 0.23075514 0.23047315 0.22997409 0.22833668 0.22720596 0.22723897
 0.22654468 0.22485456 0.22389221 0.22358175 0.22210693 0.21968038
 0.21805684 0.21680875 0.21486591 0.21325225 0.21240476 0.21057245
 0.20797321 0.20679434 0.2060705  0.20407534 0.20233981 0.20194292
 0.2012575  0.1995776  0.19919832 0.2000561  0.19961584 0.19814423
 0.19774124 0.19808537 0.19734772 0.19609493 0.195557   0.19480136
 0.19297576 0.1916661  0.19133897 0.19022246 0.18849005 0.18776867
 0.1876127  0.18654892 0.18560025 0.1855747  0.18546817 0.18460886
 0.18424304 0.18469824 0.18434401 0.18323581 0.18293974 0.18318006
 0.18243785 0.18134098 0.18149814 0.18182218 0.18122146 0.18044056
 0.18003209 0.17927761 0.17817743 0.1775568  0.17714888 0.17618376
 0.17530027 0.17489743 0.17441365 0.17342569 0.17292619 0.17293692
 0.17212528 0.17098516 0.17060132 0.17049827 0.16961628 0.16883509
 0.16874273 0.1683917  0.16729058 0.16627376 0.16584373 0.1646997
 0.16271612 0.16108571 0.16048005 0.15975302 0.15844114 0.1572956
 0.15657537 0.1558285  0.1552529  0.15467954 0.15379135 0.15258946
 0.15185982 0.15120141 0.15029545 0.14983663 0.14984588 0.14929938
 0.1478084  0.14670157 0.14647254 0.14597163 0.14469166 0.14320938
 0.14167488 0.13986544 0.13804062 0.13676313 0.1358711  0.134937
 0.1344015  0.13375272 0.13300364 0.13222337 0.13198237 0.13157639
 0.13065246 0.12999104 0.13042562 0.13070074 0.12955698 0.12926741
 0.13009211 0.12962468 0.12797022 0.12786596 0.12912299 0.12806009
 0.12512815 0.12345006 0.12345764 0.12205621 0.12013253 0.11929733
 0.11886615 0.11686933 0.11530546 0.11549536 0.11451739 0.1130516
 0.11264012 0.11210719 0.11072234 0.11124956 0.11174811 0.10839149
 0.10481787 0.10658139 0.10697244 0.09891967 0.10259593 0.11625624]
