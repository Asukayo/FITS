Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13823040.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5470514
	speed: 0.1177s/iter; left time: 706.1738s
Epoch: 1 cost time: 14.225140810012817
Epoch: 1, Steps: 122 | Train Loss: 0.5383868 Vali Loss: 0.2577069 Test Loss: 0.2859219
Validation loss decreased (inf --> 0.257707).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5075037
	speed: 0.3022s/iter; left time: 1776.5462s
Epoch: 2 cost time: 14.022835493087769
Epoch: 2, Steps: 122 | Train Loss: 0.4469991 Vali Loss: 0.2387512 Test Loss: 0.2792523
Validation loss decreased (0.257707 --> 0.238751).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3567053
	speed: 0.3173s/iter; left time: 1826.6532s
Epoch: 3 cost time: 14.561443567276001
Epoch: 3, Steps: 122 | Train Loss: 0.4307027 Vali Loss: 0.2312126 Test Loss: 0.2768818
Validation loss decreased (0.238751 --> 0.231213).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5398837
	speed: 0.3081s/iter; left time: 1736.3085s
Epoch: 4 cost time: 14.257728815078735
Epoch: 4, Steps: 122 | Train Loss: 0.4206498 Vali Loss: 0.2267881 Test Loss: 0.2760317
Validation loss decreased (0.231213 --> 0.226788).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2476485
	speed: 0.2697s/iter; left time: 1486.8011s
Epoch: 5 cost time: 11.679455518722534
Epoch: 5, Steps: 122 | Train Loss: 0.4171189 Vali Loss: 0.2231680 Test Loss: 0.2749527
Validation loss decreased (0.226788 --> 0.223168).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3995357
	speed: 0.3192s/iter; left time: 1720.5907s
Epoch: 6 cost time: 15.632743120193481
Epoch: 6, Steps: 122 | Train Loss: 0.4134879 Vali Loss: 0.2216045 Test Loss: 0.2740284
Validation loss decreased (0.223168 --> 0.221604).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3849115
	speed: 0.3429s/iter; left time: 1806.8116s
Epoch: 7 cost time: 15.52869987487793
Epoch: 7, Steps: 122 | Train Loss: 0.4125616 Vali Loss: 0.2198133 Test Loss: 0.2733369
Validation loss decreased (0.221604 --> 0.219813).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6223406
	speed: 0.3459s/iter; left time: 1780.2703s
Epoch: 8 cost time: 16.359843730926514
Epoch: 8, Steps: 122 | Train Loss: 0.4107054 Vali Loss: 0.2186997 Test Loss: 0.2729100
Validation loss decreased (0.219813 --> 0.218700).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5111929
	speed: 0.3503s/iter; left time: 1760.1862s
Epoch: 9 cost time: 15.909562349319458
Epoch: 9, Steps: 122 | Train Loss: 0.4093294 Vali Loss: 0.2177925 Test Loss: 0.2730616
Validation loss decreased (0.218700 --> 0.217793).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5467938
	speed: 0.3357s/iter; left time: 1645.9563s
Epoch: 10 cost time: 15.297811508178711
Epoch: 10, Steps: 122 | Train Loss: 0.4079858 Vali Loss: 0.2168622 Test Loss: 0.2727310
Validation loss decreased (0.217793 --> 0.216862).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3294875
	speed: 0.3094s/iter; left time: 1479.2113s
Epoch: 11 cost time: 14.237990856170654
Epoch: 11, Steps: 122 | Train Loss: 0.4069157 Vali Loss: 0.2162932 Test Loss: 0.2723314
Validation loss decreased (0.216862 --> 0.216293).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3746167
	speed: 0.3179s/iter; left time: 1480.8721s
Epoch: 12 cost time: 14.753071784973145
Epoch: 12, Steps: 122 | Train Loss: 0.4066927 Vali Loss: 0.2157642 Test Loss: 0.2725285
Validation loss decreased (0.216293 --> 0.215764).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3292464
	speed: 0.2866s/iter; left time: 1300.3929s
Epoch: 13 cost time: 11.527951955795288
Epoch: 13, Steps: 122 | Train Loss: 0.4058211 Vali Loss: 0.2152178 Test Loss: 0.2721387
Validation loss decreased (0.215764 --> 0.215218).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3282992
	speed: 0.2258s/iter; left time: 997.0904s
Epoch: 14 cost time: 10.290405511856079
Epoch: 14, Steps: 122 | Train Loss: 0.4055432 Vali Loss: 0.2156071 Test Loss: 0.2717049
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3506208
	speed: 0.2737s/iter; left time: 1174.8548s
Epoch: 15 cost time: 12.661044597625732
Epoch: 15, Steps: 122 | Train Loss: 0.4049665 Vali Loss: 0.2155664 Test Loss: 0.2717703
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2497603
	speed: 0.2699s/iter; left time: 1125.7730s
Epoch: 16 cost time: 12.567082166671753
Epoch: 16, Steps: 122 | Train Loss: 0.4039303 Vali Loss: 0.2149420 Test Loss: 0.2717495
Validation loss decreased (0.215218 --> 0.214942).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5609623
	speed: 0.2830s/iter; left time: 1146.0356s
Epoch: 17 cost time: 13.437480449676514
Epoch: 17, Steps: 122 | Train Loss: 0.4042095 Vali Loss: 0.2139019 Test Loss: 0.2717333
Validation loss decreased (0.214942 --> 0.213902).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5049978
	speed: 0.2777s/iter; left time: 1090.5063s
Epoch: 18 cost time: 12.240862369537354
Epoch: 18, Steps: 122 | Train Loss: 0.4034595 Vali Loss: 0.2152890 Test Loss: 0.2716422
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4801152
	speed: 0.2948s/iter; left time: 1121.7143s
Epoch: 19 cost time: 13.875209331512451
Epoch: 19, Steps: 122 | Train Loss: 0.4037835 Vali Loss: 0.2142906 Test Loss: 0.2713664
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2553605
	speed: 0.2849s/iter; left time: 1049.4508s
Epoch: 20 cost time: 13.428627729415894
Epoch: 20, Steps: 122 | Train Loss: 0.4033402 Vali Loss: 0.2143126 Test Loss: 0.2714505
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.27246373891830444, mae:0.3372473418712616, rse:0.42066431045532227, corr:[0.2705969  0.27565846 0.2739913  0.27359772 0.27403495 0.27335048
 0.27213383 0.27148232 0.2710924  0.26997334 0.26829073 0.2665465
 0.26537845 0.26466805 0.26397488 0.2634344  0.26311627 0.26271898
 0.2617259  0.26028904 0.25900784 0.25809342 0.25686112 0.25499007
 0.25292537 0.251198   0.24973866 0.24839215 0.24733298 0.2465669
 0.2454274  0.24348095 0.24117562 0.23958653 0.23879604 0.23803674
 0.23697424 0.23601174 0.23536432 0.2345054  0.23334868 0.2323366
 0.23172681 0.23103926 0.23011927 0.22918919 0.22844616 0.22730175
 0.22543819 0.22324818 0.22154905 0.22020122 0.21922943 0.21817721
 0.21678007 0.21528356 0.21327482 0.21089594 0.20920761 0.2088026
 0.20868747 0.20783074 0.20704946 0.20728871 0.20738752 0.20711774
 0.20661055 0.20642805 0.20636347 0.20580007 0.20459811 0.20352358
 0.20252447 0.20094478 0.19910324 0.19755147 0.19739076 0.1973148
 0.19674373 0.19569765 0.19564854 0.19556864 0.1946643  0.19429149
 0.19478406 0.19482303 0.19374073 0.19332239 0.19364591 0.19357875
 0.19250597 0.1922134  0.19333664 0.19101824 0.18636768 0.19096883]
