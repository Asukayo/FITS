Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20290816.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 7.628988981246948
Epoch: 1, Steps: 60 | Train Loss: 0.7170463 Vali Loss: 0.3830353 Test Loss: 0.3975316
Validation loss decreased (inf --> 0.383035).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.045121192932129
Epoch: 2, Steps: 60 | Train Loss: 0.5901807 Vali Loss: 0.3381181 Test Loss: 0.3746203
Validation loss decreased (0.383035 --> 0.338118).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 6.0169594287872314
Epoch: 3, Steps: 60 | Train Loss: 0.5614665 Vali Loss: 0.3214540 Test Loss: 0.3667433
Validation loss decreased (0.338118 --> 0.321454).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.791114807128906
Epoch: 4, Steps: 60 | Train Loss: 0.5486720 Vali Loss: 0.3127768 Test Loss: 0.3629096
Validation loss decreased (0.321454 --> 0.312777).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 6.1278603076934814
Epoch: 5, Steps: 60 | Train Loss: 0.5406499 Vali Loss: 0.3077451 Test Loss: 0.3608118
Validation loss decreased (0.312777 --> 0.307745).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 7.2575342655181885
Epoch: 6, Steps: 60 | Train Loss: 0.5352027 Vali Loss: 0.3034539 Test Loss: 0.3594156
Validation loss decreased (0.307745 --> 0.303454).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 7.335042476654053
Epoch: 7, Steps: 60 | Train Loss: 0.5324242 Vali Loss: 0.2998640 Test Loss: 0.3587346
Validation loss decreased (0.303454 --> 0.299864).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 6.802170515060425
Epoch: 8, Steps: 60 | Train Loss: 0.5280949 Vali Loss: 0.2979407 Test Loss: 0.3580570
Validation loss decreased (0.299864 --> 0.297941).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 7.298082590103149
Epoch: 9, Steps: 60 | Train Loss: 0.5261908 Vali Loss: 0.2967356 Test Loss: 0.3574413
Validation loss decreased (0.297941 --> 0.296736).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 7.00736665725708
Epoch: 10, Steps: 60 | Train Loss: 0.5236733 Vali Loss: 0.2949209 Test Loss: 0.3570849
Validation loss decreased (0.296736 --> 0.294921).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.9057393074035645
Epoch: 11, Steps: 60 | Train Loss: 0.5241383 Vali Loss: 0.2936459 Test Loss: 0.3567907
Validation loss decreased (0.294921 --> 0.293646).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 7.065304517745972
Epoch: 12, Steps: 60 | Train Loss: 0.5213227 Vali Loss: 0.2929058 Test Loss: 0.3564339
Validation loss decreased (0.293646 --> 0.292906).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 6.466307163238525
Epoch: 13, Steps: 60 | Train Loss: 0.5224838 Vali Loss: 0.2919727 Test Loss: 0.3563277
Validation loss decreased (0.292906 --> 0.291973).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 6.828094244003296
Epoch: 14, Steps: 60 | Train Loss: 0.5209989 Vali Loss: 0.2909401 Test Loss: 0.3561246
Validation loss decreased (0.291973 --> 0.290940).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 6.800525426864624
Epoch: 15, Steps: 60 | Train Loss: 0.5194643 Vali Loss: 0.2902671 Test Loss: 0.3558236
Validation loss decreased (0.290940 --> 0.290267).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 7.109227418899536
Epoch: 16, Steps: 60 | Train Loss: 0.5184551 Vali Loss: 0.2898085 Test Loss: 0.3556789
Validation loss decreased (0.290267 --> 0.289808).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 7.563218593597412
Epoch: 17, Steps: 60 | Train Loss: 0.5192034 Vali Loss: 0.2893532 Test Loss: 0.3554396
Validation loss decreased (0.289808 --> 0.289353).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 7.201543092727661
Epoch: 18, Steps: 60 | Train Loss: 0.5189951 Vali Loss: 0.2888948 Test Loss: 0.3551828
Validation loss decreased (0.289353 --> 0.288895).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 7.7126078605651855
Epoch: 19, Steps: 60 | Train Loss: 0.5182752 Vali Loss: 0.2881970 Test Loss: 0.3552517
Validation loss decreased (0.288895 --> 0.288197).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 7.321519374847412
Epoch: 20, Steps: 60 | Train Loss: 0.5170317 Vali Loss: 0.2880411 Test Loss: 0.3550214
Validation loss decreased (0.288197 --> 0.288041).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 7.051420211791992
Epoch: 21, Steps: 60 | Train Loss: 0.5163485 Vali Loss: 0.2874451 Test Loss: 0.3550293
Validation loss decreased (0.288041 --> 0.287445).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 7.40587306022644
Epoch: 22, Steps: 60 | Train Loss: 0.5157097 Vali Loss: 0.2873270 Test Loss: 0.3549433
Validation loss decreased (0.287445 --> 0.287327).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 7.120105743408203
Epoch: 23, Steps: 60 | Train Loss: 0.5155720 Vali Loss: 0.2869190 Test Loss: 0.3546869
Validation loss decreased (0.287327 --> 0.286919).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 5.457072019577026
Epoch: 24, Steps: 60 | Train Loss: 0.5131974 Vali Loss: 0.2867777 Test Loss: 0.3546688
Validation loss decreased (0.286919 --> 0.286778).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.795740127563477
Epoch: 25, Steps: 60 | Train Loss: 0.5161285 Vali Loss: 0.2864388 Test Loss: 0.3545591
Validation loss decreased (0.286778 --> 0.286439).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 6.618967294692993
Epoch: 26, Steps: 60 | Train Loss: 0.5143997 Vali Loss: 0.2862588 Test Loss: 0.3545614
Validation loss decreased (0.286439 --> 0.286259).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 6.901301145553589
Epoch: 27, Steps: 60 | Train Loss: 0.5154020 Vali Loss: 0.2859859 Test Loss: 0.3544790
Validation loss decreased (0.286259 --> 0.285986).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 5.683408975601196
Epoch: 28, Steps: 60 | Train Loss: 0.5141795 Vali Loss: 0.2858420 Test Loss: 0.3544442
Validation loss decreased (0.285986 --> 0.285842).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.095351934432983
Epoch: 29, Steps: 60 | Train Loss: 0.5147370 Vali Loss: 0.2855192 Test Loss: 0.3543465
Validation loss decreased (0.285842 --> 0.285519).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 7.779431104660034
Epoch: 30, Steps: 60 | Train Loss: 0.5142830 Vali Loss: 0.2855657 Test Loss: 0.3543648
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 7.917496681213379
Epoch: 31, Steps: 60 | Train Loss: 0.5136790 Vali Loss: 0.2851320 Test Loss: 0.3542446
Validation loss decreased (0.285519 --> 0.285132).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 6.958544015884399
Epoch: 32, Steps: 60 | Train Loss: 0.5147708 Vali Loss: 0.2851609 Test Loss: 0.3542537
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 7.291661977767944
Epoch: 33, Steps: 60 | Train Loss: 0.5142019 Vali Loss: 0.2852274 Test Loss: 0.3541971
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 7.198582649230957
Epoch: 34, Steps: 60 | Train Loss: 0.5138597 Vali Loss: 0.2849614 Test Loss: 0.3541891
Validation loss decreased (0.285132 --> 0.284961).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 7.1581690311431885
Epoch: 35, Steps: 60 | Train Loss: 0.5127677 Vali Loss: 0.2846847 Test Loss: 0.3540876
Validation loss decreased (0.284961 --> 0.284685).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 7.32645058631897
Epoch: 36, Steps: 60 | Train Loss: 0.5124188 Vali Loss: 0.2847154 Test Loss: 0.3540932
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 7.097973585128784
Epoch: 37, Steps: 60 | Train Loss: 0.5134074 Vali Loss: 0.2845430 Test Loss: 0.3540614
Validation loss decreased (0.284685 --> 0.284543).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 6.89613938331604
Epoch: 38, Steps: 60 | Train Loss: 0.5135041 Vali Loss: 0.2845968 Test Loss: 0.3540187
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 7.487767696380615
Epoch: 39, Steps: 60 | Train Loss: 0.5137841 Vali Loss: 0.2843813 Test Loss: 0.3540424
Validation loss decreased (0.284543 --> 0.284381).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 6.774446964263916
Epoch: 40, Steps: 60 | Train Loss: 0.5131674 Vali Loss: 0.2843769 Test Loss: 0.3539680
Validation loss decreased (0.284381 --> 0.284377).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 8.317775964736938
Epoch: 41, Steps: 60 | Train Loss: 0.5131801 Vali Loss: 0.2842444 Test Loss: 0.3540311
Validation loss decreased (0.284377 --> 0.284244).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 8.735120058059692
Epoch: 42, Steps: 60 | Train Loss: 0.5135692 Vali Loss: 0.2842702 Test Loss: 0.3539725
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 8.759869813919067
Epoch: 43, Steps: 60 | Train Loss: 0.5121262 Vali Loss: 0.2841415 Test Loss: 0.3539147
Validation loss decreased (0.284244 --> 0.284142).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 8.78648066520691
Epoch: 44, Steps: 60 | Train Loss: 0.5112767 Vali Loss: 0.2841591 Test Loss: 0.3538857
EarlyStopping counter: 1 out of 3
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 8.474675416946411
Epoch: 45, Steps: 60 | Train Loss: 0.5105165 Vali Loss: 0.2840525 Test Loss: 0.3538928
Validation loss decreased (0.284142 --> 0.284053).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 7.74111008644104
Epoch: 46, Steps: 60 | Train Loss: 0.5116690 Vali Loss: 0.2840853 Test Loss: 0.3539002
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 7.390755653381348
Epoch: 47, Steps: 60 | Train Loss: 0.5122386 Vali Loss: 0.2840559 Test Loss: 0.3538700
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 7.291511535644531
Epoch: 48, Steps: 60 | Train Loss: 0.5134516 Vali Loss: 0.2839689 Test Loss: 0.3538588
Validation loss decreased (0.284053 --> 0.283969).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 7.04266095161438
Epoch: 49, Steps: 60 | Train Loss: 0.5110383 Vali Loss: 0.2838893 Test Loss: 0.3538373
Validation loss decreased (0.283969 --> 0.283889).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 6.70511794090271
Epoch: 50, Steps: 60 | Train Loss: 0.5112824 Vali Loss: 0.2838951 Test Loss: 0.3538119
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.0497355408796396e-05
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.33231091499328613, mae:0.37515732645988464, rse:0.46228858828544617, corr:[0.26217192 0.26770058 0.26683098 0.26542723 0.2654781  0.26604185
 0.2657387  0.2645316  0.26343456 0.26239118 0.2612448  0.25992247
 0.25880554 0.25801134 0.25729045 0.25667906 0.256076   0.25528792
 0.25432327 0.25328043 0.2521248  0.25085214 0.24932352 0.24754058
 0.24574696 0.24405399 0.24262546 0.2414122  0.24021749 0.2387739
 0.23716429 0.23555969 0.23447892 0.23366459 0.2327533  0.23131038
 0.2296608  0.2286008  0.22828831 0.22805493 0.22735067 0.22618167
 0.22497167 0.22417659 0.22365779 0.22282383 0.22126979 0.21921146
 0.21732359 0.21596286 0.21480419 0.21324678 0.21140763 0.20958832
 0.20774959 0.20605335 0.20432183 0.20249231 0.201114   0.20034091
 0.20005412 0.19961101 0.19908383 0.19872478 0.19841836 0.1982802
 0.19786713 0.19704111 0.19609639 0.19538487 0.1948435  0.19424604
 0.19320299 0.19178516 0.19038068 0.18920226 0.18851039 0.18793376
 0.18720262 0.18628857 0.1859029  0.1857387  0.18543161 0.18485549
 0.18428785 0.18407427 0.1840397  0.18390782 0.18337934 0.18270792
 0.1821784  0.1818145  0.18184972 0.1817967  0.1813437  0.18055017
 0.1796533  0.17889652 0.1781884  0.17724104 0.1760963  0.17501628
 0.17443295 0.1740038  0.17358592 0.1729655  0.172322   0.17200255
 0.17151159 0.1708266  0.16993806 0.1692904  0.16886672 0.1688312
 0.16873005 0.16811463 0.16707028 0.16571611 0.16454464 0.16336544
 0.16208439 0.16044101 0.15891969 0.15785524 0.15710755 0.15637298
 0.15544805 0.15418294 0.15308592 0.1524164  0.15212533 0.15148103
 0.1504914  0.14947015 0.14888316 0.1488148  0.1486408  0.14798722
 0.1469972  0.14622697 0.14596082 0.14569813 0.14489569 0.14333172
 0.1413536  0.13974601 0.13852867 0.13722293 0.13565843 0.1342961
 0.1337848  0.13348061 0.13324976 0.13257466 0.13156784 0.13068753
 0.13065991 0.13102143 0.13112475 0.13098712 0.13059567 0.13062632
 0.13079755 0.13047111 0.1297349  0.12915345 0.12931336 0.12926956
 0.1283007  0.126201   0.12443178 0.1237075  0.1239358  0.12310972
 0.12073934 0.11760923 0.1160109  0.11673965 0.1172452  0.11631848
 0.11439686 0.1139161  0.11604188 0.11845578 0.11774939 0.11509017
 0.11417803 0.11618834 0.11862513 0.11750501 0.11694738 0.12628731]
