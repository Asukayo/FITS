Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  97574400.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.1618006229400635
Epoch: 1, Steps: 28 | Train Loss: 1.1253050 Vali Loss: 0.8502705 Test Loss: 0.4950237
Validation loss decreased (inf --> 0.850270).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.450561761856079
Epoch: 2, Steps: 28 | Train Loss: 0.9765840 Vali Loss: 0.7921834 Test Loss: 0.4512237
Validation loss decreased (0.850270 --> 0.792183).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.410800218582153
Epoch: 3, Steps: 28 | Train Loss: 0.9152067 Vali Loss: 0.7625731 Test Loss: 0.4333517
Validation loss decreased (0.792183 --> 0.762573).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.288052797317505
Epoch: 4, Steps: 28 | Train Loss: 0.8873215 Vali Loss: 0.7400931 Test Loss: 0.4245535
Validation loss decreased (0.762573 --> 0.740093).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.6259095668792725
Epoch: 5, Steps: 28 | Train Loss: 0.8721371 Vali Loss: 0.7302687 Test Loss: 0.4191159
Validation loss decreased (0.740093 --> 0.730269).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.604407548904419
Epoch: 6, Steps: 28 | Train Loss: 0.8619523 Vali Loss: 0.7181194 Test Loss: 0.4152872
Validation loss decreased (0.730269 --> 0.718119).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.517345666885376
Epoch: 7, Steps: 28 | Train Loss: 0.8569725 Vali Loss: 0.7138789 Test Loss: 0.4123521
Validation loss decreased (0.718119 --> 0.713879).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.170422792434692
Epoch: 8, Steps: 28 | Train Loss: 0.8510345 Vali Loss: 0.7089736 Test Loss: 0.4099966
Validation loss decreased (0.713879 --> 0.708974).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.309295654296875
Epoch: 9, Steps: 28 | Train Loss: 0.8454183 Vali Loss: 0.7048119 Test Loss: 0.4080344
Validation loss decreased (0.708974 --> 0.704812).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.132589101791382
Epoch: 10, Steps: 28 | Train Loss: 0.8419914 Vali Loss: 0.6969725 Test Loss: 0.4063280
Validation loss decreased (0.704812 --> 0.696973).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.220455169677734
Epoch: 11, Steps: 28 | Train Loss: 0.8377156 Vali Loss: 0.6942605 Test Loss: 0.4050093
Validation loss decreased (0.696973 --> 0.694261).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.796907186508179
Epoch: 12, Steps: 28 | Train Loss: 0.8360784 Vali Loss: 0.6929544 Test Loss: 0.4037829
Validation loss decreased (0.694261 --> 0.692954).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.284442186355591
Epoch: 13, Steps: 28 | Train Loss: 0.8341172 Vali Loss: 0.6892843 Test Loss: 0.4027222
Validation loss decreased (0.692954 --> 0.689284).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.0503973960876465
Epoch: 14, Steps: 28 | Train Loss: 0.8322932 Vali Loss: 0.6823072 Test Loss: 0.4017886
Validation loss decreased (0.689284 --> 0.682307).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.433505296707153
Epoch: 15, Steps: 28 | Train Loss: 0.8302805 Vali Loss: 0.6845174 Test Loss: 0.4010226
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.306537389755249
Epoch: 16, Steps: 28 | Train Loss: 0.8292351 Vali Loss: 0.6792275 Test Loss: 0.4001697
Validation loss decreased (0.682307 --> 0.679227).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.934398889541626
Epoch: 17, Steps: 28 | Train Loss: 0.8285833 Vali Loss: 0.6776736 Test Loss: 0.3995193
Validation loss decreased (0.679227 --> 0.677674).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.739287376403809
Epoch: 18, Steps: 28 | Train Loss: 0.8235223 Vali Loss: 0.6780357 Test Loss: 0.3989522
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.7994489669799805
Epoch: 19, Steps: 28 | Train Loss: 0.8243893 Vali Loss: 0.6769513 Test Loss: 0.3984081
Validation loss decreased (0.677674 --> 0.676951).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.080167770385742
Epoch: 20, Steps: 28 | Train Loss: 0.8230224 Vali Loss: 0.6769505 Test Loss: 0.3979312
Validation loss decreased (0.676951 --> 0.676950).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.845749616622925
Epoch: 21, Steps: 28 | Train Loss: 0.8229241 Vali Loss: 0.6737259 Test Loss: 0.3974899
Validation loss decreased (0.676950 --> 0.673726).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.791223764419556
Epoch: 22, Steps: 28 | Train Loss: 0.8208939 Vali Loss: 0.6731499 Test Loss: 0.3971148
Validation loss decreased (0.673726 --> 0.673150).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.81603217124939
Epoch: 23, Steps: 28 | Train Loss: 0.8201314 Vali Loss: 0.6714725 Test Loss: 0.3967592
Validation loss decreased (0.673150 --> 0.671473).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.765192747116089
Epoch: 24, Steps: 28 | Train Loss: 0.8205548 Vali Loss: 0.6718051 Test Loss: 0.3963935
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.665361166000366
Epoch: 25, Steps: 28 | Train Loss: 0.8188638 Vali Loss: 0.6708639 Test Loss: 0.3961168
Validation loss decreased (0.671473 --> 0.670864).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 5.011919736862183
Epoch: 26, Steps: 28 | Train Loss: 0.8196844 Vali Loss: 0.6718241 Test Loss: 0.3958159
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.942521810531616
Epoch: 27, Steps: 28 | Train Loss: 0.8168884 Vali Loss: 0.6682543 Test Loss: 0.3955443
Validation loss decreased (0.670864 --> 0.668254).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.897898197174072
Epoch: 28, Steps: 28 | Train Loss: 0.8183828 Vali Loss: 0.6695056 Test Loss: 0.3953295
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.551799535751343
Epoch: 29, Steps: 28 | Train Loss: 0.8165128 Vali Loss: 0.6682567 Test Loss: 0.3951040
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.711430311203003
Epoch: 30, Steps: 28 | Train Loss: 0.8168367 Vali Loss: 0.6676013 Test Loss: 0.3949164
Validation loss decreased (0.668254 --> 0.667601).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 5.073879957199097
Epoch: 31, Steps: 28 | Train Loss: 0.8167053 Vali Loss: 0.6655129 Test Loss: 0.3947363
Validation loss decreased (0.667601 --> 0.665513).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.073322772979736
Epoch: 32, Steps: 28 | Train Loss: 0.8151697 Vali Loss: 0.6693571 Test Loss: 0.3945877
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 5.193860054016113
Epoch: 33, Steps: 28 | Train Loss: 0.8154959 Vali Loss: 0.6686165 Test Loss: 0.3944221
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.790590047836304
Epoch: 34, Steps: 28 | Train Loss: 0.8155583 Vali Loss: 0.6659517 Test Loss: 0.3942583
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.38216227293014526, mae:0.426904559135437, rse:0.49411681294441223, corr:[ 0.2126277   0.21855424  0.2174387   0.21568145  0.21621186  0.21676023
  0.21567844  0.21428104  0.21297139  0.21187785  0.2106146   0.20931874
  0.20809378  0.20668119  0.20541266  0.20450555  0.20390889  0.20292161
  0.20175876  0.20079151  0.2001909   0.1995274   0.19803526  0.1965886
  0.1954661   0.19480038  0.19391103  0.19304371  0.19234256  0.19176988
  0.1912121   0.19059911  0.19002321  0.1890176   0.18780272  0.18679266
  0.18617883  0.18558687  0.1845491   0.18354267  0.18296197  0.18261005
  0.18210548  0.18138453  0.18049958  0.17991774  0.17903805  0.17751254
  0.175979    0.17496504  0.17419904  0.17349964  0.17287867  0.17222673
  0.17163667  0.1710106   0.1702976   0.16972885  0.16941051  0.16904847
  0.16837703  0.16779983  0.16787435  0.16815583  0.16814683  0.16792852
  0.1679572   0.1684537   0.16879119  0.16851118  0.16778544  0.16726957
  0.16702352  0.16653453  0.16569363  0.16513415  0.16523457  0.1654721
  0.16526398  0.16506426  0.16498035  0.16475046  0.16426226  0.16390947
  0.16400446  0.16417553  0.16403961  0.16366276  0.16366053  0.16398777
  0.16419329  0.16423063  0.16427961  0.16455492  0.16492383  0.16486312
  0.16448641  0.16424322  0.16425867  0.16429473  0.16413695  0.16386265
  0.16380441  0.16386075  0.1636141   0.16304122  0.16271517  0.16270475
  0.16263787  0.16214854  0.16160218  0.16132326  0.1613677   0.16129611
  0.16100687  0.16094814  0.16104619  0.16082706  0.16004388  0.15904014
  0.15826917  0.15792991  0.15732788  0.15633911  0.1556414   0.15575187
  0.15587936  0.15540399  0.15464418  0.15403639  0.15365484  0.15302454
  0.15212236  0.15137956  0.1510225   0.1505492   0.14966358  0.14892247
  0.14853416  0.14846598  0.14829946  0.14807276  0.14776082  0.1470196
  0.1456426   0.14421234  0.14337228  0.14316218  0.14314772  0.1428556
  0.14231534  0.14198004  0.14177983  0.14122312  0.14043006  0.13978493
  0.13939163  0.13897064  0.13838418  0.13791166  0.13781495  0.13796937
  0.1379982   0.13810599  0.13862498  0.13913153  0.13893056  0.13795437
  0.13711067  0.1368075   0.1367031   0.13634779  0.13600595  0.13593934
  0.13562112  0.13467567  0.13345659  0.13261008  0.13218613  0.13184142
  0.13124856  0.13073903  0.13066976  0.13065597  0.13061465  0.13073283
  0.13116151  0.13177967  0.13214856  0.13234913  0.13265203  0.13294487
  0.1329323   0.13266785  0.1325346   0.13282172  0.13330121  0.13351867
  0.13359843  0.13366798  0.13361382  0.13326192  0.13292074  0.13284995
  0.13313372  0.13314232  0.13265456  0.13240367  0.13301659  0.13402535
  0.13444938  0.13440733  0.13444218  0.13473098  0.13466966  0.13403867
  0.13331173  0.13293596  0.13279314  0.13242479  0.13199905  0.13201725
  0.13237128  0.13246359  0.1319632   0.13142411  0.13133425  0.13146244
  0.1314377   0.13134645  0.13160014  0.13206409  0.13233885  0.1325036
  0.13291876  0.13365242  0.13429572  0.13469124  0.1350155   0.13535634
  0.13563909  0.13576058  0.13598903  0.136346    0.13643098  0.13617265
  0.13602403  0.13622473  0.1366316   0.1368364   0.1369526   0.13729636
  0.13797231  0.13868591  0.1392338   0.13978627  0.1404257   0.14112829
  0.14188103  0.14273474  0.14357454  0.14443938  0.14505933  0.14536563
  0.14568174  0.14612429  0.14655265  0.14669429  0.14689921  0.14758588
  0.14839183  0.1485599   0.14813474  0.14791937  0.14841199  0.149264
  0.149806    0.15027624  0.15085617  0.15150115  0.15202343  0.15243249
  0.15294577  0.15365294  0.15422653  0.1547376   0.1554777   0.15629344
  0.15671788  0.15648413  0.15609413  0.15612294  0.1564208   0.15658055
  0.15649256  0.15638985  0.15643221  0.15662412  0.1567861   0.157003
  0.1572896   0.15768051  0.15794158  0.15812889  0.15860268  0.15933663
  0.15978323  0.1599465   0.16025844  0.1609088   0.16148134  0.16131292
  0.16071784  0.16034815  0.1603708   0.1602239   0.15977147  0.15947527
  0.15957038  0.15963088  0.15926088  0.1588331   0.15867859  0.1586869
  0.15851036  0.15839964  0.15890427  0.1596121   0.16006555  0.160165
  0.16034004  0.16102192  0.16171308  0.16208567  0.16226633  0.16235957
  0.16237214  0.16220851  0.16186711  0.16161127  0.16178001  0.16190174
  0.16167708  0.16131791  0.16117314  0.16126606  0.16112685  0.16113596
  0.16140658  0.16194692  0.16244094  0.16274706  0.1631966   0.16401476
  0.16475652  0.16514397  0.1652948   0.16569093  0.16622381  0.16651107
  0.16640495  0.16628316  0.16638057  0.16654398  0.16662885  0.16684118
  0.16748784  0.16815889  0.16816501  0.16775699  0.1673038   0.16732217
  0.16760178  0.16782174  0.16800104  0.16840345  0.16884218  0.16876169
  0.1683496   0.16818711  0.16839047  0.16843648  0.16820909  0.16808853
  0.16819064  0.1681668   0.1678343   0.16757646  0.16788976  0.16861017
  0.16907986  0.16911435  0.16924758  0.16989376  0.17053792  0.17052874
  0.17020229  0.16988702  0.1701842   0.17035975  0.16986711  0.16945055
  0.16981955  0.17037223  0.17023544  0.16977146  0.16967191  0.16997981
  0.16973394  0.16908054  0.16865766  0.16877156  0.1688518   0.16840303
  0.16805105  0.16841957  0.16892345  0.16860896  0.1677939   0.16743961
  0.16741475  0.16696241  0.16588064  0.16494557  0.16454096  0.16400476
  0.16286321  0.16158475  0.16105737  0.16098644  0.16033769  0.15922162
  0.15842462  0.1582815   0.15802917  0.15689583  0.15573965  0.15547322
  0.15560111  0.15523729  0.154291    0.15377623  0.153545    0.15288338
  0.15162323  0.15105654  0.15134762  0.15170947  0.15130271  0.15081586
  0.15092747  0.15120324  0.15052882  0.14966671  0.14983742  0.15063994
  0.15066123  0.14948979  0.14862557  0.14905459  0.14947137  0.14875808
  0.14783978  0.1480298   0.14858739  0.14842343  0.14747772  0.14696912
  0.14744058  0.14755946  0.1467341   0.14584689  0.14597224  0.14644814
  0.14629059  0.14561942  0.1455663   0.14624366  0.14624861  0.14516427
  0.14417948  0.14414166  0.14406767  0.14280567  0.14139962  0.14108872
  0.14138457  0.14075273  0.13935746  0.13849457  0.13865012  0.13838512
  0.13704261  0.13576771  0.1355864   0.13561907  0.13464926  0.13354185
  0.13340425  0.13386674  0.13378555  0.13292333  0.13227053  0.13192149
  0.13099389  0.12926015  0.12767655  0.12713282  0.12711702  0.12655242
  0.12529495  0.1246687   0.12486257  0.12465049  0.12345764  0.12213368
  0.12165149  0.12136436  0.12048414  0.1194262   0.11940528  0.12001529
  0.11958232  0.11812749  0.11718271  0.11731245  0.11703225  0.1153589
  0.11333162  0.11245953  0.11220735  0.11089209  0.10902923  0.10798889
  0.10776979  0.10690966  0.10502883  0.10352225  0.10323068  0.1030017
  0.10155631  0.09956212  0.09858609  0.09843854  0.09774546  0.09650292
  0.09551602  0.09541905  0.09542864  0.09467684  0.09379338  0.09285563
  0.09145302  0.08927244  0.08743571  0.08686594  0.08678452  0.08573399
  0.08383048  0.08256811  0.0828121   0.0829156   0.08204554  0.08097422
  0.08094884  0.08139059  0.08092108  0.07951348  0.07879095  0.07890265
  0.07863405  0.07762493  0.07675388  0.0764821   0.07579926  0.07404479
  0.07195438  0.07079727  0.0700046   0.06865712  0.06717741  0.0665031
  0.0663802   0.06552882  0.06379999  0.06266754  0.06274789  0.06286195
  0.06164383  0.05995397  0.05930435  0.05957104  0.05956199  0.05875442
  0.05804013  0.05827006  0.0584932   0.05780057  0.05644064  0.0554227
  0.0545291   0.05311712  0.05114601  0.04988234  0.0490103   0.04769772
  0.04566821  0.04418258  0.04376     0.04350395  0.0424374   0.04084529
  0.04015342  0.04026869  0.03998976  0.03909719  0.03864603  0.03941531
  0.0401007   0.03994915  0.03949486  0.03972605  0.03987812  0.03932903
  0.03795959  0.03652884  0.03540865  0.03431617  0.03353264  0.0337303
  0.03418183  0.03359811  0.03215775  0.03151591  0.03210603  0.03278742
  0.0319798   0.03032678  0.02998245  0.03097508  0.03106169  0.02993194
  0.02886782  0.02872329  0.02916575  0.02922787  0.0283284   0.02726306
  0.02612688  0.02484917  0.02386153  0.02344127  0.02339914  0.02243414
  0.02081013  0.02017372  0.02085417  0.02140047  0.02073335  0.01988658
  0.01993978  0.02047118  0.02029935  0.01938645  0.01945435  0.02073824
  0.02157454  0.02102038  0.02013002  0.0205841   0.02149616  0.02121027
  0.01981327  0.01882881  0.01843938  0.01741014  0.01658013  0.0165364
  0.01723992  0.01645695  0.0151504   0.01543991  0.01746122  0.01859263
  0.01716777  0.01471644  0.01493897  0.01718617  0.0179788   0.01662362
  0.01582726  0.01663276  0.01765275  0.01666594  0.01473838  0.01334116
  0.01210224  0.00972866  0.00766271  0.00696423  0.00697424  0.00554631
  0.00427417  0.0050096   0.00705792  0.0068282   0.00375052  0.0028853
  0.00564768  0.00669012  0.00310805 -0.00076413  0.0012665   0.0061143
  0.0037512  -0.00600325 -0.01248058 -0.00866246 -0.00637227 -0.02004422]
