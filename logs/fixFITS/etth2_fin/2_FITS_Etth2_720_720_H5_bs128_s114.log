Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  97574400.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.765286445617676
Epoch: 1, Steps: 28 | Train Loss: 0.9179401 Vali Loss: 0.8962865 Test Loss: 0.5306076
Validation loss decreased (inf --> 0.896286).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.03848934173584
Epoch: 2, Steps: 28 | Train Loss: 0.7931076 Vali Loss: 0.8436286 Test Loss: 0.4920491
Validation loss decreased (0.896286 --> 0.843629).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.8638527393341064
Epoch: 3, Steps: 28 | Train Loss: 0.7156070 Vali Loss: 0.8094026 Test Loss: 0.4675768
Validation loss decreased (0.843629 --> 0.809403).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.0395917892456055
Epoch: 4, Steps: 28 | Train Loss: 0.6640824 Vali Loss: 0.7813544 Test Loss: 0.4525339
Validation loss decreased (0.809403 --> 0.781354).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.143759489059448
Epoch: 5, Steps: 28 | Train Loss: 0.6301839 Vali Loss: 0.7677548 Test Loss: 0.4429789
Validation loss decreased (0.781354 --> 0.767755).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.177834510803223
Epoch: 6, Steps: 28 | Train Loss: 0.6068458 Vali Loss: 0.7534677 Test Loss: 0.4367085
Validation loss decreased (0.767755 --> 0.753468).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.5093600749969482
Epoch: 7, Steps: 28 | Train Loss: 0.5906680 Vali Loss: 0.7476488 Test Loss: 0.4324259
Validation loss decreased (0.753468 --> 0.747649).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.763004779815674
Epoch: 8, Steps: 28 | Train Loss: 0.5773441 Vali Loss: 0.7419450 Test Loss: 0.4295088
Validation loss decreased (0.747649 --> 0.741945).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.9612205028533936
Epoch: 9, Steps: 28 | Train Loss: 0.5661896 Vali Loss: 0.7372108 Test Loss: 0.4272912
Validation loss decreased (0.741945 --> 0.737211).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.973836898803711
Epoch: 10, Steps: 28 | Train Loss: 0.5574923 Vali Loss: 0.7290187 Test Loss: 0.4255673
Validation loss decreased (0.737211 --> 0.729019).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.695548057556152
Epoch: 11, Steps: 28 | Train Loss: 0.5496520 Vali Loss: 0.7258413 Test Loss: 0.4242969
Validation loss decreased (0.729019 --> 0.725841).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.813137769699097
Epoch: 12, Steps: 28 | Train Loss: 0.5435032 Vali Loss: 0.7244964 Test Loss: 0.4232136
Validation loss decreased (0.725841 --> 0.724496).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.555067539215088
Epoch: 13, Steps: 28 | Train Loss: 0.5379890 Vali Loss: 0.7203977 Test Loss: 0.4222775
Validation loss decreased (0.724496 --> 0.720398).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.241467237472534
Epoch: 14, Steps: 28 | Train Loss: 0.5331920 Vali Loss: 0.7135842 Test Loss: 0.4214677
Validation loss decreased (0.720398 --> 0.713584).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.221451282501221
Epoch: 15, Steps: 28 | Train Loss: 0.5284908 Vali Loss: 0.7150812 Test Loss: 0.4208002
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.836548089981079
Epoch: 16, Steps: 28 | Train Loss: 0.5247147 Vali Loss: 0.7100373 Test Loss: 0.4201010
Validation loss decreased (0.713584 --> 0.710037).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.750678300857544
Epoch: 17, Steps: 28 | Train Loss: 0.5213068 Vali Loss: 0.7082517 Test Loss: 0.4195086
Validation loss decreased (0.710037 --> 0.708252).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.443280935287476
Epoch: 18, Steps: 28 | Train Loss: 0.5160758 Vali Loss: 0.7083424 Test Loss: 0.4190114
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.8905045986175537
Epoch: 19, Steps: 28 | Train Loss: 0.5139312 Vali Loss: 0.7071285 Test Loss: 0.4185026
Validation loss decreased (0.708252 --> 0.707129).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.27378511428833
Epoch: 20, Steps: 28 | Train Loss: 0.5107848 Vali Loss: 0.7067657 Test Loss: 0.4180271
Validation loss decreased (0.707129 --> 0.706766).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.932097911834717
Epoch: 21, Steps: 28 | Train Loss: 0.5086088 Vali Loss: 0.7033844 Test Loss: 0.4175989
Validation loss decreased (0.706766 --> 0.703384).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.931384325027466
Epoch: 22, Steps: 28 | Train Loss: 0.5054254 Vali Loss: 0.7029458 Test Loss: 0.4172324
Validation loss decreased (0.703384 --> 0.702946).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.3629565238952637
Epoch: 23, Steps: 28 | Train Loss: 0.5032895 Vali Loss: 0.7010546 Test Loss: 0.4168660
Validation loss decreased (0.702946 --> 0.701055).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.5288820266723633
Epoch: 24, Steps: 28 | Train Loss: 0.5015663 Vali Loss: 0.7012735 Test Loss: 0.4164843
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2686824798583984
Epoch: 25, Steps: 28 | Train Loss: 0.4991529 Vali Loss: 0.7002027 Test Loss: 0.4161903
Validation loss decreased (0.701055 --> 0.700203).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.3127996921539307
Epoch: 26, Steps: 28 | Train Loss: 0.4980902 Vali Loss: 0.7010038 Test Loss: 0.4158569
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.4923999309539795
Epoch: 27, Steps: 28 | Train Loss: 0.4952242 Vali Loss: 0.6972333 Test Loss: 0.4155237
Validation loss decreased (0.700203 --> 0.697233).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.5084245204925537
Epoch: 28, Steps: 28 | Train Loss: 0.4945900 Vali Loss: 0.6984035 Test Loss: 0.4152765
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.0051934719085693
Epoch: 29, Steps: 28 | Train Loss: 0.4924198 Vali Loss: 0.6970140 Test Loss: 0.4149902
Validation loss decreased (0.697233 --> 0.697014).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.1899960041046143
Epoch: 30, Steps: 28 | Train Loss: 0.4914509 Vali Loss: 0.6961800 Test Loss: 0.4147530
Validation loss decreased (0.697014 --> 0.696180).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.155665397644043
Epoch: 31, Steps: 28 | Train Loss: 0.4903018 Vali Loss: 0.6940098 Test Loss: 0.4145233
Validation loss decreased (0.696180 --> 0.694010).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.213869094848633
Epoch: 32, Steps: 28 | Train Loss: 0.4885778 Vali Loss: 0.6977202 Test Loss: 0.4143091
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.367802143096924
Epoch: 33, Steps: 28 | Train Loss: 0.4877256 Vali Loss: 0.6966642 Test Loss: 0.4140958
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.3825879096984863
Epoch: 34, Steps: 28 | Train Loss: 0.4867485 Vali Loss: 0.6941359 Test Loss: 0.4138764
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  97574400.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.232412815093994
Epoch: 1, Steps: 28 | Train Loss: 0.8453168 Vali Loss: 0.6862732 Test Loss: 0.4102722
Validation loss decreased (inf --> 0.686273).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.3038809299468994
Epoch: 2, Steps: 28 | Train Loss: 0.8373569 Vali Loss: 0.6796791 Test Loss: 0.4069696
Validation loss decreased (0.686273 --> 0.679679).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.625643014907837
Epoch: 3, Steps: 28 | Train Loss: 0.8295812 Vali Loss: 0.6771610 Test Loss: 0.4044315
Validation loss decreased (0.679679 --> 0.677161).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.487215280532837
Epoch: 4, Steps: 28 | Train Loss: 0.8251718 Vali Loss: 0.6717823 Test Loss: 0.4024954
Validation loss decreased (0.677161 --> 0.671782).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.48187255859375
Epoch: 5, Steps: 28 | Train Loss: 0.8228271 Vali Loss: 0.6697360 Test Loss: 0.4008708
Validation loss decreased (0.671782 --> 0.669736).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.7857799530029297
Epoch: 6, Steps: 28 | Train Loss: 0.8184826 Vali Loss: 0.6638089 Test Loss: 0.3995085
Validation loss decreased (0.669736 --> 0.663809).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.954789876937866
Epoch: 7, Steps: 28 | Train Loss: 0.8163772 Vali Loss: 0.6649357 Test Loss: 0.3983233
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.791263818740845
Epoch: 8, Steps: 28 | Train Loss: 0.8154083 Vali Loss: 0.6607597 Test Loss: 0.3973502
Validation loss decreased (0.663809 --> 0.660760).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.9119110107421875
Epoch: 9, Steps: 28 | Train Loss: 0.8109787 Vali Loss: 0.6612402 Test Loss: 0.3965240
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.926008701324463
Epoch: 10, Steps: 28 | Train Loss: 0.8132014 Vali Loss: 0.6611959 Test Loss: 0.3958810
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.736325025558472
Epoch: 11, Steps: 28 | Train Loss: 0.8097896 Vali Loss: 0.6584672 Test Loss: 0.3951925
Validation loss decreased (0.660760 --> 0.658467).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.957819938659668
Epoch: 12, Steps: 28 | Train Loss: 0.8093468 Vali Loss: 0.6580762 Test Loss: 0.3947335
Validation loss decreased (0.658467 --> 0.658076).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.86053204536438
Epoch: 13, Steps: 28 | Train Loss: 0.8080856 Vali Loss: 0.6600840 Test Loss: 0.3942934
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.667351484298706
Epoch: 14, Steps: 28 | Train Loss: 0.8096199 Vali Loss: 0.6537105 Test Loss: 0.3939120
Validation loss decreased (0.658076 --> 0.653710).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.763138055801392
Epoch: 15, Steps: 28 | Train Loss: 0.8065323 Vali Loss: 0.6536653 Test Loss: 0.3935250
Validation loss decreased (0.653710 --> 0.653665).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.165676832199097
Epoch: 16, Steps: 28 | Train Loss: 0.8062788 Vali Loss: 0.6547798 Test Loss: 0.3933206
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.789469480514526
Epoch: 17, Steps: 28 | Train Loss: 0.8048732 Vali Loss: 0.6543090 Test Loss: 0.3930564
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.667982578277588
Epoch: 18, Steps: 28 | Train Loss: 0.8050473 Vali Loss: 0.6536293 Test Loss: 0.3928140
Validation loss decreased (0.653665 --> 0.653629).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.476278781890869
Epoch: 19, Steps: 28 | Train Loss: 0.8053630 Vali Loss: 0.6503898 Test Loss: 0.3926209
Validation loss decreased (0.653629 --> 0.650390).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.795081853866577
Epoch: 20, Steps: 28 | Train Loss: 0.8048919 Vali Loss: 0.6503165 Test Loss: 0.3924439
Validation loss decreased (0.650390 --> 0.650317).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.886702060699463
Epoch: 21, Steps: 28 | Train Loss: 0.8038865 Vali Loss: 0.6508067 Test Loss: 0.3922698
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.829717397689819
Epoch: 22, Steps: 28 | Train Loss: 0.8030473 Vali Loss: 0.6493886 Test Loss: 0.3921229
Validation loss decreased (0.650317 --> 0.649389).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.923189878463745
Epoch: 23, Steps: 28 | Train Loss: 0.8034829 Vali Loss: 0.6495297 Test Loss: 0.3919893
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.704268455505371
Epoch: 24, Steps: 28 | Train Loss: 0.8011555 Vali Loss: 0.6509171 Test Loss: 0.3919254
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 5.03367280960083
Epoch: 25, Steps: 28 | Train Loss: 0.8031504 Vali Loss: 0.6518097 Test Loss: 0.3918282
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3793308436870575, mae:0.4239439368247986, rse:0.4922829568386078, corr:[ 0.21675344  0.22101267  0.21913934  0.21856228  0.2193702   0.21939088
  0.21817353  0.21706456  0.21652362  0.21562082  0.21418908  0.21257634
  0.21124639  0.21018799  0.20914698  0.20822674  0.20736197  0.20644651
  0.20534219  0.2043652   0.20360605  0.2027721   0.20155126  0.20008264
  0.1988554   0.19791324  0.19692437  0.19575134  0.19487871  0.19435836
  0.19389115  0.19299412  0.19181821  0.19072814  0.18986021  0.18910158
  0.18821622  0.18719354  0.18614838  0.18534076  0.18490662  0.1845394
  0.18385378  0.18300495  0.18236007  0.18195093  0.1813525   0.17988637
  0.17807916  0.17663482  0.17582127  0.17523615  0.17471333  0.1739534
  0.1728041   0.17184824  0.17120467  0.17061178  0.16987272  0.16935627
  0.16924642  0.16918956  0.16920772  0.1694576   0.16946588  0.16961381
  0.16979267  0.16983171  0.1697441   0.16962014  0.16943349  0.16918117
  0.16888131  0.16850841  0.16807571  0.16743803  0.16718866  0.16740543
  0.16758932  0.16728826  0.16692051  0.16665933  0.16648147  0.16645506
  0.16645509  0.1664063   0.16631712  0.16641459  0.16656543  0.16661516
  0.16643935  0.16624239  0.16666874  0.16723534  0.16744927  0.16716605
  0.16685857  0.16687924  0.16695662  0.16680814  0.16660139  0.16635843
  0.16630292  0.16606317  0.16581722  0.16562913  0.16572744  0.16598335
  0.16589479  0.16557135  0.1652157   0.16489634  0.16449103  0.16436054
  0.16450728  0.16461307  0.16437526  0.16374247  0.16328275  0.16284207
  0.16211818  0.1610845   0.16022672  0.15974928  0.15928377  0.15878715
  0.15831031  0.15787633  0.15743156  0.15673976  0.15614137  0.15560111
  0.15516573  0.15448563  0.15374711  0.15341681  0.15312563  0.15257744
  0.1516523   0.15094686  0.15122733  0.15184861  0.15177886  0.15054756
  0.14871101  0.14759047  0.14720805  0.14671521  0.14582875  0.14505492
  0.14474337  0.1445091   0.14415078  0.14354734  0.14283708  0.14208494
  0.14147298  0.14110902  0.1409278   0.14066084  0.14002205  0.13968213
  0.1398631   0.14023726  0.14055559  0.14061998  0.140676    0.14045091
  0.13984261  0.13889168  0.13817562  0.1380442   0.13828735  0.13813724
  0.13731143  0.13622458  0.13550493  0.1350987   0.1343641   0.1334304
  0.1327607   0.13253325  0.13258266  0.13253587  0.13241483  0.13253157
  0.13262774  0.13266481  0.1330112   0.1337222   0.13454148  0.13472791
  0.13442525  0.13448179  0.13488846  0.13522202  0.13520648  0.13501509
  0.13516997  0.13522542  0.1349382   0.134394    0.13417912  0.1341054
  0.13403964  0.1340957   0.1342226   0.13445272  0.13471949  0.13508746
  0.13554     0.1359378   0.13618962  0.1362122   0.13631406  0.13641599
  0.13620327  0.13535681  0.1343181   0.13405016  0.13440248  0.13494936
  0.13478826  0.1344098   0.13415095  0.1342514   0.1345654   0.13469093
  0.13459705  0.13434842  0.13441981  0.1351089   0.13605115  0.1367099
  0.1368968   0.13708995  0.137882    0.13893545  0.13952668  0.13942738
  0.1393216   0.13980669  0.14070646  0.14122874  0.14081219  0.14015031
  0.14028604  0.14102125  0.14176953  0.1420536   0.14233775  0.14306231
  0.14400095  0.14475773  0.14509061  0.14553094  0.1461605   0.14710967
  0.14807203  0.14892554  0.14957762  0.15017676  0.15102412  0.15175825
  0.15230422  0.15256053  0.15272792  0.15295911  0.15309198  0.1534123
  0.15385737  0.1542028   0.15439269  0.1544951   0.15486899  0.15539347
  0.15577517  0.15619752  0.15659794  0.15707707  0.15756485  0.15797687
  0.1583645   0.15895362  0.15957242  0.16006723  0.16052352  0.16090503
  0.1612527   0.16149229  0.16154563  0.16149509  0.16119313  0.16081902
  0.16072819  0.16098951  0.16135429  0.16156313  0.16165525  0.16178812
  0.16196674  0.1623143   0.1625662   0.16265787  0.16276674  0.16332476
  0.16383085  0.16417418  0.16429862  0.16463739  0.16527858  0.16559586
  0.1653096   0.16456594  0.16427901  0.16439298  0.16441733  0.16393787
  0.16319844  0.16284429  0.16293097  0.163404    0.16363472  0.16348657
  0.16306429  0.16291349  0.16345851  0.16405694  0.16429922  0.16424455
  0.16430415  0.16504942  0.16604085  0.16676624  0.16724308  0.16752537
  0.16764814  0.16771564  0.16766688  0.16734678  0.16729032  0.16753922
  0.16792367  0.16799715  0.16786392  0.16776933  0.16755445  0.16761824
  0.16791211  0.1685116   0.16921629  0.16983674  0.17040043  0.17127821
  0.17211723  0.17256317  0.17245641  0.172516    0.17327693  0.17430277
  0.1747765   0.17441753  0.17391077  0.17384265  0.17431995  0.17480761
  0.17513847  0.1755071   0.17585811  0.17631407  0.17637685  0.17610195
  0.17571975  0.17564297  0.17595482  0.17641859  0.1767435   0.17666581
  0.17628942  0.17607746  0.17616399  0.17630917  0.17635056  0.17639019
  0.17637567  0.17632933  0.1763838   0.17654903  0.17678894  0.17711431
  0.17755178  0.17804554  0.1783188   0.17830265  0.17828514  0.17839213
  0.17862755  0.17839046  0.17801702  0.17781803  0.17759599  0.17727956
  0.17699517  0.17705256  0.17733057  0.17758115  0.17752811  0.17723608
  0.17682494  0.17651403  0.17622574  0.17589821  0.17579757  0.17586724
  0.17584783  0.17569311  0.17550774  0.17540951  0.17521928  0.17484675
  0.17418547  0.17368998  0.17337325  0.17276615  0.17149885  0.1699814
  0.16906092  0.16856052  0.1680178   0.1673063   0.16668211  0.16644987
  0.16602628  0.16513889  0.16434585  0.1637726   0.16330516  0.16263562
  0.16185537  0.16146953  0.16118759  0.1608085   0.15988332  0.15890762
  0.15815306  0.15788513  0.15761055  0.15743148  0.15739588  0.15739529
  0.1570958   0.15649915  0.15569435  0.15532915  0.15559642  0.15607122
  0.15630537  0.15596533  0.15523095  0.15472594  0.15452702  0.15438896
  0.15413032  0.15378186  0.15325858  0.15313394  0.15314412  0.15258084
  0.15170515  0.15102215  0.151078    0.15129639  0.1509977   0.15023974
  0.14979342  0.14985234  0.14996594  0.14996892  0.14968708  0.14910309
  0.14821744  0.14741193  0.1470156   0.14657053  0.14588295  0.14483209
  0.14382079  0.14311223  0.1426773   0.14193231  0.1409247   0.1400049
  0.1395131   0.13920517  0.1385549   0.13746884  0.13639131  0.1360864
  0.1362296   0.13607489  0.13556457  0.13502342  0.13464616  0.13400611
  0.13300031  0.13192448  0.130866    0.12981836  0.12909073  0.12881632
  0.12826996  0.12733632  0.12644629  0.12605552  0.12592012  0.12513466
  0.12372828  0.12250108  0.12213679  0.12203     0.12158403  0.12097086
  0.12044928  0.12020811  0.11982976  0.11908724  0.11803957  0.1170278
  0.11601237  0.11492258  0.11369687  0.11210586  0.11068147  0.10956167
  0.10875293  0.10804147  0.1072214   0.10621016  0.10512044  0.10419711
  0.1034392   0.10253061  0.10132881  0.09997316  0.0988095   0.09826068
  0.09768297  0.09695099  0.0962216   0.09547397  0.09490971  0.09395828
  0.09256112  0.09090819  0.08953366  0.08869284  0.08817592  0.08746809
  0.08633237  0.08512332  0.08480155  0.0848335   0.08456132  0.08353788
  0.08254991  0.08230305  0.0824275   0.08179794  0.08066878  0.07980946
  0.07961141  0.07958244  0.07883346  0.07747253  0.07602558  0.07485513
  0.07351346  0.07208771  0.0707695   0.06987818  0.06932113  0.06853618
  0.067385    0.06616323  0.06512074  0.06435547  0.06370185  0.06324415
  0.06262866  0.06172484  0.0607589   0.06023979  0.06052164  0.06083344
  0.06015844  0.05894523  0.05808232  0.05786885  0.0574154   0.05626376
  0.05473205  0.053395    0.05194382  0.05060258  0.04931442  0.04849049
  0.0476164   0.04637565  0.04504013  0.04440568  0.04431232  0.04348368
  0.04194668  0.04087555  0.0411306   0.04190812  0.04187537  0.04131091
  0.04090896  0.04121266  0.04142556  0.04104922  0.04007627  0.03965244
  0.03955839  0.03898665  0.03772295  0.03630195  0.03537752  0.03521284
  0.03523695  0.03496299  0.03443485  0.03394634  0.03356106  0.0336651
  0.03372002  0.03295941  0.03181702  0.03121214  0.03113701  0.03139746
  0.03104778  0.02985701  0.02912431  0.02950368  0.02961457  0.02867446
  0.02702068  0.02575733  0.02517218  0.02464022  0.02428535  0.02387472
  0.02319701  0.02232986  0.02177501  0.0220931   0.02277179  0.02268898
  0.02148375  0.02065794  0.02140759  0.02240166  0.0223078   0.02140513
  0.02125246  0.02217729  0.02255491  0.02186705  0.02084131  0.02054922
  0.02068169  0.02045389  0.01957747  0.01815785  0.01733367  0.01681914
  0.01690666  0.01675232  0.01680749  0.01677112  0.01654042  0.01650624
  0.01674142  0.01601457  0.01499321  0.01475227  0.01570543  0.01671171
  0.01624229  0.01409696  0.0129059   0.0132616   0.01336003  0.01122193
  0.00797631  0.00601704  0.00637679  0.00672084  0.0057333   0.00369161
  0.00278748  0.00332672  0.00465525  0.00524973  0.00380787  0.00193945
  0.00166649  0.00257908  0.00296428  0.00046613 -0.00211324 -0.00070906
  0.00158176 -0.00214275 -0.01140563 -0.01470289 -0.00530786 -0.00097422]
