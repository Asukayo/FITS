Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  23532544.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.313123226165771
Epoch: 1, Steps: 59 | Train Loss: 0.6637788 Vali Loss: 0.5869524 Test Loss: 0.4391463
Validation loss decreased (inf --> 0.586952).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.453794002532959
Epoch: 2, Steps: 59 | Train Loss: 0.5251640 Vali Loss: 0.5251566 Test Loss: 0.4057986
Validation loss decreased (0.586952 --> 0.525157).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.886727333068848
Epoch: 3, Steps: 59 | Train Loss: 0.4539626 Vali Loss: 0.4905111 Test Loss: 0.3916437
Validation loss decreased (0.525157 --> 0.490511).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.876513481140137
Epoch: 4, Steps: 59 | Train Loss: 0.4129450 Vali Loss: 0.4713795 Test Loss: 0.3854727
Validation loss decreased (0.490511 --> 0.471379).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.488197803497314
Epoch: 5, Steps: 59 | Train Loss: 0.3844891 Vali Loss: 0.4595993 Test Loss: 0.3821991
Validation loss decreased (0.471379 --> 0.459599).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.19137692451477
Epoch: 6, Steps: 59 | Train Loss: 0.3636749 Vali Loss: 0.4493845 Test Loss: 0.3804262
Validation loss decreased (0.459599 --> 0.449385).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.128405809402466
Epoch: 7, Steps: 59 | Train Loss: 0.3475990 Vali Loss: 0.4429332 Test Loss: 0.3790834
Validation loss decreased (0.449385 --> 0.442933).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.119882345199585
Epoch: 8, Steps: 59 | Train Loss: 0.3342542 Vali Loss: 0.4374870 Test Loss: 0.3780191
Validation loss decreased (0.442933 --> 0.437487).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.152534484863281
Epoch: 9, Steps: 59 | Train Loss: 0.3231051 Vali Loss: 0.4345393 Test Loss: 0.3769992
Validation loss decreased (0.437487 --> 0.434539).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.24035906791687
Epoch: 10, Steps: 59 | Train Loss: 0.3127735 Vali Loss: 0.4299534 Test Loss: 0.3760161
Validation loss decreased (0.434539 --> 0.429953).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.992850303649902
Epoch: 11, Steps: 59 | Train Loss: 0.3038509 Vali Loss: 0.4268083 Test Loss: 0.3752873
Validation loss decreased (0.429953 --> 0.426808).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.312052488327026
Epoch: 12, Steps: 59 | Train Loss: 0.2969737 Vali Loss: 0.4216304 Test Loss: 0.3745143
Validation loss decreased (0.426808 --> 0.421630).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 10.224072933197021
Epoch: 13, Steps: 59 | Train Loss: 0.2909015 Vali Loss: 0.4211980 Test Loss: 0.3737658
Validation loss decreased (0.421630 --> 0.421198).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.250980138778687
Epoch: 14, Steps: 59 | Train Loss: 0.2849495 Vali Loss: 0.4198799 Test Loss: 0.3730928
Validation loss decreased (0.421198 --> 0.419880).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.379335880279541
Epoch: 15, Steps: 59 | Train Loss: 0.2795457 Vali Loss: 0.4169032 Test Loss: 0.3724313
Validation loss decreased (0.419880 --> 0.416903).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.119582891464233
Epoch: 16, Steps: 59 | Train Loss: 0.2751740 Vali Loss: 0.4156577 Test Loss: 0.3719710
Validation loss decreased (0.416903 --> 0.415658).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 7.941707372665405
Epoch: 17, Steps: 59 | Train Loss: 0.2712921 Vali Loss: 0.4152257 Test Loss: 0.3712588
Validation loss decreased (0.415658 --> 0.415226).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.025522708892822
Epoch: 18, Steps: 59 | Train Loss: 0.2675843 Vali Loss: 0.4147332 Test Loss: 0.3707502
Validation loss decreased (0.415226 --> 0.414733).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 8.18483281135559
Epoch: 19, Steps: 59 | Train Loss: 0.2642106 Vali Loss: 0.4119645 Test Loss: 0.3703779
Validation loss decreased (0.414733 --> 0.411965).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.035154104232788
Epoch: 20, Steps: 59 | Train Loss: 0.2610483 Vali Loss: 0.4126431 Test Loss: 0.3699647
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 11.20293641090393
Epoch: 21, Steps: 59 | Train Loss: 0.2586616 Vali Loss: 0.4113908 Test Loss: 0.3695756
Validation loss decreased (0.411965 --> 0.411391).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 10.73833441734314
Epoch: 22, Steps: 59 | Train Loss: 0.2563003 Vali Loss: 0.4105268 Test Loss: 0.3691472
Validation loss decreased (0.411391 --> 0.410527).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 10.808736085891724
Epoch: 23, Steps: 59 | Train Loss: 0.2537887 Vali Loss: 0.4075521 Test Loss: 0.3688536
Validation loss decreased (0.410527 --> 0.407552).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 10.267845630645752
Epoch: 24, Steps: 59 | Train Loss: 0.2520669 Vali Loss: 0.4079719 Test Loss: 0.3686209
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 10.030937671661377
Epoch: 25, Steps: 59 | Train Loss: 0.2503067 Vali Loss: 0.4080715 Test Loss: 0.3683110
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.211349248886108
Epoch: 26, Steps: 59 | Train Loss: 0.2483017 Vali Loss: 0.4055978 Test Loss: 0.3680735
Validation loss decreased (0.407552 --> 0.405598).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.300340414047241
Epoch: 27, Steps: 59 | Train Loss: 0.2474731 Vali Loss: 0.4043216 Test Loss: 0.3678738
Validation loss decreased (0.405598 --> 0.404322).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 10.281534433364868
Epoch: 28, Steps: 59 | Train Loss: 0.2452317 Vali Loss: 0.4051478 Test Loss: 0.3676968
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.958247184753418
Epoch: 29, Steps: 59 | Train Loss: 0.2442631 Vali Loss: 0.4034386 Test Loss: 0.3674722
Validation loss decreased (0.404322 --> 0.403439).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.118172407150269
Epoch: 30, Steps: 59 | Train Loss: 0.2434877 Vali Loss: 0.4048409 Test Loss: 0.3673744
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 7.2588536739349365
Epoch: 31, Steps: 59 | Train Loss: 0.2423219 Vali Loss: 0.4012490 Test Loss: 0.3671788
Validation loss decreased (0.403439 --> 0.401249).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.390897274017334
Epoch: 32, Steps: 59 | Train Loss: 0.2414420 Vali Loss: 0.4032336 Test Loss: 0.3670383
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 10.44594931602478
Epoch: 33, Steps: 59 | Train Loss: 0.2401940 Vali Loss: 0.4011878 Test Loss: 0.3669108
Validation loss decreased (0.401249 --> 0.401188).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 10.182655572891235
Epoch: 34, Steps: 59 | Train Loss: 0.2391250 Vali Loss: 0.4005425 Test Loss: 0.3667728
Validation loss decreased (0.401188 --> 0.400543).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 10.118523120880127
Epoch: 35, Steps: 59 | Train Loss: 0.2381572 Vali Loss: 0.4004667 Test Loss: 0.3667018
Validation loss decreased (0.400543 --> 0.400467).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 9.925880432128906
Epoch: 36, Steps: 59 | Train Loss: 0.2371176 Vali Loss: 0.4010232 Test Loss: 0.3666263
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 10.504289150238037
Epoch: 37, Steps: 59 | Train Loss: 0.2367596 Vali Loss: 0.4002416 Test Loss: 0.3665248
Validation loss decreased (0.400467 --> 0.400242).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 9.785837411880493
Epoch: 38, Steps: 59 | Train Loss: 0.2366258 Vali Loss: 0.3991085 Test Loss: 0.3663962
Validation loss decreased (0.400242 --> 0.399108).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 10.216430187225342
Epoch: 39, Steps: 59 | Train Loss: 0.2356809 Vali Loss: 0.4003443 Test Loss: 0.3663132
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 10.244234800338745
Epoch: 40, Steps: 59 | Train Loss: 0.2350930 Vali Loss: 0.3976766 Test Loss: 0.3662863
Validation loss decreased (0.399108 --> 0.397677).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 11.333207130432129
Epoch: 41, Steps: 59 | Train Loss: 0.2345676 Vali Loss: 0.3975629 Test Loss: 0.3662546
Validation loss decreased (0.397677 --> 0.397563).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 9.90966010093689
Epoch: 42, Steps: 59 | Train Loss: 0.2344151 Vali Loss: 0.3979576 Test Loss: 0.3661603
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 10.029284477233887
Epoch: 43, Steps: 59 | Train Loss: 0.2341923 Vali Loss: 0.3995857 Test Loss: 0.3661377
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 9.62352204322815
Epoch: 44, Steps: 59 | Train Loss: 0.2331018 Vali Loss: 0.3965112 Test Loss: 0.3660949
Validation loss decreased (0.397563 --> 0.396511).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 10.168235778808594
Epoch: 45, Steps: 59 | Train Loss: 0.2333687 Vali Loss: 0.3954593 Test Loss: 0.3660146
Validation loss decreased (0.396511 --> 0.395459).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.939237356185913
Epoch: 46, Steps: 59 | Train Loss: 0.2328587 Vali Loss: 0.4015066 Test Loss: 0.3659759
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 10.076564073562622
Epoch: 47, Steps: 59 | Train Loss: 0.2323359 Vali Loss: 0.3984979 Test Loss: 0.3659723
EarlyStopping counter: 2 out of 3
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 9.331777572631836
Epoch: 48, Steps: 59 | Train Loss: 0.2320316 Vali Loss: 0.3969662 Test Loss: 0.3659105
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  23532544.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.819004535675049
Epoch: 1, Steps: 59 | Train Loss: 0.6269813 Vali Loss: 0.3889632 Test Loss: 0.3630338
Validation loss decreased (inf --> 0.388963).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 8.400125980377197
Epoch: 2, Steps: 59 | Train Loss: 0.6203825 Vali Loss: 0.3836224 Test Loss: 0.3610971
Validation loss decreased (0.388963 --> 0.383622).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 7.829329013824463
Epoch: 3, Steps: 59 | Train Loss: 0.6171989 Vali Loss: 0.3837725 Test Loss: 0.3606468
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.780843734741211
Epoch: 4, Steps: 59 | Train Loss: 0.6153412 Vali Loss: 0.3802176 Test Loss: 0.3602577
Validation loss decreased (0.383622 --> 0.380218).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.615124464035034
Epoch: 5, Steps: 59 | Train Loss: 0.6153407 Vali Loss: 0.3810538 Test Loss: 0.3597316
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 10.175180435180664
Epoch: 6, Steps: 59 | Train Loss: 0.6153202 Vali Loss: 0.3771478 Test Loss: 0.3596646
Validation loss decreased (0.380218 --> 0.377148).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.029068231582642
Epoch: 7, Steps: 59 | Train Loss: 0.6143907 Vali Loss: 0.3788713 Test Loss: 0.3597757
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.029747486114502
Epoch: 8, Steps: 59 | Train Loss: 0.6132767 Vali Loss: 0.3778596 Test Loss: 0.3595661
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 7.805305004119873
Epoch: 9, Steps: 59 | Train Loss: 0.6138682 Vali Loss: 0.3778875 Test Loss: 0.3597131
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.35536783933639526, mae:0.39636802673339844, rse:0.4766269326210022, corr:[0.26273346 0.2661383  0.2651386  0.2631064  0.2626125  0.2628258
 0.26237687 0.2610596  0.25979885 0.25892594 0.2580212  0.2565794
 0.25506255 0.25384876 0.2532616  0.25300965 0.25252253 0.25149098
 0.25036743 0.2495721  0.24889667 0.24786739 0.24612056 0.24418232
 0.24265988 0.24171743 0.24081342 0.23965086 0.23846385 0.2376365
 0.23714595 0.23653316 0.23550259 0.23418985 0.23302157 0.23227434
 0.23177612 0.23127516 0.23062152 0.22990194 0.22928517 0.22862643
 0.227601   0.22621974 0.224781   0.2236134  0.22258337 0.22134751
 0.21979737 0.21810037 0.21644282 0.21497953 0.21382633 0.21255496
 0.2106764  0.20854886 0.20658313 0.20498443 0.20394795 0.20333856
 0.2028973  0.20241664 0.20203689 0.20192021 0.20166758 0.20139387
 0.20085754 0.20025228 0.19962087 0.19909565 0.19841555 0.19759877
 0.19666246 0.19557199 0.19422565 0.19257505 0.1911555  0.1902818
 0.19002664 0.18997641 0.18991454 0.18939634 0.18855734 0.18790852
 0.18775254 0.18777622 0.18748689 0.18683843 0.18614475 0.18583138
 0.1859483  0.18602626 0.18612638 0.18622194 0.1863302  0.18624718
 0.18565032 0.18443279 0.18312898 0.18218854 0.18193491 0.18196543
 0.18189372 0.18120433 0.18038014 0.17979571 0.17973794 0.17980844
 0.17944857 0.17893025 0.17846616 0.17824508 0.17780225 0.17719047
 0.1765547  0.17623575 0.17631178 0.17600325 0.17491388 0.17302984
 0.17120168 0.17022295 0.1701131  0.17003927 0.16918062 0.16774634
 0.16637501 0.16568989 0.16538818 0.1646311  0.16319467 0.1614957
 0.16052422 0.16049694 0.16092351 0.1610233  0.16044292 0.15960096
 0.15894999 0.15860245 0.15827762 0.15753819 0.15635167 0.1548453
 0.15314251 0.15144661 0.14969528 0.14800861 0.14659615 0.14569509
 0.14521623 0.14490208 0.14494908 0.14515027 0.14515592 0.14463429
 0.14383918 0.14326361 0.1433618  0.14398849 0.14420809 0.14361867
 0.14224418 0.14082192 0.14017282 0.14030786 0.14054054 0.13993211
 0.13853925 0.13679688 0.13535514 0.13429384 0.13338472 0.13214189
 0.13090675 0.12969938 0.1288353  0.12817433 0.12729163 0.12646523
 0.1259838  0.1259358  0.12596823 0.12572108 0.12489    0.12426417
 0.12425822 0.12456276 0.12474477 0.12450925 0.12439807 0.12444997
 0.12475067 0.1249386  0.12443101 0.12353224 0.1227754  0.12262349
 0.12306634 0.12336054 0.12309364 0.12218801 0.1211674  0.12047809
 0.12016682 0.12015764 0.12000671 0.11990152 0.11998079 0.12049638
 0.121264   0.12215149 0.12285354 0.12296242 0.12238814 0.12118139
 0.11989761 0.1190196  0.11860495 0.11844613 0.11783954 0.11706051
 0.11627478 0.11636881 0.11706202 0.11754208 0.11699647 0.11535012
 0.11353915 0.11281289 0.1133159  0.11455864 0.11527959 0.11533625
 0.11521738 0.1157317  0.11685887 0.11795779 0.11835187 0.11793727
 0.11717773 0.11656909 0.11618482 0.11593405 0.1153957  0.11483225
 0.11514389 0.11604379 0.11668482 0.11611845 0.11454499 0.1132657
 0.11319605 0.1148416  0.11704753 0.11909424 0.11987101 0.12012878
 0.12029935 0.12093946 0.12190841 0.12243453 0.12274151 0.12258504
 0.1224739  0.12263124 0.12360682 0.12501785 0.12573515 0.12536882
 0.12428214 0.12342484 0.12362451 0.12414784 0.12477664 0.12453557
 0.12386832 0.12349731 0.12385236 0.12461171 0.12497611 0.12457138
 0.12392461 0.12405264 0.124839   0.12543423 0.12530193 0.12453055
 0.12382909 0.12377053 0.12402097 0.12388178 0.12269495 0.12077138
 0.11960582 0.11968577 0.12033913 0.12025375 0.11940755 0.11838538
 0.11759333 0.11770494 0.11814209 0.11865971 0.1185839  0.11920663
 0.11995157 0.12067613 0.12063684 0.12003572 0.1194776  0.11927286
 0.11956369 0.11930558 0.11864296 0.11752807 0.11638192 0.11558224
 0.11542511 0.11603453 0.1167073  0.11770907 0.11818101 0.11796393
 0.11673436 0.1161148  0.11750073 0.1198357  0.12097687 0.12026168
 0.11912644 0.11930566 0.12063781 0.12102155 0.12071123 0.12054239]
