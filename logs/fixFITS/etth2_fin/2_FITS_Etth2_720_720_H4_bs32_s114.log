Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16088576.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6289750
	speed: 0.1185s/iter; left time: 651.8159s
Epoch: 1 cost time: 13.229544401168823
Epoch: 1, Steps: 112 | Train Loss: 0.7684014 Vali Loss: 0.7814296 Test Loss: 0.4444852
Validation loss decreased (inf --> 0.781430).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5475914
	speed: 0.1893s/iter; left time: 1020.2845s
Epoch: 2 cost time: 7.809679269790649
Epoch: 2, Steps: 112 | Train Loss: 0.5930315 Vali Loss: 0.7309422 Test Loss: 0.4158508
Validation loss decreased (0.781430 --> 0.730942).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6161342
	speed: 0.1526s/iter; left time: 805.0118s
Epoch: 3 cost time: 7.848206520080566
Epoch: 3, Steps: 112 | Train Loss: 0.5361007 Vali Loss: 0.7099992 Test Loss: 0.4068441
Validation loss decreased (0.730942 --> 0.709999).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4705590
	speed: 0.3138s/iter; left time: 1620.8107s
Epoch: 4 cost time: 16.04300045967102
Epoch: 4, Steps: 112 | Train Loss: 0.5060498 Vali Loss: 0.6994119 Test Loss: 0.4021301
Validation loss decreased (0.709999 --> 0.699412).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3934049
	speed: 0.3054s/iter; left time: 1543.1088s
Epoch: 5 cost time: 15.208135843276978
Epoch: 5, Steps: 112 | Train Loss: 0.4851186 Vali Loss: 0.6900786 Test Loss: 0.3991759
Validation loss decreased (0.699412 --> 0.690079).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3803868
	speed: 0.3129s/iter; left time: 1545.8039s
Epoch: 6 cost time: 15.478663206100464
Epoch: 6, Steps: 112 | Train Loss: 0.4708591 Vali Loss: 0.6848950 Test Loss: 0.3963418
Validation loss decreased (0.690079 --> 0.684895).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4731255
	speed: 0.3082s/iter; left time: 1488.1629s
Epoch: 7 cost time: 15.658579587936401
Epoch: 7, Steps: 112 | Train Loss: 0.4595172 Vali Loss: 0.6785083 Test Loss: 0.3943264
Validation loss decreased (0.684895 --> 0.678508).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5454664
	speed: 0.3327s/iter; left time: 1569.3438s
Epoch: 8 cost time: 16.909712314605713
Epoch: 8, Steps: 112 | Train Loss: 0.4510397 Vali Loss: 0.6769816 Test Loss: 0.3925208
Validation loss decreased (0.678508 --> 0.676982).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3522687
	speed: 0.3303s/iter; left time: 1520.8555s
Epoch: 9 cost time: 16.07783579826355
Epoch: 9, Steps: 112 | Train Loss: 0.4442396 Vali Loss: 0.6708258 Test Loss: 0.3910025
Validation loss decreased (0.676982 --> 0.670826).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4487723
	speed: 0.3274s/iter; left time: 1471.1483s
Epoch: 10 cost time: 16.038363695144653
Epoch: 10, Steps: 112 | Train Loss: 0.4404427 Vali Loss: 0.6667548 Test Loss: 0.3896563
Validation loss decreased (0.670826 --> 0.666755).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3993408
	speed: 0.3446s/iter; left time: 1509.5069s
Epoch: 11 cost time: 16.964853286743164
Epoch: 11, Steps: 112 | Train Loss: 0.4348854 Vali Loss: 0.6684510 Test Loss: 0.3886208
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3972670
	speed: 0.3358s/iter; left time: 1433.4018s
Epoch: 12 cost time: 17.50942587852478
Epoch: 12, Steps: 112 | Train Loss: 0.4320932 Vali Loss: 0.6614088 Test Loss: 0.3877638
Validation loss decreased (0.666755 --> 0.661409).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4915127
	speed: 0.3561s/iter; left time: 1480.1858s
Epoch: 13 cost time: 16.73044466972351
Epoch: 13, Steps: 112 | Train Loss: 0.4294025 Vali Loss: 0.6626632 Test Loss: 0.3870474
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5079677
	speed: 0.3356s/iter; left time: 1357.3701s
Epoch: 14 cost time: 17.361058712005615
Epoch: 14, Steps: 112 | Train Loss: 0.4281248 Vali Loss: 0.6589003 Test Loss: 0.3864402
Validation loss decreased (0.661409 --> 0.658900).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3509196
	speed: 0.3378s/iter; left time: 1328.7065s
Epoch: 15 cost time: 17.041001558303833
Epoch: 15, Steps: 112 | Train Loss: 0.4262920 Vali Loss: 0.6597658 Test Loss: 0.3858656
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4706174
	speed: 0.3263s/iter; left time: 1246.7475s
Epoch: 16 cost time: 16.72369384765625
Epoch: 16, Steps: 112 | Train Loss: 0.4243772 Vali Loss: 0.6538023 Test Loss: 0.3853753
Validation loss decreased (0.658900 --> 0.653802).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5048297
	speed: 0.3094s/iter; left time: 1147.6910s
Epoch: 17 cost time: 14.096031188964844
Epoch: 17, Steps: 112 | Train Loss: 0.4236232 Vali Loss: 0.6553367 Test Loss: 0.3849854
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3500800
	speed: 0.2746s/iter; left time: 987.8212s
Epoch: 18 cost time: 15.201079368591309
Epoch: 18, Steps: 112 | Train Loss: 0.4223516 Vali Loss: 0.6545735 Test Loss: 0.3846951
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3240350
	speed: 0.3226s/iter; left time: 1124.2213s
Epoch: 19 cost time: 16.606088399887085
Epoch: 19, Steps: 112 | Train Loss: 0.4213382 Vali Loss: 0.6541328 Test Loss: 0.3843904
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16088576.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6053393
	speed: 0.1412s/iter; left time: 776.6048s
Epoch: 1 cost time: 15.999049663543701
Epoch: 1, Steps: 112 | Train Loss: 0.8094264 Vali Loss: 0.6511711 Test Loss: 0.3823267
Validation loss decreased (inf --> 0.651171).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4681292
	speed: 0.3366s/iter; left time: 1813.8149s
Epoch: 2 cost time: 16.42692542076111
Epoch: 2, Steps: 112 | Train Loss: 0.8053142 Vali Loss: 0.6459173 Test Loss: 0.3811879
Validation loss decreased (0.651171 --> 0.645917).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.7350236
	speed: 0.3069s/iter; left time: 1619.3987s
Epoch: 3 cost time: 13.774269104003906
Epoch: 3, Steps: 112 | Train Loss: 0.8037402 Vali Loss: 0.6443245 Test Loss: 0.3804612
Validation loss decreased (0.645917 --> 0.644325).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.8546441
	speed: 0.2146s/iter; left time: 1108.3514s
Epoch: 4 cost time: 11.722153425216675
Epoch: 4, Steps: 112 | Train Loss: 0.8021080 Vali Loss: 0.6441163 Test Loss: 0.3805022
Validation loss decreased (0.644325 --> 0.644116).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.7572116
	speed: 0.3306s/iter; left time: 1670.3834s
Epoch: 5 cost time: 16.957432985305786
Epoch: 5, Steps: 112 | Train Loss: 0.8027188 Vali Loss: 0.6424428 Test Loss: 0.3804419
Validation loss decreased (0.644116 --> 0.642443).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.8327683
	speed: 0.2911s/iter; left time: 1438.4288s
Epoch: 6 cost time: 14.216920852661133
Epoch: 6, Steps: 112 | Train Loss: 0.8010620 Vali Loss: 0.6431134 Test Loss: 0.3806128
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.8079318
	speed: 0.3573s/iter; left time: 1725.3065s
Epoch: 7 cost time: 17.620615005493164
Epoch: 7, Steps: 112 | Train Loss: 0.8007745 Vali Loss: 0.6423333 Test Loss: 0.3806688
Validation loss decreased (0.642443 --> 0.642333).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.8413979
	speed: 0.3334s/iter; left time: 1572.8268s
Epoch: 8 cost time: 15.0713529586792
Epoch: 8, Steps: 112 | Train Loss: 0.8006344 Vali Loss: 0.6423370 Test Loss: 0.3806291
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.8581524
	speed: 0.2830s/iter; left time: 1303.0574s
Epoch: 9 cost time: 13.594549894332886
Epoch: 9, Steps: 112 | Train Loss: 0.7988977 Vali Loss: 0.6442825 Test Loss: 0.3803903
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.7896372
	speed: 0.2835s/iter; left time: 1273.8248s
Epoch: 10 cost time: 14.366647481918335
Epoch: 10, Steps: 112 | Train Loss: 0.7991796 Vali Loss: 0.6423331 Test Loss: 0.3805006
Validation loss decreased (0.642333 --> 0.642333).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6696929
	speed: 0.2765s/iter; left time: 1211.2462s
Epoch: 11 cost time: 14.224336624145508
Epoch: 11, Steps: 112 | Train Loss: 0.8006527 Vali Loss: 0.6410959 Test Loss: 0.3805660
Validation loss decreased (0.642333 --> 0.641096).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.9606002
	speed: 0.2752s/iter; left time: 1174.6579s
Epoch: 12 cost time: 14.143582344055176
Epoch: 12, Steps: 112 | Train Loss: 0.7991626 Vali Loss: 0.6396152 Test Loss: 0.3806537
Validation loss decreased (0.641096 --> 0.639615).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6753754
	speed: 0.2891s/iter; left time: 1201.8706s
Epoch: 13 cost time: 14.940576553344727
Epoch: 13, Steps: 112 | Train Loss: 0.7984266 Vali Loss: 0.6406839 Test Loss: 0.3805236
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.7291065
	speed: 0.3082s/iter; left time: 1246.8311s
Epoch: 14 cost time: 15.259764671325684
Epoch: 14, Steps: 112 | Train Loss: 0.7997408 Vali Loss: 0.6404281 Test Loss: 0.3804558
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.8150185
	speed: 0.2859s/iter; left time: 1124.3885s
Epoch: 15 cost time: 14.770774364471436
Epoch: 15, Steps: 112 | Train Loss: 0.7994101 Vali Loss: 0.6400774 Test Loss: 0.3805000
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3791118860244751, mae:0.4234304130077362, rse:0.49214088916778564, corr:[ 2.19022065e-01  2.21360430e-01  2.20638171e-01  2.19035521e-01
  2.18197986e-01  2.18052149e-01  2.17622027e-01  2.16461271e-01
  2.15283364e-01  2.14185640e-01  2.13199586e-01  2.11866125e-01
  2.10378125e-01  2.09185049e-01  2.08418936e-01  2.07954615e-01
  2.07251221e-01  2.06118599e-01  2.04896033e-01  2.03995451e-01
  2.03341454e-01  2.02622786e-01  2.01558888e-01  2.00153649e-01
  1.98803231e-01  1.97652757e-01  1.96567416e-01  1.95405543e-01
  1.94433630e-01  1.93754897e-01  1.93357885e-01  1.92871973e-01
  1.92027286e-01  1.90884605e-01  1.89748973e-01  1.88752174e-01
  1.87914252e-01  1.87309548e-01  1.86888859e-01  1.86435327e-01
  1.85908109e-01  1.85276926e-01  1.84556782e-01  1.83943629e-01
  1.83460072e-01  1.82899147e-01  1.82052568e-01  1.80559084e-01
  1.78871155e-01  1.77443877e-01  1.76528066e-01  1.75876334e-01
  1.75317392e-01  1.74619213e-01  1.73560351e-01  1.72504783e-01
  1.71751231e-01  1.71284959e-01  1.70928538e-01  1.70574859e-01
  1.70201391e-01  1.69766217e-01  1.69672281e-01  1.70116633e-01
  1.70514464e-01  1.70925036e-01  1.71080053e-01  1.70865402e-01
  1.70424893e-01  1.70016214e-01  1.69726461e-01  1.69541150e-01
  1.69487745e-01  1.69412851e-01  1.69263050e-01  1.68782562e-01
  1.68275490e-01  1.67934224e-01  1.67756140e-01  1.67624369e-01
  1.67630866e-01  1.67639345e-01  1.67519629e-01  1.67514816e-01
  1.67816699e-01  1.68322653e-01  1.68660700e-01  1.68682367e-01
  1.68403074e-01  1.68215439e-01  1.68256491e-01  1.68309584e-01
  1.68513745e-01  1.68551058e-01  1.68438464e-01  1.68213874e-01
  1.68000832e-01  1.67855829e-01  1.67614222e-01  1.67239666e-01
  1.66959628e-01  1.66692823e-01  1.66693583e-01  1.66733116e-01
  1.66872367e-01  1.66915491e-01  1.66971400e-01  1.67061567e-01
  1.66853920e-01  1.66375026e-01  1.65873528e-01  1.65649429e-01
  1.65609568e-01  1.65739253e-01  1.65841952e-01  1.65804178e-01
  1.65589094e-01  1.64993688e-01  1.64287373e-01  1.63416490e-01
  1.62492037e-01  1.61655560e-01  1.61078885e-01  1.60716116e-01
  1.60181865e-01  1.59478903e-01  1.58744887e-01  1.58244297e-01
  1.58094198e-01  1.57858089e-01  1.57379389e-01  1.56396717e-01
  1.55374289e-01  1.54709190e-01  1.54670343e-01  1.55058146e-01
  1.55004203e-01  1.54397398e-01  1.53589755e-01  1.53132722e-01
  1.53357029e-01  1.53602272e-01  1.53265372e-01  1.52082771e-01
  1.50309056e-01  1.48979664e-01  1.48341954e-01  1.47992879e-01
  1.47395253e-01  1.46563917e-01  1.45836517e-01  1.45349294e-01
  1.45207927e-01  1.45043314e-01  1.44573033e-01  1.43795237e-01
  1.43086895e-01  1.42681494e-01  1.42422482e-01  1.42068848e-01
  1.41518012e-01  1.41348958e-01  1.41722396e-01  1.42284289e-01
  1.42665759e-01  1.42462179e-01  1.41898423e-01  1.41216367e-01
  1.40895829e-01  1.40891165e-01  1.40904948e-01  1.40561774e-01
  1.39873877e-01  1.38961926e-01  1.38115972e-01  1.37568131e-01
  1.37409195e-01  1.37405515e-01  1.37091726e-01  1.36409849e-01
  1.35551631e-01  1.34863436e-01  1.34704351e-01  1.34956345e-01
  1.35378554e-01  1.35863274e-01  1.36066005e-01  1.36001989e-01
  1.35993138e-01  1.36329502e-01  1.37110636e-01  1.37669131e-01
  1.37588546e-01  1.37176439e-01  1.36810020e-01  1.36892766e-01
  1.37338534e-01  1.37595013e-01  1.37617767e-01  1.37272120e-01
  1.36891901e-01  1.36655644e-01  1.36791259e-01  1.36891022e-01
  1.36925444e-01  1.37137085e-01  1.37457132e-01  1.37791827e-01
  1.38057142e-01  1.38210192e-01  1.38372242e-01  1.38743758e-01
  1.39439061e-01  1.40077546e-01  1.40309826e-01  1.39872849e-01
  1.39121190e-01  1.38470203e-01  1.38202906e-01  1.38551667e-01
  1.38999030e-01  1.39461070e-01  1.39414772e-01  1.39197037e-01
  1.38826460e-01  1.38515085e-01  1.38421625e-01  1.38343677e-01
  1.38294697e-01  1.38171405e-01  1.38190985e-01  1.38618991e-01
  1.39385447e-01  1.40311837e-01  1.41202360e-01  1.41944468e-01
  1.42639384e-01  1.43326670e-01  1.44074857e-01  1.44816503e-01
  1.45470276e-01  1.45937204e-01  1.46347955e-01  1.46847576e-01
  1.47134960e-01  1.46998018e-01  1.46734029e-01  1.46393523e-01
  1.46330148e-01  1.46549314e-01  1.47107795e-01  1.47947580e-01
  1.48837805e-01  1.49609268e-01  1.50030270e-01  1.50401071e-01
  1.50690302e-01  1.51179880e-01  1.51874065e-01  1.52748585e-01
  1.53579995e-01  1.54217303e-01  1.54845908e-01  1.55297682e-01
  1.55620754e-01  1.55727848e-01  1.55839682e-01  1.56233281e-01
  1.56713843e-01  1.57289639e-01  1.57792479e-01  1.58205539e-01
  1.58814698e-01  1.59692213e-01  1.60762578e-01  1.61449775e-01
  1.61384031e-01  1.61095515e-01  1.61216483e-01  1.62239477e-01
  1.63801864e-01  1.64961442e-01  1.65040031e-01  1.64427817e-01
  1.63996518e-01  1.64423659e-01  1.65652499e-01  1.66682795e-01
  1.66885674e-01  1.66308686e-01  1.65403992e-01  1.64897323e-01
  1.64689392e-01  1.64687395e-01  1.64929718e-01  1.65423244e-01
  1.65848792e-01  1.65814191e-01  1.65319100e-01  1.64899454e-01
  1.65075526e-01  1.66258439e-01  1.67839959e-01  1.68800220e-01
  1.68483928e-01  1.67513445e-01  1.66541323e-01  1.66486666e-01
  1.67130798e-01  1.67977333e-01  1.68676957e-01  1.69115499e-01
  1.69540301e-01  1.69621021e-01  1.69404387e-01  1.68851659e-01
  1.68510720e-01  1.68778703e-01  1.69418260e-01  1.69765562e-01
  1.69154778e-01  1.68141648e-01  1.67514816e-01  1.67788118e-01
  1.68456614e-01  1.68872446e-01  1.69026330e-01  1.69015154e-01
  1.69279426e-01  1.69914410e-01  1.70581281e-01  1.71247289e-01
  1.71760410e-01  1.72305644e-01  1.73071310e-01  1.73556194e-01
  1.73181802e-01  1.72191709e-01  1.71416894e-01  1.71299487e-01
  1.71986103e-01  1.72822356e-01  1.73337415e-01  1.73236817e-01
  1.72877327e-01  1.72761574e-01  1.72778711e-01  1.73057884e-01
  1.73243210e-01  1.73507646e-01  1.74138948e-01  1.75182208e-01
  1.76245898e-01  1.77130386e-01  1.77762657e-01  1.78547457e-01
  1.79484874e-01  1.80328310e-01  1.80648431e-01  1.80309698e-01
  1.79700911e-01  1.79401666e-01  1.79830968e-01  1.80718243e-01
  1.81652889e-01  1.82210371e-01  1.82362512e-01  1.82327479e-01
  1.82096407e-01  1.81975737e-01  1.81820199e-01  1.81735650e-01
  1.81712389e-01  1.81918889e-01  1.82411686e-01  1.83027744e-01
  1.83555260e-01  1.83723614e-01  1.83317870e-01  1.82701305e-01
  1.82262644e-01  1.82207078e-01  1.82510734e-01  1.83020890e-01
  1.83305562e-01  1.83198065e-01  1.82939008e-01  1.82864413e-01
  1.83133498e-01  1.83625489e-01  1.84135541e-01  1.84613779e-01
  1.85038403e-01  1.85428649e-01  1.85891554e-01  1.86192796e-01
  1.86273381e-01  1.85842454e-01  1.85283810e-01  1.84926435e-01
  1.84683383e-01  1.84520081e-01  1.84305608e-01  1.84102535e-01
  1.83889329e-01  1.83699802e-01  1.83533669e-01  1.83451802e-01
  1.83327228e-01  1.83132648e-01  1.82810187e-01  1.82339549e-01
  1.81927770e-01  1.81686193e-01  1.81649223e-01  1.81898937e-01
  1.82249993e-01  1.82492837e-01  1.82464495e-01  1.82341591e-01
  1.82002366e-01  1.81503505e-01  1.80751681e-01  1.79686710e-01
  1.78372175e-01  1.77009806e-01  1.76134929e-01  1.75648361e-01
  1.75321668e-01  1.74868733e-01  1.74071386e-01  1.73331603e-01
  1.72827318e-01  1.72628164e-01  1.72682345e-01  1.72377989e-01
  1.71500072e-01  1.70265839e-01  1.69102699e-01  1.68509275e-01
  1.68233618e-01  1.68319121e-01  1.68281585e-01  1.68011278e-01
  1.67397499e-01  1.66917607e-01  1.66610003e-01  1.66571856e-01
  1.66469738e-01  1.66040748e-01  1.65317506e-01  1.64671913e-01
  1.64091885e-01  1.63776144e-01  1.63686782e-01  1.63616151e-01
  1.63435593e-01  1.63093165e-01  1.62811235e-01  1.62938386e-01
  1.63070098e-01  1.62624776e-01  1.61538824e-01  1.60452113e-01
  1.59728870e-01  1.59850389e-01  1.60395235e-01  1.60542384e-01
  1.60234109e-01  1.59574434e-01  1.59081548e-01  1.58669665e-01
  1.58074230e-01  1.57120615e-01  1.56205848e-01  1.55716702e-01
  1.55705184e-01  1.55940980e-01  1.55827150e-01  1.55116647e-01
  1.54082015e-01  1.53428793e-01  1.53483808e-01  1.53475896e-01
  1.52933419e-01  1.51735350e-01  1.50296479e-01  1.49098530e-01
  1.48605242e-01  1.48595974e-01  1.48642063e-01  1.48166031e-01
  1.47115320e-01  1.45914897e-01  1.44996703e-01  1.44466221e-01
  1.44030288e-01  1.43780127e-01  1.43590868e-01  1.43239230e-01
  1.42711639e-01  1.42124653e-01  1.41719908e-01  1.41222343e-01
  1.40395805e-01  1.39297813e-01  1.38119832e-01  1.37012124e-01
  1.36241958e-01  1.35856643e-01  1.35228977e-01  1.34207591e-01
  1.32993057e-01  1.32072970e-01  1.31799549e-01  1.31725535e-01
  1.31502122e-01  1.31008729e-01  1.30649671e-01  1.30512133e-01
  1.30423427e-01  1.30069479e-01  1.28971145e-01  1.27520502e-01
  1.26362070e-01  1.25882521e-01  1.25542223e-01  1.24655999e-01
  1.22983910e-01  1.21305212e-01  1.20369874e-01  1.19733140e-01
  1.18971623e-01  1.17615633e-01  1.15909621e-01  1.14430614e-01
  1.13533542e-01  1.13044918e-01  1.12391114e-01  1.11132286e-01
  1.09486908e-01  1.08075835e-01  1.07441582e-01  1.07310586e-01
  1.06947534e-01  1.06243186e-01  1.05070196e-01  1.04017287e-01
  1.03389405e-01  1.02819957e-01  1.02292717e-01  1.01551041e-01
  1.00736536e-01  9.97111201e-02  9.84701067e-02  9.70838368e-02
  9.58527848e-02  9.50321481e-02  9.44707245e-02  9.36279595e-02
  9.27608758e-02  9.18578207e-02  9.14029256e-02  9.12825391e-02
  9.12320390e-02  9.08216387e-02  8.99681672e-02  8.87620077e-02
  8.78634900e-02  8.72937813e-02  8.66628438e-02  8.57526213e-02
  8.45700353e-02  8.34333748e-02  8.22616816e-02  8.09947476e-02
  7.95477778e-02  7.84157440e-02  7.76371658e-02  7.70286471e-02
  7.62672946e-02  7.50762895e-02  7.36479536e-02  7.24591538e-02
  7.18358010e-02  7.16303885e-02  7.11865947e-02  7.02803060e-02
  6.88657239e-02  6.75878152e-02  6.69321492e-02  6.67460561e-02
  6.67114258e-02  6.65717050e-02  6.62014708e-02  6.59186989e-02
  6.56018406e-02  6.49720654e-02  6.37567043e-02  6.23855144e-02
  6.14935718e-02  6.13204502e-02  6.09775111e-02  5.99726625e-02
  5.79049774e-02  5.55982627e-02  5.37126102e-02  5.26503213e-02
  5.21085002e-02  5.18047176e-02  5.15259914e-02  5.08977957e-02
  5.02624661e-02  4.98740487e-02  4.97733355e-02  4.95949797e-02
  4.89595793e-02  4.81686294e-02  4.72079776e-02  4.64876033e-02
  4.61772569e-02  4.64688390e-02  4.67388369e-02  4.68166359e-02
  4.62139882e-02  4.49883118e-02  4.37091477e-02  4.29307744e-02
  4.27405499e-02  4.28632200e-02  4.26310860e-02  4.17942069e-02
  4.07934077e-02  4.01785374e-02  3.98129597e-02  3.96345891e-02
  3.93820554e-02  3.90091538e-02  3.88090238e-02  3.86810228e-02
  3.79162394e-02  3.70154195e-02  3.65111977e-02  3.64620909e-02
  3.67313810e-02  3.66994962e-02  3.55029739e-02  3.36055048e-02
  3.21748517e-02  3.20553780e-02  3.27458009e-02  3.28817517e-02
  3.21152359e-02  3.06783598e-02  2.93867365e-02  2.87215803e-02
  2.83452813e-02  2.77700964e-02  2.70475168e-02  2.66792625e-02
  2.68035494e-02  2.73557045e-02  2.80050989e-02  2.80635469e-02
  2.77863070e-02  2.77226903e-02  2.82216463e-02  2.89393160e-02
  2.89281290e-02  2.82232352e-02  2.72448454e-02  2.67069079e-02
  2.68256627e-02  2.73895226e-02  2.78001893e-02  2.70748157e-02
  2.56245267e-02  2.37039104e-02  2.24014260e-02  2.15484016e-02
  2.16330532e-02  2.21859291e-02  2.22282838e-02  2.13083234e-02
  1.99582167e-02  1.85512323e-02  1.82861425e-02  1.90106574e-02
  1.97966415e-02  1.99293606e-02  1.94063485e-02  1.83147956e-02
  1.77278817e-02  1.75780896e-02  1.72892045e-02  1.60420910e-02
  1.41565884e-02  1.24037396e-02  1.16412165e-02  1.11892968e-02
  1.05073815e-02  8.95119552e-03  7.19453488e-03  5.45435492e-03
  4.38483665e-03  3.92888207e-03  3.29322531e-03  2.47375481e-03
  1.40522269e-03  4.31632980e-05 -4.72804473e-04 -2.07236139e-04
  6.29179820e-04  1.69947755e-03  2.15954101e-03  2.01341347e-03
  2.06091464e-03  3.29667237e-03  4.50551463e-03  3.27417906e-03]
