Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  94130176.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.1472513675689697
Epoch: 1, Steps: 14 | Train Loss: 0.7427252 Vali Loss: 0.6750695 Test Loss: 0.4829229
Validation loss decreased (inf --> 0.675069).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.29349946975708
Epoch: 2, Steps: 14 | Train Loss: 0.6778506 Vali Loss: 0.6381084 Test Loss: 0.4648959
Validation loss decreased (0.675069 --> 0.638108).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.121978998184204
Epoch: 3, Steps: 14 | Train Loss: 0.6297549 Vali Loss: 0.6078192 Test Loss: 0.4499529
Validation loss decreased (0.638108 --> 0.607819).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.9636991024017334
Epoch: 4, Steps: 14 | Train Loss: 0.5948461 Vali Loss: 0.5876060 Test Loss: 0.4376682
Validation loss decreased (0.607819 --> 0.587606).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.982433557510376
Epoch: 5, Steps: 14 | Train Loss: 0.5678839 Vali Loss: 0.5705493 Test Loss: 0.4277937
Validation loss decreased (0.587606 --> 0.570549).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.066133499145508
Epoch: 6, Steps: 14 | Train Loss: 0.5433317 Vali Loss: 0.5607554 Test Loss: 0.4198161
Validation loss decreased (0.570549 --> 0.560755).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.3107385635375977
Epoch: 7, Steps: 14 | Train Loss: 0.5196720 Vali Loss: 0.5502727 Test Loss: 0.4134073
Validation loss decreased (0.560755 --> 0.550273).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.751149892807007
Epoch: 8, Steps: 14 | Train Loss: 0.5055652 Vali Loss: 0.5411992 Test Loss: 0.4082778
Validation loss decreased (0.550273 --> 0.541199).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.7231223583221436
Epoch: 9, Steps: 14 | Train Loss: 0.4916953 Vali Loss: 0.5291937 Test Loss: 0.4041559
Validation loss decreased (0.541199 --> 0.529194).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.8272624015808105
Epoch: 10, Steps: 14 | Train Loss: 0.4781715 Vali Loss: 0.5212401 Test Loss: 0.4006954
Validation loss decreased (0.529194 --> 0.521240).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.9355950355529785
Epoch: 11, Steps: 14 | Train Loss: 0.4684842 Vali Loss: 0.5122595 Test Loss: 0.3978156
Validation loss decreased (0.521240 --> 0.512259).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.9263038635253906
Epoch: 12, Steps: 14 | Train Loss: 0.4581335 Vali Loss: 0.5067605 Test Loss: 0.3954300
Validation loss decreased (0.512259 --> 0.506760).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.701277732849121
Epoch: 13, Steps: 14 | Train Loss: 0.4531099 Vali Loss: 0.5093210 Test Loss: 0.3934429
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.844958782196045
Epoch: 14, Steps: 14 | Train Loss: 0.4432662 Vali Loss: 0.4993977 Test Loss: 0.3917797
Validation loss decreased (0.506760 --> 0.499398).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.614351987838745
Epoch: 15, Steps: 14 | Train Loss: 0.4350665 Vali Loss: 0.5003695 Test Loss: 0.3903479
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.6039700508117676
Epoch: 16, Steps: 14 | Train Loss: 0.4316408 Vali Loss: 0.4914474 Test Loss: 0.3891643
Validation loss decreased (0.499398 --> 0.491447).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.5437376499176025
Epoch: 17, Steps: 14 | Train Loss: 0.4283598 Vali Loss: 0.4913840 Test Loss: 0.3881312
Validation loss decreased (0.491447 --> 0.491384).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.6168997287750244
Epoch: 18, Steps: 14 | Train Loss: 0.4243248 Vali Loss: 0.4851264 Test Loss: 0.3872318
Validation loss decreased (0.491384 --> 0.485126).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.405608892440796
Epoch: 19, Steps: 14 | Train Loss: 0.4185592 Vali Loss: 0.4785174 Test Loss: 0.3864850
Validation loss decreased (0.485126 --> 0.478517).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.2386789321899414
Epoch: 20, Steps: 14 | Train Loss: 0.4158432 Vali Loss: 0.4781200 Test Loss: 0.3858031
Validation loss decreased (0.478517 --> 0.478120).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.260272741317749
Epoch: 21, Steps: 14 | Train Loss: 0.4126309 Vali Loss: 0.4797460 Test Loss: 0.3852068
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.3023014068603516
Epoch: 22, Steps: 14 | Train Loss: 0.4078064 Vali Loss: 0.4708750 Test Loss: 0.3846707
Validation loss decreased (0.478120 --> 0.470875).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.3005175590515137
Epoch: 23, Steps: 14 | Train Loss: 0.4063720 Vali Loss: 0.4754142 Test Loss: 0.3842255
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.752995491027832
Epoch: 24, Steps: 14 | Train Loss: 0.4008900 Vali Loss: 0.4792855 Test Loss: 0.3838357
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.818222761154175
Epoch: 25, Steps: 14 | Train Loss: 0.4032302 Vali Loss: 0.4713010 Test Loss: 0.3834597
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  94130176.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.9242820739746094
Epoch: 1, Steps: 14 | Train Loss: 0.6973721 Vali Loss: 0.4569554 Test Loss: 0.3770730
Validation loss decreased (inf --> 0.456955).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.8643863201141357
Epoch: 2, Steps: 14 | Train Loss: 0.6800434 Vali Loss: 0.4402743 Test Loss: 0.3729533
Validation loss decreased (0.456955 --> 0.440274).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.9017930030822754
Epoch: 3, Steps: 14 | Train Loss: 0.6687297 Vali Loss: 0.4367485 Test Loss: 0.3702113
Validation loss decreased (0.440274 --> 0.436748).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.9257524013519287
Epoch: 4, Steps: 14 | Train Loss: 0.6631933 Vali Loss: 0.4321600 Test Loss: 0.3682863
Validation loss decreased (0.436748 --> 0.432160).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.7270753383636475
Epoch: 5, Steps: 14 | Train Loss: 0.6617867 Vali Loss: 0.4252803 Test Loss: 0.3667399
Validation loss decreased (0.432160 --> 0.425280).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.9602701663970947
Epoch: 6, Steps: 14 | Train Loss: 0.6485145 Vali Loss: 0.4188036 Test Loss: 0.3655844
Validation loss decreased (0.425280 --> 0.418804).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.9220895767211914
Epoch: 7, Steps: 14 | Train Loss: 0.6425893 Vali Loss: 0.4167920 Test Loss: 0.3645433
Validation loss decreased (0.418804 --> 0.416792).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.783824920654297
Epoch: 8, Steps: 14 | Train Loss: 0.6399305 Vali Loss: 0.4127278 Test Loss: 0.3636933
Validation loss decreased (0.416792 --> 0.412728).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.866530656814575
Epoch: 9, Steps: 14 | Train Loss: 0.6380920 Vali Loss: 0.4150745 Test Loss: 0.3628589
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.791818380355835
Epoch: 10, Steps: 14 | Train Loss: 0.6406724 Vali Loss: 0.4078298 Test Loss: 0.3622274
Validation loss decreased (0.412728 --> 0.407830).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.8726022243499756
Epoch: 11, Steps: 14 | Train Loss: 0.6411172 Vali Loss: 0.4021570 Test Loss: 0.3615254
Validation loss decreased (0.407830 --> 0.402157).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.9302561283111572
Epoch: 12, Steps: 14 | Train Loss: 0.6401318 Vali Loss: 0.4072137 Test Loss: 0.3609605
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.9087066650390625
Epoch: 13, Steps: 14 | Train Loss: 0.6366869 Vali Loss: 0.3990984 Test Loss: 0.3604362
Validation loss decreased (0.402157 --> 0.399098).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.886773109436035
Epoch: 14, Steps: 14 | Train Loss: 0.6349795 Vali Loss: 0.4048512 Test Loss: 0.3600761
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.837562084197998
Epoch: 15, Steps: 14 | Train Loss: 0.6359908 Vali Loss: 0.4013383 Test Loss: 0.3596585
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.8702383041381836
Epoch: 16, Steps: 14 | Train Loss: 0.6285564 Vali Loss: 0.4033721 Test Loss: 0.3592283
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.35912156105041504, mae:0.4012333154678345, rse:0.4791376292705536, corr:[0.25754485 0.26598307 0.26617718 0.26326177 0.26287314 0.26404327
 0.26422057 0.26295942 0.26121753 0.26009902 0.2593659  0.25841555
 0.25731656 0.25608784 0.2552322  0.25465986 0.25397015 0.2527955
 0.25149333 0.25051692 0.24982044 0.24896991 0.24742208 0.24555926
 0.24394841 0.24300946 0.242363   0.24161653 0.24063447 0.23947206
 0.2383018  0.23723346 0.23641397 0.23571151 0.23495895 0.23405929
 0.23300576 0.23202054 0.23123704 0.23062833 0.23001316 0.22916612
 0.22815195 0.22731595 0.22660373 0.22586319 0.22468336 0.22291067
 0.22090183 0.21910617 0.217627   0.21640119 0.21529955 0.21399724
 0.21221659 0.21029398 0.20855074 0.20716329 0.20618576 0.20538543
 0.20451336 0.20357788 0.20289308 0.2026324  0.20242763 0.20211454
 0.20129444 0.20037313 0.19964269 0.1991827  0.1984821  0.1971115
 0.19523287 0.19351704 0.19252622 0.19205491 0.19155103 0.19046512
 0.18899748 0.18789203 0.18763818 0.18762429 0.18723612 0.18633893
 0.18543167 0.18489362 0.18473859 0.18458892 0.18408053 0.18324296
 0.18249808 0.18216391 0.1823496  0.18256788 0.1824172  0.18178825
 0.18091127 0.1800429  0.17934127 0.17871459 0.17827974 0.17797285
 0.17770495 0.17720953 0.1767666  0.1764934  0.17657892 0.17644975
 0.17565878 0.17450094 0.17340071 0.17287679 0.1726857  0.17243555
 0.17169449 0.17062616 0.16969427 0.16898838 0.1683324  0.16715755
 0.1654247  0.16368574 0.1625566  0.16199659 0.16141029 0.16051741
 0.15934637 0.15852237 0.15827543 0.15809329 0.15736839 0.1558815
 0.15424061 0.1531068  0.1527923  0.15281518 0.15243012 0.15146478
 0.15030189 0.14954823 0.1492726  0.1489931  0.14801621 0.14609106
 0.14352037 0.1411255  0.13939501 0.13831621 0.13751537 0.13674095
 0.13581224 0.13476916 0.13396007 0.13326903 0.13250531 0.13151307
 0.13047265 0.12967268 0.1292862  0.12913777 0.12870641 0.12797743
 0.12706758 0.12641868 0.12634659 0.12661932 0.12665327 0.12571862
 0.12402008 0.12218451 0.12085862 0.12003277 0.11918107 0.11763547
 0.11576602 0.11413394 0.11342608 0.11342476 0.11312257 0.11210176
 0.11060183 0.10939268 0.10889312 0.10886858 0.10842614 0.10754814
 0.10665281 0.1063017  0.10683525 0.1077424  0.10844777 0.10804927
 0.10676544 0.10534134 0.10427155 0.10386871 0.10375403 0.10347676
 0.10294568 0.10229209 0.10189399 0.10168097 0.10149544 0.10105525
 0.10044951 0.10002112 0.09981826 0.09996915 0.10017038 0.10020936
 0.09995873 0.09983775 0.10007057 0.10040499 0.10032433 0.09928589
 0.09750342 0.09559965 0.09429759 0.09394926 0.09388231 0.09373796
 0.09300934 0.0924507  0.09240087 0.09290652 0.09335791 0.09300729
 0.09173156 0.090335   0.08964074 0.09021101 0.09131858 0.0922823
 0.0926111  0.09261247 0.09279469 0.09323401 0.09339116 0.09258919
 0.09077086 0.08872404 0.08756087 0.08772291 0.08815051 0.08779044
 0.08697546 0.08631142 0.08637842 0.08702403 0.08767074 0.08802994
 0.08794896 0.08828878 0.08926621 0.09089545 0.09217407 0.09308731
 0.0936685  0.09448882 0.09561957 0.09638479 0.09693651 0.09686635
 0.09655762 0.09610347 0.09611452 0.09649492 0.09655731 0.09632398
 0.09605275 0.09628518 0.09739716 0.09863475 0.09992628 0.10026164
 0.09977651 0.09892677 0.09855545 0.09912596 0.10028609 0.10106451
 0.10075294 0.09986582 0.09916978 0.09943844 0.1006057  0.1015499
 0.10103029 0.09911372 0.09704942 0.09646437 0.09715598 0.09774946
 0.09768385 0.0968831  0.09608255 0.09574615 0.09638167 0.09709238
 0.09703587 0.09692044 0.09731185 0.0985891  0.09927733 0.0997786
 0.09957903 0.09976203 0.1005748  0.10164694 0.10198411 0.10102426
 0.09988394 0.09919874 0.09977441 0.10021596 0.09946787 0.09773664
 0.09665566 0.09769583 0.09970876 0.10189477 0.10288136 0.10309397
 0.10252151 0.10243468 0.10366011 0.10524139 0.10586451 0.10524104
 0.10361075 0.10190185 0.1015657  0.10415587 0.11028035 0.11090048]
