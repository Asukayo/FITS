Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20290816.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.281632900238037
Epoch: 1, Steps: 60 | Train Loss: 0.5939043 Vali Loss: 0.4738081 Test Loss: 0.4555569
Validation loss decreased (inf --> 0.473808).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.72766661643982
Epoch: 2, Steps: 60 | Train Loss: 0.4656968 Vali Loss: 0.4178649 Test Loss: 0.4160650
Validation loss decreased (0.473808 --> 0.417865).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.394136190414429
Epoch: 3, Steps: 60 | Train Loss: 0.3980339 Vali Loss: 0.3906931 Test Loss: 0.3986507
Validation loss decreased (0.417865 --> 0.390693).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.683107614517212
Epoch: 4, Steps: 60 | Train Loss: 0.3580892 Vali Loss: 0.3759646 Test Loss: 0.3908110
Validation loss decreased (0.390693 --> 0.375965).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.889331340789795
Epoch: 5, Steps: 60 | Train Loss: 0.3298921 Vali Loss: 0.3677008 Test Loss: 0.3870026
Validation loss decreased (0.375965 --> 0.367701).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.900937795639038
Epoch: 6, Steps: 60 | Train Loss: 0.3089573 Vali Loss: 0.3616729 Test Loss: 0.3848561
Validation loss decreased (0.367701 --> 0.361673).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.92279052734375
Epoch: 7, Steps: 60 | Train Loss: 0.2919407 Vali Loss: 0.3567288 Test Loss: 0.3833959
Validation loss decreased (0.361673 --> 0.356729).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 10.081146240234375
Epoch: 8, Steps: 60 | Train Loss: 0.2768771 Vali Loss: 0.3526848 Test Loss: 0.3820576
Validation loss decreased (0.356729 --> 0.352685).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 10.090825319290161
Epoch: 9, Steps: 60 | Train Loss: 0.2645118 Vali Loss: 0.3499402 Test Loss: 0.3809455
Validation loss decreased (0.352685 --> 0.349940).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 10.004597187042236
Epoch: 10, Steps: 60 | Train Loss: 0.2538239 Vali Loss: 0.3469704 Test Loss: 0.3799384
Validation loss decreased (0.349940 --> 0.346970).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 10.147677183151245
Epoch: 11, Steps: 60 | Train Loss: 0.2446774 Vali Loss: 0.3444183 Test Loss: 0.3790458
Validation loss decreased (0.346970 --> 0.344418).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.989084005355835
Epoch: 12, Steps: 60 | Train Loss: 0.2359196 Vali Loss: 0.3419819 Test Loss: 0.3779882
Validation loss decreased (0.344418 --> 0.341982).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.738769769668579
Epoch: 13, Steps: 60 | Train Loss: 0.2292066 Vali Loss: 0.3399659 Test Loss: 0.3771471
Validation loss decreased (0.341982 --> 0.339966).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 10.19353461265564
Epoch: 14, Steps: 60 | Train Loss: 0.2224996 Vali Loss: 0.3377579 Test Loss: 0.3762632
Validation loss decreased (0.339966 --> 0.337758).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.756187915802002
Epoch: 15, Steps: 60 | Train Loss: 0.2164935 Vali Loss: 0.3358575 Test Loss: 0.3754286
Validation loss decreased (0.337758 --> 0.335858).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.596291065216064
Epoch: 16, Steps: 60 | Train Loss: 0.2111164 Vali Loss: 0.3342285 Test Loss: 0.3745472
Validation loss decreased (0.335858 --> 0.334229).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.62072205543518
Epoch: 17, Steps: 60 | Train Loss: 0.2067531 Vali Loss: 0.3326063 Test Loss: 0.3737303
Validation loss decreased (0.334229 --> 0.332606).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 10.509228467941284
Epoch: 18, Steps: 60 | Train Loss: 0.2025826 Vali Loss: 0.3311454 Test Loss: 0.3728938
Validation loss decreased (0.332606 --> 0.331145).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 10.914254903793335
Epoch: 19, Steps: 60 | Train Loss: 0.1984012 Vali Loss: 0.3296780 Test Loss: 0.3723323
Validation loss decreased (0.331145 --> 0.329678).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 10.353982210159302
Epoch: 20, Steps: 60 | Train Loss: 0.1947686 Vali Loss: 0.3284117 Test Loss: 0.3716643
Validation loss decreased (0.329678 --> 0.328412).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 10.18564772605896
Epoch: 21, Steps: 60 | Train Loss: 0.1914572 Vali Loss: 0.3270485 Test Loss: 0.3711030
Validation loss decreased (0.328412 --> 0.327049).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.601959705352783
Epoch: 22, Steps: 60 | Train Loss: 0.1884117 Vali Loss: 0.3261195 Test Loss: 0.3705539
Validation loss decreased (0.327049 --> 0.326119).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 9.934507131576538
Epoch: 23, Steps: 60 | Train Loss: 0.1856673 Vali Loss: 0.3248470 Test Loss: 0.3699881
Validation loss decreased (0.326119 --> 0.324847).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.543363571166992
Epoch: 24, Steps: 60 | Train Loss: 0.1827355 Vali Loss: 0.3239642 Test Loss: 0.3695048
Validation loss decreased (0.324847 --> 0.323964).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.862957239151001
Epoch: 25, Steps: 60 | Train Loss: 0.1812314 Vali Loss: 0.3228785 Test Loss: 0.3689981
Validation loss decreased (0.323964 --> 0.322879).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 10.264437437057495
Epoch: 26, Steps: 60 | Train Loss: 0.1786750 Vali Loss: 0.3219979 Test Loss: 0.3685990
Validation loss decreased (0.322879 --> 0.321998).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 10.509727001190186
Epoch: 27, Steps: 60 | Train Loss: 0.1771497 Vali Loss: 0.3211747 Test Loss: 0.3682202
Validation loss decreased (0.321998 --> 0.321175).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.754914999008179
Epoch: 28, Steps: 60 | Train Loss: 0.1751240 Vali Loss: 0.3203598 Test Loss: 0.3677838
Validation loss decreased (0.321175 --> 0.320360).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 10.078130722045898
Epoch: 29, Steps: 60 | Train Loss: 0.1735988 Vali Loss: 0.3195608 Test Loss: 0.3674617
Validation loss decreased (0.320360 --> 0.319561).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 10.278125047683716
Epoch: 30, Steps: 60 | Train Loss: 0.1720200 Vali Loss: 0.3190534 Test Loss: 0.3671193
Validation loss decreased (0.319561 --> 0.319053).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 10.182180643081665
Epoch: 31, Steps: 60 | Train Loss: 0.1705368 Vali Loss: 0.3181488 Test Loss: 0.3668159
Validation loss decreased (0.319053 --> 0.318149).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 10.479518175125122
Epoch: 32, Steps: 60 | Train Loss: 0.1694224 Vali Loss: 0.3176308 Test Loss: 0.3665299
Validation loss decreased (0.318149 --> 0.317631).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 10.170595407485962
Epoch: 33, Steps: 60 | Train Loss: 0.1681103 Vali Loss: 0.3171782 Test Loss: 0.3662386
Validation loss decreased (0.317631 --> 0.317178).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 11.002689838409424
Epoch: 34, Steps: 60 | Train Loss: 0.1669600 Vali Loss: 0.3165509 Test Loss: 0.3660296
Validation loss decreased (0.317178 --> 0.316551).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 11.254633665084839
Epoch: 35, Steps: 60 | Train Loss: 0.1656879 Vali Loss: 0.3158225 Test Loss: 0.3657476
Validation loss decreased (0.316551 --> 0.315822).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 10.754207372665405
Epoch: 36, Steps: 60 | Train Loss: 0.1646299 Vali Loss: 0.3154874 Test Loss: 0.3655607
Validation loss decreased (0.315822 --> 0.315487).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 11.437474250793457
Epoch: 37, Steps: 60 | Train Loss: 0.1639466 Vali Loss: 0.3149220 Test Loss: 0.3653341
Validation loss decreased (0.315487 --> 0.314922).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 10.24911117553711
Epoch: 38, Steps: 60 | Train Loss: 0.1630490 Vali Loss: 0.3145966 Test Loss: 0.3651320
Validation loss decreased (0.314922 --> 0.314597).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 9.926539182662964
Epoch: 39, Steps: 60 | Train Loss: 0.1624021 Vali Loss: 0.3140511 Test Loss: 0.3649707
Validation loss decreased (0.314597 --> 0.314051).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 9.270665884017944
Epoch: 40, Steps: 60 | Train Loss: 0.1614848 Vali Loss: 0.3136921 Test Loss: 0.3647602
Validation loss decreased (0.314051 --> 0.313692).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 7.775085687637329
Epoch: 41, Steps: 60 | Train Loss: 0.1607696 Vali Loss: 0.3133493 Test Loss: 0.3646741
Validation loss decreased (0.313692 --> 0.313349).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 10.20169997215271
Epoch: 42, Steps: 60 | Train Loss: 0.1602484 Vali Loss: 0.3130389 Test Loss: 0.3644877
Validation loss decreased (0.313349 --> 0.313039).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 9.314647912979126
Epoch: 43, Steps: 60 | Train Loss: 0.1593284 Vali Loss: 0.3126297 Test Loss: 0.3643384
Validation loss decreased (0.313039 --> 0.312630).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 8.800795078277588
Epoch: 44, Steps: 60 | Train Loss: 0.1585048 Vali Loss: 0.3123672 Test Loss: 0.3641692
Validation loss decreased (0.312630 --> 0.312367).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 9.213956594467163
Epoch: 45, Steps: 60 | Train Loss: 0.1577804 Vali Loss: 0.3120352 Test Loss: 0.3640774
Validation loss decreased (0.312367 --> 0.312035).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.283548355102539
Epoch: 46, Steps: 60 | Train Loss: 0.1575657 Vali Loss: 0.3118007 Test Loss: 0.3639748
Validation loss decreased (0.312035 --> 0.311801).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 9.47492504119873
Epoch: 47, Steps: 60 | Train Loss: 0.1571745 Vali Loss: 0.3115519 Test Loss: 0.3638625
Validation loss decreased (0.311801 --> 0.311552).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 9.486814260482788
Epoch: 48, Steps: 60 | Train Loss: 0.1570129 Vali Loss: 0.3112846 Test Loss: 0.3637549
Validation loss decreased (0.311552 --> 0.311285).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 9.261065006256104
Epoch: 49, Steps: 60 | Train Loss: 0.1560025 Vali Loss: 0.3110180 Test Loss: 0.3636668
Validation loss decreased (0.311285 --> 0.311018).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 9.345409870147705
Epoch: 50, Steps: 60 | Train Loss: 0.1556990 Vali Loss: 0.3107873 Test Loss: 0.3635444
Validation loss decreased (0.311018 --> 0.310787).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20290816.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.79350233078003
Epoch: 1, Steps: 60 | Train Loss: 0.5316890 Vali Loss: 0.2938342 Test Loss: 0.3555502
Validation loss decreased (inf --> 0.293834).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.145197868347168
Epoch: 2, Steps: 60 | Train Loss: 0.5210791 Vali Loss: 0.2887386 Test Loss: 0.3544977
Validation loss decreased (0.293834 --> 0.288739).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.709887981414795
Epoch: 3, Steps: 60 | Train Loss: 0.5164044 Vali Loss: 0.2857650 Test Loss: 0.3543650
Validation loss decreased (0.288739 --> 0.285765).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 10.321186780929565
Epoch: 4, Steps: 60 | Train Loss: 0.5150574 Vali Loss: 0.2852151 Test Loss: 0.3543749
Validation loss decreased (0.285765 --> 0.285215).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.170747756958008
Epoch: 5, Steps: 60 | Train Loss: 0.5156375 Vali Loss: 0.2847402 Test Loss: 0.3540854
Validation loss decreased (0.285215 --> 0.284740).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.615565299987793
Epoch: 6, Steps: 60 | Train Loss: 0.5124494 Vali Loss: 0.2837462 Test Loss: 0.3539777
Validation loss decreased (0.284740 --> 0.283746).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.683568954467773
Epoch: 7, Steps: 60 | Train Loss: 0.5122283 Vali Loss: 0.2834114 Test Loss: 0.3538360
Validation loss decreased (0.283746 --> 0.283411).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.722018718719482
Epoch: 8, Steps: 60 | Train Loss: 0.5120769 Vali Loss: 0.2825988 Test Loss: 0.3539125
Validation loss decreased (0.283411 --> 0.282599).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.929995775222778
Epoch: 9, Steps: 60 | Train Loss: 0.5125549 Vali Loss: 0.2826394 Test Loss: 0.3534193
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.189931631088257
Epoch: 10, Steps: 60 | Train Loss: 0.5101715 Vali Loss: 0.2826283 Test Loss: 0.3532533
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.836668491363525
Epoch: 11, Steps: 60 | Train Loss: 0.5101648 Vali Loss: 0.2819878 Test Loss: 0.3533988
Validation loss decreased (0.282599 --> 0.281988).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 9.524842977523804
Epoch: 12, Steps: 60 | Train Loss: 0.5118271 Vali Loss: 0.2815797 Test Loss: 0.3533731
Validation loss decreased (0.281988 --> 0.281580).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.549377679824829
Epoch: 13, Steps: 60 | Train Loss: 0.5114166 Vali Loss: 0.2816389 Test Loss: 0.3533697
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.320720434188843
Epoch: 14, Steps: 60 | Train Loss: 0.5092045 Vali Loss: 0.2817847 Test Loss: 0.3531438
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.713199377059937
Epoch: 15, Steps: 60 | Train Loss: 0.5093917 Vali Loss: 0.2816418 Test Loss: 0.3530149
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3329372704029083, mae:0.37536582350730896, rse:0.4627240300178528, corr:[0.2663552  0.26914793 0.2685938  0.2672209  0.2667679  0.26686627
 0.266595   0.26547715 0.26424176 0.26311204 0.2620685  0.260841
 0.25959936 0.25854254 0.25765568 0.2570935  0.25662556 0.25594532
 0.25490522 0.2538014  0.25273368 0.25158158 0.25009567 0.24812992
 0.24612546 0.24441403 0.24312545 0.24195892 0.24072245 0.2393431
 0.23793542 0.23645967 0.2351678  0.23395848 0.23273608 0.23127379
 0.22979508 0.22880898 0.22842321 0.22806123 0.22734343 0.226239
 0.22498773 0.22397222 0.22330602 0.22256485 0.22135921 0.21952592
 0.21747105 0.21570219 0.21434668 0.21300004 0.21167803 0.21022302
 0.20827459 0.20622237 0.20426981 0.20243417 0.20107661 0.20015398
 0.1996323  0.19909343 0.19878682 0.19884875 0.198679   0.19824181
 0.19722043 0.19584131 0.19479261 0.19442579 0.19430572 0.19390762
 0.19272459 0.19089685 0.18907315 0.18773027 0.18733737 0.18737186
 0.1871766  0.18619925 0.1852608  0.18442027 0.18376772 0.1834294
 0.1834306  0.18363266 0.18351182 0.18290326 0.18182021 0.1808408
 0.18019581 0.17967035 0.17955656 0.17943902 0.17904672 0.17834108
 0.17742567 0.17668054 0.17631274 0.17615327 0.17594652 0.17533888
 0.17451283 0.17342372 0.17265585 0.17238605 0.1724988  0.17270677
 0.17197514 0.17048126 0.16869085 0.16750275 0.16678457 0.16657892
 0.1664447  0.16617975 0.1660267  0.16565469 0.16499697 0.16358206
 0.16163346 0.15939409 0.15773775 0.15683408 0.15612309 0.15524139
 0.15426926 0.15318428 0.15238744 0.1517993  0.15126029 0.15020248
 0.14895964 0.14798583 0.1475733  0.14763336 0.14753804 0.14706449
 0.14636397 0.14580473 0.14558703 0.1451991  0.14431824 0.14283048
 0.1409265  0.1392793  0.13791941 0.13656059 0.13513829 0.13407685
 0.13390484 0.13376205 0.13355666 0.13276951 0.13144144 0.13021597
 0.13022898 0.13120262 0.13225205 0.13284594 0.13237731 0.13174891
 0.13134696 0.1309899  0.13057807 0.13010155 0.12999544 0.12965406
 0.12896322 0.12770802 0.12678841 0.12623698 0.12610354 0.12504295
 0.12297364 0.12020072 0.11843909 0.11859719 0.11872502 0.11812276
 0.11670244 0.116065   0.11746307 0.11957057 0.1195578  0.11799476
 0.11734558 0.11872514 0.12112681 0.12090504 0.11938507 0.1228321 ]
