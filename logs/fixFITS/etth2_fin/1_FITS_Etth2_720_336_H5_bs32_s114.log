Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  17814720.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6940817
	speed: 0.1327s/iter; left time: 769.9714s
Epoch: 1 cost time: 15.994787693023682
Epoch: 1, Steps: 118 | Train Loss: 0.7898469 Vali Loss: 0.4698007 Test Loss: 0.3770009
Validation loss decreased (inf --> 0.469801).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6168374
	speed: 0.3477s/iter; left time: 1976.0532s
Epoch: 2 cost time: 17.51484751701355
Epoch: 2, Steps: 118 | Train Loss: 0.6702520 Vali Loss: 0.4270595 Test Loss: 0.3660867
Validation loss decreased (0.469801 --> 0.427059).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.8363158
	speed: 0.3032s/iter; left time: 1687.1088s
Epoch: 3 cost time: 13.315001010894775
Epoch: 3, Steps: 118 | Train Loss: 0.6491932 Vali Loss: 0.4135100 Test Loss: 0.3630051
Validation loss decreased (0.427059 --> 0.413510).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6230795
	speed: 0.3553s/iter; left time: 1935.5619s
Epoch: 4 cost time: 16.170842170715332
Epoch: 4, Steps: 118 | Train Loss: 0.6396183 Vali Loss: 0.4041484 Test Loss: 0.3617299
Validation loss decreased (0.413510 --> 0.404148).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6228235
	speed: 0.3594s/iter; left time: 1915.3727s
Epoch: 5 cost time: 17.510338306427002
Epoch: 5, Steps: 118 | Train Loss: 0.6340452 Vali Loss: 0.4000193 Test Loss: 0.3607035
Validation loss decreased (0.404148 --> 0.400019).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.7239926
	speed: 0.3532s/iter; left time: 1840.6731s
Epoch: 6 cost time: 16.61043882369995
Epoch: 6, Steps: 118 | Train Loss: 0.6286558 Vali Loss: 0.3964543 Test Loss: 0.3606851
Validation loss decreased (0.400019 --> 0.396454).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.7605563
	speed: 0.3563s/iter; left time: 1814.4324s
Epoch: 7 cost time: 16.744945526123047
Epoch: 7, Steps: 118 | Train Loss: 0.6262525 Vali Loss: 0.3924581 Test Loss: 0.3605036
Validation loss decreased (0.396454 --> 0.392458).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.7573627
	speed: 0.3247s/iter; left time: 1615.2361s
Epoch: 8 cost time: 14.754045486450195
Epoch: 8, Steps: 118 | Train Loss: 0.6238694 Vali Loss: 0.3893772 Test Loss: 0.3602916
Validation loss decreased (0.392458 --> 0.389377).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5988926
	speed: 0.3316s/iter; left time: 1610.5509s
Epoch: 9 cost time: 18.026843786239624
Epoch: 9, Steps: 118 | Train Loss: 0.6233855 Vali Loss: 0.3872618 Test Loss: 0.3600729
Validation loss decreased (0.389377 --> 0.387262).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5809360
	speed: 0.4229s/iter; left time: 2004.3554s
Epoch: 10 cost time: 20.062847137451172
Epoch: 10, Steps: 118 | Train Loss: 0.6207616 Vali Loss: 0.3871793 Test Loss: 0.3600492
Validation loss decreased (0.387262 --> 0.387179).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6245927
	speed: 0.4630s/iter; left time: 2139.4059s
Epoch: 11 cost time: 23.701119899749756
Epoch: 11, Steps: 118 | Train Loss: 0.6193606 Vali Loss: 0.3862083 Test Loss: 0.3600921
Validation loss decreased (0.387179 --> 0.386208).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.8339370
	speed: 0.5165s/iter; left time: 2325.7480s
Epoch: 12 cost time: 23.880676984786987
Epoch: 12, Steps: 118 | Train Loss: 0.6190230 Vali Loss: 0.3862385 Test Loss: 0.3599409
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.9505745
	speed: 0.4611s/iter; left time: 2021.7466s
Epoch: 13 cost time: 21.934085845947266
Epoch: 13, Steps: 118 | Train Loss: 0.6186197 Vali Loss: 0.3835494 Test Loss: 0.3596274
Validation loss decreased (0.386208 --> 0.383549).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.8128035
	speed: 0.4393s/iter; left time: 1874.3451s
Epoch: 14 cost time: 20.798611164093018
Epoch: 14, Steps: 118 | Train Loss: 0.6158156 Vali Loss: 0.3837073 Test Loss: 0.3596818
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5819401
	speed: 0.4395s/iter; left time: 1823.4996s
Epoch: 15 cost time: 19.477280616760254
Epoch: 15, Steps: 118 | Train Loss: 0.6162843 Vali Loss: 0.3839141 Test Loss: 0.3595157
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.7627614
	speed: 0.4217s/iter; left time: 1699.9997s
Epoch: 16 cost time: 20.398013591766357
Epoch: 16, Steps: 118 | Train Loss: 0.6144655 Vali Loss: 0.3824154 Test Loss: 0.3594409
Validation loss decreased (0.383549 --> 0.382415).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5117151
	speed: 0.4317s/iter; left time: 1689.2343s
Epoch: 17 cost time: 20.634852170944214
Epoch: 17, Steps: 118 | Train Loss: 0.6160340 Vali Loss: 0.3834434 Test Loss: 0.3593875
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.6218002
	speed: 0.4331s/iter; left time: 1643.5020s
Epoch: 18 cost time: 20.22894859313965
Epoch: 18, Steps: 118 | Train Loss: 0.6153035 Vali Loss: 0.3821043 Test Loss: 0.3593118
Validation loss decreased (0.382415 --> 0.382104).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4585152
	speed: 0.4453s/iter; left time: 1637.5498s
Epoch: 19 cost time: 21.819721937179565
Epoch: 19, Steps: 118 | Train Loss: 0.6157129 Vali Loss: 0.3817571 Test Loss: 0.3592965
Validation loss decreased (0.382104 --> 0.381757).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.8134046
	speed: 0.4318s/iter; left time: 1536.8385s
Epoch: 20 cost time: 19.4017117023468
Epoch: 20, Steps: 118 | Train Loss: 0.6146970 Vali Loss: 0.3817971 Test Loss: 0.3593115
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.6207224
	speed: 0.4425s/iter; left time: 1522.6625s
Epoch: 21 cost time: 19.454970836639404
Epoch: 21, Steps: 118 | Train Loss: 0.6156206 Vali Loss: 0.3815424 Test Loss: 0.3592543
Validation loss decreased (0.381757 --> 0.381542).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.6585619
	speed: 0.4137s/iter; left time: 1374.7002s
Epoch: 22 cost time: 19.587088108062744
Epoch: 22, Steps: 118 | Train Loss: 0.6149273 Vali Loss: 0.3808233 Test Loss: 0.3593341
Validation loss decreased (0.381542 --> 0.380823).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.6975653
	speed: 0.4088s/iter; left time: 1310.2141s
Epoch: 23 cost time: 19.203928470611572
Epoch: 23, Steps: 118 | Train Loss: 0.6150676 Vali Loss: 0.3810141 Test Loss: 0.3592507
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.6108336
	speed: 0.4106s/iter; left time: 1267.6761s
Epoch: 24 cost time: 19.85583734512329
Epoch: 24, Steps: 118 | Train Loss: 0.6147860 Vali Loss: 0.3782847 Test Loss: 0.3592967
Validation loss decreased (0.380823 --> 0.378285).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3371837
	speed: 0.3868s/iter; left time: 1148.4860s
Epoch: 25 cost time: 17.182035207748413
Epoch: 25, Steps: 118 | Train Loss: 0.6127449 Vali Loss: 0.3792166 Test Loss: 0.3593850
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.6185636
	speed: 0.3751s/iter; left time: 1069.5493s
Epoch: 26 cost time: 19.832019329071045
Epoch: 26, Steps: 118 | Train Loss: 0.6135662 Vali Loss: 0.3763440 Test Loss: 0.3593636
Validation loss decreased (0.378285 --> 0.376344).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4472734
	speed: 0.4038s/iter; left time: 1103.6947s
Epoch: 27 cost time: 19.350741386413574
Epoch: 27, Steps: 118 | Train Loss: 0.6131306 Vali Loss: 0.3788503 Test Loss: 0.3592170
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.6104320
	speed: 0.4425s/iter; left time: 1157.1665s
Epoch: 28 cost time: 22.622936725616455
Epoch: 28, Steps: 118 | Train Loss: 0.6126159 Vali Loss: 0.3802306 Test Loss: 0.3593787
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.5563154
	speed: 0.4868s/iter; left time: 1215.4959s
Epoch: 29 cost time: 23.479504823684692
Epoch: 29, Steps: 118 | Train Loss: 0.6141795 Vali Loss: 0.3787980 Test Loss: 0.3593113
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.35471221804618835, mae:0.39593660831451416, rse:0.4761870801448822, corr:[0.2582325  0.2636568  0.26106906 0.26144534 0.26226637 0.26080897
 0.2594562  0.2595532  0.25940764 0.25779262 0.25618112 0.2553246
 0.25453207 0.2530476  0.25195178 0.2517792  0.2517197  0.2507765
 0.2493475  0.248373   0.24770634 0.24679159 0.2453977  0.24414022
 0.24294195 0.24161403 0.24014802 0.23915769 0.2386376  0.23792112
 0.23677704 0.23574434 0.23517025 0.23436542 0.23299743 0.23182286
 0.23137136 0.23106249 0.23009713 0.22893947 0.22841837 0.2282549
 0.22760756 0.22649297 0.22558182 0.225004   0.22377874 0.22155926
 0.21947953 0.21824932 0.21717995 0.21558425 0.213799   0.21208102
 0.21030618 0.20854507 0.20677555 0.20522976 0.20415998 0.20342724
 0.20283033 0.20239697 0.20245454 0.2027395  0.20238505 0.20163417
 0.2008488  0.20055066 0.20036322 0.19977398 0.19872478 0.19784139
 0.19722512 0.19631958 0.1949033  0.19345205 0.19278441 0.19262055
 0.19227648 0.19161776 0.1913522  0.19119217 0.19065823 0.18976495
 0.18895245 0.18841465 0.18810922 0.18799987 0.18800241 0.18789999
 0.1874359  0.18668471 0.18643618 0.18664566 0.18678232 0.18651178
 0.18611057 0.18574907 0.18514207 0.18382631 0.18251085 0.18204738
 0.1825228  0.18276203 0.18251543 0.18208708 0.18194553 0.181571
 0.18046597 0.17952164 0.17928167 0.17942825 0.17890498 0.1780254
 0.17752765 0.17763864 0.17757715 0.1764371  0.17485021 0.17354675
 0.17266743 0.17162988 0.17036375 0.1693122  0.16865954 0.16832444
 0.16781719 0.16722871 0.16672204 0.1660965  0.16528445 0.16444731
 0.16391408 0.16334935 0.16266681 0.16222456 0.16220151 0.16210878
 0.16117701 0.15984863 0.15921791 0.15920927 0.15862522 0.15679003
 0.15464033 0.15346558 0.15278573 0.15153618 0.14992037 0.14904787
 0.14890374 0.14829223 0.14720805 0.14654006 0.14675097 0.14681444
 0.14588247 0.1446648  0.14451249 0.14507766 0.1445839  0.14314657
 0.14219765 0.14251664 0.14304985 0.14264664 0.14190602 0.14165011
 0.14155537 0.14017005 0.13777411 0.13598144 0.13559556 0.13506413
 0.13357833 0.13181324 0.13139012 0.1317875  0.1310402  0.12927301
 0.12814872 0.12855317 0.12917086 0.12860559 0.12699059 0.12606862
 0.12607758 0.12612782 0.12624352 0.1267985  0.12769751 0.1274713
 0.12614518 0.12508558 0.12485167 0.12484293 0.12423492 0.123468
 0.12333544 0.12325325 0.12251135 0.12142188 0.12119345 0.12184733
 0.12221146 0.1219238  0.12151347 0.12182325 0.12215772 0.12181771
 0.1211675  0.12128977 0.12198884 0.12207896 0.12138507 0.12054903
 0.11996324 0.11902148 0.11755225 0.11668365 0.11676542 0.11719535
 0.11669738 0.11624809 0.1166712  0.11757571 0.11761361 0.11640532
 0.1149973  0.11434397 0.11403167 0.1141767  0.11500006 0.11656664
 0.11753989 0.1172791  0.11675784 0.11724617 0.11810255 0.11768534
 0.116103   0.11523888 0.1158619  0.11645144 0.11539058 0.11395418
 0.1144693  0.11605318 0.11644575 0.11526366 0.11455175 0.11556294
 0.11669503 0.11709253 0.11715733 0.11836948 0.11950193 0.11979829
 0.11929942 0.11947516 0.12041701 0.12082761 0.12111298 0.12174601
 0.12280951 0.1230166  0.12255523 0.12260193 0.12332202 0.12347402
 0.1220993  0.12063438 0.12118113 0.12271886 0.12359781 0.12281692
 0.12241621 0.12333864 0.12409388 0.12345114 0.12252127 0.12292458
 0.12397052 0.12401613 0.12290738 0.12247396 0.123398   0.12389204
 0.12290622 0.12187457 0.12200548 0.12228452 0.12103117 0.1190264
 0.11871261 0.11944512 0.11898287 0.11714402 0.11648834 0.11730652
 0.11724313 0.11617821 0.11606156 0.11828595 0.11975807 0.11914907
 0.11730972 0.11754248 0.11945184 0.12053181 0.11989979 0.11924809
 0.11963491 0.11912976 0.11771785 0.11683813 0.1171551  0.11686312
 0.11523832 0.11460693 0.11602433 0.11788053 0.11729071 0.11610938
 0.11680465 0.11896151 0.11991694 0.1198262  0.12136952 0.12366507
 0.1226287  0.11928533 0.12012645 0.12469383 0.12352794 0.10934052]
