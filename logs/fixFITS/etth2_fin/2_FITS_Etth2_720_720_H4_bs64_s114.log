Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32177152.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.910847663879395
Epoch: 1, Steps: 56 | Train Loss: 0.8417158 Vali Loss: 0.8353835 Test Loss: 0.4804295
Validation loss decreased (inf --> 0.835384).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.539294719696045
Epoch: 2, Steps: 56 | Train Loss: 0.6830677 Vali Loss: 0.7765285 Test Loss: 0.4401139
Validation loss decreased (0.835384 --> 0.776528).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.184242725372314
Epoch: 3, Steps: 56 | Train Loss: 0.6075099 Vali Loss: 0.7435510 Test Loss: 0.4224230
Validation loss decreased (0.776528 --> 0.743551).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.794618844985962
Epoch: 4, Steps: 56 | Train Loss: 0.5685769 Vali Loss: 0.7243325 Test Loss: 0.4140650
Validation loss decreased (0.743551 --> 0.724332).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.792381763458252
Epoch: 5, Steps: 56 | Train Loss: 0.5446489 Vali Loss: 0.7180061 Test Loss: 0.4095119
Validation loss decreased (0.724332 --> 0.718006).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.731198072433472
Epoch: 6, Steps: 56 | Train Loss: 0.5270405 Vali Loss: 0.7098573 Test Loss: 0.4067297
Validation loss decreased (0.718006 --> 0.709857).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.670511722564697
Epoch: 7, Steps: 56 | Train Loss: 0.5142410 Vali Loss: 0.7014766 Test Loss: 0.4045421
Validation loss decreased (0.709857 --> 0.701477).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.974520683288574
Epoch: 8, Steps: 56 | Train Loss: 0.5041089 Vali Loss: 0.7047244 Test Loss: 0.4028775
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.75515103340149
Epoch: 9, Steps: 56 | Train Loss: 0.4949522 Vali Loss: 0.6961182 Test Loss: 0.4014756
Validation loss decreased (0.701477 --> 0.696118).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.922407865524292
Epoch: 10, Steps: 56 | Train Loss: 0.4877785 Vali Loss: 0.6962790 Test Loss: 0.4001950
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.729703426361084
Epoch: 11, Steps: 56 | Train Loss: 0.4802343 Vali Loss: 0.6872272 Test Loss: 0.3990386
Validation loss decreased (0.696118 --> 0.687227).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 10.090760946273804
Epoch: 12, Steps: 56 | Train Loss: 0.4750307 Vali Loss: 0.6873041 Test Loss: 0.3981201
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 9.778590202331543
Epoch: 13, Steps: 56 | Train Loss: 0.4711595 Vali Loss: 0.6834909 Test Loss: 0.3971513
Validation loss decreased (0.687227 --> 0.683491).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.91858458518982
Epoch: 14, Steps: 56 | Train Loss: 0.4670333 Vali Loss: 0.6834711 Test Loss: 0.3963175
Validation loss decreased (0.683491 --> 0.683471).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 9.87991213798523
Epoch: 15, Steps: 56 | Train Loss: 0.4622039 Vali Loss: 0.6811182 Test Loss: 0.3955883
Validation loss decreased (0.683471 --> 0.681118).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.44615364074707
Epoch: 16, Steps: 56 | Train Loss: 0.4588421 Vali Loss: 0.6772654 Test Loss: 0.3948759
Validation loss decreased (0.681118 --> 0.677265).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 10.016176223754883
Epoch: 17, Steps: 56 | Train Loss: 0.4556504 Vali Loss: 0.6819358 Test Loss: 0.3942312
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.844294309616089
Epoch: 18, Steps: 56 | Train Loss: 0.4542521 Vali Loss: 0.6770487 Test Loss: 0.3936475
Validation loss decreased (0.677265 --> 0.677049).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.292871236801147
Epoch: 19, Steps: 56 | Train Loss: 0.4520541 Vali Loss: 0.6720715 Test Loss: 0.3930667
Validation loss decreased (0.677049 --> 0.672072).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 9.538578748703003
Epoch: 20, Steps: 56 | Train Loss: 0.4492662 Vali Loss: 0.6741557 Test Loss: 0.3925930
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.749186754226685
Epoch: 21, Steps: 56 | Train Loss: 0.4475699 Vali Loss: 0.6712701 Test Loss: 0.3921640
Validation loss decreased (0.672072 --> 0.671270).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.481513261795044
Epoch: 22, Steps: 56 | Train Loss: 0.4459183 Vali Loss: 0.6732116 Test Loss: 0.3917080
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.95283842086792
Epoch: 23, Steps: 56 | Train Loss: 0.4439808 Vali Loss: 0.6712357 Test Loss: 0.3913118
Validation loss decreased (0.671270 --> 0.671236).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 6.367839574813843
Epoch: 24, Steps: 56 | Train Loss: 0.4431379 Vali Loss: 0.6706901 Test Loss: 0.3909878
Validation loss decreased (0.671236 --> 0.670690).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 7.5271759033203125
Epoch: 25, Steps: 56 | Train Loss: 0.4411828 Vali Loss: 0.6714365 Test Loss: 0.3906265
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 8.642712354660034
Epoch: 26, Steps: 56 | Train Loss: 0.4400375 Vali Loss: 0.6694183 Test Loss: 0.3903027
Validation loss decreased (0.670690 --> 0.669418).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 9.295831680297852
Epoch: 27, Steps: 56 | Train Loss: 0.4390210 Vali Loss: 0.6691751 Test Loss: 0.3900384
Validation loss decreased (0.669418 --> 0.669175).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 7.714336395263672
Epoch: 28, Steps: 56 | Train Loss: 0.4372441 Vali Loss: 0.6698018 Test Loss: 0.3897972
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 7.777883291244507
Epoch: 29, Steps: 56 | Train Loss: 0.4365781 Vali Loss: 0.6666747 Test Loss: 0.3895107
Validation loss decreased (0.669175 --> 0.666675).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.679628133773804
Epoch: 30, Steps: 56 | Train Loss: 0.4365778 Vali Loss: 0.6672580 Test Loss: 0.3893200
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 9.892566919326782
Epoch: 31, Steps: 56 | Train Loss: 0.4350910 Vali Loss: 0.6643916 Test Loss: 0.3891245
Validation loss decreased (0.666675 --> 0.664392).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.734487295150757
Epoch: 32, Steps: 56 | Train Loss: 0.4340870 Vali Loss: 0.6661914 Test Loss: 0.3889069
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.615412473678589
Epoch: 33, Steps: 56 | Train Loss: 0.4341931 Vali Loss: 0.6655418 Test Loss: 0.3887392
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 9.529781818389893
Epoch: 34, Steps: 56 | Train Loss: 0.4334736 Vali Loss: 0.6615689 Test Loss: 0.3885547
Validation loss decreased (0.664392 --> 0.661569).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 10.070706367492676
Epoch: 35, Steps: 56 | Train Loss: 0.4332793 Vali Loss: 0.6630744 Test Loss: 0.3883979
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 10.472508907318115
Epoch: 36, Steps: 56 | Train Loss: 0.4319108 Vali Loss: 0.6661770 Test Loss: 0.3882712
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 9.817731380462646
Epoch: 37, Steps: 56 | Train Loss: 0.4323279 Vali Loss: 0.6642376 Test Loss: 0.3881407
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32177152.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 10.399360179901123
Epoch: 1, Steps: 56 | Train Loss: 0.8160602 Vali Loss: 0.6569537 Test Loss: 0.3851251
Validation loss decreased (inf --> 0.656954).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 10.275895595550537
Epoch: 2, Steps: 56 | Train Loss: 0.8101794 Vali Loss: 0.6488953 Test Loss: 0.3832377
Validation loss decreased (0.656954 --> 0.648895).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.59909701347351
Epoch: 3, Steps: 56 | Train Loss: 0.8056947 Vali Loss: 0.6489596 Test Loss: 0.3820488
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.528592586517334
Epoch: 4, Steps: 56 | Train Loss: 0.8040250 Vali Loss: 0.6472341 Test Loss: 0.3814487
Validation loss decreased (0.648895 --> 0.647234).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.906757354736328
Epoch: 5, Steps: 56 | Train Loss: 0.8038758 Vali Loss: 0.6469895 Test Loss: 0.3806867
Validation loss decreased (0.647234 --> 0.646989).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.699573993682861
Epoch: 6, Steps: 56 | Train Loss: 0.8014071 Vali Loss: 0.6414498 Test Loss: 0.3805455
Validation loss decreased (0.646989 --> 0.641450).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.705340385437012
Epoch: 7, Steps: 56 | Train Loss: 0.8015060 Vali Loss: 0.6445098 Test Loss: 0.3803754
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 9.477474689483643
Epoch: 8, Steps: 56 | Train Loss: 0.8016372 Vali Loss: 0.6443393 Test Loss: 0.3802303
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 9.57206106185913
Epoch: 9, Steps: 56 | Train Loss: 0.8010011 Vali Loss: 0.6442763 Test Loss: 0.3802737
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.37913066148757935, mae:0.4236513376235962, rse:0.4921530485153198, corr:[ 2.19326973e-01  2.21326694e-01  2.21176624e-01  2.19792038e-01
  2.18883470e-01  2.18537942e-01  2.18140975e-01  2.17153490e-01
  2.15828180e-01  2.14437753e-01  2.13245884e-01  2.12057486e-01
  2.10856795e-01  2.09697753e-01  2.08653808e-01  2.07807034e-01
  2.07029372e-01  2.06095532e-01  2.05056503e-01  2.04086795e-01
  2.03114465e-01  2.02059582e-01  2.00791627e-01  1.99443176e-01
  1.98271483e-01  1.97341815e-01  1.96497515e-01  1.95609316e-01
  1.94757432e-01  1.93883002e-01  1.93018124e-01  1.92119479e-01
  1.91301659e-01  1.90548867e-01  1.89811036e-01  1.88885495e-01
  1.87676191e-01  1.86553702e-01  1.85916930e-01  1.85711667e-01
  1.85602859e-01  1.85128003e-01  1.84107229e-01  1.82976514e-01
  1.82106256e-01  1.81478739e-01  1.80764809e-01  1.79404229e-01
  1.77705586e-01  1.76250458e-01  1.75370112e-01  1.74879625e-01
  1.74570054e-01  1.74134046e-01  1.73305109e-01  1.72404870e-01
  1.71728536e-01  1.71346858e-01  1.71138182e-01  1.70924276e-01
  1.70525208e-01  1.69845000e-01  1.69290394e-01  1.69234470e-01
  1.69360384e-01  1.69761524e-01  1.70041382e-01  1.70084089e-01
  1.69985399e-01  1.70011729e-01  1.70149997e-01  1.70117110e-01
  1.69716120e-01  1.68917984e-01  1.68123662e-01  1.67591229e-01
  1.67635396e-01  1.67963520e-01  1.68064147e-01  1.67735174e-01
  1.67340979e-01  1.67112306e-01  1.67017698e-01  1.67026743e-01
  1.67003348e-01  1.66858435e-01  1.66623354e-01  1.66509911e-01
  1.66557893e-01  1.66784644e-01  1.66969597e-01  1.66913509e-01
  1.66920528e-01  1.66940585e-01  1.67038038e-01  1.67092726e-01
  1.67108878e-01  1.67159855e-01  1.67101949e-01  1.66817635e-01
  1.66378692e-01  1.65732414e-01  1.65336877e-01  1.65309966e-01
  1.65748790e-01  1.66201532e-01  1.66407079e-01  1.66247919e-01
  1.65689334e-01  1.65112928e-01  1.64820835e-01  1.64797321e-01
  1.64720491e-01  1.64614171e-01  1.64530382e-01  1.64528787e-01
  1.64497226e-01  1.64096549e-01  1.63484037e-01  1.62702173e-01
  1.61928535e-01  1.61221594e-01  1.60565063e-01  1.59905478e-01
  1.59127206e-01  1.58462524e-01  1.57960221e-01  1.57559663e-01
  1.57187626e-01  1.56640187e-01  1.56148806e-01  1.55560523e-01
  1.54954627e-01  1.54261619e-01  1.53686509e-01  1.53466463e-01
  1.53294891e-01  1.53128967e-01  1.52793989e-01  1.52299881e-01
  1.51953548e-01  1.51608929e-01  1.51120350e-01  1.50207520e-01
  1.48707286e-01  1.47292227e-01  1.46361068e-01  1.45897582e-01
  1.45549685e-01  1.45156413e-01  1.44764066e-01  1.44405022e-01
  1.44253835e-01  1.44045159e-01  1.43493399e-01  1.42554551e-01
  1.41688555e-01  1.41329795e-01  1.41405374e-01  1.41457498e-01
  1.41010895e-01  1.40462801e-01  1.40217841e-01  1.40400499e-01
  1.40894711e-01  1.41095906e-01  1.40786499e-01  1.39961720e-01
  1.39240667e-01  1.38901234e-01  1.38870537e-01  1.38778895e-01
  1.38521180e-01  1.38171449e-01  1.37862548e-01  1.37570947e-01
  1.37164831e-01  1.36445105e-01  1.35331437e-01  1.34303540e-01
  1.33803636e-01  1.33863583e-01  1.34184808e-01  1.34234130e-01
  1.33904263e-01  1.33556604e-01  1.33361742e-01  1.33449689e-01
  1.33833975e-01  1.34479016e-01  1.35393813e-01  1.36004791e-01
  1.35989726e-01  1.35662660e-01  1.35373220e-01  1.35528520e-01
  1.36127383e-01  1.36681184e-01  1.37033969e-01  1.36897355e-01
  1.36520013e-01  1.36116162e-01  1.35946989e-01  1.35633260e-01
  1.35185540e-01  1.34975180e-01  1.35095105e-01  1.35526448e-01
  1.36079356e-01  1.36493310e-01  1.36716962e-01  1.36941090e-01
  1.37407601e-01  1.37938932e-01  1.38259932e-01  1.38085768e-01
  1.37596026e-01  1.36950627e-01  1.36271253e-01  1.35863885e-01
  1.35606796e-01  1.35829613e-01  1.36211053e-01  1.36741698e-01
  1.36836544e-01  1.36371106e-01  1.35741040e-01  1.35327056e-01
  1.35468245e-01  1.35891259e-01  1.36297598e-01  1.36578336e-01
  1.36808559e-01  1.37218952e-01  1.37880996e-01  1.38636455e-01
  1.39414340e-01  1.40153810e-01  1.40960842e-01  1.41770825e-01
  1.42371207e-01  1.42550051e-01  1.42519146e-01  1.42697349e-01
  1.42998353e-01  1.43215090e-01  1.43432796e-01  1.43577814e-01
  1.43879235e-01  1.44378200e-01  1.45004809e-01  1.45722553e-01
  1.46461412e-01  1.47275105e-01  1.47930071e-01  1.48440555e-01
  1.48561925e-01  1.48784116e-01  1.49651930e-01  1.51302651e-01
  1.53008968e-01  1.53872520e-01  1.53836608e-01  1.53421879e-01
  1.53809503e-01  1.55252218e-01  1.57017127e-01  1.57949045e-01
  1.57361165e-01  1.56072855e-01  1.55276746e-01  1.55579090e-01
  1.56647786e-01  1.57546073e-01  1.57844871e-01  1.57698378e-01
  1.57564566e-01  1.57992870e-01  1.58686608e-01  1.59345344e-01
  1.59902513e-01  1.60458192e-01  1.61006778e-01  1.61525190e-01
  1.61853760e-01  1.62086651e-01  1.62578106e-01  1.63261369e-01
  1.63992971e-01  1.64531916e-01  1.64633200e-01  1.64400786e-01
  1.63781166e-01  1.63080007e-01  1.62698895e-01  1.62844002e-01
  1.63406402e-01  1.64103910e-01  1.64683416e-01  1.65038183e-01
  1.65057376e-01  1.65162116e-01  1.65472701e-01  1.65866390e-01
  1.66031316e-01  1.66094914e-01  1.65971577e-01  1.66153148e-01
  1.66611835e-01  1.67176455e-01  1.67610392e-01  1.67674363e-01
  1.67659491e-01  1.67555541e-01  1.67550430e-01  1.67327613e-01
  1.66900992e-01  1.66468292e-01  1.66271031e-01  1.66397631e-01
  1.66472375e-01  1.66452080e-01  1.66228786e-01  1.66045725e-01
  1.65985927e-01  1.66164219e-01  1.66723058e-01  1.67127132e-01
  1.67233527e-01  1.67228803e-01  1.67396709e-01  1.68098047e-01
  1.68947861e-01  1.69541523e-01  1.69931948e-01  1.70197859e-01
  1.70409471e-01  1.70785278e-01  1.71347097e-01  1.71685219e-01
  1.71881348e-01  1.71977401e-01  1.72154799e-01  1.72207385e-01
  1.71991616e-01  1.71640873e-01  1.71181768e-01  1.71160027e-01
  1.71259671e-01  1.71317682e-01  1.71398968e-01  1.71793759e-01
  1.72616675e-01  1.73756599e-01  1.74624041e-01  1.74982369e-01
  1.74820229e-01  1.74671382e-01  1.75034836e-01  1.75978422e-01
  1.77070349e-01  1.77778780e-01  1.78027660e-01  1.77917346e-01
  1.77838475e-01  1.77980006e-01  1.78354353e-01  1.78851470e-01
  1.79222986e-01  1.79778576e-01  1.80347160e-01  1.80845529e-01
  1.80952147e-01  1.80646449e-01  1.80108830e-01  1.79646939e-01
  1.79528683e-01  1.79651454e-01  1.79646641e-01  1.79466754e-01
  1.79201707e-01  1.79069459e-01  1.79305375e-01  1.79975852e-01
  1.80627480e-01  1.80826813e-01  1.80528641e-01  1.79993108e-01
  1.79669693e-01  1.79897785e-01  1.80663943e-01  1.81670696e-01
  1.82480887e-01  1.82853028e-01  1.82922184e-01  1.82818130e-01
  1.82863027e-01  1.82819650e-01  1.82842895e-01  1.82847649e-01
  1.82524517e-01  1.81992024e-01  1.81472570e-01  1.81264549e-01
  1.81293234e-01  1.81358621e-01  1.81261018e-01  1.81029767e-01
  1.80669919e-01  1.80357859e-01  1.80093795e-01  1.79783136e-01
  1.79495871e-01  1.79256335e-01  1.79090783e-01  1.79110497e-01
  1.79242343e-01  1.79405779e-01  1.79548159e-01  1.79802626e-01
  1.79810449e-01  1.79383621e-01  1.78354636e-01  1.76875189e-01
  1.75292462e-01  1.73907146e-01  1.73079237e-01  1.72526106e-01
  1.72014102e-01  1.71404123e-01  1.70553073e-01  1.69724569e-01
  1.68916255e-01  1.68243289e-01  1.67947739e-01  1.67828292e-01
  1.67760938e-01  1.67619124e-01  1.67242140e-01  1.66706696e-01
  1.65825710e-01  1.65017560e-01  1.64332151e-01  1.63934335e-01
  1.63647413e-01  1.63600475e-01  1.63408712e-01  1.63002148e-01
  1.62280098e-01  1.61506131e-01  1.61047384e-01  1.61159992e-01
  1.61262333e-01  1.61055505e-01  1.60504505e-01  1.59872890e-01
  1.59533471e-01  1.59470975e-01  1.59551993e-01  1.59801245e-01
  1.59887865e-01  1.59649551e-01  1.59257740e-01  1.58982366e-01
  1.58554554e-01  1.58128574e-01  1.57632142e-01  1.56928256e-01
  1.56339481e-01  1.55871674e-01  1.55687511e-01  1.55496687e-01
  1.55136168e-01  1.54534876e-01  1.53968215e-01  1.53579965e-01
  1.53462753e-01  1.53661355e-01  1.53765887e-01  1.53335825e-01
  1.52137384e-01  1.50589049e-01  1.49488121e-01  1.49014369e-01
  1.49220869e-01  1.49506330e-01  1.49239361e-01  1.48153365e-01
  1.46806538e-01  1.45774692e-01  1.45375043e-01  1.45115137e-01
  1.44565970e-01  1.43725574e-01  1.42963186e-01  1.42472029e-01
  1.41974553e-01  1.41497925e-01  1.40965283e-01  1.40391946e-01
  1.39880836e-01  1.39339268e-01  1.38664857e-01  1.37525320e-01
  1.36045411e-01  1.34800613e-01  1.34157345e-01  1.33901119e-01
  1.33721828e-01  1.33478537e-01  1.32898778e-01  1.32206634e-01
  1.31531268e-01  1.30839169e-01  1.30041584e-01  1.28813192e-01
  1.27344996e-01  1.26030236e-01  1.25400886e-01  1.25353038e-01
  1.25552624e-01  1.25724643e-01  1.25470728e-01  1.24931492e-01
  1.24291077e-01  1.23651862e-01  1.22689314e-01  1.21169195e-01
  1.19172797e-01  1.17417239e-01  1.16370030e-01  1.15485400e-01
  1.14529438e-01  1.13347173e-01  1.12193152e-01  1.11302420e-01
  1.10631973e-01  1.09884962e-01  1.08674943e-01  1.06821440e-01
  1.04665309e-01  1.02834672e-01  1.01906896e-01  1.01695143e-01
  1.01546258e-01  1.01290636e-01  1.00712724e-01  1.00157544e-01
  9.97063518e-02  9.88690704e-02  9.77190360e-02  9.63386819e-02
  9.53179449e-02  9.47553515e-02  9.43756402e-02  9.36458781e-02
  9.24005955e-02  9.10107270e-02  8.99078324e-02  8.90347809e-02
  8.84909257e-02  8.76961797e-02  8.67521390e-02  8.58169273e-02
  8.53216648e-02  8.52448940e-02  8.52343142e-02  8.47544074e-02
  8.41378197e-02  8.36349949e-02  8.32967833e-02  8.29809457e-02
  8.22759792e-02  8.10403526e-02  7.91927427e-02  7.71143660e-02
  7.51886293e-02  7.40131289e-02  7.33669922e-02  7.28469118e-02
  7.22470805e-02  7.14863166e-02  7.06191510e-02  6.96492121e-02
  6.85841441e-02  6.75168857e-02  6.65333942e-02  6.59630671e-02
  6.55921623e-02  6.53133616e-02  6.49287030e-02  6.42434582e-02
  6.34131357e-02  6.26705661e-02  6.20561242e-02  6.17108792e-02
  6.13075309e-02  6.06238320e-02  5.95646910e-02  5.84818013e-02
  5.76194525e-02  5.68283051e-02  5.53609356e-02  5.34053929e-02
  5.12820557e-02  4.99632023e-02  4.95132022e-02  4.94256243e-02
  4.89024334e-02  4.78962064e-02  4.68204096e-02  4.57492210e-02
  4.49534468e-02  4.43211459e-02  4.37068492e-02  4.30410653e-02
  4.24966551e-02  4.26071584e-02  4.29522619e-02  4.31326441e-02
  4.28506136e-02  4.26101163e-02  4.25902009e-02  4.30416353e-02
  4.31389622e-02  4.21745144e-02  4.02579643e-02  3.83236334e-02
  3.72975618e-02  3.73763144e-02  3.76528166e-02  3.72851528e-02
  3.63998637e-02  3.57300676e-02  3.54364254e-02  3.53731997e-02
  3.49062681e-02  3.39120403e-02  3.31575051e-02  3.31222266e-02
  3.31006274e-02  3.28499749e-02  3.20918374e-02  3.09745576e-02
  3.04213967e-02  3.08019090e-02  3.12165711e-02  3.09407301e-02
  2.97932588e-02  2.84094065e-02  2.74459664e-02  2.69674417e-02
  2.70057321e-02  2.70023458e-02  2.66744867e-02  2.60943174e-02
  2.53335778e-02  2.44380683e-02  2.35126819e-02  2.27571931e-02
  2.22694948e-02  2.23532692e-02  2.30810605e-02  2.36930251e-02
  2.38796603e-02  2.36931872e-02  2.35222094e-02  2.35271789e-02
  2.32840739e-02  2.29444504e-02  2.26422511e-02  2.26601474e-02
  2.28351615e-02  2.28035655e-02  2.21915487e-02  2.06750818e-02
  1.94396842e-02  1.90087631e-02  1.98237039e-02  2.04707067e-02
  2.05641296e-02  1.99487451e-02  1.89597886e-02  1.81669332e-02
  1.78900901e-02  1.73116550e-02  1.66962445e-02  1.61253158e-02
  1.58603266e-02  1.60854422e-02  1.64613854e-02  1.59905460e-02
  1.51474923e-02  1.43582253e-02  1.40039511e-02  1.35489628e-02
  1.25096506e-02  1.07134711e-02  9.05668270e-03  7.85063300e-03
  7.46408431e-03  7.05168303e-03  6.28050137e-03  4.68908576e-03
  3.38184345e-03  3.24298581e-03  3.57733294e-03  3.46971513e-03
  2.01604282e-03 -5.40297362e-04 -1.76857540e-03 -1.01254054e-03
  7.04013219e-04  1.45668851e-03 -2.20843722e-04 -3.20669170e-03
 -4.56581637e-03 -2.12500175e-03  8.92674550e-04 -1.55382417e-03]
