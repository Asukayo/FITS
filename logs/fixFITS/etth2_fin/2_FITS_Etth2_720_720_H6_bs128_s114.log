Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  137682944.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.263734340667725
Epoch: 1, Steps: 28 | Train Loss: 0.9031798 Vali Loss: 0.8724060 Test Loss: 0.5189506
Validation loss decreased (inf --> 0.872406).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.465001583099365
Epoch: 2, Steps: 28 | Train Loss: 0.7745633 Vali Loss: 0.8204837 Test Loss: 0.4796878
Validation loss decreased (0.872406 --> 0.820484).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.0341997146606445
Epoch: 3, Steps: 28 | Train Loss: 0.7005139 Vali Loss: 0.7884268 Test Loss: 0.4563446
Validation loss decreased (0.820484 --> 0.788427).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.5205161571502686
Epoch: 4, Steps: 28 | Train Loss: 0.6532694 Vali Loss: 0.7693055 Test Loss: 0.4424542
Validation loss decreased (0.788427 --> 0.769305).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.88783597946167
Epoch: 5, Steps: 28 | Train Loss: 0.6233647 Vali Loss: 0.7540868 Test Loss: 0.4338215
Validation loss decreased (0.769305 --> 0.754087).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.7041499614715576
Epoch: 6, Steps: 28 | Train Loss: 0.6026693 Vali Loss: 0.7424617 Test Loss: 0.4284086
Validation loss decreased (0.754087 --> 0.742462).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.631842613220215
Epoch: 7, Steps: 28 | Train Loss: 0.5871188 Vali Loss: 0.7366812 Test Loss: 0.4249033
Validation loss decreased (0.742462 --> 0.736681).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.213277578353882
Epoch: 8, Steps: 28 | Train Loss: 0.5749089 Vali Loss: 0.7302665 Test Loss: 0.4223813
Validation loss decreased (0.736681 --> 0.730266).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.991932392120361
Epoch: 9, Steps: 28 | Train Loss: 0.5654337 Vali Loss: 0.7263005 Test Loss: 0.4205412
Validation loss decreased (0.730266 --> 0.726300).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.888664722442627
Epoch: 10, Steps: 28 | Train Loss: 0.5576131 Vali Loss: 0.7208652 Test Loss: 0.4191414
Validation loss decreased (0.726300 --> 0.720865).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.370794296264648
Epoch: 11, Steps: 28 | Train Loss: 0.5501625 Vali Loss: 0.7185558 Test Loss: 0.4179828
Validation loss decreased (0.720865 --> 0.718556).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.830223321914673
Epoch: 12, Steps: 28 | Train Loss: 0.5431071 Vali Loss: 0.7176597 Test Loss: 0.4170124
Validation loss decreased (0.718556 --> 0.717660).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.885899066925049
Epoch: 13, Steps: 28 | Train Loss: 0.5382585 Vali Loss: 0.7134731 Test Loss: 0.4161983
Validation loss decreased (0.717660 --> 0.713473).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.732010841369629
Epoch: 14, Steps: 28 | Train Loss: 0.5340820 Vali Loss: 0.7105547 Test Loss: 0.4155106
Validation loss decreased (0.713473 --> 0.710555).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.994727849960327
Epoch: 15, Steps: 28 | Train Loss: 0.5281178 Vali Loss: 0.7082052 Test Loss: 0.4148573
Validation loss decreased (0.710555 --> 0.708205).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.121601343154907
Epoch: 16, Steps: 28 | Train Loss: 0.5248606 Vali Loss: 0.7072527 Test Loss: 0.4142718
Validation loss decreased (0.708205 --> 0.707253).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.159919738769531
Epoch: 17, Steps: 28 | Train Loss: 0.5208134 Vali Loss: 0.7075397 Test Loss: 0.4137525
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.225684404373169
Epoch: 18, Steps: 28 | Train Loss: 0.5175443 Vali Loss: 0.7065761 Test Loss: 0.4132190
Validation loss decreased (0.707253 --> 0.706576).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.921650171279907
Epoch: 19, Steps: 28 | Train Loss: 0.5145613 Vali Loss: 0.7058805 Test Loss: 0.4127773
Validation loss decreased (0.706576 --> 0.705880).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.107616186141968
Epoch: 20, Steps: 28 | Train Loss: 0.5115067 Vali Loss: 0.7000644 Test Loss: 0.4123124
Validation loss decreased (0.705880 --> 0.700064).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.082039833068848
Epoch: 21, Steps: 28 | Train Loss: 0.5090414 Vali Loss: 0.7029930 Test Loss: 0.4119995
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.862339973449707
Epoch: 22, Steps: 28 | Train Loss: 0.5066855 Vali Loss: 0.6994574 Test Loss: 0.4115992
Validation loss decreased (0.700064 --> 0.699457).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.404691696166992
Epoch: 23, Steps: 28 | Train Loss: 0.5037144 Vali Loss: 0.6980264 Test Loss: 0.4112475
Validation loss decreased (0.699457 --> 0.698026).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 5.436026334762573
Epoch: 24, Steps: 28 | Train Loss: 0.5022211 Vali Loss: 0.6998520 Test Loss: 0.4108918
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.5415782928466797
Epoch: 25, Steps: 28 | Train Loss: 0.4995260 Vali Loss: 0.6969461 Test Loss: 0.4105804
Validation loss decreased (0.698026 --> 0.696946).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.240220546722412
Epoch: 26, Steps: 28 | Train Loss: 0.4981135 Vali Loss: 0.6942403 Test Loss: 0.4102853
Validation loss decreased (0.696946 --> 0.694240).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.5635714530944824
Epoch: 27, Steps: 28 | Train Loss: 0.4964366 Vali Loss: 0.6994575 Test Loss: 0.4100268
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.030146360397339
Epoch: 28, Steps: 28 | Train Loss: 0.4951514 Vali Loss: 0.6943796 Test Loss: 0.4097756
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.8177340030670166
Epoch: 29, Steps: 28 | Train Loss: 0.4929142 Vali Loss: 0.6940500 Test Loss: 0.4095383
Validation loss decreased (0.694240 --> 0.694050).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 5.03125786781311
Epoch: 30, Steps: 28 | Train Loss: 0.4908419 Vali Loss: 0.6947986 Test Loss: 0.4093014
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 5.15659236907959
Epoch: 31, Steps: 28 | Train Loss: 0.4902001 Vali Loss: 0.6949852 Test Loss: 0.4090724
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 5.550017356872559
Epoch: 32, Steps: 28 | Train Loss: 0.4889948 Vali Loss: 0.6916463 Test Loss: 0.4088680
Validation loss decreased (0.694050 --> 0.691646).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 5.605314493179321
Epoch: 33, Steps: 28 | Train Loss: 0.4876734 Vali Loss: 0.6927797 Test Loss: 0.4086769
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 5.51076602935791
Epoch: 34, Steps: 28 | Train Loss: 0.4863753 Vali Loss: 0.6939893 Test Loss: 0.4084978
EarlyStopping counter: 2 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.955002546310425
Epoch: 35, Steps: 28 | Train Loss: 0.4850005 Vali Loss: 0.6941791 Test Loss: 0.4083228
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  137682944.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.073115825653076
Epoch: 1, Steps: 28 | Train Loss: 0.8419768 Vali Loss: 0.6856526 Test Loss: 0.4053316
Validation loss decreased (inf --> 0.685653).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.093283653259277
Epoch: 2, Steps: 28 | Train Loss: 0.8347231 Vali Loss: 0.6776029 Test Loss: 0.4024040
Validation loss decreased (0.685653 --> 0.677603).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.599904775619507
Epoch: 3, Steps: 28 | Train Loss: 0.8271037 Vali Loss: 0.6688263 Test Loss: 0.4004208
Validation loss decreased (0.677603 --> 0.668826).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.16270899772644
Epoch: 4, Steps: 28 | Train Loss: 0.8223477 Vali Loss: 0.6668483 Test Loss: 0.3990574
Validation loss decreased (0.668826 --> 0.666848).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.653854131698608
Epoch: 5, Steps: 28 | Train Loss: 0.8192979 Vali Loss: 0.6654958 Test Loss: 0.3977000
Validation loss decreased (0.666848 --> 0.665496).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.289994478225708
Epoch: 6, Steps: 28 | Train Loss: 0.8162767 Vali Loss: 0.6630740 Test Loss: 0.3966190
Validation loss decreased (0.665496 --> 0.663074).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.151639461517334
Epoch: 7, Steps: 28 | Train Loss: 0.8145189 Vali Loss: 0.6627884 Test Loss: 0.3957205
Validation loss decreased (0.663074 --> 0.662788).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.344109773635864
Epoch: 8, Steps: 28 | Train Loss: 0.8121413 Vali Loss: 0.6584737 Test Loss: 0.3950087
Validation loss decreased (0.662788 --> 0.658474).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.092071056365967
Epoch: 9, Steps: 28 | Train Loss: 0.8121032 Vali Loss: 0.6574904 Test Loss: 0.3944470
Validation loss decreased (0.658474 --> 0.657490).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.42669153213501
Epoch: 10, Steps: 28 | Train Loss: 0.8113054 Vali Loss: 0.6584895 Test Loss: 0.3938500
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.301556348800659
Epoch: 11, Steps: 28 | Train Loss: 0.8072869 Vali Loss: 0.6557332 Test Loss: 0.3934075
Validation loss decreased (0.657490 --> 0.655733).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.654271125793457
Epoch: 12, Steps: 28 | Train Loss: 0.8071231 Vali Loss: 0.6598672 Test Loss: 0.3930216
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.486555099487305
Epoch: 13, Steps: 28 | Train Loss: 0.8060213 Vali Loss: 0.6543379 Test Loss: 0.3926202
Validation loss decreased (0.655733 --> 0.654338).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.283489227294922
Epoch: 14, Steps: 28 | Train Loss: 0.8064195 Vali Loss: 0.6519519 Test Loss: 0.3923157
Validation loss decreased (0.654338 --> 0.651952).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.401105880737305
Epoch: 15, Steps: 28 | Train Loss: 0.8042453 Vali Loss: 0.6582096 Test Loss: 0.3920941
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.634279251098633
Epoch: 16, Steps: 28 | Train Loss: 0.8043046 Vali Loss: 0.6519605 Test Loss: 0.3918236
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.760530471801758
Epoch: 17, Steps: 28 | Train Loss: 0.8029949 Vali Loss: 0.6526285 Test Loss: 0.3916060
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3793901801109314, mae:0.424226313829422, rse:0.4923214614391327, corr:[ 0.21794865  0.22150677  0.2193702   0.2200852   0.22047351  0.21904632
  0.21834701  0.21840777  0.21760443  0.21589717  0.21472456  0.21372095
  0.21228597  0.21075256  0.20954718  0.20869169  0.20792617  0.2070917
  0.20611513  0.20504043  0.20414883  0.20339034  0.20229061  0.2008205
  0.19938968  0.19818349  0.1969275   0.19570455  0.19512616  0.19482751
  0.19413485  0.1930596   0.19227263  0.19162689  0.19047548  0.18908277
  0.18814456  0.18770033  0.18692125  0.18585268  0.18519653  0.18479878
  0.18409438  0.18316293  0.1824752   0.18211071  0.18154852  0.18000475
  0.17811874  0.17667277  0.175865    0.17524113  0.17469701  0.17412202
  0.17353025  0.17315656  0.17243129  0.1713751   0.17064643  0.17037398
  0.169925    0.16942388  0.1696285   0.17011325  0.16985042  0.1697849
  0.17007816  0.16999887  0.16970454  0.16997875  0.17044094  0.17009303
  0.16929077  0.16902511  0.16899157  0.16820508  0.16769136  0.16797982
  0.16798504  0.16716847  0.16685665  0.16725035  0.1672121   0.1666592
  0.16649738  0.16691627  0.1670496   0.16695808  0.16715364  0.16732809
  0.16674913  0.16607319  0.16659543  0.16733776  0.16741231  0.16721424
  0.16744111  0.16760959  0.16718279  0.16673751  0.16676569  0.16657332
  0.16625309  0.1660696   0.16618021  0.16592917  0.16573639  0.16610156
  0.16628799  0.16586407  0.16526946  0.16494383  0.16436073  0.16379839
  0.16392006  0.16447511  0.16447553  0.163699    0.1632581   0.16289788
  0.16197102  0.16081972  0.16038059  0.16021623  0.15963896  0.159115
  0.15886964  0.15822874  0.15734032  0.1568298   0.15671839  0.15603268
  0.15514004  0.15454333  0.15397614  0.1532118   0.15262593  0.15270896
  0.15241578  0.15127282  0.15073967  0.15116778  0.15149869  0.15072025
  0.14919703  0.1479048   0.14666076  0.1458219   0.14604221  0.14666006
  0.14616339  0.14460403  0.14393042  0.14432131  0.14419836  0.14289217
  0.1415824   0.14131835  0.14157797  0.14124684  0.14024308  0.14004207
  0.14075288  0.14122695  0.1411876   0.1411232   0.14131857  0.140766
  0.13946812  0.13836472  0.13815509  0.13812034  0.1377539   0.13741967
  0.13737494  0.13681436  0.13568084  0.13483046  0.13449767  0.1343321
  0.13389319  0.13330366  0.13291566  0.13268891  0.13249971  0.13265623
  0.13292438  0.13336529  0.13391888  0.13425301  0.13464355  0.13497533
  0.13489586  0.13439061  0.13411634  0.1350398   0.13618676  0.13583739
  0.1348304   0.13457894  0.13539149  0.13571677  0.13515693  0.13425899
  0.13373217  0.13372032  0.13390388  0.13424651  0.1345117   0.13491474
  0.1355731   0.1361232   0.13631721  0.13643095  0.13674936  0.13650508
  0.135578    0.13481727  0.1347147   0.13464381  0.13380241  0.1333329
  0.1334208   0.13369544  0.13346665  0.13344121  0.13415442  0.13487612
  0.13496287  0.13436593  0.13399737  0.13467106  0.13588272  0.13651574
  0.1364588   0.13685203  0.13809319  0.13889067  0.13886298  0.13911115
  0.1399892   0.14026247  0.13988335  0.14037663  0.1417356   0.14230321
  0.14145613  0.14062434  0.14130996  0.14271857  0.1435563   0.14355478
  0.14330462  0.14380807  0.14505254  0.14633408  0.14665855  0.14676726
  0.14737673  0.14834365  0.14913552  0.1498912   0.1507313   0.15084058
  0.15079926  0.15160741  0.15310252  0.15383954  0.15344125  0.15335123
  0.15368272  0.15367576  0.15371427  0.15460184  0.15573202  0.15567745
  0.15509905  0.15586163  0.15729928  0.1576799   0.15729201  0.15784195
  0.15931633  0.16023773  0.16017504  0.16013418  0.16061963  0.16110662
  0.16136816  0.16130848  0.16090213  0.16081996  0.16137783  0.16183676
  0.16148083  0.16112481  0.16191679  0.16294417  0.16254629  0.16121621
  0.16095556  0.1620021   0.16263066  0.1626373   0.16321008  0.16427168
  0.16415331  0.16361463  0.16401304  0.1651377   0.16556604  0.16527233
  0.1654443   0.16567566  0.16514584  0.16383839  0.16319868  0.16356523
  0.16401583  0.16398653  0.16363752  0.16366374  0.16372266  0.16355093
  0.16288719  0.16221116  0.16251329  0.16359718  0.16456594  0.16450156
  0.16388588  0.16422997  0.16546398  0.16656365  0.16716625  0.16746943
  0.16758318  0.16743156  0.16694637  0.1666157   0.16718534  0.16798468
  0.1683262   0.16804978  0.16767505  0.1674905   0.16764922  0.16857544
  0.16922747  0.16910593  0.16892216  0.16967481  0.17078853  0.17129435
  0.17126201  0.17169513  0.17238769  0.17279659  0.17288044  0.17299795
  0.17319198  0.17337444  0.17369108  0.17379372  0.1736308   0.17377117
  0.17475729  0.17568009  0.17539674  0.17506303  0.17539038  0.17560348
  0.17502184  0.17482331  0.17558095  0.17610456  0.1759012   0.17598589
  0.17660314  0.17653494  0.17561014  0.1753171   0.17615779  0.17680027
  0.17644121  0.17592382  0.17575836  0.17553861  0.17542009  0.17583263
  0.17645754  0.17669386  0.17658977  0.17655368  0.17680538  0.177431
  0.17818649  0.17778645  0.17663282  0.17616306  0.17659128  0.1768748
  0.17651261  0.17621657  0.17620999  0.17625819  0.17626949  0.17640859
  0.1763545   0.17582345  0.17509402  0.17455259  0.17435694  0.17438562
  0.17462969  0.17471947  0.17420422  0.17368557  0.17396872  0.17447087
  0.17361495  0.17207754  0.1713165   0.17121454  0.17048408  0.16903345
  0.1679236   0.16719662  0.16665187  0.16613367  0.16557558  0.16513681
  0.16454731  0.16386436  0.16339384  0.16292095  0.16232273  0.16135196
  0.16029814  0.15987594  0.15944754  0.15865406  0.15769912  0.157677
  0.15755531  0.15638977  0.15508267  0.15532005  0.15581658  0.15473177
  0.153286    0.15362762  0.15463239  0.15440568  0.15341458  0.1531473
  0.15346472  0.1532542   0.15277842  0.15263909  0.15222237  0.15148145
  0.15152693  0.15221837  0.15199311  0.15122382  0.1508809   0.15073861
  0.15014434  0.14898573  0.14845003  0.14862612  0.14870405  0.14825793
  0.14783587  0.14769469  0.14751825  0.14739601  0.14739035  0.14753819
  0.14733362  0.14636466  0.14480203  0.14345305  0.14318568  0.14314894
  0.14251548  0.14144967  0.14071307  0.14020568  0.13988613  0.13951105
  0.13871792  0.13739617  0.136163    0.1357739   0.13589087  0.13591745
  0.13536252  0.13442217  0.13367619  0.13325807  0.13302901  0.1325352
  0.13154286  0.13008802  0.12850557  0.12751807  0.127447    0.12755357
  0.12704763  0.12662359  0.12647301  0.12562633  0.12392335  0.12264273
  0.12265986  0.12262876  0.12141424  0.11982535  0.11934356  0.11950777
  0.11877757  0.11761856  0.1171201   0.11713907  0.11676224  0.11591934
  0.11472624  0.11332246  0.11199507  0.11066614  0.10938675  0.1079365
  0.10705563  0.10681521  0.10606641  0.10434905  0.10285553  0.10229699
  0.10158074  0.1002178   0.09928904  0.09874438  0.09735066  0.09555546
  0.09466746  0.09521691  0.09551559  0.09418904  0.09255574  0.0915399
  0.09096573  0.08980329  0.08784834  0.08610669  0.08558137  0.08561328
  0.08473726  0.08308173  0.08249791  0.08279376  0.08267932  0.08158616
  0.08071627  0.08058454  0.08036587  0.07948662  0.07893424  0.078486
  0.07731839  0.07595263  0.07539158  0.07546176  0.07478501  0.07312994
  0.07125869  0.0701093   0.06916699  0.0678441   0.06659874  0.06610788
  0.06613635  0.06528848  0.06333188  0.06212366  0.06235891  0.0624718
  0.06104335  0.05967336  0.05937611  0.0585403   0.05684802  0.05642868
  0.05774081  0.05831293  0.05681382  0.05556269  0.05559422  0.05499284
  0.05276638  0.0509565   0.05042224  0.05015109  0.04894102  0.0477002
  0.04621416  0.04418616  0.04263172  0.04250417  0.04237008  0.04088073
  0.03968641  0.03968824  0.0396377   0.03902418  0.03914283  0.04025336
  0.04023206  0.03930506  0.0391141   0.03961046  0.03869133  0.0371257
  0.03651993  0.03673806  0.03636173  0.03502261  0.0337852   0.03348119
  0.03345646  0.03293995  0.0321267   0.03189176  0.03222781  0.03234949
  0.03127242  0.03017204  0.03070852  0.03133086  0.02990399  0.02854931
  0.02891116  0.02886305  0.0271544   0.02620673  0.02714067  0.02773466
  0.02572017  0.02328734  0.02303664  0.0235505   0.02297489  0.02152977
  0.02089549  0.02110591  0.02084594  0.01993656  0.01930032  0.01968875
  0.02024116  0.02001138  0.0193032   0.01919088  0.02023859  0.02079117
  0.02014232  0.01976218  0.02007468  0.0199958   0.01891501  0.01839821
  0.01869872  0.01837996  0.01707112  0.01601698  0.01648847  0.0164931
  0.01578604  0.01487117  0.01558306  0.01685821  0.01674272  0.01507846
  0.01358682  0.01324449  0.01411768  0.01383991  0.01247975  0.01287274
  0.01482523  0.01413233  0.01141905  0.01049346  0.01197527  0.01118942
  0.00720573  0.00433573  0.00519506  0.005583    0.00373562  0.00233723
  0.00388365  0.00501235  0.00413868  0.00337753  0.00398121  0.00481643
  0.00340715  0.0005551   0.0008749   0.00249917  0.00046611 -0.0036468
 -0.00375821 -0.00207564 -0.00797217 -0.01637922 -0.01020828  0.00215779]
