Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  174211072.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.398439884185791
Epoch: 1, Steps: 15 | Train Loss: 0.6633362 Vali Loss: 0.5629191 Test Loss: 0.5480896
Validation loss decreased (inf --> 0.562919).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.044555425643921
Epoch: 2, Steps: 15 | Train Loss: 0.5981226 Vali Loss: 0.5203980 Test Loss: 0.5196735
Validation loss decreased (0.562919 --> 0.520398).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.223012924194336
Epoch: 3, Steps: 15 | Train Loss: 0.5473960 Vali Loss: 0.4881727 Test Loss: 0.4982736
Validation loss decreased (0.520398 --> 0.488173).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.160085916519165
Epoch: 4, Steps: 15 | Train Loss: 0.5113732 Vali Loss: 0.4669620 Test Loss: 0.4816234
Validation loss decreased (0.488173 --> 0.466962).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2220346927642822
Epoch: 5, Steps: 15 | Train Loss: 0.4835739 Vali Loss: 0.4471977 Test Loss: 0.4689683
Validation loss decreased (0.466962 --> 0.447198).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2435646057128906
Epoch: 6, Steps: 15 | Train Loss: 0.4597144 Vali Loss: 0.4334911 Test Loss: 0.4596183
Validation loss decreased (0.447198 --> 0.433491).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.2567358016967773
Epoch: 7, Steps: 15 | Train Loss: 0.4430594 Vali Loss: 0.4240884 Test Loss: 0.4525511
Validation loss decreased (0.433491 --> 0.424088).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.267411470413208
Epoch: 8, Steps: 15 | Train Loss: 0.4270506 Vali Loss: 0.4151362 Test Loss: 0.4470640
Validation loss decreased (0.424088 --> 0.415136).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.5661206245422363
Epoch: 9, Steps: 15 | Train Loss: 0.4149797 Vali Loss: 0.4082271 Test Loss: 0.4428730
Validation loss decreased (0.415136 --> 0.408227).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.3623807430267334
Epoch: 10, Steps: 15 | Train Loss: 0.4044708 Vali Loss: 0.4037175 Test Loss: 0.4396060
Validation loss decreased (0.408227 --> 0.403717).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.440412998199463
Epoch: 11, Steps: 15 | Train Loss: 0.3954863 Vali Loss: 0.3976575 Test Loss: 0.4370592
Validation loss decreased (0.403717 --> 0.397657).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.2941009998321533
Epoch: 12, Steps: 15 | Train Loss: 0.3876509 Vali Loss: 0.3918932 Test Loss: 0.4349860
Validation loss decreased (0.397657 --> 0.391893).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.1999409198760986
Epoch: 13, Steps: 15 | Train Loss: 0.3801900 Vali Loss: 0.3909658 Test Loss: 0.4333928
Validation loss decreased (0.391893 --> 0.390966).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.522594451904297
Epoch: 14, Steps: 15 | Train Loss: 0.3744381 Vali Loss: 0.3865529 Test Loss: 0.4320806
Validation loss decreased (0.390966 --> 0.386553).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.2480146884918213
Epoch: 15, Steps: 15 | Train Loss: 0.3690716 Vali Loss: 0.3857689 Test Loss: 0.4310210
Validation loss decreased (0.386553 --> 0.385769).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.975661516189575
Epoch: 16, Steps: 15 | Train Loss: 0.3639979 Vali Loss: 0.3823884 Test Loss: 0.4301685
Validation loss decreased (0.385769 --> 0.382388).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.1327075958251953
Epoch: 17, Steps: 15 | Train Loss: 0.3594160 Vali Loss: 0.3818405 Test Loss: 0.4294475
Validation loss decreased (0.382388 --> 0.381841).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.2104623317718506
Epoch: 18, Steps: 15 | Train Loss: 0.3557457 Vali Loss: 0.3801023 Test Loss: 0.4289266
Validation loss decreased (0.381841 --> 0.380102).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.949803113937378
Epoch: 19, Steps: 15 | Train Loss: 0.3519650 Vali Loss: 0.3793365 Test Loss: 0.4284340
Validation loss decreased (0.380102 --> 0.379337).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.716721534729004
Epoch: 20, Steps: 15 | Train Loss: 0.3476478 Vali Loss: 0.3754989 Test Loss: 0.4280573
Validation loss decreased (0.379337 --> 0.375499).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.9937586784362793
Epoch: 21, Steps: 15 | Train Loss: 0.3452139 Vali Loss: 0.3761938 Test Loss: 0.4276778
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.4051005840301514
Epoch: 22, Steps: 15 | Train Loss: 0.3423817 Vali Loss: 0.3757250 Test Loss: 0.4273876
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.252964973449707
Epoch: 23, Steps: 15 | Train Loss: 0.3396090 Vali Loss: 0.3746128 Test Loss: 0.4271472
Validation loss decreased (0.375499 --> 0.374613).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.1423721313476562
Epoch: 24, Steps: 15 | Train Loss: 0.3363275 Vali Loss: 0.3740141 Test Loss: 0.4269114
Validation loss decreased (0.374613 --> 0.374014).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.914036989212036
Epoch: 25, Steps: 15 | Train Loss: 0.3343970 Vali Loss: 0.3706880 Test Loss: 0.4267393
Validation loss decreased (0.374014 --> 0.370688).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.1019999980926514
Epoch: 26, Steps: 15 | Train Loss: 0.3321181 Vali Loss: 0.3732677 Test Loss: 0.4266353
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.2014431953430176
Epoch: 27, Steps: 15 | Train Loss: 0.3299615 Vali Loss: 0.3708914 Test Loss: 0.4264925
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.33577561378479
Epoch: 28, Steps: 15 | Train Loss: 0.3287461 Vali Loss: 0.3695022 Test Loss: 0.4263487
Validation loss decreased (0.370688 --> 0.369502).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.296553611755371
Epoch: 29, Steps: 15 | Train Loss: 0.3260486 Vali Loss: 0.3703204 Test Loss: 0.4262449
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.3539276123046875
Epoch: 30, Steps: 15 | Train Loss: 0.3242189 Vali Loss: 0.3684270 Test Loss: 0.4261563
Validation loss decreased (0.369502 --> 0.368427).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.557915210723877
Epoch: 31, Steps: 15 | Train Loss: 0.3231983 Vali Loss: 0.3686745 Test Loss: 0.4260780
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.275693416595459
Epoch: 32, Steps: 15 | Train Loss: 0.3217986 Vali Loss: 0.3681514 Test Loss: 0.4260202
Validation loss decreased (0.368427 --> 0.368151).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.18583083152771
Epoch: 33, Steps: 15 | Train Loss: 0.3204972 Vali Loss: 0.3675936 Test Loss: 0.4259399
Validation loss decreased (0.368151 --> 0.367594).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.006803274154663
Epoch: 34, Steps: 15 | Train Loss: 0.3186925 Vali Loss: 0.3675302 Test Loss: 0.4259071
Validation loss decreased (0.367594 --> 0.367530).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.8316056728363037
Epoch: 35, Steps: 15 | Train Loss: 0.3172274 Vali Loss: 0.3652856 Test Loss: 0.4258322
Validation loss decreased (0.367530 --> 0.365286).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.097564458847046
Epoch: 36, Steps: 15 | Train Loss: 0.3163397 Vali Loss: 0.3659563 Test Loss: 0.4258079
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.1209065914154053
Epoch: 37, Steps: 15 | Train Loss: 0.3154546 Vali Loss: 0.3655220 Test Loss: 0.4257651
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.126760721206665
Epoch: 38, Steps: 15 | Train Loss: 0.3136392 Vali Loss: 0.3658355 Test Loss: 0.4257329
EarlyStopping counter: 3 out of 3
Early stopping
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  174211072.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.407867908477783
Epoch: 1, Steps: 15 | Train Loss: 0.5952608 Vali Loss: 0.3500532 Test Loss: 0.4138328
Validation loss decreased (inf --> 0.350053).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.4753167629241943
Epoch: 2, Steps: 15 | Train Loss: 0.5770782 Vali Loss: 0.3379136 Test Loss: 0.4059945
Validation loss decreased (0.350053 --> 0.337914).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.438706159591675
Epoch: 3, Steps: 15 | Train Loss: 0.5635011 Vali Loss: 0.3282728 Test Loss: 0.4007112
Validation loss decreased (0.337914 --> 0.328273).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.523042917251587
Epoch: 4, Steps: 15 | Train Loss: 0.5541512 Vali Loss: 0.3233536 Test Loss: 0.3969329
Validation loss decreased (0.328273 --> 0.323354).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.3042099475860596
Epoch: 5, Steps: 15 | Train Loss: 0.5480472 Vali Loss: 0.3176427 Test Loss: 0.3942201
Validation loss decreased (0.323354 --> 0.317643).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.411271333694458
Epoch: 6, Steps: 15 | Train Loss: 0.5415734 Vali Loss: 0.3129884 Test Loss: 0.3922999
Validation loss decreased (0.317643 --> 0.312988).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.4203574657440186
Epoch: 7, Steps: 15 | Train Loss: 0.5387316 Vali Loss: 0.3099598 Test Loss: 0.3908573
Validation loss decreased (0.312988 --> 0.309960).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.2026002407073975
Epoch: 8, Steps: 15 | Train Loss: 0.5361683 Vali Loss: 0.3078904 Test Loss: 0.3897752
Validation loss decreased (0.309960 --> 0.307890).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.2786874771118164
Epoch: 9, Steps: 15 | Train Loss: 0.5327905 Vali Loss: 0.3054729 Test Loss: 0.3890470
Validation loss decreased (0.307890 --> 0.305473).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.470216989517212
Epoch: 10, Steps: 15 | Train Loss: 0.5297346 Vali Loss: 0.3021170 Test Loss: 0.3884243
Validation loss decreased (0.305473 --> 0.302117).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.500551223754883
Epoch: 11, Steps: 15 | Train Loss: 0.5261471 Vali Loss: 0.3037162 Test Loss: 0.3879237
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.1395766735076904
Epoch: 12, Steps: 15 | Train Loss: 0.5270749 Vali Loss: 0.3018228 Test Loss: 0.3876714
Validation loss decreased (0.302117 --> 0.301823).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.469801425933838
Epoch: 13, Steps: 15 | Train Loss: 0.5251891 Vali Loss: 0.3004000 Test Loss: 0.3873777
Validation loss decreased (0.301823 --> 0.300400).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.5222153663635254
Epoch: 14, Steps: 15 | Train Loss: 0.5233743 Vali Loss: 0.3000907 Test Loss: 0.3872024
Validation loss decreased (0.300400 --> 0.300091).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.5791587829589844
Epoch: 15, Steps: 15 | Train Loss: 0.5198209 Vali Loss: 0.2956604 Test Loss: 0.3871207
Validation loss decreased (0.300091 --> 0.295660).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.66827392578125
Epoch: 16, Steps: 15 | Train Loss: 0.5208439 Vali Loss: 0.2978402 Test Loss: 0.3869851
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.569615364074707
Epoch: 17, Steps: 15 | Train Loss: 0.5172137 Vali Loss: 0.2951430 Test Loss: 0.3868861
Validation loss decreased (0.295660 --> 0.295143).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.3793561458587646
Epoch: 18, Steps: 15 | Train Loss: 0.5206754 Vali Loss: 0.2953732 Test Loss: 0.3867883
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.2556192874908447
Epoch: 19, Steps: 15 | Train Loss: 0.5179116 Vali Loss: 0.2948827 Test Loss: 0.3867738
Validation loss decreased (0.295143 --> 0.294883).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.2323946952819824
Epoch: 20, Steps: 15 | Train Loss: 0.5176350 Vali Loss: 0.2928372 Test Loss: 0.3867667
Validation loss decreased (0.294883 --> 0.292837).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.559628963470459
Epoch: 21, Steps: 15 | Train Loss: 0.5170356 Vali Loss: 0.2945996 Test Loss: 0.3867625
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.444755792617798
Epoch: 22, Steps: 15 | Train Loss: 0.5146508 Vali Loss: 0.2908725 Test Loss: 0.3867315
Validation loss decreased (0.292837 --> 0.290872).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.6487667560577393
Epoch: 23, Steps: 15 | Train Loss: 0.5170854 Vali Loss: 0.2918762 Test Loss: 0.3867432
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.340182065963745
Epoch: 24, Steps: 15 | Train Loss: 0.5182172 Vali Loss: 0.2930273 Test Loss: 0.3867935
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.4555904865264893
Epoch: 25, Steps: 15 | Train Loss: 0.5169902 Vali Loss: 0.2930565 Test Loss: 0.3867618
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3284055292606354, mae:0.3741437792778015, rse:0.45956408977508545, corr:[0.2643579  0.26986277 0.26764905 0.26927233 0.26985532 0.2684658
 0.2682681  0.26844975 0.2675515  0.26621816 0.2652359  0.26417372
 0.2628722  0.2615698  0.26046225 0.25979736 0.2593227  0.2587792
 0.25807267 0.25697863 0.25555706 0.25425717 0.2530801  0.25144884
 0.24947126 0.2477779  0.2463412  0.24461153 0.24299541 0.24192648
 0.24091816 0.23930494 0.2378938  0.23710483 0.23603426 0.23450221
 0.2333651  0.23286764 0.23199344 0.23073335 0.23004411 0.22970003
 0.22892489 0.227824   0.2268171  0.22580045 0.22464702 0.2231431
 0.22130075 0.21947342 0.21799813 0.21665284 0.21523118 0.21336475
 0.21133897 0.20979837 0.20806369 0.20616288 0.20508401 0.20410217
 0.20269907 0.20161752 0.20184954 0.2021851  0.20145334 0.20082061
 0.20048846 0.19990283 0.19915952 0.19850066 0.19760066 0.19647874
 0.19539493 0.19447602 0.19337122 0.191981   0.19105215 0.1902553
 0.18931468 0.18855357 0.188519   0.18790007 0.18680587 0.18653727
 0.18675351 0.18646139 0.18570934 0.18550485 0.18534152 0.18482238
 0.18411615 0.1834974  0.18334347 0.18324883 0.18319337 0.1828029
 0.1818691  0.18102331 0.1803909  0.1792899  0.17804538 0.17741182
 0.17711744 0.17596225 0.17491296 0.1747319  0.17473425 0.17432626
 0.17344305 0.17286573 0.17212579 0.1714077  0.17085092 0.17061657
 0.17012548 0.16965601 0.16931501 0.1679652  0.16627727 0.1651487
 0.16418333 0.16205457 0.1602322  0.15976469 0.15911707 0.15739998
 0.15628944 0.1562108  0.15561178 0.15396279 0.15303445 0.152681
 0.15177436 0.15054451 0.15017602 0.15019345 0.1495053  0.14885506
 0.14834468 0.14722031 0.14627393 0.14633131 0.14607084 0.14397712
 0.14159113 0.14070803 0.1394546  0.13693784 0.13557085 0.13600008
 0.1356606  0.13348465 0.13281468 0.13327713 0.13243741 0.13098711
 0.13118768 0.13147834 0.13082209 0.1308002  0.13093667 0.13074659
 0.13030362 0.13008523 0.1295775  0.12889807 0.12960681 0.12976466
 0.12733224 0.12417204 0.12438651 0.12504113 0.12311958 0.12053268
 0.12071518 0.1199013  0.11676528 0.11555106 0.11589099 0.11516716
 0.11334935 0.11314706 0.1133624  0.11264826 0.11178239 0.11119149
 0.10879907 0.1073307  0.10942374 0.10642114 0.10587697 0.11994738]
