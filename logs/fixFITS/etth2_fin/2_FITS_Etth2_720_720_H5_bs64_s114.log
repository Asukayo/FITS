Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  48787200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.141962766647339
Epoch: 1, Steps: 56 | Train Loss: 0.8583689 Vali Loss: 0.8418466 Test Loss: 0.4801316
Validation loss decreased (inf --> 0.841847).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.255761623382568
Epoch: 2, Steps: 56 | Train Loss: 0.6915851 Vali Loss: 0.7849383 Test Loss: 0.4399781
Validation loss decreased (0.841847 --> 0.784938).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 6.856573581695557
Epoch: 3, Steps: 56 | Train Loss: 0.6170545 Vali Loss: 0.7564941 Test Loss: 0.4233693
Validation loss decreased (0.784938 --> 0.756494).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.554431438446045
Epoch: 4, Steps: 56 | Train Loss: 0.5789096 Vali Loss: 0.7370706 Test Loss: 0.4157550
Validation loss decreased (0.756494 --> 0.737071).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.00154447555542
Epoch: 5, Steps: 56 | Train Loss: 0.5543516 Vali Loss: 0.7257868 Test Loss: 0.4116742
Validation loss decreased (0.737071 --> 0.725787).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.64576268196106
Epoch: 6, Steps: 56 | Train Loss: 0.5377758 Vali Loss: 0.7150528 Test Loss: 0.4089271
Validation loss decreased (0.725787 --> 0.715053).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.128714799880981
Epoch: 7, Steps: 56 | Train Loss: 0.5234418 Vali Loss: 0.7095291 Test Loss: 0.4068650
Validation loss decreased (0.715053 --> 0.709529).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.290818214416504
Epoch: 8, Steps: 56 | Train Loss: 0.5128130 Vali Loss: 0.7038909 Test Loss: 0.4052067
Validation loss decreased (0.709529 --> 0.703891).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.256067752838135
Epoch: 9, Steps: 56 | Train Loss: 0.5025554 Vali Loss: 0.7039365 Test Loss: 0.4036575
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.749314546585083
Epoch: 10, Steps: 56 | Train Loss: 0.4958280 Vali Loss: 0.6946236 Test Loss: 0.4024107
Validation loss decreased (0.703891 --> 0.694624).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.528364419937134
Epoch: 11, Steps: 56 | Train Loss: 0.4875367 Vali Loss: 0.6964441 Test Loss: 0.4012001
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.212403059005737
Epoch: 12, Steps: 56 | Train Loss: 0.4825435 Vali Loss: 0.6914318 Test Loss: 0.4001305
Validation loss decreased (0.694624 --> 0.691432).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 6.255313873291016
Epoch: 13, Steps: 56 | Train Loss: 0.4769733 Vali Loss: 0.6906976 Test Loss: 0.3991427
Validation loss decreased (0.691432 --> 0.690698).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 7.676227807998657
Epoch: 14, Steps: 56 | Train Loss: 0.4721805 Vali Loss: 0.6877190 Test Loss: 0.3982638
Validation loss decreased (0.690698 --> 0.687719).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 7.446089267730713
Epoch: 15, Steps: 56 | Train Loss: 0.4675529 Vali Loss: 0.6876876 Test Loss: 0.3974275
Validation loss decreased (0.687719 --> 0.687688).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 7.988265037536621
Epoch: 16, Steps: 56 | Train Loss: 0.4635386 Vali Loss: 0.6860037 Test Loss: 0.3966341
Validation loss decreased (0.687688 --> 0.686004).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.068839311599731
Epoch: 17, Steps: 56 | Train Loss: 0.4600893 Vali Loss: 0.6804401 Test Loss: 0.3959885
Validation loss decreased (0.686004 --> 0.680440).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.212617635726929
Epoch: 18, Steps: 56 | Train Loss: 0.4583397 Vali Loss: 0.6826564 Test Loss: 0.3953347
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 7.978445529937744
Epoch: 19, Steps: 56 | Train Loss: 0.4550721 Vali Loss: 0.6773473 Test Loss: 0.3947532
Validation loss decreased (0.680440 --> 0.677347).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 7.851783752441406
Epoch: 20, Steps: 56 | Train Loss: 0.4525491 Vali Loss: 0.6776201 Test Loss: 0.3942262
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 7.725725173950195
Epoch: 21, Steps: 56 | Train Loss: 0.4502628 Vali Loss: 0.6819438 Test Loss: 0.3936521
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.447612047195435
Epoch: 22, Steps: 56 | Train Loss: 0.4485437 Vali Loss: 0.6752269 Test Loss: 0.3931791
Validation loss decreased (0.677347 --> 0.675227).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.422922372817993
Epoch: 23, Steps: 56 | Train Loss: 0.4468763 Vali Loss: 0.6774246 Test Loss: 0.3927624
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 7.883157968521118
Epoch: 24, Steps: 56 | Train Loss: 0.4456353 Vali Loss: 0.6744189 Test Loss: 0.3923816
Validation loss decreased (0.675227 --> 0.674419).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.044828414916992
Epoch: 25, Steps: 56 | Train Loss: 0.4437037 Vali Loss: 0.6723384 Test Loss: 0.3920141
Validation loss decreased (0.674419 --> 0.672338).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 7.99348783493042
Epoch: 26, Steps: 56 | Train Loss: 0.4430855 Vali Loss: 0.6706932 Test Loss: 0.3917176
Validation loss decreased (0.672338 --> 0.670693).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.388954401016235
Epoch: 27, Steps: 56 | Train Loss: 0.4413637 Vali Loss: 0.6736844 Test Loss: 0.3913920
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 8.390543460845947
Epoch: 28, Steps: 56 | Train Loss: 0.4403233 Vali Loss: 0.6710898 Test Loss: 0.3911145
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 8.212530851364136
Epoch: 29, Steps: 56 | Train Loss: 0.4392334 Vali Loss: 0.6697274 Test Loss: 0.3908408
Validation loss decreased (0.670693 --> 0.669727).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 7.3893327713012695
Epoch: 30, Steps: 56 | Train Loss: 0.4388182 Vali Loss: 0.6728524 Test Loss: 0.3905766
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 6.1815056800842285
Epoch: 31, Steps: 56 | Train Loss: 0.4372348 Vali Loss: 0.6674963 Test Loss: 0.3903784
Validation loss decreased (0.669727 --> 0.667496).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 6.950502157211304
Epoch: 32, Steps: 56 | Train Loss: 0.4364920 Vali Loss: 0.6674924 Test Loss: 0.3901564
Validation loss decreased (0.667496 --> 0.667492).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 7.2103681564331055
Epoch: 33, Steps: 56 | Train Loss: 0.4365206 Vali Loss: 0.6658710 Test Loss: 0.3899586
Validation loss decreased (0.667492 --> 0.665871).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 6.4426209926605225
Epoch: 34, Steps: 56 | Train Loss: 0.4356745 Vali Loss: 0.6694844 Test Loss: 0.3897717
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 6.746396541595459
Epoch: 35, Steps: 56 | Train Loss: 0.4337649 Vali Loss: 0.6708128 Test Loss: 0.3895801
EarlyStopping counter: 2 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 6.4470603466033936
Epoch: 36, Steps: 56 | Train Loss: 0.4341884 Vali Loss: 0.6637380 Test Loss: 0.3894326
Validation loss decreased (0.665871 --> 0.663738).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 8.00998568534851
Epoch: 37, Steps: 56 | Train Loss: 0.4334861 Vali Loss: 0.6673510 Test Loss: 0.3892921
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 8.160217761993408
Epoch: 38, Steps: 56 | Train Loss: 0.4323830 Vali Loss: 0.6662964 Test Loss: 0.3891417
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 7.5837249755859375
Epoch: 39, Steps: 56 | Train Loss: 0.4313348 Vali Loss: 0.6640106 Test Loss: 0.3890104
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  48787200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 7.9247636795043945
Epoch: 1, Steps: 56 | Train Loss: 0.8182800 Vali Loss: 0.6644050 Test Loss: 0.3863693
Validation loss decreased (inf --> 0.664405).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.93918776512146
Epoch: 2, Steps: 56 | Train Loss: 0.8110806 Vali Loss: 0.6503990 Test Loss: 0.3843042
Validation loss decreased (0.664405 --> 0.650399).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.86982536315918
Epoch: 3, Steps: 56 | Train Loss: 0.8065487 Vali Loss: 0.6529397 Test Loss: 0.3828594
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.740394115447998
Epoch: 4, Steps: 56 | Train Loss: 0.8037874 Vali Loss: 0.6512043 Test Loss: 0.3820105
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.06718635559082
Epoch: 5, Steps: 56 | Train Loss: 0.8016012 Vali Loss: 0.6488706 Test Loss: 0.3812457
Validation loss decreased (0.650399 --> 0.648871).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 7.34182333946228
Epoch: 6, Steps: 56 | Train Loss: 0.8029480 Vali Loss: 0.6459534 Test Loss: 0.3808044
Validation loss decreased (0.648871 --> 0.645953).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 7.389004707336426
Epoch: 7, Steps: 56 | Train Loss: 0.8023850 Vali Loss: 0.6457573 Test Loss: 0.3805018
Validation loss decreased (0.645953 --> 0.645757).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.46085524559021
Epoch: 8, Steps: 56 | Train Loss: 0.8004871 Vali Loss: 0.6416111 Test Loss: 0.3804228
Validation loss decreased (0.645757 --> 0.641611).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.581772089004517
Epoch: 9, Steps: 56 | Train Loss: 0.8018330 Vali Loss: 0.6423985 Test Loss: 0.3803154
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 7.642632484436035
Epoch: 10, Steps: 56 | Train Loss: 0.8000677 Vali Loss: 0.6438386 Test Loss: 0.3801539
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 7.552449703216553
Epoch: 11, Steps: 56 | Train Loss: 0.7994879 Vali Loss: 0.6441969 Test Loss: 0.3801081
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3790138363838196, mae:0.42352020740509033, rse:0.4920772314071655, corr:[ 2.19476968e-01  2.21919432e-01  2.20474809e-01  2.19338834e-01
  2.19418466e-01  2.19310597e-01  2.18199760e-01  2.16938421e-01
  2.16028497e-01  2.14953959e-01  2.13557899e-01  2.12045729e-01
  2.10743800e-01  2.09563434e-01  2.08335266e-01  2.07340449e-01
  2.06685007e-01  2.06070602e-01  2.05125973e-01  2.04120114e-01
  2.03396872e-01  2.02824682e-01  2.01786593e-01  2.00163126e-01
  1.98531702e-01  1.97259307e-01  1.96142897e-01  1.95066795e-01
  1.94283321e-01  1.93733886e-01  1.93191007e-01  1.92371652e-01
  1.91490471e-01  1.90662012e-01  1.89817786e-01  1.88882306e-01
  1.87911481e-01  1.87103480e-01  1.86387807e-01  1.85678035e-01
  1.85057640e-01  1.84538260e-01  1.83986321e-01  1.83389261e-01
  1.82768822e-01  1.82198733e-01  1.81423694e-01  1.79939136e-01
  1.78232208e-01  1.76832393e-01  1.75937727e-01  1.75293311e-01
  1.74783647e-01  1.74122050e-01  1.73221305e-01  1.72513634e-01
  1.71950921e-01  1.71275571e-01  1.70496807e-01  1.70029134e-01
  1.69922486e-01  1.69738591e-01  1.69583648e-01  1.69757843e-01
  1.69922784e-01  1.70273840e-01  1.70556173e-01  1.70648605e-01
  1.70628339e-01  1.70510054e-01  1.70164034e-01  1.69657677e-01
  1.69279680e-01  1.69069424e-01  1.68861747e-01  1.68301597e-01
  1.67824164e-01  1.67662501e-01  1.67602196e-01  1.67409152e-01
  1.67266503e-01  1.67086095e-01  1.66796073e-01  1.66602463e-01
  1.66604891e-01  1.66744262e-01  1.66858748e-01  1.67086959e-01
  1.67389318e-01  1.67616904e-01  1.67468607e-01  1.67033613e-01
  1.67072102e-01  1.67361498e-01  1.67438298e-01  1.66946039e-01
  1.66384518e-01  1.66445822e-01  1.66981563e-01  1.67340979e-01
  1.67202070e-01  1.66584685e-01  1.66245401e-01  1.66199699e-01
  1.66285813e-01  1.66028708e-01  1.65674165e-01  1.65670067e-01
  1.65961638e-01  1.66277811e-01  1.66151762e-01  1.65420398e-01
  1.64432794e-01  1.64063931e-01  1.64360732e-01  1.64694324e-01
  1.64574772e-01  1.64051682e-01  1.63734287e-01  1.63312033e-01
  1.62256062e-01  1.60701573e-01  1.59562588e-01  1.59296781e-01
  1.59246370e-01  1.58833787e-01  1.57947406e-01  1.57095253e-01
  1.56735927e-01  1.56568080e-01  1.56460136e-01  1.56219497e-01
  1.56089723e-01  1.55782551e-01  1.55167192e-01  1.54531091e-01
  1.53969213e-01  1.53657541e-01  1.53195947e-01  1.52444825e-01
  1.51983738e-01  1.51975706e-01  1.52129054e-01  1.51650801e-01
  1.50129378e-01  1.48562059e-01  1.47567078e-01  1.47066265e-01
  1.46659955e-01  1.46103218e-01  1.45417437e-01  1.44655496e-01
  1.44121259e-01  1.43755585e-01  1.43429533e-01  1.42954886e-01
  1.42411515e-01  1.41876489e-01  1.41432792e-01  1.41155243e-01
  1.40992031e-01  1.41299754e-01  1.41836479e-01  1.42155170e-01
  1.42201930e-01  1.41938612e-01  1.41542196e-01  1.40865579e-01
  1.40193552e-01  1.39760867e-01  1.39739156e-01  1.39807537e-01
  1.39594644e-01  1.38936922e-01  1.38127357e-01  1.37508512e-01
  1.37145981e-01  1.36656433e-01  1.35682374e-01  1.34718463e-01
  1.34233594e-01  1.34157807e-01  1.34228393e-01  1.34246379e-01
  1.34369418e-01  1.34763688e-01  1.34937719e-01  1.34853661e-01
  1.35043755e-01  1.35759667e-01  1.36658251e-01  1.36828169e-01
  1.36320695e-01  1.36045247e-01  1.36322156e-01  1.36925951e-01
  1.37302309e-01  1.37204722e-01  1.37001619e-01  1.36600316e-01
  1.36285394e-01  1.36204854e-01  1.36529371e-01  1.36676326e-01
  1.36441246e-01  1.36093393e-01  1.35709822e-01  1.35497212e-01
  1.35640413e-01  1.36279449e-01  1.37203783e-01  1.37912974e-01
  1.38107777e-01  1.37891859e-01  1.37841702e-01  1.37857944e-01
  1.37381822e-01  1.36138052e-01  1.35010138e-01  1.35195285e-01
  1.36089906e-01  1.36613905e-01  1.35941461e-01  1.35315716e-01
  1.35644764e-01  1.36692286e-01  1.37300923e-01  1.36862859e-01
  1.36162236e-01  1.35947287e-01  1.36404201e-01  1.37159199e-01
  1.37889579e-01  1.38728201e-01  1.39632106e-01  1.40297636e-01
  1.40753254e-01  1.41284004e-01  1.42141506e-01  1.42954707e-01
  1.43197015e-01  1.42845184e-01  1.42662585e-01  1.43248126e-01
  1.44136131e-01  1.44564897e-01  1.44438848e-01  1.44078225e-01
  1.44479498e-01  1.45696938e-01  1.47097796e-01  1.47766396e-01
  1.47663623e-01  1.47740811e-01  1.48518473e-01  1.49741814e-01
  1.50305331e-01  1.50377914e-01  1.50708392e-01  1.51611015e-01
  1.52390957e-01  1.52639389e-01  1.53028458e-01  1.53941125e-01
  1.54998198e-01  1.55240878e-01  1.54787749e-01  1.54747754e-01
  1.55722037e-01  1.57269076e-01  1.58178389e-01  1.58217788e-01
  1.58340380e-01  1.59020007e-01  1.59738705e-01  1.59512997e-01
  1.58605680e-01  1.58596843e-01  1.59979746e-01  1.61887228e-01
  1.63100064e-01  1.63381457e-01  1.63277984e-01  1.63373441e-01
  1.63601339e-01  1.63920254e-01  1.64390519e-01  1.64600715e-01
  1.64259434e-01  1.63597465e-01  1.63422108e-01  1.64283067e-01
  1.65358305e-01  1.65724367e-01  1.65286034e-01  1.64669126e-01
  1.64353460e-01  1.64450616e-01  1.64949432e-01  1.65678605e-01
  1.66180462e-01  1.66273996e-01  1.65982828e-01  1.65918991e-01
  1.66449562e-01  1.67398348e-01  1.67705804e-01  1.67495742e-01
  1.67331070e-01  1.67612433e-01  1.67816520e-01  1.67366877e-01
  1.66941583e-01  1.67332396e-01  1.68401182e-01  1.68647572e-01
  1.67572036e-01  1.66301951e-01  1.66061819e-01  1.66636258e-01
  1.66751519e-01  1.66361347e-01  1.66114897e-01  1.66381687e-01
  1.66406795e-01  1.65858269e-01  1.65673450e-01  1.66395515e-01
  1.67588472e-01  1.68188199e-01  1.67971686e-01  1.68108463e-01
  1.68973684e-01  1.69952273e-01  1.70397356e-01  1.70228615e-01
  1.69972554e-01  1.70096472e-01  1.70597851e-01  1.71160892e-01
  1.71932638e-01  1.72320724e-01  1.71820849e-01  1.70675442e-01
  1.70294508e-01  1.71352416e-01  1.72465265e-01  1.72570288e-01
  1.71809390e-01  1.71740890e-01  1.72872350e-01  1.74055293e-01
  1.74259856e-01  1.74235836e-01  1.74927175e-01  1.76232472e-01
  1.77011758e-01  1.77123696e-01  1.77340925e-01  1.78023636e-01
  1.78601950e-01  1.78541020e-01  1.78389236e-01  1.78818986e-01
  1.79765642e-01  1.80386856e-01  1.80490837e-01  1.80557221e-01
  1.80595368e-01  1.80542916e-01  1.80102482e-01  1.79792002e-01
  1.79995537e-01  1.80575728e-01  1.80974081e-01  1.80950731e-01
  1.80772111e-01  1.80617228e-01  1.80475682e-01  1.80471048e-01
  1.80554926e-01  1.80510998e-01  1.80344269e-01  1.80363625e-01
  1.80624381e-01  1.80958077e-01  1.81243628e-01  1.81521103e-01
  1.81940034e-01  1.82483912e-01  1.82846338e-01  1.82861432e-01
  1.82814419e-01  1.83186755e-01  1.83941782e-01  1.84303835e-01
  1.83989286e-01  1.83161497e-01  1.82812482e-01  1.83000088e-01
  1.82842419e-01  1.82164550e-01  1.81617558e-01  1.81772128e-01
  1.82150394e-01  1.82112932e-01  1.81604341e-01  1.81199387e-01
  1.81101963e-01  1.81142822e-01  1.80947587e-01  1.80602133e-01
  1.80526808e-01  1.80662438e-01  1.80683538e-01  1.80557281e-01
  1.80390701e-01  1.80188924e-01  1.79786742e-01  1.79301769e-01
  1.78769663e-01  1.78468198e-01  1.78246781e-01  1.77712843e-01
  1.76673442e-01  1.75448835e-01  1.74620181e-01  1.73918083e-01
  1.73014730e-01  1.71945870e-01  1.71030492e-01  1.70635253e-01
  1.70305595e-01  1.69782668e-01  1.69419557e-01  1.69143170e-01
  1.68811768e-01  1.68218777e-01  1.67498872e-01  1.67088583e-01
  1.66785195e-01  1.66544303e-01  1.65964350e-01  1.65287599e-01
  1.64530084e-01  1.63921848e-01  1.63293973e-01  1.63004458e-01
  1.63090169e-01  1.63233444e-01  1.62954360e-01  1.62288681e-01
  1.61470041e-01  1.61167845e-01  1.61460340e-01  1.61835998e-01
  1.61893472e-01  1.61510870e-01  1.61006719e-01  1.60845697e-01
  1.60803333e-01  1.60461918e-01  1.59747839e-01  1.59008250e-01
  1.58412218e-01  1.58537522e-01  1.58965766e-01  1.58765584e-01
  1.58017322e-01  1.57142073e-01  1.56705305e-01  1.56256646e-01
  1.55394718e-01  1.54481292e-01  1.54341832e-01  1.54827923e-01
  1.54986709e-01  1.54543221e-01  1.53812841e-01  1.53367564e-01
  1.53264061e-01  1.53279945e-01  1.53185472e-01  1.52634487e-01
  1.52012452e-01  1.51601478e-01  1.51439920e-01  1.50944233e-01
  1.49536431e-01  1.47083819e-01  1.44791856e-01  1.43813372e-01
  1.44081578e-01  1.44403324e-01  1.43839493e-01  1.42761990e-01
  1.42038256e-01  1.41910896e-01  1.41317457e-01  1.39798597e-01
  1.38593733e-01  1.38856813e-01  1.40162095e-01  1.40823632e-01
  1.39983848e-01  1.38032496e-01  1.35628402e-01  1.33510306e-01
  1.32763132e-01  1.33676156e-01  1.34724319e-01  1.34568274e-01
  1.33068934e-01  1.31516114e-01  1.30842835e-01  1.30195186e-01
  1.28861427e-01  1.27324715e-01  1.26809999e-01  1.27117500e-01
  1.27230331e-01  1.26770049e-01  1.26082003e-01  1.25726998e-01
  1.25209108e-01  1.24009751e-01  1.22496441e-01  1.21714950e-01
  1.21614479e-01  1.21289387e-01  1.20029092e-01  1.17957398e-01
  1.16248012e-01  1.15061283e-01  1.14012294e-01  1.12884372e-01
  1.11871094e-01  1.11010402e-01  1.10043019e-01  1.08920112e-01
  1.07815549e-01  1.06655195e-01  1.05301730e-01  1.03886671e-01
  1.02836452e-01  1.02449723e-01  1.01822250e-01  1.00886665e-01
  1.00400671e-01  1.00626148e-01  1.01015143e-01  1.00072637e-01
  9.78259742e-02  9.54795182e-02  9.41433534e-02  9.34353322e-02
  9.25344154e-02  9.13417637e-02  9.02500898e-02  8.92001018e-02
  8.83949548e-02  8.77222121e-02  8.77449512e-02  8.80327523e-02
  8.77010003e-02  8.63692164e-02  8.50692987e-02  8.48995000e-02
  8.59612077e-02  8.66167396e-02  8.57475474e-02  8.40722099e-02
  8.28187615e-02  8.22698921e-02  8.15573856e-02  8.03835168e-02
  7.90392831e-02  7.80462995e-02  7.68282041e-02  7.50397593e-02
  7.32636973e-02  7.21781477e-02  7.17927963e-02  7.13594928e-02
  7.03198239e-02  6.90854266e-02  6.82514086e-02  6.80567846e-02
  6.76683038e-02  6.65456876e-02  6.49650171e-02  6.37682602e-02
  6.36929721e-02  6.42092898e-02  6.41618669e-02  6.34751096e-02
  6.26624152e-02  6.22701570e-02  6.18167594e-02  6.06897399e-02
  5.88307492e-02  5.70799112e-02  5.58863468e-02  5.55005744e-02
  5.49178310e-02  5.36554754e-02  5.15517704e-02  4.95235436e-02
  4.85415719e-02  4.88999151e-02  4.96079475e-02  4.89900857e-02
  4.70380448e-02  4.50736545e-02  4.43859957e-02  4.48067933e-02
  4.52001616e-02  4.52603810e-02  4.50344943e-02  4.51014265e-02
  4.52374034e-02  4.50293198e-02  4.39712144e-02  4.29321155e-02
  4.23241518e-02  4.20699678e-02  4.16466817e-02  4.05676179e-02
  3.90048027e-02  3.79110463e-02  3.76920141e-02  3.78960669e-02
  3.78089361e-02  3.71896029e-02  3.62938046e-02  3.59974466e-02
  3.60042416e-02  3.54040712e-02  3.44083719e-02  3.38861570e-02
  3.38657722e-02  3.41655761e-02  3.38341258e-02  3.26167941e-02
  3.18297781e-02  3.22241597e-02  3.25434282e-02  3.19341086e-02
  3.06542795e-02  2.97865309e-02  2.94899624e-02  2.88554262e-02
  2.78211199e-02  2.66399998e-02  2.58484818e-02  2.56131664e-02
  2.56496798e-02  2.58236118e-02  2.60902420e-02  2.62605119e-02
  2.59431116e-02  2.54593343e-02  2.53952071e-02  2.54156217e-02
  2.54701599e-02  2.56479904e-02  2.60641612e-02  2.63231490e-02
  2.57146526e-02  2.49672476e-02  2.49794498e-02  2.57884469e-02
  2.61044241e-02  2.52289400e-02  2.37787645e-02  2.25735102e-02
  2.24072672e-02  2.21013855e-02  2.14968175e-02  2.03457065e-02
  1.99696664e-02  2.02299096e-02  2.02466063e-02  1.98867116e-02
  1.96195804e-02  1.88793149e-02  1.82312541e-02  1.80764347e-02
  1.85926165e-02  1.91795900e-02  1.88055765e-02  1.70698538e-02
  1.61061510e-02  1.64875984e-02  1.66515242e-02  1.46802245e-02
  1.15805687e-02  9.64437984e-03  9.77380201e-03  9.73494072e-03
  8.43574665e-03  6.40250323e-03  5.58067020e-03  5.68663701e-03
  6.00458449e-03  5.74454153e-03  4.31277696e-03  2.59228563e-03
  1.39133178e-03  7.71718682e-04  9.18208505e-04  4.19921853e-04
 -3.47020279e-04  2.77628191e-04  1.34522840e-03 -8.25854659e-05
 -4.70033614e-03 -6.90238643e-03 -1.85305730e-03  3.60589707e-03]
