Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  35629440.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.204336166381836
Epoch: 1, Steps: 59 | Train Loss: 0.6834582 Vali Loss: 0.6060339 Test Loss: 0.4463582
Validation loss decreased (inf --> 0.606034).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.428063631057739
Epoch: 2, Steps: 59 | Train Loss: 0.5340988 Vali Loss: 0.5410020 Test Loss: 0.4113327
Validation loss decreased (0.606034 --> 0.541002).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.678587913513184
Epoch: 3, Steps: 59 | Train Loss: 0.4602439 Vali Loss: 0.5023904 Test Loss: 0.3967778
Validation loss decreased (0.541002 --> 0.502390).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 7.121777057647705
Epoch: 4, Steps: 59 | Train Loss: 0.4174931 Vali Loss: 0.4826281 Test Loss: 0.3901400
Validation loss decreased (0.502390 --> 0.482628).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 6.210138559341431
Epoch: 5, Steps: 59 | Train Loss: 0.3892296 Vali Loss: 0.4677954 Test Loss: 0.3867463
Validation loss decreased (0.482628 --> 0.467795).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.847556352615356
Epoch: 6, Steps: 59 | Train Loss: 0.3671654 Vali Loss: 0.4581958 Test Loss: 0.3846972
Validation loss decreased (0.467795 --> 0.458196).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 9.366426229476929
Epoch: 7, Steps: 59 | Train Loss: 0.3501992 Vali Loss: 0.4496018 Test Loss: 0.3831397
Validation loss decreased (0.458196 --> 0.449602).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.452868223190308
Epoch: 8, Steps: 59 | Train Loss: 0.3362434 Vali Loss: 0.4440597 Test Loss: 0.3817444
Validation loss decreased (0.449602 --> 0.444060).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.72830581665039
Epoch: 9, Steps: 59 | Train Loss: 0.3242994 Vali Loss: 0.4393896 Test Loss: 0.3805472
Validation loss decreased (0.444060 --> 0.439390).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.439392805099487
Epoch: 10, Steps: 59 | Train Loss: 0.3139835 Vali Loss: 0.4366867 Test Loss: 0.3795651
Validation loss decreased (0.439390 --> 0.436687).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 9.258112907409668
Epoch: 11, Steps: 59 | Train Loss: 0.3050280 Vali Loss: 0.4298383 Test Loss: 0.3783396
Validation loss decreased (0.436687 --> 0.429838).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 8.704206466674805
Epoch: 12, Steps: 59 | Train Loss: 0.2977034 Vali Loss: 0.4285557 Test Loss: 0.3774152
Validation loss decreased (0.429838 --> 0.428556).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.346159219741821
Epoch: 13, Steps: 59 | Train Loss: 0.2906964 Vali Loss: 0.4273849 Test Loss: 0.3764585
Validation loss decreased (0.428556 --> 0.427385).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.487731456756592
Epoch: 14, Steps: 59 | Train Loss: 0.2846920 Vali Loss: 0.4246029 Test Loss: 0.3756296
Validation loss decreased (0.427385 --> 0.424603).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.451122283935547
Epoch: 15, Steps: 59 | Train Loss: 0.2790849 Vali Loss: 0.4226709 Test Loss: 0.3749476
Validation loss decreased (0.424603 --> 0.422671).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.230257987976074
Epoch: 16, Steps: 59 | Train Loss: 0.2744447 Vali Loss: 0.4179806 Test Loss: 0.3740253
Validation loss decreased (0.422671 --> 0.417981).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 8.738418817520142
Epoch: 17, Steps: 59 | Train Loss: 0.2695584 Vali Loss: 0.4181653 Test Loss: 0.3733529
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 6.872546195983887
Epoch: 18, Steps: 59 | Train Loss: 0.2669617 Vali Loss: 0.4176904 Test Loss: 0.3727616
Validation loss decreased (0.417981 --> 0.417690).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 7.122727155685425
Epoch: 19, Steps: 59 | Train Loss: 0.2630617 Vali Loss: 0.4148805 Test Loss: 0.3721767
Validation loss decreased (0.417690 --> 0.414881).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 8.311024904251099
Epoch: 20, Steps: 59 | Train Loss: 0.2595830 Vali Loss: 0.4124077 Test Loss: 0.3716212
Validation loss decreased (0.414881 --> 0.412408).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 9.145024299621582
Epoch: 21, Steps: 59 | Train Loss: 0.2574965 Vali Loss: 0.4122715 Test Loss: 0.3711014
Validation loss decreased (0.412408 --> 0.412271).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 9.185061931610107
Epoch: 22, Steps: 59 | Train Loss: 0.2548406 Vali Loss: 0.4104185 Test Loss: 0.3706668
Validation loss decreased (0.412271 --> 0.410418).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.11325216293335
Epoch: 23, Steps: 59 | Train Loss: 0.2528382 Vali Loss: 0.4094053 Test Loss: 0.3701935
Validation loss decreased (0.410418 --> 0.409405).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 7.978392124176025
Epoch: 24, Steps: 59 | Train Loss: 0.2506032 Vali Loss: 0.4094405 Test Loss: 0.3697939
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 8.4918794631958
Epoch: 25, Steps: 59 | Train Loss: 0.2486385 Vali Loss: 0.4082260 Test Loss: 0.3695014
Validation loss decreased (0.409405 --> 0.408226).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 8.694732904434204
Epoch: 26, Steps: 59 | Train Loss: 0.2466814 Vali Loss: 0.4065552 Test Loss: 0.3692316
Validation loss decreased (0.408226 --> 0.406555).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 8.151895523071289
Epoch: 27, Steps: 59 | Train Loss: 0.2451953 Vali Loss: 0.4078346 Test Loss: 0.3689457
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 7.903160095214844
Epoch: 28, Steps: 59 | Train Loss: 0.2439900 Vali Loss: 0.4075204 Test Loss: 0.3685851
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 7.857516527175903
Epoch: 29, Steps: 59 | Train Loss: 0.2426914 Vali Loss: 0.4051785 Test Loss: 0.3683721
Validation loss decreased (0.406555 --> 0.405178).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 7.796579599380493
Epoch: 30, Steps: 59 | Train Loss: 0.2408490 Vali Loss: 0.4066316 Test Loss: 0.3681704
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 8.286774635314941
Epoch: 31, Steps: 59 | Train Loss: 0.2397566 Vali Loss: 0.4055793 Test Loss: 0.3678938
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 8.567355155944824
Epoch: 32, Steps: 59 | Train Loss: 0.2389257 Vali Loss: 0.4037587 Test Loss: 0.3677678
Validation loss decreased (0.405178 --> 0.403759).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 8.259995937347412
Epoch: 33, Steps: 59 | Train Loss: 0.2374031 Vali Loss: 0.4027128 Test Loss: 0.3675800
Validation loss decreased (0.403759 --> 0.402713).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 7.466575860977173
Epoch: 34, Steps: 59 | Train Loss: 0.2369716 Vali Loss: 0.4013371 Test Loss: 0.3674487
Validation loss decreased (0.402713 --> 0.401337).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 8.065603494644165
Epoch: 35, Steps: 59 | Train Loss: 0.2365149 Vali Loss: 0.4032536 Test Loss: 0.3673088
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 8.586848974227905
Epoch: 36, Steps: 59 | Train Loss: 0.2356893 Vali Loss: 0.4013168 Test Loss: 0.3670963
Validation loss decreased (0.401337 --> 0.401317).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 7.893635988235474
Epoch: 37, Steps: 59 | Train Loss: 0.2353216 Vali Loss: 0.4008824 Test Loss: 0.3670155
Validation loss decreased (0.401317 --> 0.400882).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 7.674570798873901
Epoch: 38, Steps: 59 | Train Loss: 0.2342068 Vali Loss: 0.4000410 Test Loss: 0.3668935
Validation loss decreased (0.400882 --> 0.400041).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 7.96455192565918
Epoch: 39, Steps: 59 | Train Loss: 0.2335796 Vali Loss: 0.4031675 Test Loss: 0.3667736
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 7.9992835521698
Epoch: 40, Steps: 59 | Train Loss: 0.2325871 Vali Loss: 0.4028342 Test Loss: 0.3667292
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 8.259757995605469
Epoch: 41, Steps: 59 | Train Loss: 0.2321418 Vali Loss: 0.4000641 Test Loss: 0.3665789
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  35629440.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 7.902756214141846
Epoch: 1, Steps: 59 | Train Loss: 0.6307754 Vali Loss: 0.3863993 Test Loss: 0.3632307
Validation loss decreased (inf --> 0.386399).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.514161109924316
Epoch: 2, Steps: 59 | Train Loss: 0.6210307 Vali Loss: 0.3850563 Test Loss: 0.3614752
Validation loss decreased (0.386399 --> 0.385056).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 7.6930248737335205
Epoch: 3, Steps: 59 | Train Loss: 0.6177906 Vali Loss: 0.3829547 Test Loss: 0.3607830
Validation loss decreased (0.385056 --> 0.382955).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 7.9994447231292725
Epoch: 4, Steps: 59 | Train Loss: 0.6143275 Vali Loss: 0.3812346 Test Loss: 0.3602708
Validation loss decreased (0.382955 --> 0.381235).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.370563983917236
Epoch: 5, Steps: 59 | Train Loss: 0.6148932 Vali Loss: 0.3821746 Test Loss: 0.3601602
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.726280450820923
Epoch: 6, Steps: 59 | Train Loss: 0.6141675 Vali Loss: 0.3796211 Test Loss: 0.3597083
Validation loss decreased (0.381235 --> 0.379621).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.106925964355469
Epoch: 7, Steps: 59 | Train Loss: 0.6129665 Vali Loss: 0.3771521 Test Loss: 0.3597328
Validation loss decreased (0.379621 --> 0.377152).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 7.948606014251709
Epoch: 8, Steps: 59 | Train Loss: 0.6136376 Vali Loss: 0.3792322 Test Loss: 0.3594722
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 6.676811456680298
Epoch: 9, Steps: 59 | Train Loss: 0.6131402 Vali Loss: 0.3782402 Test Loss: 0.3596752
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.204592943191528
Epoch: 10, Steps: 59 | Train Loss: 0.6126329 Vali Loss: 0.3793113 Test Loss: 0.3595586
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3554501235485077, mae:0.3963654339313507, rse:0.4766820967197418, corr:[0.2626471  0.26647708 0.2646019  0.26359558 0.2637784  0.26318115
 0.26175332 0.26110858 0.26091415 0.25986665 0.25806114 0.25658238
 0.25588718 0.2550238  0.2538096  0.25289762 0.25254962 0.25210536
 0.2510193  0.24967021 0.24868251 0.24790291 0.24661425 0.24486005
 0.24316804 0.241979   0.24091175 0.23983175 0.23897293 0.23827362
 0.2374254  0.2364298  0.23562042 0.23484693 0.23377305 0.23258127
 0.23168473 0.23111947 0.23036684 0.22941232 0.22866803 0.22819535
 0.22756542 0.22657716 0.22547606 0.22455454 0.2233568  0.22142391
 0.21929987 0.21767277 0.21661112 0.21569133 0.21453053 0.21258779
 0.2101521  0.20826627 0.2070411  0.2057022  0.20393886 0.20234694
 0.20170702 0.20204456 0.20261274 0.2027086  0.20198925 0.20134561
 0.20098689 0.200849   0.20065577 0.200398   0.19991226 0.1992024
 0.19798806 0.19616385 0.19425188 0.19277413 0.19204946 0.19156554
 0.19098616 0.19035745 0.1901628  0.18986315 0.1893289  0.18901874
 0.18914887 0.18920812 0.18876752 0.1882278  0.18810458 0.18816005
 0.18762496 0.1863377  0.18565032 0.18605852 0.18681611 0.1868518
 0.18603516 0.1851903  0.1848347  0.18413523 0.18266198 0.18099158
 0.180417   0.18083294 0.18156938 0.18172762 0.18155637 0.18139416
 0.18099184 0.18031162 0.17927572 0.17863972 0.17862074 0.17900842
 0.1787196  0.17746763 0.17614686 0.17541437 0.17530629 0.17463556
 0.17278261 0.17039734 0.16877367 0.16817766 0.16779521 0.16713783
 0.16614985 0.1652783  0.16471209 0.16428974 0.16407865 0.16406009
 0.16409877 0.16344549 0.1620236  0.1606316  0.15998091 0.15978922
 0.1589144  0.15729728 0.15611991 0.15590355 0.155841   0.15486775
 0.15311314 0.15180819 0.15120274 0.15037748 0.14882746 0.14746137
 0.14733063 0.14816843 0.14891034 0.1482885  0.14625292 0.14385276
 0.14243983 0.14238161 0.14325264 0.14402962 0.14343452 0.14180075
 0.1401772  0.13974336 0.14069174 0.14188814 0.14202292 0.1405484
 0.13854165 0.1370965  0.13671683 0.13675745 0.1364779  0.13530646
 0.13388014 0.13231096 0.13088964 0.12953986 0.12816687 0.12746102
 0.12728538 0.1269184  0.12600929 0.12521265 0.12493767 0.12524989
 0.12498529 0.12377921 0.12312155 0.12391932 0.12551735 0.12564568
 0.12410287 0.1226071  0.12215949 0.12246643 0.12251577 0.12218272
 0.12187196 0.12120046 0.12003349 0.11895075 0.11902333 0.11991989
 0.12012243 0.11923672 0.1181956  0.11872517 0.12036509 0.12143995
 0.12108953 0.12036239 0.12028777 0.12047675 0.1204187  0.12010816
 0.11997575 0.11963494 0.11848638 0.11723001 0.11675615 0.1175797
 0.11813158 0.11802521 0.1174782  0.11732931 0.11750107 0.11730168
 0.11657134 0.11587835 0.11522056 0.11487864 0.1148199  0.11568495
 0.11707335 0.11789569 0.11740687 0.11639184 0.116074   0.11656209
 0.11693866 0.11661271 0.11617521 0.11636526 0.11643758 0.11572342
 0.11476295 0.11388733 0.11356887 0.11384338 0.11469926 0.11587914
 0.11632749 0.11625327 0.11594191 0.11658724 0.11750876 0.11852369
 0.11905249 0.11956283 0.12030844 0.12080169 0.12143873 0.12189694
 0.12272    0.12370236 0.1246963  0.12492699 0.12413576 0.12350199
 0.12379334 0.12458117 0.12481787 0.12364592 0.12284154 0.12291529
 0.12380104 0.1243504  0.12421805 0.12427943 0.12503636 0.1256878
 0.12523726 0.12421813 0.12351593 0.12371814 0.12444672 0.12478396
 0.12448425 0.12396481 0.12337779 0.12278759 0.12213133 0.12151154
 0.12132452 0.1207577  0.11967542 0.11884694 0.11931192 0.11982068
 0.11875552 0.11721901 0.11670008 0.11807974 0.11932757 0.12011819
 0.11947013 0.11845561 0.11763326 0.11781784 0.1188986  0.1196598
 0.11929633 0.11753502 0.11648976 0.11664236 0.11679509 0.11581013
 0.11487791 0.11579817 0.11723192 0.11769354 0.11654718 0.11616383
 0.11675733 0.11707868 0.1163931  0.11625092 0.11809637 0.12006953
 0.11908427 0.11645355 0.11664939 0.11951914 0.12153554 0.12018289]
