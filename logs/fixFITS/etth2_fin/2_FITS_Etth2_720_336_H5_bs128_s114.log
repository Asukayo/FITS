Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=256, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  71258880.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.539295434951782
Epoch: 1, Steps: 29 | Train Loss: 0.7328510 Vali Loss: 0.6601741 Test Loss: 0.4748506
Validation loss decreased (inf --> 0.660174).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.484588384628296
Epoch: 2, Steps: 29 | Train Loss: 0.6236942 Vali Loss: 0.5999624 Test Loss: 0.4427414
Validation loss decreased (0.660174 --> 0.599962).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.770473003387451
Epoch: 3, Steps: 29 | Train Loss: 0.5516031 Vali Loss: 0.5642580 Test Loss: 0.4224336
Validation loss decreased (0.599962 --> 0.564258).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.617038726806641
Epoch: 4, Steps: 29 | Train Loss: 0.5066758 Vali Loss: 0.5358522 Test Loss: 0.4100355
Validation loss decreased (0.564258 --> 0.535852).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.3508689403533936
Epoch: 5, Steps: 29 | Train Loss: 0.4738507 Vali Loss: 0.5192335 Test Loss: 0.4018385
Validation loss decreased (0.535852 --> 0.519233).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.77803111076355
Epoch: 6, Steps: 29 | Train Loss: 0.4517459 Vali Loss: 0.5056882 Test Loss: 0.3965695
Validation loss decreased (0.519233 --> 0.505688).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.9752912521362305
Epoch: 7, Steps: 29 | Train Loss: 0.4324078 Vali Loss: 0.4991519 Test Loss: 0.3929857
Validation loss decreased (0.505688 --> 0.499152).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.357248306274414
Epoch: 8, Steps: 29 | Train Loss: 0.4190662 Vali Loss: 0.4899649 Test Loss: 0.3904835
Validation loss decreased (0.499152 --> 0.489965).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.349289655685425
Epoch: 9, Steps: 29 | Train Loss: 0.4048069 Vali Loss: 0.4855172 Test Loss: 0.3887678
Validation loss decreased (0.489965 --> 0.485517).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.774508476257324
Epoch: 10, Steps: 29 | Train Loss: 0.3942256 Vali Loss: 0.4728743 Test Loss: 0.3873345
Validation loss decreased (0.485517 --> 0.472874).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.6680734157562256
Epoch: 11, Steps: 29 | Train Loss: 0.3839987 Vali Loss: 0.4694396 Test Loss: 0.3862816
Validation loss decreased (0.472874 --> 0.469440).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.423852443695068
Epoch: 12, Steps: 29 | Train Loss: 0.3755104 Vali Loss: 0.4637868 Test Loss: 0.3854703
Validation loss decreased (0.469440 --> 0.463787).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.492858409881592
Epoch: 13, Steps: 29 | Train Loss: 0.3696928 Vali Loss: 0.4651463 Test Loss: 0.3847324
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.739893436431885
Epoch: 14, Steps: 29 | Train Loss: 0.3636229 Vali Loss: 0.4631761 Test Loss: 0.3841166
Validation loss decreased (0.463787 --> 0.463176).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.400588035583496
Epoch: 15, Steps: 29 | Train Loss: 0.3565190 Vali Loss: 0.4561537 Test Loss: 0.3835701
Validation loss decreased (0.463176 --> 0.456154).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.9538164138793945
Epoch: 16, Steps: 29 | Train Loss: 0.3526227 Vali Loss: 0.4481620 Test Loss: 0.3830886
Validation loss decreased (0.456154 --> 0.448162).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.822833776473999
Epoch: 17, Steps: 29 | Train Loss: 0.3489363 Vali Loss: 0.4492021 Test Loss: 0.3826225
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.6684253215789795
Epoch: 18, Steps: 29 | Train Loss: 0.3436277 Vali Loss: 0.4507324 Test Loss: 0.3822445
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.083648920059204
Epoch: 19, Steps: 29 | Train Loss: 0.3380066 Vali Loss: 0.4484404 Test Loss: 0.3818499
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  71258880.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.933244466781616
Epoch: 1, Steps: 29 | Train Loss: 0.6730981 Vali Loss: 0.4342873 Test Loss: 0.3742641
Validation loss decreased (inf --> 0.434287).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.074284076690674
Epoch: 2, Steps: 29 | Train Loss: 0.6570131 Vali Loss: 0.4173819 Test Loss: 0.3692617
Validation loss decreased (0.434287 --> 0.417382).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.0870349407196045
Epoch: 3, Steps: 29 | Train Loss: 0.6481561 Vali Loss: 0.4158452 Test Loss: 0.3661249
Validation loss decreased (0.417382 --> 0.415845).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.812882423400879
Epoch: 4, Steps: 29 | Train Loss: 0.6381696 Vali Loss: 0.4013395 Test Loss: 0.3638030
Validation loss decreased (0.415845 --> 0.401339).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.144240140914917
Epoch: 5, Steps: 29 | Train Loss: 0.6344921 Vali Loss: 0.4023613 Test Loss: 0.3623270
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.296887397766113
Epoch: 6, Steps: 29 | Train Loss: 0.6321255 Vali Loss: 0.4013521 Test Loss: 0.3609282
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.355968475341797
Epoch: 7, Steps: 29 | Train Loss: 0.6279944 Vali Loss: 0.3952659 Test Loss: 0.3602408
Validation loss decreased (0.401339 --> 0.395266).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.4476318359375
Epoch: 8, Steps: 29 | Train Loss: 0.6251653 Vali Loss: 0.3928893 Test Loss: 0.3594121
Validation loss decreased (0.395266 --> 0.392889).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.137920618057251
Epoch: 9, Steps: 29 | Train Loss: 0.6229211 Vali Loss: 0.3934943 Test Loss: 0.3589568
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.114095449447632
Epoch: 10, Steps: 29 | Train Loss: 0.6208484 Vali Loss: 0.3908829 Test Loss: 0.3585183
Validation loss decreased (0.392889 --> 0.390883).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.126479625701904
Epoch: 11, Steps: 29 | Train Loss: 0.6203363 Vali Loss: 0.3871242 Test Loss: 0.3583561
Validation loss decreased (0.390883 --> 0.387124).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.000386953353882
Epoch: 12, Steps: 29 | Train Loss: 0.6184772 Vali Loss: 0.3892957 Test Loss: 0.3580407
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.05189323425293
Epoch: 13, Steps: 29 | Train Loss: 0.6197922 Vali Loss: 0.3885049 Test Loss: 0.3579802
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.241343021392822
Epoch: 14, Steps: 29 | Train Loss: 0.6190860 Vali Loss: 0.3884012 Test Loss: 0.3577086
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.355596661567688, mae:0.3971664309501648, rse:0.47678035497665405, corr:[0.26014498 0.26658764 0.26385114 0.2630026  0.2645358  0.2645664
 0.26284382 0.2620437  0.26211637 0.26124954 0.25938052 0.2579063
 0.2573875  0.2565799  0.25519496 0.25406474 0.2536176  0.2530406
 0.25179923 0.25035945 0.24939771 0.24860148 0.24725564 0.24566692
 0.24439241 0.24343044 0.24205664 0.2404707  0.2396322  0.2395895
 0.23929657 0.23807383 0.23655935 0.23550121 0.2348411  0.23418061
 0.23339766 0.23271762 0.23204085 0.23129568 0.23057626 0.22996405
 0.2293055  0.22847123 0.22745319 0.22651717 0.22547929 0.22387345
 0.22192031 0.22015081 0.21892543 0.21810186 0.21712604 0.21520548
 0.21270467 0.21106598 0.21026464 0.20906772 0.20712881 0.20539157
 0.2047867  0.2048691  0.20479648 0.20439252 0.20379668 0.20365143
 0.20337579 0.20282412 0.20222898 0.2017866  0.20103054 0.19985364
 0.19864438 0.19777197 0.19703463 0.195901   0.19469248 0.19369705
 0.19298635 0.19236694 0.19200547 0.1916181  0.19120108 0.19079334
 0.19032621 0.18974802 0.18922567 0.18886515 0.18835711 0.18755545
 0.18683973 0.18673266 0.18738522 0.18785737 0.18756075 0.18676111
 0.18621305 0.18612292 0.18609934 0.18550833 0.18452437 0.18362331
 0.18326107 0.18303415 0.18279527 0.18224317 0.18167223 0.18114856
 0.18058774 0.18020059 0.17966476 0.17897357 0.1781199  0.17755872
 0.17718364 0.17689018 0.17666668 0.17626293 0.17565168 0.17443039
 0.17265293 0.17078339 0.16945331 0.1687474  0.16819349 0.16739978
 0.166145   0.16506624 0.16475466 0.16477439 0.16409287 0.16227342
 0.16055185 0.16005342 0.16059789 0.16073632 0.15971361 0.1583582
 0.15751603 0.15718684 0.15680602 0.15613991 0.15523276 0.15382433
 0.15165424 0.14951241 0.1483114  0.14803636 0.1476438  0.14648421
 0.14503987 0.14418165 0.14405616 0.14352533 0.14234388 0.14138281
 0.14107518 0.14058909 0.13951157 0.13861202 0.13849856 0.13885933
 0.13829786 0.13686071 0.13604483 0.13652319 0.1370543  0.13605414
 0.13409944 0.13258956 0.13204393 0.13147861 0.13035178 0.12886141
 0.12781093 0.12666704 0.12524278 0.12381741 0.12268593 0.12214804
 0.12182593 0.12160455 0.1214577  0.12105379 0.11984837 0.11872823
 0.11832404 0.1183797  0.11845618 0.11840716 0.11890393 0.11941942
 0.11920541 0.11801335 0.11661927 0.11632826 0.11669657 0.1163267
 0.11492608 0.1135877  0.11356156 0.11404671 0.11369821 0.11242155
 0.11150398 0.1118397  0.1123644  0.11236138 0.11182138 0.11142965
 0.11134643 0.11160151 0.11210847 0.11246287 0.1123766  0.11163388
 0.11075804 0.11011762 0.10946653 0.10855532 0.1073778  0.10696927
 0.10704201 0.10737199 0.10714629 0.10664784 0.10633748 0.10597314
 0.10505839 0.10410485 0.10377914 0.10444291 0.10489792 0.1048513
 0.10483506 0.10551996 0.10639413 0.10664263 0.10630347 0.105947
 0.105566   0.10446032 0.1029078  0.10220311 0.10244916 0.10266776
 0.1023401  0.10151552 0.10131636 0.10196832 0.10256469 0.10251954
 0.10199179 0.10267103 0.1045699  0.10695802 0.10809851 0.10828288
 0.10788339 0.10763179 0.10786265 0.10852005 0.11011526 0.11134076
 0.1115673  0.11065756 0.11009574 0.11039576 0.11085159 0.11116949
 0.11111425 0.11079802 0.11058281 0.11039953 0.11103399 0.11125485
 0.11082344 0.11046583 0.11135013 0.11319475 0.1140394  0.11275977
 0.11095522 0.11101738 0.11219424 0.11236906 0.11135782 0.11103935
 0.11209884 0.11284093 0.11162409 0.10976598 0.10916602 0.10957411
 0.10962114 0.10834436 0.10719528 0.10735199 0.10832573 0.10793226
 0.10617005 0.10579165 0.10714723 0.10875355 0.10845155 0.1082427
 0.10832703 0.10889678 0.10923025 0.10983419 0.11060102 0.11013479
 0.10838882 0.10656992 0.10681041 0.1073697  0.10600972 0.10354485
 0.10332428 0.10606929 0.10774287 0.10732149 0.10646828 0.10758346
 0.10852569 0.10820231 0.10860917 0.11099458 0.11270871 0.11135241
 0.109042   0.10978018 0.11220156 0.11164062 0.11379484 0.12366147]
