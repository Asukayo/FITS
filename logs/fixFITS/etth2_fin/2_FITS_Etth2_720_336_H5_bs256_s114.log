Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  142517760.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.6332719326019287
Epoch: 1, Steps: 14 | Train Loss: 0.7700757 Vali Loss: 0.7008286 Test Loss: 0.4948657
Validation loss decreased (inf --> 0.700829).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.4929049015045166
Epoch: 2, Steps: 14 | Train Loss: 0.6998666 Vali Loss: 0.6597577 Test Loss: 0.4742260
Validation loss decreased (0.700829 --> 0.659758).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.045466184616089
Epoch: 3, Steps: 14 | Train Loss: 0.6464888 Vali Loss: 0.6325938 Test Loss: 0.4575706
Validation loss decreased (0.659758 --> 0.632594).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.810774564743042
Epoch: 4, Steps: 14 | Train Loss: 0.6098347 Vali Loss: 0.6078700 Test Loss: 0.4441080
Validation loss decreased (0.632594 --> 0.607870).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.788524627685547
Epoch: 5, Steps: 14 | Train Loss: 0.5744574 Vali Loss: 0.5897585 Test Loss: 0.4335316
Validation loss decreased (0.607870 --> 0.589758).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.7058589458465576
Epoch: 6, Steps: 14 | Train Loss: 0.5498126 Vali Loss: 0.5698905 Test Loss: 0.4250565
Validation loss decreased (0.589758 --> 0.569890).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.433353900909424
Epoch: 7, Steps: 14 | Train Loss: 0.5307419 Vali Loss: 0.5589887 Test Loss: 0.4183450
Validation loss decreased (0.569890 --> 0.558989).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.7214674949645996
Epoch: 8, Steps: 14 | Train Loss: 0.5112178 Vali Loss: 0.5485435 Test Loss: 0.4130155
Validation loss decreased (0.558989 --> 0.548543).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.6208274364471436
Epoch: 9, Steps: 14 | Train Loss: 0.4967750 Vali Loss: 0.5440692 Test Loss: 0.4087047
Validation loss decreased (0.548543 --> 0.544069).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.2192299365997314
Epoch: 10, Steps: 14 | Train Loss: 0.4844389 Vali Loss: 0.5320460 Test Loss: 0.4051664
Validation loss decreased (0.544069 --> 0.532046).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.6990649700164795
Epoch: 11, Steps: 14 | Train Loss: 0.4742594 Vali Loss: 0.5249605 Test Loss: 0.4023198
Validation loss decreased (0.532046 --> 0.524960).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.7763659954071045
Epoch: 12, Steps: 14 | Train Loss: 0.4632071 Vali Loss: 0.5206279 Test Loss: 0.3999000
Validation loss decreased (0.524960 --> 0.520628).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.726444959640503
Epoch: 13, Steps: 14 | Train Loss: 0.4565700 Vali Loss: 0.5152407 Test Loss: 0.3979068
Validation loss decreased (0.520628 --> 0.515241).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.685497522354126
Epoch: 14, Steps: 14 | Train Loss: 0.4466678 Vali Loss: 0.5153941 Test Loss: 0.3962358
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.1507034301757812
Epoch: 15, Steps: 14 | Train Loss: 0.4425593 Vali Loss: 0.5112759 Test Loss: 0.3948563
Validation loss decreased (0.515241 --> 0.511276).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.960472822189331
Epoch: 16, Steps: 14 | Train Loss: 0.4368200 Vali Loss: 0.5084683 Test Loss: 0.3936020
Validation loss decreased (0.511276 --> 0.508468).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.1819229125976562
Epoch: 17, Steps: 14 | Train Loss: 0.4313121 Vali Loss: 0.4954984 Test Loss: 0.3926043
Validation loss decreased (0.508468 --> 0.495498).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.14302921295166
Epoch: 18, Steps: 14 | Train Loss: 0.4275409 Vali Loss: 0.4973900 Test Loss: 0.3917271
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.0090997219085693
Epoch: 19, Steps: 14 | Train Loss: 0.4233177 Vali Loss: 0.4916662 Test Loss: 0.3909113
Validation loss decreased (0.495498 --> 0.491666).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.252904176712036
Epoch: 20, Steps: 14 | Train Loss: 0.4183305 Vali Loss: 0.4957055 Test Loss: 0.3902480
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.870084524154663
Epoch: 21, Steps: 14 | Train Loss: 0.4172208 Vali Loss: 0.4869252 Test Loss: 0.3896534
Validation loss decreased (0.491666 --> 0.486925).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.349318265914917
Epoch: 22, Steps: 14 | Train Loss: 0.4115668 Vali Loss: 0.4921859 Test Loss: 0.3890978
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.431424617767334
Epoch: 23, Steps: 14 | Train Loss: 0.4093842 Vali Loss: 0.4810617 Test Loss: 0.3886329
Validation loss decreased (0.486925 --> 0.481062).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.421902894973755
Epoch: 24, Steps: 14 | Train Loss: 0.4078159 Vali Loss: 0.4839773 Test Loss: 0.3882542
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.1363577842712402
Epoch: 25, Steps: 14 | Train Loss: 0.4041023 Vali Loss: 0.4874388 Test Loss: 0.3878806
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.7777719497680664
Epoch: 26, Steps: 14 | Train Loss: 0.4024818 Vali Loss: 0.4766123 Test Loss: 0.3875337
Validation loss decreased (0.481062 --> 0.476612).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.5203261375427246
Epoch: 27, Steps: 14 | Train Loss: 0.3986791 Vali Loss: 0.4812677 Test Loss: 0.3872566
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.22471284866333
Epoch: 28, Steps: 14 | Train Loss: 0.3966414 Vali Loss: 0.4788438 Test Loss: 0.3869731
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.2893998622894287
Epoch: 29, Steps: 14 | Train Loss: 0.3970608 Vali Loss: 0.4770495 Test Loss: 0.3867059
EarlyStopping counter: 3 out of 3
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  142517760.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.870882749557495
Epoch: 1, Steps: 14 | Train Loss: 0.7015321 Vali Loss: 0.4624288 Test Loss: 0.3805323
Validation loss decreased (inf --> 0.462429).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.683584690093994
Epoch: 2, Steps: 14 | Train Loss: 0.6816244 Vali Loss: 0.4482260 Test Loss: 0.3763200
Validation loss decreased (0.462429 --> 0.448226).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.69799542427063
Epoch: 3, Steps: 14 | Train Loss: 0.6721478 Vali Loss: 0.4419894 Test Loss: 0.3735154
Validation loss decreased (0.448226 --> 0.441989).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.350050449371338
Epoch: 4, Steps: 14 | Train Loss: 0.6644110 Vali Loss: 0.4352363 Test Loss: 0.3713219
Validation loss decreased (0.441989 --> 0.435236).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.276899576187134
Epoch: 5, Steps: 14 | Train Loss: 0.6601419 Vali Loss: 0.4301809 Test Loss: 0.3697421
Validation loss decreased (0.435236 --> 0.430181).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2780137062072754
Epoch: 6, Steps: 14 | Train Loss: 0.6549933 Vali Loss: 0.4222209 Test Loss: 0.3683847
Validation loss decreased (0.430181 --> 0.422221).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.2828381061553955
Epoch: 7, Steps: 14 | Train Loss: 0.6541007 Vali Loss: 0.4184664 Test Loss: 0.3671666
Validation loss decreased (0.422221 --> 0.418466).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.3639440536499023
Epoch: 8, Steps: 14 | Train Loss: 0.6476808 Vali Loss: 0.4220987 Test Loss: 0.3660950
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.487853765487671
Epoch: 9, Steps: 14 | Train Loss: 0.6432597 Vali Loss: 0.4158491 Test Loss: 0.3651537
Validation loss decreased (0.418466 --> 0.415849).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.4719464778900146
Epoch: 10, Steps: 14 | Train Loss: 0.6430987 Vali Loss: 0.4150245 Test Loss: 0.3643433
Validation loss decreased (0.415849 --> 0.415025).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.437310218811035
Epoch: 11, Steps: 14 | Train Loss: 0.6435117 Vali Loss: 0.4142920 Test Loss: 0.3636336
Validation loss decreased (0.415025 --> 0.414292).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.2740232944488525
Epoch: 12, Steps: 14 | Train Loss: 0.6432483 Vali Loss: 0.4094739 Test Loss: 0.3629410
Validation loss decreased (0.414292 --> 0.409474).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.347865581512451
Epoch: 13, Steps: 14 | Train Loss: 0.6395075 Vali Loss: 0.4065855 Test Loss: 0.3625172
Validation loss decreased (0.409474 --> 0.406585).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.076075792312622
Epoch: 14, Steps: 14 | Train Loss: 0.6349550 Vali Loss: 0.4081260 Test Loss: 0.3620235
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.482348918914795
Epoch: 15, Steps: 14 | Train Loss: 0.6403013 Vali Loss: 0.4073933 Test Loss: 0.3615741
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.309845209121704
Epoch: 16, Steps: 14 | Train Loss: 0.6359062 Vali Loss: 0.4081583 Test Loss: 0.3611540
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.36119163036346436, mae:0.40258434414863586, rse:0.48051658272743225, corr:[0.25871125 0.26746067 0.26460359 0.26283914 0.26472265 0.26534972
 0.2636261  0.2624378  0.26224995 0.2614903  0.25978634 0.25829884
 0.25770235 0.2567177  0.25514343 0.25376612 0.2532178  0.2526512
 0.2516426  0.25043985 0.24953328 0.24866182 0.24699068 0.24500087
 0.24350215 0.24272084 0.24178928 0.2405309  0.23954095 0.23901701
 0.23841868 0.23735443 0.23615506 0.23504499 0.23406757 0.23323263
 0.23256058 0.23197259 0.23115121 0.23025115 0.22955392 0.22904865
 0.2285085  0.22774431 0.22671627 0.22565258 0.22438732 0.22272965
 0.22100896 0.21929894 0.21763952 0.21621582 0.21507141 0.21352114
 0.21140943 0.20958023 0.20827846 0.20694166 0.20518728 0.2035679
 0.20277622 0.20271961 0.20271331 0.20234503 0.2017317  0.20148428
 0.20108812 0.20045559 0.19980106 0.19930996 0.19848418 0.19707094
 0.19558747 0.19456312 0.19381155 0.19259237 0.19108337 0.18986708
 0.1892745  0.18893968 0.18822311 0.18684717 0.18565348 0.18529506
 0.18533388 0.18497102 0.18420568 0.18362476 0.18342285 0.18320635
 0.18277873 0.18233383 0.18214853 0.18193989 0.18169405 0.18151748
 0.18130939 0.18072711 0.17984949 0.17902394 0.17854705 0.17798106
 0.17718244 0.17637968 0.1762618  0.17643936 0.176403   0.17574854
 0.17481668 0.17424238 0.17369978 0.1730195  0.17230932 0.1718791
 0.17132877 0.17055808 0.1698375  0.1691122  0.16809668 0.16638438
 0.16453753 0.1633305  0.16263317 0.16151462 0.15994684 0.15895042
 0.15864386 0.1581987  0.15710732 0.15594056 0.1553656  0.15490048
 0.15395205 0.15274248 0.15222505 0.15235631 0.15208973 0.15099522
 0.1496656  0.14891416 0.14854652 0.14799137 0.14701645 0.14570823
 0.14383814 0.14145304 0.13911743 0.13764949 0.13708639 0.13654271
 0.13538092 0.1341413  0.13350663 0.13275574 0.13129115 0.12965746
 0.12872007 0.12832533 0.12777573 0.1269642  0.12629606 0.12613353
 0.12582318 0.12536941 0.12547223 0.12601699 0.12573317 0.12370845
 0.1212264  0.11984471 0.11946213 0.1185316  0.1166477  0.11487239
 0.11435761 0.11406529 0.11308705 0.11167938 0.1105314  0.10985351
 0.10888693 0.10766686 0.10692373 0.10675216 0.10612505 0.1051973
 0.1046963  0.10498427 0.10562533 0.10582346 0.10586049 0.10563333
 0.10503371 0.10391434 0.10270904 0.10245708 0.10279641 0.10254364
 0.10145254 0.10049736 0.10062567 0.10094125 0.10041067 0.0992066
 0.0983886  0.09823526 0.09773747 0.09702142 0.0967466  0.09710374
 0.09722834 0.09703657 0.09728993 0.09795875 0.09792067 0.09623304
 0.09406778 0.09308162 0.09327225 0.0931038  0.09170384 0.09061861
 0.09044953 0.09093843 0.09063794 0.08986402 0.08964013 0.08975583
 0.08902819 0.08766284 0.08693157 0.08791862 0.08925436 0.08987187
 0.08982331 0.0900167  0.09061287 0.09099846 0.09085015 0.09012583
 0.08871749 0.08661114 0.08499556 0.08524618 0.08641343 0.08659689
 0.08572551 0.08477288 0.08489137 0.08562459 0.0858409  0.08550889
 0.08501257 0.08544469 0.08647835 0.08796995 0.08928595 0.09057074
 0.09132043 0.09167109 0.09209909 0.09269537 0.09366499 0.09359418
 0.09284851 0.09241255 0.09341925 0.09467115 0.09452467 0.09376353
 0.09383445 0.09504649 0.09629259 0.09631671 0.09623412 0.09596586
 0.09568985 0.0953952  0.09558873 0.09645738 0.09728653 0.09745836
 0.09743418 0.09804373 0.09838426 0.09789834 0.09764259 0.09868064
 0.09974457 0.09889107 0.09649608 0.09562314 0.09697586 0.09776483
 0.09626229 0.09373256 0.09326167 0.0948861  0.09628099 0.09522939
 0.09319814 0.09324396 0.09479973 0.09614176 0.09590599 0.09635612
 0.09672492 0.09688875 0.09722561 0.09883974 0.10041992 0.09953359
 0.0972838  0.0963642  0.0980078  0.09832768 0.09560747 0.09312552
 0.09470163 0.09841609 0.09885315 0.09722056 0.09735748 0.10028294
 0.10112395 0.09938578 0.0994669  0.10251752 0.1041676  0.10259146
 0.10147122 0.10315072 0.10353293 0.10127939 0.10771167 0.11954261]
