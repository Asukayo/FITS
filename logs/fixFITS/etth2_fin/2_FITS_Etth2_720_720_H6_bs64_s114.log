Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.221980094909668
Epoch: 1, Steps: 56 | Train Loss: 0.8439728 Vali Loss: 0.8233936 Test Loss: 0.4685059
Validation loss decreased (inf --> 0.823394).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.729665040969849
Epoch: 2, Steps: 56 | Train Loss: 0.6805049 Vali Loss: 0.7694822 Test Loss: 0.4303380
Validation loss decreased (0.823394 --> 0.769482).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 7.752500772476196
Epoch: 3, Steps: 56 | Train Loss: 0.6127637 Vali Loss: 0.7426922 Test Loss: 0.4157301
Validation loss decreased (0.769482 --> 0.742692).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 7.981153726577759
Epoch: 4, Steps: 56 | Train Loss: 0.5776770 Vali Loss: 0.7294474 Test Loss: 0.4091251
Validation loss decreased (0.742692 --> 0.729447).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.111960411071777
Epoch: 5, Steps: 56 | Train Loss: 0.5549880 Vali Loss: 0.7190615 Test Loss: 0.4054163
Validation loss decreased (0.729447 --> 0.719061).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.324726819992065
Epoch: 6, Steps: 56 | Train Loss: 0.5381637 Vali Loss: 0.7112554 Test Loss: 0.4029370
Validation loss decreased (0.719061 --> 0.711255).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.650648355484009
Epoch: 7, Steps: 56 | Train Loss: 0.5241843 Vali Loss: 0.7065098 Test Loss: 0.4010228
Validation loss decreased (0.711255 --> 0.706510).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.316894054412842
Epoch: 8, Steps: 56 | Train Loss: 0.5128724 Vali Loss: 0.6988652 Test Loss: 0.3995028
Validation loss decreased (0.706510 --> 0.698865).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.479479312896729
Epoch: 9, Steps: 56 | Train Loss: 0.5035322 Vali Loss: 0.6985557 Test Loss: 0.3981951
Validation loss decreased (0.698865 --> 0.698556).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 7.016199111938477
Epoch: 10, Steps: 56 | Train Loss: 0.4956075 Vali Loss: 0.6957970 Test Loss: 0.3969595
Validation loss decreased (0.698556 --> 0.695797).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 7.141724109649658
Epoch: 11, Steps: 56 | Train Loss: 0.4883749 Vali Loss: 0.6897939 Test Loss: 0.3958244
Validation loss decreased (0.695797 --> 0.689794).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 6.25992226600647
Epoch: 12, Steps: 56 | Train Loss: 0.4822478 Vali Loss: 0.6901047 Test Loss: 0.3948789
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.823612451553345
Epoch: 13, Steps: 56 | Train Loss: 0.4766132 Vali Loss: 0.6871611 Test Loss: 0.3939822
Validation loss decreased (0.689794 --> 0.687161).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 9.600173950195312
Epoch: 14, Steps: 56 | Train Loss: 0.4701826 Vali Loss: 0.6823565 Test Loss: 0.3931614
Validation loss decreased (0.687161 --> 0.682356).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.948527574539185
Epoch: 15, Steps: 56 | Train Loss: 0.4671918 Vali Loss: 0.6847184 Test Loss: 0.3923631
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.077828645706177
Epoch: 16, Steps: 56 | Train Loss: 0.4634589 Vali Loss: 0.6801493 Test Loss: 0.3917192
Validation loss decreased (0.682356 --> 0.680149).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.41506028175354
Epoch: 17, Steps: 56 | Train Loss: 0.4599186 Vali Loss: 0.6837078 Test Loss: 0.3910541
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.341736793518066
Epoch: 18, Steps: 56 | Train Loss: 0.4564750 Vali Loss: 0.6809015 Test Loss: 0.3904948
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 9.56208324432373
Epoch: 19, Steps: 56 | Train Loss: 0.4536076 Vali Loss: 0.6784077 Test Loss: 0.3899240
Validation loss decreased (0.680149 --> 0.678408).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 8.081756591796875
Epoch: 20, Steps: 56 | Train Loss: 0.4510100 Vali Loss: 0.6778778 Test Loss: 0.3894759
Validation loss decreased (0.678408 --> 0.677878).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.660921335220337
Epoch: 21, Steps: 56 | Train Loss: 0.4487929 Vali Loss: 0.6743657 Test Loss: 0.3890871
Validation loss decreased (0.677878 --> 0.674366).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.172810554504395
Epoch: 22, Steps: 56 | Train Loss: 0.4469336 Vali Loss: 0.6716020 Test Loss: 0.3886207
Validation loss decreased (0.674366 --> 0.671602).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 8.081718921661377
Epoch: 23, Steps: 56 | Train Loss: 0.4456039 Vali Loss: 0.6772313 Test Loss: 0.3882783
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 9.694140434265137
Epoch: 24, Steps: 56 | Train Loss: 0.4427195 Vali Loss: 0.6729290 Test Loss: 0.3879431
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 9.651304483413696
Epoch: 25, Steps: 56 | Train Loss: 0.4416257 Vali Loss: 0.6720150 Test Loss: 0.3876270
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.268278121948242
Epoch: 1, Steps: 56 | Train Loss: 0.8242721 Vali Loss: 0.6622004 Test Loss: 0.3850488
Validation loss decreased (inf --> 0.662200).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.830213785171509
Epoch: 2, Steps: 56 | Train Loss: 0.8149389 Vali Loss: 0.6622275 Test Loss: 0.3830966
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.753318309783936
Epoch: 3, Steps: 56 | Train Loss: 0.8107305 Vali Loss: 0.6542888 Test Loss: 0.3818598
Validation loss decreased (0.662200 --> 0.654289).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.488332033157349
Epoch: 4, Steps: 56 | Train Loss: 0.8059181 Vali Loss: 0.6518726 Test Loss: 0.3809408
Validation loss decreased (0.654289 --> 0.651873).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 9.51325798034668
Epoch: 5, Steps: 56 | Train Loss: 0.8027171 Vali Loss: 0.6512809 Test Loss: 0.3802934
Validation loss decreased (0.651873 --> 0.651281).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 9.362277507781982
Epoch: 6, Steps: 56 | Train Loss: 0.8028833 Vali Loss: 0.6502948 Test Loss: 0.3798278
Validation loss decreased (0.651281 --> 0.650295).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 10.10980486869812
Epoch: 7, Steps: 56 | Train Loss: 0.8017064 Vali Loss: 0.6463345 Test Loss: 0.3795875
Validation loss decreased (0.650295 --> 0.646334).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.972638607025146
Epoch: 8, Steps: 56 | Train Loss: 0.8004433 Vali Loss: 0.6472235 Test Loss: 0.3793992
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.126928806304932
Epoch: 9, Steps: 56 | Train Loss: 0.7995777 Vali Loss: 0.6478460 Test Loss: 0.3792576
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 9.41005539894104
Epoch: 10, Steps: 56 | Train Loss: 0.7986384 Vali Loss: 0.6479424 Test Loss: 0.3792088
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.37816867232322693, mae:0.4231681525707245, rse:0.4915282726287842, corr:[ 0.2196582   0.22225565  0.22042173  0.22035466  0.22038707  0.2191513
  0.21833174  0.21812254  0.21721387  0.2155044   0.2142542   0.2132852
  0.21198685  0.21057905  0.20949218  0.20875761  0.20813435  0.20744617
  0.20650578  0.20530574  0.20421936  0.20336074  0.20219827  0.20078619
  0.19952542  0.19845271  0.1972008   0.19593115  0.19529895  0.19494876
  0.19427104  0.19323222  0.1924715   0.19185019  0.19080946  0.18957561
  0.1887442   0.18839999  0.18775426  0.18666591  0.18573995  0.18515438
  0.18460375  0.18388523  0.18307094  0.18229996  0.18139513  0.17992407
  0.17841737  0.17726627  0.17636302  0.17543165  0.17478247  0.17450498
  0.17425436  0.173846    0.1728098   0.17163433  0.17114833  0.1711279
  0.17066889  0.16993175  0.17008267  0.17102642  0.17141353  0.17135935
  0.17108722  0.17083803  0.17091402  0.17118815  0.17110962  0.17062788
  0.17030948  0.17012371  0.16940501  0.16823028  0.16806102  0.16871
  0.1685633   0.16757083  0.16729292  0.16778219  0.16777769  0.16734867
  0.1674541   0.16800848  0.16800609  0.16762067  0.16770059  0.16806826
  0.16773783  0.16688925  0.16689616  0.16749004  0.1679248   0.16787905
  0.16772565  0.16757871  0.16733041  0.16716686  0.16716716  0.16677819
  0.16645467  0.16658786  0.16707632  0.16688122  0.1663274   0.16629542
  0.16659705  0.16662028  0.16629373  0.16609342  0.16600637  0.1659832
  0.16597462  0.16604672  0.16611452  0.1657514   0.16498113  0.16356231
  0.16202743  0.16114089  0.16094722  0.16059963  0.15993075  0.15967186
  0.15960301  0.15885201  0.15773483  0.1571047   0.15715048  0.1569415
  0.1563768   0.15565081  0.1549377   0.15438439  0.15395501  0.15373702
  0.15325828  0.15264015  0.15260383  0.15267096  0.15223435  0.15113074
  0.1497757   0.14877146  0.1480817   0.1478568   0.14785038  0.1474408
  0.14657019  0.14597677  0.14620836  0.14615546  0.14525172  0.1441743
  0.14356428  0.14312781  0.14261174  0.14229776  0.14216265  0.14227264
  0.14220963  0.14189093  0.14187111  0.14217521  0.14244545  0.14200415
  0.14126667  0.14078413  0.14072114  0.14062983  0.14040512  0.14007752
  0.1395199   0.13837315  0.13713783  0.13657646  0.13645849  0.13629913
  0.1358314   0.13522716  0.13490757  0.13501602  0.13529108  0.13546698
  0.1351723   0.1350985   0.1356975   0.13640442  0.13684443  0.13684464
  0.13680108  0.13699637  0.13727653  0.13780616  0.13833137  0.13838516
  0.13823344  0.13778321  0.13735184  0.13718714  0.13753799  0.13756125
  0.13693513  0.13641039  0.13633794  0.13660468  0.13709086  0.13817568
  0.13924637  0.13930804  0.13907798  0.13980445  0.14081532  0.14012656
  0.13827491  0.13728921  0.1374624   0.13760924  0.13741647  0.13812944
  0.13886246  0.13877007  0.13819617  0.13817108  0.13835779  0.13806577
  0.13783002  0.13757566  0.13691963  0.13668665  0.13788044  0.13961501
  0.14007662  0.13979004  0.14039896  0.14151421  0.14211579  0.14264864
  0.1435244   0.1435877   0.14298943  0.14378445  0.14594108  0.1467586
  0.14525451  0.143951    0.14489867  0.14622833  0.14640245  0.14661607
  0.1477067   0.14844961  0.1480673   0.14841574  0.15030548  0.1523406
  0.1529748   0.1528868   0.15316121  0.15377639  0.15446506  0.15518242
  0.15600878  0.15600714  0.15512572  0.154778    0.15589462  0.15785298
  0.1588967   0.15875478  0.15851595  0.15875135  0.15930054  0.15972954
  0.15999323  0.16023931  0.16026187  0.16034694  0.1608765   0.16169432
  0.16227916  0.16261846  0.16306981  0.16389921  0.16469873  0.1649624
  0.16509576  0.1653497   0.16530308  0.16480894  0.16441754  0.16471872
  0.16503873  0.1645318   0.16397664  0.1646047   0.16586073  0.16606691
  0.16531841  0.16527608  0.16610494  0.16660619  0.1665506   0.16710863
  0.16803713  0.1687853   0.1690436   0.16934517  0.16974151  0.1696941
  0.16907568  0.16811794  0.16768649  0.1677479   0.16792011  0.16764283
  0.16720262  0.16749746  0.1681601   0.16816679  0.16710703  0.16632113
  0.16622654  0.16620281  0.16642383  0.16720861  0.16799898  0.16761407
  0.16669321  0.16726065  0.16897632  0.17025122  0.17097692  0.17167194
  0.172074    0.17186344  0.1715407   0.17161068  0.17201628  0.1722253
  0.17262755  0.17308865  0.17293133  0.17199253  0.17105024  0.17105341
  0.1714641   0.17215957  0.17311695  0.17384198  0.17396581  0.17401545
  0.17425409  0.17478079  0.17582726  0.17742054  0.17813747  0.17727552
  0.17644794  0.17703985  0.17811762  0.17838344  0.17848723  0.17914693
  0.17986685  0.18022838  0.1806561   0.18142517  0.18122149  0.17997961
  0.179153    0.17953989  0.18016501  0.18026239  0.1804067   0.18098089
  0.18144542  0.1812568   0.18062267  0.18018164  0.1802999   0.18048787
  0.179982    0.17907098  0.17877558  0.17951585  0.18070857  0.18142313
  0.1814679   0.1813534   0.18162939  0.18223675  0.18258432  0.18239152
  0.18233334  0.18254873  0.18282533  0.18240659  0.18122014  0.18050633
  0.18095602  0.18168429  0.18162628  0.18118778  0.18108825  0.18101388
  0.18034019  0.17977254  0.17996433  0.180097    0.17947888  0.17880628
  0.17907807  0.17979662  0.17982906  0.17934264  0.17907126  0.17886625
  0.17817064  0.17772476  0.1774198   0.17613241  0.17405832  0.1730714
  0.17341137  0.17290892  0.17137575  0.17057522  0.17076132  0.17062844
  0.16967997  0.16905063  0.16900979  0.16847524  0.16787551  0.16788112
  0.16782883  0.16724807  0.1664975   0.16624424  0.16558038  0.16454755
  0.16364823  0.16307779  0.16209674  0.16112304  0.16091579  0.16116886
  0.16101034  0.16039781  0.15968324  0.15940659  0.15956968  0.15977393
  0.15955088  0.15876721  0.15834823  0.15904424  0.15979858  0.15944153
  0.15852028  0.15822306  0.15828493  0.1581491   0.15731077  0.15606134
  0.15525965  0.1546      0.15396248  0.15348357  0.1536226   0.15412061
  0.15429929  0.15387265  0.15340242  0.1535248   0.15353587  0.15280035
  0.15189356  0.15175323  0.15186422  0.15099606  0.14949764  0.14829859
  0.14787307  0.14766768  0.1471345   0.14619623  0.14553563  0.14502978
  0.14406332  0.14287493  0.14248241  0.14296561  0.14284568  0.14169642
  0.14038672  0.13964424  0.13917626  0.13843307  0.13772824  0.13731615
  0.13703935  0.1363894   0.13507791  0.13361011  0.13301565  0.13331278
  0.13317198  0.13237196  0.1317411   0.13162348  0.13110784  0.12946522
  0.12784858  0.12716986  0.12677135  0.12594657  0.12568274  0.12630373
  0.12609336  0.12485917  0.12414743  0.12420591  0.12319461  0.12104478
  0.11971878  0.11993696  0.11969993  0.11794808  0.11641943  0.11537181
  0.11353155  0.11106938  0.11007801  0.11063468  0.11028688  0.10803033
  0.105808    0.10568015  0.10683478  0.10688919  0.10496858  0.10253485
  0.10069484  0.10009367  0.10042284  0.10071448  0.10065916  0.09970887
  0.09818374  0.09638491  0.09434419  0.0923854   0.09152836  0.09151398
  0.09067209  0.08894948  0.08878427  0.09007101  0.09024161  0.08809481
  0.08637002  0.08697085  0.0876781   0.0862608   0.08471359  0.08468438
  0.08475947  0.08355207  0.08208633  0.08145005  0.08035799  0.0782229
  0.07635818  0.07583974  0.07525509  0.07378602  0.07247221  0.07185963
  0.07122513  0.07003742  0.0690918   0.06916683  0.06927243  0.06823255
  0.06617355  0.06490723  0.0651525   0.06549869  0.06511462  0.0644391
  0.06379499  0.06292181  0.06178318  0.0613551   0.06119858  0.0598022
  0.05736687  0.05622074  0.05635358  0.05550086  0.05242028  0.05015637
  0.05038495  0.05117635  0.05026801  0.0485006   0.04765115  0.04764939
  0.04784147  0.04747766  0.046482    0.04558175  0.04556949  0.04605136
  0.04520752  0.04355116  0.04294289  0.0439093   0.04427575  0.04351905
  0.04253659  0.04216409  0.04194597  0.0409784   0.03967497  0.03941819
  0.04006243  0.04026596  0.03958587  0.0392299   0.03967357  0.03984602
  0.03837707  0.03659979  0.03665973  0.03709555  0.03531312  0.0329791
  0.0323561   0.03266183  0.03248656  0.03228787  0.03211683  0.03134563
  0.02950603  0.02796694  0.02764038  0.027743    0.02792294  0.02763914
  0.02668888  0.02568968  0.02565256  0.02639002  0.02646576  0.02576084
  0.02525385  0.02514547  0.02472632  0.02432763  0.02535333  0.0264743
  0.02582278  0.02462424  0.02488871  0.02613387  0.02604109  0.0250715
  0.02465615  0.02441021  0.02331376  0.02199354  0.02232485  0.02281653
  0.02243994  0.02098982  0.0205101   0.02092897  0.0211359   0.0208282
  0.0202844   0.01941371  0.01924204  0.01902193  0.01816431  0.01783611
  0.01863055  0.01833725  0.0169357   0.01586889  0.01586357  0.01477966
  0.01217685  0.01019589  0.01027337  0.01013098  0.0092512   0.00870979
  0.00910676  0.00800787  0.00572586  0.00482441  0.00602681  0.00761836
  0.00668786  0.00344034  0.00257741  0.00414636  0.00425319  0.00201607
  0.00075374  0.00083734 -0.00266462 -0.00714748 -0.00303123  0.0042949 ]
