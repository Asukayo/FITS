Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30898560.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.2776358127594
Epoch: 1, Steps: 60 | Train Loss: 0.7266541 Vali Loss: 0.3898551 Test Loss: 0.3916116
Validation loss decreased (inf --> 0.389855).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 6.631613254547119
Epoch: 2, Steps: 60 | Train Loss: 0.5932668 Vali Loss: 0.3427807 Test Loss: 0.3706144
Validation loss decreased (0.389855 --> 0.342781).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 7.03270411491394
Epoch: 3, Steps: 60 | Train Loss: 0.5616183 Vali Loss: 0.3250397 Test Loss: 0.3638083
Validation loss decreased (0.342781 --> 0.325040).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.088441133499146
Epoch: 4, Steps: 60 | Train Loss: 0.5507412 Vali Loss: 0.3163123 Test Loss: 0.3604243
Validation loss decreased (0.325040 --> 0.316312).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.230480909347534
Epoch: 5, Steps: 60 | Train Loss: 0.5412737 Vali Loss: 0.3103412 Test Loss: 0.3588821
Validation loss decreased (0.316312 --> 0.310341).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 8.212502479553223
Epoch: 6, Steps: 60 | Train Loss: 0.5375219 Vali Loss: 0.3057990 Test Loss: 0.3580307
Validation loss decreased (0.310341 --> 0.305799).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 8.070168256759644
Epoch: 7, Steps: 60 | Train Loss: 0.5328516 Vali Loss: 0.3027614 Test Loss: 0.3572851
Validation loss decreased (0.305799 --> 0.302761).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 7.866120100021362
Epoch: 8, Steps: 60 | Train Loss: 0.5307522 Vali Loss: 0.3002657 Test Loss: 0.3567329
Validation loss decreased (0.302761 --> 0.300266).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.052056074142456
Epoch: 9, Steps: 60 | Train Loss: 0.5279147 Vali Loss: 0.2986348 Test Loss: 0.3560706
Validation loss decreased (0.300266 --> 0.298635).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 8.306106805801392
Epoch: 10, Steps: 60 | Train Loss: 0.5257669 Vali Loss: 0.2975631 Test Loss: 0.3557432
Validation loss decreased (0.298635 --> 0.297563).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 7.932776212692261
Epoch: 11, Steps: 60 | Train Loss: 0.5233510 Vali Loss: 0.2954721 Test Loss: 0.3554564
Validation loss decreased (0.297563 --> 0.295472).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 7.960496425628662
Epoch: 12, Steps: 60 | Train Loss: 0.5231030 Vali Loss: 0.2943934 Test Loss: 0.3552513
Validation loss decreased (0.295472 --> 0.294393).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 7.704445123672485
Epoch: 13, Steps: 60 | Train Loss: 0.5200760 Vali Loss: 0.2934603 Test Loss: 0.3549142
Validation loss decreased (0.294393 --> 0.293460).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 8.278947830200195
Epoch: 14, Steps: 60 | Train Loss: 0.5210341 Vali Loss: 0.2923722 Test Loss: 0.3547854
Validation loss decreased (0.293460 --> 0.292372).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 8.521283626556396
Epoch: 15, Steps: 60 | Train Loss: 0.5200485 Vali Loss: 0.2917769 Test Loss: 0.3545645
Validation loss decreased (0.292372 --> 0.291777).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 9.36764907836914
Epoch: 16, Steps: 60 | Train Loss: 0.5185585 Vali Loss: 0.2911856 Test Loss: 0.3543299
Validation loss decreased (0.291777 --> 0.291186).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 9.340134382247925
Epoch: 17, Steps: 60 | Train Loss: 0.5192785 Vali Loss: 0.2902365 Test Loss: 0.3542388
Validation loss decreased (0.291186 --> 0.290237).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 9.452422142028809
Epoch: 18, Steps: 60 | Train Loss: 0.5185035 Vali Loss: 0.2900782 Test Loss: 0.3540691
Validation loss decreased (0.290237 --> 0.290078).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 8.257438898086548
Epoch: 19, Steps: 60 | Train Loss: 0.5187688 Vali Loss: 0.2896958 Test Loss: 0.3538692
Validation loss decreased (0.290078 --> 0.289696).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 7.902633190155029
Epoch: 20, Steps: 60 | Train Loss: 0.5176193 Vali Loss: 0.2890955 Test Loss: 0.3538806
Validation loss decreased (0.289696 --> 0.289096).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.128326416015625
Epoch: 21, Steps: 60 | Train Loss: 0.5173753 Vali Loss: 0.2884203 Test Loss: 0.3538394
Validation loss decreased (0.289096 --> 0.288420).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 7.790339469909668
Epoch: 22, Steps: 60 | Train Loss: 0.5164729 Vali Loss: 0.2883065 Test Loss: 0.3535862
Validation loss decreased (0.288420 --> 0.288306).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 7.781290292739868
Epoch: 23, Steps: 60 | Train Loss: 0.5160957 Vali Loss: 0.2880588 Test Loss: 0.3536511
Validation loss decreased (0.288306 --> 0.288059).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 7.872460126876831
Epoch: 24, Steps: 60 | Train Loss: 0.5156087 Vali Loss: 0.2877955 Test Loss: 0.3534492
Validation loss decreased (0.288059 --> 0.287796).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 7.170219659805298
Epoch: 25, Steps: 60 | Train Loss: 0.5158447 Vali Loss: 0.2875124 Test Loss: 0.3534310
Validation loss decreased (0.287796 --> 0.287512).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 7.349048376083374
Epoch: 26, Steps: 60 | Train Loss: 0.5155871 Vali Loss: 0.2871794 Test Loss: 0.3533522
Validation loss decreased (0.287512 --> 0.287179).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 6.6150429248809814
Epoch: 27, Steps: 60 | Train Loss: 0.5140928 Vali Loss: 0.2871089 Test Loss: 0.3532560
Validation loss decreased (0.287179 --> 0.287109).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 6.471440315246582
Epoch: 28, Steps: 60 | Train Loss: 0.5150420 Vali Loss: 0.2866507 Test Loss: 0.3532546
Validation loss decreased (0.287109 --> 0.286651).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 6.33133602142334
Epoch: 29, Steps: 60 | Train Loss: 0.5132926 Vali Loss: 0.2867733 Test Loss: 0.3532143
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 7.182353973388672
Epoch: 30, Steps: 60 | Train Loss: 0.5139839 Vali Loss: 0.2863573 Test Loss: 0.3532096
Validation loss decreased (0.286651 --> 0.286357).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 8.476462125778198
Epoch: 31, Steps: 60 | Train Loss: 0.5141710 Vali Loss: 0.2863352 Test Loss: 0.3531053
Validation loss decreased (0.286357 --> 0.286335).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 9.887036800384521
Epoch: 32, Steps: 60 | Train Loss: 0.5141574 Vali Loss: 0.2862396 Test Loss: 0.3530234
Validation loss decreased (0.286335 --> 0.286240).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.663677453994751
Epoch: 33, Steps: 60 | Train Loss: 0.5145196 Vali Loss: 0.2857623 Test Loss: 0.3530904
Validation loss decreased (0.286240 --> 0.285762).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 9.464687585830688
Epoch: 34, Steps: 60 | Train Loss: 0.5124090 Vali Loss: 0.2856827 Test Loss: 0.3530016
Validation loss decreased (0.285762 --> 0.285683).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 9.972703456878662
Epoch: 35, Steps: 60 | Train Loss: 0.5141201 Vali Loss: 0.2858026 Test Loss: 0.3529930
EarlyStopping counter: 1 out of 3
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 9.94507646560669
Epoch: 36, Steps: 60 | Train Loss: 0.5133886 Vali Loss: 0.2856097 Test Loss: 0.3529982
Validation loss decreased (0.285683 --> 0.285610).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 10.192245960235596
Epoch: 37, Steps: 60 | Train Loss: 0.5120657 Vali Loss: 0.2854760 Test Loss: 0.3529763
Validation loss decreased (0.285610 --> 0.285476).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 10.025991201400757
Epoch: 38, Steps: 60 | Train Loss: 0.5139998 Vali Loss: 0.2855008 Test Loss: 0.3529128
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 9.928633689880371
Epoch: 39, Steps: 60 | Train Loss: 0.5125924 Vali Loss: 0.2853462 Test Loss: 0.3528875
Validation loss decreased (0.285476 --> 0.285346).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 9.020829200744629
Epoch: 40, Steps: 60 | Train Loss: 0.5103798 Vali Loss: 0.2852024 Test Loss: 0.3529533
Validation loss decreased (0.285346 --> 0.285202).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 9.351933717727661
Epoch: 41, Steps: 60 | Train Loss: 0.5132449 Vali Loss: 0.2850768 Test Loss: 0.3528815
Validation loss decreased (0.285202 --> 0.285077).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 8.297426462173462
Epoch: 42, Steps: 60 | Train Loss: 0.5136678 Vali Loss: 0.2851017 Test Loss: 0.3528769
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 8.017386436462402
Epoch: 43, Steps: 60 | Train Loss: 0.5130777 Vali Loss: 0.2850882 Test Loss: 0.3528341
EarlyStopping counter: 2 out of 3
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 9.545539617538452
Epoch: 44, Steps: 60 | Train Loss: 0.5113902 Vali Loss: 0.2849270 Test Loss: 0.3528809
Validation loss decreased (0.285077 --> 0.284927).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 9.296221733093262
Epoch: 45, Steps: 60 | Train Loss: 0.5132771 Vali Loss: 0.2848418 Test Loss: 0.3528007
Validation loss decreased (0.284927 --> 0.284842).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 9.810667991638184
Epoch: 46, Steps: 60 | Train Loss: 0.5124334 Vali Loss: 0.2848972 Test Loss: 0.3528116
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 9.438971757888794
Epoch: 47, Steps: 60 | Train Loss: 0.5131157 Vali Loss: 0.2848103 Test Loss: 0.3528063
Validation loss decreased (0.284842 --> 0.284810).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 9.665807485580444
Epoch: 48, Steps: 60 | Train Loss: 0.5119026 Vali Loss: 0.2846670 Test Loss: 0.3528280
Validation loss decreased (0.284810 --> 0.284667).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 9.084281206130981
Epoch: 49, Steps: 60 | Train Loss: 0.5120890 Vali Loss: 0.2847680 Test Loss: 0.3527630
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 9.066357135772705
Epoch: 50, Steps: 60 | Train Loss: 0.5108613 Vali Loss: 0.2846214 Test Loss: 0.3527736
Validation loss decreased (0.284667 --> 0.284621).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3320881426334381, mae:0.3751178979873657, rse:0.46213358640670776, corr:[0.26221156 0.26744375 0.2654131  0.26548457 0.26632777 0.26567304
 0.26460806 0.2642705  0.2638683  0.2625823  0.26107517 0.2598746
 0.25890198 0.25794968 0.25711536 0.256675   0.25639492 0.25575662
 0.2545199  0.25323084 0.25214502 0.25114623 0.24971353 0.24787977
 0.24600494 0.24442427 0.24296696 0.24139333 0.23977281 0.23836748
 0.2373641  0.23623016 0.23496154 0.23354405 0.23238611 0.23151733
 0.23057373 0.22944458 0.22841533 0.22771494 0.2271686  0.22644952
 0.22550467 0.224635   0.22392035 0.22304961 0.22160389 0.21967405
 0.21786956 0.21634415 0.21480472 0.21315715 0.2116508  0.20998624
 0.20789742 0.20609693 0.20470256 0.20322102 0.20165287 0.20041034
 0.19991155 0.1996478  0.19944553 0.19920233 0.19868316 0.1982036
 0.19763422 0.19703409 0.1965093  0.19584836 0.1949441  0.19414663
 0.19347127 0.19249858 0.1910013  0.18939388 0.18871963 0.18860458
 0.18806228 0.18687707 0.18629348 0.18619807 0.18586364 0.18513224
 0.18461931 0.18474191 0.18481384 0.18430494 0.18346195 0.1830668
 0.1829749  0.18251434 0.18206804 0.18183595 0.18176079 0.18134548
 0.18045391 0.17956263 0.17888981 0.17799571 0.17675993 0.17562874
 0.17515518 0.17472048 0.17399673 0.17313024 0.17268889 0.17269556
 0.1720306  0.17091945 0.17004833 0.16986407 0.16948865 0.16888298
 0.1683479  0.16810349 0.1677121  0.16642073 0.16486666 0.16362123
 0.16274092 0.16137014 0.15976286 0.1585226  0.15765472 0.1566329
 0.15542541 0.15447612 0.1542016  0.15369272 0.15239204 0.15080708
 0.15028557 0.15041098 0.14985535 0.1486785  0.14791521 0.14796272
 0.14766645 0.14656785 0.14576022 0.1457336  0.14541471 0.14364819
 0.14119343 0.13979745 0.13908888 0.13758141 0.13551453 0.1344899
 0.13484311 0.13429616 0.13281664 0.13172895 0.13179754 0.13169469
 0.13088721 0.13027819 0.13094969 0.13193901 0.13115202 0.1298426
 0.12979789 0.13055983 0.13035548 0.12918669 0.12902579 0.12943584
 0.12839772 0.1254325  0.12371489 0.12426504 0.12459372 0.12182298
 0.11849684 0.11765917 0.11835495 0.11728512 0.11414082 0.11354151
 0.11542826 0.11561769 0.11389908 0.11440331 0.11697838 0.1172683
 0.1144331  0.11397849 0.11900641 0.12026955 0.11717638 0.12704603]
