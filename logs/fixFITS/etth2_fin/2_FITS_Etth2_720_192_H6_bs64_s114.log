Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 7.160537242889404
Epoch: 1, Steps: 60 | Train Loss: 0.5838889 Vali Loss: 0.4684516 Test Loss: 0.4426567
Validation loss decreased (inf --> 0.468452).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.286020517349243
Epoch: 2, Steps: 60 | Train Loss: 0.4525189 Vali Loss: 0.4096630 Test Loss: 0.4072169
Validation loss decreased (0.468452 --> 0.409663).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 7.716751575469971
Epoch: 3, Steps: 60 | Train Loss: 0.3882278 Vali Loss: 0.3844860 Test Loss: 0.3943003
Validation loss decreased (0.409663 --> 0.384486).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 7.757628679275513
Epoch: 4, Steps: 60 | Train Loss: 0.3515529 Vali Loss: 0.3721849 Test Loss: 0.3889666
Validation loss decreased (0.384486 --> 0.372185).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 7.269676923751831
Epoch: 5, Steps: 60 | Train Loss: 0.3250415 Vali Loss: 0.3648631 Test Loss: 0.3864702
Validation loss decreased (0.372185 --> 0.364863).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 7.466310739517212
Epoch: 6, Steps: 60 | Train Loss: 0.3036579 Vali Loss: 0.3598995 Test Loss: 0.3846235
Validation loss decreased (0.364863 --> 0.359899).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 7.839120149612427
Epoch: 7, Steps: 60 | Train Loss: 0.2870255 Vali Loss: 0.3558949 Test Loss: 0.3835575
Validation loss decreased (0.359899 --> 0.355895).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 7.507869005203247
Epoch: 8, Steps: 60 | Train Loss: 0.2723687 Vali Loss: 0.3524753 Test Loss: 0.3824368
Validation loss decreased (0.355895 --> 0.352475).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 7.807880401611328
Epoch: 9, Steps: 60 | Train Loss: 0.2600146 Vali Loss: 0.3492812 Test Loss: 0.3811371
Validation loss decreased (0.352475 --> 0.349281).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 7.913757085800171
Epoch: 10, Steps: 60 | Train Loss: 0.2491488 Vali Loss: 0.3464760 Test Loss: 0.3801704
Validation loss decreased (0.349281 --> 0.346476).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 8.149858951568604
Epoch: 11, Steps: 60 | Train Loss: 0.2394910 Vali Loss: 0.3438695 Test Loss: 0.3793511
Validation loss decreased (0.346476 --> 0.343870).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 7.7576775550842285
Epoch: 12, Steps: 60 | Train Loss: 0.2309881 Vali Loss: 0.3418820 Test Loss: 0.3783929
Validation loss decreased (0.343870 --> 0.341882).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 8.304028272628784
Epoch: 13, Steps: 60 | Train Loss: 0.2236629 Vali Loss: 0.3395342 Test Loss: 0.3773000
Validation loss decreased (0.341882 --> 0.339534).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 7.884640216827393
Epoch: 14, Steps: 60 | Train Loss: 0.2167430 Vali Loss: 0.3372633 Test Loss: 0.3764043
Validation loss decreased (0.339534 --> 0.337263).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 7.980915069580078
Epoch: 15, Steps: 60 | Train Loss: 0.2105969 Vali Loss: 0.3354909 Test Loss: 0.3754791
Validation loss decreased (0.337263 --> 0.335491).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 8.503633260726929
Epoch: 16, Steps: 60 | Train Loss: 0.2051959 Vali Loss: 0.3337315 Test Loss: 0.3745161
Validation loss decreased (0.335491 --> 0.333732).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 7.874394178390503
Epoch: 17, Steps: 60 | Train Loss: 0.2002376 Vali Loss: 0.3320148 Test Loss: 0.3737533
Validation loss decreased (0.333732 --> 0.332015).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 8.262006521224976
Epoch: 18, Steps: 60 | Train Loss: 0.1956052 Vali Loss: 0.3300182 Test Loss: 0.3728829
Validation loss decreased (0.332015 --> 0.330018).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 8.021551609039307
Epoch: 19, Steps: 60 | Train Loss: 0.1918861 Vali Loss: 0.3288620 Test Loss: 0.3723159
Validation loss decreased (0.330018 --> 0.328862).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 7.3910040855407715
Epoch: 20, Steps: 60 | Train Loss: 0.1878119 Vali Loss: 0.3274131 Test Loss: 0.3715492
Validation loss decreased (0.328862 --> 0.327413).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 8.097176551818848
Epoch: 21, Steps: 60 | Train Loss: 0.1850495 Vali Loss: 0.3261875 Test Loss: 0.3709660
Validation loss decreased (0.327413 --> 0.326187).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 8.42497444152832
Epoch: 22, Steps: 60 | Train Loss: 0.1817537 Vali Loss: 0.3250787 Test Loss: 0.3704019
Validation loss decreased (0.326187 --> 0.325079).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 5.805246353149414
Epoch: 23, Steps: 60 | Train Loss: 0.1791024 Vali Loss: 0.3239585 Test Loss: 0.3697902
Validation loss decreased (0.325079 --> 0.323958).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.795999526977539
Epoch: 24, Steps: 60 | Train Loss: 0.1763831 Vali Loss: 0.3227678 Test Loss: 0.3692757
Validation loss decreased (0.323958 --> 0.322768).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.93652606010437
Epoch: 25, Steps: 60 | Train Loss: 0.1742588 Vali Loss: 0.3219352 Test Loss: 0.3687202
Validation loss decreased (0.322768 --> 0.321935).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 5.027217388153076
Epoch: 26, Steps: 60 | Train Loss: 0.1721736 Vali Loss: 0.3210953 Test Loss: 0.3683213
Validation loss decreased (0.321935 --> 0.321095).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 6.192252159118652
Epoch: 27, Steps: 60 | Train Loss: 0.1705516 Vali Loss: 0.3202151 Test Loss: 0.3678637
Validation loss decreased (0.321095 --> 0.320215).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 9.410805702209473
Epoch: 28, Steps: 60 | Train Loss: 0.1684013 Vali Loss: 0.3193637 Test Loss: 0.3674276
Validation loss decreased (0.320215 --> 0.319364).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 9.060696840286255
Epoch: 29, Steps: 60 | Train Loss: 0.1665358 Vali Loss: 0.3186564 Test Loss: 0.3670894
Validation loss decreased (0.319364 --> 0.318656).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 9.04242730140686
Epoch: 30, Steps: 60 | Train Loss: 0.1652768 Vali Loss: 0.3178686 Test Loss: 0.3667572
Validation loss decreased (0.318656 --> 0.317869).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 9.032680034637451
Epoch: 31, Steps: 60 | Train Loss: 0.1638155 Vali Loss: 0.3173062 Test Loss: 0.3663865
Validation loss decreased (0.317869 --> 0.317306).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 7.925939321517944
Epoch: 32, Steps: 60 | Train Loss: 0.1623201 Vali Loss: 0.3164678 Test Loss: 0.3661174
Validation loss decreased (0.317306 --> 0.316468).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 6.545773029327393
Epoch: 33, Steps: 60 | Train Loss: 0.1612939 Vali Loss: 0.3158830 Test Loss: 0.3658557
Validation loss decreased (0.316468 --> 0.315883).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 7.627897024154663
Epoch: 34, Steps: 60 | Train Loss: 0.1600295 Vali Loss: 0.3153984 Test Loss: 0.3656247
Validation loss decreased (0.315883 --> 0.315398).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 8.288424730300903
Epoch: 35, Steps: 60 | Train Loss: 0.1588434 Vali Loss: 0.3147995 Test Loss: 0.3653789
Validation loss decreased (0.315398 --> 0.314799).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 9.75903058052063
Epoch: 36, Steps: 60 | Train Loss: 0.1579274 Vali Loss: 0.3144093 Test Loss: 0.3650977
Validation loss decreased (0.314799 --> 0.314409).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 9.477586269378662
Epoch: 37, Steps: 60 | Train Loss: 0.1566186 Vali Loss: 0.3137273 Test Loss: 0.3648661
Validation loss decreased (0.314409 --> 0.313727).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 9.214672565460205
Epoch: 38, Steps: 60 | Train Loss: 0.1560770 Vali Loss: 0.3132231 Test Loss: 0.3646600
Validation loss decreased (0.313727 --> 0.313223).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 9.531506299972534
Epoch: 39, Steps: 60 | Train Loss: 0.1551114 Vali Loss: 0.3129492 Test Loss: 0.3644828
Validation loss decreased (0.313223 --> 0.312949).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 9.817110776901245
Epoch: 40, Steps: 60 | Train Loss: 0.1540901 Vali Loss: 0.3125510 Test Loss: 0.3643683
Validation loss decreased (0.312949 --> 0.312551).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 9.897964477539062
Epoch: 41, Steps: 60 | Train Loss: 0.1533772 Vali Loss: 0.3121587 Test Loss: 0.3641534
Validation loss decreased (0.312551 --> 0.312159).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 8.862941980361938
Epoch: 42, Steps: 60 | Train Loss: 0.1526665 Vali Loss: 0.3117666 Test Loss: 0.3640163
Validation loss decreased (0.312159 --> 0.311767).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 9.188315153121948
Epoch: 43, Steps: 60 | Train Loss: 0.1524230 Vali Loss: 0.3113437 Test Loss: 0.3638620
Validation loss decreased (0.311767 --> 0.311344).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 8.819148540496826
Epoch: 44, Steps: 60 | Train Loss: 0.1518058 Vali Loss: 0.3111560 Test Loss: 0.3636912
Validation loss decreased (0.311344 --> 0.311156).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 9.149736642837524
Epoch: 45, Steps: 60 | Train Loss: 0.1509870 Vali Loss: 0.3106922 Test Loss: 0.3635961
Validation loss decreased (0.311156 --> 0.310692).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 8.954780340194702
Epoch: 46, Steps: 60 | Train Loss: 0.1501713 Vali Loss: 0.3104884 Test Loss: 0.3634708
Validation loss decreased (0.310692 --> 0.310488).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 8.553006410598755
Epoch: 47, Steps: 60 | Train Loss: 0.1498900 Vali Loss: 0.3102417 Test Loss: 0.3633300
Validation loss decreased (0.310488 --> 0.310242).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 9.332722663879395
Epoch: 48, Steps: 60 | Train Loss: 0.1493850 Vali Loss: 0.3099611 Test Loss: 0.3632607
Validation loss decreased (0.310242 --> 0.309961).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 9.345414161682129
Epoch: 49, Steps: 60 | Train Loss: 0.1487185 Vali Loss: 0.3096339 Test Loss: 0.3631511
Validation loss decreased (0.309961 --> 0.309634).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 8.602924108505249
Epoch: 50, Steps: 60 | Train Loss: 0.1485477 Vali Loss: 0.3094948 Test Loss: 0.3630559
Validation loss decreased (0.309634 --> 0.309495).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 8.677904605865479
Epoch: 1, Steps: 60 | Train Loss: 0.5301029 Vali Loss: 0.2917377 Test Loss: 0.3552084
Validation loss decreased (inf --> 0.291738).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.603976726531982
Epoch: 2, Steps: 60 | Train Loss: 0.5159961 Vali Loss: 0.2870710 Test Loss: 0.3537282
Validation loss decreased (0.291738 --> 0.287071).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 9.336767435073853
Epoch: 3, Steps: 60 | Train Loss: 0.5133805 Vali Loss: 0.2862233 Test Loss: 0.3531321
Validation loss decreased (0.287071 --> 0.286223).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 9.290993928909302
Epoch: 4, Steps: 60 | Train Loss: 0.5126537 Vali Loss: 0.2842151 Test Loss: 0.3532092
Validation loss decreased (0.286223 --> 0.284215).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 8.809647798538208
Epoch: 5, Steps: 60 | Train Loss: 0.5117147 Vali Loss: 0.2837452 Test Loss: 0.3529956
Validation loss decreased (0.284215 --> 0.283745).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 7.955029010772705
Epoch: 6, Steps: 60 | Train Loss: 0.5108247 Vali Loss: 0.2828486 Test Loss: 0.3530931
Validation loss decreased (0.283745 --> 0.282849).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 7.74013352394104
Epoch: 7, Steps: 60 | Train Loss: 0.5104487 Vali Loss: 0.2823791 Test Loss: 0.3528883
Validation loss decreased (0.282849 --> 0.282379).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 8.495728492736816
Epoch: 8, Steps: 60 | Train Loss: 0.5096802 Vali Loss: 0.2816857 Test Loss: 0.3526727
Validation loss decreased (0.282379 --> 0.281686).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 8.200138092041016
Epoch: 9, Steps: 60 | Train Loss: 0.5101813 Vali Loss: 0.2817289 Test Loss: 0.3522778
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 7.905477285385132
Epoch: 10, Steps: 60 | Train Loss: 0.5097272 Vali Loss: 0.2821470 Test Loss: 0.3521266
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 7.820510625839233
Epoch: 11, Steps: 60 | Train Loss: 0.5089843 Vali Loss: 0.2818215 Test Loss: 0.3520027
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3324335515499115, mae:0.3750617504119873, rse:0.46237385272979736, corr:[0.26665416 0.26986623 0.26835257 0.26857919 0.2687138  0.26775298
 0.26712263 0.26663527 0.26548976 0.26413125 0.26304945 0.26174667
 0.2602998  0.25927338 0.2586084  0.25792044 0.25723448 0.2568698
 0.2564778  0.2553236  0.2538392  0.25261313 0.25122243 0.24934267
 0.24752925 0.24612543 0.24454084 0.24260348 0.2412151  0.24027705
 0.23896219 0.2373022  0.23629375 0.23546168 0.23384704 0.23213196
 0.2313262  0.23072423 0.2293487  0.22806798 0.22782777 0.22766317
 0.22690755 0.22621049 0.22564344 0.22440304 0.22263521 0.22100097
 0.2196174  0.21784276 0.21573928 0.21405928 0.2131046  0.2114892
 0.20894495 0.20696042 0.20563413 0.20418687 0.20278138 0.20145787
 0.20044866 0.1999302  0.19998792 0.1999059  0.19914699 0.19862871
 0.19821808 0.19770528 0.1973728  0.196911   0.19575047 0.19426802
 0.19310144 0.19229105 0.19140697 0.1903271  0.18963467 0.18891284
 0.1881543  0.18778619 0.18818577 0.18758306 0.18603969 0.18514037
 0.1849901  0.18470779 0.18432546 0.18451093 0.18444443 0.18366915
 0.18253805 0.18144995 0.18081218 0.18040186 0.1807583  0.18144488
 0.1812176  0.17980905 0.17794003 0.1762739  0.17539355 0.17544892
 0.17563793 0.17446923 0.17308995 0.1728372  0.17347196 0.17377542
 0.1727896  0.17153639 0.17053545 0.17024891 0.1700484  0.16967718
 0.16869935 0.16768917 0.16718397 0.16609444 0.16466488 0.16360728
 0.16296953 0.16163632 0.16043131 0.15999204 0.15917473 0.1572854
 0.15593159 0.15587907 0.15557769 0.1534376  0.15102021 0.14981386
 0.14959435 0.14921679 0.14878464 0.14845918 0.14756398 0.1464517
 0.14567134 0.14517052 0.14510627 0.14539085 0.14502919 0.14291245
 0.14039904 0.1395555  0.1389173  0.13684876 0.13524131 0.13610402
 0.13748932 0.1362698  0.13421363 0.13281634 0.13170779 0.1307126
 0.13071711 0.13102701 0.13159975 0.13328443 0.13385855 0.13233657
 0.13063402 0.13123645 0.13260989 0.1321074  0.13164018 0.1322115
 0.13167602 0.12871428 0.12717186 0.12726869 0.12614079 0.12344767
 0.12329222 0.12382154 0.12159158 0.11912997 0.11854642 0.11866163
 0.11737182 0.11773179 0.12005113 0.12009528 0.11795019 0.11897402
 0.1201975  0.11810847 0.12006409 0.12404754 0.12172996 0.12214235]
