Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=512, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  201607168.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.176992416381836
Epoch: 1, Steps: 14 | Train Loss: 0.9413350 Vali Loss: 0.6211877 Test Loss: 0.4578039
Validation loss decreased (inf --> 0.621188).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.092533826828003
Epoch: 2, Steps: 14 | Train Loss: 0.8311198 Vali Loss: 0.5602372 Test Loss: 0.4208878
Validation loss decreased (0.621188 --> 0.560237).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.750916004180908
Epoch: 3, Steps: 14 | Train Loss: 0.7709720 Vali Loss: 0.5278147 Test Loss: 0.4008212
Validation loss decreased (0.560237 --> 0.527815).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.8376693725585938
Epoch: 4, Steps: 14 | Train Loss: 0.7308740 Vali Loss: 0.5078243 Test Loss: 0.3901075
Validation loss decreased (0.527815 --> 0.507824).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.9397876262664795
Epoch: 5, Steps: 14 | Train Loss: 0.7091535 Vali Loss: 0.4868081 Test Loss: 0.3837582
Validation loss decreased (0.507824 --> 0.486808).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.9794416427612305
Epoch: 6, Steps: 14 | Train Loss: 0.7018185 Vali Loss: 0.4735743 Test Loss: 0.3796266
Validation loss decreased (0.486808 --> 0.473574).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.5828335285186768
Epoch: 7, Steps: 14 | Train Loss: 0.6886513 Vali Loss: 0.4676079 Test Loss: 0.3767132
Validation loss decreased (0.473574 --> 0.467608).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.2928965091705322
Epoch: 8, Steps: 14 | Train Loss: 0.6841751 Vali Loss: 0.4616141 Test Loss: 0.3744558
Validation loss decreased (0.467608 --> 0.461614).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.1543149948120117
Epoch: 9, Steps: 14 | Train Loss: 0.6812869 Vali Loss: 0.4467920 Test Loss: 0.3726377
Validation loss decreased (0.461614 --> 0.446792).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.427396774291992
Epoch: 10, Steps: 14 | Train Loss: 0.6727073 Vali Loss: 0.4487982 Test Loss: 0.3712629
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.2511465549468994
Epoch: 11, Steps: 14 | Train Loss: 0.6675178 Vali Loss: 0.4411029 Test Loss: 0.3701068
Validation loss decreased (0.446792 --> 0.441103).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.446284770965576
Epoch: 12, Steps: 14 | Train Loss: 0.6659391 Vali Loss: 0.4431098 Test Loss: 0.3690841
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.58689546585083
Epoch: 13, Steps: 14 | Train Loss: 0.6664122 Vali Loss: 0.4379807 Test Loss: 0.3681380
Validation loss decreased (0.441103 --> 0.437981).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.7141454219818115
Epoch: 14, Steps: 14 | Train Loss: 0.6645967 Vali Loss: 0.4366882 Test Loss: 0.3673300
Validation loss decreased (0.437981 --> 0.436688).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.0936036109924316
Epoch: 15, Steps: 14 | Train Loss: 0.6613832 Vali Loss: 0.4336959 Test Loss: 0.3667242
Validation loss decreased (0.436688 --> 0.433696).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.9643256664276123
Epoch: 16, Steps: 14 | Train Loss: 0.6587428 Vali Loss: 0.4329507 Test Loss: 0.3661013
Validation loss decreased (0.433696 --> 0.432951).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.068007230758667
Epoch: 17, Steps: 14 | Train Loss: 0.6538874 Vali Loss: 0.4242745 Test Loss: 0.3655189
Validation loss decreased (0.432951 --> 0.424275).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.1147499084472656
Epoch: 18, Steps: 14 | Train Loss: 0.6562513 Vali Loss: 0.4286289 Test Loss: 0.3650013
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.979921340942383
Epoch: 19, Steps: 14 | Train Loss: 0.6533951 Vali Loss: 0.4246832 Test Loss: 0.3645250
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.011845588684082
Epoch: 20, Steps: 14 | Train Loss: 0.6537138 Vali Loss: 0.4214249 Test Loss: 0.3640969
Validation loss decreased (0.424275 --> 0.421425).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.0132086277008057
Epoch: 21, Steps: 14 | Train Loss: 0.6487846 Vali Loss: 0.4193377 Test Loss: 0.3637498
Validation loss decreased (0.421425 --> 0.419338).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.0370004177093506
Epoch: 22, Steps: 14 | Train Loss: 0.6499949 Vali Loss: 0.4222013 Test Loss: 0.3634043
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.9051108360290527
Epoch: 23, Steps: 14 | Train Loss: 0.6485629 Vali Loss: 0.4218399 Test Loss: 0.3631212
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.1703691482543945
Epoch: 24, Steps: 14 | Train Loss: 0.6496860 Vali Loss: 0.4174567 Test Loss: 0.3628196
Validation loss decreased (0.419338 --> 0.417457).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.7810330390930176
Epoch: 25, Steps: 14 | Train Loss: 0.6496865 Vali Loss: 0.4146073 Test Loss: 0.3624960
Validation loss decreased (0.417457 --> 0.414607).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.0274953842163086
Epoch: 26, Steps: 14 | Train Loss: 0.6458426 Vali Loss: 0.4214827 Test Loss: 0.3622568
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.007431983947754
Epoch: 27, Steps: 14 | Train Loss: 0.6488727 Vali Loss: 0.4213340 Test Loss: 0.3620690
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.928273916244507
Epoch: 28, Steps: 14 | Train Loss: 0.6446851 Vali Loss: 0.4178494 Test Loss: 0.3618437
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.36116823554039, mae:0.4037620723247528, rse:0.4805010259151459, corr:[0.25149983 0.26019457 0.25782388 0.2579717  0.26003072 0.2593345
 0.25810316 0.2587063  0.25800484 0.25664255 0.25556728 0.254862
 0.25379473 0.2523112  0.25127497 0.25035855 0.2498557  0.24927065
 0.2485469  0.24744546 0.24642281 0.24589923 0.24448664 0.24264789
 0.2411997  0.2403542  0.23916718 0.23791215 0.23717794 0.23664334
 0.23585798 0.23477496 0.23403528 0.23324184 0.23229803 0.23134348
 0.23069967 0.23004699 0.22896399 0.22791238 0.22715046 0.22652389
 0.2259145  0.22498764 0.22359402 0.22251251 0.22136661 0.21965013
 0.21784258 0.21640319 0.21520944 0.21417423 0.21295461 0.21111321
 0.20930551 0.20822413 0.20709762 0.20532288 0.20358871 0.20252155
 0.20168714 0.20091861 0.20059851 0.20050995 0.20008798 0.19949354
 0.19896841 0.1985519  0.19781347 0.19698757 0.19622679 0.19504505
 0.19344051 0.19217397 0.19147167 0.19071929 0.18991692 0.1893077
 0.1885505  0.18749271 0.18647037 0.18590851 0.18577318 0.18553387
 0.18518509 0.18497114 0.18474609 0.18418367 0.1835877  0.18304493
 0.18240874 0.18192406 0.18183452 0.18174982 0.18146971 0.18104029
 0.18055761 0.17979969 0.17899163 0.17856175 0.17841625 0.17798956
 0.17734039 0.17666711 0.17604373 0.17520967 0.1747307  0.17433418
 0.17372712 0.1731773  0.17281152 0.1723093  0.17149884 0.17094713
 0.17052917 0.16973509 0.16860603 0.16772544 0.16709787 0.16585582
 0.1643136  0.16348182 0.16292703 0.16157183 0.16021994 0.15997462
 0.15981963 0.1586817  0.15743177 0.1569615  0.1563765  0.15491892
 0.15362068 0.15316664 0.15303656 0.15273435 0.1524055  0.15178777
 0.15033302 0.1488053  0.14820245 0.14810629 0.1474329  0.14597702
 0.14415847 0.14246131 0.14106417 0.1401316  0.13952968 0.13901769
 0.13828705 0.1372268  0.13622299 0.13552527 0.13519509 0.13440599
 0.1329032  0.13189435 0.13205656 0.131928   0.13072465 0.13020362
 0.1305992  0.13022807 0.12905873 0.1289501  0.12966825 0.12854749
 0.12581863 0.12415596 0.1241161  0.12350343 0.12204025 0.12113669
 0.12094238 0.11977449 0.11830403 0.11768534 0.11682726 0.11524
 0.11419135 0.11441606 0.11431113 0.11316506 0.11207374 0.11197039
 0.11151513 0.11056662 0.11061492 0.1113489  0.11147834 0.11055433
 0.10967158 0.10873958 0.10729395 0.10660036 0.1069686  0.10685974
 0.10559748 0.10439046 0.10439996 0.10467423 0.10438596 0.10371782
 0.1031169  0.10271798 0.10230981 0.1020743  0.10191762 0.10196246
 0.1018839  0.10135457 0.10061742 0.10055235 0.10103334 0.10038536
 0.0985629  0.09732711 0.09733573 0.09720976 0.09604231 0.09531275
 0.09545255 0.09588669 0.0955795  0.09495082 0.09458259 0.09448607
 0.09442735 0.09412275 0.0934089  0.09328624 0.09387983 0.09420756
 0.09354601 0.09332484 0.09433584 0.09494032 0.09386235 0.09257337
 0.09217601 0.09139245 0.08980475 0.08924726 0.08990657 0.08996487
 0.08918858 0.08877669 0.08911073 0.08922849 0.08910479 0.08942264
 0.08939558 0.08917129 0.08951814 0.09092782 0.09187337 0.09259208
 0.09345521 0.09404071 0.09370629 0.09353585 0.09460706 0.0948815
 0.09387832 0.09357576 0.09495813 0.09547032 0.09441582 0.0945503
 0.09594032 0.09600759 0.09493044 0.09492064 0.09647381 0.09663628
 0.09572741 0.09567487 0.0965158  0.09723773 0.09800892 0.09884859
 0.09903602 0.09889136 0.09885984 0.09882849 0.09876863 0.09935652
 0.09982041 0.09849327 0.0962424  0.09591249 0.09685303 0.09631137
 0.09480306 0.0942952  0.09471939 0.09476501 0.09511724 0.09543192
 0.09476107 0.09441704 0.09542115 0.09687418 0.0969306  0.09781585
 0.09883332 0.09823033 0.09651015 0.09709495 0.09907907 0.09826197
 0.09586965 0.09544839 0.09606468 0.09416097 0.0926417  0.09436777
 0.09557139 0.09437088 0.09459245 0.09819627 0.09956063 0.0988336
 0.09956137 0.10082052 0.10013236 0.10065579 0.1033636  0.10243668
 0.09930322 0.10184384 0.10377584 0.09754723 0.10488692 0.12245291]
