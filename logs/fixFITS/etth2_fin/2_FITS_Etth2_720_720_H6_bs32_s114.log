Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  34420736.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7217212
	speed: 0.1068s/iter; left time: 587.6634s
Epoch: 1 cost time: 11.864554405212402
Epoch: 1, Steps: 112 | Train Loss: 0.7737379 Vali Loss: 0.7746694 Test Loss: 0.4358729
Validation loss decreased (inf --> 0.774669).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5311136
	speed: 0.2777s/iter; left time: 1496.6130s
Epoch: 2 cost time: 14.450231552124023
Epoch: 2, Steps: 112 | Train Loss: 0.6015945 Vali Loss: 0.7301760 Test Loss: 0.4108886
Validation loss decreased (0.774669 --> 0.730176).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5115852
	speed: 0.2701s/iter; left time: 1425.5676s
Epoch: 3 cost time: 13.485348224639893
Epoch: 3, Steps: 112 | Train Loss: 0.5475582 Vali Loss: 0.7113326 Test Loss: 0.4033012
Validation loss decreased (0.730176 --> 0.711333).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5788778
	speed: 0.2567s/iter; left time: 1326.0477s
Epoch: 4 cost time: 12.86796498298645
Epoch: 4, Steps: 112 | Train Loss: 0.5149222 Vali Loss: 0.7025306 Test Loss: 0.3989685
Validation loss decreased (0.711333 --> 0.702531).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5233092
	speed: 0.2652s/iter; left time: 1340.0938s
Epoch: 5 cost time: 13.452109098434448
Epoch: 5, Steps: 112 | Train Loss: 0.4934703 Vali Loss: 0.6936974 Test Loss: 0.3958169
Validation loss decreased (0.702531 --> 0.693697).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5082965
	speed: 0.2649s/iter; left time: 1308.9848s
Epoch: 6 cost time: 13.356053113937378
Epoch: 6, Steps: 112 | Train Loss: 0.4767731 Vali Loss: 0.6865920 Test Loss: 0.3934519
Validation loss decreased (0.693697 --> 0.686592).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3678230
	speed: 0.2762s/iter; left time: 1333.6486s
Epoch: 7 cost time: 12.781112432479858
Epoch: 7, Steps: 112 | Train Loss: 0.4643279 Vali Loss: 0.6820517 Test Loss: 0.3913141
Validation loss decreased (0.686592 --> 0.682052).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3428796
	speed: 0.2511s/iter; left time: 1184.2727s
Epoch: 8 cost time: 12.135334253311157
Epoch: 8, Steps: 112 | Train Loss: 0.4539754 Vali Loss: 0.6769000 Test Loss: 0.3894513
Validation loss decreased (0.682052 --> 0.676900).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4161754
	speed: 0.2507s/iter; left time: 1154.5554s
Epoch: 9 cost time: 12.690373420715332
Epoch: 9, Steps: 112 | Train Loss: 0.4462238 Vali Loss: 0.6715404 Test Loss: 0.3880513
Validation loss decreased (0.676900 --> 0.671540).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4366207
	speed: 0.2598s/iter; left time: 1167.3316s
Epoch: 10 cost time: 12.980608940124512
Epoch: 10, Steps: 112 | Train Loss: 0.4409776 Vali Loss: 0.6715198 Test Loss: 0.3868015
Validation loss decreased (0.671540 --> 0.671520).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3771682
	speed: 0.2551s/iter; left time: 1117.3747s
Epoch: 11 cost time: 12.661858081817627
Epoch: 11, Steps: 112 | Train Loss: 0.4351400 Vali Loss: 0.6670790 Test Loss: 0.3858399
Validation loss decreased (0.671520 --> 0.667079).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3327861
	speed: 0.2566s/iter; left time: 1095.3712s
Epoch: 12 cost time: 12.283475637435913
Epoch: 12, Steps: 112 | Train Loss: 0.4312627 Vali Loss: 0.6636985 Test Loss: 0.3849317
Validation loss decreased (0.667079 --> 0.663698).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3347345
	speed: 0.2505s/iter; left time: 1041.3260s
Epoch: 13 cost time: 12.562925338745117
Epoch: 13, Steps: 112 | Train Loss: 0.4276894 Vali Loss: 0.6608776 Test Loss: 0.3842790
Validation loss decreased (0.663698 --> 0.660878).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3701578
	speed: 0.2531s/iter; left time: 1023.7304s
Epoch: 14 cost time: 12.827544450759888
Epoch: 14, Steps: 112 | Train Loss: 0.4251523 Vali Loss: 0.6582654 Test Loss: 0.3838171
Validation loss decreased (0.660878 --> 0.658265).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4052648
	speed: 0.2566s/iter; left time: 1009.2230s
Epoch: 15 cost time: 12.743399620056152
Epoch: 15, Steps: 112 | Train Loss: 0.4223543 Vali Loss: 0.6588684 Test Loss: 0.3833872
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3268839
	speed: 0.2521s/iter; left time: 963.1296s
Epoch: 16 cost time: 12.90428900718689
Epoch: 16, Steps: 112 | Train Loss: 0.4214455 Vali Loss: 0.6560306 Test Loss: 0.3830345
Validation loss decreased (0.658265 --> 0.656031).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4036029
	speed: 0.2589s/iter; left time: 960.1433s
Epoch: 17 cost time: 12.708178758621216
Epoch: 17, Steps: 112 | Train Loss: 0.4197770 Vali Loss: 0.6539382 Test Loss: 0.3827854
Validation loss decreased (0.656031 --> 0.653938).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4647232
	speed: 0.2585s/iter; left time: 929.6586s
Epoch: 18 cost time: 12.855133295059204
Epoch: 18, Steps: 112 | Train Loss: 0.4185693 Vali Loss: 0.6558259 Test Loss: 0.3824679
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4438108
	speed: 0.2549s/iter; left time: 888.3248s
Epoch: 19 cost time: 12.851974487304688
Epoch: 19, Steps: 112 | Train Loss: 0.4171036 Vali Loss: 0.6518595 Test Loss: 0.3823948
Validation loss decreased (0.653938 --> 0.651860).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3065018
	speed: 0.2584s/iter; left time: 871.4757s
Epoch: 20 cost time: 12.928939819335938
Epoch: 20, Steps: 112 | Train Loss: 0.4157083 Vali Loss: 0.6547717 Test Loss: 0.3821554
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4754110
	speed: 0.2484s/iter; left time: 810.0549s
Epoch: 21 cost time: 12.562192440032959
Epoch: 21, Steps: 112 | Train Loss: 0.4152917 Vali Loss: 0.6525031 Test Loss: 0.3820626
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4617695
	speed: 0.2529s/iter; left time: 796.3983s
Epoch: 22 cost time: 12.765797853469849
Epoch: 22, Steps: 112 | Train Loss: 0.4146621 Vali Loss: 0.6511040 Test Loss: 0.3819550
Validation loss decreased (0.651860 --> 0.651104).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3307105
	speed: 0.2581s/iter; left time: 783.7866s
Epoch: 23 cost time: 12.76723861694336
Epoch: 23, Steps: 112 | Train Loss: 0.4139456 Vali Loss: 0.6465058 Test Loss: 0.3819126
Validation loss decreased (0.651104 --> 0.646506).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4609044
	speed: 0.2581s/iter; left time: 754.9059s
Epoch: 24 cost time: 13.257572174072266
Epoch: 24, Steps: 112 | Train Loss: 0.4139562 Vali Loss: 0.6526443 Test Loss: 0.3818076
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3291824
	speed: 0.2752s/iter; left time: 774.2380s
Epoch: 25 cost time: 14.128427267074585
Epoch: 25, Steps: 112 | Train Loss: 0.4135110 Vali Loss: 0.6494558 Test Loss: 0.3817471
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3620684
	speed: 0.3373s/iter; left time: 911.0947s
Epoch: 26 cost time: 18.10390043258667
Epoch: 26, Steps: 112 | Train Loss: 0.4130293 Vali Loss: 0.6489868 Test Loss: 0.3817134
EarlyStopping counter: 3 out of 3
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  34420736.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8644572
	speed: 0.1721s/iter; left time: 946.5848s
Epoch: 1 cost time: 19.246658086776733
Epoch: 1, Steps: 112 | Train Loss: 0.8042270 Vali Loss: 0.6475933 Test Loss: 0.3807943
Validation loss decreased (inf --> 0.647593).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6488408
	speed: 0.3087s/iter; left time: 1663.7773s
Epoch: 2 cost time: 14.96259617805481
Epoch: 2, Steps: 112 | Train Loss: 0.7999213 Vali Loss: 0.6463614 Test Loss: 0.3801013
Validation loss decreased (0.647593 --> 0.646361).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.7295550
	speed: 0.2905s/iter; left time: 1533.1205s
Epoch: 3 cost time: 13.950641870498657
Epoch: 3, Steps: 112 | Train Loss: 0.8002640 Vali Loss: 0.6419992 Test Loss: 0.3800518
Validation loss decreased (0.646361 --> 0.641999).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 1.1348767
	speed: 0.2790s/iter; left time: 1441.0816s
Epoch: 4 cost time: 13.632289409637451
Epoch: 4, Steps: 112 | Train Loss: 0.7998163 Vali Loss: 0.6444535 Test Loss: 0.3799033
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.9200308
	speed: 0.2580s/iter; left time: 1303.5115s
Epoch: 5 cost time: 12.464795351028442
Epoch: 5, Steps: 112 | Train Loss: 0.7996220 Vali Loss: 0.6416000 Test Loss: 0.3797510
Validation loss decreased (0.641999 --> 0.641600).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 1.1270726
	speed: 0.2523s/iter; left time: 1246.8461s
Epoch: 6 cost time: 11.143983364105225
Epoch: 6, Steps: 112 | Train Loss: 0.7988338 Vali Loss: 0.6402259 Test Loss: 0.3798633
Validation loss decreased (0.641600 --> 0.640226).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6371167
	speed: 0.1764s/iter; left time: 851.9320s
Epoch: 7 cost time: 8.611894607543945
Epoch: 7, Steps: 112 | Train Loss: 0.7967903 Vali Loss: 0.6417465 Test Loss: 0.3799507
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6072555
	speed: 0.2321s/iter; left time: 1094.9425s
Epoch: 8 cost time: 12.820646524429321
Epoch: 8, Steps: 112 | Train Loss: 0.8002407 Vali Loss: 0.6444358 Test Loss: 0.3797454
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.8588796
	speed: 0.2607s/iter; left time: 1200.4228s
Epoch: 9 cost time: 12.814591646194458
Epoch: 9, Steps: 112 | Train Loss: 0.7981251 Vali Loss: 0.6400706 Test Loss: 0.3798973
Validation loss decreased (0.640226 --> 0.640071).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.7014115
	speed: 0.2682s/iter; left time: 1205.1357s
Epoch: 10 cost time: 13.0398108959198
Epoch: 10, Steps: 112 | Train Loss: 0.7977819 Vali Loss: 0.6417778 Test Loss: 0.3798778
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.8783278
	speed: 0.2619s/iter; left time: 1147.1708s
Epoch: 11 cost time: 13.213894367218018
Epoch: 11, Steps: 112 | Train Loss: 0.7959849 Vali Loss: 0.6409031 Test Loss: 0.3797984
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6800717
	speed: 0.2599s/iter; left time: 1109.5595s
Epoch: 12 cost time: 12.896502017974854
Epoch: 12, Steps: 112 | Train Loss: 0.7961476 Vali Loss: 0.6426716 Test Loss: 0.3798047
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3783496916294098, mae:0.4229614734649658, rse:0.4916459023952484, corr:[0.2201203  0.22234476 0.22059605 0.220121   0.21964233 0.21822919
 0.21757954 0.21735446 0.21623918 0.21473323 0.21383627 0.21260649
 0.2108249  0.20966177 0.20924087 0.20851922 0.2074592  0.20695573
 0.20674814 0.2058282  0.20436345 0.2032434  0.20226787 0.20111462
 0.19974658 0.1984281  0.19715291 0.19602184 0.19538082 0.19499876
 0.19453037 0.1937077  0.19271198 0.19177212 0.19104455 0.19048013
 0.18976055 0.18894948 0.1881467  0.18745431 0.18676677 0.18596295
 0.18520787 0.18448369 0.18346731 0.18238707 0.18173024 0.18096834
 0.17975774 0.17804399 0.17655553 0.17597897 0.17603979 0.1757329
 0.17494278 0.17443115 0.1739644  0.17314138 0.17232536 0.17187914
 0.17150252 0.17110412 0.17124121 0.17169039 0.17152433 0.17135471
 0.171522   0.17173162 0.17173418 0.17153256 0.17118113 0.17069598
 0.17037891 0.1701372  0.16966356 0.16880107 0.16843632 0.16869006
 0.16882202 0.1686877  0.16870002 0.16871026 0.16841154 0.16822803
 0.16850536 0.16903631 0.16919737 0.16893749 0.16855486 0.16842668
 0.16851747 0.16874982 0.16923244 0.16930144 0.16906726 0.16900541
 0.16923478 0.16925046 0.16889994 0.16859613 0.16853744 0.16801727
 0.16738468 0.16724059 0.16783847 0.16813825 0.16790861 0.16782878
 0.16800018 0.16787119 0.1671687  0.16653313 0.16642366 0.16675602
 0.16687751 0.16658421 0.16617951 0.16575637 0.16553813 0.1651174
 0.16430745 0.16324148 0.16237399 0.16184801 0.16130018 0.16050372
 0.15965825 0.15922554 0.15931468 0.15909736 0.15850051 0.15797244
 0.1579096  0.1574211  0.1562431  0.15523201 0.15462068 0.15407972
 0.15332863 0.15293586 0.15337345 0.15370363 0.15338472 0.15248497
 0.15110609 0.14990814 0.14929096 0.14946988 0.14982635 0.14952675
 0.1486327  0.14771287 0.14724939 0.1468917  0.14650103 0.14593805
 0.1451578  0.14448594 0.14419736 0.1438347  0.14300609 0.14283584
 0.14364181 0.14433871 0.14440577 0.14440614 0.14483121 0.14445424
 0.14311656 0.1421904  0.1425727  0.1429305  0.1422383  0.14137751
 0.14119951 0.14070128 0.13937667 0.13827832 0.13775724 0.13728791
 0.13666068 0.13663447 0.1373619  0.13763805 0.13707106 0.13675116
 0.1366603  0.13640878 0.13632701 0.13702464 0.13825843 0.138776
 0.13887459 0.13949312 0.14007677 0.13986051 0.13917154 0.13887504
 0.13924067 0.13929707 0.13905129 0.13886683 0.13899873 0.13900618
 0.13895802 0.13892868 0.13860004 0.13869594 0.13973683 0.14062124
 0.1398963  0.13875379 0.13939112 0.14114699 0.14162636 0.14027101
 0.13911    0.13899419 0.13913848 0.13928983 0.13954619 0.1400608
 0.13999437 0.14004967 0.14042616 0.14054154 0.14002863 0.1395562
 0.13973397 0.13975352 0.13936192 0.13933328 0.13977008 0.14015837
 0.14073771 0.14206365 0.14345591 0.14370133 0.14375067 0.14521194
 0.14746216 0.1481445  0.14719397 0.14659365 0.14694206 0.1472069
 0.14703679 0.14733954 0.14877227 0.14959954 0.14901862 0.14852957
 0.14942703 0.15066044 0.15089664 0.15147842 0.15314241 0.15453029
 0.15443666 0.15461358 0.15635051 0.15785027 0.1576369  0.15704657
 0.15799645 0.15906388 0.15878908 0.15831003 0.15873756 0.15942219
 0.15950923 0.1599413  0.16114278 0.16186947 0.16195141 0.16207324
 0.16220883 0.16211595 0.16250774 0.16400096 0.16513246 0.16460575
 0.16357227 0.16357613 0.16390337 0.16388166 0.164574   0.1663882
 0.16790685 0.16762498 0.16631223 0.16613437 0.1673319  0.16843481
 0.16807237 0.1666677  0.16607615 0.16725641 0.16904001 0.16998638
 0.17020336 0.17023227 0.16963543 0.16895649 0.16974074 0.1717346
 0.17182359 0.16955893 0.16781715 0.1689322  0.17122136 0.17217936
 0.17174472 0.17080456 0.17024267 0.17025036 0.17069112 0.17053737
 0.16945969 0.16887125 0.16955829 0.1705701  0.17030069 0.16926481
 0.16844565 0.1681227  0.16851781 0.16963598 0.17107837 0.17169754
 0.17114264 0.1707375  0.1711989  0.17233048 0.1734415  0.17362092
 0.17285268 0.17216825 0.1722368  0.17244565 0.17259723 0.17280272
 0.17356785 0.17447706 0.17508727 0.17519401 0.17485607 0.17468789
 0.17445087 0.17427479 0.1747257  0.17635982 0.17830305 0.17921533
 0.17911291 0.17960551 0.18074407 0.18133964 0.18114802 0.18125585
 0.1815525  0.18100415 0.18048826 0.18147291 0.18268503 0.18186237
 0.18053824 0.18181482 0.18413758 0.18449219 0.18326303 0.18326169
 0.18425995 0.18416835 0.18325344 0.18335998 0.18423428 0.18443431
 0.18417221 0.18429075 0.18435255 0.18387455 0.18360014 0.18362996
 0.18334828 0.18324001 0.18401141 0.18476897 0.18442886 0.18359348
 0.18358843 0.18433675 0.18501695 0.185502   0.18590342 0.18598482
 0.18598327 0.18575391 0.18543291 0.18512851 0.184985   0.18506181
 0.18488856 0.18441775 0.18411508 0.18399219 0.18342938 0.18270434
 0.18260953 0.18293417 0.18290731 0.18297978 0.18381329 0.18417314
 0.18310791 0.18211734 0.18267165 0.18349928 0.18291202 0.18190642
 0.1816108  0.18170938 0.18111902 0.18014699 0.17945603 0.17873174
 0.1776726  0.1765273  0.17604704 0.1757348  0.17473263 0.17379393
 0.17369266 0.17402267 0.1737684  0.1724796  0.1714999  0.17141385
 0.1712428  0.170489   0.16971745 0.1699419  0.17021686 0.16962488
 0.16807514 0.16727914 0.16786544 0.16876557 0.1683822  0.1667806
 0.16577403 0.16604954 0.16630343 0.16592532 0.16551174 0.16539188
 0.16511004 0.16451992 0.16427647 0.1642612  0.16358122 0.1626738
 0.1630282  0.16371769 0.16235982 0.16011629 0.1597324  0.16073781
 0.16058187 0.15870458 0.15796834 0.15894154 0.15931112 0.15819158
 0.1574912  0.15786524 0.1578161  0.15715821 0.15680256 0.15664122
 0.1558064  0.15493776 0.15488373 0.15455446 0.15332876 0.15224005
 0.1521184  0.15167414 0.1503266  0.14957693 0.15031007 0.15041244
 0.14855224 0.14686519 0.14733313 0.14839274 0.1475974  0.14565693
 0.14462218 0.14499722 0.14570855 0.14556889 0.14444795 0.1430251
 0.14205861 0.141225   0.13968971 0.13803048 0.13755125 0.13789098
 0.13742445 0.1363323  0.13557604 0.1352205  0.13467307 0.1335687
 0.13229361 0.13127024 0.13109037 0.131554   0.13175653 0.1311045
 0.12983726 0.12904951 0.12863007 0.12792046 0.12688698 0.12623845
 0.12593071 0.12528047 0.12345488 0.12053787 0.11880606 0.11904033
 0.11924966 0.11750216 0.1155649  0.11590853 0.11656806 0.11419395
 0.11024196 0.10915796 0.11068232 0.11078247 0.10887105 0.10771073
 0.10701776 0.10536488 0.10364924 0.10326117 0.10320211 0.10152493
 0.09960886 0.09899705 0.09854385 0.09721154 0.09634104 0.09666822
 0.09665412 0.09559328 0.09520454 0.09531126 0.09449486 0.09256242
 0.09146699 0.09186241 0.09220408 0.09144638 0.09078097 0.09064465
 0.09011991 0.08872803 0.0873216  0.086858   0.08627732 0.08429933
 0.08166945 0.08055831 0.08024539 0.07879718 0.07690728 0.07678851
 0.07768102 0.07660498 0.07366385 0.07227497 0.07269264 0.07229943
 0.07070175 0.07048508 0.07116082 0.06995352 0.06786045 0.06805503
 0.06968407 0.06946206 0.06706922 0.06558242 0.06578235 0.06585988
 0.06482994 0.0634613  0.06197512 0.06063598 0.0592022  0.05771754
 0.05601626 0.05482116 0.05412128 0.05295678 0.05121933 0.05022671
 0.05061822 0.05068854 0.04980726 0.0491772  0.04899474 0.04866714
 0.04856813 0.05003529 0.05156596 0.05131427 0.04979916 0.04914345
 0.04849877 0.04706859 0.04648158 0.04703362 0.04637385 0.04420184
 0.04313273 0.04393173 0.04402541 0.04254173 0.04166084 0.04256351
 0.04258416 0.04044758 0.03891361 0.03929865 0.03960018 0.03905411
 0.03813022 0.0372358  0.03683157 0.03702122 0.03663414 0.03550417
 0.03453074 0.03464559 0.03484956 0.03359326 0.0318644  0.03086271
 0.03051868 0.03001676 0.0295967  0.03020512 0.03121735 0.03107082
 0.02959719 0.02867059 0.02922265 0.02952278 0.02887689 0.02851977
 0.02945809 0.03022043 0.0291329  0.02801056 0.02864043 0.02985262
 0.02947196 0.02823944 0.02771964 0.02691033 0.02554254 0.02459211
 0.02526765 0.02498282 0.02330353 0.0221582  0.0226773  0.02305541
 0.02184633 0.01992455 0.01967054 0.02049919 0.02146246 0.02259589
 0.02291812 0.02070382 0.01826425 0.01805486 0.01925499 0.01919779
 0.01795703 0.01647474 0.01461167 0.01212184 0.01110752 0.01140369
 0.01089674 0.00826154 0.00622156 0.00607741 0.00595428 0.00526945
 0.00437954 0.0030783  0.00246299 0.00273562 0.00364954 0.00417076
 0.00370313 0.00348935 0.00415056 0.00590081 0.00801369 0.00808705]
