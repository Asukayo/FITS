Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_96', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=134, out_features=151, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9064832.0
params:  20385.0
Trainable parameters:  20385
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8316460
	speed: 0.1295s/iter; left time: 776.9556s
Epoch: 1 cost time: 15.809942245483398
Epoch: 1, Steps: 122 | Train Loss: 0.5338879 Vali Loss: 0.2522976 Test Loss: 0.2887564
Validation loss decreased (inf --> 0.252298).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5130483
	speed: 0.3283s/iter; left time: 1929.9073s
Epoch: 2 cost time: 13.885602235794067
Epoch: 2, Steps: 122 | Train Loss: 0.4426997 Vali Loss: 0.2342911 Test Loss: 0.2803326
Validation loss decreased (0.252298 --> 0.234291).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4928301
	speed: 0.3242s/iter; left time: 1866.6281s
Epoch: 3 cost time: 13.467705726623535
Epoch: 3, Steps: 122 | Train Loss: 0.4282850 Vali Loss: 0.2285103 Test Loss: 0.2775640
Validation loss decreased (0.234291 --> 0.228510).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3562446
	speed: 0.2642s/iter; left time: 1488.6783s
Epoch: 4 cost time: 13.567367553710938
Epoch: 4, Steps: 122 | Train Loss: 0.4217660 Vali Loss: 0.2232827 Test Loss: 0.2763110
Validation loss decreased (0.228510 --> 0.223283).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3632044
	speed: 0.3280s/iter; left time: 1808.0594s
Epoch: 5 cost time: 15.047104358673096
Epoch: 5, Steps: 122 | Train Loss: 0.4173655 Vali Loss: 0.2218010 Test Loss: 0.2752294
Validation loss decreased (0.223283 --> 0.221801).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4599393
	speed: 0.3235s/iter; left time: 1744.1659s
Epoch: 6 cost time: 15.014014959335327
Epoch: 6, Steps: 122 | Train Loss: 0.4144142 Vali Loss: 0.2204666 Test Loss: 0.2743927
Validation loss decreased (0.221801 --> 0.220467).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4615395
	speed: 0.3171s/iter; left time: 1670.9468s
Epoch: 7 cost time: 14.579929828643799
Epoch: 7, Steps: 122 | Train Loss: 0.4124795 Vali Loss: 0.2193182 Test Loss: 0.2743938
Validation loss decreased (0.220467 --> 0.219318).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3780020
	speed: 0.2876s/iter; left time: 1480.2168s
Epoch: 8 cost time: 14.392612218856812
Epoch: 8, Steps: 122 | Train Loss: 0.4105006 Vali Loss: 0.2192576 Test Loss: 0.2736379
Validation loss decreased (0.219318 --> 0.219258).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6221535
	speed: 0.2144s/iter; left time: 1077.3508s
Epoch: 9 cost time: 10.02190375328064
Epoch: 9, Steps: 122 | Train Loss: 0.4094314 Vali Loss: 0.2183315 Test Loss: 0.2733531
Validation loss decreased (0.219258 --> 0.218332).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4367473
	speed: 0.2870s/iter; left time: 1407.0891s
Epoch: 10 cost time: 15.646464824676514
Epoch: 10, Steps: 122 | Train Loss: 0.4082700 Vali Loss: 0.2161933 Test Loss: 0.2729476
Validation loss decreased (0.218332 --> 0.216193).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4584496
	speed: 0.3412s/iter; left time: 1631.3517s
Epoch: 11 cost time: 16.26829767227173
Epoch: 11, Steps: 122 | Train Loss: 0.4077308 Vali Loss: 0.2177207 Test Loss: 0.2723953
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3579173
	speed: 0.3492s/iter; left time: 1626.9375s
Epoch: 12 cost time: 15.28872537612915
Epoch: 12, Steps: 122 | Train Loss: 0.4072785 Vali Loss: 0.2167080 Test Loss: 0.2723680
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3350171
	speed: 0.3153s/iter; left time: 1430.4749s
Epoch: 13 cost time: 14.72442626953125
Epoch: 13, Steps: 122 | Train Loss: 0.4062913 Vali Loss: 0.2155540 Test Loss: 0.2724676
Validation loss decreased (0.216193 --> 0.215554).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4868883
	speed: 0.3140s/iter; left time: 1386.3986s
Epoch: 14 cost time: 14.663262605667114
Epoch: 14, Steps: 122 | Train Loss: 0.4058859 Vali Loss: 0.2154848 Test Loss: 0.2722970
Validation loss decreased (0.215554 --> 0.215485).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5220985
	speed: 0.3155s/iter; left time: 1354.4407s
Epoch: 15 cost time: 14.770574569702148
Epoch: 15, Steps: 122 | Train Loss: 0.4055638 Vali Loss: 0.2149410 Test Loss: 0.2723413
Validation loss decreased (0.215485 --> 0.214941).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3732732
	speed: 0.3100s/iter; left time: 1293.1414s
Epoch: 16 cost time: 14.518556594848633
Epoch: 16, Steps: 122 | Train Loss: 0.4047450 Vali Loss: 0.2140406 Test Loss: 0.2719345
Validation loss decreased (0.214941 --> 0.214041).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3334310
	speed: 0.3166s/iter; left time: 1281.8671s
Epoch: 17 cost time: 14.569876194000244
Epoch: 17, Steps: 122 | Train Loss: 0.4033559 Vali Loss: 0.2145646 Test Loss: 0.2721643
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3631954
	speed: 0.3135s/iter; left time: 1230.9526s
Epoch: 18 cost time: 14.551350116729736
Epoch: 18, Steps: 122 | Train Loss: 0.4044103 Vali Loss: 0.2142311 Test Loss: 0.2720460
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4921277
	speed: 0.3033s/iter; left time: 1153.9003s
Epoch: 19 cost time: 13.300252914428711
Epoch: 19, Steps: 122 | Train Loss: 0.4043487 Vali Loss: 0.2143608 Test Loss: 0.2718848
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.2726564407348633, mae:0.3374425172805786, rse:0.42081305384635925, corr:[0.27102536 0.2759094  0.27543694 0.27387446 0.2731868  0.27318144
 0.27281415 0.2717548  0.27054095 0.26909322 0.2676668  0.26628733
 0.2651703  0.26425135 0.26329404 0.26267338 0.26235142 0.2620184
 0.2612874  0.26011473 0.25882939 0.2577029  0.25655684 0.25505504
 0.25313938 0.25106204 0.24927828 0.24795398 0.2469343  0.24586336
 0.24455059 0.24312662 0.24178104 0.24063434 0.23939037 0.23779315
 0.2362087  0.23519643 0.23483835 0.23438342 0.2335079  0.23240727
 0.23147003 0.2308514  0.23048285 0.22982377 0.22858158 0.22674134
 0.2246835  0.22280866 0.22130968 0.21979    0.2185848  0.2176385
 0.21629506 0.2144844  0.21219236 0.20987579 0.20837498 0.20801343
 0.20814706 0.20769079 0.20685795 0.2064426  0.20638992 0.20698315
 0.2072526  0.20649248 0.20499131 0.2036514  0.20283674 0.20247345
 0.20180881 0.20047058 0.19903769 0.1976601  0.19700575 0.19654569
 0.19594747 0.19480366 0.19445291 0.19479892 0.19472599 0.19414575
 0.19376408 0.19446042 0.19530578 0.19505946 0.1930606  0.19172607
 0.19255792 0.19416256 0.1942452  0.19187365 0.19056657 0.19510052]
